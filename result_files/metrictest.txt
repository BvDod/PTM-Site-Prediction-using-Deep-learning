{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005199757555567337,
 'learning_rate_Hydroxylation-K': 0.009094826767926776,
 'learning_rate_Hydroxylation-P': 0.008891363729249296,
 'log_base': 1.8818097089265964,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2000846228,
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.279694827975284,
 'weight_decay_Hydroxylation-K': 6.670579333389868,
 'weight_decay_Hydroxylation-P': 2.911094087936903}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1468.674
[2,     1] loss: 1474.788
[3,     1] loss: 1464.277
[4,     1] loss: 1468.384
[5,     1] loss: 1463.434
[6,     1] loss: 1464.329
[7,     1] loss: 1464.052
[8,     1] loss: 1463.068
[9,     1] loss: 1467.324
[10,     1] loss: 1464.350
[11,     1] loss: 1457.315
[12,     1] loss: 1458.073
[13,     1] loss: 1449.565
[14,     1] loss: 1436.058
[15,     1] loss: 1425.048
[16,     1] loss: 1394.652
[17,     1] loss: 1358.603
[18,     1] loss: 1336.946
[19,     1] loss: 1286.891
[20,     1] loss: 1241.823
[21,     1] loss: 1262.233
[22,     1] loss: 1219.191
[23,     1] loss: 1248.224
[24,     1] loss: 1195.372
[25,     1] loss: 1188.038
[26,     1] loss: 1212.034
[27,     1] loss: 1151.688
[28,     1] loss: 1202.975
[29,     1] loss: 1201.582
[30,     1] loss: 1146.369
[31,     1] loss: 1127.171
[32,     1] loss: 1104.548
[33,     1] loss: 1169.593
[34,     1] loss: 1269.990
[35,     1] loss: 1069.351
[36,     1] loss: 1161.802
[37,     1] loss: 1091.493
[38,     1] loss: 1115.031
[39,     1] loss: 1066.257
[40,     1] loss: 1047.828
[41,     1] loss: 1098.220
[42,     1] loss: 1011.632
[43,     1] loss: 1047.713
[44,     1] loss: 947.314
[45,     1] loss: 1019.782
[46,     1] loss: 964.705
[47,     1] loss: 921.412
[48,     1] loss: 980.004
[49,     1] loss: 900.230
[50,     1] loss: 946.443
[51,     1] loss: 892.276
[52,     1] loss: 886.188
[53,     1] loss: 893.506
[54,     1] loss: 1260.705
[55,     1] loss: 1281.393
[56,     1] loss: 903.247
[57,     1] loss: 998.209
[58,     1] loss: 1095.181
[59,     1] loss: 1015.442
[60,     1] loss: 1017.458
[61,     1] loss: 1069.014
[62,     1] loss: 962.453
[63,     1] loss: 955.247
[64,     1] loss: 1037.468
[65,     1] loss: 947.667
[66,     1] loss: 892.919
[67,     1] loss: 894.248
[68,     1] loss: 880.379
[69,     1] loss: 905.935
[70,     1] loss: 944.651
[71,     1] loss: 873.078
[72,     1] loss: 774.049
[73,     1] loss: 748.255
[74,     1] loss: 711.131
[75,     1] loss: 665.358
[76,     1] loss: 701.352
[77,     1] loss: 702.142
[78,     1] loss: 748.035
[79,     1] loss: 1583.556
[80,     1] loss: 2170.949
[81,     1] loss: 1644.828
[82,     1] loss: 1220.757
[83,     1] loss: 1328.011
[84,     1] loss: 1382.358
[85,     1] loss: 1385.222
[86,     1] loss: 1370.688
[87,     1] loss: 1349.385
[88,     1] loss: 1459.871
[89,     1] loss: 1341.202
[90,     1] loss: 1364.888
[91,     1] loss: 1364.245
[92,     1] loss: 1378.564
[93,     1] loss: 1360.783
[94,     1] loss: 1338.658
[95,     1] loss: 1317.083
[96,     1] loss: 1318.669
[97,     1] loss: 1313.699
[98,     1] loss: 1316.974
[99,     1] loss: 1315.360
Early stopping applied (best metric=0.3654079735279083)
Finished Training
Total time taken: 24.83259415626526
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1469.835
[2,     1] loss: 1466.698
[3,     1] loss: 1470.764
[4,     1] loss: 1469.751
[5,     1] loss: 1465.333
[6,     1] loss: 1466.675
[7,     1] loss: 1464.537
[8,     1] loss: 1461.400
[9,     1] loss: 1464.104
[10,     1] loss: 1456.925
[11,     1] loss: 1458.803
[12,     1] loss: 1451.378
[13,     1] loss: 1444.017
[14,     1] loss: 1424.189
[15,     1] loss: 1402.035
[16,     1] loss: 1369.024
[17,     1] loss: 1350.908
[18,     1] loss: 1298.600
[19,     1] loss: 1236.097
[20,     1] loss: 1229.038
[21,     1] loss: 1218.144
[22,     1] loss: 1260.879
[23,     1] loss: 1205.752
[24,     1] loss: 1137.184
[25,     1] loss: 1159.390
[26,     1] loss: 1126.163
[27,     1] loss: 1199.518
[28,     1] loss: 1134.711
[29,     1] loss: 1168.050
[30,     1] loss: 1157.589
[31,     1] loss: 1119.761
[32,     1] loss: 1092.417
[33,     1] loss: 1077.258
[34,     1] loss: 1116.117
[35,     1] loss: 1064.281
[36,     1] loss: 982.264
[37,     1] loss: 1001.251
[38,     1] loss: 968.587
[39,     1] loss: 912.177
[40,     1] loss: 994.274
[41,     1] loss: 1158.854
[42,     1] loss: 1296.833
[43,     1] loss: 1015.180
[44,     1] loss: 1088.930
[45,     1] loss: 1097.152
[46,     1] loss: 1065.673
[47,     1] loss: 1067.496
[48,     1] loss: 1034.809
[49,     1] loss: 980.159
[50,     1] loss: 912.170
[51,     1] loss: 1031.774
[52,     1] loss: 951.361
[53,     1] loss: 945.886
[54,     1] loss: 863.124
[55,     1] loss: 969.388
[56,     1] loss: 873.303
[57,     1] loss: 969.405
[58,     1] loss: 866.302
[59,     1] loss: 860.755
[60,     1] loss: 856.855
[61,     1] loss: 867.243
[62,     1] loss: 849.140
[63,     1] loss: 779.152
[64,     1] loss: 816.755
[65,     1] loss: 880.852
[66,     1] loss: 1286.593
[67,     1] loss: 809.326
[68,     1] loss: 1031.765
[69,     1] loss: 878.756
[70,     1] loss: 1003.517
[71,     1] loss: 914.329
[72,     1] loss: 915.138
[73,     1] loss: 933.331
[74,     1] loss: 820.398
[75,     1] loss: 870.103
[76,     1] loss: 749.993
[77,     1] loss: 817.847
[78,     1] loss: 764.124
[79,     1] loss: 731.157
[80,     1] loss: 735.285
[81,     1] loss: 693.965
[82,     1] loss: 763.154
[83,     1] loss: 670.247
[84,     1] loss: 637.311
[85,     1] loss: 663.273
[86,     1] loss: 685.471
[87,     1] loss: 740.516
[88,     1] loss: 751.702
[89,     1] loss: 865.306
[90,     1] loss: 704.626
[91,     1] loss: 853.619
[92,     1] loss: 864.250
Early stopping applied (best metric=0.3636190891265869)
Finished Training
Total time taken: 13.18361496925354
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1465.628
[2,     1] loss: 1465.052
[3,     1] loss: 1463.728
[4,     1] loss: 1465.143
[5,     1] loss: 1466.123
[6,     1] loss: 1464.271
[7,     1] loss: 1459.428
[8,     1] loss: 1464.043
[9,     1] loss: 1460.837
[10,     1] loss: 1459.627
[11,     1] loss: 1452.787
[12,     1] loss: 1447.506
[13,     1] loss: 1424.529
[14,     1] loss: 1391.577
[15,     1] loss: 1364.508
[16,     1] loss: 1335.526
[17,     1] loss: 1293.979
[18,     1] loss: 1266.400
[19,     1] loss: 1276.261
[20,     1] loss: 1230.080
[21,     1] loss: 1203.540
[22,     1] loss: 1205.968
[23,     1] loss: 1193.147
[24,     1] loss: 1194.453
[25,     1] loss: 1154.896
[26,     1] loss: 1154.314
[27,     1] loss: 1132.833
[28,     1] loss: 1078.911
[29,     1] loss: 1142.870
[30,     1] loss: 1102.712
[31,     1] loss: 1103.533
[32,     1] loss: 1092.287
[33,     1] loss: 1069.403
[34,     1] loss: 1077.007
[35,     1] loss: 1015.309
[36,     1] loss: 1172.468
[37,     1] loss: 1094.022
[38,     1] loss: 1064.383
[39,     1] loss: 1053.724
[40,     1] loss: 1120.488
[41,     1] loss: 994.454
[42,     1] loss: 1049.373
[43,     1] loss: 1016.041
[44,     1] loss: 1030.768
[45,     1] loss: 1041.164
[46,     1] loss: 1026.048
[47,     1] loss: 1032.987
[48,     1] loss: 896.561
[49,     1] loss: 1052.015
[50,     1] loss: 931.287
[51,     1] loss: 868.514
[52,     1] loss: 967.018
[53,     1] loss: 863.676
[54,     1] loss: 921.700
[55,     1] loss: 832.052
[56,     1] loss: 856.199
[57,     1] loss: 944.475
[58,     1] loss: 1153.362
[59,     1] loss: 861.824
[60,     1] loss: 998.280
[61,     1] loss: 900.864
[62,     1] loss: 1011.404
[63,     1] loss: 917.122
[64,     1] loss: 1076.584
[65,     1] loss: 866.392
[66,     1] loss: 913.208
[67,     1] loss: 923.221
[68,     1] loss: 893.012
[69,     1] loss: 782.447
[70,     1] loss: 770.575
[71,     1] loss: 839.275
[72,     1] loss: 729.195
[73,     1] loss: 758.212
[74,     1] loss: 716.543
[75,     1] loss: 732.386
[76,     1] loss: 833.016
[77,     1] loss: 1075.483
[78,     1] loss: 1080.959
[79,     1] loss: 840.094
[80,     1] loss: 936.236
[81,     1] loss: 940.220
[82,     1] loss: 779.598
[83,     1] loss: 873.574
[84,     1] loss: 777.249
[85,     1] loss: 820.539
[86,     1] loss: 784.048
[87,     1] loss: 842.584
[88,     1] loss: 734.452
[89,     1] loss: 934.180
[90,     1] loss: 774.871
[91,     1] loss: 756.117
[92,     1] loss: 714.188
[93,     1] loss: 831.847
[94,     1] loss: 844.789
[95,     1] loss: 670.737
[96,     1] loss: 812.824
[97,     1] loss: 640.655
[98,     1] loss: 773.261
[99,     1] loss: 706.374
[100,     1] loss: 563.564
[101,     1] loss: 782.789
[102,     1] loss: 656.719
[103,     1] loss: 604.043
[104,     1] loss: 658.354
[105,     1] loss: 618.440
[106,     1] loss: 590.751
[107,     1] loss: 614.998
[108,     1] loss: 966.232
[109,     1] loss: 2203.559
[110,     1] loss: 758.381
[111,     1] loss: 1244.878
[112,     1] loss: 1106.330
[113,     1] loss: 1152.500
[114,     1] loss: 1157.677
[115,     1] loss: 1056.155
[116,     1] loss: 1046.613
[117,     1] loss: 1093.008
[118,     1] loss: 1062.027
[119,     1] loss: 1093.635
[120,     1] loss: 1021.972
[121,     1] loss: 1021.030
[122,     1] loss: 1010.513
[123,     1] loss: 962.407
Early stopping applied (best metric=0.2793270945549011)
Finished Training
Total time taken: 17.450125455856323
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1467.070
[2,     1] loss: 1464.260
[3,     1] loss: 1467.821
[4,     1] loss: 1463.304
[5,     1] loss: 1468.111
[6,     1] loss: 1466.323
[7,     1] loss: 1463.809
[8,     1] loss: 1462.001
[9,     1] loss: 1463.962
[10,     1] loss: 1462.965
[11,     1] loss: 1462.258
[12,     1] loss: 1455.285
[13,     1] loss: 1456.177
[14,     1] loss: 1446.737
[15,     1] loss: 1437.424
[16,     1] loss: 1409.479
[17,     1] loss: 1382.925
[18,     1] loss: 1349.162
[19,     1] loss: 1289.688
[20,     1] loss: 1289.372
[21,     1] loss: 1248.033
[22,     1] loss: 1232.033
[23,     1] loss: 1196.756
[24,     1] loss: 1215.019
[25,     1] loss: 1173.223
[26,     1] loss: 1256.081
[27,     1] loss: 1275.868
[28,     1] loss: 1122.345
[29,     1] loss: 1199.810
[30,     1] loss: 1156.176
[31,     1] loss: 1123.377
[32,     1] loss: 1135.376
[33,     1] loss: 1091.791
[34,     1] loss: 1107.359
[35,     1] loss: 1067.501
[36,     1] loss: 1156.558
[37,     1] loss: 1082.605
[38,     1] loss: 1126.566
[39,     1] loss: 1026.276
[40,     1] loss: 1150.309
[41,     1] loss: 1020.203
[42,     1] loss: 1074.635
[43,     1] loss: 971.854
[44,     1] loss: 973.074
[45,     1] loss: 975.457
[46,     1] loss: 931.634
[47,     1] loss: 964.984
[48,     1] loss: 1014.539
[49,     1] loss: 1088.303
[50,     1] loss: 1069.872
[51,     1] loss: 959.720
[52,     1] loss: 1057.832
[53,     1] loss: 966.960
[54,     1] loss: 991.614
[55,     1] loss: 942.819
[56,     1] loss: 867.631
[57,     1] loss: 864.350
[58,     1] loss: 925.133
[59,     1] loss: 924.210
[60,     1] loss: 1031.043
[61,     1] loss: 838.078
[62,     1] loss: 1020.939
[63,     1] loss: 1013.395
[64,     1] loss: 904.210
[65,     1] loss: 970.209
[66,     1] loss: 987.576
[67,     1] loss: 911.540
[68,     1] loss: 1059.725
[69,     1] loss: 810.704
[70,     1] loss: 998.645
[71,     1] loss: 782.020
[72,     1] loss: 947.383
[73,     1] loss: 830.929
[74,     1] loss: 803.459
[75,     1] loss: 879.043
[76,     1] loss: 804.041
[77,     1] loss: 964.259
[78,     1] loss: 1014.024
[79,     1] loss: 743.760
[80,     1] loss: 898.847
[81,     1] loss: 780.475
[82,     1] loss: 870.715
[83,     1] loss: 779.841
[84,     1] loss: 782.576
[85,     1] loss: 655.865
[86,     1] loss: 765.284
[87,     1] loss: 795.706
[88,     1] loss: 931.624
[89,     1] loss: 856.903
[90,     1] loss: 727.255
[91,     1] loss: 762.909
[92,     1] loss: 721.554
[93,     1] loss: 765.501
[94,     1] loss: 743.071
[95,     1] loss: 665.873
[96,     1] loss: 668.137
[97,     1] loss: 954.518
[98,     1] loss: 1685.326
[99,     1] loss: 805.456
[100,     1] loss: 995.584
Early stopping applied (best metric=0.35506361722946167)
Finished Training
Total time taken: 14.169594049453735
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1470.642
[2,     1] loss: 1469.021
[3,     1] loss: 1466.581
[4,     1] loss: 1466.051
[5,     1] loss: 1463.568
[6,     1] loss: 1464.260
[7,     1] loss: 1457.065
[8,     1] loss: 1454.978
[9,     1] loss: 1438.014
[10,     1] loss: 1413.589
[11,     1] loss: 1376.115
[12,     1] loss: 1332.503
[13,     1] loss: 1305.764
[14,     1] loss: 1303.096
[15,     1] loss: 1291.208
[16,     1] loss: 1231.859
[17,     1] loss: 1244.204
[18,     1] loss: 1197.284
[19,     1] loss: 1198.934
[20,     1] loss: 1197.781
[21,     1] loss: 1142.123
[22,     1] loss: 1133.561
[23,     1] loss: 1165.690
[24,     1] loss: 1094.668
[25,     1] loss: 1091.968
[26,     1] loss: 1079.345
[27,     1] loss: 1080.635
[28,     1] loss: 1103.808
[29,     1] loss: 1101.593
[30,     1] loss: 1046.151
[31,     1] loss: 1055.940
[32,     1] loss: 1010.184
[33,     1] loss: 1103.352
[34,     1] loss: 1065.889
[35,     1] loss: 955.942
[36,     1] loss: 993.616
[37,     1] loss: 932.647
[38,     1] loss: 903.982
[39,     1] loss: 950.852
[40,     1] loss: 1040.218
[41,     1] loss: 913.540
[42,     1] loss: 971.785
[43,     1] loss: 946.415
[44,     1] loss: 946.467
[45,     1] loss: 922.812
[46,     1] loss: 830.409
[47,     1] loss: 948.038
[48,     1] loss: 840.041
[49,     1] loss: 915.457
[50,     1] loss: 913.855
[51,     1] loss: 807.477
[52,     1] loss: 875.927
[53,     1] loss: 805.313
[54,     1] loss: 896.130
[55,     1] loss: 703.130
[56,     1] loss: 847.097
[57,     1] loss: 747.870
[58,     1] loss: 741.703
[59,     1] loss: 880.773
[60,     1] loss: 723.314
[61,     1] loss: 702.762
[62,     1] loss: 829.384
[63,     1] loss: 704.599
[64,     1] loss: 754.045
[65,     1] loss: 807.889
[66,     1] loss: 654.829
[67,     1] loss: 1041.736
[68,     1] loss: 1077.172
[69,     1] loss: 990.942
[70,     1] loss: 836.978
[71,     1] loss: 1081.598
[72,     1] loss: 990.007
Early stopping applied (best metric=0.4315606951713562)
Finished Training
Total time taken: 10.763155221939087
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1466.707
[2,     1] loss: 1466.532
[3,     1] loss: 1469.020
[4,     1] loss: 1465.619
[5,     1] loss: 1463.428
[6,     1] loss: 1462.484
[7,     1] loss: 1458.532
[8,     1] loss: 1455.131
[9,     1] loss: 1444.214
[10,     1] loss: 1430.094
[11,     1] loss: 1408.849
[12,     1] loss: 1359.780
[13,     1] loss: 1324.908
[14,     1] loss: 1319.136
[15,     1] loss: 1219.312
[16,     1] loss: 1268.517
[17,     1] loss: 1241.859
[18,     1] loss: 1222.285
[19,     1] loss: 1202.260
[20,     1] loss: 1212.414
[21,     1] loss: 1231.225
[22,     1] loss: 1181.440
[23,     1] loss: 1223.926
[24,     1] loss: 1147.889
[25,     1] loss: 1151.024
[26,     1] loss: 1111.512
[27,     1] loss: 1082.274
[28,     1] loss: 1102.878
[29,     1] loss: 1031.362
[30,     1] loss: 1106.439
[31,     1] loss: 1079.874
[32,     1] loss: 989.640
[33,     1] loss: 1005.687
[34,     1] loss: 960.635
[35,     1] loss: 1035.896
[36,     1] loss: 967.174
[37,     1] loss: 985.643
[38,     1] loss: 944.322
[39,     1] loss: 990.942
[40,     1] loss: 884.545
[41,     1] loss: 877.103
[42,     1] loss: 940.506
[43,     1] loss: 981.065
[44,     1] loss: 1162.821
[45,     1] loss: 867.964
[46,     1] loss: 1144.760
[47,     1] loss: 973.503
[48,     1] loss: 985.036
[49,     1] loss: 1032.425
[50,     1] loss: 998.690
[51,     1] loss: 932.491
[52,     1] loss: 913.735
[53,     1] loss: 823.716
[54,     1] loss: 913.207
[55,     1] loss: 866.102
[56,     1] loss: 952.021
[57,     1] loss: 890.506
[58,     1] loss: 800.419
[59,     1] loss: 854.176
[60,     1] loss: 885.881
[61,     1] loss: 877.749
[62,     1] loss: 833.148
[63,     1] loss: 816.825
[64,     1] loss: 881.528
[65,     1] loss: 866.646
[66,     1] loss: 739.461
[67,     1] loss: 836.126
[68,     1] loss: 851.222
[69,     1] loss: 682.428
[70,     1] loss: 838.506
[71,     1] loss: 675.284
[72,     1] loss: 698.065
[73,     1] loss: 667.343
[74,     1] loss: 649.522
[75,     1] loss: 629.928
[76,     1] loss: 711.266
[77,     1] loss: 1647.215
[78,     1] loss: 1986.339
[79,     1] loss: 1420.899
[80,     1] loss: 1111.752
[81,     1] loss: 1217.939
[82,     1] loss: 1248.606
[83,     1] loss: 1292.078
[84,     1] loss: 1315.376
[85,     1] loss: 1351.668
[86,     1] loss: 1319.233
[87,     1] loss: 1316.613
[88,     1] loss: 1308.127
[89,     1] loss: 1319.816
[90,     1] loss: 1295.906
[91,     1] loss: 1270.595
[92,     1] loss: 1225.204
[93,     1] loss: 1209.281
[94,     1] loss: 1186.170
[95,     1] loss: 1224.056
[96,     1] loss: 1147.864
[97,     1] loss: 1142.223
[98,     1] loss: 1137.307
[99,     1] loss: 1117.651
Early stopping applied (best metric=0.37957531213760376)
Finished Training
Total time taken: 15.09212875366211
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1470.946
[2,     1] loss: 1473.320
[3,     1] loss: 1466.118
[4,     1] loss: 1466.196
[5,     1] loss: 1464.020
[6,     1] loss: 1466.648
[7,     1] loss: 1463.506
[8,     1] loss: 1463.642
[9,     1] loss: 1462.803
[10,     1] loss: 1461.084
[11,     1] loss: 1459.361
[12,     1] loss: 1464.023
[13,     1] loss: 1457.899
[14,     1] loss: 1459.318
[15,     1] loss: 1452.747
[16,     1] loss: 1443.672
[17,     1] loss: 1439.756
[18,     1] loss: 1413.350
[19,     1] loss: 1404.054
[20,     1] loss: 1382.420
[21,     1] loss: 1361.098
[22,     1] loss: 1336.063
[23,     1] loss: 1319.453
[24,     1] loss: 1286.604
[25,     1] loss: 1240.971
[26,     1] loss: 1224.844
[27,     1] loss: 1331.444
[28,     1] loss: 1405.181
[29,     1] loss: 1197.965
[30,     1] loss: 1236.500
[31,     1] loss: 1307.685
[32,     1] loss: 1195.027
[33,     1] loss: 1202.653
[34,     1] loss: 1240.862
[35,     1] loss: 1258.042
[36,     1] loss: 1214.675
[37,     1] loss: 1188.999
[38,     1] loss: 1219.044
[39,     1] loss: 1190.621
[40,     1] loss: 1179.653
[41,     1] loss: 1153.866
[42,     1] loss: 1223.229
[43,     1] loss: 1101.886
[44,     1] loss: 1140.825
[45,     1] loss: 1091.240
[46,     1] loss: 1093.129
[47,     1] loss: 1089.704
[48,     1] loss: 1065.690
[49,     1] loss: 1036.868
[50,     1] loss: 1044.406
[51,     1] loss: 1027.918
[52,     1] loss: 1079.713
[53,     1] loss: 1005.360
[54,     1] loss: 1002.819
[55,     1] loss: 992.489
[56,     1] loss: 969.149
[57,     1] loss: 1008.301
[58,     1] loss: 1626.730
[59,     1] loss: 980.918
[60,     1] loss: 1239.218
[61,     1] loss: 1099.000
[62,     1] loss: 1122.799
[63,     1] loss: 1189.229
[64,     1] loss: 1205.378
[65,     1] loss: 1115.504
[66,     1] loss: 1074.516
[67,     1] loss: 1104.658
[68,     1] loss: 1099.688
[69,     1] loss: 1045.106
[70,     1] loss: 1042.271
[71,     1] loss: 1078.436
[72,     1] loss: 1000.804
[73,     1] loss: 984.279
[74,     1] loss: 996.364
[75,     1] loss: 983.706
[76,     1] loss: 979.815
[77,     1] loss: 979.805
[78,     1] loss: 889.163
[79,     1] loss: 951.748
[80,     1] loss: 964.038
[81,     1] loss: 878.116
[82,     1] loss: 862.626
[83,     1] loss: 1001.173
[84,     1] loss: 881.657
[85,     1] loss: 970.464
[86,     1] loss: 915.074
[87,     1] loss: 801.942
[88,     1] loss: 844.149
[89,     1] loss: 873.971
[90,     1] loss: 886.392
[91,     1] loss: 840.309
[92,     1] loss: 772.205
[93,     1] loss: 834.603
[94,     1] loss: 862.873
[95,     1] loss: 1126.229
[96,     1] loss: 981.724
[97,     1] loss: 818.482
[98,     1] loss: 938.261
[99,     1] loss: 898.422
[100,     1] loss: 963.820
[101,     1] loss: 841.070
[102,     1] loss: 826.790
[103,     1] loss: 800.814
[104,     1] loss: 789.630
[105,     1] loss: 736.643
[106,     1] loss: 734.173
[107,     1] loss: 902.169
[108,     1] loss: 1146.777
Early stopping applied (best metric=0.3404507339000702)
Finished Training
Total time taken: 16.034627437591553
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1462.174
[2,     1] loss: 1463.060
[3,     1] loss: 1469.054
[4,     1] loss: 1466.786
[5,     1] loss: 1464.126
[6,     1] loss: 1457.558
[7,     1] loss: 1458.399
[8,     1] loss: 1445.232
[9,     1] loss: 1431.560
[10,     1] loss: 1412.736
[11,     1] loss: 1360.561
[12,     1] loss: 1310.880
[13,     1] loss: 1289.277
[14,     1] loss: 1284.872
[15,     1] loss: 1219.373
[16,     1] loss: 1217.509
[17,     1] loss: 1165.002
[18,     1] loss: 1129.464
[19,     1] loss: 1213.999
[20,     1] loss: 1178.933
[21,     1] loss: 1081.786
[22,     1] loss: 1151.279
[23,     1] loss: 1133.270
[24,     1] loss: 1125.218
[25,     1] loss: 1104.562
[26,     1] loss: 1052.105
[27,     1] loss: 1066.721
[28,     1] loss: 1024.479
[29,     1] loss: 975.758
[30,     1] loss: 1085.356
[31,     1] loss: 999.678
[32,     1] loss: 1009.771
[33,     1] loss: 1325.832
[34,     1] loss: 1195.813
[35,     1] loss: 1125.343
[36,     1] loss: 1066.670
[37,     1] loss: 1110.983
[38,     1] loss: 1147.313
[39,     1] loss: 1050.614
[40,     1] loss: 1062.747
[41,     1] loss: 1047.201
[42,     1] loss: 1012.032
[43,     1] loss: 1004.628
[44,     1] loss: 978.584
[45,     1] loss: 990.130
[46,     1] loss: 991.532
[47,     1] loss: 930.215
[48,     1] loss: 930.413
[49,     1] loss: 918.977
[50,     1] loss: 851.819
[51,     1] loss: 859.160
[52,     1] loss: 901.726
[53,     1] loss: 785.486
[54,     1] loss: 855.468
[55,     1] loss: 936.562
[56,     1] loss: 726.500
[57,     1] loss: 930.836
[58,     1] loss: 721.049
[59,     1] loss: 794.515
[60,     1] loss: 809.638
[61,     1] loss: 720.059
[62,     1] loss: 752.001
[63,     1] loss: 846.447
[64,     1] loss: 799.310
[65,     1] loss: 743.780
[66,     1] loss: 726.045
[67,     1] loss: 751.263
[68,     1] loss: 701.482
[69,     1] loss: 726.434
[70,     1] loss: 764.301
[71,     1] loss: 688.213
[72,     1] loss: 713.953
[73,     1] loss: 845.477
[74,     1] loss: 679.560
[75,     1] loss: 656.618
[76,     1] loss: 778.694
[77,     1] loss: 949.271
[78,     1] loss: 811.628
[79,     1] loss: 776.828
[80,     1] loss: 758.591
[81,     1] loss: 766.715
Early stopping applied (best metric=0.3812403678894043)
Finished Training
Total time taken: 12.66133451461792
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1466.802
[2,     1] loss: 1463.295
[3,     1] loss: 1464.752
[4,     1] loss: 1467.586
[5,     1] loss: 1461.851
[6,     1] loss: 1461.738
[7,     1] loss: 1459.873
[8,     1] loss: 1451.331
[9,     1] loss: 1439.896
[10,     1] loss: 1421.717
[11,     1] loss: 1385.351
[12,     1] loss: 1342.401
[13,     1] loss: 1339.287
[14,     1] loss: 1282.683
[15,     1] loss: 1246.987
[16,     1] loss: 1228.589
[17,     1] loss: 1170.398
[18,     1] loss: 1252.916
[19,     1] loss: 1194.458
[20,     1] loss: 1297.655
[21,     1] loss: 1222.246
[22,     1] loss: 1203.188
[23,     1] loss: 1142.758
[24,     1] loss: 1207.720
[25,     1] loss: 1137.151
[26,     1] loss: 1153.524
[27,     1] loss: 1122.931
[28,     1] loss: 1108.362
[29,     1] loss: 1093.770
[30,     1] loss: 1015.505
[31,     1] loss: 1073.594
[32,     1] loss: 1038.128
[33,     1] loss: 1080.914
[34,     1] loss: 1065.060
[35,     1] loss: 1023.686
[36,     1] loss: 977.651
[37,     1] loss: 1005.527
[38,     1] loss: 977.780
[39,     1] loss: 1004.946
[40,     1] loss: 905.726
[41,     1] loss: 944.231
[42,     1] loss: 1019.165
[43,     1] loss: 1232.562
[44,     1] loss: 973.766
[45,     1] loss: 971.733
[46,     1] loss: 941.743
[47,     1] loss: 994.279
[48,     1] loss: 960.753
[49,     1] loss: 975.865
[50,     1] loss: 938.155
[51,     1] loss: 925.608
[52,     1] loss: 864.118
[53,     1] loss: 915.071
[54,     1] loss: 829.704
[55,     1] loss: 933.400
[56,     1] loss: 1018.050
[57,     1] loss: 793.696
[58,     1] loss: 938.950
[59,     1] loss: 832.612
[60,     1] loss: 816.068
[61,     1] loss: 832.912
[62,     1] loss: 757.342
[63,     1] loss: 773.663
[64,     1] loss: 858.242
[65,     1] loss: 777.592
[66,     1] loss: 708.075
[67,     1] loss: 755.735
[68,     1] loss: 928.313
[69,     1] loss: 1210.664
[70,     1] loss: 752.601
[71,     1] loss: 1025.272
[72,     1] loss: 1013.239
[73,     1] loss: 900.465
[74,     1] loss: 953.522
[75,     1] loss: 944.506
[76,     1] loss: 797.785
[77,     1] loss: 889.673
[78,     1] loss: 713.064
[79,     1] loss: 868.273
[80,     1] loss: 753.326
[81,     1] loss: 805.824
Early stopping applied (best metric=0.38961514830589294)
Finished Training
Total time taken: 12.132824420928955
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1466.967
[2,     1] loss: 1465.000
[3,     1] loss: 1467.732
[4,     1] loss: 1471.251
[5,     1] loss: 1466.737
[6,     1] loss: 1466.712
[7,     1] loss: 1467.017
[8,     1] loss: 1461.937
[9,     1] loss: 1458.340
[10,     1] loss: 1450.856
[11,     1] loss: 1445.632
[12,     1] loss: 1428.061
[13,     1] loss: 1401.092
[14,     1] loss: 1356.131
[15,     1] loss: 1331.377
[16,     1] loss: 1271.068
[17,     1] loss: 1273.918
[18,     1] loss: 1217.780
[19,     1] loss: 1215.325
[20,     1] loss: 1224.611
[21,     1] loss: 1155.894
[22,     1] loss: 1198.797
[23,     1] loss: 1153.764
[24,     1] loss: 1129.671
[25,     1] loss: 1150.359
[26,     1] loss: 1106.495
[27,     1] loss: 1120.427
[28,     1] loss: 1121.568
[29,     1] loss: 1081.789
[30,     1] loss: 1098.130
[31,     1] loss: 1059.245
[32,     1] loss: 1051.557
[33,     1] loss: 1050.683
[34,     1] loss: 1031.053
[35,     1] loss: 1002.899
[36,     1] loss: 984.903
[37,     1] loss: 959.461
[38,     1] loss: 910.592
[39,     1] loss: 928.524
[40,     1] loss: 930.452
[41,     1] loss: 862.140
[42,     1] loss: 862.977
[43,     1] loss: 865.900
[44,     1] loss: 1084.014
[45,     1] loss: 1556.395
[46,     1] loss: 1006.278
[47,     1] loss: 1116.107
[48,     1] loss: 1041.495
[49,     1] loss: 1049.401
[50,     1] loss: 1091.287
[51,     1] loss: 1060.778
[52,     1] loss: 1081.244
[53,     1] loss: 1022.126
[54,     1] loss: 1012.177
[55,     1] loss: 1001.184
[56,     1] loss: 930.397
[57,     1] loss: 994.020
[58,     1] loss: 1008.543
[59,     1] loss: 910.560
[60,     1] loss: 1006.865
[61,     1] loss: 859.991
[62,     1] loss: 904.363
[63,     1] loss: 863.859
[64,     1] loss: 832.527
[65,     1] loss: 795.356
[66,     1] loss: 827.257
[67,     1] loss: 775.797
[68,     1] loss: 790.473
[69,     1] loss: 1009.309
[70,     1] loss: 1133.865
[71,     1] loss: 810.861
[72,     1] loss: 944.282
[73,     1] loss: 998.326
[74,     1] loss: 902.417
[75,     1] loss: 942.988
[76,     1] loss: 867.802
[77,     1] loss: 764.069
[78,     1] loss: 844.124
[79,     1] loss: 851.451
[80,     1] loss: 821.963
[81,     1] loss: 859.580
[82,     1] loss: 750.710
[83,     1] loss: 768.382
[84,     1] loss: 776.135
[85,     1] loss: 836.862
[86,     1] loss: 757.276
[87,     1] loss: 747.111
[88,     1] loss: 726.211
[89,     1] loss: 757.479
[90,     1] loss: 650.613
[91,     1] loss: 616.751
[92,     1] loss: 686.292
[93,     1] loss: 1027.112
[94,     1] loss: 1962.124
[95,     1] loss: 874.329
[96,     1] loss: 1081.056
[97,     1] loss: 1181.862
[98,     1] loss: 1142.143
[99,     1] loss: 1193.805
[100,     1] loss: 1195.132
[101,     1] loss: 1161.882
[102,     1] loss: 1088.838
[103,     1] loss: 1096.963
[104,     1] loss: 1098.428
[105,     1] loss: 1022.518
[106,     1] loss: 1033.361
[107,     1] loss: 1058.728
[108,     1] loss: 1069.030
[109,     1] loss: 1047.609
[110,     1] loss: 1056.623
[111,     1] loss: 980.333
[112,     1] loss: 974.348
[113,     1] loss: 987.764
Early stopping applied (best metric=0.35095614194869995)
Finished Training
Total time taken: 16.66926074028015
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1469.551
[2,     1] loss: 1468.330
[3,     1] loss: 1464.552
[4,     1] loss: 1460.773
[5,     1] loss: 1461.019
[6,     1] loss: 1461.527
[7,     1] loss: 1465.348
[8,     1] loss: 1463.415
[9,     1] loss: 1462.897
[10,     1] loss: 1458.306
[11,     1] loss: 1454.564
[12,     1] loss: 1452.699
[13,     1] loss: 1452.943
[14,     1] loss: 1433.564
[15,     1] loss: 1409.544
[16,     1] loss: 1383.928
[17,     1] loss: 1344.163
[18,     1] loss: 1314.309
[19,     1] loss: 1271.380
[20,     1] loss: 1247.815
[21,     1] loss: 1248.098
[22,     1] loss: 1290.536
[23,     1] loss: 1209.207
[24,     1] loss: 1237.070
[25,     1] loss: 1156.620
[26,     1] loss: 1211.649
[27,     1] loss: 1194.425
[28,     1] loss: 1162.283
[29,     1] loss: 1194.502
[30,     1] loss: 1156.797
[31,     1] loss: 1164.908
[32,     1] loss: 1106.373
[33,     1] loss: 1203.772
[34,     1] loss: 1111.062
[35,     1] loss: 1134.674
[36,     1] loss: 1111.085
[37,     1] loss: 1058.362
[38,     1] loss: 1056.458
[39,     1] loss: 1021.232
[40,     1] loss: 1066.428
[41,     1] loss: 1074.448
[42,     1] loss: 1030.011
[43,     1] loss: 1107.841
[44,     1] loss: 1022.422
[45,     1] loss: 965.805
[46,     1] loss: 1045.024
[47,     1] loss: 973.406
[48,     1] loss: 950.010
[49,     1] loss: 907.376
[50,     1] loss: 1031.491
[51,     1] loss: 1255.000
[52,     1] loss: 1017.496
[53,     1] loss: 951.472
[54,     1] loss: 1012.083
[55,     1] loss: 980.043
[56,     1] loss: 967.696
[57,     1] loss: 977.498
[58,     1] loss: 844.535
[59,     1] loss: 972.268
[60,     1] loss: 887.653
[61,     1] loss: 926.713
[62,     1] loss: 826.662
[63,     1] loss: 867.864
[64,     1] loss: 827.574
[65,     1] loss: 814.820
[66,     1] loss: 761.403
[67,     1] loss: 746.051
[68,     1] loss: 1009.872
[69,     1] loss: 942.902
[70,     1] loss: 759.479
[71,     1] loss: 773.716
[72,     1] loss: 775.689
[73,     1] loss: 864.354
[74,     1] loss: 884.788
[75,     1] loss: 743.708
[76,     1] loss: 940.609
[77,     1] loss: 1142.121
[78,     1] loss: 747.422
[79,     1] loss: 929.650
[80,     1] loss: 758.482
[81,     1] loss: 851.061
[82,     1] loss: 728.938
[83,     1] loss: 859.240
[84,     1] loss: 765.387
[85,     1] loss: 784.900
[86,     1] loss: 832.031
[87,     1] loss: 657.123
[88,     1] loss: 695.896
[89,     1] loss: 668.176
[90,     1] loss: 703.068
[91,     1] loss: 651.186
[92,     1] loss: 635.337
[93,     1] loss: 592.711
[94,     1] loss: 541.448
[95,     1] loss: 493.808
[96,     1] loss: 744.971
[97,     1] loss: 2203.376
[98,     1] loss: 1252.218
Early stopping applied (best metric=0.38282740116119385)
Finished Training
Total time taken: 14.80261492729187
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1469.805
[2,     1] loss: 1467.443
[3,     1] loss: 1469.947
[4,     1] loss: 1463.590
[5,     1] loss: 1466.092
[6,     1] loss: 1463.622
[7,     1] loss: 1463.509
[8,     1] loss: 1461.117
[9,     1] loss: 1466.327
[10,     1] loss: 1461.748
[11,     1] loss: 1461.470
[12,     1] loss: 1460.589
[13,     1] loss: 1465.101
[14,     1] loss: 1463.821
[15,     1] loss: 1462.830
[16,     1] loss: 1460.624
[17,     1] loss: 1456.569
[18,     1] loss: 1453.808
[19,     1] loss: 1441.185
[20,     1] loss: 1426.515
[21,     1] loss: 1398.193
[22,     1] loss: 1367.152
[23,     1] loss: 1323.191
[24,     1] loss: 1290.814
[25,     1] loss: 1240.843
[26,     1] loss: 1249.227
[27,     1] loss: 1243.132
[28,     1] loss: 1255.393
[29,     1] loss: 1201.870
[30,     1] loss: 1178.970
[31,     1] loss: 1189.502
[32,     1] loss: 1124.861
[33,     1] loss: 1196.249
[34,     1] loss: 1164.772
[35,     1] loss: 1100.286
[36,     1] loss: 1154.786
[37,     1] loss: 1059.816
[38,     1] loss: 1052.833
[39,     1] loss: 1035.501
[40,     1] loss: 1066.367
[41,     1] loss: 1038.875
[42,     1] loss: 1007.230
[43,     1] loss: 1005.797
[44,     1] loss: 1006.562
[45,     1] loss: 977.874
[46,     1] loss: 1038.524
[47,     1] loss: 1053.014
[48,     1] loss: 1039.967
[49,     1] loss: 956.049
[50,     1] loss: 981.111
[51,     1] loss: 972.021
[52,     1] loss: 943.961
[53,     1] loss: 951.342
[54,     1] loss: 952.236
[55,     1] loss: 873.973
[56,     1] loss: 897.472
[57,     1] loss: 838.154
[58,     1] loss: 959.366
[59,     1] loss: 1201.807
[60,     1] loss: 1010.398
[61,     1] loss: 962.790
[62,     1] loss: 989.758
[63,     1] loss: 1039.086
[64,     1] loss: 996.896
[65,     1] loss: 905.099
[66,     1] loss: 975.844
[67,     1] loss: 877.643
[68,     1] loss: 907.923
[69,     1] loss: 887.428
[70,     1] loss: 843.211
[71,     1] loss: 822.877
[72,     1] loss: 786.155
[73,     1] loss: 786.457
[74,     1] loss: 782.196
[75,     1] loss: 819.563
[76,     1] loss: 988.997
[77,     1] loss: 829.640
[78,     1] loss: 700.674
[79,     1] loss: 790.185
[80,     1] loss: 782.576
[81,     1] loss: 746.292
[82,     1] loss: 859.130
[83,     1] loss: 683.224
[84,     1] loss: 831.599
[85,     1] loss: 948.008
[86,     1] loss: 684.865
[87,     1] loss: 902.581
[88,     1] loss: 724.434
Early stopping applied (best metric=0.3796932101249695)
Finished Training
Total time taken: 12.644157409667969
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1467.552
[2,     1] loss: 1469.920
[3,     1] loss: 1463.893
[4,     1] loss: 1461.862
[5,     1] loss: 1455.352
[6,     1] loss: 1449.725
[7,     1] loss: 1431.266
[8,     1] loss: 1385.110
[9,     1] loss: 1347.167
[10,     1] loss: 1312.463
[11,     1] loss: 1272.199
[12,     1] loss: 1247.568
[13,     1] loss: 1217.633
[14,     1] loss: 1236.779
[15,     1] loss: 1157.386
[16,     1] loss: 1255.746
[17,     1] loss: 1192.551
[18,     1] loss: 1182.730
[19,     1] loss: 1187.900
[20,     1] loss: 1144.969
[21,     1] loss: 1197.289
[22,     1] loss: 1101.509
[23,     1] loss: 1116.143
[24,     1] loss: 1072.259
[25,     1] loss: 1064.323
[26,     1] loss: 1034.901
[27,     1] loss: 1048.920
[28,     1] loss: 1247.381
[29,     1] loss: 1043.290
[30,     1] loss: 1061.262
[31,     1] loss: 1058.302
[32,     1] loss: 1047.957
[33,     1] loss: 996.043
[34,     1] loss: 1149.191
[35,     1] loss: 1046.165
[36,     1] loss: 1062.211
[37,     1] loss: 947.203
[38,     1] loss: 991.242
[39,     1] loss: 934.939
[40,     1] loss: 937.959
[41,     1] loss: 919.621
[42,     1] loss: 863.276
[43,     1] loss: 976.193
[44,     1] loss: 955.220
[45,     1] loss: 890.146
[46,     1] loss: 882.067
[47,     1] loss: 1004.058
[48,     1] loss: 865.100
[49,     1] loss: 928.185
[50,     1] loss: 877.629
[51,     1] loss: 868.038
[52,     1] loss: 900.633
[53,     1] loss: 817.057
[54,     1] loss: 822.995
[55,     1] loss: 834.175
[56,     1] loss: 723.551
[57,     1] loss: 806.878
[58,     1] loss: 1061.703
[59,     1] loss: 959.892
[60,     1] loss: 759.323
[61,     1] loss: 907.594
[62,     1] loss: 786.891
[63,     1] loss: 916.811
[64,     1] loss: 771.372
[65,     1] loss: 875.744
[66,     1] loss: 726.676
[67,     1] loss: 771.847
[68,     1] loss: 737.773
[69,     1] loss: 731.819
[70,     1] loss: 745.627
[71,     1] loss: 729.133
[72,     1] loss: 659.980
[73,     1] loss: 686.222
[74,     1] loss: 673.676
[75,     1] loss: 666.257
[76,     1] loss: 682.134
[77,     1] loss: 813.842
[78,     1] loss: 2379.514
[79,     1] loss: 875.355
[80,     1] loss: 1404.143
[81,     1] loss: 1179.392
[82,     1] loss: 1199.261
[83,     1] loss: 1267.047
[84,     1] loss: 1297.061
[85,     1] loss: 1280.447
[86,     1] loss: 1215.597
[87,     1] loss: 1155.107
[88,     1] loss: 1115.476
[89,     1] loss: 1113.436
[90,     1] loss: 1089.332
[91,     1] loss: 1133.279
Early stopping applied (best metric=0.373475581407547)
Finished Training
Total time taken: 13.334058284759521
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1464.129
[2,     1] loss: 1467.291
[3,     1] loss: 1466.723
[4,     1] loss: 1461.559
[5,     1] loss: 1456.144
[6,     1] loss: 1451.252
[7,     1] loss: 1441.192
[8,     1] loss: 1410.704
[9,     1] loss: 1369.191
[10,     1] loss: 1310.848
[11,     1] loss: 1293.143
[12,     1] loss: 1236.190
[13,     1] loss: 1258.002
[14,     1] loss: 1180.409
[15,     1] loss: 1206.454
[16,     1] loss: 1167.870
[17,     1] loss: 1148.900
[18,     1] loss: 1109.378
[19,     1] loss: 1143.775
[20,     1] loss: 1115.044
[21,     1] loss: 1111.616
[22,     1] loss: 1079.188
[23,     1] loss: 1067.137
[24,     1] loss: 1131.664
[25,     1] loss: 1066.143
[26,     1] loss: 1016.967
[27,     1] loss: 1014.131
[28,     1] loss: 1066.783
[29,     1] loss: 1098.909
[30,     1] loss: 1103.193
[31,     1] loss: 980.372
[32,     1] loss: 1007.217
[33,     1] loss: 992.386
[34,     1] loss: 986.021
[35,     1] loss: 973.300
[36,     1] loss: 905.684
[37,     1] loss: 903.720
[38,     1] loss: 954.210
[39,     1] loss: 836.572
[40,     1] loss: 933.094
[41,     1] loss: 1107.890
[42,     1] loss: 947.299
[43,     1] loss: 919.110
[44,     1] loss: 915.023
[45,     1] loss: 949.287
[46,     1] loss: 877.337
[47,     1] loss: 882.765
[48,     1] loss: 857.578
[49,     1] loss: 903.305
[50,     1] loss: 794.521
[51,     1] loss: 785.192
[52,     1] loss: 852.829
[53,     1] loss: 742.117
[54,     1] loss: 754.742
[55,     1] loss: 719.909
[56,     1] loss: 704.975
[57,     1] loss: 759.168
[58,     1] loss: 701.103
[59,     1] loss: 680.471
[60,     1] loss: 718.734
[61,     1] loss: 831.172
[62,     1] loss: 2450.448
[63,     1] loss: 1121.639
[64,     1] loss: 1191.213
[65,     1] loss: 1144.621
[66,     1] loss: 1208.771
[67,     1] loss: 1217.414
[68,     1] loss: 1226.830
[69,     1] loss: 1204.661
[70,     1] loss: 1207.036
[71,     1] loss: 1150.401
[72,     1] loss: 1146.641
[73,     1] loss: 1157.115
[74,     1] loss: 1093.291
[75,     1] loss: 1108.218
[76,     1] loss: 1135.880
[77,     1] loss: 1067.046
[78,     1] loss: 1012.493
[79,     1] loss: 1068.048
[80,     1] loss: 1094.371
Early stopping applied (best metric=0.4089375436306)
Finished Training
Total time taken: 11.69660210609436
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1472.555
[2,     1] loss: 1465.951
[3,     1] loss: 1466.007
[4,     1] loss: 1467.883
[5,     1] loss: 1464.068
[6,     1] loss: 1466.753
[7,     1] loss: 1462.999
[8,     1] loss: 1459.625
[9,     1] loss: 1458.942
[10,     1] loss: 1448.172
[11,     1] loss: 1438.404
[12,     1] loss: 1404.748
[13,     1] loss: 1364.487
[14,     1] loss: 1341.740
[15,     1] loss: 1276.351
[16,     1] loss: 1228.300
[17,     1] loss: 1263.706
[18,     1] loss: 1228.174
[19,     1] loss: 1204.973
[20,     1] loss: 1195.825
[21,     1] loss: 1120.970
[22,     1] loss: 1164.042
[23,     1] loss: 1200.065
[24,     1] loss: 1125.236
[25,     1] loss: 1137.095
[26,     1] loss: 1129.193
[27,     1] loss: 1120.409
[28,     1] loss: 1089.400
[29,     1] loss: 1032.158
[30,     1] loss: 1081.334
[31,     1] loss: 1030.717
[32,     1] loss: 1043.815
[33,     1] loss: 1129.526
[34,     1] loss: 1201.743
[35,     1] loss: 1010.710
[36,     1] loss: 1064.334
[37,     1] loss: 1014.112
[38,     1] loss: 1024.314
[39,     1] loss: 986.977
[40,     1] loss: 956.393
[41,     1] loss: 1010.636
[42,     1] loss: 951.759
[43,     1] loss: 959.949
[44,     1] loss: 977.499
[45,     1] loss: 929.286
[46,     1] loss: 920.240
[47,     1] loss: 867.047
[48,     1] loss: 958.258
[49,     1] loss: 1027.040
[50,     1] loss: 897.395
[51,     1] loss: 895.733
[52,     1] loss: 925.135
[53,     1] loss: 854.859
[54,     1] loss: 978.573
[55,     1] loss: 823.176
[56,     1] loss: 858.997
[57,     1] loss: 915.282
[58,     1] loss: 835.551
[59,     1] loss: 796.495
[60,     1] loss: 920.258
[61,     1] loss: 1018.332
[62,     1] loss: 774.459
[63,     1] loss: 849.317
[64,     1] loss: 750.292
[65,     1] loss: 842.637
[66,     1] loss: 756.944
[67,     1] loss: 667.216
[68,     1] loss: 724.328
[69,     1] loss: 729.928
[70,     1] loss: 852.299
[71,     1] loss: 1238.009
[72,     1] loss: 629.716
[73,     1] loss: 985.778
[74,     1] loss: 853.283
[75,     1] loss: 882.793
[76,     1] loss: 951.670
[77,     1] loss: 919.077
[78,     1] loss: 828.518
[79,     1] loss: 945.253
[80,     1] loss: 725.195
[81,     1] loss: 880.869
[82,     1] loss: 734.739
[83,     1] loss: 910.983
[84,     1] loss: 742.527
[85,     1] loss: 768.513
[86,     1] loss: 686.988
[87,     1] loss: 683.068
[88,     1] loss: 639.698
[89,     1] loss: 620.671
[90,     1] loss: 717.428
[91,     1] loss: 700.115
[92,     1] loss: 573.755
[93,     1] loss: 836.914
Early stopping applied (best metric=0.41321617364883423)
Finished Training
Total time taken: 13.614607334136963
{'Hydroxylation-K Validation Accuracy': 0.7296394799054373, 'Hydroxylation-K Validation Sensitivity': 0.6525925925925926, 'Hydroxylation-K Validation Specificity': 0.7491228070175439, 'Hydroxylation-K Validation Precision': 0.4115768344715713, 'Hydroxylation-K AUC ROC': 0.7625146198830409, 'Hydroxylation-K AUC PR': 0.5356734712163643, 'Hydroxylation-K MCC': 0.3505433499031377, 'Hydroxylation-K F1': 0.4971369988234748, 'Validation Loss (Hydroxylation-K)': 0.4948662519454956, 'Hydroxylation-P Validation Accuracy': 0.7926024736476998, 'Hydroxylation-P Validation Sensitivity': 0.7934920634920635, 'Hydroxylation-P Validation Specificity': 0.7924185744924934, 'Hydroxylation-P Validation Precision': 0.46009703537351876, 'Hydroxylation-P AUC ROC': 0.839385495615108, 'Hydroxylation-P AUC PR': 0.5890053255158847, 'Hydroxylation-P MCC': 0.4880350570659077, 'Hydroxylation-P F1': 0.579259149263101, 'Validation Loss (Hydroxylation-P)': 0.3729977389176687, 'Validation Loss (total)': 0.8678639888763428, 'TimeToTrain': 14.605419985453288}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008845736661633685,
 'learning_rate_Hydroxylation-K': 0.006381449264184637,
 'learning_rate_Hydroxylation-P': 0.007259070059273224,
 'log_base': 2.7125750377966815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3668021586,
 'sample_weights': [2.6425053634703466, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7111038256789017,
 'weight_decay_Hydroxylation-K': 9.655547348799555,
 'weight_decay_Hydroxylation-P': 9.234091112360481}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.590
[2,     1] loss: 1271.459
[3,     1] loss: 1261.125
[4,     1] loss: 1261.099
[5,     1] loss: 1261.939
[6,     1] loss: 1260.553
[7,     1] loss: 1261.243
[8,     1] loss: 1261.057
[9,     1] loss: 1259.264
[10,     1] loss: 1259.785
[11,     1] loss: 1260.106
[12,     1] loss: 1258.634
[13,     1] loss: 1257.934
[14,     1] loss: 1259.139
[15,     1] loss: 1257.319
[16,     1] loss: 1255.222
[17,     1] loss: 1252.495
[18,     1] loss: 1253.542
[19,     1] loss: 1240.880
[20,     1] loss: 1226.514
[21,     1] loss: 1210.502
[22,     1] loss: 1185.808
[23,     1] loss: 1140.312
[24,     1] loss: 1109.244
[25,     1] loss: 1099.279
[26,     1] loss: 1075.991
[27,     1] loss: 1025.515
[28,     1] loss: 1002.616
[29,     1] loss: 1096.557
[30,     1] loss: 969.788
[31,     1] loss: 1047.797
[32,     1] loss: 1057.542
[33,     1] loss: 998.717
[34,     1] loss: 1027.096
[35,     1] loss: 1006.532
[36,     1] loss: 985.037
[37,     1] loss: 1006.330
[38,     1] loss: 985.902
[39,     1] loss: 996.997
[40,     1] loss: 970.936
[41,     1] loss: 960.061
[42,     1] loss: 956.046
[43,     1] loss: 918.382
[44,     1] loss: 954.119
[45,     1] loss: 889.650
[46,     1] loss: 913.027
[47,     1] loss: 938.449
[48,     1] loss: 871.239
[49,     1] loss: 886.942
[50,     1] loss: 841.000
[51,     1] loss: 881.017
[52,     1] loss: 956.321
[53,     1] loss: 945.868
[54,     1] loss: 831.407
[55,     1] loss: 910.285
[56,     1] loss: 865.777
[57,     1] loss: 823.408
[58,     1] loss: 778.785
[59,     1] loss: 815.952
[60,     1] loss: 797.366
[61,     1] loss: 823.520
[62,     1] loss: 932.962
[63,     1] loss: 807.430
[64,     1] loss: 751.462
[65,     1] loss: 866.034
[66,     1] loss: 756.598
[67,     1] loss: 780.538
[68,     1] loss: 802.362
[69,     1] loss: 734.090
[70,     1] loss: 728.785
[71,     1] loss: 690.143
[72,     1] loss: 746.127
[73,     1] loss: 1120.335
[74,     1] loss: 963.860
[75,     1] loss: 697.347
[76,     1] loss: 807.333
[77,     1] loss: 755.392
[78,     1] loss: 739.753
[79,     1] loss: 847.906
[80,     1] loss: 734.823
[81,     1] loss: 769.074
[82,     1] loss: 780.090
[83,     1] loss: 710.159
[84,     1] loss: 773.956
[85,     1] loss: 761.745
[86,     1] loss: 750.844
[87,     1] loss: 695.219
[88,     1] loss: 683.167
[89,     1] loss: 756.932
[90,     1] loss: 580.139
[91,     1] loss: 678.906
[92,     1] loss: 712.123
[93,     1] loss: 584.945
[94,     1] loss: 769.137
[95,     1] loss: 1132.181
[96,     1] loss: 565.349
[97,     1] loss: 965.300
[98,     1] loss: 680.127
[99,     1] loss: 769.802
[100,     1] loss: 738.865
[101,     1] loss: 673.340
[102,     1] loss: 745.414
[103,     1] loss: 643.818
Early stopping applied (best metric=0.3707921802997589)
Finished Training
Total time taken: 16.467078924179077
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.011
[2,     1] loss: 1264.521
[3,     1] loss: 1259.406
[4,     1] loss: 1263.233
[5,     1] loss: 1260.515
[6,     1] loss: 1259.793
[7,     1] loss: 1261.769
[8,     1] loss: 1264.147
[9,     1] loss: 1259.201
[10,     1] loss: 1261.877
[11,     1] loss: 1259.995
[12,     1] loss: 1258.863
[13,     1] loss: 1260.728
[14,     1] loss: 1261.038
[15,     1] loss: 1258.069
[16,     1] loss: 1255.203
[17,     1] loss: 1255.768
[18,     1] loss: 1254.878
[19,     1] loss: 1247.845
[20,     1] loss: 1239.616
[21,     1] loss: 1214.429
[22,     1] loss: 1192.309
[23,     1] loss: 1139.625
[24,     1] loss: 1089.734
[25,     1] loss: 1096.529
[26,     1] loss: 1045.597
[27,     1] loss: 1083.815
[28,     1] loss: 1041.946
[29,     1] loss: 1067.546
[30,     1] loss: 980.778
[31,     1] loss: 1042.707
[32,     1] loss: 1016.440
[33,     1] loss: 1019.802
[34,     1] loss: 970.469
[35,     1] loss: 987.700
[36,     1] loss: 954.790
[37,     1] loss: 959.004
[38,     1] loss: 944.635
[39,     1] loss: 1014.059
[40,     1] loss: 968.384
[41,     1] loss: 950.406
[42,     1] loss: 892.319
[43,     1] loss: 929.246
[44,     1] loss: 859.528
[45,     1] loss: 889.287
[46,     1] loss: 850.210
[47,     1] loss: 841.434
[48,     1] loss: 873.421
[49,     1] loss: 795.522
[50,     1] loss: 886.412
[51,     1] loss: 894.006
[52,     1] loss: 817.168
[53,     1] loss: 773.744
[54,     1] loss: 784.984
[55,     1] loss: 773.486
[56,     1] loss: 720.729
[57,     1] loss: 741.161
[58,     1] loss: 831.955
[59,     1] loss: 1099.974
[60,     1] loss: 821.474
[61,     1] loss: 829.415
[62,     1] loss: 780.720
[63,     1] loss: 899.966
[64,     1] loss: 816.918
[65,     1] loss: 796.390
[66,     1] loss: 868.230
[67,     1] loss: 763.349
[68,     1] loss: 802.966
[69,     1] loss: 770.161
[70,     1] loss: 713.197
[71,     1] loss: 801.532
[72,     1] loss: 624.888
[73,     1] loss: 763.137
[74,     1] loss: 712.894
[75,     1] loss: 729.238
[76,     1] loss: 678.229
[77,     1] loss: 717.959
[78,     1] loss: 720.383
[79,     1] loss: 579.971
[80,     1] loss: 617.771
[81,     1] loss: 634.081
[82,     1] loss: 574.380
[83,     1] loss: 577.857
[84,     1] loss: 476.904
[85,     1] loss: 642.836
[86,     1] loss: 1517.701
[87,     1] loss: 1774.565
[88,     1] loss: 1375.540
[89,     1] loss: 1013.535
[90,     1] loss: 988.171
[91,     1] loss: 1087.663
[92,     1] loss: 1134.305
[93,     1] loss: 1116.528
[94,     1] loss: 1100.901
[95,     1] loss: 1116.268
[96,     1] loss: 1118.772
[97,     1] loss: 1096.804
[98,     1] loss: 1066.027
Early stopping applied (best metric=0.3916569948196411)
Finished Training
Total time taken: 14.961978673934937
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.658
[2,     1] loss: 1261.091
[3,     1] loss: 1260.290
[4,     1] loss: 1263.276
[5,     1] loss: 1262.360
[6,     1] loss: 1262.112
[7,     1] loss: 1255.120
[8,     1] loss: 1260.718
[9,     1] loss: 1260.407
[10,     1] loss: 1253.963
[11,     1] loss: 1256.256
[12,     1] loss: 1254.752
[13,     1] loss: 1250.922
[14,     1] loss: 1241.699
[15,     1] loss: 1233.610
[16,     1] loss: 1207.661
[17,     1] loss: 1179.700
[18,     1] loss: 1148.240
[19,     1] loss: 1088.011
[20,     1] loss: 1081.984
[21,     1] loss: 1043.959
[22,     1] loss: 1066.357
[23,     1] loss: 1070.250
[24,     1] loss: 1041.985
[25,     1] loss: 1017.265
[26,     1] loss: 1029.141
[27,     1] loss: 999.322
[28,     1] loss: 973.023
[29,     1] loss: 1014.205
[30,     1] loss: 955.011
[31,     1] loss: 929.568
[32,     1] loss: 919.238
[33,     1] loss: 939.138
[34,     1] loss: 971.645
[35,     1] loss: 957.398
[36,     1] loss: 989.196
[37,     1] loss: 909.024
[38,     1] loss: 928.830
[39,     1] loss: 884.647
[40,     1] loss: 851.777
[41,     1] loss: 908.067
[42,     1] loss: 868.040
[43,     1] loss: 856.540
[44,     1] loss: 869.595
[45,     1] loss: 858.070
[46,     1] loss: 952.631
[47,     1] loss: 1052.574
[48,     1] loss: 855.973
[49,     1] loss: 983.193
[50,     1] loss: 849.582
[51,     1] loss: 970.032
[52,     1] loss: 855.371
[53,     1] loss: 872.342
[54,     1] loss: 838.650
[55,     1] loss: 785.529
[56,     1] loss: 917.072
[57,     1] loss: 784.443
[58,     1] loss: 897.285
[59,     1] loss: 748.688
[60,     1] loss: 841.629
[61,     1] loss: 787.041
[62,     1] loss: 786.754
[63,     1] loss: 759.215
[64,     1] loss: 731.938
[65,     1] loss: 747.782
[66,     1] loss: 686.497
[67,     1] loss: 690.483
[68,     1] loss: 642.183
[69,     1] loss: 998.104
[70,     1] loss: 1369.560
[71,     1] loss: 702.313
[72,     1] loss: 939.244
[73,     1] loss: 966.542
[74,     1] loss: 868.044
[75,     1] loss: 909.393
[76,     1] loss: 944.283
[77,     1] loss: 890.539
[78,     1] loss: 807.827
[79,     1] loss: 798.421
[80,     1] loss: 847.957
[81,     1] loss: 761.553
[82,     1] loss: 737.009
[83,     1] loss: 700.824
[84,     1] loss: 688.535
[85,     1] loss: 658.028
[86,     1] loss: 650.399
[87,     1] loss: 643.029
[88,     1] loss: 585.266
[89,     1] loss: 616.980
[90,     1] loss: 630.645
[91,     1] loss: 725.777
[92,     1] loss: 542.507
[93,     1] loss: 507.639
[94,     1] loss: 544.235
[95,     1] loss: 476.879
[96,     1] loss: 928.334
[97,     1] loss: 1099.303
[98,     1] loss: 755.432
[99,     1] loss: 838.272
[100,     1] loss: 723.218
[101,     1] loss: 814.243
[102,     1] loss: 852.052
[103,     1] loss: 648.620
[104,     1] loss: 779.243
[105,     1] loss: 738.621
[106,     1] loss: 650.834
[107,     1] loss: 686.157
[108,     1] loss: 610.950
[109,     1] loss: 597.439
[110,     1] loss: 559.018
[111,     1] loss: 513.923
[112,     1] loss: 487.425
[113,     1] loss: 474.323
[114,     1] loss: 498.988
[115,     1] loss: 819.801
[116,     1] loss: 1067.735
[117,     1] loss: 747.397
[118,     1] loss: 900.020
[119,     1] loss: 704.925
[120,     1] loss: 737.566
[121,     1] loss: 641.316
[122,     1] loss: 716.240
[123,     1] loss: 655.809
[124,     1] loss: 771.808
[125,     1] loss: 557.455
[126,     1] loss: 621.657
[127,     1] loss: 533.820
[128,     1] loss: 523.257
[129,     1] loss: 491.592
[130,     1] loss: 454.895
[131,     1] loss: 457.709
[132,     1] loss: 453.673
[133,     1] loss: 456.819
[134,     1] loss: 515.083
[135,     1] loss: 898.415
[136,     1] loss: 619.443
[137,     1] loss: 479.984
[138,     1] loss: 580.536
[139,     1] loss: 429.007
[140,     1] loss: 549.031
[141,     1] loss: 620.360
[142,     1] loss: 438.932
[143,     1] loss: 447.273
[144,     1] loss: 749.482
[145,     1] loss: 414.636
[146,     1] loss: 755.543
[147,     1] loss: 829.169
[148,     1] loss: 771.898
[149,     1] loss: 629.145
[150,     1] loss: 917.313
[151,     1] loss: 532.555
[152,     1] loss: 805.151
[153,     1] loss: 525.219
[154,     1] loss: 767.324
Early stopping applied (best metric=0.35734179615974426)
Finished Training
Total time taken: 24.312378883361816
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.838
[2,     1] loss: 1259.569
[3,     1] loss: 1265.422
[4,     1] loss: 1259.042
[5,     1] loss: 1268.053
[6,     1] loss: 1261.373
[7,     1] loss: 1258.280
[8,     1] loss: 1258.691
[9,     1] loss: 1257.555
[10,     1] loss: 1257.091
[11,     1] loss: 1255.731
[12,     1] loss: 1252.416
[13,     1] loss: 1243.802
[14,     1] loss: 1230.140
[15,     1] loss: 1207.423
[16,     1] loss: 1177.439
[17,     1] loss: 1142.446
[18,     1] loss: 1089.797
[19,     1] loss: 1095.334
[20,     1] loss: 1111.081
[21,     1] loss: 1055.141
[22,     1] loss: 1058.701
[23,     1] loss: 1028.912
[24,     1] loss: 1024.095
[25,     1] loss: 1064.594
[26,     1] loss: 986.510
[27,     1] loss: 1006.314
[28,     1] loss: 954.865
[29,     1] loss: 969.699
[30,     1] loss: 940.392
[31,     1] loss: 927.958
[32,     1] loss: 950.931
[33,     1] loss: 918.091
[34,     1] loss: 1021.788
[35,     1] loss: 914.492
[36,     1] loss: 888.714
[37,     1] loss: 930.279
[38,     1] loss: 948.983
[39,     1] loss: 928.620
[40,     1] loss: 880.443
[41,     1] loss: 898.215
[42,     1] loss: 859.025
[43,     1] loss: 850.365
[44,     1] loss: 890.661
[45,     1] loss: 874.277
[46,     1] loss: 879.845
[47,     1] loss: 891.605
[48,     1] loss: 818.202
[49,     1] loss: 818.290
[50,     1] loss: 811.363
[51,     1] loss: 837.095
[52,     1] loss: 881.853
[53,     1] loss: 765.701
[54,     1] loss: 834.389
[55,     1] loss: 864.317
[56,     1] loss: 781.073
[57,     1] loss: 737.626
[58,     1] loss: 739.063
[59,     1] loss: 714.459
[60,     1] loss: 725.766
[61,     1] loss: 732.094
[62,     1] loss: 950.992
[63,     1] loss: 1866.218
[64,     1] loss: 808.153
[65,     1] loss: 1023.997
[66,     1] loss: 1091.361
[67,     1] loss: 1006.698
[68,     1] loss: 989.279
[69,     1] loss: 1016.833
[70,     1] loss: 1025.574
[71,     1] loss: 1017.526
[72,     1] loss: 969.421
[73,     1] loss: 992.198
[74,     1] loss: 991.034
[75,     1] loss: 931.119
[76,     1] loss: 910.596
[77,     1] loss: 931.552
[78,     1] loss: 858.572
[79,     1] loss: 820.755
[80,     1] loss: 871.138
[81,     1] loss: 811.519
[82,     1] loss: 822.390
[83,     1] loss: 791.269
[84,     1] loss: 815.528
[85,     1] loss: 806.740
[86,     1] loss: 730.366
[87,     1] loss: 768.988
[88,     1] loss: 773.363
[89,     1] loss: 713.631
[90,     1] loss: 666.559
[91,     1] loss: 705.484
[92,     1] loss: 684.087
[93,     1] loss: 994.558
[94,     1] loss: 1202.413
Early stopping applied (best metric=0.37462174892425537)
Finished Training
Total time taken: 14.317313194274902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1263.101
[2,     1] loss: 1259.957
[3,     1] loss: 1262.745
[4,     1] loss: 1260.995
[5,     1] loss: 1260.204
[6,     1] loss: 1256.102
[7,     1] loss: 1251.959
[8,     1] loss: 1239.670
[9,     1] loss: 1222.280
[10,     1] loss: 1186.447
[11,     1] loss: 1133.231
[12,     1] loss: 1127.853
[13,     1] loss: 1144.342
[14,     1] loss: 1108.942
[15,     1] loss: 1074.429
[16,     1] loss: 1098.908
[17,     1] loss: 1068.241
[18,     1] loss: 1028.790
[19,     1] loss: 1025.417
[20,     1] loss: 987.979
[21,     1] loss: 972.567
[22,     1] loss: 967.925
[23,     1] loss: 1050.164
[24,     1] loss: 975.395
[25,     1] loss: 982.939
[26,     1] loss: 952.390
[27,     1] loss: 953.519
[28,     1] loss: 895.685
[29,     1] loss: 953.829
[30,     1] loss: 921.096
[31,     1] loss: 895.104
[32,     1] loss: 973.537
[33,     1] loss: 940.273
[34,     1] loss: 921.232
[35,     1] loss: 889.516
[36,     1] loss: 936.484
[37,     1] loss: 864.347
[38,     1] loss: 890.544
[39,     1] loss: 860.318
[40,     1] loss: 926.931
[41,     1] loss: 864.079
[42,     1] loss: 851.004
[43,     1] loss: 873.702
[44,     1] loss: 882.163
[45,     1] loss: 784.461
[46,     1] loss: 829.038
[47,     1] loss: 893.636
[48,     1] loss: 805.542
[49,     1] loss: 822.047
[50,     1] loss: 864.703
[51,     1] loss: 768.145
[52,     1] loss: 720.132
[53,     1] loss: 747.078
[54,     1] loss: 726.874
[55,     1] loss: 667.568
[56,     1] loss: 878.575
[57,     1] loss: 1024.463
[58,     1] loss: 685.232
[59,     1] loss: 839.792
[60,     1] loss: 713.428
[61,     1] loss: 800.051
[62,     1] loss: 738.450
[63,     1] loss: 774.398
[64,     1] loss: 626.769
[65,     1] loss: 699.331
[66,     1] loss: 648.872
[67,     1] loss: 626.711
[68,     1] loss: 805.570
[69,     1] loss: 800.253
[70,     1] loss: 715.698
[71,     1] loss: 577.283
[72,     1] loss: 813.712
[73,     1] loss: 596.238
[74,     1] loss: 696.057
[75,     1] loss: 579.993
[76,     1] loss: 650.883
[77,     1] loss: 550.784
[78,     1] loss: 572.436
[79,     1] loss: 551.655
[80,     1] loss: 514.343
[81,     1] loss: 469.089
[82,     1] loss: 508.979
[83,     1] loss: 954.952
[84,     1] loss: 657.349
[85,     1] loss: 465.281
[86,     1] loss: 582.620
[87,     1] loss: 472.210
[88,     1] loss: 551.729
[89,     1] loss: 632.371
[90,     1] loss: 469.396
[91,     1] loss: 472.235
[92,     1] loss: 575.527
[93,     1] loss: 510.961
[94,     1] loss: 529.199
[95,     1] loss: 452.556
[96,     1] loss: 432.791
[97,     1] loss: 639.365
Early stopping applied (best metric=0.353807657957077)
Finished Training
Total time taken: 14.691589832305908
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.818
[2,     1] loss: 1270.745
[3,     1] loss: 1272.342
[4,     1] loss: 1257.907
[5,     1] loss: 1257.440
[6,     1] loss: 1260.001
[7,     1] loss: 1261.051
[8,     1] loss: 1258.722
[9,     1] loss: 1259.063
[10,     1] loss: 1260.026
[11,     1] loss: 1258.774
[12,     1] loss: 1260.940
[13,     1] loss: 1260.686
[14,     1] loss: 1255.890
[15,     1] loss: 1252.039
[16,     1] loss: 1247.085
[17,     1] loss: 1240.900
[18,     1] loss: 1223.016
[19,     1] loss: 1200.432
[20,     1] loss: 1161.936
[21,     1] loss: 1138.376
[22,     1] loss: 1103.839
[23,     1] loss: 1119.213
[24,     1] loss: 1081.002
[25,     1] loss: 1114.783
[26,     1] loss: 1050.012
[27,     1] loss: 1028.691
[28,     1] loss: 1028.950
[29,     1] loss: 1050.390
[30,     1] loss: 1007.442
[31,     1] loss: 1002.221
[32,     1] loss: 1008.988
[33,     1] loss: 959.969
[34,     1] loss: 996.928
[35,     1] loss: 994.054
[36,     1] loss: 991.095
[37,     1] loss: 995.228
[38,     1] loss: 934.264
[39,     1] loss: 1005.453
[40,     1] loss: 931.427
[41,     1] loss: 989.214
[42,     1] loss: 921.612
[43,     1] loss: 937.589
[44,     1] loss: 890.115
[45,     1] loss: 924.026
[46,     1] loss: 874.885
[47,     1] loss: 960.316
[48,     1] loss: 869.450
[49,     1] loss: 930.319
[50,     1] loss: 857.539
[51,     1] loss: 903.785
[52,     1] loss: 821.518
[53,     1] loss: 887.512
[54,     1] loss: 976.798
[55,     1] loss: 812.008
[56,     1] loss: 844.012
[57,     1] loss: 842.168
[58,     1] loss: 757.741
[59,     1] loss: 869.938
[60,     1] loss: 843.908
[61,     1] loss: 727.741
[62,     1] loss: 844.863
[63,     1] loss: 737.574
[64,     1] loss: 733.312
[65,     1] loss: 688.371
[66,     1] loss: 644.878
[67,     1] loss: 676.423
[68,     1] loss: 775.343
[69,     1] loss: 909.358
[70,     1] loss: 696.733
[71,     1] loss: 614.174
[72,     1] loss: 721.106
[73,     1] loss: 607.251
[74,     1] loss: 695.726
[75,     1] loss: 615.760
[76,     1] loss: 606.564
[77,     1] loss: 560.333
[78,     1] loss: 550.270
[79,     1] loss: 686.761
[80,     1] loss: 720.321
[81,     1] loss: 769.938
[82,     1] loss: 533.877
[83,     1] loss: 671.644
[84,     1] loss: 513.218
[85,     1] loss: 620.309
[86,     1] loss: 491.682
[87,     1] loss: 671.759
[88,     1] loss: 701.321
[89,     1] loss: 437.687
[90,     1] loss: 660.530
[91,     1] loss: 648.049
[92,     1] loss: 536.740
[93,     1] loss: 694.887
[94,     1] loss: 491.376
[95,     1] loss: 682.236
[96,     1] loss: 507.196
[97,     1] loss: 669.100
[98,     1] loss: 639.030
[99,     1] loss: 556.091
[100,     1] loss: 724.050
[101,     1] loss: 440.158
[102,     1] loss: 598.637
[103,     1] loss: 430.317
[104,     1] loss: 562.446
[105,     1] loss: 417.816
[106,     1] loss: 491.240
[107,     1] loss: 590.859
[108,     1] loss: 521.255
[109,     1] loss: 403.775
[110,     1] loss: 575.720
[111,     1] loss: 399.445
[112,     1] loss: 430.725
[113,     1] loss: 501.499
[114,     1] loss: 428.989
[115,     1] loss: 378.965
[116,     1] loss: 516.389
[117,     1] loss: 633.474
[118,     1] loss: 398.574
[119,     1] loss: 466.634
[120,     1] loss: 533.659
[121,     1] loss: 370.555
[122,     1] loss: 537.539
[123,     1] loss: 699.262
[124,     1] loss: 387.297
Early stopping applied (best metric=0.326036661863327)
Finished Training
Total time taken: 18.574801206588745
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.373
[2,     1] loss: 1262.122
[3,     1] loss: 1261.006
[4,     1] loss: 1265.024
[5,     1] loss: 1263.250
[6,     1] loss: 1261.099
[7,     1] loss: 1260.704
[8,     1] loss: 1258.769
[9,     1] loss: 1261.738
[10,     1] loss: 1256.939
[11,     1] loss: 1258.350
[12,     1] loss: 1257.152
[13,     1] loss: 1254.928
[14,     1] loss: 1251.662
[15,     1] loss: 1246.011
[16,     1] loss: 1240.505
[17,     1] loss: 1227.087
[18,     1] loss: 1197.774
[19,     1] loss: 1160.115
[20,     1] loss: 1122.822
[21,     1] loss: 1097.863
[22,     1] loss: 1094.719
[23,     1] loss: 1037.358
[24,     1] loss: 1017.369
[25,     1] loss: 1012.864
[26,     1] loss: 1053.405
[27,     1] loss: 1117.206
[28,     1] loss: 1023.697
[29,     1] loss: 1057.496
[30,     1] loss: 1004.755
[31,     1] loss: 1040.700
[32,     1] loss: 982.348
[33,     1] loss: 1002.745
[34,     1] loss: 945.043
[35,     1] loss: 937.721
[36,     1] loss: 901.788
[37,     1] loss: 904.790
[38,     1] loss: 956.668
[39,     1] loss: 945.489
[40,     1] loss: 837.207
[41,     1] loss: 903.438
[42,     1] loss: 866.178
[43,     1] loss: 842.652
[44,     1] loss: 872.941
[45,     1] loss: 897.865
[46,     1] loss: 960.628
[47,     1] loss: 770.465
[48,     1] loss: 836.083
[49,     1] loss: 806.867
[50,     1] loss: 844.482
[51,     1] loss: 813.646
[52,     1] loss: 839.884
[53,     1] loss: 822.756
[54,     1] loss: 759.381
[55,     1] loss: 837.143
[56,     1] loss: 787.611
[57,     1] loss: 722.850
[58,     1] loss: 869.121
[59,     1] loss: 825.825
[60,     1] loss: 705.954
[61,     1] loss: 740.668
[62,     1] loss: 654.863
[63,     1] loss: 657.551
[64,     1] loss: 657.036
[65,     1] loss: 657.160
[66,     1] loss: 621.671
[67,     1] loss: 677.934
[68,     1] loss: 943.468
[69,     1] loss: 823.662
[70,     1] loss: 629.252
[71,     1] loss: 775.428
[72,     1] loss: 677.728
[73,     1] loss: 723.520
[74,     1] loss: 689.044
[75,     1] loss: 667.352
[76,     1] loss: 641.308
[77,     1] loss: 622.143
[78,     1] loss: 605.061
[79,     1] loss: 586.141
[80,     1] loss: 570.307
[81,     1] loss: 568.628
[82,     1] loss: 686.805
[83,     1] loss: 478.464
[84,     1] loss: 558.795
[85,     1] loss: 820.524
[86,     1] loss: 491.990
Early stopping applied (best metric=0.36792394518852234)
Finished Training
Total time taken: 13.156728982925415
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.806
[2,     1] loss: 1263.048
[3,     1] loss: 1262.875
[4,     1] loss: 1259.811
[5,     1] loss: 1261.217
[6,     1] loss: 1264.829
[7,     1] loss: 1263.708
[8,     1] loss: 1261.769
[9,     1] loss: 1260.403
[10,     1] loss: 1260.714
[11,     1] loss: 1259.579
[12,     1] loss: 1260.125
[13,     1] loss: 1260.694
[14,     1] loss: 1260.281
[15,     1] loss: 1260.750
[16,     1] loss: 1262.232
[17,     1] loss: 1259.221
[18,     1] loss: 1259.880
[19,     1] loss: 1259.899
[20,     1] loss: 1257.806
[21,     1] loss: 1256.925
[22,     1] loss: 1257.281
[23,     1] loss: 1252.100
[24,     1] loss: 1248.087
[25,     1] loss: 1234.304
[26,     1] loss: 1211.049
[27,     1] loss: 1189.121
[28,     1] loss: 1158.215
[29,     1] loss: 1149.581
[30,     1] loss: 1121.823
[31,     1] loss: 1080.345
[32,     1] loss: 1142.639
[33,     1] loss: 1035.853
[34,     1] loss: 1184.734
[35,     1] loss: 1051.244
[36,     1] loss: 1089.752
[37,     1] loss: 1063.826
[38,     1] loss: 1048.637
[39,     1] loss: 1080.527
[40,     1] loss: 1023.562
[41,     1] loss: 1034.551
[42,     1] loss: 1013.404
[43,     1] loss: 966.595
[44,     1] loss: 997.359
[45,     1] loss: 951.030
[46,     1] loss: 964.109
[47,     1] loss: 972.701
[48,     1] loss: 964.708
[49,     1] loss: 919.357
[50,     1] loss: 933.865
[51,     1] loss: 883.678
[52,     1] loss: 881.097
[53,     1] loss: 900.017
[54,     1] loss: 900.774
[55,     1] loss: 868.910
[56,     1] loss: 881.322
[57,     1] loss: 898.432
[58,     1] loss: 1272.175
[59,     1] loss: 1090.919
[60,     1] loss: 965.242
[61,     1] loss: 956.674
[62,     1] loss: 1011.011
[63,     1] loss: 1024.350
[64,     1] loss: 967.006
[65,     1] loss: 951.582
[66,     1] loss: 964.185
[67,     1] loss: 942.988
[68,     1] loss: 914.391
[69,     1] loss: 908.798
[70,     1] loss: 904.147
[71,     1] loss: 846.987
[72,     1] loss: 875.466
[73,     1] loss: 811.784
[74,     1] loss: 818.541
[75,     1] loss: 800.023
[76,     1] loss: 786.039
[77,     1] loss: 806.771
[78,     1] loss: 742.620
[79,     1] loss: 741.829
[80,     1] loss: 753.327
[81,     1] loss: 795.399
[82,     1] loss: 766.563
[83,     1] loss: 739.411
[84,     1] loss: 749.808
[85,     1] loss: 634.271
[86,     1] loss: 806.052
[87,     1] loss: 972.612
[88,     1] loss: 703.264
[89,     1] loss: 825.647
[90,     1] loss: 733.274
[91,     1] loss: 825.708
[92,     1] loss: 656.806
[93,     1] loss: 754.606
[94,     1] loss: 687.016
[95,     1] loss: 653.831
[96,     1] loss: 895.320
[97,     1] loss: 728.911
[98,     1] loss: 720.898
[99,     1] loss: 701.613
[100,     1] loss: 702.721
[101,     1] loss: 643.616
[102,     1] loss: 706.834
[103,     1] loss: 634.980
[104,     1] loss: 577.773
[105,     1] loss: 644.635
[106,     1] loss: 524.639
[107,     1] loss: 669.615
[108,     1] loss: 855.401
[109,     1] loss: 522.316
[110,     1] loss: 815.235
[111,     1] loss: 622.223
[112,     1] loss: 742.570
[113,     1] loss: 537.263
[114,     1] loss: 762.301
[115,     1] loss: 603.073
[116,     1] loss: 766.554
[117,     1] loss: 576.439
[118,     1] loss: 816.213
[119,     1] loss: 524.609
[120,     1] loss: 724.535
[121,     1] loss: 524.655
[122,     1] loss: 627.339
[123,     1] loss: 489.929
[124,     1] loss: 556.210
[125,     1] loss: 445.493
[126,     1] loss: 484.629
[127,     1] loss: 538.097
[128,     1] loss: 411.621
[129,     1] loss: 391.959
[130,     1] loss: 389.127
[131,     1] loss: 446.721
[132,     1] loss: 852.155
[133,     1] loss: 962.810
[134,     1] loss: 650.383
[135,     1] loss: 911.295
[136,     1] loss: 801.247
[137,     1] loss: 707.430
[138,     1] loss: 737.981
[139,     1] loss: 614.385
[140,     1] loss: 745.745
[141,     1] loss: 696.460
[142,     1] loss: 556.339
[143,     1] loss: 580.592
[144,     1] loss: 541.451
[145,     1] loss: 486.873
[146,     1] loss: 511.423
[147,     1] loss: 412.043
[148,     1] loss: 437.457
[149,     1] loss: 606.277
[150,     1] loss: 759.963
[151,     1] loss: 1107.974
[152,     1] loss: 535.176
[153,     1] loss: 854.667
[154,     1] loss: 566.913
[155,     1] loss: 550.438
[156,     1] loss: 588.910
[157,     1] loss: 545.627
[158,     1] loss: 565.054
[159,     1] loss: 526.296
[160,     1] loss: 740.663
[161,     1] loss: 1100.222
[162,     1] loss: 585.584
[163,     1] loss: 888.946
[164,     1] loss: 713.790
[165,     1] loss: 738.612
[166,     1] loss: 725.171
[167,     1] loss: 593.958
[168,     1] loss: 619.866
[169,     1] loss: 608.923
[170,     1] loss: 631.847
[171,     1] loss: 522.699
[172,     1] loss: 467.883
[173,     1] loss: 456.555
[174,     1] loss: 443.842
[175,     1] loss: 432.828
[176,     1] loss: 435.964
[177,     1] loss: 428.725
[178,     1] loss: 446.557
[179,     1] loss: 387.363
[180,     1] loss: 349.147
[181,     1] loss: 350.721
[182,     1] loss: 374.915
[183,     1] loss: 617.119
[184,     1] loss: 2002.203
[185,     1] loss: 2701.878
[186,     1] loss: 1191.256
[187,     1] loss: 1194.886
[188,     1] loss: 1254.623
Early stopping applied (best metric=0.3064687252044678)
Finished Training
Total time taken: 27.325700521469116
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.324
[2,     1] loss: 1271.882
[3,     1] loss: 1262.561
[4,     1] loss: 1263.705
[5,     1] loss: 1262.964
[6,     1] loss: 1262.026
[7,     1] loss: 1259.426
[8,     1] loss: 1260.980
[9,     1] loss: 1260.545
[10,     1] loss: 1260.345
[11,     1] loss: 1260.011
[12,     1] loss: 1260.483
[13,     1] loss: 1260.988
[14,     1] loss: 1261.357
[15,     1] loss: 1260.908
[16,     1] loss: 1260.458
[17,     1] loss: 1260.359
[18,     1] loss: 1259.822
[19,     1] loss: 1260.053
[20,     1] loss: 1259.690
[21,     1] loss: 1259.653
[22,     1] loss: 1259.687
[23,     1] loss: 1258.334
[24,     1] loss: 1258.920
[25,     1] loss: 1259.279
[26,     1] loss: 1260.567
[27,     1] loss: 1260.606
[28,     1] loss: 1254.791
[29,     1] loss: 1252.531
[30,     1] loss: 1241.726
[31,     1] loss: 1230.181
[32,     1] loss: 1214.796
[33,     1] loss: 1193.101
[34,     1] loss: 1170.746
[35,     1] loss: 1156.311
[36,     1] loss: 1129.078
[37,     1] loss: 1115.750
[38,     1] loss: 1145.757
[39,     1] loss: 1062.978
[40,     1] loss: 1081.277
[41,     1] loss: 1084.130
[42,     1] loss: 1022.189
[43,     1] loss: 1052.028
[44,     1] loss: 1048.039
[45,     1] loss: 1024.185
[46,     1] loss: 1006.968
[47,     1] loss: 962.168
[48,     1] loss: 946.271
[49,     1] loss: 953.800
[50,     1] loss: 1018.400
[51,     1] loss: 940.837
[52,     1] loss: 979.470
[53,     1] loss: 901.748
[54,     1] loss: 957.217
[55,     1] loss: 884.055
[56,     1] loss: 1032.696
[57,     1] loss: 893.188
[58,     1] loss: 952.036
[59,     1] loss: 890.459
[60,     1] loss: 897.764
[61,     1] loss: 891.725
[62,     1] loss: 889.839
[63,     1] loss: 861.621
[64,     1] loss: 894.981
[65,     1] loss: 870.981
[66,     1] loss: 856.511
[67,     1] loss: 823.576
[68,     1] loss: 782.922
[69,     1] loss: 758.894
[70,     1] loss: 778.790
[71,     1] loss: 765.598
[72,     1] loss: 797.738
[73,     1] loss: 755.182
[74,     1] loss: 890.551
[75,     1] loss: 1319.736
[76,     1] loss: 776.177
[77,     1] loss: 1028.662
[78,     1] loss: 857.799
[79,     1] loss: 900.124
[80,     1] loss: 951.620
[81,     1] loss: 912.734
[82,     1] loss: 855.516
[83,     1] loss: 898.081
[84,     1] loss: 815.936
[85,     1] loss: 826.395
[86,     1] loss: 856.537
[87,     1] loss: 821.049
[88,     1] loss: 808.495
[89,     1] loss: 758.594
[90,     1] loss: 782.084
[91,     1] loss: 763.950
[92,     1] loss: 756.805
[93,     1] loss: 715.339
[94,     1] loss: 699.685
[95,     1] loss: 786.644
[96,     1] loss: 887.369
[97,     1] loss: 718.535
[98,     1] loss: 656.449
[99,     1] loss: 690.518
[100,     1] loss: 650.965
[101,     1] loss: 640.482
[102,     1] loss: 625.333
[103,     1] loss: 651.352
[104,     1] loss: 714.217
[105,     1] loss: 1216.130
[106,     1] loss: 1229.927
[107,     1] loss: 807.980
[108,     1] loss: 888.303
Early stopping applied (best metric=0.42323434352874756)
Finished Training
Total time taken: 15.973057746887207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1259.190
[2,     1] loss: 1273.350
[3,     1] loss: 1264.864
[4,     1] loss: 1260.541
[5,     1] loss: 1260.087
[6,     1] loss: 1255.610
[7,     1] loss: 1246.356
[8,     1] loss: 1233.348
[9,     1] loss: 1192.532
[10,     1] loss: 1151.965
[11,     1] loss: 1112.672
[12,     1] loss: 1090.645
[13,     1] loss: 1155.582
[14,     1] loss: 1126.356
[15,     1] loss: 1073.550
[16,     1] loss: 1052.022
[17,     1] loss: 1042.245
[18,     1] loss: 1035.599
[19,     1] loss: 1041.016
[20,     1] loss: 1021.225
[21,     1] loss: 996.452
[22,     1] loss: 1000.319
[23,     1] loss: 958.575
[24,     1] loss: 993.249
[25,     1] loss: 1011.048
[26,     1] loss: 975.052
[27,     1] loss: 965.839
[28,     1] loss: 1001.518
[29,     1] loss: 958.617
[30,     1] loss: 944.307
[31,     1] loss: 965.481
[32,     1] loss: 937.347
[33,     1] loss: 931.031
[34,     1] loss: 887.199
[35,     1] loss: 912.432
[36,     1] loss: 918.365
[37,     1] loss: 946.874
[38,     1] loss: 831.798
[39,     1] loss: 845.160
[40,     1] loss: 869.071
[41,     1] loss: 842.685
[42,     1] loss: 874.641
[43,     1] loss: 930.208
[44,     1] loss: 871.864
[45,     1] loss: 798.256
[46,     1] loss: 826.893
[47,     1] loss: 792.444
[48,     1] loss: 894.896
[49,     1] loss: 778.602
[50,     1] loss: 747.021
[51,     1] loss: 782.471
[52,     1] loss: 747.564
[53,     1] loss: 709.232
[54,     1] loss: 716.771
[55,     1] loss: 740.156
[56,     1] loss: 895.164
[57,     1] loss: 943.624
[58,     1] loss: 732.730
[59,     1] loss: 845.229
[60,     1] loss: 756.748
[61,     1] loss: 862.156
[62,     1] loss: 761.798
[63,     1] loss: 779.932
[64,     1] loss: 711.648
[65,     1] loss: 694.822
[66,     1] loss: 714.361
[67,     1] loss: 753.942
[68,     1] loss: 659.696
[69,     1] loss: 669.644
[70,     1] loss: 591.719
[71,     1] loss: 605.176
[72,     1] loss: 709.498
[73,     1] loss: 857.653
[74,     1] loss: 649.128
[75,     1] loss: 678.402
[76,     1] loss: 670.481
[77,     1] loss: 721.813
[78,     1] loss: 640.017
[79,     1] loss: 624.734
[80,     1] loss: 670.123
[81,     1] loss: 517.050
[82,     1] loss: 624.663
[83,     1] loss: 727.772
[84,     1] loss: 498.076
[85,     1] loss: 565.019
[86,     1] loss: 722.786
[87,     1] loss: 487.844
[88,     1] loss: 660.011
[89,     1] loss: 644.616
[90,     1] loss: 529.574
[91,     1] loss: 686.964
[92,     1] loss: 535.144
[93,     1] loss: 558.178
[94,     1] loss: 497.284
[95,     1] loss: 418.582
[96,     1] loss: 510.347
[97,     1] loss: 496.245
[98,     1] loss: 412.599
[99,     1] loss: 367.638
[100,     1] loss: 401.649
[101,     1] loss: 484.331
[102,     1] loss: 928.116
[103,     1] loss: 2200.619
[104,     1] loss: 946.803
[105,     1] loss: 1050.056
[106,     1] loss: 1046.557
[107,     1] loss: 1202.114
[108,     1] loss: 1055.872
[109,     1] loss: 1073.202
[110,     1] loss: 1070.580
[111,     1] loss: 1020.347
[112,     1] loss: 1038.425
[113,     1] loss: 1007.102
[114,     1] loss: 1037.969
[115,     1] loss: 993.627
[116,     1] loss: 979.962
[117,     1] loss: 973.993
[118,     1] loss: 946.563
[119,     1] loss: 918.275
[120,     1] loss: 881.668
[121,     1] loss: 793.202
[122,     1] loss: 802.443
[123,     1] loss: 814.367
[124,     1] loss: 887.571
[125,     1] loss: 686.372
[126,     1] loss: 808.742
[127,     1] loss: 743.572
[128,     1] loss: 703.702
[129,     1] loss: 774.701
[130,     1] loss: 621.622
[131,     1] loss: 667.157
[132,     1] loss: 572.466
[133,     1] loss: 529.734
[134,     1] loss: 593.441
[135,     1] loss: 602.076
[136,     1] loss: 763.781
[137,     1] loss: 1047.072
[138,     1] loss: 1072.030
[139,     1] loss: 1111.959
[140,     1] loss: 1061.371
[141,     1] loss: 1006.668
[142,     1] loss: 842.287
[143,     1] loss: 765.574
[144,     1] loss: 895.302
[145,     1] loss: 818.729
[146,     1] loss: 868.366
[147,     1] loss: 981.343
[148,     1] loss: 781.302
[149,     1] loss: 810.896
[150,     1] loss: 731.527
[151,     1] loss: 824.001
[152,     1] loss: 704.776
[153,     1] loss: 724.792
[154,     1] loss: 650.674
[155,     1] loss: 718.758
[156,     1] loss: 663.414
[157,     1] loss: 761.534
[158,     1] loss: 592.542
[159,     1] loss: 635.639
[160,     1] loss: 540.847
[161,     1] loss: 572.152
[162,     1] loss: 580.290
[163,     1] loss: 511.469
[164,     1] loss: 490.258
[165,     1] loss: 628.216
[166,     1] loss: 1038.830
[167,     1] loss: 1511.039
[168,     1] loss: 1003.179
[169,     1] loss: 1115.633
[170,     1] loss: 1149.756
[171,     1] loss: 1119.650
[172,     1] loss: 1082.512
[173,     1] loss: 1009.985
[174,     1] loss: 979.940
[175,     1] loss: 947.990
[176,     1] loss: 984.531
[177,     1] loss: 934.852
[178,     1] loss: 932.499
[179,     1] loss: 866.367
[180,     1] loss: 879.027
Early stopping applied (best metric=0.3666338622570038)
Finished Training
Total time taken: 28.429250240325928
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.519
[2,     1] loss: 1256.081
[3,     1] loss: 1262.418
[4,     1] loss: 1262.176
[5,     1] loss: 1255.799
[6,     1] loss: 1250.543
[7,     1] loss: 1229.681
[8,     1] loss: 1213.775
[9,     1] loss: 1169.028
[10,     1] loss: 1125.197
[11,     1] loss: 1076.734
[12,     1] loss: 1060.660
[13,     1] loss: 1050.724
[14,     1] loss: 1057.019
[15,     1] loss: 1040.137
[16,     1] loss: 1034.607
[17,     1] loss: 1048.176
[18,     1] loss: 997.383
[19,     1] loss: 1023.823
[20,     1] loss: 1040.515
[21,     1] loss: 991.511
[22,     1] loss: 1016.771
[23,     1] loss: 956.205
[24,     1] loss: 1001.258
[25,     1] loss: 978.237
[26,     1] loss: 1007.884
[27,     1] loss: 985.877
[28,     1] loss: 951.553
[29,     1] loss: 1013.806
[30,     1] loss: 890.924
[31,     1] loss: 927.888
[32,     1] loss: 897.043
[33,     1] loss: 893.314
[34,     1] loss: 900.229
[35,     1] loss: 882.120
[36,     1] loss: 932.717
[37,     1] loss: 898.831
[38,     1] loss: 930.381
[39,     1] loss: 877.138
[40,     1] loss: 872.076
[41,     1] loss: 830.799
[42,     1] loss: 876.733
[43,     1] loss: 887.739
[44,     1] loss: 788.281
[45,     1] loss: 799.905
[46,     1] loss: 788.918
[47,     1] loss: 789.286
[48,     1] loss: 782.782
[49,     1] loss: 801.468
[50,     1] loss: 928.976
[51,     1] loss: 851.261
[52,     1] loss: 754.679
[53,     1] loss: 758.505
[54,     1] loss: 767.101
[55,     1] loss: 745.819
[56,     1] loss: 710.307
[57,     1] loss: 638.847
[58,     1] loss: 672.500
[59,     1] loss: 725.313
[60,     1] loss: 911.095
[61,     1] loss: 1283.483
[62,     1] loss: 675.081
[63,     1] loss: 929.121
[64,     1] loss: 834.177
[65,     1] loss: 803.261
[66,     1] loss: 869.868
[67,     1] loss: 839.118
[68,     1] loss: 764.753
[69,     1] loss: 744.797
[70,     1] loss: 794.944
[71,     1] loss: 717.174
[72,     1] loss: 700.730
[73,     1] loss: 700.866
[74,     1] loss: 646.868
[75,     1] loss: 662.715
[76,     1] loss: 562.421
[77,     1] loss: 580.937
[78,     1] loss: 654.935
[79,     1] loss: 588.493
[80,     1] loss: 839.530
[81,     1] loss: 718.924
[82,     1] loss: 553.261
[83,     1] loss: 617.294
[84,     1] loss: 522.729
Early stopping applied (best metric=0.35290566086769104)
Finished Training
Total time taken: 12.594603776931763
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.962
[2,     1] loss: 1271.434
[3,     1] loss: 1262.930
[4,     1] loss: 1265.622
[5,     1] loss: 1259.244
[6,     1] loss: 1266.039
[7,     1] loss: 1258.261
[8,     1] loss: 1259.001
[9,     1] loss: 1262.662
[10,     1] loss: 1257.864
[11,     1] loss: 1260.470
[12,     1] loss: 1263.122
[13,     1] loss: 1258.829
[14,     1] loss: 1260.226
[15,     1] loss: 1258.883
[16,     1] loss: 1258.418
[17,     1] loss: 1257.324
[18,     1] loss: 1254.602
[19,     1] loss: 1252.257
[20,     1] loss: 1247.008
[21,     1] loss: 1240.403
[22,     1] loss: 1217.199
[23,     1] loss: 1197.282
[24,     1] loss: 1171.315
[25,     1] loss: 1146.323
[26,     1] loss: 1097.019
[27,     1] loss: 1095.294
[28,     1] loss: 1078.087
[29,     1] loss: 1055.491
[30,     1] loss: 1020.154
[31,     1] loss: 1049.972
[32,     1] loss: 1016.189
[33,     1] loss: 968.430
[34,     1] loss: 958.414
[35,     1] loss: 965.978
[36,     1] loss: 997.983
[37,     1] loss: 937.970
[38,     1] loss: 930.794
[39,     1] loss: 943.460
[40,     1] loss: 993.991
[41,     1] loss: 911.791
[42,     1] loss: 931.679
[43,     1] loss: 963.927
[44,     1] loss: 876.956
[45,     1] loss: 957.586
[46,     1] loss: 898.560
[47,     1] loss: 904.986
[48,     1] loss: 877.030
[49,     1] loss: 897.986
[50,     1] loss: 889.492
[51,     1] loss: 854.092
[52,     1] loss: 883.340
[53,     1] loss: 820.332
[54,     1] loss: 874.243
[55,     1] loss: 860.841
[56,     1] loss: 870.718
[57,     1] loss: 803.130
[58,     1] loss: 804.710
[59,     1] loss: 779.639
[60,     1] loss: 769.384
[61,     1] loss: 713.473
[62,     1] loss: 748.878
[63,     1] loss: 1011.479
[64,     1] loss: 1926.743
[65,     1] loss: 811.665
[66,     1] loss: 1074.833
[67,     1] loss: 1063.751
[68,     1] loss: 993.333
[69,     1] loss: 1029.500
[70,     1] loss: 1026.314
[71,     1] loss: 1016.881
[72,     1] loss: 1012.964
[73,     1] loss: 960.568
[74,     1] loss: 933.639
[75,     1] loss: 943.470
[76,     1] loss: 910.913
[77,     1] loss: 907.555
[78,     1] loss: 922.417
[79,     1] loss: 915.101
[80,     1] loss: 846.200
[81,     1] loss: 901.851
[82,     1] loss: 872.823
[83,     1] loss: 876.028
[84,     1] loss: 865.372
[85,     1] loss: 841.839
[86,     1] loss: 846.898
[87,     1] loss: 786.410
[88,     1] loss: 782.058
[89,     1] loss: 768.696
[90,     1] loss: 791.877
[91,     1] loss: 771.728
[92,     1] loss: 803.417
[93,     1] loss: 923.144
[94,     1] loss: 779.651
[95,     1] loss: 859.659
[96,     1] loss: 854.831
[97,     1] loss: 782.636
[98,     1] loss: 779.684
[99,     1] loss: 773.049
[100,     1] loss: 768.939
[101,     1] loss: 736.923
[102,     1] loss: 740.137
[103,     1] loss: 662.803
[104,     1] loss: 695.885
[105,     1] loss: 665.685
[106,     1] loss: 651.892
[107,     1] loss: 537.328
[108,     1] loss: 676.473
[109,     1] loss: 1191.232
[110,     1] loss: 994.607
[111,     1] loss: 641.343
[112,     1] loss: 892.825
[113,     1] loss: 728.625
[114,     1] loss: 771.689
[115,     1] loss: 762.922
[116,     1] loss: 704.402
[117,     1] loss: 781.343
[118,     1] loss: 693.091
[119,     1] loss: 758.028
[120,     1] loss: 635.640
[121,     1] loss: 661.275
[122,     1] loss: 616.062
[123,     1] loss: 733.250
[124,     1] loss: 604.998
[125,     1] loss: 573.067
[126,     1] loss: 609.007
[127,     1] loss: 557.899
[128,     1] loss: 519.320
[129,     1] loss: 593.140
[130,     1] loss: 868.367
[131,     1] loss: 818.735
[132,     1] loss: 696.367
[133,     1] loss: 611.201
[134,     1] loss: 617.244
[135,     1] loss: 691.002
[136,     1] loss: 589.908
[137,     1] loss: 694.026
[138,     1] loss: 725.366
[139,     1] loss: 550.056
[140,     1] loss: 639.976
[141,     1] loss: 528.735
[142,     1] loss: 651.052
[143,     1] loss: 661.575
[144,     1] loss: 466.979
[145,     1] loss: 596.797
[146,     1] loss: 552.396
Early stopping applied (best metric=0.3697940707206726)
Finished Training
Total time taken: 21.875624895095825
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.586
[2,     1] loss: 1286.557
[3,     1] loss: 1271.674
[4,     1] loss: 1264.686
[5,     1] loss: 1263.898
[6,     1] loss: 1259.092
[7,     1] loss: 1262.065
[8,     1] loss: 1260.974
[9,     1] loss: 1259.710
[10,     1] loss: 1262.820
[11,     1] loss: 1258.868
[12,     1] loss: 1258.957
[13,     1] loss: 1261.277
[14,     1] loss: 1259.186
[15,     1] loss: 1261.005
[16,     1] loss: 1261.909
[17,     1] loss: 1261.946
[18,     1] loss: 1259.351
[19,     1] loss: 1260.026
[20,     1] loss: 1259.354
[21,     1] loss: 1259.317
[22,     1] loss: 1259.495
[23,     1] loss: 1260.270
[24,     1] loss: 1257.119
[25,     1] loss: 1255.389
[26,     1] loss: 1256.585
[27,     1] loss: 1247.712
[28,     1] loss: 1241.175
[29,     1] loss: 1219.520
[30,     1] loss: 1199.757
[31,     1] loss: 1168.434
[32,     1] loss: 1109.875
[33,     1] loss: 1077.505
[34,     1] loss: 1058.175
[35,     1] loss: 1059.781
[36,     1] loss: 1100.731
[37,     1] loss: 997.077
[38,     1] loss: 971.938
[39,     1] loss: 1023.331
[40,     1] loss: 998.253
[41,     1] loss: 985.289
[42,     1] loss: 987.959
[43,     1] loss: 976.450
[44,     1] loss: 986.754
[45,     1] loss: 962.727
[46,     1] loss: 1074.399
[47,     1] loss: 881.004
[48,     1] loss: 944.052
[49,     1] loss: 949.759
[50,     1] loss: 992.409
[51,     1] loss: 956.457
[52,     1] loss: 885.989
[53,     1] loss: 893.671
[54,     1] loss: 905.000
[55,     1] loss: 909.806
[56,     1] loss: 887.500
[57,     1] loss: 864.558
[58,     1] loss: 840.351
[59,     1] loss: 873.292
[60,     1] loss: 932.010
[61,     1] loss: 943.544
[62,     1] loss: 831.988
[63,     1] loss: 909.925
[64,     1] loss: 845.054
[65,     1] loss: 858.696
[66,     1] loss: 825.065
[67,     1] loss: 849.791
[68,     1] loss: 810.380
[69,     1] loss: 831.289
[70,     1] loss: 768.897
[71,     1] loss: 738.625
[72,     1] loss: 705.070
[73,     1] loss: 724.708
[74,     1] loss: 776.687
[75,     1] loss: 912.527
[76,     1] loss: 1033.766
[77,     1] loss: 850.701
[78,     1] loss: 863.808
[79,     1] loss: 801.637
[80,     1] loss: 884.006
[81,     1] loss: 827.154
[82,     1] loss: 800.648
[83,     1] loss: 824.954
[84,     1] loss: 831.771
[85,     1] loss: 719.557
[86,     1] loss: 752.504
[87,     1] loss: 703.277
[88,     1] loss: 698.986
[89,     1] loss: 654.157
[90,     1] loss: 728.840
[91,     1] loss: 898.340
[92,     1] loss: 768.853
[93,     1] loss: 678.289
[94,     1] loss: 705.683
[95,     1] loss: 624.005
[96,     1] loss: 687.401
[97,     1] loss: 651.103
[98,     1] loss: 646.247
[99,     1] loss: 642.424
[100,     1] loss: 603.062
[101,     1] loss: 590.028
[102,     1] loss: 515.568
[103,     1] loss: 488.050
[104,     1] loss: 512.662
[105,     1] loss: 1186.498
[106,     1] loss: 2691.371
[107,     1] loss: 705.274
[108,     1] loss: 1031.954
Early stopping applied (best metric=0.3905039131641388)
Finished Training
Total time taken: 16.032620429992676
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.879
[2,     1] loss: 1261.583
[3,     1] loss: 1258.578
[4,     1] loss: 1262.149
[5,     1] loss: 1276.755
[6,     1] loss: 1260.061
[7,     1] loss: 1258.667
[8,     1] loss: 1257.942
[9,     1] loss: 1254.237
[10,     1] loss: 1248.839
[11,     1] loss: 1240.870
[12,     1] loss: 1223.351
[13,     1] loss: 1204.528
[14,     1] loss: 1148.372
[15,     1] loss: 1089.392
[16,     1] loss: 1050.653
[17,     1] loss: 1053.476
[18,     1] loss: 1015.547
[19,     1] loss: 1006.990
[20,     1] loss: 1018.837
[21,     1] loss: 1011.965
[22,     1] loss: 985.227
[23,     1] loss: 1009.703
[24,     1] loss: 992.744
[25,     1] loss: 1036.931
[26,     1] loss: 967.636
[27,     1] loss: 980.007
[28,     1] loss: 956.173
[29,     1] loss: 933.409
[30,     1] loss: 891.626
[31,     1] loss: 918.611
[32,     1] loss: 938.516
[33,     1] loss: 962.916
[34,     1] loss: 869.602
[35,     1] loss: 893.995
[36,     1] loss: 850.932
[37,     1] loss: 863.921
[38,     1] loss: 847.172
[39,     1] loss: 890.909
[40,     1] loss: 894.410
[41,     1] loss: 816.586
[42,     1] loss: 793.173
[43,     1] loss: 780.502
[44,     1] loss: 895.054
[45,     1] loss: 930.201
[46,     1] loss: 933.896
[47,     1] loss: 742.142
[48,     1] loss: 846.117
[49,     1] loss: 828.534
[50,     1] loss: 803.156
[51,     1] loss: 793.001
[52,     1] loss: 789.358
[53,     1] loss: 723.933
[54,     1] loss: 808.187
[55,     1] loss: 747.564
[56,     1] loss: 663.916
[57,     1] loss: 766.932
[58,     1] loss: 838.330
[59,     1] loss: 682.821
[60,     1] loss: 719.024
[61,     1] loss: 731.881
[62,     1] loss: 607.976
[63,     1] loss: 699.052
[64,     1] loss: 599.531
[65,     1] loss: 638.610
[66,     1] loss: 653.984
[67,     1] loss: 769.506
[68,     1] loss: 1024.822
[69,     1] loss: 948.234
[70,     1] loss: 786.289
[71,     1] loss: 730.155
[72,     1] loss: 876.089
[73,     1] loss: 836.134
[74,     1] loss: 756.857
[75,     1] loss: 817.958
[76,     1] loss: 772.364
[77,     1] loss: 729.513
[78,     1] loss: 751.606
[79,     1] loss: 629.564
[80,     1] loss: 660.499
[81,     1] loss: 595.059
[82,     1] loss: 609.400
[83,     1] loss: 550.027
[84,     1] loss: 597.220
[85,     1] loss: 518.388
[86,     1] loss: 505.092
[87,     1] loss: 545.952
[88,     1] loss: 471.734
[89,     1] loss: 463.180
[90,     1] loss: 1051.536
[91,     1] loss: 2611.771
[92,     1] loss: 1314.122
[93,     1] loss: 1061.510
[94,     1] loss: 1113.521
[95,     1] loss: 1168.684
[96,     1] loss: 1120.810
[97,     1] loss: 1102.167
[98,     1] loss: 1096.414
[99,     1] loss: 1090.319
[100,     1] loss: 1094.914
[101,     1] loss: 1089.716
[102,     1] loss: 1056.362
Early stopping applied (best metric=0.3614986538887024)
Finished Training
Total time taken: 14.89357042312622
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1264.123
[2,     1] loss: 1263.145
[3,     1] loss: 1264.749
[4,     1] loss: 1269.844
[5,     1] loss: 1259.452
[6,     1] loss: 1259.568
[7,     1] loss: 1263.083
[8,     1] loss: 1261.347
[9,     1] loss: 1256.250
[10,     1] loss: 1256.650
[11,     1] loss: 1247.197
[12,     1] loss: 1245.022
[13,     1] loss: 1226.002
[14,     1] loss: 1200.297
[15,     1] loss: 1154.881
[16,     1] loss: 1088.774
[17,     1] loss: 1086.472
[18,     1] loss: 1048.560
[19,     1] loss: 1124.374
[20,     1] loss: 1084.251
[21,     1] loss: 1027.583
[22,     1] loss: 1018.473
[23,     1] loss: 1024.471
[24,     1] loss: 1019.139
[25,     1] loss: 1025.859
[26,     1] loss: 988.708
[27,     1] loss: 997.750
[28,     1] loss: 969.474
[29,     1] loss: 952.614
[30,     1] loss: 917.818
[31,     1] loss: 898.173
[32,     1] loss: 905.572
[33,     1] loss: 928.857
[34,     1] loss: 912.258
[35,     1] loss: 915.677
[36,     1] loss: 916.215
[37,     1] loss: 877.933
[38,     1] loss: 847.024
[39,     1] loss: 855.015
[40,     1] loss: 836.476
[41,     1] loss: 855.083
[42,     1] loss: 984.513
[43,     1] loss: 855.404
[44,     1] loss: 867.895
[45,     1] loss: 833.962
[46,     1] loss: 849.287
[47,     1] loss: 802.157
[48,     1] loss: 804.887
[49,     1] loss: 796.898
[50,     1] loss: 772.645
[51,     1] loss: 722.577
[52,     1] loss: 786.485
[53,     1] loss: 751.250
[54,     1] loss: 806.273
[55,     1] loss: 884.452
[56,     1] loss: 935.903
[57,     1] loss: 720.901
[58,     1] loss: 888.567
[59,     1] loss: 708.259
[60,     1] loss: 758.956
[61,     1] loss: 787.205
[62,     1] loss: 737.800
[63,     1] loss: 818.056
[64,     1] loss: 686.613
[65,     1] loss: 773.969
[66,     1] loss: 674.803
[67,     1] loss: 667.696
[68,     1] loss: 781.180
[69,     1] loss: 611.070
[70,     1] loss: 689.758
[71,     1] loss: 701.469
[72,     1] loss: 582.954
[73,     1] loss: 750.397
[74,     1] loss: 597.628
[75,     1] loss: 588.638
[76,     1] loss: 638.257
[77,     1] loss: 508.591
[78,     1] loss: 516.435
[79,     1] loss: 611.750
[80,     1] loss: 638.189
[81,     1] loss: 725.295
[82,     1] loss: 490.543
[83,     1] loss: 742.529
[84,     1] loss: 693.531
[85,     1] loss: 672.391
[86,     1] loss: 556.736
[87,     1] loss: 626.109
[88,     1] loss: 577.914
[89,     1] loss: 529.642
[90,     1] loss: 681.108
[91,     1] loss: 504.371
[92,     1] loss: 506.512
[93,     1] loss: 476.867
[94,     1] loss: 519.579
[95,     1] loss: 431.846
[96,     1] loss: 421.612
[97,     1] loss: 702.496
[98,     1] loss: 779.149
[99,     1] loss: 455.589
[100,     1] loss: 699.457
[101,     1] loss: 487.826
[102,     1] loss: 623.678
[103,     1] loss: 471.105
[104,     1] loss: 500.073
[105,     1] loss: 769.566
[106,     1] loss: 472.784
[107,     1] loss: 861.356
[108,     1] loss: 530.173
[109,     1] loss: 667.673
[110,     1] loss: 507.975
[111,     1] loss: 527.075
[112,     1] loss: 486.636
[113,     1] loss: 488.094
[114,     1] loss: 468.955
[115,     1] loss: 658.794
[116,     1] loss: 425.920
[117,     1] loss: 497.974
[118,     1] loss: 422.591
[119,     1] loss: 422.278
[120,     1] loss: 407.694
[121,     1] loss: 370.229
[122,     1] loss: 531.048
[123,     1] loss: 650.025
[124,     1] loss: 422.631
[125,     1] loss: 421.112
[126,     1] loss: 427.739
[127,     1] loss: 334.583
[128,     1] loss: 516.107
[129,     1] loss: 1106.112
[130,     1] loss: 633.909
[131,     1] loss: 540.836
[132,     1] loss: 568.690
[133,     1] loss: 501.491
Early stopping applied (best metric=0.37765827775001526)
Finished Training
Total time taken: 19.698187112808228
{'Hydroxylation-K Validation Accuracy': 0.712854609929078, 'Hydroxylation-K Validation Sensitivity': 0.5911111111111111, 'Hydroxylation-K Validation Specificity': 0.743859649122807, 'Hydroxylation-K Validation Precision': 0.36747043204473545, 'Hydroxylation-K AUC ROC': 0.7638986354775829, 'Hydroxylation-K AUC PR': 0.5453217232685308, 'Hydroxylation-K MCC': 0.28774099808333387, 'Hydroxylation-K F1': 0.44813785817575097, 'Validation Loss (Hydroxylation-K)': 0.4973079442977905, 'Hydroxylation-P Validation Accuracy': 0.7979626076510499, 'Hydroxylation-P Validation Sensitivity': 0.7802645502645503, 'Hydroxylation-P Validation Specificity': 0.8017532046486109, 'Hydroxylation-P Validation Precision': 0.4677742762408337, 'Hydroxylation-P AUC ROC': 0.8487823978079103, 'Hydroxylation-P AUC PR': 0.6067748717426534, 'Hydroxylation-P MCC': 0.4894545048284409, 'Hydroxylation-P F1': 0.5800186696395191, 'Validation Loss (Hydroxylation-P)': 0.3660585661729177, 'Validation Loss (total)': 0.8633665124575297, 'TimeToTrain': 18.220298989613852}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004446768810190902,
 'learning_rate_Hydroxylation-K': 0.008521776435780615,
 'learning_rate_Hydroxylation-P': 0.005448746025913672,
 'log_base': 1.9160423903029395,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3514911809,
 'sample_weights': [1.674200066355474, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.040796125256537,
 'weight_decay_Hydroxylation-K': 5.318052723565254,
 'weight_decay_Hydroxylation-P': 1.3292157263969562}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1457.567
[2,     1] loss: 1450.095
[3,     1] loss: 1448.383
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00910462813174258,
 'learning_rate_Hydroxylation-K': 0.007609590301268058,
 'learning_rate_Hydroxylation-P': 0.00688826070847546,
 'log_base': 2.2323338177237906,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2343021986,
 'sample_weights': [2.567340012865623, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.755488148360392,
 'weight_decay_Hydroxylation-K': 1.4987134425505833,
 'weight_decay_Hydroxylation-P': 1.4319226747330316}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.769
[2,     1] loss: 1356.824
[3,     1] loss: 1351.374
[4,     1] loss: 1351.754
[5,     1] loss: 1348.961
[6,     1] loss: 1348.217
[7,     1] loss: 1344.665
[8,     1] loss: 1345.962
[9,     1] loss: 1346.980
[10,     1] loss: 1345.735
[11,     1] loss: 1345.606
[12,     1] loss: 1345.091
[13,     1] loss: 1347.121
[14,     1] loss: 1345.977
[15,     1] loss: 1345.574
[16,     1] loss: 1345.558
[17,     1] loss: 1344.643
[18,     1] loss: 1343.677
[19,     1] loss: 1344.797
[20,     1] loss: 1342.287
[21,     1] loss: 1340.453
[22,     1] loss: 1331.919
[23,     1] loss: 1320.216
[24,     1] loss: 1301.566
[25,     1] loss: 1278.247
[26,     1] loss: 1294.379
[27,     1] loss: 1221.298
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004530594441121464,
 'learning_rate_Hydroxylation-K': 0.00080780351553747,
 'learning_rate_Hydroxylation-P': 0.0015643014483919756,
 'log_base': 1.4773779220657972,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1505317032,
 'sample_weights': [2.0788844415448207, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.436747303958294,
 'weight_decay_Hydroxylation-K': 7.853532417891147,
 'weight_decay_Hydroxylation-P': 0.4498409910126089}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1819.456
[2,     1] loss: 1808.498
[3,     1] loss: 1806.281
[4,     1] loss: 1809.112
[5,     1] loss: 1810.198
[6,     1] loss: 1812.483
[7,     1] loss: 1812.674
[8,     1] loss: 1811.004
[9,     1] loss: 1807.281
[10,     1] loss: 1807.585
[11,     1] loss: 1804.169
[12,     1] loss: 1803.687
[13,     1] loss: 1802.825
[14,     1] loss: 1789.358
[15,     1] loss: 1780.202
[16,     1] loss: 1759.897
[17,     1] loss: 1730.564
[18,     1] loss: 1691.832
[19,     1] loss: 1675.329
[20,     1] loss: 1635.262
[21,     1] loss: 1606.239
[22,     1] loss: 1619.983
[23,     1] loss: 1548.984
[24,     1] loss: 1518.009
[25,     1] loss: 1501.910
[26,     1] loss: 1554.339
[27,     1] loss: 1494.770
[28,     1] loss: 1529.517
[29,     1] loss: 1490.239
[30,     1] loss: 1463.659
[31,     1] loss: 1449.070
[32,     1] loss: 1403.136
[33,     1] loss: 1418.513
[34,     1] loss: 1323.080
[35,     1] loss: 1343.101
[36,     1] loss: 1290.514
[37,     1] loss: 1231.136
[38,     1] loss: 1259.862
[39,     1] loss: 1293.755
[40,     1] loss: 1277.559
[41,     1] loss: 1359.085
[42,     1] loss: 1505.198
[43,     1] loss: 1197.338
[44,     1] loss: 1438.125
[45,     1] loss: 1272.694
[46,     1] loss: 1300.411
[47,     1] loss: 1262.178
[48,     1] loss: 1257.743
[49,     1] loss: 1238.631
[50,     1] loss: 1159.282
[51,     1] loss: 1201.139
[52,     1] loss: 1110.770
[53,     1] loss: 1126.826
[54,     1] loss: 1049.592
[55,     1] loss: 1124.942
[56,     1] loss: 1078.468
[57,     1] loss: 991.161
[58,     1] loss: 918.079
[59,     1] loss: 1077.216
[60,     1] loss: 997.453
[61,     1] loss: 933.412
[62,     1] loss: 914.709
[63,     1] loss: 917.095
[64,     1] loss: 918.356
[65,     1] loss: 904.634
[66,     1] loss: 921.727
[67,     1] loss: 1049.217
[68,     1] loss: 875.910
[69,     1] loss: 921.814
[70,     1] loss: 943.536
[71,     1] loss: 920.621
[72,     1] loss: 981.326
[73,     1] loss: 888.420
[74,     1] loss: 928.220
[75,     1] loss: 829.805
[76,     1] loss: 813.541
[77,     1] loss: 932.215
[78,     1] loss: 939.066
[79,     1] loss: 952.805
[80,     1] loss: 857.528
[81,     1] loss: 1153.141
[82,     1] loss: 943.867
[83,     1] loss: 1083.281
[84,     1] loss: 857.517
[85,     1] loss: 1066.081
[86,     1] loss: 927.972
[87,     1] loss: 910.685
Early stopping applied (best metric=0.3633560538291931)
Finished Training
Total time taken: 13.341229677200317
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1812.580
[2,     1] loss: 1812.037
[3,     1] loss: 1810.568
[4,     1] loss: 1808.052
[5,     1] loss: 1804.624
[6,     1] loss: 1807.290
[7,     1] loss: 1808.443
[8,     1] loss: 1803.192
[9,     1] loss: 1811.109
[10,     1] loss: 1804.689
[11,     1] loss: 1789.768
[12,     1] loss: 1787.629
[13,     1] loss: 1762.023
[14,     1] loss: 1738.133
[15,     1] loss: 1700.275
[16,     1] loss: 1662.477
[17,     1] loss: 1630.145
[18,     1] loss: 1595.376
[19,     1] loss: 1567.275
[20,     1] loss: 1565.921
[21,     1] loss: 1515.224
[22,     1] loss: 1547.516
[23,     1] loss: 1474.151
[24,     1] loss: 1494.618
[25,     1] loss: 1500.250
[26,     1] loss: 1438.683
[27,     1] loss: 1425.072
[28,     1] loss: 1449.347
[29,     1] loss: 1336.750
[30,     1] loss: 1343.317
[31,     1] loss: 1424.675
[32,     1] loss: 1370.102
[33,     1] loss: 1392.277
[34,     1] loss: 1315.446
[35,     1] loss: 1306.184
[36,     1] loss: 1244.720
[37,     1] loss: 1343.171
[38,     1] loss: 1253.985
[39,     1] loss: 1227.579
[40,     1] loss: 1202.502
[41,     1] loss: 1282.900
[42,     1] loss: 1139.227
[43,     1] loss: 1213.054
[44,     1] loss: 1068.416
[45,     1] loss: 1220.576
[46,     1] loss: 1289.994
[47,     1] loss: 2075.452
[48,     1] loss: 1232.545
[49,     1] loss: 1587.309
[50,     1] loss: 1369.732
[51,     1] loss: 1297.454
[52,     1] loss: 1444.426
[53,     1] loss: 1452.561
[54,     1] loss: 1480.597
[55,     1] loss: 1342.413
[56,     1] loss: 1346.594
[57,     1] loss: 1378.315
[58,     1] loss: 1411.235
[59,     1] loss: 1399.979
[60,     1] loss: 1232.767
[61,     1] loss: 1213.358
[62,     1] loss: 1304.574
[63,     1] loss: 1274.326
[64,     1] loss: 1199.133
[65,     1] loss: 1147.563
[66,     1] loss: 1218.050
[67,     1] loss: 1109.836
[68,     1] loss: 1125.304
[69,     1] loss: 1178.512
[70,     1] loss: 1053.733
[71,     1] loss: 1106.110
[72,     1] loss: 1053.240
[73,     1] loss: 1055.962
[74,     1] loss: 1008.034
[75,     1] loss: 1022.300
[76,     1] loss: 975.951
[77,     1] loss: 1031.636
[78,     1] loss: 914.615
[79,     1] loss: 924.139
[80,     1] loss: 1056.266
[81,     1] loss: 948.090
[82,     1] loss: 869.910
[83,     1] loss: 880.413
[84,     1] loss: 912.814
[85,     1] loss: 903.371
[86,     1] loss: 865.261
[87,     1] loss: 837.627
[88,     1] loss: 846.669
[89,     1] loss: 844.662
[90,     1] loss: 1041.464
[91,     1] loss: 1250.253
[92,     1] loss: 1443.106
[93,     1] loss: 977.858
[94,     1] loss: 1212.574
Early stopping applied (best metric=0.3321053087711334)
Finished Training
Total time taken: 14.219603300094604
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1814.424
[2,     1] loss: 1816.174
[3,     1] loss: 1818.643
[4,     1] loss: 1803.398
[5,     1] loss: 1799.862
[6,     1] loss: 1805.160
[7,     1] loss: 1805.141
[8,     1] loss: 1791.186
[9,     1] loss: 1783.292
[10,     1] loss: 1763.418
[11,     1] loss: 1752.879
[12,     1] loss: 1688.534
[13,     1] loss: 1672.723
[14,     1] loss: 1573.935
[15,     1] loss: 1598.297
[16,     1] loss: 1526.287
[17,     1] loss: 1531.697
[18,     1] loss: 1495.202
[19,     1] loss: 1464.054
[20,     1] loss: 1481.158
[21,     1] loss: 1479.686
[22,     1] loss: 1436.890
[23,     1] loss: 1449.974
[24,     1] loss: 1338.598
[25,     1] loss: 1337.852
[26,     1] loss: 1410.183
[27,     1] loss: 1268.416
[28,     1] loss: 1272.263
[29,     1] loss: 1416.913
[30,     1] loss: 1264.720
[31,     1] loss: 1289.714
[32,     1] loss: 1334.034
[33,     1] loss: 1242.132
[34,     1] loss: 1224.410
[35,     1] loss: 1231.923
[36,     1] loss: 1289.294
[37,     1] loss: 1220.994
[38,     1] loss: 1164.236
[39,     1] loss: 1125.948
[40,     1] loss: 1081.877
[41,     1] loss: 1094.792
[42,     1] loss: 1071.224
[43,     1] loss: 1092.988
[44,     1] loss: 1110.420
[45,     1] loss: 981.745
[46,     1] loss: 1008.051
[47,     1] loss: 983.949
[48,     1] loss: 1055.990
[49,     1] loss: 1017.497
[50,     1] loss: 916.381
[51,     1] loss: 965.846
[52,     1] loss: 1083.693
[53,     1] loss: 934.542
[54,     1] loss: 951.126
[55,     1] loss: 885.183
[56,     1] loss: 927.668
[57,     1] loss: 1141.452
[58,     1] loss: 986.665
[59,     1] loss: 914.417
[60,     1] loss: 961.475
[61,     1] loss: 888.418
[62,     1] loss: 902.940
[63,     1] loss: 845.621
[64,     1] loss: 1151.811
[65,     1] loss: 993.266
[66,     1] loss: 887.702
[67,     1] loss: 922.458
[68,     1] loss: 865.899
[69,     1] loss: 894.788
[70,     1] loss: 837.692
[71,     1] loss: 841.797
[72,     1] loss: 781.508
[73,     1] loss: 882.749
Early stopping applied (best metric=0.44199812412261963)
Finished Training
Total time taken: 11.107140302658081
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1807.413
[2,     1] loss: 1810.143
[3,     1] loss: 1826.585
[4,     1] loss: 1811.439
[5,     1] loss: 1805.882
[6,     1] loss: 1803.626
[7,     1] loss: 1800.322
[8,     1] loss: 1802.920
[9,     1] loss: 1787.650
[10,     1] loss: 1765.736
[11,     1] loss: 1748.474
[12,     1] loss: 1710.199
[13,     1] loss: 1672.508
[14,     1] loss: 1638.659
[15,     1] loss: 1590.700
[16,     1] loss: 1551.183
[17,     1] loss: 1566.226
[18,     1] loss: 1593.649
[19,     1] loss: 1579.472
[20,     1] loss: 1511.634
[21,     1] loss: 1496.011
[22,     1] loss: 1511.589
[23,     1] loss: 1490.273
[24,     1] loss: 1436.523
[25,     1] loss: 1522.373
[26,     1] loss: 1500.554
[27,     1] loss: 1485.201
[28,     1] loss: 1442.894
[29,     1] loss: 1458.702
[30,     1] loss: 1401.400
[31,     1] loss: 1387.328
[32,     1] loss: 1331.889
[33,     1] loss: 1396.982
[34,     1] loss: 1290.589
[35,     1] loss: 1328.167
[36,     1] loss: 1294.573
[37,     1] loss: 1325.138
[38,     1] loss: 1259.247
[39,     1] loss: 1200.867
[40,     1] loss: 1153.033
[41,     1] loss: 1255.983
[42,     1] loss: 1376.226
[43,     1] loss: 1349.078
[44,     1] loss: 1191.823
[45,     1] loss: 1212.545
[46,     1] loss: 1244.191
[47,     1] loss: 1190.456
[48,     1] loss: 1200.579
[49,     1] loss: 1178.101
[50,     1] loss: 1160.152
[51,     1] loss: 1113.899
[52,     1] loss: 1059.053
[53,     1] loss: 1146.865
[54,     1] loss: 1024.992
[55,     1] loss: 1029.303
[56,     1] loss: 968.845
[57,     1] loss: 1385.072
[58,     1] loss: 1033.597
[59,     1] loss: 1005.899
[60,     1] loss: 1029.499
[61,     1] loss: 1127.853
[62,     1] loss: 1110.710
[63,     1] loss: 1064.083
[64,     1] loss: 990.242
[65,     1] loss: 983.083
[66,     1] loss: 1040.174
[67,     1] loss: 909.216
[68,     1] loss: 908.646
[69,     1] loss: 1025.965
[70,     1] loss: 1128.923
[71,     1] loss: 931.651
[72,     1] loss: 1040.407
[73,     1] loss: 876.634
[74,     1] loss: 1029.675
[75,     1] loss: 1010.624
[76,     1] loss: 890.917
[77,     1] loss: 974.104
[78,     1] loss: 1004.783
[79,     1] loss: 942.742
[80,     1] loss: 851.322
[81,     1] loss: 884.958
[82,     1] loss: 864.888
[83,     1] loss: 924.373
[84,     1] loss: 756.191
[85,     1] loss: 943.965
[86,     1] loss: 898.735
Early stopping applied (best metric=0.3644428253173828)
Finished Training
Total time taken: 13.90143895149231
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1816.433
[2,     1] loss: 1817.984
[3,     1] loss: 1801.732
[4,     1] loss: 1808.871
[5,     1] loss: 1808.300
[6,     1] loss: 1801.396
[7,     1] loss: 1817.375
[8,     1] loss: 1805.982
[9,     1] loss: 1796.703
[10,     1] loss: 1782.324
[11,     1] loss: 1765.367
[12,     1] loss: 1739.779
[13,     1] loss: 1681.637
[14,     1] loss: 1641.245
[15,     1] loss: 1588.400
[16,     1] loss: 1582.475
[17,     1] loss: 1547.480
[18,     1] loss: 1519.598
[19,     1] loss: 1502.488
[20,     1] loss: 1578.183
[21,     1] loss: 1518.039
[22,     1] loss: 1496.995
[23,     1] loss: 1444.933
[24,     1] loss: 1427.776
[25,     1] loss: 1508.598
[26,     1] loss: 1514.876
[27,     1] loss: 1432.068
[28,     1] loss: 1404.720
[29,     1] loss: 1386.924
[30,     1] loss: 1398.748
[31,     1] loss: 1344.197
[32,     1] loss: 1368.985
[33,     1] loss: 1254.214
[34,     1] loss: 1312.204
[35,     1] loss: 1221.562
[36,     1] loss: 1241.994
[37,     1] loss: 1189.581
[38,     1] loss: 1191.935
[39,     1] loss: 1175.780
[40,     1] loss: 1188.358
[41,     1] loss: 1240.719
[42,     1] loss: 1256.392
[43,     1] loss: 1240.002
[44,     1] loss: 1128.485
[45,     1] loss: 1265.122
[46,     1] loss: 1299.202
[47,     1] loss: 1112.758
[48,     1] loss: 1219.090
[49,     1] loss: 1222.755
[50,     1] loss: 1144.703
[51,     1] loss: 1115.498
[52,     1] loss: 1065.314
[53,     1] loss: 1009.673
[54,     1] loss: 1055.622
[55,     1] loss: 977.813
[56,     1] loss: 1228.095
[57,     1] loss: 1138.256
[58,     1] loss: 975.542
[59,     1] loss: 1058.433
[60,     1] loss: 958.638
[61,     1] loss: 1038.423
[62,     1] loss: 948.588
[63,     1] loss: 1071.718
[64,     1] loss: 961.339
[65,     1] loss: 980.626
[66,     1] loss: 922.221
[67,     1] loss: 849.057
[68,     1] loss: 886.552
[69,     1] loss: 1054.734
[70,     1] loss: 871.286
[71,     1] loss: 854.664
[72,     1] loss: 909.020
[73,     1] loss: 875.876
[74,     1] loss: 864.662
[75,     1] loss: 946.743
[76,     1] loss: 747.195
[77,     1] loss: 885.302
[78,     1] loss: 1065.215
[79,     1] loss: 808.739
[80,     1] loss: 926.352
[81,     1] loss: 783.709
[82,     1] loss: 879.045
[83,     1] loss: 837.623
[84,     1] loss: 822.452
[85,     1] loss: 797.993
[86,     1] loss: 745.798
[87,     1] loss: 782.005
[88,     1] loss: 732.516
[89,     1] loss: 794.372
[90,     1] loss: 832.909
[91,     1] loss: 714.115
[92,     1] loss: 806.113
[93,     1] loss: 1184.750
[94,     1] loss: 817.666
[95,     1] loss: 1167.006
[96,     1] loss: 821.848
[97,     1] loss: 1198.685
[98,     1] loss: 848.388
Early stopping applied (best metric=0.40306684374809265)
Finished Training
Total time taken: 14.654261589050293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1810.996
[2,     1] loss: 1808.950
[3,     1] loss: 1810.863
[4,     1] loss: 1811.112
[5,     1] loss: 1802.089
[6,     1] loss: 1801.081
[7,     1] loss: 1797.168
[8,     1] loss: 1808.000
[9,     1] loss: 1786.139
[10,     1] loss: 1795.345
[11,     1] loss: 1771.660
[12,     1] loss: 1724.557
[13,     1] loss: 1680.641
[14,     1] loss: 1613.783
[15,     1] loss: 1556.398
[16,     1] loss: 1526.820
[17,     1] loss: 1502.124
[18,     1] loss: 1486.063
[19,     1] loss: 1538.440
[20,     1] loss: 1549.062
[21,     1] loss: 1456.665
[22,     1] loss: 1396.667
[23,     1] loss: 1419.896
[24,     1] loss: 1508.743
[25,     1] loss: 1407.869
[26,     1] loss: 1437.468
[27,     1] loss: 1559.438
[28,     1] loss: 1367.736
[29,     1] loss: 1427.632
[30,     1] loss: 1344.015
[31,     1] loss: 1381.490
[32,     1] loss: 1345.138
[33,     1] loss: 1378.270
[34,     1] loss: 1316.495
[35,     1] loss: 1292.590
[36,     1] loss: 1306.300
[37,     1] loss: 1251.921
[38,     1] loss: 1182.182
[39,     1] loss: 1249.547
[40,     1] loss: 1186.505
[41,     1] loss: 1158.922
[42,     1] loss: 1086.053
[43,     1] loss: 1107.928
[44,     1] loss: 1140.404
[45,     1] loss: 1144.834
[46,     1] loss: 1149.752
[47,     1] loss: 1205.371
[48,     1] loss: 1052.756
[49,     1] loss: 1001.526
[50,     1] loss: 1042.381
[51,     1] loss: 1010.180
[52,     1] loss: 989.456
[53,     1] loss: 984.487
[54,     1] loss: 1665.835
[55,     1] loss: 1133.134
[56,     1] loss: 1228.802
[57,     1] loss: 1061.061
[58,     1] loss: 1242.374
[59,     1] loss: 1137.263
[60,     1] loss: 1045.143
[61,     1] loss: 1238.336
[62,     1] loss: 1100.552
[63,     1] loss: 982.939
[64,     1] loss: 1237.429
[65,     1] loss: 1030.141
[66,     1] loss: 1088.328
[67,     1] loss: 999.149
[68,     1] loss: 885.312
[69,     1] loss: 1110.645
[70,     1] loss: 885.760
[71,     1] loss: 1032.106
[72,     1] loss: 828.944
[73,     1] loss: 1018.688
[74,     1] loss: 799.757
[75,     1] loss: 890.752
[76,     1] loss: 886.528
[77,     1] loss: 790.792
[78,     1] loss: 964.115
[79,     1] loss: 858.872
[80,     1] loss: 760.464
[81,     1] loss: 871.755
[82,     1] loss: 857.280
[83,     1] loss: 872.935
[84,     1] loss: 812.597
[85,     1] loss: 826.199
[86,     1] loss: 743.707
[87,     1] loss: 837.322
[88,     1] loss: 803.798
[89,     1] loss: 728.240
[90,     1] loss: 825.669
[91,     1] loss: 678.751
Early stopping applied (best metric=0.40676993131637573)
Finished Training
Total time taken: 14.494104862213135
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1819.812
[2,     1] loss: 1819.816
[3,     1] loss: 1813.900
[4,     1] loss: 1809.852
[5,     1] loss: 1815.197
[6,     1] loss: 1811.141
[7,     1] loss: 1805.446
[8,     1] loss: 1802.804
[9,     1] loss: 1810.460
[10,     1] loss: 1812.171
[11,     1] loss: 1810.357
[12,     1] loss: 1803.681
[13,     1] loss: 1804.658
[14,     1] loss: 1798.773
[15,     1] loss: 1800.779
[16,     1] loss: 1784.573
[17,     1] loss: 1770.047
[18,     1] loss: 1737.804
[19,     1] loss: 1695.768
[20,     1] loss: 1683.251
[21,     1] loss: 1609.086
[22,     1] loss: 1564.434
[23,     1] loss: 1563.577
[24,     1] loss: 1519.733
[25,     1] loss: 1566.265
[26,     1] loss: 1527.125
[27,     1] loss: 1539.525
[28,     1] loss: 1530.668
[29,     1] loss: 1501.754
[30,     1] loss: 1578.610
[31,     1] loss: 1513.115
[32,     1] loss: 1493.959
[33,     1] loss: 1463.035
[34,     1] loss: 1430.772
[35,     1] loss: 1406.894
[36,     1] loss: 1398.530
[37,     1] loss: 1366.922
[38,     1] loss: 1380.846
[39,     1] loss: 1354.129
[40,     1] loss: 1379.883
[41,     1] loss: 1291.534
[42,     1] loss: 1310.753
[43,     1] loss: 1309.231
[44,     1] loss: 1243.750
[45,     1] loss: 1211.580
[46,     1] loss: 1191.992
[47,     1] loss: 1192.062
[48,     1] loss: 1117.727
[49,     1] loss: 1108.084
[50,     1] loss: 1141.420
[51,     1] loss: 1219.554
[52,     1] loss: 1765.783
[53,     1] loss: 1115.312
[54,     1] loss: 1623.448
[55,     1] loss: 1232.644
[56,     1] loss: 1209.929
[57,     1] loss: 1413.565
[58,     1] loss: 1428.036
[59,     1] loss: 1405.588
[60,     1] loss: 1310.060
[61,     1] loss: 1343.775
[62,     1] loss: 1338.793
[63,     1] loss: 1289.660
[64,     1] loss: 1245.703
[65,     1] loss: 1193.314
[66,     1] loss: 1255.492
[67,     1] loss: 1158.735
[68,     1] loss: 1188.646
[69,     1] loss: 1115.948
[70,     1] loss: 1066.634
[71,     1] loss: 1044.477
[72,     1] loss: 987.697
[73,     1] loss: 1028.342
[74,     1] loss: 907.138
[75,     1] loss: 1008.049
[76,     1] loss: 917.823
[77,     1] loss: 917.234
[78,     1] loss: 919.885
[79,     1] loss: 905.863
[80,     1] loss: 940.810
[81,     1] loss: 861.813
[82,     1] loss: 830.795
[83,     1] loss: 1058.159
[84,     1] loss: 2004.308
[85,     1] loss: 908.212
[86,     1] loss: 1539.210
[87,     1] loss: 1120.073
[88,     1] loss: 1144.580
[89,     1] loss: 1409.458
[90,     1] loss: 1358.688
[91,     1] loss: 1258.199
[92,     1] loss: 1191.980
[93,     1] loss: 1154.516
[94,     1] loss: 1316.564
[95,     1] loss: 1195.487
Early stopping applied (best metric=0.3795172870159149)
Finished Training
Total time taken: 13.965880393981934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1809.014
[2,     1] loss: 1812.601
[3,     1] loss: 1811.611
[4,     1] loss: 1808.617
[5,     1] loss: 1808.103
[6,     1] loss: 1813.011
[7,     1] loss: 1816.163
[8,     1] loss: 1805.125
[9,     1] loss: 1803.979
[10,     1] loss: 1803.964
[11,     1] loss: 1801.552
[12,     1] loss: 1789.828
[13,     1] loss: 1789.065
[14,     1] loss: 1766.535
[15,     1] loss: 1723.207
[16,     1] loss: 1694.978
[17,     1] loss: 1665.688
[18,     1] loss: 1639.702
[19,     1] loss: 1563.094
[20,     1] loss: 1573.476
[21,     1] loss: 1599.387
[22,     1] loss: 1477.903
[23,     1] loss: 1521.133
[24,     1] loss: 1510.230
[25,     1] loss: 1534.262
[26,     1] loss: 1510.178
[27,     1] loss: 1476.277
[28,     1] loss: 1464.922
[29,     1] loss: 1404.119
[30,     1] loss: 1346.025
[31,     1] loss: 1410.870
[32,     1] loss: 1398.666
[33,     1] loss: 1388.510
[34,     1] loss: 1316.769
[35,     1] loss: 1284.632
[36,     1] loss: 1335.645
[37,     1] loss: 1347.266
[38,     1] loss: 1250.157
[39,     1] loss: 1234.579
[40,     1] loss: 1316.377
[41,     1] loss: 1265.922
[42,     1] loss: 1491.588
[43,     1] loss: 1211.949
[44,     1] loss: 1218.531
[45,     1] loss: 1294.365
[46,     1] loss: 1184.614
[47,     1] loss: 1278.586
[48,     1] loss: 1190.521
[49,     1] loss: 1216.099
[50,     1] loss: 1109.901
[51,     1] loss: 1094.130
[52,     1] loss: 1081.661
[53,     1] loss: 1175.945
[54,     1] loss: 1066.519
[55,     1] loss: 994.982
[56,     1] loss: 944.082
[57,     1] loss: 987.997
[58,     1] loss: 967.096
[59,     1] loss: 974.015
[60,     1] loss: 911.359
[61,     1] loss: 1225.110
[62,     1] loss: 1837.238
[63,     1] loss: 1152.322
[64,     1] loss: 1330.850
[65,     1] loss: 1068.673
[66,     1] loss: 1164.454
[67,     1] loss: 1288.339
[68,     1] loss: 1178.610
[69,     1] loss: 1146.032
[70,     1] loss: 1095.177
[71,     1] loss: 1099.755
[72,     1] loss: 1101.151
[73,     1] loss: 1087.680
[74,     1] loss: 1027.338
[75,     1] loss: 950.456
[76,     1] loss: 1000.953
[77,     1] loss: 956.641
[78,     1] loss: 927.568
[79,     1] loss: 984.183
[80,     1] loss: 914.723
[81,     1] loss: 888.755
[82,     1] loss: 857.750
[83,     1] loss: 957.614
[84,     1] loss: 888.651
[85,     1] loss: 809.115
[86,     1] loss: 800.926
[87,     1] loss: 815.749
[88,     1] loss: 754.336
[89,     1] loss: 782.388
[90,     1] loss: 813.129
[91,     1] loss: 796.796
[92,     1] loss: 797.309
[93,     1] loss: 883.883
[94,     1] loss: 782.186
[95,     1] loss: 829.284
[96,     1] loss: 848.519
[97,     1] loss: 994.142
[98,     1] loss: 1423.727
[99,     1] loss: 970.429
[100,     1] loss: 986.021
[101,     1] loss: 925.009
[102,     1] loss: 1066.442
[103,     1] loss: 881.555
[104,     1] loss: 1078.454
[105,     1] loss: 828.130
[106,     1] loss: 927.539
Early stopping applied (best metric=0.33346661925315857)
Finished Training
Total time taken: 15.536585092544556
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1825.303
[2,     1] loss: 1810.607
[3,     1] loss: 1804.614
[4,     1] loss: 1809.685
[5,     1] loss: 1813.632
[6,     1] loss: 1806.255
[7,     1] loss: 1809.201
[8,     1] loss: 1808.157
[9,     1] loss: 1803.615
[10,     1] loss: 1795.048
[11,     1] loss: 1801.062
[12,     1] loss: 1786.182
[13,     1] loss: 1773.655
[14,     1] loss: 1745.927
[15,     1] loss: 1710.127
[16,     1] loss: 1657.248
[17,     1] loss: 1629.763
[18,     1] loss: 1614.649
[19,     1] loss: 1625.666
[20,     1] loss: 1560.621
[21,     1] loss: 1571.189
[22,     1] loss: 1534.774
[23,     1] loss: 1458.868
[24,     1] loss: 1454.847
[25,     1] loss: 1498.551
[26,     1] loss: 1435.870
[27,     1] loss: 1470.706
[28,     1] loss: 1453.328
[29,     1] loss: 1407.108
[30,     1] loss: 1386.169
[31,     1] loss: 1348.882
[32,     1] loss: 1353.561
[33,     1] loss: 1275.043
[34,     1] loss: 1295.733
[35,     1] loss: 1190.765
[36,     1] loss: 1276.792
[37,     1] loss: 1205.341
[38,     1] loss: 1334.452
[39,     1] loss: 1151.425
[40,     1] loss: 1198.526
[41,     1] loss: 1212.784
[42,     1] loss: 1112.645
[43,     1] loss: 1267.279
[44,     1] loss: 1232.768
[45,     1] loss: 1138.526
[46,     1] loss: 1116.701
[47,     1] loss: 1265.177
[48,     1] loss: 1380.960
[49,     1] loss: 1083.876
[50,     1] loss: 1310.829
[51,     1] loss: 1159.513
[52,     1] loss: 1300.785
[53,     1] loss: 1154.302
[54,     1] loss: 1095.484
[55,     1] loss: 1227.803
[56,     1] loss: 1075.032
[57,     1] loss: 1165.212
[58,     1] loss: 1048.812
[59,     1] loss: 1067.031
[60,     1] loss: 1029.405
[61,     1] loss: 973.453
[62,     1] loss: 966.765
[63,     1] loss: 1147.564
[64,     1] loss: 1294.369
[65,     1] loss: 901.877
[66,     1] loss: 1251.836
[67,     1] loss: 1027.309
[68,     1] loss: 1164.584
[69,     1] loss: 1066.617
[70,     1] loss: 1017.859
[71,     1] loss: 1085.757
[72,     1] loss: 923.584
[73,     1] loss: 990.537
[74,     1] loss: 917.465
[75,     1] loss: 978.548
[76,     1] loss: 890.482
[77,     1] loss: 846.454
[78,     1] loss: 869.671
[79,     1] loss: 864.180
[80,     1] loss: 833.446
[81,     1] loss: 914.982
[82,     1] loss: 1281.750
[83,     1] loss: 974.331
[84,     1] loss: 935.072
[85,     1] loss: 968.392
[86,     1] loss: 893.056
[87,     1] loss: 885.992
[88,     1] loss: 830.491
[89,     1] loss: 880.494
[90,     1] loss: 778.903
[91,     1] loss: 802.097
[92,     1] loss: 827.447
[93,     1] loss: 778.217
[94,     1] loss: 776.859
[95,     1] loss: 844.906
[96,     1] loss: 732.250
[97,     1] loss: 759.780
[98,     1] loss: 813.909
[99,     1] loss: 804.325
[100,     1] loss: 885.680
[101,     1] loss: 944.574
[102,     1] loss: 832.291
[103,     1] loss: 805.942
[104,     1] loss: 1049.629
[105,     1] loss: 827.850
[106,     1] loss: 879.974
[107,     1] loss: 821.167
[108,     1] loss: 799.976
[109,     1] loss: 956.037
[110,     1] loss: 848.856
[111,     1] loss: 760.579
[112,     1] loss: 873.997
[113,     1] loss: 750.936
[114,     1] loss: 996.423
[115,     1] loss: 773.812
[116,     1] loss: 758.694
[117,     1] loss: 805.844
[118,     1] loss: 701.688
Early stopping applied (best metric=0.33663830161094666)
Finished Training
Total time taken: 17.455574989318848
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1810.480
[2,     1] loss: 1807.600
[3,     1] loss: 1819.148
[4,     1] loss: 1804.255
[5,     1] loss: 1815.420
[6,     1] loss: 1805.750
[7,     1] loss: 1795.494
[8,     1] loss: 1799.527
[9,     1] loss: 1793.377
[10,     1] loss: 1759.154
[11,     1] loss: 1712.714
[12,     1] loss: 1680.135
[13,     1] loss: 1610.331
[14,     1] loss: 1594.333
[15,     1] loss: 1568.411
[16,     1] loss: 1582.247
[17,     1] loss: 1503.157
[18,     1] loss: 1539.442
[19,     1] loss: 1486.599
[20,     1] loss: 1515.106
[21,     1] loss: 1482.806
[22,     1] loss: 1477.006
[23,     1] loss: 1441.594
[24,     1] loss: 1451.492
[25,     1] loss: 1421.556
[26,     1] loss: 1366.804
[27,     1] loss: 1409.426
[28,     1] loss: 1268.415
[29,     1] loss: 1471.748
[30,     1] loss: 1277.799
[31,     1] loss: 1344.933
[32,     1] loss: 1306.454
[33,     1] loss: 1325.180
[34,     1] loss: 1259.716
[35,     1] loss: 1283.684
[36,     1] loss: 1481.530
[37,     1] loss: 1271.372
[38,     1] loss: 1305.894
[39,     1] loss: 1272.856
[40,     1] loss: 1241.540
[41,     1] loss: 1231.075
[42,     1] loss: 1234.149
[43,     1] loss: 1139.674
[44,     1] loss: 1265.561
[45,     1] loss: 1080.898
[46,     1] loss: 1274.884
[47,     1] loss: 1121.955
[48,     1] loss: 1122.084
[49,     1] loss: 1164.490
[50,     1] loss: 1003.386
[51,     1] loss: 997.380
[52,     1] loss: 1045.272
[53,     1] loss: 981.018
[54,     1] loss: 1005.224
[55,     1] loss: 1056.929
[56,     1] loss: 1040.375
[57,     1] loss: 1120.433
[58,     1] loss: 923.841
[59,     1] loss: 977.974
[60,     1] loss: 1042.846
[61,     1] loss: 958.310
[62,     1] loss: 1050.866
[63,     1] loss: 995.494
[64,     1] loss: 883.865
[65,     1] loss: 934.652
[66,     1] loss: 848.202
[67,     1] loss: 837.422
[68,     1] loss: 900.123
[69,     1] loss: 923.962
[70,     1] loss: 870.187
Early stopping applied (best metric=0.4246850907802582)
Finished Training
Total time taken: 10.39353346824646
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1808.549
[2,     1] loss: 1806.822
[3,     1] loss: 1807.495
[4,     1] loss: 1809.518
[5,     1] loss: 1817.412
[6,     1] loss: 1811.542
[7,     1] loss: 1799.967
[8,     1] loss: 1810.535
[9,     1] loss: 1798.844
[10,     1] loss: 1795.322
[11,     1] loss: 1784.625
[12,     1] loss: 1763.197
[13,     1] loss: 1733.939
[14,     1] loss: 1691.842
[15,     1] loss: 1671.256
[16,     1] loss: 1642.255
[17,     1] loss: 1606.990
[18,     1] loss: 1622.512
[19,     1] loss: 1615.733
[20,     1] loss: 1562.816
[21,     1] loss: 1589.507
[22,     1] loss: 1561.339
[23,     1] loss: 1479.505
[24,     1] loss: 1517.795
[25,     1] loss: 1508.757
[26,     1] loss: 1484.605
[27,     1] loss: 1558.625
[28,     1] loss: 1453.602
[29,     1] loss: 1402.018
[30,     1] loss: 1436.588
[31,     1] loss: 1329.582
[32,     1] loss: 1344.694
[33,     1] loss: 1386.996
[34,     1] loss: 1382.599
[35,     1] loss: 1345.210
[36,     1] loss: 1314.079
[37,     1] loss: 1258.450
[38,     1] loss: 1296.505
[39,     1] loss: 1231.934
[40,     1] loss: 1178.600
[41,     1] loss: 1240.847
[42,     1] loss: 1214.373
[43,     1] loss: 1094.920
[44,     1] loss: 1163.678
[45,     1] loss: 1628.673
[46,     1] loss: 1203.344
[47,     1] loss: 1438.374
[48,     1] loss: 1156.843
[49,     1] loss: 1364.837
[50,     1] loss: 1337.693
[51,     1] loss: 1203.214
[52,     1] loss: 1225.993
[53,     1] loss: 1225.580
[54,     1] loss: 1264.429
[55,     1] loss: 1134.641
[56,     1] loss: 1166.844
[57,     1] loss: 1054.433
[58,     1] loss: 1054.533
[59,     1] loss: 1199.179
[60,     1] loss: 997.092
[61,     1] loss: 1121.126
[62,     1] loss: 1127.276
[63,     1] loss: 1096.829
[64,     1] loss: 1060.118
[65,     1] loss: 999.778
[66,     1] loss: 1149.728
[67,     1] loss: 954.619
[68,     1] loss: 983.076
[69,     1] loss: 887.870
[70,     1] loss: 1079.154
[71,     1] loss: 930.960
[72,     1] loss: 815.218
[73,     1] loss: 910.833
[74,     1] loss: 768.024
[75,     1] loss: 899.303
[76,     1] loss: 817.909
[77,     1] loss: 899.445
[78,     1] loss: 956.055
[79,     1] loss: 758.405
[80,     1] loss: 885.457
[81,     1] loss: 1227.217
[82,     1] loss: 764.645
[83,     1] loss: 1159.922
[84,     1] loss: 844.566
[85,     1] loss: 1076.907
[86,     1] loss: 1028.368
[87,     1] loss: 1009.906
[88,     1] loss: 896.488
[89,     1] loss: 970.133
[90,     1] loss: 987.852
[91,     1] loss: 819.293
[92,     1] loss: 776.812
[93,     1] loss: 814.634
Early stopping applied (best metric=0.3504236936569214)
Finished Training
Total time taken: 13.543107271194458
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1808.218
[2,     1] loss: 1812.399
[3,     1] loss: 1807.403
[4,     1] loss: 1808.749
[5,     1] loss: 1803.829
[6,     1] loss: 1807.392
[7,     1] loss: 1813.131
[8,     1] loss: 1804.165
[9,     1] loss: 1795.918
[10,     1] loss: 1789.411
[11,     1] loss: 1773.213
[12,     1] loss: 1755.229
[13,     1] loss: 1707.340
[14,     1] loss: 1665.760
[15,     1] loss: 1637.237
[16,     1] loss: 1595.955
[17,     1] loss: 1502.107
[18,     1] loss: 1573.823
[19,     1] loss: 1523.167
[20,     1] loss: 1504.809
[21,     1] loss: 1513.928
[22,     1] loss: 1559.978
[23,     1] loss: 1427.584
[24,     1] loss: 1420.207
[25,     1] loss: 1410.025
[26,     1] loss: 1472.605
[27,     1] loss: 1396.814
[28,     1] loss: 1397.919
[29,     1] loss: 1407.832
[30,     1] loss: 1359.342
[31,     1] loss: 1330.242
[32,     1] loss: 1310.214
[33,     1] loss: 1330.412
[34,     1] loss: 1347.413
[35,     1] loss: 1314.938
[36,     1] loss: 1291.936
[37,     1] loss: 1567.115
[38,     1] loss: 1184.073
[39,     1] loss: 1353.911
[40,     1] loss: 1268.542
[41,     1] loss: 1315.441
[42,     1] loss: 1258.905
[43,     1] loss: 1227.161
[44,     1] loss: 1166.383
[45,     1] loss: 1213.530
[46,     1] loss: 1120.139
[47,     1] loss: 1090.187
[48,     1] loss: 1097.568
[49,     1] loss: 1124.850
[50,     1] loss: 1119.181
[51,     1] loss: 1276.070
[52,     1] loss: 1078.212
[53,     1] loss: 1052.289
[54,     1] loss: 1024.465
[55,     1] loss: 1046.245
[56,     1] loss: 1044.984
[57,     1] loss: 996.064
[58,     1] loss: 1002.388
[59,     1] loss: 920.188
[60,     1] loss: 901.234
[61,     1] loss: 1106.144
[62,     1] loss: 1444.016
[63,     1] loss: 1405.587
[64,     1] loss: 1022.245
[65,     1] loss: 1177.168
[66,     1] loss: 1197.876
[67,     1] loss: 1049.524
[68,     1] loss: 1215.564
[69,     1] loss: 1108.759
[70,     1] loss: 1043.763
[71,     1] loss: 1031.142
Early stopping applied (best metric=0.37996330857276917)
Finished Training
Total time taken: 10.979049444198608
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1814.196
[2,     1] loss: 1808.201
[3,     1] loss: 1816.158
[4,     1] loss: 1815.671
[5,     1] loss: 1807.832
[6,     1] loss: 1802.899
[7,     1] loss: 1796.937
[8,     1] loss: 1797.812
[9,     1] loss: 1816.530
[10,     1] loss: 1806.036
[11,     1] loss: 1791.911
[12,     1] loss: 1781.461
[13,     1] loss: 1763.710
[14,     1] loss: 1724.865
[15,     1] loss: 1721.983
[16,     1] loss: 1688.000
[17,     1] loss: 1604.546
[18,     1] loss: 1610.133
[19,     1] loss: 1614.574
[20,     1] loss: 1622.950
[21,     1] loss: 1661.124
[22,     1] loss: 1547.147
[23,     1] loss: 1597.173
[24,     1] loss: 1549.515
[25,     1] loss: 1528.520
[26,     1] loss: 1496.759
[27,     1] loss: 1474.728
[28,     1] loss: 1548.162
[29,     1] loss: 1421.158
[30,     1] loss: 1446.355
[31,     1] loss: 1435.763
[32,     1] loss: 1431.226
[33,     1] loss: 1418.261
[34,     1] loss: 1387.357
[35,     1] loss: 1325.320
[36,     1] loss: 1358.892
[37,     1] loss: 1315.630
[38,     1] loss: 1230.196
[39,     1] loss: 1309.943
[40,     1] loss: 1315.443
[41,     1] loss: 1287.116
[42,     1] loss: 1238.259
[43,     1] loss: 1230.333
[44,     1] loss: 1249.869
[45,     1] loss: 1158.847
[46,     1] loss: 1297.285
[47,     1] loss: 1331.801
[48,     1] loss: 1134.750
[49,     1] loss: 1252.576
[50,     1] loss: 1129.468
[51,     1] loss: 1114.341
[52,     1] loss: 1108.560
[53,     1] loss: 1121.592
[54,     1] loss: 1040.271
[55,     1] loss: 1049.834
[56,     1] loss: 1006.163
[57,     1] loss: 990.652
[58,     1] loss: 1001.829
[59,     1] loss: 912.313
[60,     1] loss: 1027.433
[61,     1] loss: 1139.731
[62,     1] loss: 913.169
[63,     1] loss: 916.937
[64,     1] loss: 980.503
[65,     1] loss: 948.972
[66,     1] loss: 828.592
[67,     1] loss: 867.062
[68,     1] loss: 825.992
[69,     1] loss: 860.888
[70,     1] loss: 1096.146
[71,     1] loss: 974.391
[72,     1] loss: 806.704
[73,     1] loss: 751.626
[74,     1] loss: 830.593
[75,     1] loss: 751.504
[76,     1] loss: 788.021
[77,     1] loss: 742.036
[78,     1] loss: 728.334
[79,     1] loss: 756.201
[80,     1] loss: 727.150
[81,     1] loss: 753.043
[82,     1] loss: 779.893
[83,     1] loss: 834.471
[84,     1] loss: 774.758
[85,     1] loss: 727.394
[86,     1] loss: 712.882
[87,     1] loss: 714.646
[88,     1] loss: 820.141
[89,     1] loss: 995.918
Early stopping applied (best metric=0.366951584815979)
Finished Training
Total time taken: 14.067352294921875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1810.167
[2,     1] loss: 1820.509
[3,     1] loss: 1809.665
[4,     1] loss: 1799.972
[5,     1] loss: 1812.505
[6,     1] loss: 1799.889
[7,     1] loss: 1801.373
[8,     1] loss: 1803.528
[9,     1] loss: 1798.434
[10,     1] loss: 1774.240
[11,     1] loss: 1762.210
[12,     1] loss: 1712.263
[13,     1] loss: 1683.302
[14,     1] loss: 1635.809
[15,     1] loss: 1623.188
[16,     1] loss: 1561.109
[17,     1] loss: 1469.955
[18,     1] loss: 1507.807
[19,     1] loss: 1527.722
[20,     1] loss: 1538.979
[21,     1] loss: 1440.171
[22,     1] loss: 1453.677
[23,     1] loss: 1423.863
[24,     1] loss: 1443.561
[25,     1] loss: 1477.508
[26,     1] loss: 1367.054
[27,     1] loss: 1359.445
[28,     1] loss: 1266.742
[29,     1] loss: 1356.343
[30,     1] loss: 1312.646
[31,     1] loss: 1256.935
[32,     1] loss: 1304.781
[33,     1] loss: 1315.173
[34,     1] loss: 1175.216
[35,     1] loss: 1198.995
[36,     1] loss: 1170.364
[37,     1] loss: 1162.430
[38,     1] loss: 1210.884
[39,     1] loss: 1145.552
[40,     1] loss: 1144.741
[41,     1] loss: 1178.118
[42,     1] loss: 1465.654
[43,     1] loss: 1543.360
[44,     1] loss: 1135.983
[45,     1] loss: 1252.951
[46,     1] loss: 1178.058
[47,     1] loss: 1177.219
[48,     1] loss: 1264.277
[49,     1] loss: 1163.751
[50,     1] loss: 1105.945
[51,     1] loss: 1215.153
[52,     1] loss: 1089.048
[53,     1] loss: 1072.359
[54,     1] loss: 1050.201
[55,     1] loss: 978.861
[56,     1] loss: 1077.341
[57,     1] loss: 969.664
[58,     1] loss: 979.291
[59,     1] loss: 943.095
[60,     1] loss: 960.275
[61,     1] loss: 849.904
[62,     1] loss: 891.819
[63,     1] loss: 909.011
[64,     1] loss: 891.508
[65,     1] loss: 848.410
[66,     1] loss: 1100.653
[67,     1] loss: 1905.006
[68,     1] loss: 1119.354
[69,     1] loss: 1109.385
[70,     1] loss: 1035.788
[71,     1] loss: 1171.005
[72,     1] loss: 1223.047
[73,     1] loss: 1083.522
[74,     1] loss: 1103.923
[75,     1] loss: 1149.115
[76,     1] loss: 993.752
[77,     1] loss: 918.121
[78,     1] loss: 1049.205
[79,     1] loss: 1010.887
[80,     1] loss: 975.597
[81,     1] loss: 992.503
[82,     1] loss: 888.390
[83,     1] loss: 990.277
[84,     1] loss: 888.605
[85,     1] loss: 862.749
[86,     1] loss: 780.870
[87,     1] loss: 860.500
[88,     1] loss: 817.209
[89,     1] loss: 855.841
[90,     1] loss: 823.874
[91,     1] loss: 831.250
[92,     1] loss: 741.563
[93,     1] loss: 758.229
[94,     1] loss: 670.270
[95,     1] loss: 762.162
[96,     1] loss: 833.511
[97,     1] loss: 676.724
[98,     1] loss: 811.102
[99,     1] loss: 688.325
[100,     1] loss: 756.985
[101,     1] loss: 798.431
Early stopping applied (best metric=0.3425179719924927)
Finished Training
Total time taken: 15.718871355056763
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1826.204
[2,     1] loss: 1818.001
[3,     1] loss: 1814.650
[4,     1] loss: 1812.047
[5,     1] loss: 1810.562
[6,     1] loss: 1811.605
[7,     1] loss: 1810.672
[8,     1] loss: 1804.712
[9,     1] loss: 1815.304
[10,     1] loss: 1812.942
[11,     1] loss: 1810.278
[12,     1] loss: 1809.643
[13,     1] loss: 1802.709
[14,     1] loss: 1802.480
[15,     1] loss: 1793.452
[16,     1] loss: 1774.614
[17,     1] loss: 1751.395
[18,     1] loss: 1740.016
[19,     1] loss: 1710.885
[20,     1] loss: 1668.132
[21,     1] loss: 1645.657
[22,     1] loss: 1650.949
[23,     1] loss: 1640.452
[24,     1] loss: 1608.542
[25,     1] loss: 1599.895
[26,     1] loss: 1546.460
[27,     1] loss: 1528.810
[28,     1] loss: 1495.345
[29,     1] loss: 1518.741
[30,     1] loss: 1476.521
[31,     1] loss: 1398.049
[32,     1] loss: 1415.205
[33,     1] loss: 1427.628
[34,     1] loss: 1493.595
[35,     1] loss: 1353.650
[36,     1] loss: 1326.046
[37,     1] loss: 1287.370
[38,     1] loss: 1356.937
[39,     1] loss: 1244.450
[40,     1] loss: 1273.254
[41,     1] loss: 1295.688
[42,     1] loss: 1209.102
[43,     1] loss: 1210.303
[44,     1] loss: 1207.205
[45,     1] loss: 1453.633
[46,     1] loss: 1460.270
[47,     1] loss: 1258.252
[48,     1] loss: 1309.428
[49,     1] loss: 1237.954
[50,     1] loss: 1233.634
[51,     1] loss: 1316.977
[52,     1] loss: 1196.745
[53,     1] loss: 1167.371
[54,     1] loss: 1398.720
[55,     1] loss: 1146.340
[56,     1] loss: 1357.120
[57,     1] loss: 1151.125
[58,     1] loss: 1337.383
[59,     1] loss: 1045.733
[60,     1] loss: 1137.667
[61,     1] loss: 1105.355
[62,     1] loss: 1109.234
[63,     1] loss: 1166.253
[64,     1] loss: 1049.436
[65,     1] loss: 1037.846
[66,     1] loss: 1016.185
[67,     1] loss: 1104.716
[68,     1] loss: 908.007
[69,     1] loss: 1053.080
[70,     1] loss: 976.640
[71,     1] loss: 929.327
[72,     1] loss: 1052.626
[73,     1] loss: 857.240
[74,     1] loss: 1184.799
[75,     1] loss: 1384.236
[76,     1] loss: 853.700
[77,     1] loss: 1092.823
[78,     1] loss: 1037.226
[79,     1] loss: 1080.436
[80,     1] loss: 1081.703
[81,     1] loss: 987.219
Early stopping applied (best metric=0.42642131447792053)
Finished Training
Total time taken: 12.189562559127808
{'Hydroxylation-K Validation Accuracy': 0.7298463356973995, 'Hydroxylation-K Validation Sensitivity': 0.7037037037037037, 'Hydroxylation-K Validation Specificity': 0.7350877192982456, 'Hydroxylation-K Validation Precision': 0.4035278177847837, 'Hydroxylation-K AUC ROC': 0.811345029239766, 'Hydroxylation-K AUC PR': 0.5697127268724255, 'Hydroxylation-K MCC': 0.3712052366262017, 'Hydroxylation-K F1': 0.5082075102134083, 'Validation Loss (Hydroxylation-K)': 0.4540694713592529, 'Hydroxylation-P Validation Accuracy': 0.7761986193594234, 'Hydroxylation-P Validation Sensitivity': 0.8085714285714286, 'Hydroxylation-P Validation Specificity': 0.7692303855553893, 'Hydroxylation-P Validation Precision': 0.43393545217389584, 'Hydroxylation-P AUC ROC': 0.8379804993638601, 'Hydroxylation-P AUC PR': 0.5690570585978652, 'Hydroxylation-P MCC': 0.47036718684137463, 'Hydroxylation-P F1': 0.5633666858978217, 'Validation Loss (Hydroxylation-P)': 0.3768216172854106, 'Validation Loss (total)': 0.8308910886446635, 'TimeToTrain': 13.70448637008667}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007444574573100147,
 'learning_rate_Hydroxylation-K': 0.00818489787097535,
 'learning_rate_Hydroxylation-P': 0.0066248294466017855,
 'log_base': 1.928974496785201,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 703074862,
 'sample_weights': [4.280847859002224, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2133449468021347,
 'weight_decay_Hydroxylation-K': 2.2823852369042106,
 'weight_decay_Hydroxylation-P': 6.878227942369817}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1450.801
[2,     1] loss: 1456.912
[3,     1] loss: 1443.759
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0043160535810220476,
 'learning_rate_Hydroxylation-K': 0.009836618686515537,
 'learning_rate_Hydroxylation-P': 0.005877131126829445,
 'log_base': 1.7495493639904907,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3214475256,
 'sample_weights': [2.5410537850847037, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.820265440158422,
 'weight_decay_Hydroxylation-K': 7.837920316762648,
 'weight_decay_Hydroxylation-P': 2.423705175904587}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1537.798
[2,     1] loss: 1530.448
[3,     1] loss: 1546.819
[4,     1] loss: 1526.643
[5,     1] loss: 1539.145
[6,     1] loss: 1539.183
[7,     1] loss: 1533.240
[8,     1] loss: 1518.353
[9,     1] loss: 1514.521
[10,     1] loss: 1505.211
[11,     1] loss: 1484.304
[12,     1] loss: 1462.554
[13,     1] loss: 1423.109
[14,     1] loss: 1375.819
[15,     1] loss: 1358.418
[16,     1] loss: 1336.857
[17,     1] loss: 1287.812
[18,     1] loss: 1350.723
[19,     1] loss: 1259.380
[20,     1] loss: 1322.395
[21,     1] loss: 1256.951
[22,     1] loss: 1267.063
[23,     1] loss: 1217.697
[24,     1] loss: 1282.238
[25,     1] loss: 1243.941
[26,     1] loss: 1218.251
[27,     1] loss: 1235.541
[28,     1] loss: 1190.275
[29,     1] loss: 1200.166
[30,     1] loss: 1178.347
[31,     1] loss: 1182.359
[32,     1] loss: 1166.213
[33,     1] loss: 1163.380
[34,     1] loss: 1115.040
[35,     1] loss: 1220.516
[36,     1] loss: 1143.566
[37,     1] loss: 1077.052
[38,     1] loss: 1106.732
[39,     1] loss: 1081.591
[40,     1] loss: 1081.248
[41,     1] loss: 1122.081
[42,     1] loss: 1069.963
[43,     1] loss: 1115.326
[44,     1] loss: 1180.728
[45,     1] loss: 1057.900
[46,     1] loss: 1142.682
[47,     1] loss: 1070.969
[48,     1] loss: 1095.585
[49,     1] loss: 1023.454
[50,     1] loss: 1045.127
[51,     1] loss: 1002.476
[52,     1] loss: 948.627
[53,     1] loss: 1039.523
[54,     1] loss: 1002.294
[55,     1] loss: 969.695
[56,     1] loss: 975.401
[57,     1] loss: 1000.869
[58,     1] loss: 917.489
[59,     1] loss: 903.577
[60,     1] loss: 959.476
[61,     1] loss: 920.636
[62,     1] loss: 909.936
[63,     1] loss: 922.575
[64,     1] loss: 994.499
[65,     1] loss: 905.590
[66,     1] loss: 912.135
[67,     1] loss: 867.974
[68,     1] loss: 891.789
[69,     1] loss: 945.945
[70,     1] loss: 800.978
[71,     1] loss: 860.137
[72,     1] loss: 808.391
[73,     1] loss: 784.223
[74,     1] loss: 855.324
[75,     1] loss: 799.018
[76,     1] loss: 839.053
[77,     1] loss: 834.276
[78,     1] loss: 813.058
[79,     1] loss: 743.302
[80,     1] loss: 739.739
[81,     1] loss: 762.546
[82,     1] loss: 776.026
[83,     1] loss: 758.836
[84,     1] loss: 728.524
[85,     1] loss: 758.184
[86,     1] loss: 666.834
[87,     1] loss: 667.916
[88,     1] loss: 673.467
[89,     1] loss: 602.425
[90,     1] loss: 655.104
[91,     1] loss: 647.142
[92,     1] loss: 678.230
[93,     1] loss: 732.494
[94,     1] loss: 647.710
[95,     1] loss: 695.632
[96,     1] loss: 676.026
[97,     1] loss: 685.282
[98,     1] loss: 658.952
[99,     1] loss: 664.474
Early stopping applied (best metric=0.33031371235847473)
Finished Training
Total time taken: 14.128250360488892
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1541.408
[2,     1] loss: 1534.029
[3,     1] loss: 1540.665
[4,     1] loss: 1541.271
[5,     1] loss: 1535.085
[6,     1] loss: 1540.220
[7,     1] loss: 1536.873
[8,     1] loss: 1532.878
[9,     1] loss: 1538.703
[10,     1] loss: 1530.915
[11,     1] loss: 1531.901
[12,     1] loss: 1524.935
[13,     1] loss: 1529.592
[14,     1] loss: 1519.330
[15,     1] loss: 1505.041
[16,     1] loss: 1501.873
[17,     1] loss: 1485.047
[18,     1] loss: 1444.443
[19,     1] loss: 1417.125
[20,     1] loss: 1379.117
[21,     1] loss: 1384.955
[22,     1] loss: 1349.201
[23,     1] loss: 1329.781
[24,     1] loss: 1353.984
[25,     1] loss: 1301.032
[26,     1] loss: 1282.297
[27,     1] loss: 1276.586
[28,     1] loss: 1284.709
[29,     1] loss: 1277.635
[30,     1] loss: 1317.284
[31,     1] loss: 1262.852
[32,     1] loss: 1266.975
[33,     1] loss: 1257.734
[34,     1] loss: 1237.314
[35,     1] loss: 1234.284
[36,     1] loss: 1184.061
[37,     1] loss: 1166.530
[38,     1] loss: 1180.514
[39,     1] loss: 1171.420
[40,     1] loss: 1171.333
[41,     1] loss: 1225.219
[42,     1] loss: 1152.321
[43,     1] loss: 1193.179
[44,     1] loss: 1111.624
[45,     1] loss: 1134.610
[46,     1] loss: 1116.987
[47,     1] loss: 1159.010
[48,     1] loss: 1122.959
[49,     1] loss: 1124.398
[50,     1] loss: 1024.969
[51,     1] loss: 1052.516
[52,     1] loss: 1083.058
[53,     1] loss: 1116.906
[54,     1] loss: 1090.884
[55,     1] loss: 1085.746
[56,     1] loss: 1009.806
[57,     1] loss: 1011.727
[58,     1] loss: 1030.647
[59,     1] loss: 1056.908
[60,     1] loss: 993.020
[61,     1] loss: 970.180
[62,     1] loss: 964.179
[63,     1] loss: 1064.294
[64,     1] loss: 1020.064
[65,     1] loss: 995.629
[66,     1] loss: 966.867
[67,     1] loss: 1033.866
[68,     1] loss: 996.851
[69,     1] loss: 951.902
[70,     1] loss: 975.309
[71,     1] loss: 1018.703
[72,     1] loss: 963.723
[73,     1] loss: 903.407
[74,     1] loss: 964.573
[75,     1] loss: 924.078
[76,     1] loss: 878.224
[77,     1] loss: 933.061
[78,     1] loss: 883.677
[79,     1] loss: 901.866
[80,     1] loss: 878.373
[81,     1] loss: 843.772
[82,     1] loss: 766.946
[83,     1] loss: 876.212
[84,     1] loss: 838.229
[85,     1] loss: 898.859
[86,     1] loss: 872.353
[87,     1] loss: 766.604
Early stopping applied (best metric=0.4056171774864197)
Finished Training
Total time taken: 12.532151699066162
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1538.360
[2,     1] loss: 1535.062
[3,     1] loss: 1538.872
[4,     1] loss: 1538.866
[5,     1] loss: 1535.473
[6,     1] loss: 1541.641
[7,     1] loss: 1538.624
[8,     1] loss: 1532.793
[9,     1] loss: 1525.697
[10,     1] loss: 1525.422
[11,     1] loss: 1517.227
[12,     1] loss: 1505.035
[13,     1] loss: 1499.309
[14,     1] loss: 1475.650
[15,     1] loss: 1446.774
[16,     1] loss: 1408.514
[17,     1] loss: 1362.194
[18,     1] loss: 1331.895
[19,     1] loss: 1334.671
[20,     1] loss: 1312.442
[21,     1] loss: 1252.947
[22,     1] loss: 1288.704
[23,     1] loss: 1304.001
[24,     1] loss: 1374.238
[25,     1] loss: 1300.062
[26,     1] loss: 1247.513
[27,     1] loss: 1238.690
[28,     1] loss: 1279.825
[29,     1] loss: 1226.565
[30,     1] loss: 1255.253
[31,     1] loss: 1232.858
[32,     1] loss: 1205.170
[33,     1] loss: 1243.177
[34,     1] loss: 1180.708
[35,     1] loss: 1199.781
[36,     1] loss: 1193.826
[37,     1] loss: 1085.079
[38,     1] loss: 1196.285
[39,     1] loss: 1173.543
[40,     1] loss: 1185.954
[41,     1] loss: 1127.918
[42,     1] loss: 1171.434
[43,     1] loss: 1053.688
[44,     1] loss: 1120.959
[45,     1] loss: 1056.194
[46,     1] loss: 1091.870
[47,     1] loss: 1084.299
[48,     1] loss: 1159.738
[49,     1] loss: 1085.100
[50,     1] loss: 1068.465
[51,     1] loss: 1059.534
[52,     1] loss: 1070.579
[53,     1] loss: 1021.292
[54,     1] loss: 1045.436
[55,     1] loss: 1063.198
[56,     1] loss: 1025.699
[57,     1] loss: 1078.557
[58,     1] loss: 1065.263
[59,     1] loss: 1067.572
[60,     1] loss: 1046.897
[61,     1] loss: 1039.882
[62,     1] loss: 1052.062
[63,     1] loss: 977.617
[64,     1] loss: 1029.129
[65,     1] loss: 1002.664
[66,     1] loss: 1046.178
[67,     1] loss: 925.314
[68,     1] loss: 974.198
[69,     1] loss: 875.706
[70,     1] loss: 951.037
[71,     1] loss: 909.938
[72,     1] loss: 882.032
[73,     1] loss: 895.880
[74,     1] loss: 845.420
[75,     1] loss: 848.082
Early stopping applied (best metric=0.401502788066864)
Finished Training
Total time taken: 11.172016382217407
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1539.857
[2,     1] loss: 1545.816
[3,     1] loss: 1537.015
[4,     1] loss: 1535.237
[5,     1] loss: 1535.667
[6,     1] loss: 1535.867
[7,     1] loss: 1529.934
[8,     1] loss: 1533.675
[9,     1] loss: 1529.463
[10,     1] loss: 1515.292
[11,     1] loss: 1504.674
[12,     1] loss: 1481.360
[13,     1] loss: 1454.551
[14,     1] loss: 1423.663
[15,     1] loss: 1402.310
[16,     1] loss: 1342.223
[17,     1] loss: 1326.095
[18,     1] loss: 1326.052
[19,     1] loss: 1307.363
[20,     1] loss: 1279.945
[21,     1] loss: 1314.016
[22,     1] loss: 1266.403
[23,     1] loss: 1244.908
[24,     1] loss: 1255.766
[25,     1] loss: 1273.528
[26,     1] loss: 1284.306
[27,     1] loss: 1236.960
[28,     1] loss: 1247.732
[29,     1] loss: 1200.341
[30,     1] loss: 1141.777
[31,     1] loss: 1218.717
[32,     1] loss: 1263.854
[33,     1] loss: 1168.018
[34,     1] loss: 1168.403
[35,     1] loss: 1247.778
[36,     1] loss: 1138.551
[37,     1] loss: 1182.182
[38,     1] loss: 1242.931
[39,     1] loss: 1124.308
[40,     1] loss: 1169.314
[41,     1] loss: 1164.113
[42,     1] loss: 1116.179
[43,     1] loss: 1074.757
[44,     1] loss: 1086.120
[45,     1] loss: 1084.538
[46,     1] loss: 1101.509
[47,     1] loss: 1098.003
[48,     1] loss: 987.703
[49,     1] loss: 1063.089
[50,     1] loss: 1060.539
[51,     1] loss: 1011.593
[52,     1] loss: 1015.389
[53,     1] loss: 1062.873
[54,     1] loss: 1008.118
[55,     1] loss: 1049.298
[56,     1] loss: 987.050
[57,     1] loss: 1020.110
[58,     1] loss: 974.409
[59,     1] loss: 972.709
[60,     1] loss: 996.075
[61,     1] loss: 992.215
[62,     1] loss: 968.888
[63,     1] loss: 972.805
[64,     1] loss: 973.729
[65,     1] loss: 950.207
[66,     1] loss: 973.768
[67,     1] loss: 942.797
[68,     1] loss: 889.069
[69,     1] loss: 968.610
[70,     1] loss: 882.233
[71,     1] loss: 955.080
[72,     1] loss: 912.187
[73,     1] loss: 971.115
Early stopping applied (best metric=0.367612361907959)
Finished Training
Total time taken: 10.671529531478882
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1542.819
[2,     1] loss: 1537.200
[3,     1] loss: 1540.286
[4,     1] loss: 1539.180
[5,     1] loss: 1537.004
[6,     1] loss: 1532.221
[7,     1] loss: 1536.204
[8,     1] loss: 1530.927
[9,     1] loss: 1542.462
[10,     1] loss: 1528.874
[11,     1] loss: 1530.978
[12,     1] loss: 1531.412
[13,     1] loss: 1526.659
[14,     1] loss: 1518.638
[15,     1] loss: 1513.336
[16,     1] loss: 1488.318
[17,     1] loss: 1478.034
[18,     1] loss: 1448.945
[19,     1] loss: 1419.803
[20,     1] loss: 1397.472
[21,     1] loss: 1349.063
[22,     1] loss: 1355.580
[23,     1] loss: 1367.032
[24,     1] loss: 1350.788
[25,     1] loss: 1260.309
[26,     1] loss: 1336.813
[27,     1] loss: 1320.473
[28,     1] loss: 1287.593
[29,     1] loss: 1235.276
[30,     1] loss: 1245.928
[31,     1] loss: 1292.494
[32,     1] loss: 1267.788
[33,     1] loss: 1245.681
[34,     1] loss: 1251.729
[35,     1] loss: 1223.981
[36,     1] loss: 1168.198
[37,     1] loss: 1187.468
[38,     1] loss: 1218.974
[39,     1] loss: 1231.040
[40,     1] loss: 1193.572
[41,     1] loss: 1170.975
[42,     1] loss: 1126.606
[43,     1] loss: 1132.195
[44,     1] loss: 1138.668
[45,     1] loss: 1140.673
[46,     1] loss: 1151.869
[47,     1] loss: 1184.553
[48,     1] loss: 1096.845
[49,     1] loss: 1081.047
[50,     1] loss: 1120.969
[51,     1] loss: 1093.231
[52,     1] loss: 1130.906
[53,     1] loss: 1078.100
[54,     1] loss: 1123.155
[55,     1] loss: 1132.495
[56,     1] loss: 1082.363
[57,     1] loss: 1075.646
[58,     1] loss: 992.837
[59,     1] loss: 1016.260
[60,     1] loss: 1017.471
[61,     1] loss: 1033.201
[62,     1] loss: 981.276
[63,     1] loss: 971.390
[64,     1] loss: 1063.236
[65,     1] loss: 974.913
[66,     1] loss: 1082.534
[67,     1] loss: 986.170
[68,     1] loss: 970.623
[69,     1] loss: 961.474
[70,     1] loss: 957.536
[71,     1] loss: 913.887
[72,     1] loss: 980.961
[73,     1] loss: 911.913
[74,     1] loss: 898.613
[75,     1] loss: 888.970
[76,     1] loss: 950.310
[77,     1] loss: 884.051
[78,     1] loss: 856.281
[79,     1] loss: 900.629
[80,     1] loss: 807.805
[81,     1] loss: 847.875
[82,     1] loss: 818.878
[83,     1] loss: 828.857
[84,     1] loss: 879.946
[85,     1] loss: 796.560
[86,     1] loss: 919.854
[87,     1] loss: 794.068
[88,     1] loss: 874.184
[89,     1] loss: 797.305
[90,     1] loss: 902.890
[91,     1] loss: 789.139
Early stopping applied (best metric=0.41381359100341797)
Finished Training
Total time taken: 13.334097385406494
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1543.619
[2,     1] loss: 1539.601
[3,     1] loss: 1538.044
[4,     1] loss: 1537.094
[5,     1] loss: 1533.507
[6,     1] loss: 1537.328
[7,     1] loss: 1526.282
[8,     1] loss: 1521.375
[9,     1] loss: 1516.874
[10,     1] loss: 1501.175
[11,     1] loss: 1490.228
[12,     1] loss: 1448.136
[13,     1] loss: 1420.849
[14,     1] loss: 1417.531
[15,     1] loss: 1335.910
[16,     1] loss: 1321.336
[17,     1] loss: 1308.658
[18,     1] loss: 1284.720
[19,     1] loss: 1304.489
[20,     1] loss: 1338.245
[21,     1] loss: 1273.373
[22,     1] loss: 1241.171
[23,     1] loss: 1242.642
[24,     1] loss: 1309.772
[25,     1] loss: 1304.935
[26,     1] loss: 1236.808
[27,     1] loss: 1271.799
[28,     1] loss: 1228.810
[29,     1] loss: 1205.097
[30,     1] loss: 1178.851
[31,     1] loss: 1213.180
[32,     1] loss: 1189.192
[33,     1] loss: 1171.182
[34,     1] loss: 1155.455
[35,     1] loss: 1141.909
[36,     1] loss: 1213.937
[37,     1] loss: 1171.504
[38,     1] loss: 1132.000
[39,     1] loss: 1116.645
[40,     1] loss: 1101.702
[41,     1] loss: 1188.659
[42,     1] loss: 1116.477
[43,     1] loss: 1085.238
[44,     1] loss: 1057.202
[45,     1] loss: 1121.295
[46,     1] loss: 1073.236
[47,     1] loss: 1079.171
[48,     1] loss: 1061.408
[49,     1] loss: 1041.084
[50,     1] loss: 1015.529
[51,     1] loss: 996.238
[52,     1] loss: 1082.348
[53,     1] loss: 1025.519
[54,     1] loss: 1009.228
[55,     1] loss: 993.788
[56,     1] loss: 949.194
[57,     1] loss: 946.939
[58,     1] loss: 899.809
[59,     1] loss: 949.971
[60,     1] loss: 916.208
[61,     1] loss: 892.315
[62,     1] loss: 902.662
[63,     1] loss: 993.410
[64,     1] loss: 950.746
[65,     1] loss: 890.736
[66,     1] loss: 888.702
[67,     1] loss: 933.894
[68,     1] loss: 864.722
[69,     1] loss: 855.367
[70,     1] loss: 842.964
[71,     1] loss: 878.271
[72,     1] loss: 870.626
[73,     1] loss: 852.608
[74,     1] loss: 795.363
[75,     1] loss: 826.960
[76,     1] loss: 921.795
[77,     1] loss: 785.932
[78,     1] loss: 845.734
[79,     1] loss: 785.010
[80,     1] loss: 786.507
[81,     1] loss: 794.444
[82,     1] loss: 812.580
[83,     1] loss: 771.798
[84,     1] loss: 794.350
[85,     1] loss: 763.337
[86,     1] loss: 712.917
[87,     1] loss: 768.797
Early stopping applied (best metric=0.38075727224349976)
Finished Training
Total time taken: 13.290085315704346
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1540.090
[2,     1] loss: 1547.320
[3,     1] loss: 1538.322
[4,     1] loss: 1542.041
[5,     1] loss: 1536.361
[6,     1] loss: 1536.394
[7,     1] loss: 1539.222
[8,     1] loss: 1537.205
[9,     1] loss: 1536.221
[10,     1] loss: 1538.085
[11,     1] loss: 1541.689
[12,     1] loss: 1533.600
[13,     1] loss: 1534.319
[14,     1] loss: 1539.710
[15,     1] loss: 1533.385
[16,     1] loss: 1535.981
[17,     1] loss: 1534.930
[18,     1] loss: 1530.045
[19,     1] loss: 1530.273
[20,     1] loss: 1528.466
[21,     1] loss: 1532.235
[22,     1] loss: 1520.398
[23,     1] loss: 1512.775
[24,     1] loss: 1506.113
[25,     1] loss: 1484.621
[26,     1] loss: 1444.482
[27,     1] loss: 1412.269
[28,     1] loss: 1396.259
[29,     1] loss: 1361.545
[30,     1] loss: 1326.601
[31,     1] loss: 1289.986
[32,     1] loss: 1306.059
[33,     1] loss: 1220.636
[34,     1] loss: 1277.382
[35,     1] loss: 1315.262
[36,     1] loss: 1182.442
[37,     1] loss: 1227.091
[38,     1] loss: 1257.310
[39,     1] loss: 1195.205
[40,     1] loss: 1212.020
[41,     1] loss: 1227.852
[42,     1] loss: 1207.193
[43,     1] loss: 1152.404
[44,     1] loss: 1170.082
[45,     1] loss: 1146.144
[46,     1] loss: 1129.907
[47,     1] loss: 1129.480
[48,     1] loss: 1131.910
[49,     1] loss: 1096.543
[50,     1] loss: 1140.435
[51,     1] loss: 1109.106
[52,     1] loss: 1162.085
[53,     1] loss: 1176.368
[54,     1] loss: 1070.342
[55,     1] loss: 1171.277
[56,     1] loss: 1137.969
[57,     1] loss: 1045.356
[58,     1] loss: 1100.915
[59,     1] loss: 1091.419
[60,     1] loss: 1167.649
[61,     1] loss: 1023.096
[62,     1] loss: 1063.939
[63,     1] loss: 1127.240
[64,     1] loss: 1046.496
[65,     1] loss: 1087.435
[66,     1] loss: 1012.119
[67,     1] loss: 1040.342
[68,     1] loss: 1006.919
[69,     1] loss: 1121.340
[70,     1] loss: 1022.460
[71,     1] loss: 1092.993
[72,     1] loss: 1044.100
[73,     1] loss: 976.042
[74,     1] loss: 990.699
[75,     1] loss: 967.250
[76,     1] loss: 1018.822
[77,     1] loss: 938.066
[78,     1] loss: 1054.338
[79,     1] loss: 980.682
[80,     1] loss: 987.385
[81,     1] loss: 950.574
[82,     1] loss: 983.563
[83,     1] loss: 974.399
[84,     1] loss: 1025.637
[85,     1] loss: 1053.099
[86,     1] loss: 1018.641
[87,     1] loss: 914.096
[88,     1] loss: 991.847
[89,     1] loss: 932.780
[90,     1] loss: 932.749
[91,     1] loss: 943.886
[92,     1] loss: 883.427
[93,     1] loss: 844.886
[94,     1] loss: 889.759
[95,     1] loss: 922.117
[96,     1] loss: 874.786
[97,     1] loss: 838.532
[98,     1] loss: 725.372
[99,     1] loss: 866.608
[100,     1] loss: 793.084
[101,     1] loss: 825.013
[102,     1] loss: 798.282
[103,     1] loss: 785.091
[104,     1] loss: 746.683
Early stopping applied (best metric=0.46728983521461487)
Finished Training
Total time taken: 15.480319499969482
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1539.994
[2,     1] loss: 1536.852
[3,     1] loss: 1537.771
[4,     1] loss: 1541.297
[5,     1] loss: 1542.050
[6,     1] loss: 1539.541
[7,     1] loss: 1535.001
[8,     1] loss: 1537.924
[9,     1] loss: 1542.807
[10,     1] loss: 1536.091
[11,     1] loss: 1534.499
[12,     1] loss: 1532.461
[13,     1] loss: 1533.573
[14,     1] loss: 1530.988
[15,     1] loss: 1525.599
[16,     1] loss: 1535.800
[17,     1] loss: 1526.475
[18,     1] loss: 1518.228
[19,     1] loss: 1512.593
[20,     1] loss: 1496.161
[21,     1] loss: 1486.326
[22,     1] loss: 1457.119
[23,     1] loss: 1431.026
[24,     1] loss: 1417.597
[25,     1] loss: 1384.073
[26,     1] loss: 1339.401
[27,     1] loss: 1323.997
[28,     1] loss: 1342.387
[29,     1] loss: 1356.934
[30,     1] loss: 1319.733
[31,     1] loss: 1331.694
[32,     1] loss: 1286.462
[33,     1] loss: 1220.176
[34,     1] loss: 1257.172
[35,     1] loss: 1251.676
[36,     1] loss: 1302.176
[37,     1] loss: 1246.055
[38,     1] loss: 1203.910
[39,     1] loss: 1247.909
[40,     1] loss: 1217.887
[41,     1] loss: 1150.749
[42,     1] loss: 1187.242
[43,     1] loss: 1214.051
[44,     1] loss: 1160.172
[45,     1] loss: 1156.755
[46,     1] loss: 1191.153
[47,     1] loss: 1170.957
[48,     1] loss: 1161.239
[49,     1] loss: 1169.501
[50,     1] loss: 1172.801
[51,     1] loss: 1117.892
[52,     1] loss: 1127.824
[53,     1] loss: 1115.493
[54,     1] loss: 1117.734
[55,     1] loss: 1087.916
[56,     1] loss: 1065.100
[57,     1] loss: 1105.125
[58,     1] loss: 1047.145
[59,     1] loss: 1036.215
[60,     1] loss: 1081.036
[61,     1] loss: 1024.001
[62,     1] loss: 1077.197
[63,     1] loss: 1109.335
[64,     1] loss: 1055.746
[65,     1] loss: 1048.958
[66,     1] loss: 1008.572
[67,     1] loss: 1095.311
[68,     1] loss: 1081.011
[69,     1] loss: 1058.499
[70,     1] loss: 1011.363
[71,     1] loss: 1028.467
[72,     1] loss: 967.219
[73,     1] loss: 1018.199
[74,     1] loss: 995.367
[75,     1] loss: 977.480
[76,     1] loss: 1017.556
[77,     1] loss: 960.783
[78,     1] loss: 1004.538
[79,     1] loss: 901.670
[80,     1] loss: 963.296
[81,     1] loss: 969.773
[82,     1] loss: 855.200
[83,     1] loss: 878.265
[84,     1] loss: 910.008
[85,     1] loss: 894.513
[86,     1] loss: 910.676
[87,     1] loss: 908.695
[88,     1] loss: 860.576
[89,     1] loss: 815.076
[90,     1] loss: 870.420
Early stopping applied (best metric=0.39225688576698303)
Finished Training
Total time taken: 14.089742660522461
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1539.432
[2,     1] loss: 1544.066
[3,     1] loss: 1534.940
[4,     1] loss: 1542.123
[5,     1] loss: 1537.748
[6,     1] loss: 1537.109
[7,     1] loss: 1539.990
[8,     1] loss: 1536.711
[9,     1] loss: 1540.760
[10,     1] loss: 1535.632
[11,     1] loss: 1538.883
[12,     1] loss: 1533.812
[13,     1] loss: 1533.897
[14,     1] loss: 1528.091
[15,     1] loss: 1529.955
[16,     1] loss: 1523.276
[17,     1] loss: 1520.830
[18,     1] loss: 1516.641
[19,     1] loss: 1492.233
[20,     1] loss: 1482.892
[21,     1] loss: 1448.568
[22,     1] loss: 1424.567
[23,     1] loss: 1373.254
[24,     1] loss: 1400.696
[25,     1] loss: 1320.722
[26,     1] loss: 1341.336
[27,     1] loss: 1303.783
[28,     1] loss: 1338.225
[29,     1] loss: 1306.734
[30,     1] loss: 1279.754
[31,     1] loss: 1305.322
[32,     1] loss: 1226.487
[33,     1] loss: 1267.375
[34,     1] loss: 1230.642
[35,     1] loss: 1252.118
[36,     1] loss: 1241.868
[37,     1] loss: 1244.306
[38,     1] loss: 1217.525
[39,     1] loss: 1272.284
[40,     1] loss: 1277.237
[41,     1] loss: 1247.140
[42,     1] loss: 1190.297
[43,     1] loss: 1185.043
[44,     1] loss: 1246.750
[45,     1] loss: 1179.050
[46,     1] loss: 1208.700
[47,     1] loss: 1150.808
[48,     1] loss: 1146.001
[49,     1] loss: 1122.533
[50,     1] loss: 1186.703
[51,     1] loss: 1183.727
[52,     1] loss: 1101.300
[53,     1] loss: 1165.784
[54,     1] loss: 1087.103
[55,     1] loss: 1091.603
[56,     1] loss: 1027.398
[57,     1] loss: 1080.138
[58,     1] loss: 1145.753
[59,     1] loss: 1102.988
[60,     1] loss: 1117.064
[61,     1] loss: 1059.539
[62,     1] loss: 1105.707
[63,     1] loss: 1070.249
[64,     1] loss: 1056.098
[65,     1] loss: 1033.888
[66,     1] loss: 985.289
[67,     1] loss: 1039.011
[68,     1] loss: 1013.759
[69,     1] loss: 1029.196
[70,     1] loss: 975.554
[71,     1] loss: 1043.505
[72,     1] loss: 996.060
[73,     1] loss: 1043.317
[74,     1] loss: 949.415
[75,     1] loss: 975.629
[76,     1] loss: 994.352
[77,     1] loss: 979.313
[78,     1] loss: 989.807
[79,     1] loss: 961.417
[80,     1] loss: 979.154
[81,     1] loss: 937.426
[82,     1] loss: 923.934
[83,     1] loss: 924.788
[84,     1] loss: 897.570
[85,     1] loss: 893.740
[86,     1] loss: 849.091
[87,     1] loss: 945.085
[88,     1] loss: 834.299
[89,     1] loss: 874.542
[90,     1] loss: 841.218
[91,     1] loss: 896.980
[92,     1] loss: 887.643
[93,     1] loss: 862.743
[94,     1] loss: 792.584
Early stopping applied (best metric=0.3661250174045563)
Finished Training
Total time taken: 14.970917224884033
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1543.323
[2,     1] loss: 1538.224
[3,     1] loss: 1539.200
[4,     1] loss: 1537.513
[5,     1] loss: 1534.095
[6,     1] loss: 1539.456
[7,     1] loss: 1538.532
[8,     1] loss: 1537.716
[9,     1] loss: 1537.314
[10,     1] loss: 1542.108
[11,     1] loss: 1536.359
[12,     1] loss: 1528.784
[13,     1] loss: 1527.194
[14,     1] loss: 1520.955
[15,     1] loss: 1514.207
[16,     1] loss: 1495.905
[17,     1] loss: 1479.207
[18,     1] loss: 1442.836
[19,     1] loss: 1419.347
[20,     1] loss: 1408.155
[21,     1] loss: 1337.021
[22,     1] loss: 1404.878
[23,     1] loss: 1359.142
[24,     1] loss: 1351.599
[25,     1] loss: 1397.114
[26,     1] loss: 1364.827
[27,     1] loss: 1275.504
[28,     1] loss: 1263.678
[29,     1] loss: 1296.591
[30,     1] loss: 1270.945
[31,     1] loss: 1285.279
[32,     1] loss: 1241.014
[33,     1] loss: 1227.197
[34,     1] loss: 1221.213
[35,     1] loss: 1151.611
[36,     1] loss: 1191.069
[37,     1] loss: 1201.580
[38,     1] loss: 1250.281
[39,     1] loss: 1165.453
[40,     1] loss: 1139.031
[41,     1] loss: 1202.335
[42,     1] loss: 1171.199
[43,     1] loss: 1144.300
[44,     1] loss: 1163.878
[45,     1] loss: 1181.343
[46,     1] loss: 1156.574
[47,     1] loss: 1128.862
[48,     1] loss: 1134.018
[49,     1] loss: 1121.847
[50,     1] loss: 1144.630
[51,     1] loss: 1076.017
[52,     1] loss: 1055.481
[53,     1] loss: 1106.773
[54,     1] loss: 1092.517
[55,     1] loss: 1026.418
[56,     1] loss: 1060.714
[57,     1] loss: 1043.825
[58,     1] loss: 998.501
[59,     1] loss: 1135.218
[60,     1] loss: 935.455
[61,     1] loss: 1042.986
[62,     1] loss: 1033.069
[63,     1] loss: 1029.591
[64,     1] loss: 1000.651
[65,     1] loss: 973.703
[66,     1] loss: 974.077
[67,     1] loss: 912.131
[68,     1] loss: 971.007
[69,     1] loss: 946.137
[70,     1] loss: 937.955
[71,     1] loss: 963.069
[72,     1] loss: 929.272
[73,     1] loss: 949.645
[74,     1] loss: 919.473
[75,     1] loss: 951.941
[76,     1] loss: 927.188
[77,     1] loss: 892.452
[78,     1] loss: 957.093
[79,     1] loss: 854.240
[80,     1] loss: 842.504
[81,     1] loss: 830.110
[82,     1] loss: 901.462
[83,     1] loss: 735.973
[84,     1] loss: 919.506
[85,     1] loss: 857.906
[86,     1] loss: 892.697
[87,     1] loss: 741.938
[88,     1] loss: 895.890
[89,     1] loss: 813.703
[90,     1] loss: 832.639
[91,     1] loss: 737.815
[92,     1] loss: 799.568
[93,     1] loss: 721.399
[94,     1] loss: 779.841
[95,     1] loss: 774.506
[96,     1] loss: 742.287
[97,     1] loss: 829.470
[98,     1] loss: 710.414
[99,     1] loss: 815.954
[100,     1] loss: 707.253
Early stopping applied (best metric=0.3725304901599884)
Finished Training
Total time taken: 14.916644096374512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1538.693
[2,     1] loss: 1540.453
[3,     1] loss: 1543.195
[4,     1] loss: 1536.513
[5,     1] loss: 1534.458
[6,     1] loss: 1535.990
[7,     1] loss: 1534.462
[8,     1] loss: 1537.072
[9,     1] loss: 1539.001
[10,     1] loss: 1527.672
[11,     1] loss: 1533.330
[12,     1] loss: 1530.117
[13,     1] loss: 1533.184
[14,     1] loss: 1522.551
[15,     1] loss: 1516.243
[16,     1] loss: 1504.933
[17,     1] loss: 1478.725
[18,     1] loss: 1452.219
[19,     1] loss: 1404.204
[20,     1] loss: 1362.617
[21,     1] loss: 1355.499
[22,     1] loss: 1304.445
[23,     1] loss: 1349.319
[24,     1] loss: 1312.931
[25,     1] loss: 1257.901
[26,     1] loss: 1281.309
[27,     1] loss: 1287.109
[28,     1] loss: 1246.008
[29,     1] loss: 1220.901
[30,     1] loss: 1239.492
[31,     1] loss: 1243.140
[32,     1] loss: 1258.849
[33,     1] loss: 1187.872
[34,     1] loss: 1193.292
[35,     1] loss: 1216.808
[36,     1] loss: 1135.204
[37,     1] loss: 1193.827
[38,     1] loss: 1180.947
[39,     1] loss: 1200.353
[40,     1] loss: 1128.191
[41,     1] loss: 1135.253
[42,     1] loss: 1124.831
[43,     1] loss: 1144.081
[44,     1] loss: 1166.165
[45,     1] loss: 1127.881
[46,     1] loss: 1091.266
[47,     1] loss: 1096.108
[48,     1] loss: 1121.810
[49,     1] loss: 1122.202
[50,     1] loss: 1094.493
[51,     1] loss: 1089.949
[52,     1] loss: 1080.860
[53,     1] loss: 1034.986
[54,     1] loss: 1026.535
[55,     1] loss: 1044.636
[56,     1] loss: 1076.969
[57,     1] loss: 1100.975
[58,     1] loss: 992.769
[59,     1] loss: 1044.994
[60,     1] loss: 990.913
[61,     1] loss: 986.168
[62,     1] loss: 890.951
[63,     1] loss: 947.960
[64,     1] loss: 945.154
[65,     1] loss: 924.979
[66,     1] loss: 919.228
[67,     1] loss: 902.797
[68,     1] loss: 928.604
[69,     1] loss: 911.587
[70,     1] loss: 882.835
[71,     1] loss: 1011.996
[72,     1] loss: 843.687
[73,     1] loss: 803.592
[74,     1] loss: 855.309
[75,     1] loss: 855.950
[76,     1] loss: 773.568
[77,     1] loss: 770.731
[78,     1] loss: 808.704
[79,     1] loss: 840.062
[80,     1] loss: 737.445
[81,     1] loss: 777.724
[82,     1] loss: 816.750
[83,     1] loss: 739.296
[84,     1] loss: 719.906
[85,     1] loss: 755.790
[86,     1] loss: 724.849
[87,     1] loss: 727.431
[88,     1] loss: 683.004
[89,     1] loss: 720.676
[90,     1] loss: 700.793
Early stopping applied (best metric=0.4099816381931305)
Finished Training
Total time taken: 13.458712339401245
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1535.702
[2,     1] loss: 1528.740
[3,     1] loss: 1557.861
[4,     1] loss: 1537.097
[5,     1] loss: 1539.252
[6,     1] loss: 1534.030
[7,     1] loss: 1536.124
[8,     1] loss: 1531.831
[9,     1] loss: 1526.482
[10,     1] loss: 1523.464
[11,     1] loss: 1511.101
[12,     1] loss: 1497.707
[13,     1] loss: 1473.892
[14,     1] loss: 1427.107
[15,     1] loss: 1392.283
[16,     1] loss: 1354.997
[17,     1] loss: 1344.565
[18,     1] loss: 1302.408
[19,     1] loss: 1359.555
[20,     1] loss: 1294.950
[21,     1] loss: 1292.004
[22,     1] loss: 1244.401
[23,     1] loss: 1277.167
[24,     1] loss: 1274.681
[25,     1] loss: 1264.213
[26,     1] loss: 1281.345
[27,     1] loss: 1206.805
[28,     1] loss: 1248.328
[29,     1] loss: 1207.270
[30,     1] loss: 1225.814
[31,     1] loss: 1198.567
[32,     1] loss: 1261.121
[33,     1] loss: 1232.202
[34,     1] loss: 1141.190
[35,     1] loss: 1215.617
[36,     1] loss: 1161.367
[37,     1] loss: 1190.031
[38,     1] loss: 1188.705
[39,     1] loss: 1193.995
[40,     1] loss: 1207.696
[41,     1] loss: 1114.491
[42,     1] loss: 1095.008
[43,     1] loss: 1156.951
[44,     1] loss: 1086.161
[45,     1] loss: 1155.057
[46,     1] loss: 1106.526
[47,     1] loss: 1092.502
[48,     1] loss: 1071.711
[49,     1] loss: 1123.918
[50,     1] loss: 1058.261
[51,     1] loss: 1010.117
[52,     1] loss: 1097.129
[53,     1] loss: 999.682
[54,     1] loss: 971.829
[55,     1] loss: 984.319
[56,     1] loss: 1021.149
[57,     1] loss: 946.056
[58,     1] loss: 1029.787
[59,     1] loss: 954.162
[60,     1] loss: 924.892
[61,     1] loss: 979.658
[62,     1] loss: 888.387
[63,     1] loss: 979.941
[64,     1] loss: 869.860
[65,     1] loss: 893.587
[66,     1] loss: 905.064
[67,     1] loss: 878.249
[68,     1] loss: 873.336
[69,     1] loss: 869.055
[70,     1] loss: 820.299
[71,     1] loss: 874.702
[72,     1] loss: 832.263
[73,     1] loss: 838.969
[74,     1] loss: 776.028
[75,     1] loss: 754.327
[76,     1] loss: 779.427
[77,     1] loss: 762.886
[78,     1] loss: 786.213
[79,     1] loss: 844.395
[80,     1] loss: 733.097
[81,     1] loss: 767.962
[82,     1] loss: 748.159
[83,     1] loss: 791.802
[84,     1] loss: 779.501
[85,     1] loss: 721.077
Early stopping applied (best metric=0.4283060133457184)
Finished Training
Total time taken: 12.5656578540802
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1543.767
[2,     1] loss: 1546.627
[3,     1] loss: 1540.847
[4,     1] loss: 1539.178
[5,     1] loss: 1534.721
[6,     1] loss: 1538.982
[7,     1] loss: 1534.099
[8,     1] loss: 1530.168
[9,     1] loss: 1528.903
[10,     1] loss: 1525.656
[11,     1] loss: 1518.166
[12,     1] loss: 1512.177
[13,     1] loss: 1495.957
[14,     1] loss: 1470.715
[15,     1] loss: 1434.270
[16,     1] loss: 1393.219
[17,     1] loss: 1391.403
[18,     1] loss: 1320.440
[19,     1] loss: 1303.176
[20,     1] loss: 1363.583
[21,     1] loss: 1305.639
[22,     1] loss: 1339.125
[23,     1] loss: 1293.024
[24,     1] loss: 1255.745
[25,     1] loss: 1283.424
[26,     1] loss: 1294.073
[27,     1] loss: 1262.204
[28,     1] loss: 1277.945
[29,     1] loss: 1262.885
[30,     1] loss: 1228.677
[31,     1] loss: 1239.954
[32,     1] loss: 1209.329
[33,     1] loss: 1176.219
[34,     1] loss: 1242.660
[35,     1] loss: 1207.407
[36,     1] loss: 1248.506
[37,     1] loss: 1221.795
[38,     1] loss: 1149.493
[39,     1] loss: 1203.092
[40,     1] loss: 1167.182
[41,     1] loss: 1142.050
[42,     1] loss: 1200.972
[43,     1] loss: 1200.238
[44,     1] loss: 1140.100
[45,     1] loss: 1091.437
[46,     1] loss: 1087.621
[47,     1] loss: 1090.141
[48,     1] loss: 1158.624
[49,     1] loss: 1111.241
[50,     1] loss: 1069.662
[51,     1] loss: 1093.829
[52,     1] loss: 1097.729
[53,     1] loss: 1025.561
[54,     1] loss: 1044.983
[55,     1] loss: 1062.248
[56,     1] loss: 1061.721
[57,     1] loss: 1050.967
[58,     1] loss: 1030.125
[59,     1] loss: 1079.934
[60,     1] loss: 1062.363
[61,     1] loss: 992.793
[62,     1] loss: 1044.843
[63,     1] loss: 996.208
[64,     1] loss: 986.384
[65,     1] loss: 953.420
[66,     1] loss: 984.502
[67,     1] loss: 938.805
[68,     1] loss: 924.560
[69,     1] loss: 1025.131
[70,     1] loss: 885.691
[71,     1] loss: 840.957
[72,     1] loss: 906.800
[73,     1] loss: 898.440
[74,     1] loss: 917.524
[75,     1] loss: 900.769
[76,     1] loss: 877.897
[77,     1] loss: 856.009
[78,     1] loss: 836.441
[79,     1] loss: 813.357
[80,     1] loss: 833.131
[81,     1] loss: 971.451
[82,     1] loss: 729.880
[83,     1] loss: 897.436
[84,     1] loss: 796.163
[85,     1] loss: 872.635
[86,     1] loss: 822.133
[87,     1] loss: 884.258
[88,     1] loss: 810.280
[89,     1] loss: 808.614
[90,     1] loss: 768.489
[91,     1] loss: 780.142
[92,     1] loss: 787.229
[93,     1] loss: 740.889
Early stopping applied (best metric=0.3899143636226654)
Finished Training
Total time taken: 14.21565055847168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1536.471
[2,     1] loss: 1549.099
[3,     1] loss: 1534.311
[4,     1] loss: 1544.827
[5,     1] loss: 1535.455
[6,     1] loss: 1538.876
[7,     1] loss: 1538.367
[8,     1] loss: 1530.332
[9,     1] loss: 1533.593
[10,     1] loss: 1525.285
[11,     1] loss: 1516.152
[12,     1] loss: 1512.363
[13,     1] loss: 1492.576
[14,     1] loss: 1466.465
[15,     1] loss: 1445.065
[16,     1] loss: 1416.525
[17,     1] loss: 1384.427
[18,     1] loss: 1355.528
[19,     1] loss: 1351.338
[20,     1] loss: 1337.097
[21,     1] loss: 1336.026
[22,     1] loss: 1300.399
[23,     1] loss: 1278.671
[24,     1] loss: 1266.968
[25,     1] loss: 1275.085
[26,     1] loss: 1275.241
[27,     1] loss: 1260.510
[28,     1] loss: 1282.806
[29,     1] loss: 1228.273
[30,     1] loss: 1232.596
[31,     1] loss: 1226.121
[32,     1] loss: 1208.562
[33,     1] loss: 1140.049
[34,     1] loss: 1169.348
[35,     1] loss: 1172.016
[36,     1] loss: 1123.033
[37,     1] loss: 1155.787
[38,     1] loss: 1175.402
[39,     1] loss: 1156.918
[40,     1] loss: 1174.334
[41,     1] loss: 1213.938
[42,     1] loss: 1116.462
[43,     1] loss: 1138.455
[44,     1] loss: 1096.414
[45,     1] loss: 1122.749
[46,     1] loss: 1088.458
[47,     1] loss: 1070.786
[48,     1] loss: 1055.436
[49,     1] loss: 1093.991
[50,     1] loss: 1095.340
[51,     1] loss: 1092.237
[52,     1] loss: 1062.334
[53,     1] loss: 1071.213
[54,     1] loss: 1090.684
[55,     1] loss: 1064.666
[56,     1] loss: 1028.303
[57,     1] loss: 1048.338
[58,     1] loss: 1023.662
[59,     1] loss: 983.819
[60,     1] loss: 1049.620
[61,     1] loss: 983.057
[62,     1] loss: 967.034
[63,     1] loss: 1047.191
[64,     1] loss: 942.869
[65,     1] loss: 956.885
[66,     1] loss: 880.840
[67,     1] loss: 878.717
[68,     1] loss: 1006.396
[69,     1] loss: 911.182
[70,     1] loss: 952.829
[71,     1] loss: 826.008
[72,     1] loss: 897.705
[73,     1] loss: 844.072
[74,     1] loss: 842.329
[75,     1] loss: 894.241
[76,     1] loss: 858.012
Early stopping applied (best metric=0.3679896891117096)
Finished Training
Total time taken: 11.651699304580688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1538.723
[2,     1] loss: 1540.933
[3,     1] loss: 1537.179
[4,     1] loss: 1539.139
[5,     1] loss: 1534.096
[6,     1] loss: 1535.975
[7,     1] loss: 1534.366
[8,     1] loss: 1540.185
[9,     1] loss: 1530.099
[10,     1] loss: 1528.645
[11,     1] loss: 1520.100
[12,     1] loss: 1514.781
[13,     1] loss: 1491.023
[14,     1] loss: 1475.544
[15,     1] loss: 1459.460
[16,     1] loss: 1412.973
[17,     1] loss: 1413.577
[18,     1] loss: 1369.643
[19,     1] loss: 1376.648
[20,     1] loss: 1357.408
[21,     1] loss: 1340.206
[22,     1] loss: 1308.802
[23,     1] loss: 1304.059
[24,     1] loss: 1249.264
[25,     1] loss: 1287.755
[26,     1] loss: 1292.155
[27,     1] loss: 1259.249
[28,     1] loss: 1294.576
[29,     1] loss: 1287.221
[30,     1] loss: 1235.371
[31,     1] loss: 1241.072
[32,     1] loss: 1245.937
[33,     1] loss: 1260.449
[34,     1] loss: 1221.621
[35,     1] loss: 1224.990
[36,     1] loss: 1227.597
[37,     1] loss: 1241.190
[38,     1] loss: 1204.301
[39,     1] loss: 1135.232
[40,     1] loss: 1193.782
[41,     1] loss: 1158.411
[42,     1] loss: 1174.565
[43,     1] loss: 1253.009
[44,     1] loss: 1156.061
[45,     1] loss: 1147.478
[46,     1] loss: 1060.123
[47,     1] loss: 1111.722
[48,     1] loss: 1119.284
[49,     1] loss: 1111.362
[50,     1] loss: 1065.212
[51,     1] loss: 1027.618
[52,     1] loss: 1055.274
[53,     1] loss: 1073.574
[54,     1] loss: 1064.698
[55,     1] loss: 1087.129
[56,     1] loss: 989.843
[57,     1] loss: 1091.798
[58,     1] loss: 1009.740
[59,     1] loss: 1097.577
[60,     1] loss: 1015.494
[61,     1] loss: 999.778
[62,     1] loss: 1040.438
[63,     1] loss: 955.366
[64,     1] loss: 1007.194
[65,     1] loss: 945.558
[66,     1] loss: 925.447
[67,     1] loss: 997.035
[68,     1] loss: 972.583
[69,     1] loss: 966.874
[70,     1] loss: 937.931
[71,     1] loss: 965.705
[72,     1] loss: 885.055
[73,     1] loss: 891.630
[74,     1] loss: 977.731
[75,     1] loss: 907.785
[76,     1] loss: 916.615
[77,     1] loss: 901.679
[78,     1] loss: 878.992
[79,     1] loss: 934.449
[80,     1] loss: 877.894
[81,     1] loss: 892.503
[82,     1] loss: 831.558
[83,     1] loss: 827.686
[84,     1] loss: 889.934
[85,     1] loss: 809.511
[86,     1] loss: 904.556
[87,     1] loss: 837.930
[88,     1] loss: 935.105
[89,     1] loss: 830.669
[90,     1] loss: 803.557
[91,     1] loss: 746.664
[92,     1] loss: 723.813
[93,     1] loss: 751.126
[94,     1] loss: 755.002
[95,     1] loss: 821.961
Early stopping applied (best metric=0.39016491174697876)
Finished Training
Total time taken: 14.564918756484985
{'Hydroxylation-K Validation Accuracy': 0.767612293144208, 'Hydroxylation-K Validation Sensitivity': 0.657037037037037, 'Hydroxylation-K Validation Specificity': 0.7947368421052632, 'Hydroxylation-K Validation Precision': 0.44969593615878445, 'Hydroxylation-K AUC ROC': 0.812962962962963, 'Hydroxylation-K AUC PR': 0.58160418963628, 'Hydroxylation-K MCC': 0.3990655145212108, 'Hydroxylation-K F1': 0.5309312015838752, 'Validation Loss (Hydroxylation-K)': 0.46071079969406126, 'Hydroxylation-P Validation Accuracy': 0.7882644028221918, 'Hydroxylation-P Validation Sensitivity': 0.7631746031746032, 'Hydroxylation-P Validation Specificity': 0.7936829767070677, 'Hydroxylation-P Validation Precision': 0.447713696293284, 'Hydroxylation-P AUC ROC': 0.8263565752002832, 'Hydroxylation-P AUC PR': 0.5661301285635126, 'Hydroxylation-P MCC': 0.46425639205559593, 'Hydroxylation-P F1': 0.5626826714831925, 'Validation Loss (Hydroxylation-P)': 0.39227838317553204, 'Validation Loss (total)': 0.8529891769091288, 'TimeToTrain': 13.402826197942098}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008572019478648756,
 'learning_rate_Hydroxylation-K': 0.0042196611274877005,
 'learning_rate_Hydroxylation-P': 0.0006347417705308659,
 'log_base': 2.9422626421965052,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 684384505,
 'sample_weights': [2.986782696426251, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.2184065240043498,
 'weight_decay_Hydroxylation-K': 3.5930237635220106,
 'weight_decay_Hydroxylation-P': 8.478353685811824}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.896
[2,     1] loss: 1232.519
[3,     1] loss: 1236.826
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006149225100947745,
 'learning_rate_Hydroxylation-K': 9.787357740265172e-05,
 'learning_rate_Hydroxylation-P': 0.006892511431702457,
 'log_base': 2.826698970512415,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3865373127,
 'sample_weights': [1.5469568210640154, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.2929824177309115,
 'weight_decay_Hydroxylation-K': 1.8342959897427458,
 'weight_decay_Hydroxylation-P': 8.437697121250961}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.539
[2,     1] loss: 1246.983
[3,     1] loss: 1245.342
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006414248451607047,
 'learning_rate_Hydroxylation-K': 0.009245931158547738,
 'learning_rate_Hydroxylation-P': 0.009903781881140095,
 'log_base': 2.6434910836290055,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1176279089,
 'sample_weights': [1.60660931684774, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.586944788891453,
 'weight_decay_Hydroxylation-K': 9.798031181169165,
 'weight_decay_Hydroxylation-P': 8.849574342598075}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.310
[2,     1] loss: 1274.807
[3,     1] loss: 1267.752
[4,     1] loss: 1274.709
[5,     1] loss: 1270.893
[6,     1] loss: 1268.990
[7,     1] loss: 1262.175
[8,     1] loss: 1260.581
[9,     1] loss: 1246.635
[10,     1] loss: 1232.114
[11,     1] loss: 1198.029
[12,     1] loss: 1172.857
[13,     1] loss: 1116.026
[14,     1] loss: 1107.472
[15,     1] loss: 1079.509
[16,     1] loss: 1103.675
[17,     1] loss: 1097.820
[18,     1] loss: 1051.862
[19,     1] loss: 1036.257
[20,     1] loss: 1099.609
[21,     1] loss: 1074.720
[22,     1] loss: 1030.775
[23,     1] loss: 1031.886
[24,     1] loss: 998.682
[25,     1] loss: 960.209
[26,     1] loss: 1044.905
[27,     1] loss: 947.522
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009571464573657618,
 'learning_rate_Hydroxylation-K': 0.008719620509105876,
 'learning_rate_Hydroxylation-P': 0.0031107778949214593,
 'log_base': 2.219791121006423,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1265087277,
 'sample_weights': [1.717356670816207, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.005603387238168,
 'weight_decay_Hydroxylation-K': 9.279376059207264,
 'weight_decay_Hydroxylation-P': 3.1247090753669102}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.818
[2,     1] loss: 1355.649
[3,     1] loss: 1351.452
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001745360502525744,
 'learning_rate_Hydroxylation-K': 0.007453815580156523,
 'learning_rate_Hydroxylation-P': 0.0025673185538159136,
 'log_base': 1.4089861857633674,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 65278029,
 'sample_weights': [2.093573761970842, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.446538159854564,
 'weight_decay_Hydroxylation-K': 0.771792488256271,
 'weight_decay_Hydroxylation-P': 5.517566274857373}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1935.835
[2,     1] loss: 1933.771
[3,     1] loss: 1938.047
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018271181081607924,
 'learning_rate_Hydroxylation-K': 0.0061510290154657725,
 'learning_rate_Hydroxylation-P': 0.008946165431479323,
 'log_base': 1.4716445223565042,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2139816981,
 'sample_weights': [4.869020505260576, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.577604959157158,
 'weight_decay_Hydroxylation-K': 2.8811083551201664,
 'weight_decay_Hydroxylation-P': 6.339062345112099}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1821.156
[2,     1] loss: 1820.275
[3,     1] loss: 1816.661
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050766819647644545,
 'learning_rate_Hydroxylation-K': 0.004236076531186502,
 'learning_rate_Hydroxylation-P': 0.007225232243758107,
 'log_base': 1.9328090719365123,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1737371703,
 'sample_weights': [4.320723109202013, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.774512168278424,
 'weight_decay_Hydroxylation-K': 7.799097348892698,
 'weight_decay_Hydroxylation-P': 4.295367265204025}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1448.403
[2,     1] loss: 1455.406
[3,     1] loss: 1446.445
[4,     1] loss: 1443.674
[5,     1] loss: 1439.046
[6,     1] loss: 1441.622
[7,     1] loss: 1442.303
[8,     1] loss: 1437.326
[9,     1] loss: 1441.641
[10,     1] loss: 1439.222
[11,     1] loss: 1437.802
[12,     1] loss: 1433.642
[13,     1] loss: 1431.909
[14,     1] loss: 1418.567
[15,     1] loss: 1403.510
[16,     1] loss: 1379.935
[17,     1] loss: 1344.024
[18,     1] loss: 1314.012
[19,     1] loss: 1303.298
[20,     1] loss: 1249.867
[21,     1] loss: 1209.266
[22,     1] loss: 1220.814
[23,     1] loss: 1176.283
[24,     1] loss: 1180.509
[25,     1] loss: 1205.024
[26,     1] loss: 1222.606
[27,     1] loss: 1160.271
[28,     1] loss: 1118.741
[29,     1] loss: 1148.367
[30,     1] loss: 1117.293
[31,     1] loss: 1127.672
[32,     1] loss: 1169.449
[33,     1] loss: 1093.751
[34,     1] loss: 1095.007
[35,     1] loss: 1096.372
[36,     1] loss: 1149.004
[37,     1] loss: 1066.125
[38,     1] loss: 1033.550
[39,     1] loss: 1018.813
[40,     1] loss: 1030.024
[41,     1] loss: 993.178
[42,     1] loss: 997.962
[43,     1] loss: 1020.356
[44,     1] loss: 1054.803
[45,     1] loss: 1043.296
[46,     1] loss: 1036.102
[47,     1] loss: 1000.457
[48,     1] loss: 1015.291
[49,     1] loss: 996.459
[50,     1] loss: 1062.820
[51,     1] loss: 940.541
[52,     1] loss: 1014.387
[53,     1] loss: 1032.281
[54,     1] loss: 919.092
[55,     1] loss: 1036.608
[56,     1] loss: 877.460
[57,     1] loss: 937.102
[58,     1] loss: 925.094
[59,     1] loss: 943.675
[60,     1] loss: 817.508
[61,     1] loss: 976.413
[62,     1] loss: 949.586
[63,     1] loss: 875.402
[64,     1] loss: 848.462
[65,     1] loss: 904.323
[66,     1] loss: 878.105
[67,     1] loss: 947.193
[68,     1] loss: 946.832
[69,     1] loss: 849.060
[70,     1] loss: 853.013
[71,     1] loss: 871.239
[72,     1] loss: 745.975
[73,     1] loss: 820.086
[74,     1] loss: 1018.100
[75,     1] loss: 832.338
[76,     1] loss: 776.422
[77,     1] loss: 799.388
[78,     1] loss: 726.979
[79,     1] loss: 787.697
[80,     1] loss: 808.258
[81,     1] loss: 915.973
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011124260040581756,
 'learning_rate_Hydroxylation-K': 0.000892321302802879,
 'learning_rate_Hydroxylation-P': 0.0018158708985685128,
 'log_base': 1.5600128182291235,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2542777297,
 'sample_weights': [2.533395972009031, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.699503994856197,
 'weight_decay_Hydroxylation-K': 7.3807012343761,
 'weight_decay_Hydroxylation-P': 0.3848955144657018}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1702.834
[2,     1] loss: 1696.378
[3,     1] loss: 1701.896
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008978219634284893,
 'learning_rate_Hydroxylation-K': 0.003538469505731871,
 'learning_rate_Hydroxylation-P': 0.009271423470499791,
 'log_base': 2.7673863334815945,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4284984811,
 'sample_weights': [3.7541388114335246, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.9748960411210197,
 'weight_decay_Hydroxylation-K': 7.939980271858943,
 'weight_decay_Hydroxylation-P': 7.961958622896429}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.329
[2,     1] loss: 1256.460
[3,     1] loss: 1263.749
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0073970098858240334,
 'learning_rate_Hydroxylation-K': 0.007207753776818803,
 'learning_rate_Hydroxylation-P': 0.005471105358520414,
 'log_base': 2.967410103147385,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2645317334,
 'sample_weights': [1.6400802771321181, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.6885758255079715,
 'weight_decay_Hydroxylation-K': 7.081619213964831,
 'weight_decay_Hydroxylation-P': 9.28002426463446}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.667
[2,     1] loss: 1231.596
[3,     1] loss: 1230.259
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004586600814064671,
 'learning_rate_Hydroxylation-K': 0.00920712746333073,
 'learning_rate_Hydroxylation-P': 0.008616949127634034,
 'log_base': 1.4902586783586023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3736087157,
 'sample_weights': [1.5348526079403808, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.365660388893809,
 'weight_decay_Hydroxylation-K': 7.929018278491675,
 'weight_decay_Hydroxylation-P': 6.367545838671992}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1787.042
[2,     1] loss: 1789.716
[3,     1] loss: 1798.939
[4,     1] loss: 1790.611
[5,     1] loss: 1776.297
[6,     1] loss: 1798.574
[7,     1] loss: 1794.610
[8,     1] loss: 1778.279
[9,     1] loss: 1785.536
[10,     1] loss: 1788.637
[11,     1] loss: 1778.529
[12,     1] loss: 1777.065
[13,     1] loss: 1758.102
[14,     1] loss: 1748.858
[15,     1] loss: 1717.263
[16,     1] loss: 1691.670
[17,     1] loss: 1640.729
[18,     1] loss: 1604.542
[19,     1] loss: 1546.385
[20,     1] loss: 1600.440
[21,     1] loss: 1550.014
[22,     1] loss: 1532.737
[23,     1] loss: 1500.849
[24,     1] loss: 1501.217
[25,     1] loss: 1435.352
[26,     1] loss: 1481.589
[27,     1] loss: 1520.283
[28,     1] loss: 1400.585
[29,     1] loss: 1428.551
[30,     1] loss: 1414.001
[31,     1] loss: 1416.076
[32,     1] loss: 1387.577
[33,     1] loss: 1311.616
[34,     1] loss: 1298.334
[35,     1] loss: 1416.131
[36,     1] loss: 1389.071
[37,     1] loss: 1260.496
[38,     1] loss: 1396.995
[39,     1] loss: 1220.974
[40,     1] loss: 1331.674
[41,     1] loss: 1269.566
[42,     1] loss: 1237.746
[43,     1] loss: 1239.989
[44,     1] loss: 1154.681
[45,     1] loss: 1217.935
[46,     1] loss: 1201.318
[47,     1] loss: 1078.096
[48,     1] loss: 1124.632
[49,     1] loss: 1202.089
[50,     1] loss: 1543.923
[51,     1] loss: 1462.280
[52,     1] loss: 1209.341
[53,     1] loss: 1306.203
[54,     1] loss: 1340.216
[55,     1] loss: 1229.140
[56,     1] loss: 1213.604
[57,     1] loss: 1204.308
[58,     1] loss: 1180.289
[59,     1] loss: 1098.454
[60,     1] loss: 1131.605
[61,     1] loss: 1054.994
[62,     1] loss: 1039.798
[63,     1] loss: 1053.439
[64,     1] loss: 1041.431
[65,     1] loss: 1001.147
[66,     1] loss: 1020.820
[67,     1] loss: 956.420
[68,     1] loss: 969.509
[69,     1] loss: 954.111
[70,     1] loss: 931.654
[71,     1] loss: 851.770
[72,     1] loss: 1021.073
[73,     1] loss: 1712.492
[74,     1] loss: 958.877
[75,     1] loss: 1420.157
[76,     1] loss: 1018.036
[77,     1] loss: 1150.516
[78,     1] loss: 1301.104
[79,     1] loss: 1098.431
[80,     1] loss: 1103.566
[81,     1] loss: 1179.251
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0074821034385536295,
 'learning_rate_Hydroxylation-K': 0.007465447011594113,
 'learning_rate_Hydroxylation-P': 0.00564431363273189,
 'log_base': 2.4681687232589495,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1179137328,
 'sample_weights': [4.1845954180185965, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.4769979963325808,
 'weight_decay_Hydroxylation-K': 8.854099071510324,
 'weight_decay_Hydroxylation-P': 8.766515036179236}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.780
[2,     1] loss: 1302.826
[3,     1] loss: 1298.521
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029885913852998563,
 'learning_rate_Hydroxylation-K': 0.0011479069117111174,
 'learning_rate_Hydroxylation-P': 0.003662830346065248,
 'log_base': 1.6832876376635335,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2786919848,
 'sample_weights': [1.8477992581065674, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.01234621044426,
 'weight_decay_Hydroxylation-K': 6.324660976992984,
 'weight_decay_Hydroxylation-P': 6.70891386915217}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1584.835
[2,     1] loss: 1586.060
[3,     1] loss: 1582.192
[4,     1] loss: 1585.417
[5,     1] loss: 1584.032
[6,     1] loss: 1583.912
[7,     1] loss: 1580.406
[8,     1] loss: 1582.687
[9,     1] loss: 1581.240
[10,     1] loss: 1577.255
[11,     1] loss: 1575.745
[12,     1] loss: 1567.557
[13,     1] loss: 1553.659
[14,     1] loss: 1534.610
[15,     1] loss: 1500.980
[16,     1] loss: 1469.781
[17,     1] loss: 1409.544
[18,     1] loss: 1423.749
[19,     1] loss: 1339.052
[20,     1] loss: 1345.986
[21,     1] loss: 1314.932
[22,     1] loss: 1359.379
[23,     1] loss: 1369.325
[24,     1] loss: 1369.407
[25,     1] loss: 1315.806
[26,     1] loss: 1284.284
[27,     1] loss: 1303.722
[28,     1] loss: 1245.611
[29,     1] loss: 1273.446
[30,     1] loss: 1228.672
[31,     1] loss: 1212.745
[32,     1] loss: 1173.124
[33,     1] loss: 1172.765
[34,     1] loss: 1154.303
[35,     1] loss: 1184.669
[36,     1] loss: 1178.080
[37,     1] loss: 1153.638
[38,     1] loss: 1124.422
[39,     1] loss: 1147.569
[40,     1] loss: 1158.333
[41,     1] loss: 1113.802
[42,     1] loss: 1146.608
[43,     1] loss: 1095.340
[44,     1] loss: 1107.271
[45,     1] loss: 1092.244
[46,     1] loss: 1045.030
[47,     1] loss: 1082.135
[48,     1] loss: 1016.143
[49,     1] loss: 1175.209
[50,     1] loss: 1175.443
[51,     1] loss: 1079.128
[52,     1] loss: 1124.634
[53,     1] loss: 1087.105
[54,     1] loss: 1045.320
[55,     1] loss: 1088.729
[56,     1] loss: 1063.564
[57,     1] loss: 1046.635
[58,     1] loss: 1001.164
[59,     1] loss: 939.761
[60,     1] loss: 983.922
[61,     1] loss: 918.432
[62,     1] loss: 985.011
[63,     1] loss: 986.011
[64,     1] loss: 1001.743
[65,     1] loss: 918.963
[66,     1] loss: 914.995
[67,     1] loss: 854.100
[68,     1] loss: 976.468
[69,     1] loss: 852.901
[70,     1] loss: 874.463
[71,     1] loss: 900.700
[72,     1] loss: 864.719
[73,     1] loss: 914.516
[74,     1] loss: 805.381
[75,     1] loss: 1004.827
[76,     1] loss: 939.752
[77,     1] loss: 785.038
[78,     1] loss: 948.166
[79,     1] loss: 819.650
[80,     1] loss: 932.467
[81,     1] loss: 751.527
[82,     1] loss: 882.101
[83,     1] loss: 776.574
[84,     1] loss: 904.976
[85,     1] loss: 800.018
[86,     1] loss: 773.611
[87,     1] loss: 738.823
[88,     1] loss: 726.103
[89,     1] loss: 954.242
[90,     1] loss: 743.237
[91,     1] loss: 728.423
[92,     1] loss: 810.519
[93,     1] loss: 652.319
[94,     1] loss: 746.009
[95,     1] loss: 705.939
[96,     1] loss: 662.053
[97,     1] loss: 724.402
[98,     1] loss: 692.212
[99,     1] loss: 656.800
[100,     1] loss: 677.869
[101,     1] loss: 658.600
[102,     1] loss: 683.964
[103,     1] loss: 1010.674
[104,     1] loss: 1332.971
[105,     1] loss: 745.265
[106,     1] loss: 1054.923
[107,     1] loss: 790.568
[108,     1] loss: 953.788
[109,     1] loss: 944.157
[110,     1] loss: 847.982
[111,     1] loss: 864.668
[112,     1] loss: 868.122
[113,     1] loss: 770.765
[114,     1] loss: 798.118
[115,     1] loss: 772.019
[116,     1] loss: 745.524
[117,     1] loss: 690.402
[118,     1] loss: 710.563
[119,     1] loss: 828.114
[120,     1] loss: 742.760
[121,     1] loss: 676.595
[122,     1] loss: 743.403
[123,     1] loss: 631.380
[124,     1] loss: 662.727
[125,     1] loss: 612.737
[126,     1] loss: 747.940
[127,     1] loss: 829.882
[128,     1] loss: 617.516
[129,     1] loss: 599.744
[130,     1] loss: 614.125
[131,     1] loss: 550.215
[132,     1] loss: 638.569
[133,     1] loss: 669.836
[134,     1] loss: 593.263
Early stopping applied (best metric=0.2958485782146454)
Finished Training
Total time taken: 23.02619695663452
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1586.654
[2,     1] loss: 1582.313
[3,     1] loss: 1579.011
[4,     1] loss: 1586.709
[5,     1] loss: 1580.321
[6,     1] loss: 1579.595
[7,     1] loss: 1574.457
[8,     1] loss: 1560.476
[9,     1] loss: 1549.722
[10,     1] loss: 1529.925
[11,     1] loss: 1495.902
[12,     1] loss: 1464.434
[13,     1] loss: 1442.243
[14,     1] loss: 1386.234
[15,     1] loss: 1356.823
[16,     1] loss: 1323.136
[17,     1] loss: 1332.697
[18,     1] loss: 1263.427
[19,     1] loss: 1249.336
[20,     1] loss: 1264.707
[21,     1] loss: 1269.990
[22,     1] loss: 1299.948
[23,     1] loss: 1263.482
[24,     1] loss: 1286.641
[25,     1] loss: 1172.058
[26,     1] loss: 1206.277
[27,     1] loss: 1157.442
[28,     1] loss: 1178.043
[29,     1] loss: 1153.800
[30,     1] loss: 1210.201
[31,     1] loss: 1222.772
[32,     1] loss: 1117.951
[33,     1] loss: 1053.755
[34,     1] loss: 1128.967
[35,     1] loss: 1145.016
[36,     1] loss: 1081.805
[37,     1] loss: 1116.003
[38,     1] loss: 1087.176
[39,     1] loss: 1046.181
[40,     1] loss: 1062.243
[41,     1] loss: 1027.117
[42,     1] loss: 1122.297
[43,     1] loss: 951.352
[44,     1] loss: 1014.609
[45,     1] loss: 1054.554
[46,     1] loss: 946.420
[47,     1] loss: 1122.732
[48,     1] loss: 988.367
[49,     1] loss: 1066.127
[50,     1] loss: 1018.553
[51,     1] loss: 963.230
[52,     1] loss: 900.170
[53,     1] loss: 961.912
[54,     1] loss: 932.091
[55,     1] loss: 970.772
[56,     1] loss: 824.868
[57,     1] loss: 915.451
[58,     1] loss: 838.199
[59,     1] loss: 882.514
[60,     1] loss: 869.420
[61,     1] loss: 749.655
[62,     1] loss: 820.878
[63,     1] loss: 832.772
[64,     1] loss: 824.368
[65,     1] loss: 862.326
[66,     1] loss: 812.208
[67,     1] loss: 757.079
[68,     1] loss: 905.642
[69,     1] loss: 710.927
[70,     1] loss: 769.381
[71,     1] loss: 736.070
[72,     1] loss: 750.122
[73,     1] loss: 728.338
[74,     1] loss: 726.076
[75,     1] loss: 699.947
[76,     1] loss: 738.399
[77,     1] loss: 705.575
[78,     1] loss: 826.102
[79,     1] loss: 789.207
[80,     1] loss: 639.272
[81,     1] loss: 717.958
[82,     1] loss: 679.352
[83,     1] loss: 705.734
[84,     1] loss: 779.536
[85,     1] loss: 729.073
[86,     1] loss: 656.426
[87,     1] loss: 682.440
[88,     1] loss: 641.508
Early stopping applied (best metric=0.3795105516910553)
Finished Training
Total time taken: 14.611609697341919
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1581.373
[2,     1] loss: 1587.050
[3,     1] loss: 1580.108
[4,     1] loss: 1583.115
[5,     1] loss: 1581.797
[6,     1] loss: 1576.595
[7,     1] loss: 1577.499
[8,     1] loss: 1576.751
[9,     1] loss: 1568.988
[10,     1] loss: 1562.729
[11,     1] loss: 1547.053
[12,     1] loss: 1524.956
[13,     1] loss: 1507.700
[14,     1] loss: 1486.810
[15,     1] loss: 1444.253
[16,     1] loss: 1425.032
[17,     1] loss: 1399.447
[18,     1] loss: 1379.466
[19,     1] loss: 1340.177
[20,     1] loss: 1357.502
[21,     1] loss: 1299.769
[22,     1] loss: 1326.333
[23,     1] loss: 1307.783
[24,     1] loss: 1295.092
[25,     1] loss: 1263.532
[26,     1] loss: 1318.513
[27,     1] loss: 1209.181
[28,     1] loss: 1224.320
[29,     1] loss: 1196.883
[30,     1] loss: 1247.000
[31,     1] loss: 1178.597
[32,     1] loss: 1161.908
[33,     1] loss: 1141.427
[34,     1] loss: 1118.318
[35,     1] loss: 1191.333
[36,     1] loss: 1109.703
[37,     1] loss: 1083.408
[38,     1] loss: 1102.591
[39,     1] loss: 1070.678
[40,     1] loss: 1056.198
[41,     1] loss: 967.743
[42,     1] loss: 975.770
[43,     1] loss: 1094.013
[44,     1] loss: 1178.972
[45,     1] loss: 1073.886
[46,     1] loss: 1023.717
[47,     1] loss: 1073.340
[48,     1] loss: 986.111
[49,     1] loss: 986.555
[50,     1] loss: 1098.104
[51,     1] loss: 962.649
[52,     1] loss: 913.935
[53,     1] loss: 952.435
[54,     1] loss: 900.187
[55,     1] loss: 909.522
[56,     1] loss: 948.939
[57,     1] loss: 936.222
[58,     1] loss: 934.307
[59,     1] loss: 1047.629
[60,     1] loss: 1023.851
[61,     1] loss: 1019.481
[62,     1] loss: 979.590
[63,     1] loss: 945.600
[64,     1] loss: 963.236
[65,     1] loss: 898.725
[66,     1] loss: 1028.086
[67,     1] loss: 919.192
[68,     1] loss: 948.408
[69,     1] loss: 913.123
[70,     1] loss: 848.775
[71,     1] loss: 858.071
[72,     1] loss: 878.666
[73,     1] loss: 817.282
[74,     1] loss: 876.027
[75,     1] loss: 813.665
[76,     1] loss: 877.676
[77,     1] loss: 796.391
[78,     1] loss: 838.147
[79,     1] loss: 788.204
[80,     1] loss: 747.744
[81,     1] loss: 790.030
[82,     1] loss: 785.643
[83,     1] loss: 790.642
[84,     1] loss: 857.942
[85,     1] loss: 773.885
[86,     1] loss: 905.528
[87,     1] loss: 779.327
[88,     1] loss: 954.401
[89,     1] loss: 721.602
[90,     1] loss: 780.920
Early stopping applied (best metric=0.35779309272766113)
Finished Training
Total time taken: 14.63915228843689
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1586.566
[2,     1] loss: 1584.107
[3,     1] loss: 1593.303
[4,     1] loss: 1582.586
[5,     1] loss: 1580.802
[6,     1] loss: 1578.192
[7,     1] loss: 1580.801
[8,     1] loss: 1578.639
[9,     1] loss: 1567.547
[10,     1] loss: 1553.784
[11,     1] loss: 1530.752
[12,     1] loss: 1502.981
[13,     1] loss: 1453.301
[14,     1] loss: 1441.625
[15,     1] loss: 1402.125
[16,     1] loss: 1389.294
[17,     1] loss: 1338.828
[18,     1] loss: 1333.934
[19,     1] loss: 1284.642
[20,     1] loss: 1275.208
[21,     1] loss: 1310.460
[22,     1] loss: 1242.627
[23,     1] loss: 1226.526
[24,     1] loss: 1179.701
[25,     1] loss: 1286.879
[26,     1] loss: 1227.406
[27,     1] loss: 1318.654
[28,     1] loss: 1190.142
[29,     1] loss: 1267.055
[30,     1] loss: 1221.484
[31,     1] loss: 1216.848
[32,     1] loss: 1135.968
[33,     1] loss: 1147.427
[34,     1] loss: 1179.877
[35,     1] loss: 1133.741
[36,     1] loss: 1084.565
[37,     1] loss: 1061.263
[38,     1] loss: 1078.530
[39,     1] loss: 1069.847
[40,     1] loss: 1075.702
[41,     1] loss: 1049.109
[42,     1] loss: 1056.173
[43,     1] loss: 1008.280
[44,     1] loss: 1021.439
[45,     1] loss: 1055.488
[46,     1] loss: 1044.298
[47,     1] loss: 979.520
[48,     1] loss: 1070.847
[49,     1] loss: 1478.212
[50,     1] loss: 1028.353
[51,     1] loss: 1223.942
[52,     1] loss: 1094.063
[53,     1] loss: 1064.771
[54,     1] loss: 1225.125
[55,     1] loss: 1061.493
[56,     1] loss: 1132.541
[57,     1] loss: 1096.078
[58,     1] loss: 1032.163
[59,     1] loss: 1001.895
[60,     1] loss: 990.023
[61,     1] loss: 930.375
[62,     1] loss: 1003.407
[63,     1] loss: 940.233
[64,     1] loss: 940.465
[65,     1] loss: 904.837
[66,     1] loss: 894.235
[67,     1] loss: 892.653
[68,     1] loss: 913.922
[69,     1] loss: 919.335
[70,     1] loss: 1021.700
[71,     1] loss: 910.166
[72,     1] loss: 963.800
[73,     1] loss: 842.550
[74,     1] loss: 883.274
[75,     1] loss: 896.324
[76,     1] loss: 853.000
[77,     1] loss: 860.326
[78,     1] loss: 846.368
[79,     1] loss: 773.484
[80,     1] loss: 828.354
[81,     1] loss: 767.578
[82,     1] loss: 895.575
[83,     1] loss: 801.452
[84,     1] loss: 793.320
[85,     1] loss: 771.131
[86,     1] loss: 848.995
[87,     1] loss: 808.969
[88,     1] loss: 747.747
[89,     1] loss: 768.312
[90,     1] loss: 707.494
[91,     1] loss: 732.517
[92,     1] loss: 769.475
[93,     1] loss: 745.487
[94,     1] loss: 690.491
[95,     1] loss: 734.098
[96,     1] loss: 752.701
[97,     1] loss: 914.879
[98,     1] loss: 1284.257
[99,     1] loss: 753.801
[100,     1] loss: 1064.411
[101,     1] loss: 780.209
[102,     1] loss: 1007.213
[103,     1] loss: 870.474
[104,     1] loss: 885.408
[105,     1] loss: 915.296
[106,     1] loss: 789.472
[107,     1] loss: 891.759
[108,     1] loss: 757.497
[109,     1] loss: 761.960
[110,     1] loss: 746.021
[111,     1] loss: 756.215
[112,     1] loss: 775.217
[113,     1] loss: 749.751
[114,     1] loss: 732.687
[115,     1] loss: 749.435
[116,     1] loss: 691.269
[117,     1] loss: 665.395
[118,     1] loss: 683.916
[119,     1] loss: 747.168
[120,     1] loss: 720.064
[121,     1] loss: 689.157
[122,     1] loss: 713.195
[123,     1] loss: 778.799
[124,     1] loss: 604.068
[125,     1] loss: 692.999
[126,     1] loss: 802.027
[127,     1] loss: 1139.702
[128,     1] loss: 971.315
[129,     1] loss: 658.676
[130,     1] loss: 885.906
[131,     1] loss: 695.813
[132,     1] loss: 741.387
[133,     1] loss: 666.802
[134,     1] loss: 771.322
[135,     1] loss: 630.943
[136,     1] loss: 653.898
[137,     1] loss: 689.823
[138,     1] loss: 635.777
[139,     1] loss: 681.507
[140,     1] loss: 747.054
[141,     1] loss: 623.779
[142,     1] loss: 695.694
[143,     1] loss: 792.014
[144,     1] loss: 715.649
[145,     1] loss: 602.354
[146,     1] loss: 702.291
[147,     1] loss: 664.897
[148,     1] loss: 652.080
[149,     1] loss: 636.799
[150,     1] loss: 619.123
Early stopping applied (best metric=0.33843597769737244)
Finished Training
Total time taken: 21.4721896648407
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1593.329
[2,     1] loss: 1580.340
[3,     1] loss: 1587.776
[4,     1] loss: 1586.086
[5,     1] loss: 1580.012
[6,     1] loss: 1578.593
[7,     1] loss: 1574.872
[8,     1] loss: 1569.641
[9,     1] loss: 1566.211
[10,     1] loss: 1544.194
[11,     1] loss: 1525.205
[12,     1] loss: 1499.175
[13,     1] loss: 1472.432
[14,     1] loss: 1445.611
[15,     1] loss: 1404.095
[16,     1] loss: 1374.501
[17,     1] loss: 1376.659
[18,     1] loss: 1318.115
[19,     1] loss: 1275.947
[20,     1] loss: 1327.438
[21,     1] loss: 1342.665
[22,     1] loss: 1366.336
[23,     1] loss: 1319.613
[24,     1] loss: 1305.966
[25,     1] loss: 1296.436
[26,     1] loss: 1259.433
[27,     1] loss: 1295.840
[28,     1] loss: 1210.768
[29,     1] loss: 1303.264
[30,     1] loss: 1218.602
[31,     1] loss: 1201.062
[32,     1] loss: 1182.490
[33,     1] loss: 1257.117
[34,     1] loss: 1139.534
[35,     1] loss: 1126.291
[36,     1] loss: 1191.180
[37,     1] loss: 1125.471
[38,     1] loss: 1227.637
[39,     1] loss: 1188.395
[40,     1] loss: 1205.021
[41,     1] loss: 1133.732
[42,     1] loss: 1114.464
[43,     1] loss: 1138.169
[44,     1] loss: 1116.161
[45,     1] loss: 1088.990
[46,     1] loss: 1095.654
[47,     1] loss: 1088.610
[48,     1] loss: 1100.986
[49,     1] loss: 1015.682
[50,     1] loss: 998.647
[51,     1] loss: 1002.651
[52,     1] loss: 1039.784
[53,     1] loss: 890.626
[54,     1] loss: 1068.298
[55,     1] loss: 1041.227
[56,     1] loss: 974.084
[57,     1] loss: 967.109
[58,     1] loss: 917.208
[59,     1] loss: 890.042
[60,     1] loss: 912.688
[61,     1] loss: 917.597
[62,     1] loss: 928.035
[63,     1] loss: 955.413
[64,     1] loss: 854.174
[65,     1] loss: 851.357
[66,     1] loss: 897.582
[67,     1] loss: 916.631
[68,     1] loss: 794.190
[69,     1] loss: 813.200
[70,     1] loss: 982.180
[71,     1] loss: 1061.197
[72,     1] loss: 955.474
[73,     1] loss: 855.559
Early stopping applied (best metric=0.38392218947410583)
Finished Training
Total time taken: 10.375572681427002
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1585.380
[2,     1] loss: 1581.437
[3,     1] loss: 1580.924
[4,     1] loss: 1586.612
[5,     1] loss: 1582.021
[6,     1] loss: 1581.650
[7,     1] loss: 1571.774
[8,     1] loss: 1578.117
[9,     1] loss: 1563.913
[10,     1] loss: 1554.617
[11,     1] loss: 1525.545
[12,     1] loss: 1493.784
[13,     1] loss: 1463.354
[14,     1] loss: 1428.202
[15,     1] loss: 1397.397
[16,     1] loss: 1352.012
[17,     1] loss: 1374.491
[18,     1] loss: 1331.468
[19,     1] loss: 1306.641
[20,     1] loss: 1363.397
[21,     1] loss: 1301.275
[22,     1] loss: 1364.280
[23,     1] loss: 1282.831
[24,     1] loss: 1307.569
[25,     1] loss: 1291.108
[26,     1] loss: 1277.534
[27,     1] loss: 1281.614
[28,     1] loss: 1234.230
[29,     1] loss: 1275.733
[30,     1] loss: 1236.441
[31,     1] loss: 1242.263
[32,     1] loss: 1252.401
[33,     1] loss: 1217.474
[34,     1] loss: 1223.139
[35,     1] loss: 1213.582
[36,     1] loss: 1209.861
[37,     1] loss: 1132.330
[38,     1] loss: 1178.438
[39,     1] loss: 1144.590
[40,     1] loss: 1181.498
[41,     1] loss: 1146.696
[42,     1] loss: 1122.751
[43,     1] loss: 1078.798
[44,     1] loss: 1134.646
[45,     1] loss: 1044.104
[46,     1] loss: 1108.314
[47,     1] loss: 1055.246
[48,     1] loss: 1077.344
[49,     1] loss: 1091.076
[50,     1] loss: 1085.998
[51,     1] loss: 989.730
[52,     1] loss: 1008.433
[53,     1] loss: 993.454
[54,     1] loss: 973.974
[55,     1] loss: 956.809
[56,     1] loss: 1035.320
[57,     1] loss: 2019.965
[58,     1] loss: 993.352
[59,     1] loss: 1376.826
[60,     1] loss: 1061.066
[61,     1] loss: 1162.345
[62,     1] loss: 1170.898
[63,     1] loss: 1179.281
[64,     1] loss: 1154.518
[65,     1] loss: 1093.798
[66,     1] loss: 1059.633
[67,     1] loss: 1092.995
[68,     1] loss: 1033.083
[69,     1] loss: 1039.543
[70,     1] loss: 1126.469
Early stopping applied (best metric=0.4256964325904846)
Finished Training
Total time taken: 10.06414532661438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1587.880
[2,     1] loss: 1588.271
[3,     1] loss: 1588.418
[4,     1] loss: 1587.461
[5,     1] loss: 1584.083
[6,     1] loss: 1583.419
[7,     1] loss: 1580.750
[8,     1] loss: 1579.354
[9,     1] loss: 1572.440
[10,     1] loss: 1566.256
[11,     1] loss: 1549.437
[12,     1] loss: 1536.772
[13,     1] loss: 1506.273
[14,     1] loss: 1474.566
[15,     1] loss: 1440.899
[16,     1] loss: 1423.199
[17,     1] loss: 1366.874
[18,     1] loss: 1367.628
[19,     1] loss: 1328.695
[20,     1] loss: 1323.718
[21,     1] loss: 1326.251
[22,     1] loss: 1310.048
[23,     1] loss: 1306.520
[24,     1] loss: 1308.933
[25,     1] loss: 1282.789
[26,     1] loss: 1212.308
[27,     1] loss: 1248.450
[28,     1] loss: 1223.983
[29,     1] loss: 1254.559
[30,     1] loss: 1217.159
[31,     1] loss: 1177.688
[32,     1] loss: 1194.631
[33,     1] loss: 1152.736
[34,     1] loss: 1196.190
[35,     1] loss: 1183.452
[36,     1] loss: 1138.785
[37,     1] loss: 1123.150
[38,     1] loss: 1151.104
[39,     1] loss: 1134.528
[40,     1] loss: 1077.229
[41,     1] loss: 1066.212
[42,     1] loss: 1119.776
[43,     1] loss: 1077.621
[44,     1] loss: 1079.379
[45,     1] loss: 1070.157
[46,     1] loss: 1100.590
[47,     1] loss: 1143.211
[48,     1] loss: 990.001
[49,     1] loss: 1029.073
[50,     1] loss: 1001.811
[51,     1] loss: 966.307
[52,     1] loss: 1054.938
[53,     1] loss: 1002.560
[54,     1] loss: 1062.221
[55,     1] loss: 1147.931
[56,     1] loss: 986.512
[57,     1] loss: 976.394
[58,     1] loss: 978.211
[59,     1] loss: 940.596
[60,     1] loss: 905.750
[61,     1] loss: 962.270
[62,     1] loss: 810.974
[63,     1] loss: 874.773
[64,     1] loss: 877.673
[65,     1] loss: 859.126
[66,     1] loss: 865.293
[67,     1] loss: 837.793
[68,     1] loss: 960.842
[69,     1] loss: 897.587
[70,     1] loss: 786.283
[71,     1] loss: 833.640
[72,     1] loss: 832.642
[73,     1] loss: 827.160
[74,     1] loss: 919.227
[75,     1] loss: 793.922
[76,     1] loss: 726.299
[77,     1] loss: 686.510
[78,     1] loss: 793.751
[79,     1] loss: 773.458
[80,     1] loss: 796.378
[81,     1] loss: 712.956
[82,     1] loss: 726.281
[83,     1] loss: 714.873
[84,     1] loss: 714.402
[85,     1] loss: 940.132
[86,     1] loss: 1175.941
[87,     1] loss: 882.041
[88,     1] loss: 762.340
[89,     1] loss: 879.040
[90,     1] loss: 817.526
[91,     1] loss: 812.494
[92,     1] loss: 876.242
[93,     1] loss: 726.782
[94,     1] loss: 758.619
[95,     1] loss: 716.073
[96,     1] loss: 784.279
[97,     1] loss: 711.075
[98,     1] loss: 758.399
[99,     1] loss: 782.229
[100,     1] loss: 690.479
[101,     1] loss: 703.811
[102,     1] loss: 748.256
[103,     1] loss: 661.163
Early stopping applied (best metric=0.410225510597229)
Finished Training
Total time taken: 14.78458023071289
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1584.475
[2,     1] loss: 1583.393
[3,     1] loss: 1583.565
[4,     1] loss: 1582.011
[5,     1] loss: 1578.712
[6,     1] loss: 1585.434
[7,     1] loss: 1582.313
[8,     1] loss: 1581.718
[9,     1] loss: 1579.081
[10,     1] loss: 1577.678
[11,     1] loss: 1576.988
[12,     1] loss: 1574.071
[13,     1] loss: 1565.724
[14,     1] loss: 1548.375
[15,     1] loss: 1541.882
[16,     1] loss: 1504.830
[17,     1] loss: 1490.392
[18,     1] loss: 1442.997
[19,     1] loss: 1424.062
[20,     1] loss: 1372.645
[21,     1] loss: 1356.309
[22,     1] loss: 1355.467
[23,     1] loss: 1371.125
[24,     1] loss: 1345.814
[25,     1] loss: 1289.298
[26,     1] loss: 1312.164
[27,     1] loss: 1296.474
[28,     1] loss: 1306.282
[29,     1] loss: 1295.755
[30,     1] loss: 1284.557
[31,     1] loss: 1244.499
[32,     1] loss: 1299.877
[33,     1] loss: 1252.130
[34,     1] loss: 1187.199
[35,     1] loss: 1218.874
[36,     1] loss: 1196.777
[37,     1] loss: 1174.810
[38,     1] loss: 1178.330
[39,     1] loss: 1236.298
[40,     1] loss: 1116.425
[41,     1] loss: 1125.048
[42,     1] loss: 1150.962
[43,     1] loss: 1088.468
[44,     1] loss: 1088.756
[45,     1] loss: 1104.746
[46,     1] loss: 1072.993
[47,     1] loss: 1030.404
[48,     1] loss: 1063.809
[49,     1] loss: 1018.040
[50,     1] loss: 985.168
[51,     1] loss: 1066.045
[52,     1] loss: 987.634
[53,     1] loss: 982.336
[54,     1] loss: 931.869
[55,     1] loss: 959.664
[56,     1] loss: 1258.656
[57,     1] loss: 869.854
[58,     1] loss: 1244.411
[59,     1] loss: 940.940
[60,     1] loss: 1111.148
[61,     1] loss: 1057.204
[62,     1] loss: 1047.562
[63,     1] loss: 1069.348
[64,     1] loss: 998.634
[65,     1] loss: 919.373
[66,     1] loss: 1087.115
[67,     1] loss: 869.700
[68,     1] loss: 959.922
[69,     1] loss: 927.851
[70,     1] loss: 947.815
[71,     1] loss: 878.016
[72,     1] loss: 901.467
[73,     1] loss: 865.814
[74,     1] loss: 810.932
[75,     1] loss: 801.988
[76,     1] loss: 835.545
[77,     1] loss: 829.480
[78,     1] loss: 749.130
[79,     1] loss: 804.638
[80,     1] loss: 811.965
[81,     1] loss: 832.634
[82,     1] loss: 783.435
[83,     1] loss: 834.831
[84,     1] loss: 1139.537
[85,     1] loss: 782.971
[86,     1] loss: 912.624
[87,     1] loss: 819.036
[88,     1] loss: 891.604
[89,     1] loss: 844.807
[90,     1] loss: 858.092
[91,     1] loss: 806.319
[92,     1] loss: 819.638
[93,     1] loss: 785.524
[94,     1] loss: 831.377
Early stopping applied (best metric=0.3979378044605255)
Finished Training
Total time taken: 13.562055110931396
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1582.274
[2,     1] loss: 1579.288
[3,     1] loss: 1585.426
[4,     1] loss: 1582.954
[5,     1] loss: 1579.948
[6,     1] loss: 1577.888
[7,     1] loss: 1580.543
[8,     1] loss: 1581.941
[9,     1] loss: 1570.293
[10,     1] loss: 1561.281
[11,     1] loss: 1544.187
[12,     1] loss: 1522.176
[13,     1] loss: 1489.941
[14,     1] loss: 1474.522
[15,     1] loss: 1449.061
[16,     1] loss: 1393.176
[17,     1] loss: 1395.797
[18,     1] loss: 1370.238
[19,     1] loss: 1364.512
[20,     1] loss: 1372.265
[21,     1] loss: 1297.374
[22,     1] loss: 1290.174
[23,     1] loss: 1293.057
[24,     1] loss: 1259.898
[25,     1] loss: 1257.813
[26,     1] loss: 1251.165
[27,     1] loss: 1155.365
[28,     1] loss: 1239.115
[29,     1] loss: 1246.489
[30,     1] loss: 1212.398
[31,     1] loss: 1134.153
[32,     1] loss: 1111.688
[33,     1] loss: 1178.061
[34,     1] loss: 1171.226
[35,     1] loss: 1169.062
[36,     1] loss: 1180.479
[37,     1] loss: 1138.458
[38,     1] loss: 1094.547
[39,     1] loss: 1149.298
[40,     1] loss: 1123.936
[41,     1] loss: 1052.289
[42,     1] loss: 1086.029
[43,     1] loss: 1108.216
[44,     1] loss: 1030.415
[45,     1] loss: 1018.950
[46,     1] loss: 1003.801
[47,     1] loss: 1016.316
[48,     1] loss: 914.694
[49,     1] loss: 1001.950
[50,     1] loss: 942.370
[51,     1] loss: 975.324
[52,     1] loss: 974.620
[53,     1] loss: 959.572
[54,     1] loss: 1000.188
[55,     1] loss: 1096.838
[56,     1] loss: 890.444
[57,     1] loss: 955.051
[58,     1] loss: 922.726
[59,     1] loss: 973.870
[60,     1] loss: 898.757
[61,     1] loss: 915.794
[62,     1] loss: 875.058
[63,     1] loss: 905.932
[64,     1] loss: 808.074
[65,     1] loss: 894.734
[66,     1] loss: 904.572
[67,     1] loss: 1162.580
[68,     1] loss: 947.043
[69,     1] loss: 838.447
[70,     1] loss: 903.201
[71,     1] loss: 845.629
[72,     1] loss: 847.310
[73,     1] loss: 935.669
[74,     1] loss: 884.029
[75,     1] loss: 821.169
[76,     1] loss: 779.526
[77,     1] loss: 811.422
[78,     1] loss: 793.268
Early stopping applied (best metric=0.35900333523750305)
Finished Training
Total time taken: 11.265168905258179
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1583.932
[2,     1] loss: 1584.524
[3,     1] loss: 1584.847
[4,     1] loss: 1579.697
[5,     1] loss: 1579.998
[6,     1] loss: 1573.941
[7,     1] loss: 1565.412
[8,     1] loss: 1553.848
[9,     1] loss: 1525.724
[10,     1] loss: 1489.096
[11,     1] loss: 1445.888
[12,     1] loss: 1423.668
[13,     1] loss: 1393.285
[14,     1] loss: 1354.463
[15,     1] loss: 1362.272
[16,     1] loss: 1292.189
[17,     1] loss: 1343.068
[18,     1] loss: 1337.129
[19,     1] loss: 1271.599
[20,     1] loss: 1203.554
[21,     1] loss: 1308.422
[22,     1] loss: 1306.857
[23,     1] loss: 1302.440
[24,     1] loss: 1256.513
[25,     1] loss: 1208.546
[26,     1] loss: 1204.867
[27,     1] loss: 1153.221
[28,     1] loss: 1130.353
[29,     1] loss: 1092.329
[30,     1] loss: 1164.638
[31,     1] loss: 1178.817
[32,     1] loss: 1328.902
[33,     1] loss: 1139.837
[34,     1] loss: 1109.370
[35,     1] loss: 1054.943
[36,     1] loss: 1154.504
[37,     1] loss: 1088.813
[38,     1] loss: 1201.369
[39,     1] loss: 1099.531
[40,     1] loss: 1059.940
[41,     1] loss: 1118.854
[42,     1] loss: 1052.035
[43,     1] loss: 1093.010
[44,     1] loss: 985.308
[45,     1] loss: 1023.323
[46,     1] loss: 1029.189
[47,     1] loss: 1019.867
[48,     1] loss: 909.910
[49,     1] loss: 1013.336
[50,     1] loss: 977.290
[51,     1] loss: 997.764
[52,     1] loss: 970.366
[53,     1] loss: 924.660
[54,     1] loss: 881.002
[55,     1] loss: 930.146
[56,     1] loss: 1032.103
[57,     1] loss: 1224.023
[58,     1] loss: 953.271
[59,     1] loss: 1042.915
[60,     1] loss: 971.529
[61,     1] loss: 948.365
[62,     1] loss: 969.895
[63,     1] loss: 941.825
[64,     1] loss: 984.569
[65,     1] loss: 895.758
[66,     1] loss: 946.890
[67,     1] loss: 867.015
[68,     1] loss: 995.211
[69,     1] loss: 872.197
[70,     1] loss: 951.968
[71,     1] loss: 786.810
[72,     1] loss: 809.227
[73,     1] loss: 797.488
[74,     1] loss: 910.688
[75,     1] loss: 788.842
[76,     1] loss: 899.184
[77,     1] loss: 852.366
Early stopping applied (best metric=0.37147584557533264)
Finished Training
Total time taken: 11.077549695968628
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1584.861
[2,     1] loss: 1587.614
[3,     1] loss: 1579.487
[4,     1] loss: 1600.968
[5,     1] loss: 1577.216
[6,     1] loss: 1581.076
[7,     1] loss: 1577.257
[8,     1] loss: 1584.125
[9,     1] loss: 1577.802
[10,     1] loss: 1572.082
[11,     1] loss: 1566.242
[12,     1] loss: 1560.858
[13,     1] loss: 1554.052
[14,     1] loss: 1519.608
[15,     1] loss: 1506.398
[16,     1] loss: 1468.152
[17,     1] loss: 1410.255
[18,     1] loss: 1390.477
[19,     1] loss: 1318.549
[20,     1] loss: 1296.531
[21,     1] loss: 1273.428
[22,     1] loss: 1291.271
[23,     1] loss: 1355.746
[24,     1] loss: 1305.403
[25,     1] loss: 1280.410
[26,     1] loss: 1291.807
[27,     1] loss: 1280.781
[28,     1] loss: 1261.856
[29,     1] loss: 1273.453
[30,     1] loss: 1218.687
[31,     1] loss: 1228.876
[32,     1] loss: 1147.637
[33,     1] loss: 1272.561
[34,     1] loss: 1145.238
[35,     1] loss: 1206.370
[36,     1] loss: 1123.316
[37,     1] loss: 1260.335
[38,     1] loss: 1144.897
[39,     1] loss: 1158.043
[40,     1] loss: 1244.509
[41,     1] loss: 1148.846
[42,     1] loss: 1135.139
[43,     1] loss: 1110.269
[44,     1] loss: 1101.648
[45,     1] loss: 1166.664
[46,     1] loss: 1044.635
[47,     1] loss: 1042.269
[48,     1] loss: 1018.939
[49,     1] loss: 1020.994
[50,     1] loss: 1058.021
[51,     1] loss: 1078.153
[52,     1] loss: 1030.029
[53,     1] loss: 971.829
[54,     1] loss: 1002.159
[55,     1] loss: 1192.987
[56,     1] loss: 985.817
[57,     1] loss: 1016.704
[58,     1] loss: 954.474
[59,     1] loss: 1076.247
[60,     1] loss: 1038.932
[61,     1] loss: 1124.954
[62,     1] loss: 957.815
[63,     1] loss: 1038.954
[64,     1] loss: 936.575
[65,     1] loss: 880.654
[66,     1] loss: 975.567
[67,     1] loss: 933.820
[68,     1] loss: 868.916
[69,     1] loss: 845.966
[70,     1] loss: 881.366
[71,     1] loss: 876.437
[72,     1] loss: 866.112
[73,     1] loss: 807.065
[74,     1] loss: 768.272
[75,     1] loss: 848.389
[76,     1] loss: 813.274
[77,     1] loss: 797.860
[78,     1] loss: 796.830
[79,     1] loss: 747.617
[80,     1] loss: 834.334
[81,     1] loss: 803.708
[82,     1] loss: 822.918
[83,     1] loss: 1388.868
[84,     1] loss: 936.019
[85,     1] loss: 964.921
[86,     1] loss: 908.649
[87,     1] loss: 955.491
[88,     1] loss: 879.757
[89,     1] loss: 970.826
[90,     1] loss: 918.047
[91,     1] loss: 794.371
[92,     1] loss: 968.974
[93,     1] loss: 841.641
[94,     1] loss: 857.170
[95,     1] loss: 812.768
[96,     1] loss: 827.640
[97,     1] loss: 732.778
[98,     1] loss: 726.752
[99,     1] loss: 753.092
[100,     1] loss: 770.182
[101,     1] loss: 753.125
[102,     1] loss: 760.622
[103,     1] loss: 702.266
[104,     1] loss: 756.518
[105,     1] loss: 699.164
Early stopping applied (best metric=0.4221092462539673)
Finished Training
Total time taken: 14.913682222366333
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1585.675
[2,     1] loss: 1586.636
[3,     1] loss: 1583.078
[4,     1] loss: 1582.142
[5,     1] loss: 1582.275
[6,     1] loss: 1578.957
[7,     1] loss: 1579.117
[8,     1] loss: 1577.063
[9,     1] loss: 1571.952
[10,     1] loss: 1559.873
[11,     1] loss: 1535.270
[12,     1] loss: 1517.699
[13,     1] loss: 1492.072
[14,     1] loss: 1463.370
[15,     1] loss: 1437.066
[16,     1] loss: 1394.427
[17,     1] loss: 1414.795
[18,     1] loss: 1350.078
[19,     1] loss: 1326.473
[20,     1] loss: 1352.592
[21,     1] loss: 1374.394
[22,     1] loss: 1317.196
[23,     1] loss: 1271.116
[24,     1] loss: 1265.195
[25,     1] loss: 1328.194
[26,     1] loss: 1249.139
[27,     1] loss: 1273.852
[28,     1] loss: 1297.247
[29,     1] loss: 1236.588
[30,     1] loss: 1253.820
[31,     1] loss: 1193.096
[32,     1] loss: 1171.017
[33,     1] loss: 1178.103
[34,     1] loss: 1178.260
[35,     1] loss: 1201.012
[36,     1] loss: 1103.546
[37,     1] loss: 1176.912
[38,     1] loss: 1109.789
[39,     1] loss: 1054.601
[40,     1] loss: 1093.708
[41,     1] loss: 1022.931
[42,     1] loss: 1104.046
[43,     1] loss: 1129.381
[44,     1] loss: 1177.659
[45,     1] loss: 1073.437
[46,     1] loss: 1078.633
[47,     1] loss: 1083.596
[48,     1] loss: 1057.662
[49,     1] loss: 1042.570
[50,     1] loss: 1005.217
[51,     1] loss: 1000.230
[52,     1] loss: 994.992
[53,     1] loss: 1073.980
[54,     1] loss: 889.873
[55,     1] loss: 954.408
[56,     1] loss: 948.982
[57,     1] loss: 871.456
[58,     1] loss: 946.462
[59,     1] loss: 866.288
[60,     1] loss: 922.947
[61,     1] loss: 973.963
[62,     1] loss: 910.108
[63,     1] loss: 959.959
[64,     1] loss: 999.252
[65,     1] loss: 1076.389
[66,     1] loss: 843.620
[67,     1] loss: 1001.588
[68,     1] loss: 827.912
[69,     1] loss: 989.790
[70,     1] loss: 943.224
[71,     1] loss: 925.173
[72,     1] loss: 824.731
[73,     1] loss: 951.606
[74,     1] loss: 845.871
[75,     1] loss: 919.170
[76,     1] loss: 775.227
[77,     1] loss: 944.863
[78,     1] loss: 775.106
[79,     1] loss: 984.169
[80,     1] loss: 737.564
[81,     1] loss: 923.843
[82,     1] loss: 754.657
[83,     1] loss: 803.770
[84,     1] loss: 831.088
[85,     1] loss: 849.486
[86,     1] loss: 787.608
[87,     1] loss: 824.108
[88,     1] loss: 815.182
[89,     1] loss: 714.905
[90,     1] loss: 827.805
[91,     1] loss: 724.097
[92,     1] loss: 793.814
[93,     1] loss: 733.505
[94,     1] loss: 708.419
[95,     1] loss: 697.348
[96,     1] loss: 696.600
[97,     1] loss: 765.571
[98,     1] loss: 707.758
[99,     1] loss: 711.935
[100,     1] loss: 931.371
[101,     1] loss: 952.321
[102,     1] loss: 700.285
[103,     1] loss: 976.286
[104,     1] loss: 735.287
[105,     1] loss: 947.701
[106,     1] loss: 667.775
[107,     1] loss: 872.853
[108,     1] loss: 694.179
[109,     1] loss: 761.726
[110,     1] loss: 721.850
[111,     1] loss: 749.561
[112,     1] loss: 688.563
[113,     1] loss: 700.443
[114,     1] loss: 631.469
[115,     1] loss: 678.489
[116,     1] loss: 628.618
[117,     1] loss: 653.386
[118,     1] loss: 748.783
[119,     1] loss: 641.045
[120,     1] loss: 633.453
[121,     1] loss: 744.102
[122,     1] loss: 718.425
[123,     1] loss: 640.737
[124,     1] loss: 595.177
[125,     1] loss: 613.476
[126,     1] loss: 587.716
Early stopping applied (best metric=0.33991318941116333)
Finished Training
Total time taken: 18.079100847244263
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1593.599
[2,     1] loss: 1582.690
[3,     1] loss: 1589.886
[4,     1] loss: 1588.011
[5,     1] loss: 1590.269
[6,     1] loss: 1578.687
[7,     1] loss: 1575.707
[8,     1] loss: 1574.052
[9,     1] loss: 1573.533
[10,     1] loss: 1557.228
[11,     1] loss: 1542.067
[12,     1] loss: 1509.842
[13,     1] loss: 1468.531
[14,     1] loss: 1447.747
[15,     1] loss: 1416.843
[16,     1] loss: 1409.946
[17,     1] loss: 1388.499
[18,     1] loss: 1350.184
[19,     1] loss: 1316.260
[20,     1] loss: 1355.820
[21,     1] loss: 1298.726
[22,     1] loss: 1283.034
[23,     1] loss: 1277.288
[24,     1] loss: 1306.519
[25,     1] loss: 1255.048
[26,     1] loss: 1269.541
[27,     1] loss: 1225.934
[28,     1] loss: 1195.638
[29,     1] loss: 1221.928
[30,     1] loss: 1235.200
[31,     1] loss: 1201.062
[32,     1] loss: 1057.784
[33,     1] loss: 1213.397
[34,     1] loss: 1191.473
[35,     1] loss: 1111.680
[36,     1] loss: 1113.025
[37,     1] loss: 1133.173
[38,     1] loss: 1105.612
[39,     1] loss: 1035.718
[40,     1] loss: 1091.939
[41,     1] loss: 1046.229
[42,     1] loss: 1045.760
[43,     1] loss: 999.382
[44,     1] loss: 1127.493
[45,     1] loss: 1046.655
[46,     1] loss: 998.788
[47,     1] loss: 967.762
[48,     1] loss: 1009.414
[49,     1] loss: 909.736
[50,     1] loss: 979.580
[51,     1] loss: 922.933
[52,     1] loss: 827.428
[53,     1] loss: 884.766
[54,     1] loss: 967.523
[55,     1] loss: 1318.093
[56,     1] loss: 1074.537
[57,     1] loss: 907.131
[58,     1] loss: 1061.217
[59,     1] loss: 955.565
[60,     1] loss: 972.222
[61,     1] loss: 1065.550
[62,     1] loss: 914.180
[63,     1] loss: 973.614
[64,     1] loss: 936.203
[65,     1] loss: 934.229
[66,     1] loss: 930.535
[67,     1] loss: 922.750
[68,     1] loss: 942.289
[69,     1] loss: 851.604
[70,     1] loss: 810.779
[71,     1] loss: 872.949
[72,     1] loss: 793.266
[73,     1] loss: 816.537
[74,     1] loss: 864.026
[75,     1] loss: 800.408
[76,     1] loss: 820.191
[77,     1] loss: 755.215
[78,     1] loss: 811.781
[79,     1] loss: 919.556
[80,     1] loss: 711.865
[81,     1] loss: 799.992
[82,     1] loss: 939.869
[83,     1] loss: 780.569
[84,     1] loss: 748.867
[85,     1] loss: 790.713
[86,     1] loss: 739.626
[87,     1] loss: 742.829
[88,     1] loss: 671.190
Early stopping applied (best metric=0.44040897488594055)
Finished Training
Total time taken: 12.581645250320435
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1586.106
[2,     1] loss: 1584.212
[3,     1] loss: 1586.972
[4,     1] loss: 1584.710
[5,     1] loss: 1582.219
[6,     1] loss: 1580.522
[7,     1] loss: 1580.020
[8,     1] loss: 1583.914
[9,     1] loss: 1584.051
[10,     1] loss: 1579.594
[11,     1] loss: 1575.805
[12,     1] loss: 1572.170
[13,     1] loss: 1568.005
[14,     1] loss: 1549.285
[15,     1] loss: 1538.951
[16,     1] loss: 1498.783
[17,     1] loss: 1480.718
[18,     1] loss: 1428.665
[19,     1] loss: 1413.681
[20,     1] loss: 1400.806
[21,     1] loss: 1379.179
[22,     1] loss: 1324.380
[23,     1] loss: 1374.723
[24,     1] loss: 1309.866
[25,     1] loss: 1333.506
[26,     1] loss: 1299.802
[27,     1] loss: 1320.449
[28,     1] loss: 1309.695
[29,     1] loss: 1311.397
[30,     1] loss: 1266.016
[31,     1] loss: 1221.505
[32,     1] loss: 1224.538
[33,     1] loss: 1179.193
[34,     1] loss: 1240.018
[35,     1] loss: 1229.081
[36,     1] loss: 1188.686
[37,     1] loss: 1161.246
[38,     1] loss: 1191.669
[39,     1] loss: 1146.042
[40,     1] loss: 1127.772
[41,     1] loss: 1149.908
[42,     1] loss: 1151.769
[43,     1] loss: 1052.885
[44,     1] loss: 1062.168
[45,     1] loss: 1082.206
[46,     1] loss: 1083.345
[47,     1] loss: 1114.523
[48,     1] loss: 1325.388
[49,     1] loss: 1093.409
[50,     1] loss: 1142.513
[51,     1] loss: 1073.774
[52,     1] loss: 1180.242
[53,     1] loss: 1085.666
[54,     1] loss: 1076.642
[55,     1] loss: 1148.280
[56,     1] loss: 1018.271
[57,     1] loss: 1065.509
[58,     1] loss: 1058.547
[59,     1] loss: 937.408
[60,     1] loss: 1022.765
[61,     1] loss: 942.840
[62,     1] loss: 985.616
[63,     1] loss: 962.598
[64,     1] loss: 929.174
[65,     1] loss: 1015.839
[66,     1] loss: 856.267
[67,     1] loss: 968.859
[68,     1] loss: 895.903
[69,     1] loss: 936.445
[70,     1] loss: 811.305
[71,     1] loss: 936.845
[72,     1] loss: 901.427
[73,     1] loss: 867.937
[74,     1] loss: 831.473
[75,     1] loss: 804.563
[76,     1] loss: 796.567
[77,     1] loss: 794.134
[78,     1] loss: 939.112
[79,     1] loss: 984.687
[80,     1] loss: 803.795
[81,     1] loss: 860.765
[82,     1] loss: 839.257
[83,     1] loss: 880.659
[84,     1] loss: 786.003
[85,     1] loss: 936.467
[86,     1] loss: 769.098
[87,     1] loss: 866.388
[88,     1] loss: 735.254
[89,     1] loss: 792.661
[90,     1] loss: 722.900
[91,     1] loss: 728.211
[92,     1] loss: 718.914
[93,     1] loss: 703.458
Early stopping applied (best metric=0.3583470582962036)
Finished Training
Total time taken: 13.258054494857788
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1585.595
[2,     1] loss: 1583.471
[3,     1] loss: 1584.830
[4,     1] loss: 1579.873
[5,     1] loss: 1572.689
[6,     1] loss: 1577.150
[7,     1] loss: 1564.363
[8,     1] loss: 1543.919
[9,     1] loss: 1520.540
[10,     1] loss: 1476.123
[11,     1] loss: 1438.673
[12,     1] loss: 1399.901
[13,     1] loss: 1383.429
[14,     1] loss: 1357.470
[15,     1] loss: 1378.786
[16,     1] loss: 1301.550
[17,     1] loss: 1341.051
[18,     1] loss: 1328.551
[19,     1] loss: 1247.073
[20,     1] loss: 1284.909
[21,     1] loss: 1268.823
[22,     1] loss: 1279.816
[23,     1] loss: 1319.063
[24,     1] loss: 1277.583
[25,     1] loss: 1280.273
[26,     1] loss: 1262.544
[27,     1] loss: 1267.532
[28,     1] loss: 1120.029
[29,     1] loss: 1229.339
[30,     1] loss: 1191.325
[31,     1] loss: 1229.827
[32,     1] loss: 1156.090
[33,     1] loss: 1235.636
[34,     1] loss: 1105.171
[35,     1] loss: 1150.185
[36,     1] loss: 1098.818
[37,     1] loss: 1171.634
[38,     1] loss: 1106.143
[39,     1] loss: 1079.382
[40,     1] loss: 1047.841
[41,     1] loss: 1065.912
[42,     1] loss: 999.710
[43,     1] loss: 977.041
[44,     1] loss: 980.955
[45,     1] loss: 1107.684
[46,     1] loss: 1271.381
[47,     1] loss: 1050.771
[48,     1] loss: 1060.589
[49,     1] loss: 1028.087
[50,     1] loss: 1061.395
[51,     1] loss: 1002.368
[52,     1] loss: 983.657
[53,     1] loss: 990.180
[54,     1] loss: 1040.078
[55,     1] loss: 935.794
[56,     1] loss: 1076.838
[57,     1] loss: 896.183
[58,     1] loss: 950.837
[59,     1] loss: 892.211
[60,     1] loss: 945.423
[61,     1] loss: 842.821
[62,     1] loss: 898.225
[63,     1] loss: 842.106
[64,     1] loss: 1008.930
[65,     1] loss: 843.002
[66,     1] loss: 924.581
[67,     1] loss: 918.443
[68,     1] loss: 812.437
[69,     1] loss: 833.655
[70,     1] loss: 877.649
[71,     1] loss: 879.254
[72,     1] loss: 798.732
[73,     1] loss: 875.620
[74,     1] loss: 781.141
[75,     1] loss: 920.286
[76,     1] loss: 727.583
[77,     1] loss: 884.196
[78,     1] loss: 770.420
[79,     1] loss: 873.201
[80,     1] loss: 737.650
[81,     1] loss: 824.228
[82,     1] loss: 660.863
[83,     1] loss: 832.311
[84,     1] loss: 826.875
[85,     1] loss: 741.131
[86,     1] loss: 707.226
[87,     1] loss: 741.247
[88,     1] loss: 702.093
[89,     1] loss: 652.087
Early stopping applied (best metric=0.38673436641693115)
Finished Training
Total time taken: 12.682250022888184
{'Hydroxylation-K Validation Accuracy': 0.760579196217494, 'Hydroxylation-K Validation Sensitivity': 0.7, 'Hydroxylation-K Validation Specificity': 0.775438596491228, 'Hydroxylation-K Validation Precision': 0.45283091091914623, 'Hydroxylation-K AUC ROC': 0.8169590643274853, 'Hydroxylation-K AUC PR': 0.6448593758146832, 'Hydroxylation-K MCC': 0.41614287563248176, 'Hydroxylation-K F1': 0.543772631287124, 'Validation Loss (Hydroxylation-K)': 0.4438051621119181, 'Hydroxylation-P Validation Accuracy': 0.7745270967632777, 'Hydroxylation-P Validation Sensitivity': 0.8088888888888889, 'Hydroxylation-P Validation Specificity': 0.7672053469000948, 'Hydroxylation-P Validation Precision': 0.42961328880872945, 'Hydroxylation-P AUC ROC': 0.8385562455317804, 'Hydroxylation-P AUC PR': 0.5567846389489398, 'Hydroxylation-P MCC': 0.4672812654880126, 'Hydroxylation-P F1': 0.559893709349414, 'Validation Loss (Hydroxylation-P)': 0.3778241435686747, 'Validation Loss (total)': 0.8216293136278788, 'TimeToTrain': 14.426196893056234}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001964455418392711,
 'learning_rate_Hydroxylation-K': 0.008343212523926367,
 'learning_rate_Hydroxylation-P': 0.0015004771128363037,
 'log_base': 2.3955039869637433,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3985596120,
 'sample_weights': [3.2082292095921536, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.787781357688906,
 'weight_decay_Hydroxylation-K': 9.559482926311603,
 'weight_decay_Hydroxylation-P': 5.829040376041974}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1324.115
[2,     1] loss: 1311.071
[3,     1] loss: 1313.343
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005723674252079675,
 'learning_rate_Hydroxylation-K': 0.0012195469945265073,
 'learning_rate_Hydroxylation-P': 0.009698504595198248,
 'log_base': 1.0308628529838928,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4054941675,
 'sample_weights': [1.9110065226483408, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.325969006576292,
 'weight_decay_Hydroxylation-K': 6.188694213275713,
 'weight_decay_Hydroxylation-P': 1.0079983523528946}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17835.918
Exploding loss, terminate run (best metric=0.5311171412467957)
Finished Training
Total time taken: 0.1995086669921875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18046.863
Exploding loss, terminate run (best metric=0.5295793414115906)
Finished Training
Total time taken: 0.22800016403198242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17796.590
Exploding loss, terminate run (best metric=0.526372492313385)
Finished Training
Total time taken: 0.20800018310546875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17830.293
Exploding loss, terminate run (best metric=0.53018718957901)
Finished Training
Total time taken: 0.23100042343139648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17865.119
Exploding loss, terminate run (best metric=0.5282737016677856)
Finished Training
Total time taken: 0.2010021209716797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17849.131
Exploding loss, terminate run (best metric=0.5330862402915955)
Finished Training
Total time taken: 0.20903754234313965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17900.363
Exploding loss, terminate run (best metric=0.5277462005615234)
Finished Training
Total time taken: 0.2089979648590088
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17765.654
Exploding loss, terminate run (best metric=0.5267764329910278)
Finished Training
Total time taken: 0.2240004539489746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17837.559
Exploding loss, terminate run (best metric=0.5303906798362732)
Finished Training
Total time taken: 0.20501351356506348
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17815.486
Exploding loss, terminate run (best metric=0.5452007055282593)
Finished Training
Total time taken: 0.22300124168395996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17872.006
Exploding loss, terminate run (best metric=0.5317182540893555)
Finished Training
Total time taken: 0.21400046348571777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17833.912
Exploding loss, terminate run (best metric=0.5277072191238403)
Finished Training
Total time taken: 0.20000052452087402
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17893.746
Exploding loss, terminate run (best metric=0.5288649201393127)
Finished Training
Total time taken: 0.212996244430542
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17886.914
Exploding loss, terminate run (best metric=0.5252216458320618)
Finished Training
Total time taken: 0.20300054550170898
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17857.918
Exploding loss, terminate run (best metric=0.5361401438713074)
Finished Training
Total time taken: 0.21300125122070312
{'Hydroxylation-K Validation Accuracy': 0.4050236406619385, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6145224171539961, 'Hydroxylation-K AUC PR': 0.3304451125594996, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22676518883415436, 'Validation Loss (Hydroxylation-K)': 0.5579675992329916, 'Hydroxylation-P Validation Accuracy': 0.39272470094580647, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6259742826503637, 'Hydroxylation-P AUC PR': 0.3093016370961377, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.20070368111395248, 'Validation Loss (Hydroxylation-P)': 0.5305588205655416, 'Validation Loss (total)': 1.088526431719462, 'TimeToTrain': 0.21203742027282715}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005118036409498358,
 'learning_rate_Hydroxylation-K': 0.007389689836274119,
 'learning_rate_Hydroxylation-P': 0.009791753793437283,
 'log_base': 2.1803065517642004,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4256384170,
 'sample_weights': [54.96354898316795, 6.856168054919675],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.624806754135548,
 'weight_decay_Hydroxylation-K': 7.804111771962569,
 'weight_decay_Hydroxylation-P': 3.1398523336682556}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1362.748
[2,     1] loss: 1361.986
[3,     1] loss: 1360.741
[4,     1] loss: 1357.951
[5,     1] loss: 1358.528
[6,     1] loss: 1358.156
[7,     1] loss: 1359.253
[8,     1] loss: 1362.185
[9,     1] loss: 1355.950
[10,     1] loss: 1359.226
[11,     1] loss: 1358.296
[12,     1] loss: 1340.466
[13,     1] loss: 1330.285
[14,     1] loss: 1338.421
[15,     1] loss: 1305.641
[16,     1] loss: 1285.871
[17,     1] loss: 1271.920
[18,     1] loss: 1266.289
[19,     1] loss: 1211.171
[20,     1] loss: 1168.703
[21,     1] loss: 1218.557
[22,     1] loss: 1165.286
[23,     1] loss: 1184.655
[24,     1] loss: 1145.250
[25,     1] loss: 1146.796
[26,     1] loss: 1129.398
[27,     1] loss: 1130.455
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008459319693978494,
 'learning_rate_Hydroxylation-K': 0.0015539049026563818,
 'learning_rate_Hydroxylation-P': 0.0035908089023277885,
 'log_base': 1.8700388876838168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4162492451,
 'sample_weights': [2.141779431273998, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.865332505010748,
 'weight_decay_Hydroxylation-K': 8.928677672490217,
 'weight_decay_Hydroxylation-P': 1.4510399793735207}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1477.871
[2,     1] loss: 1473.859
[3,     1] loss: 1473.267
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008402166268568163,
 'learning_rate_Hydroxylation-K': 0.008253010199330562,
 'learning_rate_Hydroxylation-P': 0.0037057033573937243,
 'log_base': 1.5552165448870103,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2764374291,
 'sample_weights': [2.6670158015722967, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.411476443144991,
 'weight_decay_Hydroxylation-K': 9.824247131508603,
 'weight_decay_Hydroxylation-P': 9.561827121910634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1706.942
[2,     1] loss: 1708.624
[3,     1] loss: 1699.772
[4,     1] loss: 1702.698
[5,     1] loss: 1711.067
[6,     1] loss: 1711.481
[7,     1] loss: 1703.431
[8,     1] loss: 1703.088
[9,     1] loss: 1703.461
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038315321391293455,
 'learning_rate_Hydroxylation-K': 0.006193564360871282,
 'learning_rate_Hydroxylation-P': 0.009423632130925168,
 'log_base': 2.8892508964550725,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1143388938,
 'sample_weights': [3.7803152736634935, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.194120080706273,
 'weight_decay_Hydroxylation-K': 9.97216135728636,
 'weight_decay_Hydroxylation-P': 8.228813189399643}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.512
[2,     1] loss: 1240.246
[3,     1] loss: 1240.654
[4,     1] loss: 1240.427
[5,     1] loss: 1236.751
[6,     1] loss: 1232.454
[7,     1] loss: 1231.839
[8,     1] loss: 1221.404
[9,     1] loss: 1191.555
[10,     1] loss: 1172.149
[11,     1] loss: 1124.767
[12,     1] loss: 1090.323
[13,     1] loss: 1076.336
[14,     1] loss: 1023.989
[15,     1] loss: 1027.259
[16,     1] loss: 1045.858
[17,     1] loss: 1035.098
[18,     1] loss: 1004.062
[19,     1] loss: 1001.626
[20,     1] loss: 968.892
[21,     1] loss: 1011.540
[22,     1] loss: 974.331
[23,     1] loss: 988.841
[24,     1] loss: 971.640
[25,     1] loss: 934.989
[26,     1] loss: 935.267
[27,     1] loss: 906.242
[28,     1] loss: 986.828
[29,     1] loss: 921.951
[30,     1] loss: 931.396
[31,     1] loss: 888.539
[32,     1] loss: 945.492
[33,     1] loss: 895.928
[34,     1] loss: 907.558
[35,     1] loss: 869.371
[36,     1] loss: 876.371
[37,     1] loss: 849.940
[38,     1] loss: 864.849
[39,     1] loss: 807.020
[40,     1] loss: 864.965
[41,     1] loss: 810.988
[42,     1] loss: 841.820
[43,     1] loss: 836.841
[44,     1] loss: 799.148
[45,     1] loss: 857.030
[46,     1] loss: 824.653
[47,     1] loss: 810.630
[48,     1] loss: 735.558
[49,     1] loss: 747.976
[50,     1] loss: 721.995
[51,     1] loss: 686.201
[52,     1] loss: 682.003
[53,     1] loss: 687.275
[54,     1] loss: 838.020
[55,     1] loss: 1092.792
[56,     1] loss: 789.893
[57,     1] loss: 875.092
[58,     1] loss: 775.893
[59,     1] loss: 839.021
[60,     1] loss: 867.532
[61,     1] loss: 834.162
[62,     1] loss: 725.919
[63,     1] loss: 824.890
[64,     1] loss: 761.051
[65,     1] loss: 706.809
[66,     1] loss: 732.239
[67,     1] loss: 713.746
[68,     1] loss: 702.953
[69,     1] loss: 736.633
[70,     1] loss: 704.002
[71,     1] loss: 681.383
[72,     1] loss: 674.000
[73,     1] loss: 692.498
[74,     1] loss: 621.067
[75,     1] loss: 681.948
[76,     1] loss: 611.328
[77,     1] loss: 594.509
[78,     1] loss: 584.965
[79,     1] loss: 576.493
[80,     1] loss: 567.158
[81,     1] loss: 592.498
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008521819193808506,
 'learning_rate_Hydroxylation-K': 0.007810498601072864,
 'learning_rate_Hydroxylation-P': 0.009122928971618254,
 'log_base': 2.56310248293462,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3508694967,
 'sample_weights': [1.5734660261739248, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.748129478650203,
 'weight_decay_Hydroxylation-K': 8.016243521483325,
 'weight_decay_Hydroxylation-P': 3.8180737576745623}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.819
[2,     1] loss: 1285.969
[3,     1] loss: 1286.345
[4,     1] loss: 1280.476
[5,     1] loss: 1281.068
[6,     1] loss: 1277.869
[7,     1] loss: 1280.219
[8,     1] loss: 1277.296
[9,     1] loss: 1280.094
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004972839204086983,
 'learning_rate_Hydroxylation-K': 0.009583710479246024,
 'learning_rate_Hydroxylation-P': 0.007319345146987994,
 'log_base': 2.050243281532153,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2734431614,
 'sample_weights': [1.7737042650790107, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.903965791589568,
 'weight_decay_Hydroxylation-K': 7.1768917380448105,
 'weight_decay_Hydroxylation-P': 4.13218817150435}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1396.463
[2,     1] loss: 1399.029
[3,     1] loss: 1402.149
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007903581291413579,
 'learning_rate_Hydroxylation-K': 0.009953352992665506,
 'learning_rate_Hydroxylation-P': 0.009812592904492367,
 'log_base': 1.44807582956872,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1467177442,
 'sample_weights': [2.3252642602196145, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.369087976660705,
 'weight_decay_Hydroxylation-K': 5.271917780234367,
 'weight_decay_Hydroxylation-P': 0.6343818436326738}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1855.883
[2,     1] loss: 1861.159
[3,     1] loss: 1854.801
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009992157982394197,
 'learning_rate_Hydroxylation-K': 0.0019470575668308356,
 'learning_rate_Hydroxylation-P': 0.009055019977723153,
 'log_base': 1.7040596862440212,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1736397417,
 'sample_weights': [4.5091365390308065, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.001551181905823,
 'weight_decay_Hydroxylation-K': 5.159420657824958,
 'weight_decay_Hydroxylation-P': 4.642734844339545}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1568.148
[2,     1] loss: 1578.719
[3,     1] loss: 1569.408
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015655028334699685,
 'learning_rate_Hydroxylation-K': 0.009829455962312864,
 'learning_rate_Hydroxylation-P': 0.004313032099685766,
 'log_base': 1.5004727820715644,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2624980392,
 'sample_weights': [3.1320844379735844, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.818316969461654,
 'weight_decay_Hydroxylation-K': 7.813563176447924,
 'weight_decay_Hydroxylation-P': 3.1071233877731297}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1777.773
[2,     1] loss: 1785.626
[3,     1] loss: 1772.225
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009020390588594131,
 'learning_rate_Hydroxylation-K': 0.0015694138214188906,
 'learning_rate_Hydroxylation-P': 0.00253962533734484,
 'log_base': 2.95774664668754,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 792242990,
 'sample_weights': [4.114155782213685, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.5389055443524553,
 'weight_decay_Hydroxylation-K': 6.366178191676949,
 'weight_decay_Hydroxylation-P': 5.476201650642719}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.910
[2,     1] loss: 1236.685
[3,     1] loss: 1234.617
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008126159110912124,
 'learning_rate_Hydroxylation-K': 0.004227187798324761,
 'learning_rate_Hydroxylation-P': 0.006833608599850998,
 'log_base': 2.453173563060764,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2705100119,
 'sample_weights': [1.5394692807944805, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.610216924579656,
 'weight_decay_Hydroxylation-K': 8.686092192989413,
 'weight_decay_Hydroxylation-P': 8.269401291955347}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.340
[2,     1] loss: 1302.002
[3,     1] loss: 1301.504
[4,     1] loss: 1299.648
[5,     1] loss: 1294.944
[6,     1] loss: 1294.257
[7,     1] loss: 1284.157
[8,     1] loss: 1257.258
[9,     1] loss: 1224.825
[10,     1] loss: 1172.660
[11,     1] loss: 1157.755
[12,     1] loss: 1143.504
[13,     1] loss: 1103.629
[14,     1] loss: 1114.133
[15,     1] loss: 1115.728
[16,     1] loss: 1042.076
[17,     1] loss: 1074.095
[18,     1] loss: 1024.006
[19,     1] loss: 1054.328
[20,     1] loss: 990.346
[21,     1] loss: 1034.698
[22,     1] loss: 996.274
[23,     1] loss: 976.735
[24,     1] loss: 1031.737
[25,     1] loss: 1206.546
[26,     1] loss: 1016.632
[27,     1] loss: 1135.384
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004493860524377335,
 'learning_rate_Hydroxylation-K': 0.0032305803503702665,
 'learning_rate_Hydroxylation-P': 0.008681121541044508,
 'log_base': 1.326254811651721,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 544943177,
 'sample_weights': [1.8603473029238347, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.90609065235846,
 'weight_decay_Hydroxylation-K': 4.270939672478214,
 'weight_decay_Hydroxylation-P': 5.872118982604448}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2156.478
[2,     1] loss: 2172.339
[3,     1] loss: 2160.982
[4,     1] loss: 2153.981
[5,     1] loss: 2149.597
[6,     1] loss: 2152.014
[7,     1] loss: 2148.962
[8,     1] loss: 2155.955
[9,     1] loss: 2150.169
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014498690997772612,
 'learning_rate_Hydroxylation-K': 0.0014384761803674765,
 'learning_rate_Hydroxylation-P': 0.0006158017978840864,
 'log_base': 1.765521714032563,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2465836555,
 'sample_weights': [5.912483459426226, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0892521029537923,
 'weight_decay_Hydroxylation-K': 2.519116991870699,
 'weight_decay_Hydroxylation-P': 2.946784901574897}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1533.472
[2,     1] loss: 1529.603
[3,     1] loss: 1529.057
[4,     1] loss: 1529.762
[5,     1] loss: 1529.252
[6,     1] loss: 1528.667
[7,     1] loss: 1524.505
[8,     1] loss: 1520.244
[9,     1] loss: 1522.597
[10,     1] loss: 1528.299
[11,     1] loss: 1520.424
[12,     1] loss: 1516.955
[13,     1] loss: 1516.236
[14,     1] loss: 1512.423
[15,     1] loss: 1506.648
[16,     1] loss: 1500.458
[17,     1] loss: 1480.197
[18,     1] loss: 1466.571
[19,     1] loss: 1457.041
[20,     1] loss: 1433.849
[21,     1] loss: 1399.670
[22,     1] loss: 1377.043
[23,     1] loss: 1356.717
[24,     1] loss: 1351.913
[25,     1] loss: 1296.675
[26,     1] loss: 1326.369
[27,     1] loss: 1260.544
[28,     1] loss: 1325.983
[29,     1] loss: 1348.262
[30,     1] loss: 1240.895
[31,     1] loss: 1172.184
[32,     1] loss: 1254.048
[33,     1] loss: 1265.094
[34,     1] loss: 1306.313
[35,     1] loss: 1198.631
[36,     1] loss: 1291.760
[37,     1] loss: 1254.860
[38,     1] loss: 1221.820
[39,     1] loss: 1237.726
[40,     1] loss: 1182.286
[41,     1] loss: 1177.187
[42,     1] loss: 1160.522
[43,     1] loss: 1151.373
[44,     1] loss: 1182.991
[45,     1] loss: 1109.964
[46,     1] loss: 1175.499
[47,     1] loss: 1089.861
[48,     1] loss: 1050.340
[49,     1] loss: 1084.145
[50,     1] loss: 1049.170
[51,     1] loss: 1086.026
[52,     1] loss: 1057.558
[53,     1] loss: 1106.590
[54,     1] loss: 1042.707
[55,     1] loss: 1019.943
[56,     1] loss: 1020.018
[57,     1] loss: 1047.003
[58,     1] loss: 962.181
[59,     1] loss: 1012.926
[60,     1] loss: 1004.352
[61,     1] loss: 993.074
[62,     1] loss: 1003.227
[63,     1] loss: 897.086
[64,     1] loss: 940.234
[65,     1] loss: 988.288
[66,     1] loss: 915.299
[67,     1] loss: 1041.694
[68,     1] loss: 935.765
[69,     1] loss: 1017.663
[70,     1] loss: 940.800
[71,     1] loss: 986.793
[72,     1] loss: 848.673
[73,     1] loss: 978.845
[74,     1] loss: 826.756
[75,     1] loss: 960.583
[76,     1] loss: 913.120
[77,     1] loss: 884.650
[78,     1] loss: 1028.346
[79,     1] loss: 813.590
[80,     1] loss: 827.543
[81,     1] loss: 869.629
[82,     1] loss: 862.846
[83,     1] loss: 884.102
[84,     1] loss: 827.285
[85,     1] loss: 821.267
[86,     1] loss: 783.877
Early stopping applied (best metric=0.35403403639793396)
Finished Training
Total time taken: 12.238268375396729
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1527.819
[2,     1] loss: 1531.236
[3,     1] loss: 1527.017
[4,     1] loss: 1522.539
[5,     1] loss: 1526.788
[6,     1] loss: 1528.079
[7,     1] loss: 1518.898
[8,     1] loss: 1523.411
[9,     1] loss: 1516.296
[10,     1] loss: 1510.923
[11,     1] loss: 1500.999
[12,     1] loss: 1490.366
[13,     1] loss: 1466.607
[14,     1] loss: 1442.686
[15,     1] loss: 1421.866
[16,     1] loss: 1402.421
[17,     1] loss: 1371.198
[18,     1] loss: 1314.920
[19,     1] loss: 1321.149
[20,     1] loss: 1292.591
[21,     1] loss: 1292.497
[22,     1] loss: 1256.379
[23,     1] loss: 1242.976
[24,     1] loss: 1240.213
[25,     1] loss: 1240.448
[26,     1] loss: 1227.937
[27,     1] loss: 1174.509
[28,     1] loss: 1245.387
[29,     1] loss: 1217.966
[30,     1] loss: 1197.179
[31,     1] loss: 1167.876
[32,     1] loss: 1188.637
[33,     1] loss: 1210.047
[34,     1] loss: 1144.639
[35,     1] loss: 1153.062
[36,     1] loss: 1165.281
[37,     1] loss: 1182.974
[38,     1] loss: 1138.382
[39,     1] loss: 1139.607
[40,     1] loss: 1151.741
[41,     1] loss: 1132.786
[42,     1] loss: 1126.574
[43,     1] loss: 1076.418
[44,     1] loss: 1043.844
[45,     1] loss: 1064.904
[46,     1] loss: 1087.340
[47,     1] loss: 1046.458
[48,     1] loss: 1061.211
[49,     1] loss: 1041.301
[50,     1] loss: 1018.393
[51,     1] loss: 1062.505
[52,     1] loss: 1020.988
[53,     1] loss: 1021.720
[54,     1] loss: 984.047
[55,     1] loss: 1022.844
[56,     1] loss: 1043.819
[57,     1] loss: 995.245
[58,     1] loss: 975.372
[59,     1] loss: 971.432
[60,     1] loss: 949.079
[61,     1] loss: 977.093
[62,     1] loss: 992.248
[63,     1] loss: 982.223
[64,     1] loss: 984.107
[65,     1] loss: 853.561
[66,     1] loss: 892.949
[67,     1] loss: 922.918
[68,     1] loss: 898.027
[69,     1] loss: 868.467
[70,     1] loss: 883.443
[71,     1] loss: 824.840
[72,     1] loss: 797.494
[73,     1] loss: 844.017
[74,     1] loss: 805.822
[75,     1] loss: 824.320
[76,     1] loss: 814.307
[77,     1] loss: 809.067
[78,     1] loss: 886.864
[79,     1] loss: 698.648
[80,     1] loss: 799.305
[81,     1] loss: 774.426
[82,     1] loss: 721.873
[83,     1] loss: 738.527
[84,     1] loss: 729.109
[85,     1] loss: 724.276
[86,     1] loss: 710.978
[87,     1] loss: 712.607
[88,     1] loss: 732.888
[89,     1] loss: 687.705
[90,     1] loss: 683.776
[91,     1] loss: 687.102
[92,     1] loss: 691.394
[93,     1] loss: 660.762
[94,     1] loss: 690.759
[95,     1] loss: 609.865
[96,     1] loss: 696.982
[97,     1] loss: 700.095
[98,     1] loss: 649.641
[99,     1] loss: 684.427
[100,     1] loss: 603.117
[101,     1] loss: 582.910
[102,     1] loss: 658.045
[103,     1] loss: 626.248
[104,     1] loss: 642.596
[105,     1] loss: 584.255
[106,     1] loss: 617.012
[107,     1] loss: 605.819
[108,     1] loss: 619.804
[109,     1] loss: 607.783
[110,     1] loss: 620.529
[111,     1] loss: 584.485
[112,     1] loss: 556.896
[113,     1] loss: 543.483
[114,     1] loss: 592.447
[115,     1] loss: 511.769
Early stopping applied (best metric=0.4240458905696869)
Finished Training
Total time taken: 16.38809037208557
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1528.230
[2,     1] loss: 1534.028
[3,     1] loss: 1524.319
[4,     1] loss: 1522.641
[5,     1] loss: 1527.582
[6,     1] loss: 1529.116
[7,     1] loss: 1528.853
[8,     1] loss: 1521.410
[9,     1] loss: 1527.202
[10,     1] loss: 1519.075
[11,     1] loss: 1525.709
[12,     1] loss: 1526.267
[13,     1] loss: 1522.451
[14,     1] loss: 1519.257
[15,     1] loss: 1518.553
[16,     1] loss: 1518.892
[17,     1] loss: 1511.771
[18,     1] loss: 1510.637
[19,     1] loss: 1507.119
[20,     1] loss: 1489.172
[21,     1] loss: 1468.269
[22,     1] loss: 1446.292
[23,     1] loss: 1439.017
[24,     1] loss: 1416.470
[25,     1] loss: 1366.664
[26,     1] loss: 1379.275
[27,     1] loss: 1355.646
[28,     1] loss: 1359.588
[29,     1] loss: 1358.812
[30,     1] loss: 1320.924
[31,     1] loss: 1307.581
[32,     1] loss: 1331.943
[33,     1] loss: 1270.929
[34,     1] loss: 1291.455
[35,     1] loss: 1282.055
[36,     1] loss: 1303.009
[37,     1] loss: 1294.870
[38,     1] loss: 1303.652
[39,     1] loss: 1265.244
[40,     1] loss: 1291.923
[41,     1] loss: 1244.239
[42,     1] loss: 1241.635
[43,     1] loss: 1230.073
[44,     1] loss: 1193.652
[45,     1] loss: 1222.801
[46,     1] loss: 1240.205
[47,     1] loss: 1193.361
[48,     1] loss: 1242.115
[49,     1] loss: 1213.653
[50,     1] loss: 1206.370
[51,     1] loss: 1226.340
[52,     1] loss: 1180.150
[53,     1] loss: 1142.014
[54,     1] loss: 1215.875
[55,     1] loss: 1133.706
[56,     1] loss: 1135.036
[57,     1] loss: 1127.615
[58,     1] loss: 1079.650
[59,     1] loss: 1045.620
[60,     1] loss: 1071.500
[61,     1] loss: 1007.674
[62,     1] loss: 1089.622
[63,     1] loss: 1106.294
[64,     1] loss: 1080.790
[65,     1] loss: 998.662
[66,     1] loss: 1030.186
[67,     1] loss: 1093.945
[68,     1] loss: 1002.314
[69,     1] loss: 929.589
[70,     1] loss: 943.878
[71,     1] loss: 921.484
[72,     1] loss: 954.187
[73,     1] loss: 903.172
[74,     1] loss: 923.545
[75,     1] loss: 931.714
[76,     1] loss: 894.148
[77,     1] loss: 973.408
[78,     1] loss: 888.670
[79,     1] loss: 890.333
[80,     1] loss: 864.206
[81,     1] loss: 900.316
[82,     1] loss: 803.272
[83,     1] loss: 853.750
[84,     1] loss: 819.637
[85,     1] loss: 868.524
[86,     1] loss: 795.579
[87,     1] loss: 895.809
[88,     1] loss: 761.135
[89,     1] loss: 876.468
[90,     1] loss: 806.698
[91,     1] loss: 846.530
[92,     1] loss: 798.690
[93,     1] loss: 728.772
[94,     1] loss: 769.772
[95,     1] loss: 713.810
[96,     1] loss: 687.080
[97,     1] loss: 713.236
[98,     1] loss: 773.275
[99,     1] loss: 735.007
[100,     1] loss: 715.997
[101,     1] loss: 741.918
[102,     1] loss: 735.263
[103,     1] loss: 746.298
[104,     1] loss: 732.384
[105,     1] loss: 675.129
[106,     1] loss: 697.352
[107,     1] loss: 659.137
[108,     1] loss: 638.544
[109,     1] loss: 740.937
[110,     1] loss: 657.032
[111,     1] loss: 655.470
[112,     1] loss: 614.318
[113,     1] loss: 725.711
[114,     1] loss: 616.606
[115,     1] loss: 704.709
Early stopping applied (best metric=0.3382745385169983)
Finished Training
Total time taken: 16.548686504364014
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1532.689
[2,     1] loss: 1526.316
[3,     1] loss: 1528.496
[4,     1] loss: 1528.606
[5,     1] loss: 1524.625
[6,     1] loss: 1527.436
[7,     1] loss: 1525.628
[8,     1] loss: 1522.298
[9,     1] loss: 1523.613
[10,     1] loss: 1516.192
[11,     1] loss: 1510.815
[12,     1] loss: 1506.279
[13,     1] loss: 1494.457
[14,     1] loss: 1486.245
[15,     1] loss: 1471.286
[16,     1] loss: 1455.019
[17,     1] loss: 1420.209
[18,     1] loss: 1423.795
[19,     1] loss: 1405.223
[20,     1] loss: 1388.553
[21,     1] loss: 1372.352
[22,     1] loss: 1329.562
[23,     1] loss: 1303.630
[24,     1] loss: 1350.881
[25,     1] loss: 1282.333
[26,     1] loss: 1282.965
[27,     1] loss: 1247.034
[28,     1] loss: 1317.244
[29,     1] loss: 1282.813
[30,     1] loss: 1275.549
[31,     1] loss: 1334.291
[32,     1] loss: 1276.268
[33,     1] loss: 1327.217
[34,     1] loss: 1241.284
[35,     1] loss: 1259.374
[36,     1] loss: 1229.342
[37,     1] loss: 1236.333
[38,     1] loss: 1273.535
[39,     1] loss: 1259.885
[40,     1] loss: 1206.041
[41,     1] loss: 1227.400
[42,     1] loss: 1251.001
[43,     1] loss: 1223.161
[44,     1] loss: 1156.620
[45,     1] loss: 1127.104
[46,     1] loss: 1134.958
[47,     1] loss: 1137.677
[48,     1] loss: 1186.937
[49,     1] loss: 1056.094
[50,     1] loss: 1105.997
[51,     1] loss: 1163.395
[52,     1] loss: 1089.937
[53,     1] loss: 1080.451
[54,     1] loss: 1102.306
[55,     1] loss: 1086.190
[56,     1] loss: 1098.326
[57,     1] loss: 1128.875
[58,     1] loss: 1028.965
[59,     1] loss: 1002.757
[60,     1] loss: 955.690
[61,     1] loss: 1094.371
[62,     1] loss: 1021.980
[63,     1] loss: 978.612
[64,     1] loss: 945.436
[65,     1] loss: 970.979
[66,     1] loss: 969.493
[67,     1] loss: 1004.422
[68,     1] loss: 1032.779
[69,     1] loss: 878.606
[70,     1] loss: 982.305
[71,     1] loss: 909.048
[72,     1] loss: 852.424
[73,     1] loss: 895.744
[74,     1] loss: 884.197
[75,     1] loss: 830.049
[76,     1] loss: 922.144
[77,     1] loss: 881.975
[78,     1] loss: 866.276
[79,     1] loss: 789.737
[80,     1] loss: 862.928
[81,     1] loss: 850.947
[82,     1] loss: 865.584
[83,     1] loss: 795.652
[84,     1] loss: 774.566
[85,     1] loss: 794.981
[86,     1] loss: 785.702
[87,     1] loss: 762.934
[88,     1] loss: 769.856
[89,     1] loss: 808.999
[90,     1] loss: 702.526
[91,     1] loss: 796.900
[92,     1] loss: 782.587
[93,     1] loss: 789.708
[94,     1] loss: 735.075
[95,     1] loss: 723.182
Early stopping applied (best metric=0.32486507296562195)
Finished Training
Total time taken: 13.52827000617981
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1528.901
[2,     1] loss: 1527.778
[3,     1] loss: 1528.565
[4,     1] loss: 1527.965
[5,     1] loss: 1529.010
[6,     1] loss: 1527.338
[7,     1] loss: 1524.820
[8,     1] loss: 1526.931
[9,     1] loss: 1523.434
[10,     1] loss: 1520.570
[11,     1] loss: 1521.777
[12,     1] loss: 1514.430
[13,     1] loss: 1505.921
[14,     1] loss: 1506.257
[15,     1] loss: 1488.461
[16,     1] loss: 1464.501
[17,     1] loss: 1463.078
[18,     1] loss: 1422.829
[19,     1] loss: 1417.628
[20,     1] loss: 1405.515
[21,     1] loss: 1390.136
[22,     1] loss: 1353.490
[23,     1] loss: 1317.217
[24,     1] loss: 1362.402
[25,     1] loss: 1275.390
[26,     1] loss: 1367.920
[27,     1] loss: 1252.991
[28,     1] loss: 1245.320
[29,     1] loss: 1298.311
[30,     1] loss: 1232.756
[31,     1] loss: 1259.805
[32,     1] loss: 1279.035
[33,     1] loss: 1300.014
[34,     1] loss: 1256.319
[35,     1] loss: 1232.394
[36,     1] loss: 1252.348
[37,     1] loss: 1233.132
[38,     1] loss: 1312.438
[39,     1] loss: 1255.649
[40,     1] loss: 1177.145
[41,     1] loss: 1170.443
[42,     1] loss: 1254.418
[43,     1] loss: 1194.030
[44,     1] loss: 1197.885
[45,     1] loss: 1168.129
[46,     1] loss: 1181.377
[47,     1] loss: 1144.135
[48,     1] loss: 1149.202
[49,     1] loss: 1118.758
[50,     1] loss: 1135.373
[51,     1] loss: 1161.776
[52,     1] loss: 1083.541
[53,     1] loss: 1108.549
[54,     1] loss: 1093.747
[55,     1] loss: 1080.612
[56,     1] loss: 1060.568
[57,     1] loss: 1027.823
[58,     1] loss: 1020.284
[59,     1] loss: 1035.252
[60,     1] loss: 1019.622
[61,     1] loss: 990.980
[62,     1] loss: 1025.295
[63,     1] loss: 978.418
[64,     1] loss: 1014.394
[65,     1] loss: 1030.785
[66,     1] loss: 909.929
[67,     1] loss: 1013.105
[68,     1] loss: 973.558
[69,     1] loss: 927.136
[70,     1] loss: 935.987
[71,     1] loss: 1015.945
[72,     1] loss: 882.919
[73,     1] loss: 911.673
[74,     1] loss: 860.723
[75,     1] loss: 905.803
[76,     1] loss: 845.132
[77,     1] loss: 854.379
[78,     1] loss: 828.153
[79,     1] loss: 834.131
[80,     1] loss: 900.651
[81,     1] loss: 838.005
[82,     1] loss: 746.230
[83,     1] loss: 843.019
[84,     1] loss: 781.737
[85,     1] loss: 828.075
[86,     1] loss: 794.825
[87,     1] loss: 782.657
[88,     1] loss: 718.823
[89,     1] loss: 802.185
[90,     1] loss: 742.962
[91,     1] loss: 738.494
[92,     1] loss: 709.809
[93,     1] loss: 734.359
[94,     1] loss: 731.597
Early stopping applied (best metric=0.39338234066963196)
Finished Training
Total time taken: 13.323269605636597
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1528.668
[2,     1] loss: 1529.853
[3,     1] loss: 1523.196
[4,     1] loss: 1527.381
[5,     1] loss: 1523.189
[6,     1] loss: 1522.133
[7,     1] loss: 1519.026
[8,     1] loss: 1519.085
[9,     1] loss: 1516.026
[10,     1] loss: 1512.024
[11,     1] loss: 1502.779
[12,     1] loss: 1493.494
[13,     1] loss: 1479.987
[14,     1] loss: 1460.191
[15,     1] loss: 1443.360
[16,     1] loss: 1427.083
[17,     1] loss: 1384.781
[18,     1] loss: 1389.075
[19,     1] loss: 1363.414
[20,     1] loss: 1331.712
[21,     1] loss: 1335.376
[22,     1] loss: 1356.707
[23,     1] loss: 1291.417
[24,     1] loss: 1284.690
[25,     1] loss: 1350.389
[26,     1] loss: 1241.467
[27,     1] loss: 1284.871
[28,     1] loss: 1240.157
[29,     1] loss: 1246.502
[30,     1] loss: 1260.836
[31,     1] loss: 1277.065
[32,     1] loss: 1278.809
[33,     1] loss: 1186.958
[34,     1] loss: 1229.703
[35,     1] loss: 1201.853
[36,     1] loss: 1214.228
[37,     1] loss: 1202.560
[38,     1] loss: 1197.816
[39,     1] loss: 1247.110
[40,     1] loss: 1191.485
[41,     1] loss: 1162.407
[42,     1] loss: 1209.798
[43,     1] loss: 1175.560
[44,     1] loss: 1140.141
[45,     1] loss: 1179.747
[46,     1] loss: 1166.626
[47,     1] loss: 1118.069
[48,     1] loss: 1118.133
[49,     1] loss: 1118.750
[50,     1] loss: 1130.755
[51,     1] loss: 1083.279
[52,     1] loss: 1145.858
[53,     1] loss: 1067.325
[54,     1] loss: 1085.883
[55,     1] loss: 1015.466
[56,     1] loss: 1043.979
[57,     1] loss: 1044.124
[58,     1] loss: 1071.698
[59,     1] loss: 971.240
[60,     1] loss: 1069.224
[61,     1] loss: 952.134
[62,     1] loss: 938.865
[63,     1] loss: 899.683
[64,     1] loss: 981.111
[65,     1] loss: 916.687
[66,     1] loss: 945.812
[67,     1] loss: 924.624
[68,     1] loss: 913.596
[69,     1] loss: 939.379
[70,     1] loss: 868.642
[71,     1] loss: 888.295
[72,     1] loss: 867.489
[73,     1] loss: 894.720
[74,     1] loss: 848.209
[75,     1] loss: 873.396
[76,     1] loss: 898.645
[77,     1] loss: 796.873
[78,     1] loss: 826.395
[79,     1] loss: 853.516
[80,     1] loss: 799.008
[81,     1] loss: 844.428
[82,     1] loss: 775.334
[83,     1] loss: 775.030
[84,     1] loss: 710.299
[85,     1] loss: 718.947
[86,     1] loss: 717.831
[87,     1] loss: 744.843
[88,     1] loss: 699.979
[89,     1] loss: 705.007
[90,     1] loss: 712.460
[91,     1] loss: 693.988
[92,     1] loss: 780.998
[93,     1] loss: 712.193
[94,     1] loss: 677.100
[95,     1] loss: 741.654
[96,     1] loss: 702.031
[97,     1] loss: 712.040
[98,     1] loss: 666.703
[99,     1] loss: 628.175
[100,     1] loss: 699.523
[101,     1] loss: 659.754
[102,     1] loss: 625.310
[103,     1] loss: 610.835
Early stopping applied (best metric=0.36105242371559143)
Finished Training
Total time taken: 15.05214238166809
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1533.154
[2,     1] loss: 1527.696
[3,     1] loss: 1528.795
[4,     1] loss: 1526.246
[5,     1] loss: 1525.302
[6,     1] loss: 1528.709
[7,     1] loss: 1519.701
[8,     1] loss: 1526.305
[9,     1] loss: 1516.981
[10,     1] loss: 1518.839
[11,     1] loss: 1504.053
[12,     1] loss: 1495.034
[13,     1] loss: 1487.477
[14,     1] loss: 1463.690
[15,     1] loss: 1443.326
[16,     1] loss: 1414.137
[17,     1] loss: 1396.109
[18,     1] loss: 1362.242
[19,     1] loss: 1341.497
[20,     1] loss: 1326.116
[21,     1] loss: 1307.994
[22,     1] loss: 1291.333
[23,     1] loss: 1243.553
[24,     1] loss: 1286.882
[25,     1] loss: 1274.491
[26,     1] loss: 1265.209
[27,     1] loss: 1225.618
[28,     1] loss: 1243.337
[29,     1] loss: 1269.404
[30,     1] loss: 1253.970
[31,     1] loss: 1263.517
[32,     1] loss: 1206.351
[33,     1] loss: 1192.894
[34,     1] loss: 1217.982
[35,     1] loss: 1194.230
[36,     1] loss: 1182.577
[37,     1] loss: 1160.205
[38,     1] loss: 1188.911
[39,     1] loss: 1167.197
[40,     1] loss: 1168.633
[41,     1] loss: 1146.788
[42,     1] loss: 1161.092
[43,     1] loss: 1176.122
[44,     1] loss: 1110.412
[45,     1] loss: 1109.817
[46,     1] loss: 1044.020
[47,     1] loss: 1116.034
[48,     1] loss: 1112.510
[49,     1] loss: 1090.514
[50,     1] loss: 1108.803
[51,     1] loss: 1086.496
[52,     1] loss: 942.857
[53,     1] loss: 1011.943
[54,     1] loss: 1048.897
[55,     1] loss: 1037.317
[56,     1] loss: 961.905
[57,     1] loss: 1030.179
[58,     1] loss: 962.767
[59,     1] loss: 934.999
[60,     1] loss: 972.930
[61,     1] loss: 951.868
[62,     1] loss: 956.479
[63,     1] loss: 978.662
[64,     1] loss: 900.702
[65,     1] loss: 1032.258
[66,     1] loss: 886.031
[67,     1] loss: 925.957
[68,     1] loss: 814.036
[69,     1] loss: 913.693
[70,     1] loss: 940.746
[71,     1] loss: 870.425
[72,     1] loss: 871.271
[73,     1] loss: 972.930
[74,     1] loss: 838.965
[75,     1] loss: 861.296
[76,     1] loss: 932.581
[77,     1] loss: 820.775
[78,     1] loss: 896.896
[79,     1] loss: 882.198
[80,     1] loss: 814.225
[81,     1] loss: 802.883
[82,     1] loss: 862.240
[83,     1] loss: 792.391
[84,     1] loss: 743.497
[85,     1] loss: 790.222
[86,     1] loss: 839.662
[87,     1] loss: 733.831
[88,     1] loss: 717.656
[89,     1] loss: 799.380
[90,     1] loss: 763.962
[91,     1] loss: 754.834
[92,     1] loss: 642.115
[93,     1] loss: 724.710
[94,     1] loss: 707.718
[95,     1] loss: 687.430
[96,     1] loss: 652.437
Early stopping applied (best metric=0.4578082263469696)
Finished Training
Total time taken: 13.954247951507568
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1527.431
[2,     1] loss: 1529.009
[3,     1] loss: 1525.760
[4,     1] loss: 1523.747
[5,     1] loss: 1531.533
[6,     1] loss: 1519.965
[7,     1] loss: 1517.883
[8,     1] loss: 1512.350
[9,     1] loss: 1506.445
[10,     1] loss: 1497.171
[11,     1] loss: 1480.161
[12,     1] loss: 1463.382
[13,     1] loss: 1439.625
[14,     1] loss: 1403.991
[15,     1] loss: 1387.856
[16,     1] loss: 1322.902
[17,     1] loss: 1323.884
[18,     1] loss: 1295.183
[19,     1] loss: 1307.602
[20,     1] loss: 1261.857
[21,     1] loss: 1214.676
[22,     1] loss: 1216.829
[23,     1] loss: 1261.575
[24,     1] loss: 1261.821
[25,     1] loss: 1267.036
[26,     1] loss: 1204.978
[27,     1] loss: 1221.340
[28,     1] loss: 1242.414
[29,     1] loss: 1295.108
[30,     1] loss: 1162.630
[31,     1] loss: 1257.807
[32,     1] loss: 1228.479
[33,     1] loss: 1190.218
[34,     1] loss: 1169.672
[35,     1] loss: 1180.251
[36,     1] loss: 1156.720
[37,     1] loss: 1188.012
[38,     1] loss: 1167.469
[39,     1] loss: 1177.292
[40,     1] loss: 1114.168
[41,     1] loss: 1094.989
[42,     1] loss: 1112.467
[43,     1] loss: 1087.952
[44,     1] loss: 1123.440
[45,     1] loss: 1061.116
[46,     1] loss: 1094.039
[47,     1] loss: 993.362
[48,     1] loss: 1047.880
[49,     1] loss: 1023.073
[50,     1] loss: 1097.541
[51,     1] loss: 1089.442
[52,     1] loss: 1038.761
[53,     1] loss: 1052.024
[54,     1] loss: 1052.827
[55,     1] loss: 965.094
[56,     1] loss: 1026.020
[57,     1] loss: 982.942
[58,     1] loss: 954.372
[59,     1] loss: 951.518
[60,     1] loss: 924.819
[61,     1] loss: 992.529
[62,     1] loss: 920.454
[63,     1] loss: 902.735
[64,     1] loss: 894.362
[65,     1] loss: 886.927
[66,     1] loss: 931.758
[67,     1] loss: 850.019
[68,     1] loss: 904.427
[69,     1] loss: 926.361
[70,     1] loss: 849.289
[71,     1] loss: 833.802
[72,     1] loss: 791.663
[73,     1] loss: 856.732
[74,     1] loss: 817.922
[75,     1] loss: 830.072
[76,     1] loss: 806.818
[77,     1] loss: 782.189
[78,     1] loss: 784.282
[79,     1] loss: 852.115
[80,     1] loss: 783.269
[81,     1] loss: 745.572
[82,     1] loss: 797.843
[83,     1] loss: 744.896
[84,     1] loss: 776.772
[85,     1] loss: 705.513
[86,     1] loss: 735.080
[87,     1] loss: 673.865
[88,     1] loss: 762.373
[89,     1] loss: 711.315
[90,     1] loss: 756.747
[91,     1] loss: 660.189
[92,     1] loss: 735.683
[93,     1] loss: 612.632
[94,     1] loss: 705.674
[95,     1] loss: 673.082
[96,     1] loss: 713.431
[97,     1] loss: 624.466
Early stopping applied (best metric=0.39711031317710876)
Finished Training
Total time taken: 13.96768569946289
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1530.093
[2,     1] loss: 1528.916
[3,     1] loss: 1526.831
[4,     1] loss: 1528.891
[5,     1] loss: 1526.260
[6,     1] loss: 1524.840
[7,     1] loss: 1523.294
[8,     1] loss: 1533.348
[9,     1] loss: 1533.498
[10,     1] loss: 1523.853
[11,     1] loss: 1524.706
[12,     1] loss: 1522.987
[13,     1] loss: 1525.545
[14,     1] loss: 1522.435
[15,     1] loss: 1520.006
[16,     1] loss: 1517.999
[17,     1] loss: 1517.130
[18,     1] loss: 1504.860
[19,     1] loss: 1505.280
[20,     1] loss: 1491.595
[21,     1] loss: 1475.031
[22,     1] loss: 1460.911
[23,     1] loss: 1431.739
[24,     1] loss: 1414.613
[25,     1] loss: 1401.830
[26,     1] loss: 1366.912
[27,     1] loss: 1394.524
[28,     1] loss: 1336.720
[29,     1] loss: 1333.358
[30,     1] loss: 1342.162
[31,     1] loss: 1319.188
[32,     1] loss: 1273.179
[33,     1] loss: 1279.973
[34,     1] loss: 1264.195
[35,     1] loss: 1280.657
[36,     1] loss: 1299.813
[37,     1] loss: 1270.177
[38,     1] loss: 1226.681
[39,     1] loss: 1253.981
[40,     1] loss: 1241.257
[41,     1] loss: 1239.153
[42,     1] loss: 1218.906
[43,     1] loss: 1212.015
[44,     1] loss: 1253.007
[45,     1] loss: 1184.385
[46,     1] loss: 1160.600
[47,     1] loss: 1166.035
[48,     1] loss: 1132.830
[49,     1] loss: 1153.715
[50,     1] loss: 1186.427
[51,     1] loss: 1127.476
[52,     1] loss: 1078.974
[53,     1] loss: 1180.290
[54,     1] loss: 1197.870
[55,     1] loss: 1187.302
[56,     1] loss: 1101.792
[57,     1] loss: 1112.925
[58,     1] loss: 1094.616
[59,     1] loss: 1058.737
[60,     1] loss: 1158.604
[61,     1] loss: 1082.357
[62,     1] loss: 1116.585
[63,     1] loss: 1086.070
[64,     1] loss: 1037.366
[65,     1] loss: 1055.429
[66,     1] loss: 1042.421
[67,     1] loss: 1017.067
[68,     1] loss: 1038.126
[69,     1] loss: 1043.849
[70,     1] loss: 1021.606
[71,     1] loss: 1069.530
[72,     1] loss: 926.795
[73,     1] loss: 1011.735
[74,     1] loss: 949.962
[75,     1] loss: 997.324
[76,     1] loss: 946.127
[77,     1] loss: 886.866
[78,     1] loss: 952.269
[79,     1] loss: 927.505
[80,     1] loss: 897.567
[81,     1] loss: 927.351
[82,     1] loss: 904.003
[83,     1] loss: 879.360
[84,     1] loss: 827.024
[85,     1] loss: 835.021
[86,     1] loss: 780.618
[87,     1] loss: 895.796
[88,     1] loss: 819.699
[89,     1] loss: 871.623
[90,     1] loss: 803.155
[91,     1] loss: 785.424
[92,     1] loss: 803.097
[93,     1] loss: 738.804
[94,     1] loss: 767.737
[95,     1] loss: 730.048
[96,     1] loss: 695.681
[97,     1] loss: 700.869
[98,     1] loss: 823.457
[99,     1] loss: 817.108
[100,     1] loss: 698.384
[101,     1] loss: 717.884
[102,     1] loss: 694.085
[103,     1] loss: 708.093
[104,     1] loss: 734.016
[105,     1] loss: 633.990
[106,     1] loss: 773.971
[107,     1] loss: 640.704
[108,     1] loss: 766.189
[109,     1] loss: 669.577
[110,     1] loss: 634.897
[111,     1] loss: 620.080
[112,     1] loss: 708.578
[113,     1] loss: 641.076
Early stopping applied (best metric=0.3158559799194336)
Finished Training
Total time taken: 16.078171491622925
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1534.044
[2,     1] loss: 1529.252
[3,     1] loss: 1527.613
[4,     1] loss: 1528.647
[5,     1] loss: 1526.275
[6,     1] loss: 1528.224
[7,     1] loss: 1528.209
[8,     1] loss: 1528.954
[9,     1] loss: 1527.437
[10,     1] loss: 1518.604
[11,     1] loss: 1521.078
[12,     1] loss: 1518.061
[13,     1] loss: 1515.985
[14,     1] loss: 1510.938
[15,     1] loss: 1498.035
[16,     1] loss: 1485.469
[17,     1] loss: 1472.750
[18,     1] loss: 1455.023
[19,     1] loss: 1433.333
[20,     1] loss: 1405.407
[21,     1] loss: 1376.818
[22,     1] loss: 1364.138
[23,     1] loss: 1341.388
[24,     1] loss: 1345.311
[25,     1] loss: 1302.943
[26,     1] loss: 1296.094
[27,     1] loss: 1284.246
[28,     1] loss: 1278.183
[29,     1] loss: 1300.325
[30,     1] loss: 1211.861
[31,     1] loss: 1207.213
[32,     1] loss: 1242.257
[33,     1] loss: 1245.367
[34,     1] loss: 1241.448
[35,     1] loss: 1221.814
[36,     1] loss: 1147.620
[37,     1] loss: 1223.719
[38,     1] loss: 1183.463
[39,     1] loss: 1175.367
[40,     1] loss: 1184.086
[41,     1] loss: 1151.573
[42,     1] loss: 1151.739
[43,     1] loss: 1121.431
[44,     1] loss: 1152.797
[45,     1] loss: 1105.847
[46,     1] loss: 1123.290
[47,     1] loss: 1080.914
[48,     1] loss: 1111.668
[49,     1] loss: 1054.021
[50,     1] loss: 1020.425
[51,     1] loss: 1060.663
[52,     1] loss: 1010.289
[53,     1] loss: 1071.705
[54,     1] loss: 981.085
[55,     1] loss: 963.370
[56,     1] loss: 1049.521
[57,     1] loss: 970.914
[58,     1] loss: 941.098
[59,     1] loss: 962.563
[60,     1] loss: 958.135
[61,     1] loss: 921.492
[62,     1] loss: 939.120
[63,     1] loss: 955.574
[64,     1] loss: 905.794
[65,     1] loss: 901.256
[66,     1] loss: 900.770
[67,     1] loss: 825.065
[68,     1] loss: 891.502
[69,     1] loss: 877.423
[70,     1] loss: 893.385
[71,     1] loss: 793.043
[72,     1] loss: 807.624
[73,     1] loss: 791.243
[74,     1] loss: 739.739
[75,     1] loss: 886.554
[76,     1] loss: 746.141
[77,     1] loss: 828.967
[78,     1] loss: 814.965
[79,     1] loss: 748.679
[80,     1] loss: 755.940
[81,     1] loss: 716.060
Early stopping applied (best metric=0.4384463429450989)
Finished Training
Total time taken: 12.48970627784729
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1531.952
[2,     1] loss: 1522.483
[3,     1] loss: 1529.110
[4,     1] loss: 1525.133
[5,     1] loss: 1526.450
[6,     1] loss: 1533.163
[7,     1] loss: 1521.660
[8,     1] loss: 1520.618
[9,     1] loss: 1527.089
[10,     1] loss: 1528.700
[11,     1] loss: 1523.181
[12,     1] loss: 1523.828
[13,     1] loss: 1516.674
[14,     1] loss: 1512.402
[15,     1] loss: 1506.640
[16,     1] loss: 1497.454
[17,     1] loss: 1480.989
[18,     1] loss: 1461.926
[19,     1] loss: 1436.590
[20,     1] loss: 1400.445
[21,     1] loss: 1394.294
[22,     1] loss: 1371.733
[23,     1] loss: 1355.303
[24,     1] loss: 1338.772
[25,     1] loss: 1291.886
[26,     1] loss: 1352.102
[27,     1] loss: 1286.259
[28,     1] loss: 1297.207
[29,     1] loss: 1258.407
[30,     1] loss: 1243.810
[31,     1] loss: 1310.113
[32,     1] loss: 1326.996
[33,     1] loss: 1285.630
[34,     1] loss: 1224.725
[35,     1] loss: 1195.163
[36,     1] loss: 1276.572
[37,     1] loss: 1197.992
[38,     1] loss: 1195.482
[39,     1] loss: 1215.669
[40,     1] loss: 1211.611
[41,     1] loss: 1249.729
[42,     1] loss: 1161.464
[43,     1] loss: 1193.106
[44,     1] loss: 1149.535
[45,     1] loss: 1141.933
[46,     1] loss: 1149.130
[47,     1] loss: 1135.736
[48,     1] loss: 1184.584
[49,     1] loss: 1134.633
[50,     1] loss: 1185.628
[51,     1] loss: 1072.540
[52,     1] loss: 1084.185
[53,     1] loss: 1130.784
[54,     1] loss: 1098.542
[55,     1] loss: 1107.910
[56,     1] loss: 1094.273
[57,     1] loss: 1126.758
[58,     1] loss: 1096.069
[59,     1] loss: 1082.831
[60,     1] loss: 1060.902
[61,     1] loss: 1033.181
[62,     1] loss: 1062.950
[63,     1] loss: 1090.282
[64,     1] loss: 995.319
[65,     1] loss: 996.434
[66,     1] loss: 986.090
[67,     1] loss: 994.741
[68,     1] loss: 1007.772
[69,     1] loss: 956.533
[70,     1] loss: 913.330
[71,     1] loss: 924.045
[72,     1] loss: 928.369
[73,     1] loss: 896.288
[74,     1] loss: 881.006
[75,     1] loss: 874.969
[76,     1] loss: 901.290
[77,     1] loss: 896.588
[78,     1] loss: 845.335
[79,     1] loss: 873.182
[80,     1] loss: 825.884
[81,     1] loss: 953.955
[82,     1] loss: 774.153
[83,     1] loss: 938.239
[84,     1] loss: 813.290
[85,     1] loss: 868.842
[86,     1] loss: 794.805
[87,     1] loss: 803.111
[88,     1] loss: 875.582
[89,     1] loss: 793.439
[90,     1] loss: 787.940
[91,     1] loss: 875.098
[92,     1] loss: 841.710
[93,     1] loss: 769.324
[94,     1] loss: 750.679
[95,     1] loss: 869.031
[96,     1] loss: 774.367
[97,     1] loss: 773.419
[98,     1] loss: 764.761
[99,     1] loss: 710.750
[100,     1] loss: 742.632
Early stopping applied (best metric=0.38409027457237244)
Finished Training
Total time taken: 14.528667449951172
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1531.366
[2,     1] loss: 1523.071
[3,     1] loss: 1525.650
[4,     1] loss: 1527.490
[5,     1] loss: 1525.380
[6,     1] loss: 1523.632
[7,     1] loss: 1529.829
[8,     1] loss: 1529.774
[9,     1] loss: 1524.409
[10,     1] loss: 1518.400
[11,     1] loss: 1523.977
[12,     1] loss: 1515.524
[13,     1] loss: 1518.964
[14,     1] loss: 1509.121
[15,     1] loss: 1496.668
[16,     1] loss: 1493.537
[17,     1] loss: 1467.730
[18,     1] loss: 1452.618
[19,     1] loss: 1434.661
[20,     1] loss: 1400.532
[21,     1] loss: 1414.695
[22,     1] loss: 1396.355
[23,     1] loss: 1380.655
[24,     1] loss: 1352.850
[25,     1] loss: 1353.990
[26,     1] loss: 1350.848
[27,     1] loss: 1299.568
[28,     1] loss: 1288.521
[29,     1] loss: 1292.261
[30,     1] loss: 1297.295
[31,     1] loss: 1327.496
[32,     1] loss: 1303.344
[33,     1] loss: 1289.408
[34,     1] loss: 1263.883
[35,     1] loss: 1290.476
[36,     1] loss: 1263.661
[37,     1] loss: 1249.149
[38,     1] loss: 1276.165
[39,     1] loss: 1210.137
[40,     1] loss: 1224.576
[41,     1] loss: 1217.456
[42,     1] loss: 1195.242
[43,     1] loss: 1186.303
[44,     1] loss: 1188.184
[45,     1] loss: 1182.124
[46,     1] loss: 1177.504
[47,     1] loss: 1170.436
[48,     1] loss: 1162.571
[49,     1] loss: 1147.587
[50,     1] loss: 1105.079
[51,     1] loss: 1138.729
[52,     1] loss: 1152.813
[53,     1] loss: 1099.876
[54,     1] loss: 1057.224
[55,     1] loss: 1164.735
[56,     1] loss: 1086.746
[57,     1] loss: 1117.214
[58,     1] loss: 1037.683
[59,     1] loss: 1134.022
[60,     1] loss: 1092.224
[61,     1] loss: 1030.156
[62,     1] loss: 1052.962
[63,     1] loss: 994.313
[64,     1] loss: 972.451
[65,     1] loss: 974.922
[66,     1] loss: 1016.582
[67,     1] loss: 1006.299
[68,     1] loss: 887.009
[69,     1] loss: 995.724
[70,     1] loss: 932.060
[71,     1] loss: 1017.127
[72,     1] loss: 955.700
[73,     1] loss: 913.338
[74,     1] loss: 961.192
[75,     1] loss: 926.982
[76,     1] loss: 949.724
[77,     1] loss: 986.008
[78,     1] loss: 895.869
[79,     1] loss: 961.354
[80,     1] loss: 886.407
[81,     1] loss: 933.697
[82,     1] loss: 903.044
[83,     1] loss: 872.033
[84,     1] loss: 765.989
[85,     1] loss: 843.664
[86,     1] loss: 824.774
[87,     1] loss: 854.996
[88,     1] loss: 816.245
[89,     1] loss: 825.108
[90,     1] loss: 721.954
[91,     1] loss: 730.142
[92,     1] loss: 740.511
[93,     1] loss: 731.759
Early stopping applied (best metric=0.36570224165916443)
Finished Training
Total time taken: 14.108857154846191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1539.235
[2,     1] loss: 1523.304
[3,     1] loss: 1530.316
[4,     1] loss: 1527.895
[5,     1] loss: 1525.802
[6,     1] loss: 1528.419
[7,     1] loss: 1524.437
[8,     1] loss: 1521.964
[9,     1] loss: 1524.186
[10,     1] loss: 1524.032
[11,     1] loss: 1521.349
[12,     1] loss: 1520.268
[13,     1] loss: 1517.754
[14,     1] loss: 1511.060
[15,     1] loss: 1504.471
[16,     1] loss: 1494.130
[17,     1] loss: 1483.933
[18,     1] loss: 1479.200
[19,     1] loss: 1445.002
[20,     1] loss: 1428.263
[21,     1] loss: 1404.019
[22,     1] loss: 1385.497
[23,     1] loss: 1348.269
[24,     1] loss: 1367.035
[25,     1] loss: 1295.297
[26,     1] loss: 1297.459
[27,     1] loss: 1304.841
[28,     1] loss: 1277.079
[29,     1] loss: 1283.398
[30,     1] loss: 1284.554
[31,     1] loss: 1269.670
[32,     1] loss: 1243.934
[33,     1] loss: 1279.667
[34,     1] loss: 1285.404
[35,     1] loss: 1259.715
[36,     1] loss: 1284.762
[37,     1] loss: 1214.220
[38,     1] loss: 1208.336
[39,     1] loss: 1189.566
[40,     1] loss: 1267.820
[41,     1] loss: 1201.698
[42,     1] loss: 1198.686
[43,     1] loss: 1201.782
[44,     1] loss: 1179.404
[45,     1] loss: 1191.164
[46,     1] loss: 1186.551
[47,     1] loss: 1138.560
[48,     1] loss: 1127.154
[49,     1] loss: 1144.807
[50,     1] loss: 1113.231
[51,     1] loss: 1216.425
[52,     1] loss: 1118.340
[53,     1] loss: 1141.391
[54,     1] loss: 1105.329
[55,     1] loss: 1100.833
[56,     1] loss: 1132.901
[57,     1] loss: 1020.625
[58,     1] loss: 1063.436
[59,     1] loss: 1053.823
[60,     1] loss: 1041.267
[61,     1] loss: 1049.526
[62,     1] loss: 941.255
[63,     1] loss: 929.164
[64,     1] loss: 1000.195
[65,     1] loss: 1004.364
[66,     1] loss: 1025.931
[67,     1] loss: 1016.763
[68,     1] loss: 895.905
[69,     1] loss: 872.610
[70,     1] loss: 956.325
[71,     1] loss: 957.597
[72,     1] loss: 942.210
[73,     1] loss: 930.717
[74,     1] loss: 941.502
[75,     1] loss: 894.237
[76,     1] loss: 911.518
[77,     1] loss: 855.531
[78,     1] loss: 868.700
[79,     1] loss: 881.002
[80,     1] loss: 801.219
[81,     1] loss: 916.285
[82,     1] loss: 883.472
[83,     1] loss: 883.339
[84,     1] loss: 831.868
[85,     1] loss: 815.187
[86,     1] loss: 940.724
[87,     1] loss: 773.258
[88,     1] loss: 901.163
[89,     1] loss: 822.915
[90,     1] loss: 756.284
[91,     1] loss: 931.128
[92,     1] loss: 796.428
[93,     1] loss: 809.866
[94,     1] loss: 790.970
[95,     1] loss: 762.881
[96,     1] loss: 715.691
[97,     1] loss: 777.705
[98,     1] loss: 714.679
[99,     1] loss: 808.042
[100,     1] loss: 765.898
[101,     1] loss: 715.461
[102,     1] loss: 709.300
[103,     1] loss: 753.428
[104,     1] loss: 657.941
[105,     1] loss: 681.151
[106,     1] loss: 714.080
[107,     1] loss: 658.896
[108,     1] loss: 700.836
[109,     1] loss: 683.364
[110,     1] loss: 617.071
[111,     1] loss: 616.277
[112,     1] loss: 589.370
[113,     1] loss: 653.411
[114,     1] loss: 626.546
Early stopping applied (best metric=0.3937704563140869)
Finished Training
Total time taken: 16.108540058135986
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1529.989
[2,     1] loss: 1521.583
[3,     1] loss: 1531.511
[4,     1] loss: 1529.160
[5,     1] loss: 1526.952
[6,     1] loss: 1525.480
[7,     1] loss: 1523.709
[8,     1] loss: 1526.785
[9,     1] loss: 1522.891
[10,     1] loss: 1528.851
[11,     1] loss: 1521.662
[12,     1] loss: 1523.500
[13,     1] loss: 1520.288
[14,     1] loss: 1516.184
[15,     1] loss: 1514.821
[16,     1] loss: 1504.708
[17,     1] loss: 1495.729
[18,     1] loss: 1491.916
[19,     1] loss: 1474.670
[20,     1] loss: 1459.242
[21,     1] loss: 1437.664
[22,     1] loss: 1420.626
[23,     1] loss: 1388.781
[24,     1] loss: 1367.531
[25,     1] loss: 1382.598
[26,     1] loss: 1337.970
[27,     1] loss: 1305.810
[28,     1] loss: 1281.215
[29,     1] loss: 1301.091
[30,     1] loss: 1299.406
[31,     1] loss: 1294.516
[32,     1] loss: 1257.858
[33,     1] loss: 1239.135
[34,     1] loss: 1226.771
[35,     1] loss: 1249.170
[36,     1] loss: 1299.004
[37,     1] loss: 1167.709
[38,     1] loss: 1233.403
[39,     1] loss: 1198.322
[40,     1] loss: 1218.941
[41,     1] loss: 1215.224
[42,     1] loss: 1170.276
[43,     1] loss: 1200.368
[44,     1] loss: 1193.430
[45,     1] loss: 1162.611
[46,     1] loss: 1160.629
[47,     1] loss: 1117.998
[48,     1] loss: 1152.775
[49,     1] loss: 1121.684
[50,     1] loss: 1095.262
[51,     1] loss: 1077.081
[52,     1] loss: 1117.062
[53,     1] loss: 1029.830
[54,     1] loss: 1062.720
[55,     1] loss: 1035.132
[56,     1] loss: 1100.806
[57,     1] loss: 1091.779
[58,     1] loss: 1067.788
[59,     1] loss: 1072.342
[60,     1] loss: 1064.153
[61,     1] loss: 985.513
[62,     1] loss: 972.151
[63,     1] loss: 1037.293
[64,     1] loss: 987.043
[65,     1] loss: 1031.884
[66,     1] loss: 988.590
[67,     1] loss: 994.922
[68,     1] loss: 947.432
[69,     1] loss: 945.462
[70,     1] loss: 893.669
[71,     1] loss: 939.274
[72,     1] loss: 880.096
[73,     1] loss: 953.901
[74,     1] loss: 868.813
[75,     1] loss: 946.683
[76,     1] loss: 853.330
[77,     1] loss: 845.619
[78,     1] loss: 842.401
[79,     1] loss: 818.846
[80,     1] loss: 842.454
[81,     1] loss: 808.224
[82,     1] loss: 776.445
[83,     1] loss: 824.117
[84,     1] loss: 851.192
[85,     1] loss: 836.349
[86,     1] loss: 812.133
[87,     1] loss: 715.201
[88,     1] loss: 834.856
[89,     1] loss: 750.234
[90,     1] loss: 771.698
Early stopping applied (best metric=0.40521955490112305)
Finished Training
Total time taken: 12.738229990005493
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1530.743
[2,     1] loss: 1526.868
[3,     1] loss: 1529.043
[4,     1] loss: 1530.761
[5,     1] loss: 1526.470
[6,     1] loss: 1530.675
[7,     1] loss: 1525.463
[8,     1] loss: 1525.786
[9,     1] loss: 1523.710
[10,     1] loss: 1518.524
[11,     1] loss: 1515.939
[12,     1] loss: 1516.652
[13,     1] loss: 1506.211
[14,     1] loss: 1493.674
[15,     1] loss: 1489.898
[16,     1] loss: 1464.209
[17,     1] loss: 1448.037
[18,     1] loss: 1426.106
[19,     1] loss: 1409.496
[20,     1] loss: 1384.123
[21,     1] loss: 1371.148
[22,     1] loss: 1354.890
[23,     1] loss: 1312.264
[24,     1] loss: 1343.753
[25,     1] loss: 1326.340
[26,     1] loss: 1304.721
[27,     1] loss: 1286.687
[28,     1] loss: 1317.974
[29,     1] loss: 1266.414
[30,     1] loss: 1274.631
[31,     1] loss: 1264.492
[32,     1] loss: 1267.565
[33,     1] loss: 1252.639
[34,     1] loss: 1269.994
[35,     1] loss: 1229.530
[36,     1] loss: 1204.456
[37,     1] loss: 1244.240
[38,     1] loss: 1198.322
[39,     1] loss: 1192.792
[40,     1] loss: 1186.609
[41,     1] loss: 1212.873
[42,     1] loss: 1204.847
[43,     1] loss: 1172.926
[44,     1] loss: 1179.174
[45,     1] loss: 1135.460
[46,     1] loss: 1137.601
[47,     1] loss: 1125.149
[48,     1] loss: 1094.769
[49,     1] loss: 1063.453
[50,     1] loss: 1098.431
[51,     1] loss: 1065.304
[52,     1] loss: 1115.288
[53,     1] loss: 1054.999
[54,     1] loss: 1072.600
[55,     1] loss: 1142.697
[56,     1] loss: 1033.188
[57,     1] loss: 1050.328
[58,     1] loss: 990.740
[59,     1] loss: 975.447
[60,     1] loss: 1003.940
[61,     1] loss: 1015.775
[62,     1] loss: 959.998
[63,     1] loss: 1067.326
[64,     1] loss: 987.888
[65,     1] loss: 965.946
[66,     1] loss: 965.429
[67,     1] loss: 975.997
[68,     1] loss: 941.591
[69,     1] loss: 983.767
[70,     1] loss: 940.247
[71,     1] loss: 921.379
[72,     1] loss: 836.456
[73,     1] loss: 903.917
[74,     1] loss: 860.037
[75,     1] loss: 892.087
[76,     1] loss: 832.481
[77,     1] loss: 815.283
[78,     1] loss: 910.279
[79,     1] loss: 831.903
[80,     1] loss: 779.600
[81,     1] loss: 807.235
[82,     1] loss: 816.631
[83,     1] loss: 834.300
[84,     1] loss: 891.135
[85,     1] loss: 839.938
[86,     1] loss: 817.956
[87,     1] loss: 870.414
[88,     1] loss: 825.352
[89,     1] loss: 775.230
[90,     1] loss: 763.899
[91,     1] loss: 794.656
[92,     1] loss: 768.918
[93,     1] loss: 774.549
[94,     1] loss: 794.047
[95,     1] loss: 736.909
[96,     1] loss: 807.966
[97,     1] loss: 745.404
[98,     1] loss: 687.152
[99,     1] loss: 724.442
[100,     1] loss: 755.962
[101,     1] loss: 690.846
Early stopping applied (best metric=0.37378013134002686)
Finished Training
Total time taken: 14.130622863769531
{'Hydroxylation-K Validation Accuracy': 0.7550236406619385, 'Hydroxylation-K Validation Sensitivity': 0.6925925925925925, 'Hydroxylation-K Validation Specificity': 0.7701754385964912, 'Hydroxylation-K Validation Precision': 0.4366726024233764, 'Hydroxylation-K AUC ROC': 0.8238206627680311, 'Hydroxylation-K AUC PR': 0.6106422584616252, 'Hydroxylation-K MCC': 0.4009871551790747, 'Hydroxylation-K F1': 0.5270026307301493, 'Validation Loss (Hydroxylation-K)': 0.4396119991938273, 'Hydroxylation-P Validation Accuracy': 0.7748686530294571, 'Hydroxylation-P Validation Sensitivity': 0.7970899470899471, 'Hydroxylation-P Validation Specificity': 0.7700384059055314, 'Hydroxylation-P Validation Precision': 0.4308135787322121, 'Hydroxylation-P AUC ROC': 0.8361249640761088, 'Hydroxylation-P AUC PR': 0.5671713050540019, 'Hydroxylation-P MCC': 0.46258053325905707, 'Hydroxylation-P F1': 0.5575122954433444, 'Validation Loss (Hydroxylation-P)': 0.38182918826738993, 'Validation Loss (total)': 0.8214411894480388, 'TimeToTrain': 14.345563745498657}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024582231913403323,
 'learning_rate_Hydroxylation-K': 0.005676344972510847,
 'learning_rate_Hydroxylation-P': 0.00943963999220062,
 'log_base': 2.6866185704316625,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3400038098,
 'sample_weights': [2.9390317564778425, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.204594594062664,
 'weight_decay_Hydroxylation-K': 8.652902257678257,
 'weight_decay_Hydroxylation-P': 8.47500752969775}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.562
[2,     1] loss: 1263.233
[3,     1] loss: 1263.577
[4,     1] loss: 1264.380
[5,     1] loss: 1263.652
[6,     1] loss: 1261.705
[7,     1] loss: 1260.570
[8,     1] loss: 1253.355
[9,     1] loss: 1251.278
[10,     1] loss: 1232.631
[11,     1] loss: 1221.040
[12,     1] loss: 1203.426
[13,     1] loss: 1165.355
[14,     1] loss: 1160.713
[15,     1] loss: 1127.566
[16,     1] loss: 1093.247
[17,     1] loss: 1100.611
[18,     1] loss: 1071.172
[19,     1] loss: 1034.996
[20,     1] loss: 1012.778
[21,     1] loss: 1010.800
[22,     1] loss: 1009.644
[23,     1] loss: 1018.688
[24,     1] loss: 1093.528
[25,     1] loss: 1002.662
[26,     1] loss: 1048.986
[27,     1] loss: 979.712
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018135174044100293,
 'learning_rate_Hydroxylation-K': 0.002807226726857088,
 'learning_rate_Hydroxylation-P': 0.001754601113078444,
 'log_base': 1.1569007761968508,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 591526960,
 'sample_weights': [1.6892352993933337, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.310784915351707,
 'weight_decay_Hydroxylation-K': 4.08217344789935,
 'weight_decay_Hydroxylation-P': 2.395632576853357}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3717.052
[2,     1] loss: 3708.868
[3,     1] loss: 3720.189
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004233006277857351,
 'learning_rate_Hydroxylation-K': 0.0025092157806173468,
 'learning_rate_Hydroxylation-P': 0.007659686176140191,
 'log_base': 2.955048941977795,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1589090321,
 'sample_weights': [11.454573095717357, 1.4318767964837902],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.791581790335519,
 'weight_decay_Hydroxylation-K': 9.635897031289563,
 'weight_decay_Hydroxylation-P': 7.182079363598875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.749
[2,     1] loss: 1231.274
[3,     1] loss: 1231.327
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010554815704343766,
 'learning_rate_Hydroxylation-K': 0.009584252474795333,
 'learning_rate_Hydroxylation-P': 0.004790365239603214,
 'log_base': 1.8738062378998976,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 560661832,
 'sample_weights': [1.5407657660572691, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.7178073461934105,
 'weight_decay_Hydroxylation-K': 8.171070827337374,
 'weight_decay_Hydroxylation-P': 7.798754436578802}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1470.540
[2,     1] loss: 1468.156
[3,     1] loss: 1468.361
[4,     1] loss: 1465.033
[5,     1] loss: 1466.254
[6,     1] loss: 1466.132
[7,     1] loss: 1475.295
[8,     1] loss: 1464.063
[9,     1] loss: 1465.842
[10,     1] loss: 1464.648
[11,     1] loss: 1460.555
[12,     1] loss: 1458.779
[13,     1] loss: 1451.154
[14,     1] loss: 1451.479
[15,     1] loss: 1437.733
[16,     1] loss: 1432.873
[17,     1] loss: 1424.412
[18,     1] loss: 1405.897
[19,     1] loss: 1386.848
[20,     1] loss: 1353.976
[21,     1] loss: 1324.731
[22,     1] loss: 1302.965
[23,     1] loss: 1302.881
[24,     1] loss: 1252.542
[25,     1] loss: 1266.887
[26,     1] loss: 1235.296
[27,     1] loss: 1162.354
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026970057046830243,
 'learning_rate_Hydroxylation-K': 0.007654712410279183,
 'learning_rate_Hydroxylation-P': 0.007070007198968952,
 'log_base': 2.1270130736687514,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3798041585,
 'sample_weights': [2.658468407629563, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4780281335830507,
 'weight_decay_Hydroxylation-K': 9.736518442235091,
 'weight_decay_Hydroxylation-P': 2.329039255787006}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1371.053
[2,     1] loss: 1373.924
[3,     1] loss: 1374.692
[4,     1] loss: 1372.273
[5,     1] loss: 1370.349
[6,     1] loss: 1362.015
[7,     1] loss: 1359.237
[8,     1] loss: 1351.567
[9,     1] loss: 1334.645
[10,     1] loss: 1314.129
[11,     1] loss: 1270.194
[12,     1] loss: 1263.176
[13,     1] loss: 1230.056
[14,     1] loss: 1206.036
[15,     1] loss: 1185.942
[16,     1] loss: 1246.518
[17,     1] loss: 1176.080
[18,     1] loss: 1157.655
[19,     1] loss: 1165.090
[20,     1] loss: 1131.906
[21,     1] loss: 1159.586
[22,     1] loss: 1134.996
[23,     1] loss: 1124.634
[24,     1] loss: 1098.767
[25,     1] loss: 1113.734
[26,     1] loss: 1127.031
[27,     1] loss: 1099.979
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00884428155612756,
 'learning_rate_Hydroxylation-K': 0.0043770877715040984,
 'learning_rate_Hydroxylation-P': 0.006668918748926225,
 'log_base': 2.661136689609996,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 746191534,
 'sample_weights': [2.2120071831098977, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8651564912888077,
 'weight_decay_Hydroxylation-K': 9.545980491006773,
 'weight_decay_Hydroxylation-P': 9.968464546828804}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.066
[2,     1] loss: 1272.812
[3,     1] loss: 1266.043
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026022598035979395,
 'learning_rate_Hydroxylation-K': 0.009906216880363193,
 'learning_rate_Hydroxylation-P': 0.006904569170049979,
 'log_base': 2.9918760922223044,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1419355247,
 'sample_weights': [1.7056831867126216, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.974659442371994,
 'weight_decay_Hydroxylation-K': 6.836373817401252,
 'weight_decay_Hydroxylation-P': 8.01317203728767}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.683
[2,     1] loss: 1228.895
[3,     1] loss: 1228.632
[4,     1] loss: 1228.327
[5,     1] loss: 1226.702
[6,     1] loss: 1220.033
[7,     1] loss: 1212.888
[8,     1] loss: 1205.700
[9,     1] loss: 1182.835
[10,     1] loss: 1153.646
[11,     1] loss: 1110.752
[12,     1] loss: 1087.074
[13,     1] loss: 1062.349
[14,     1] loss: 1060.997
[15,     1] loss: 1024.972
[16,     1] loss: 1035.321
[17,     1] loss: 989.921
[18,     1] loss: 990.905
[19,     1] loss: 991.542
[20,     1] loss: 1039.527
[21,     1] loss: 1001.473
[22,     1] loss: 971.940
[23,     1] loss: 965.392
[24,     1] loss: 1026.781
[25,     1] loss: 963.501
[26,     1] loss: 948.318
[27,     1] loss: 941.922
[28,     1] loss: 979.107
[29,     1] loss: 951.688
[30,     1] loss: 967.589
[31,     1] loss: 935.068
[32,     1] loss: 949.160
[33,     1] loss: 940.161
[34,     1] loss: 891.459
[35,     1] loss: 908.921
[36,     1] loss: 914.128
[37,     1] loss: 880.431
[38,     1] loss: 922.068
[39,     1] loss: 850.085
[40,     1] loss: 832.476
[41,     1] loss: 851.777
[42,     1] loss: 859.730
[43,     1] loss: 898.655
[44,     1] loss: 855.851
[45,     1] loss: 895.278
[46,     1] loss: 819.525
[47,     1] loss: 808.414
[48,     1] loss: 830.751
[49,     1] loss: 835.576
[50,     1] loss: 797.500
[51,     1] loss: 765.927
[52,     1] loss: 800.826
[53,     1] loss: 826.053
[54,     1] loss: 764.969
[55,     1] loss: 800.195
[56,     1] loss: 770.126
[57,     1] loss: 801.223
[58,     1] loss: 736.410
[59,     1] loss: 806.563
[60,     1] loss: 841.601
[61,     1] loss: 758.799
[62,     1] loss: 744.613
[63,     1] loss: 733.988
[64,     1] loss: 724.891
[65,     1] loss: 710.043
[66,     1] loss: 703.861
[67,     1] loss: 678.376
[68,     1] loss: 704.975
[69,     1] loss: 625.512
[70,     1] loss: 636.968
[71,     1] loss: 606.151
[72,     1] loss: 591.228
[73,     1] loss: 618.456
[74,     1] loss: 604.658
[75,     1] loss: 638.584
[76,     1] loss: 623.801
[77,     1] loss: 602.107
[78,     1] loss: 611.445
[79,     1] loss: 577.958
[80,     1] loss: 610.239
[81,     1] loss: 609.756
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003057655233130472,
 'learning_rate_Hydroxylation-K': 0.008862715754751848,
 'learning_rate_Hydroxylation-P': 0.008615723129375452,
 'log_base': 2.6406975059359943,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 124958998,
 'sample_weights': [1.5233526443940866, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.747078985407136,
 'weight_decay_Hydroxylation-K': 8.01513814295998,
 'weight_decay_Hydroxylation-P': 1.0335832213049634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.800
[2,     1] loss: 1271.041
[3,     1] loss: 1269.435
[4,     1] loss: 1268.495
[5,     1] loss: 1266.486
[6,     1] loss: 1263.635
[7,     1] loss: 1257.020
[8,     1] loss: 1240.396
[9,     1] loss: 1231.193
[10,     1] loss: 1201.207
[11,     1] loss: 1152.815
[12,     1] loss: 1161.111
[13,     1] loss: 1090.484
[14,     1] loss: 1065.322
[15,     1] loss: 1062.397
[16,     1] loss: 1061.622
[17,     1] loss: 1036.814
[18,     1] loss: 1081.292
[19,     1] loss: 1017.956
[20,     1] loss: 1082.964
[21,     1] loss: 1027.124
[22,     1] loss: 1007.870
[23,     1] loss: 982.564
[24,     1] loss: 991.723
[25,     1] loss: 1021.074
[26,     1] loss: 1017.452
[27,     1] loss: 947.544
[28,     1] loss: 944.617
[29,     1] loss: 952.319
[30,     1] loss: 957.919
[31,     1] loss: 955.109
[32,     1] loss: 955.414
[33,     1] loss: 961.562
[34,     1] loss: 935.350
[35,     1] loss: 901.557
[36,     1] loss: 924.492
[37,     1] loss: 914.267
[38,     1] loss: 921.798
[39,     1] loss: 902.610
[40,     1] loss: 907.995
[41,     1] loss: 876.632
[42,     1] loss: 885.760
[43,     1] loss: 895.910
[44,     1] loss: 864.206
[45,     1] loss: 841.620
[46,     1] loss: 822.710
[47,     1] loss: 798.983
[48,     1] loss: 840.422
[49,     1] loss: 780.076
[50,     1] loss: 801.497
[51,     1] loss: 774.919
[52,     1] loss: 779.688
[53,     1] loss: 741.045
[54,     1] loss: 822.232
[55,     1] loss: 997.128
[56,     1] loss: 912.247
[57,     1] loss: 760.933
[58,     1] loss: 872.409
[59,     1] loss: 786.496
[60,     1] loss: 808.446
[61,     1] loss: 853.246
[62,     1] loss: 784.148
[63,     1] loss: 798.685
[64,     1] loss: 749.423
[65,     1] loss: 717.720
[66,     1] loss: 811.382
[67,     1] loss: 693.510
[68,     1] loss: 801.637
[69,     1] loss: 685.615
[70,     1] loss: 715.788
[71,     1] loss: 683.311
[72,     1] loss: 799.648
[73,     1] loss: 640.460
[74,     1] loss: 756.154
[75,     1] loss: 666.706
[76,     1] loss: 807.953
[77,     1] loss: 659.823
[78,     1] loss: 651.716
[79,     1] loss: 622.336
[80,     1] loss: 673.458
[81,     1] loss: 582.615
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009225498922871691,
 'learning_rate_Hydroxylation-K': 0.0036901774466499607,
 'learning_rate_Hydroxylation-P': 0.009381333580533245,
 'log_base': 1.5046350989074548,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2921388871,
 'sample_weights': [1.719226640135145, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9523208158799898,
 'weight_decay_Hydroxylation-K': 1.7312539336387955,
 'weight_decay_Hydroxylation-P': 6.800300141695322}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1769.760
[2,     1] loss: 1771.666
[3,     1] loss: 1765.097
[4,     1] loss: 1768.200
[5,     1] loss: 1774.491
[6,     1] loss: 1767.143
[7,     1] loss: 1767.807
[8,     1] loss: 1762.457
[9,     1] loss: 1771.074
[10,     1] loss: 1767.342
[11,     1] loss: 1768.677
[12,     1] loss: 1769.119
[13,     1] loss: 1764.653
[14,     1] loss: 1758.761
[15,     1] loss: 1758.763
[16,     1] loss: 1766.109
[17,     1] loss: 1760.828
[18,     1] loss: 1750.555
[19,     1] loss: 1743.814
[20,     1] loss: 1741.644
[21,     1] loss: 1724.593
[22,     1] loss: 1701.326
[23,     1] loss: 1670.301
[24,     1] loss: 1658.819
[25,     1] loss: 1635.274
[26,     1] loss: 1633.211
[27,     1] loss: 1573.349
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005635564507247734,
 'learning_rate_Hydroxylation-K': 0.0018716766174096998,
 'learning_rate_Hydroxylation-P': 0.00835803124061007,
 'log_base': 1.3999033448691685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2402824049,
 'sample_weights': [4.086259879257315, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7579380246801835,
 'weight_decay_Hydroxylation-K': 6.9016215955653015,
 'weight_decay_Hydroxylation-P': 1.0192858675085683}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1949.659
[2,     1] loss: 1961.201
[3,     1] loss: 1956.471
[4,     1] loss: 1953.762
[5,     1] loss: 1948.333
[6,     1] loss: 1942.833
[7,     1] loss: 1931.165
[8,     1] loss: 1922.735
[9,     1] loss: 1891.680
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007299051954076149,
 'learning_rate_Hydroxylation-K': 0.007411891263049557,
 'learning_rate_Hydroxylation-P': 0.009804590105506064,
 'log_base': 2.918954649161695,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2455916159,
 'sample_weights': [4.962625720973882, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.842432990025762,
 'weight_decay_Hydroxylation-K': 9.373478158040248,
 'weight_decay_Hydroxylation-P': 7.507104159417298}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.167
[2,     1] loss: 1249.196
[3,     1] loss: 1242.305
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004022374734145772,
 'learning_rate_Hydroxylation-K': 0.008587852980969295,
 'learning_rate_Hydroxylation-P': 0.008798528854710339,
 'log_base': 1.9148818368149962,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 341620932,
 'sample_weights': [1.5584422338790311, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.167210222867274,
 'weight_decay_Hydroxylation-K': 7.168234243206315,
 'weight_decay_Hydroxylation-P': 0.0078109907834540415}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1451.627
[2,     1] loss: 1448.888
[3,     1] loss: 1459.009
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009936074207287572,
 'learning_rate_Hydroxylation-K': 0.009513704770714441,
 'learning_rate_Hydroxylation-P': 0.0006895335960914253,
 'log_base': 2.3487855820865016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2734943227,
 'sample_weights': [2.5697343847235956, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7192590513217114,
 'weight_decay_Hydroxylation-K': 7.459472194341403,
 'weight_decay_Hydroxylation-P': 1.3432341772576795}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.797
[2,     1] loss: 1316.294
[3,     1] loss: 1317.980
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0054922381356967516,
 'learning_rate_Hydroxylation-K': 0.009797016508589664,
 'learning_rate_Hydroxylation-P': 0.0034669232774752505,
 'log_base': 1.9740379558429597,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 18360172,
 'sample_weights': [1.9550840080766951, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.370119566796437,
 'weight_decay_Hydroxylation-K': 6.529800124745312,
 'weight_decay_Hydroxylation-P': 1.4152259620905248}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1425.423
[2,     1] loss: 1432.449
[3,     1] loss: 1430.330
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061065627958169204,
 'learning_rate_Hydroxylation-K': 0.007947743743563808,
 'learning_rate_Hydroxylation-P': 0.0049762477714346024,
 'log_base': 2.9903800725026506,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 458589786,
 'sample_weights': [2.4547704365779235, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.988253240857052,
 'weight_decay_Hydroxylation-K': 9.90317723193258,
 'weight_decay_Hydroxylation-P': 7.655340452771291}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.960
[2,     1] loss: 1233.570
[3,     1] loss: 1230.528
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035096722168788397,
 'learning_rate_Hydroxylation-K': 0.0019225219301484174,
 'learning_rate_Hydroxylation-P': 0.005936750749700314,
 'log_base': 2.324966694052037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 342827708,
 'sample_weights': [1.5240481967651018, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.94271031213751,
 'weight_decay_Hydroxylation-K': 3.8661288515867986,
 'weight_decay_Hydroxylation-P': 8.97771249275199}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1325.455
[2,     1] loss: 1325.508
[3,     1] loss: 1324.049
[4,     1] loss: 1324.067
[5,     1] loss: 1322.974
[6,     1] loss: 1321.053
[7,     1] loss: 1318.019
[8,     1] loss: 1315.652
[9,     1] loss: 1314.515
[10,     1] loss: 1292.167
[11,     1] loss: 1277.868
[12,     1] loss: 1246.795
[13,     1] loss: 1211.283
[14,     1] loss: 1178.909
[15,     1] loss: 1168.385
[16,     1] loss: 1149.598
[17,     1] loss: 1103.460
[18,     1] loss: 1111.561
[19,     1] loss: 1088.430
[20,     1] loss: 1074.979
[21,     1] loss: 1089.356
[22,     1] loss: 1068.015
[23,     1] loss: 1085.894
[24,     1] loss: 1065.332
[25,     1] loss: 1060.458
[26,     1] loss: 1032.775
[27,     1] loss: 1057.451
[28,     1] loss: 1054.938
[29,     1] loss: 1033.204
[30,     1] loss: 979.329
[31,     1] loss: 936.811
[32,     1] loss: 997.866
[33,     1] loss: 980.084
[34,     1] loss: 1003.908
[35,     1] loss: 946.272
[36,     1] loss: 1027.483
[37,     1] loss: 903.365
[38,     1] loss: 970.519
[39,     1] loss: 958.270
[40,     1] loss: 929.650
[41,     1] loss: 886.906
[42,     1] loss: 960.253
[43,     1] loss: 939.324
[44,     1] loss: 952.747
[45,     1] loss: 839.555
[46,     1] loss: 944.681
[47,     1] loss: 847.573
[48,     1] loss: 889.079
[49,     1] loss: 865.234
[50,     1] loss: 795.866
[51,     1] loss: 813.305
[52,     1] loss: 812.271
[53,     1] loss: 804.503
[54,     1] loss: 847.520
[55,     1] loss: 905.482
[56,     1] loss: 775.250
[57,     1] loss: 942.649
[58,     1] loss: 875.411
[59,     1] loss: 897.808
[60,     1] loss: 817.749
[61,     1] loss: 845.522
[62,     1] loss: 773.106
[63,     1] loss: 868.960
[64,     1] loss: 811.739
[65,     1] loss: 793.988
[66,     1] loss: 734.687
[67,     1] loss: 691.792
[68,     1] loss: 705.203
[69,     1] loss: 716.952
[70,     1] loss: 708.148
[71,     1] loss: 647.794
[72,     1] loss: 702.578
[73,     1] loss: 707.254
[74,     1] loss: 782.992
[75,     1] loss: 741.675
[76,     1] loss: 611.190
[77,     1] loss: 662.389
[78,     1] loss: 723.636
[79,     1] loss: 740.167
[80,     1] loss: 599.669
[81,     1] loss: 646.356
[82,     1] loss: 593.799
[83,     1] loss: 607.228
[84,     1] loss: 647.476
[85,     1] loss: 697.521
[86,     1] loss: 867.480
[87,     1] loss: 731.652
[88,     1] loss: 631.399
[89,     1] loss: 675.259
[90,     1] loss: 622.258
[91,     1] loss: 641.914
[92,     1] loss: 637.058
[93,     1] loss: 602.030
[94,     1] loss: 683.237
[95,     1] loss: 923.378
[96,     1] loss: 568.427
[97,     1] loss: 812.096
[98,     1] loss: 667.923
[99,     1] loss: 678.330
[100,     1] loss: 656.268
[101,     1] loss: 760.564
[102,     1] loss: 662.455
[103,     1] loss: 667.962
[104,     1] loss: 621.068
[105,     1] loss: 644.637
Early stopping applied (best metric=0.34841489791870117)
Finished Training
Total time taken: 15.16876769065857
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.035
[2,     1] loss: 1329.249
[3,     1] loss: 1323.210
[4,     1] loss: 1326.400
[5,     1] loss: 1324.365
[6,     1] loss: 1323.223
[7,     1] loss: 1319.614
[8,     1] loss: 1318.613
[9,     1] loss: 1307.519
[10,     1] loss: 1299.095
[11,     1] loss: 1281.059
[12,     1] loss: 1244.256
[13,     1] loss: 1218.825
[14,     1] loss: 1191.752
[15,     1] loss: 1146.382
[16,     1] loss: 1126.647
[17,     1] loss: 1109.811
[18,     1] loss: 1158.433
[19,     1] loss: 1106.802
[20,     1] loss: 1090.048
[21,     1] loss: 1099.708
[22,     1] loss: 1140.893
[23,     1] loss: 1052.271
[24,     1] loss: 1054.198
[25,     1] loss: 1040.663
[26,     1] loss: 1064.361
[27,     1] loss: 1026.564
[28,     1] loss: 1029.509
[29,     1] loss: 990.699
[30,     1] loss: 1019.748
[31,     1] loss: 980.783
[32,     1] loss: 1035.885
[33,     1] loss: 959.790
[34,     1] loss: 993.980
[35,     1] loss: 999.346
[36,     1] loss: 1002.187
[37,     1] loss: 958.548
[38,     1] loss: 914.299
[39,     1] loss: 940.511
[40,     1] loss: 924.785
[41,     1] loss: 929.623
[42,     1] loss: 884.939
[43,     1] loss: 883.252
[44,     1] loss: 880.084
[45,     1] loss: 834.640
[46,     1] loss: 895.067
[47,     1] loss: 856.732
[48,     1] loss: 837.024
[49,     1] loss: 856.473
[50,     1] loss: 899.011
[51,     1] loss: 847.009
[52,     1] loss: 827.172
[53,     1] loss: 812.767
[54,     1] loss: 895.556
[55,     1] loss: 837.668
[56,     1] loss: 788.759
[57,     1] loss: 725.703
[58,     1] loss: 748.681
[59,     1] loss: 777.286
[60,     1] loss: 824.448
[61,     1] loss: 1040.665
[62,     1] loss: 842.452
[63,     1] loss: 775.678
[64,     1] loss: 821.181
[65,     1] loss: 810.552
[66,     1] loss: 804.865
[67,     1] loss: 914.018
[68,     1] loss: 756.097
[69,     1] loss: 849.330
[70,     1] loss: 754.244
[71,     1] loss: 865.978
[72,     1] loss: 731.737
[73,     1] loss: 824.310
[74,     1] loss: 704.265
[75,     1] loss: 860.829
[76,     1] loss: 734.934
[77,     1] loss: 736.905
[78,     1] loss: 729.019
[79,     1] loss: 767.399
[80,     1] loss: 755.076
Early stopping applied (best metric=0.38172778487205505)
Finished Training
Total time taken: 11.71998643875122
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.260
[2,     1] loss: 1328.703
[3,     1] loss: 1323.016
[4,     1] loss: 1322.675
[5,     1] loss: 1319.984
[6,     1] loss: 1314.292
[7,     1] loss: 1297.311
[8,     1] loss: 1275.153
[9,     1] loss: 1238.723
[10,     1] loss: 1210.877
[11,     1] loss: 1171.021
[12,     1] loss: 1137.758
[13,     1] loss: 1139.627
[14,     1] loss: 1099.284
[15,     1] loss: 1109.718
[16,     1] loss: 1091.859
[17,     1] loss: 1187.595
[18,     1] loss: 1060.128
[19,     1] loss: 1071.958
[20,     1] loss: 1051.995
[21,     1] loss: 1079.052
[22,     1] loss: 1042.289
[23,     1] loss: 1037.900
[24,     1] loss: 1046.461
[25,     1] loss: 1042.237
[26,     1] loss: 1024.569
[27,     1] loss: 988.457
[28,     1] loss: 997.186
[29,     1] loss: 1010.808
[30,     1] loss: 973.648
[31,     1] loss: 994.650
[32,     1] loss: 974.475
[33,     1] loss: 969.529
[34,     1] loss: 954.021
[35,     1] loss: 966.305
[36,     1] loss: 961.924
[37,     1] loss: 916.881
[38,     1] loss: 976.799
[39,     1] loss: 967.939
[40,     1] loss: 959.006
[41,     1] loss: 943.676
[42,     1] loss: 873.876
[43,     1] loss: 925.099
[44,     1] loss: 872.941
[45,     1] loss: 891.679
[46,     1] loss: 913.687
[47,     1] loss: 857.790
[48,     1] loss: 860.974
[49,     1] loss: 825.228
[50,     1] loss: 869.676
[51,     1] loss: 863.423
[52,     1] loss: 794.398
[53,     1] loss: 967.337
[54,     1] loss: 1128.789
[55,     1] loss: 801.682
[56,     1] loss: 994.809
[57,     1] loss: 896.647
[58,     1] loss: 854.949
[59,     1] loss: 915.067
[60,     1] loss: 934.824
[61,     1] loss: 839.451
[62,     1] loss: 859.903
[63,     1] loss: 813.878
[64,     1] loss: 787.636
[65,     1] loss: 803.380
[66,     1] loss: 761.831
[67,     1] loss: 823.302
[68,     1] loss: 806.458
[69,     1] loss: 710.476
[70,     1] loss: 865.669
[71,     1] loss: 758.560
[72,     1] loss: 809.138
[73,     1] loss: 718.401
[74,     1] loss: 720.364
[75,     1] loss: 760.829
[76,     1] loss: 750.701
[77,     1] loss: 676.911
[78,     1] loss: 658.564
[79,     1] loss: 690.652
[80,     1] loss: 754.246
[81,     1] loss: 734.529
[82,     1] loss: 710.379
[83,     1] loss: 651.222
[84,     1] loss: 783.035
[85,     1] loss: 896.910
[86,     1] loss: 687.810
[87,     1] loss: 764.303
[88,     1] loss: 685.953
[89,     1] loss: 699.479
[90,     1] loss: 638.116
[91,     1] loss: 685.715
[92,     1] loss: 680.782
[93,     1] loss: 592.415
[94,     1] loss: 571.173
[95,     1] loss: 585.253
[96,     1] loss: 567.438
[97,     1] loss: 555.989
[98,     1] loss: 654.595
[99,     1] loss: 974.831
[100,     1] loss: 1083.282
Early stopping applied (best metric=0.33994144201278687)
Finished Training
Total time taken: 14.644652366638184
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1335.316
[2,     1] loss: 1330.651
[3,     1] loss: 1329.992
[4,     1] loss: 1328.370
[5,     1] loss: 1323.791
[6,     1] loss: 1323.973
[7,     1] loss: 1323.664
[8,     1] loss: 1322.500
[9,     1] loss: 1322.749
[10,     1] loss: 1313.665
[11,     1] loss: 1310.312
[12,     1] loss: 1295.349
[13,     1] loss: 1272.415
[14,     1] loss: 1251.303
[15,     1] loss: 1235.821
[16,     1] loss: 1183.693
[17,     1] loss: 1202.929
[18,     1] loss: 1152.558
[19,     1] loss: 1173.504
[20,     1] loss: 1122.000
[21,     1] loss: 1130.989
[22,     1] loss: 1121.021
[23,     1] loss: 1079.186
[24,     1] loss: 1092.710
[25,     1] loss: 1081.782
[26,     1] loss: 1117.474
[27,     1] loss: 1042.140
[28,     1] loss: 1063.916
[29,     1] loss: 1049.169
[30,     1] loss: 1042.301
[31,     1] loss: 1069.552
[32,     1] loss: 1019.675
[33,     1] loss: 1056.277
[34,     1] loss: 958.471
[35,     1] loss: 977.943
[36,     1] loss: 994.298
[37,     1] loss: 962.775
[38,     1] loss: 966.887
[39,     1] loss: 924.561
[40,     1] loss: 970.079
[41,     1] loss: 981.412
[42,     1] loss: 931.147
[43,     1] loss: 917.786
[44,     1] loss: 929.033
[45,     1] loss: 906.419
[46,     1] loss: 897.033
[47,     1] loss: 838.365
[48,     1] loss: 934.393
[49,     1] loss: 888.375
[50,     1] loss: 845.322
[51,     1] loss: 865.075
[52,     1] loss: 870.622
[53,     1] loss: 796.516
[54,     1] loss: 917.568
[55,     1] loss: 1121.519
[56,     1] loss: 834.502
[57,     1] loss: 962.291
[58,     1] loss: 883.037
[59,     1] loss: 965.623
[60,     1] loss: 883.770
[61,     1] loss: 888.363
[62,     1] loss: 878.490
[63,     1] loss: 851.645
[64,     1] loss: 846.238
[65,     1] loss: 865.757
[66,     1] loss: 779.861
[67,     1] loss: 831.854
[68,     1] loss: 729.148
[69,     1] loss: 878.048
[70,     1] loss: 805.044
[71,     1] loss: 883.775
[72,     1] loss: 899.841
[73,     1] loss: 742.768
[74,     1] loss: 838.923
[75,     1] loss: 726.452
[76,     1] loss: 779.579
[77,     1] loss: 736.699
[78,     1] loss: 752.120
[79,     1] loss: 735.925
[80,     1] loss: 704.433
[81,     1] loss: 767.187
[82,     1] loss: 819.742
[83,     1] loss: 691.020
[84,     1] loss: 676.702
[85,     1] loss: 809.026
[86,     1] loss: 658.679
[87,     1] loss: 809.908
[88,     1] loss: 754.612
[89,     1] loss: 691.391
[90,     1] loss: 831.339
[91,     1] loss: 703.894
[92,     1] loss: 719.650
[93,     1] loss: 622.085
[94,     1] loss: 688.754
[95,     1] loss: 674.018
[96,     1] loss: 621.959
[97,     1] loss: 628.400
[98,     1] loss: 651.698
[99,     1] loss: 591.075
[100,     1] loss: 542.485
[101,     1] loss: 582.372
[102,     1] loss: 603.239
[103,     1] loss: 701.526
[104,     1] loss: 805.889
[105,     1] loss: 1178.073
[106,     1] loss: 672.603
[107,     1] loss: 978.108
[108,     1] loss: 792.072
[109,     1] loss: 829.709
[110,     1] loss: 873.079
[111,     1] loss: 732.335
[112,     1] loss: 720.653
[113,     1] loss: 800.144
[114,     1] loss: 780.625
[115,     1] loss: 736.343
[116,     1] loss: 735.269
[117,     1] loss: 642.184
[118,     1] loss: 704.905
[119,     1] loss: 576.225
[120,     1] loss: 629.190
[121,     1] loss: 643.138
[122,     1] loss: 558.825
[123,     1] loss: 626.359
[124,     1] loss: 685.489
[125,     1] loss: 576.898
[126,     1] loss: 541.085
[127,     1] loss: 523.188
[128,     1] loss: 549.780
Early stopping applied (best metric=0.3535850942134857)
Finished Training
Total time taken: 19.303611040115356
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1331.991
[2,     1] loss: 1324.904
[3,     1] loss: 1329.631
[4,     1] loss: 1325.596
[5,     1] loss: 1323.985
[6,     1] loss: 1326.307
[7,     1] loss: 1322.767
[8,     1] loss: 1320.965
[9,     1] loss: 1313.258
[10,     1] loss: 1303.941
[11,     1] loss: 1290.464
[12,     1] loss: 1267.267
[13,     1] loss: 1250.297
[14,     1] loss: 1220.413
[15,     1] loss: 1184.614
[16,     1] loss: 1146.766
[17,     1] loss: 1109.823
[18,     1] loss: 1071.816
[19,     1] loss: 1076.589
[20,     1] loss: 1060.690
[21,     1] loss: 1054.240
[22,     1] loss: 1075.096
[23,     1] loss: 1035.978
[24,     1] loss: 1047.422
[25,     1] loss: 1043.199
[26,     1] loss: 1022.871
[27,     1] loss: 992.252
[28,     1] loss: 990.692
[29,     1] loss: 1015.984
[30,     1] loss: 999.147
[31,     1] loss: 959.698
[32,     1] loss: 971.179
[33,     1] loss: 958.666
[34,     1] loss: 975.895
[35,     1] loss: 984.245
[36,     1] loss: 949.514
[37,     1] loss: 939.239
[38,     1] loss: 920.896
[39,     1] loss: 916.770
[40,     1] loss: 889.621
[41,     1] loss: 922.112
[42,     1] loss: 910.892
[43,     1] loss: 863.058
[44,     1] loss: 885.081
[45,     1] loss: 1112.996
[46,     1] loss: 905.025
[47,     1] loss: 900.528
[48,     1] loss: 872.158
[49,     1] loss: 896.736
[50,     1] loss: 939.928
[51,     1] loss: 870.720
[52,     1] loss: 887.982
[53,     1] loss: 883.646
[54,     1] loss: 921.794
[55,     1] loss: 866.548
[56,     1] loss: 874.333
[57,     1] loss: 851.823
[58,     1] loss: 872.135
[59,     1] loss: 847.476
[60,     1] loss: 820.402
[61,     1] loss: 805.759
[62,     1] loss: 806.598
[63,     1] loss: 730.096
[64,     1] loss: 763.197
[65,     1] loss: 812.990
[66,     1] loss: 719.724
[67,     1] loss: 733.667
[68,     1] loss: 721.944
[69,     1] loss: 686.212
[70,     1] loss: 1016.153
[71,     1] loss: 1266.111
[72,     1] loss: 759.008
[73,     1] loss: 963.983
[74,     1] loss: 940.400
[75,     1] loss: 876.308
[76,     1] loss: 940.099
[77,     1] loss: 977.577
[78,     1] loss: 941.224
[79,     1] loss: 875.864
[80,     1] loss: 916.883
Early stopping applied (best metric=0.3994807302951813)
Finished Training
Total time taken: 11.968015670776367
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.583
[2,     1] loss: 1329.343
[3,     1] loss: 1324.565
[4,     1] loss: 1324.346
[5,     1] loss: 1324.006
[6,     1] loss: 1314.342
[7,     1] loss: 1311.535
[8,     1] loss: 1290.598
[9,     1] loss: 1270.381
[10,     1] loss: 1244.195
[11,     1] loss: 1213.239
[12,     1] loss: 1178.633
[13,     1] loss: 1159.392
[14,     1] loss: 1153.049
[15,     1] loss: 1125.914
[16,     1] loss: 1106.339
[17,     1] loss: 1071.563
[18,     1] loss: 1044.642
[19,     1] loss: 1107.085
[20,     1] loss: 1075.791
[21,     1] loss: 1037.069
[22,     1] loss: 1014.815
[23,     1] loss: 1003.159
[24,     1] loss: 982.704
[25,     1] loss: 980.784
[26,     1] loss: 1000.578
[27,     1] loss: 968.331
[28,     1] loss: 935.421
[29,     1] loss: 934.358
[30,     1] loss: 910.436
[31,     1] loss: 922.520
[32,     1] loss: 889.900
[33,     1] loss: 864.721
[34,     1] loss: 935.303
[35,     1] loss: 996.611
[36,     1] loss: 960.878
[37,     1] loss: 837.269
[38,     1] loss: 876.568
[39,     1] loss: 871.256
[40,     1] loss: 872.407
[41,     1] loss: 853.408
[42,     1] loss: 889.009
[43,     1] loss: 910.135
[44,     1] loss: 781.065
[45,     1] loss: 801.837
[46,     1] loss: 782.834
[47,     1] loss: 804.569
[48,     1] loss: 894.392
[49,     1] loss: 1012.207
[50,     1] loss: 1012.380
[51,     1] loss: 819.685
[52,     1] loss: 899.093
[53,     1] loss: 883.005
[54,     1] loss: 808.591
[55,     1] loss: 870.026
[56,     1] loss: 849.869
[57,     1] loss: 816.797
[58,     1] loss: 816.640
[59,     1] loss: 756.743
[60,     1] loss: 796.733
[61,     1] loss: 829.778
[62,     1] loss: 719.762
[63,     1] loss: 724.832
[64,     1] loss: 723.229
[65,     1] loss: 743.475
[66,     1] loss: 705.969
[67,     1] loss: 752.481
[68,     1] loss: 723.761
[69,     1] loss: 949.839
[70,     1] loss: 760.401
[71,     1] loss: 730.011
[72,     1] loss: 744.192
[73,     1] loss: 690.947
[74,     1] loss: 713.775
[75,     1] loss: 650.625
[76,     1] loss: 809.564
[77,     1] loss: 654.193
[78,     1] loss: 692.757
[79,     1] loss: 887.843
[80,     1] loss: 633.129
[81,     1] loss: 711.278
[82,     1] loss: 790.859
[83,     1] loss: 602.859
[84,     1] loss: 730.207
[85,     1] loss: 610.229
[86,     1] loss: 691.770
[87,     1] loss: 647.299
[88,     1] loss: 596.259
[89,     1] loss: 657.635
[90,     1] loss: 573.656
[91,     1] loss: 543.670
[92,     1] loss: 571.423
Early stopping applied (best metric=0.3893812894821167)
Finished Training
Total time taken: 13.128609418869019
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.509
[2,     1] loss: 1324.304
[3,     1] loss: 1327.446
[4,     1] loss: 1328.052
[5,     1] loss: 1320.115
[6,     1] loss: 1323.962
[7,     1] loss: 1313.866
[8,     1] loss: 1305.638
[9,     1] loss: 1281.731
[10,     1] loss: 1263.338
[11,     1] loss: 1217.182
[12,     1] loss: 1190.495
[13,     1] loss: 1143.713
[14,     1] loss: 1136.010
[15,     1] loss: 1112.050
[16,     1] loss: 1070.522
[17,     1] loss: 1050.891
[18,     1] loss: 1097.714
[19,     1] loss: 1065.188
[20,     1] loss: 1031.005
[21,     1] loss: 1058.230
[22,     1] loss: 1058.423
[23,     1] loss: 1023.676
[24,     1] loss: 1024.356
[25,     1] loss: 976.515
[26,     1] loss: 988.498
[27,     1] loss: 949.018
[28,     1] loss: 931.608
[29,     1] loss: 970.734
[30,     1] loss: 985.985
[31,     1] loss: 957.527
[32,     1] loss: 919.109
[33,     1] loss: 885.488
[34,     1] loss: 972.072
[35,     1] loss: 926.778
[36,     1] loss: 919.319
[37,     1] loss: 984.563
[38,     1] loss: 899.719
[39,     1] loss: 911.030
[40,     1] loss: 965.781
[41,     1] loss: 887.003
[42,     1] loss: 902.727
[43,     1] loss: 851.763
[44,     1] loss: 830.318
[45,     1] loss: 817.570
[46,     1] loss: 813.861
[47,     1] loss: 819.206
[48,     1] loss: 823.478
[49,     1] loss: 817.729
[50,     1] loss: 780.008
[51,     1] loss: 908.767
[52,     1] loss: 1170.214
[53,     1] loss: 942.270
[54,     1] loss: 959.698
[55,     1] loss: 862.778
[56,     1] loss: 939.559
[57,     1] loss: 922.316
[58,     1] loss: 889.203
[59,     1] loss: 886.280
[60,     1] loss: 915.042
[61,     1] loss: 870.481
[62,     1] loss: 869.752
[63,     1] loss: 875.138
[64,     1] loss: 886.014
[65,     1] loss: 841.739
[66,     1] loss: 853.622
[67,     1] loss: 844.648
[68,     1] loss: 818.205
[69,     1] loss: 827.203
[70,     1] loss: 830.216
[71,     1] loss: 851.576
[72,     1] loss: 757.934
[73,     1] loss: 823.085
[74,     1] loss: 762.981
[75,     1] loss: 853.596
[76,     1] loss: 751.874
[77,     1] loss: 775.909
[78,     1] loss: 745.939
[79,     1] loss: 706.739
[80,     1] loss: 738.908
[81,     1] loss: 666.357
[82,     1] loss: 717.799
[83,     1] loss: 616.594
[84,     1] loss: 686.331
[85,     1] loss: 769.083
[86,     1] loss: 1011.188
[87,     1] loss: 686.900
[88,     1] loss: 847.509
[89,     1] loss: 780.571
[90,     1] loss: 798.724
[91,     1] loss: 771.711
[92,     1] loss: 732.558
[93,     1] loss: 700.561
[94,     1] loss: 662.392
Early stopping applied (best metric=0.4186129868030548)
Finished Training
Total time taken: 14.327648401260376
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1326.595
[2,     1] loss: 1323.283
[3,     1] loss: 1327.486
[4,     1] loss: 1324.964
[5,     1] loss: 1320.838
[6,     1] loss: 1320.772
[7,     1] loss: 1318.743
[8,     1] loss: 1312.677
[9,     1] loss: 1300.047
[10,     1] loss: 1282.503
[11,     1] loss: 1246.084
[12,     1] loss: 1217.619
[13,     1] loss: 1219.250
[14,     1] loss: 1157.912
[15,     1] loss: 1121.089
[16,     1] loss: 1113.348
[17,     1] loss: 1061.813
[18,     1] loss: 1056.013
[19,     1] loss: 1131.135
[20,     1] loss: 1038.857
[21,     1] loss: 1116.029
[22,     1] loss: 1079.488
[23,     1] loss: 1094.607
[24,     1] loss: 1031.942
[25,     1] loss: 1089.815
[26,     1] loss: 1016.794
[27,     1] loss: 1033.744
[28,     1] loss: 1040.451
[29,     1] loss: 1020.369
[30,     1] loss: 1038.448
[31,     1] loss: 994.904
[32,     1] loss: 1006.288
[33,     1] loss: 975.411
[34,     1] loss: 965.030
[35,     1] loss: 949.164
[36,     1] loss: 944.148
[37,     1] loss: 937.344
[38,     1] loss: 946.545
[39,     1] loss: 899.899
[40,     1] loss: 846.737
[41,     1] loss: 885.356
[42,     1] loss: 883.536
[43,     1] loss: 921.149
[44,     1] loss: 994.903
[45,     1] loss: 1001.353
[46,     1] loss: 880.876
[47,     1] loss: 950.976
[48,     1] loss: 884.299
[49,     1] loss: 934.076
[50,     1] loss: 869.738
[51,     1] loss: 865.277
[52,     1] loss: 826.572
[53,     1] loss: 821.286
[54,     1] loss: 838.289
[55,     1] loss: 792.341
[56,     1] loss: 743.517
[57,     1] loss: 803.924
[58,     1] loss: 833.636
[59,     1] loss: 783.458
[60,     1] loss: 810.223
[61,     1] loss: 785.410
[62,     1] loss: 787.696
[63,     1] loss: 755.482
[64,     1] loss: 725.211
[65,     1] loss: 733.772
[66,     1] loss: 764.564
[67,     1] loss: 902.972
[68,     1] loss: 844.771
[69,     1] loss: 722.184
[70,     1] loss: 849.161
[71,     1] loss: 773.251
[72,     1] loss: 724.260
[73,     1] loss: 857.534
[74,     1] loss: 714.701
[75,     1] loss: 757.921
[76,     1] loss: 667.725
[77,     1] loss: 676.572
[78,     1] loss: 724.007
[79,     1] loss: 708.554
[80,     1] loss: 642.822
[81,     1] loss: 615.292
[82,     1] loss: 643.872
[83,     1] loss: 697.799
[84,     1] loss: 1221.941
[85,     1] loss: 643.649
[86,     1] loss: 1005.056
[87,     1] loss: 752.826
[88,     1] loss: 873.023
[89,     1] loss: 897.669
[90,     1] loss: 808.503
[91,     1] loss: 874.126
[92,     1] loss: 884.656
[93,     1] loss: 770.047
[94,     1] loss: 858.858
[95,     1] loss: 788.385
[96,     1] loss: 739.931
[97,     1] loss: 775.314
[98,     1] loss: 702.738
[99,     1] loss: 747.023
[100,     1] loss: 701.393
[101,     1] loss: 780.216
[102,     1] loss: 704.184
[103,     1] loss: 706.623
[104,     1] loss: 652.779
[105,     1] loss: 621.509
[106,     1] loss: 672.115
[107,     1] loss: 594.692
[108,     1] loss: 669.518
[109,     1] loss: 622.043
[110,     1] loss: 592.665
[111,     1] loss: 542.226
[112,     1] loss: 551.784
[113,     1] loss: 527.178
[114,     1] loss: 712.026
[115,     1] loss: 1722.657
[116,     1] loss: 636.513
[117,     1] loss: 1137.670
[118,     1] loss: 839.039
[119,     1] loss: 983.550
[120,     1] loss: 986.265
[121,     1] loss: 936.812
[122,     1] loss: 865.896
[123,     1] loss: 975.207
[124,     1] loss: 895.414
[125,     1] loss: 851.146
[126,     1] loss: 963.154
[127,     1] loss: 929.487
[128,     1] loss: 861.590
[129,     1] loss: 889.432
[130,     1] loss: 916.546
[131,     1] loss: 820.698
[132,     1] loss: 877.445
[133,     1] loss: 869.497
[134,     1] loss: 834.679
[135,     1] loss: 889.549
[136,     1] loss: 758.495
[137,     1] loss: 872.256
[138,     1] loss: 755.751
[139,     1] loss: 804.784
[140,     1] loss: 754.170
[141,     1] loss: 718.861
[142,     1] loss: 699.833
[143,     1] loss: 647.106
[144,     1] loss: 720.676
[145,     1] loss: 703.566
[146,     1] loss: 848.481
[147,     1] loss: 641.751
[148,     1] loss: 691.228
[149,     1] loss: 814.121
[150,     1] loss: 666.751
[151,     1] loss: 871.912
Early stopping applied (best metric=0.3264385163784027)
Finished Training
Total time taken: 22.89525580406189
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1325.792
[2,     1] loss: 1330.722
[3,     1] loss: 1325.895
[4,     1] loss: 1326.117
[5,     1] loss: 1326.292
[6,     1] loss: 1322.123
[7,     1] loss: 1316.365
[8,     1] loss: 1310.257
[9,     1] loss: 1304.642
[10,     1] loss: 1277.317
[11,     1] loss: 1253.224
[12,     1] loss: 1216.255
[13,     1] loss: 1186.215
[14,     1] loss: 1172.568
[15,     1] loss: 1148.074
[16,     1] loss: 1097.559
[17,     1] loss: 1117.798
[18,     1] loss: 1117.135
[19,     1] loss: 1068.230
[20,     1] loss: 1089.689
[21,     1] loss: 1111.200
[22,     1] loss: 1041.109
[23,     1] loss: 1059.428
[24,     1] loss: 1052.681
[25,     1] loss: 1037.521
[26,     1] loss: 1022.498
[27,     1] loss: 1009.730
[28,     1] loss: 1008.566
[29,     1] loss: 961.026
[30,     1] loss: 987.853
[31,     1] loss: 1052.131
[32,     1] loss: 965.366
[33,     1] loss: 953.685
[34,     1] loss: 972.349
[35,     1] loss: 950.976
[36,     1] loss: 935.416
[37,     1] loss: 951.541
[38,     1] loss: 878.188
[39,     1] loss: 930.112
[40,     1] loss: 897.836
[41,     1] loss: 843.306
[42,     1] loss: 831.347
[43,     1] loss: 868.503
[44,     1] loss: 796.050
[45,     1] loss: 894.733
[46,     1] loss: 931.324
[47,     1] loss: 1245.576
[48,     1] loss: 935.913
[49,     1] loss: 1017.366
[50,     1] loss: 954.403
[51,     1] loss: 923.938
[52,     1] loss: 965.763
[53,     1] loss: 955.203
[54,     1] loss: 884.222
[55,     1] loss: 968.460
[56,     1] loss: 891.775
[57,     1] loss: 948.812
[58,     1] loss: 843.889
[59,     1] loss: 952.367
[60,     1] loss: 868.746
[61,     1] loss: 904.863
[62,     1] loss: 931.299
[63,     1] loss: 861.922
[64,     1] loss: 862.932
[65,     1] loss: 861.750
[66,     1] loss: 846.749
[67,     1] loss: 829.604
[68,     1] loss: 862.365
[69,     1] loss: 798.783
[70,     1] loss: 799.413
[71,     1] loss: 754.395
[72,     1] loss: 785.578
[73,     1] loss: 805.332
[74,     1] loss: 733.948
[75,     1] loss: 777.163
[76,     1] loss: 871.521
[77,     1] loss: 706.435
[78,     1] loss: 846.945
[79,     1] loss: 809.615
[80,     1] loss: 783.392
[81,     1] loss: 801.569
[82,     1] loss: 789.098
[83,     1] loss: 762.378
[84,     1] loss: 817.235
[85,     1] loss: 729.448
[86,     1] loss: 791.951
[87,     1] loss: 723.086
[88,     1] loss: 778.326
[89,     1] loss: 668.555
[90,     1] loss: 671.445
[91,     1] loss: 860.897
[92,     1] loss: 721.864
[93,     1] loss: 631.787
[94,     1] loss: 696.464
[95,     1] loss: 731.026
[96,     1] loss: 576.912
[97,     1] loss: 599.758
[98,     1] loss: 669.003
[99,     1] loss: 777.499
[100,     1] loss: 712.041
[101,     1] loss: 535.331
[102,     1] loss: 667.373
[103,     1] loss: 600.255
[104,     1] loss: 556.898
[105,     1] loss: 545.375
[106,     1] loss: 550.234
[107,     1] loss: 528.699
[108,     1] loss: 511.411
[109,     1] loss: 724.174
[110,     1] loss: 1510.016
[111,     1] loss: 680.396
[112,     1] loss: 771.393
[113,     1] loss: 754.186
[114,     1] loss: 879.142
[115,     1] loss: 816.304
[116,     1] loss: 814.037
[117,     1] loss: 804.048
[118,     1] loss: 722.808
[119,     1] loss: 773.868
[120,     1] loss: 711.024
[121,     1] loss: 776.483
[122,     1] loss: 747.979
[123,     1] loss: 745.879
[124,     1] loss: 670.808
[125,     1] loss: 632.832
[126,     1] loss: 673.819
[127,     1] loss: 630.789
[128,     1] loss: 648.950
[129,     1] loss: 671.147
[130,     1] loss: 545.601
[131,     1] loss: 661.051
[132,     1] loss: 687.890
[133,     1] loss: 564.422
[134,     1] loss: 617.810
[135,     1] loss: 705.097
[136,     1] loss: 517.354
[137,     1] loss: 689.635
Early stopping applied (best metric=0.3736106753349304)
Finished Training
Total time taken: 20.729888916015625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1325.948
[2,     1] loss: 1326.938
[3,     1] loss: 1326.668
[4,     1] loss: 1326.037
[5,     1] loss: 1325.368
[6,     1] loss: 1323.992
[7,     1] loss: 1322.714
[8,     1] loss: 1309.610
[9,     1] loss: 1297.720
[10,     1] loss: 1288.452
[11,     1] loss: 1249.428
[12,     1] loss: 1224.545
[13,     1] loss: 1186.428
[14,     1] loss: 1159.750
[15,     1] loss: 1135.383
[16,     1] loss: 1122.250
[17,     1] loss: 1141.877
[18,     1] loss: 1099.317
[19,     1] loss: 1084.977
[20,     1] loss: 1101.520
[21,     1] loss: 1060.851
[22,     1] loss: 1027.394
[23,     1] loss: 1055.394
[24,     1] loss: 1068.112
[25,     1] loss: 1028.865
[26,     1] loss: 1067.406
[27,     1] loss: 1044.991
[28,     1] loss: 1019.106
[29,     1] loss: 1025.148
[30,     1] loss: 1021.885
[31,     1] loss: 1009.072
[32,     1] loss: 1063.760
[33,     1] loss: 953.467
[34,     1] loss: 1017.652
[35,     1] loss: 935.983
[36,     1] loss: 977.310
[37,     1] loss: 941.969
[38,     1] loss: 996.706
[39,     1] loss: 970.801
[40,     1] loss: 906.081
[41,     1] loss: 959.268
[42,     1] loss: 976.728
[43,     1] loss: 923.356
[44,     1] loss: 990.704
[45,     1] loss: 964.272
[46,     1] loss: 898.499
[47,     1] loss: 971.979
[48,     1] loss: 937.690
[49,     1] loss: 940.349
[50,     1] loss: 897.905
[51,     1] loss: 917.557
[52,     1] loss: 872.675
[53,     1] loss: 931.187
[54,     1] loss: 905.404
[55,     1] loss: 888.126
[56,     1] loss: 923.574
[57,     1] loss: 881.977
[58,     1] loss: 913.653
[59,     1] loss: 895.961
[60,     1] loss: 842.917
[61,     1] loss: 794.979
[62,     1] loss: 844.630
[63,     1] loss: 753.281
[64,     1] loss: 775.468
[65,     1] loss: 813.823
[66,     1] loss: 825.177
[67,     1] loss: 965.680
[68,     1] loss: 799.056
[69,     1] loss: 777.738
[70,     1] loss: 830.645
[71,     1] loss: 820.608
[72,     1] loss: 775.880
[73,     1] loss: 801.105
[74,     1] loss: 758.282
[75,     1] loss: 767.275
[76,     1] loss: 751.003
[77,     1] loss: 687.410
[78,     1] loss: 669.182
[79,     1] loss: 650.175
[80,     1] loss: 656.501
[81,     1] loss: 857.856
[82,     1] loss: 1344.663
[83,     1] loss: 729.493
[84,     1] loss: 1142.426
[85,     1] loss: 846.347
[86,     1] loss: 937.466
[87,     1] loss: 1018.119
[88,     1] loss: 989.879
[89,     1] loss: 859.519
[90,     1] loss: 856.390
[91,     1] loss: 994.457
[92,     1] loss: 857.807
[93,     1] loss: 859.147
[94,     1] loss: 927.960
[95,     1] loss: 837.231
[96,     1] loss: 826.469
[97,     1] loss: 841.014
[98,     1] loss: 810.618
[99,     1] loss: 788.630
[100,     1] loss: 727.284
[101,     1] loss: 736.066
[102,     1] loss: 758.141
[103,     1] loss: 696.496
[104,     1] loss: 716.588
[105,     1] loss: 631.916
[106,     1] loss: 600.392
[107,     1] loss: 616.332
[108,     1] loss: 600.664
[109,     1] loss: 636.722
[110,     1] loss: 795.950
[111,     1] loss: 751.947
[112,     1] loss: 903.490
[113,     1] loss: 620.536
[114,     1] loss: 876.610
[115,     1] loss: 634.326
[116,     1] loss: 778.310
[117,     1] loss: 588.922
[118,     1] loss: 815.083
[119,     1] loss: 945.335
[120,     1] loss: 748.609
[121,     1] loss: 882.483
[122,     1] loss: 770.271
[123,     1] loss: 760.833
[124,     1] loss: 790.063
Early stopping applied (best metric=0.28563326597213745)
Finished Training
Total time taken: 18.51136565208435
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1330.007
[2,     1] loss: 1322.429
[3,     1] loss: 1331.408
[4,     1] loss: 1328.362
[5,     1] loss: 1325.318
[6,     1] loss: 1324.132
[7,     1] loss: 1320.868
[8,     1] loss: 1320.273
[9,     1] loss: 1316.596
[10,     1] loss: 1310.920
[11,     1] loss: 1302.849
[12,     1] loss: 1284.333
[13,     1] loss: 1267.413
[14,     1] loss: 1238.479
[15,     1] loss: 1224.566
[16,     1] loss: 1207.504
[17,     1] loss: 1195.908
[18,     1] loss: 1187.286
[19,     1] loss: 1122.964
[20,     1] loss: 1104.662
[21,     1] loss: 1113.881
[22,     1] loss: 1111.438
[23,     1] loss: 1056.288
[24,     1] loss: 1096.207
[25,     1] loss: 1043.263
[26,     1] loss: 1065.263
[27,     1] loss: 1042.708
[28,     1] loss: 1079.619
[29,     1] loss: 1074.453
[30,     1] loss: 1061.631
[31,     1] loss: 996.942
[32,     1] loss: 1054.835
[33,     1] loss: 1003.345
[34,     1] loss: 1003.240
[35,     1] loss: 1004.199
[36,     1] loss: 1036.489
[37,     1] loss: 931.262
[38,     1] loss: 1033.079
[39,     1] loss: 953.553
[40,     1] loss: 962.401
[41,     1] loss: 1038.773
[42,     1] loss: 943.400
[43,     1] loss: 984.256
[44,     1] loss: 935.567
[45,     1] loss: 982.027
[46,     1] loss: 940.271
[47,     1] loss: 913.845
[48,     1] loss: 957.603
[49,     1] loss: 881.377
[50,     1] loss: 826.060
[51,     1] loss: 889.805
[52,     1] loss: 846.710
[53,     1] loss: 866.546
[54,     1] loss: 824.278
[55,     1] loss: 859.152
[56,     1] loss: 942.686
[57,     1] loss: 1024.093
[58,     1] loss: 824.098
[59,     1] loss: 967.099
[60,     1] loss: 817.472
[61,     1] loss: 883.080
[62,     1] loss: 835.432
[63,     1] loss: 856.569
[64,     1] loss: 829.707
[65,     1] loss: 831.626
[66,     1] loss: 840.207
[67,     1] loss: 858.626
[68,     1] loss: 761.322
[69,     1] loss: 751.835
[70,     1] loss: 756.435
[71,     1] loss: 779.064
[72,     1] loss: 746.469
[73,     1] loss: 725.364
[74,     1] loss: 686.537
[75,     1] loss: 721.461
[76,     1] loss: 751.294
[77,     1] loss: 1055.196
[78,     1] loss: 985.454
[79,     1] loss: 739.571
[80,     1] loss: 871.043
[81,     1] loss: 777.944
[82,     1] loss: 876.336
[83,     1] loss: 853.606
[84,     1] loss: 780.477
[85,     1] loss: 851.802
[86,     1] loss: 762.053
[87,     1] loss: 846.795
[88,     1] loss: 705.730
[89,     1] loss: 727.164
[90,     1] loss: 652.452
[91,     1] loss: 687.411
[92,     1] loss: 755.029
[93,     1] loss: 706.613
[94,     1] loss: 673.130
[95,     1] loss: 793.799
[96,     1] loss: 636.653
[97,     1] loss: 797.250
[98,     1] loss: 864.226
[99,     1] loss: 718.008
[100,     1] loss: 850.736
[101,     1] loss: 613.266
[102,     1] loss: 708.863
[103,     1] loss: 668.534
[104,     1] loss: 783.773
[105,     1] loss: 620.830
[106,     1] loss: 680.528
[107,     1] loss: 698.693
[108,     1] loss: 648.599
[109,     1] loss: 585.358
[110,     1] loss: 592.416
[111,     1] loss: 578.311
[112,     1] loss: 566.984
[113,     1] loss: 522.886
[114,     1] loss: 549.276
[115,     1] loss: 960.445
[116,     1] loss: 2077.047
[117,     1] loss: 1437.787
[118,     1] loss: 1124.370
[119,     1] loss: 1225.066
[120,     1] loss: 1228.215
[121,     1] loss: 1206.784
[122,     1] loss: 1265.097
[123,     1] loss: 1214.071
[124,     1] loss: 1218.225
[125,     1] loss: 1221.574
[126,     1] loss: 1209.483
[127,     1] loss: 1193.790
[128,     1] loss: 1207.142
[129,     1] loss: 1219.712
[130,     1] loss: 1188.768
[131,     1] loss: 1153.943
[132,     1] loss: 1177.870
[133,     1] loss: 1199.627
[134,     1] loss: 1174.053
[135,     1] loss: 1162.927
[136,     1] loss: 1186.792
[137,     1] loss: 1177.375
[138,     1] loss: 1163.882
[139,     1] loss: 1158.888
[140,     1] loss: 1165.851
[141,     1] loss: 1152.246
[142,     1] loss: 1137.707
[143,     1] loss: 1147.102
[144,     1] loss: 1143.661
[145,     1] loss: 1125.438
[146,     1] loss: 1165.963
[147,     1] loss: 1130.321
[148,     1] loss: 1151.130
[149,     1] loss: 1154.806
[150,     1] loss: 1139.217
[151,     1] loss: 1144.290
Early stopping applied (best metric=0.35017070174217224)
Finished Training
Total time taken: 24.407514333724976
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.421
[2,     1] loss: 1323.979
[3,     1] loss: 1326.032
[4,     1] loss: 1326.698
[5,     1] loss: 1322.274
[6,     1] loss: 1325.575
[7,     1] loss: 1318.414
[8,     1] loss: 1319.281
[9,     1] loss: 1312.535
[10,     1] loss: 1300.954
[11,     1] loss: 1276.462
[12,     1] loss: 1256.765
[13,     1] loss: 1232.638
[14,     1] loss: 1194.754
[15,     1] loss: 1156.393
[16,     1] loss: 1143.066
[17,     1] loss: 1130.817
[18,     1] loss: 1143.847
[19,     1] loss: 1096.761
[20,     1] loss: 1093.840
[21,     1] loss: 1069.559
[22,     1] loss: 1042.685
[23,     1] loss: 1032.054
[24,     1] loss: 1038.886
[25,     1] loss: 1035.101
[26,     1] loss: 1029.488
[27,     1] loss: 1012.805
[28,     1] loss: 964.945
[29,     1] loss: 953.047
[30,     1] loss: 977.991
[31,     1] loss: 1011.986
[32,     1] loss: 1022.076
[33,     1] loss: 905.494
[34,     1] loss: 922.375
[35,     1] loss: 912.722
[36,     1] loss: 913.860
[37,     1] loss: 911.857
[38,     1] loss: 909.503
[39,     1] loss: 884.085
[40,     1] loss: 840.263
[41,     1] loss: 872.601
[42,     1] loss: 816.214
[43,     1] loss: 865.543
[44,     1] loss: 789.532
[45,     1] loss: 776.858
[46,     1] loss: 1058.130
[47,     1] loss: 1524.845
[48,     1] loss: 957.635
[49,     1] loss: 1002.903
[50,     1] loss: 1173.440
[51,     1] loss: 1042.501
[52,     1] loss: 1027.495
[53,     1] loss: 1065.651
[54,     1] loss: 1081.606
[55,     1] loss: 1051.033
[56,     1] loss: 1077.302
[57,     1] loss: 1029.554
[58,     1] loss: 959.550
[59,     1] loss: 963.725
[60,     1] loss: 947.195
[61,     1] loss: 993.772
[62,     1] loss: 950.777
[63,     1] loss: 977.281
[64,     1] loss: 908.518
[65,     1] loss: 922.616
[66,     1] loss: 870.092
[67,     1] loss: 903.347
[68,     1] loss: 894.257
[69,     1] loss: 894.169
[70,     1] loss: 829.014
[71,     1] loss: 903.776
[72,     1] loss: 862.006
[73,     1] loss: 856.874
[74,     1] loss: 862.204
[75,     1] loss: 790.411
[76,     1] loss: 790.170
[77,     1] loss: 774.952
[78,     1] loss: 798.593
[79,     1] loss: 764.501
[80,     1] loss: 763.473
Early stopping applied (best metric=0.35846978425979614)
Finished Training
Total time taken: 11.913617372512817
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1328.767
[2,     1] loss: 1325.322
[3,     1] loss: 1320.533
[4,     1] loss: 1324.489
[5,     1] loss: 1321.288
[6,     1] loss: 1317.436
[7,     1] loss: 1320.928
[8,     1] loss: 1308.176
[9,     1] loss: 1289.194
[10,     1] loss: 1254.681
[11,     1] loss: 1221.301
[12,     1] loss: 1183.314
[13,     1] loss: 1160.065
[14,     1] loss: 1128.747
[15,     1] loss: 1090.745
[16,     1] loss: 1117.808
[17,     1] loss: 1076.547
[18,     1] loss: 1067.511
[19,     1] loss: 1024.524
[20,     1] loss: 1039.648
[21,     1] loss: 1048.611
[22,     1] loss: 1022.351
[23,     1] loss: 1033.157
[24,     1] loss: 1044.381
[25,     1] loss: 1000.487
[26,     1] loss: 1018.104
[27,     1] loss: 983.492
[28,     1] loss: 1026.386
[29,     1] loss: 1012.261
[30,     1] loss: 960.277
[31,     1] loss: 983.483
[32,     1] loss: 923.121
[33,     1] loss: 925.224
[34,     1] loss: 916.545
[35,     1] loss: 890.792
[36,     1] loss: 890.498
[37,     1] loss: 869.335
[38,     1] loss: 938.362
[39,     1] loss: 849.387
[40,     1] loss: 850.278
[41,     1] loss: 853.209
[42,     1] loss: 814.236
[43,     1] loss: 808.896
[44,     1] loss: 802.865
[45,     1] loss: 894.101
[46,     1] loss: 1263.123
[47,     1] loss: 819.410
[48,     1] loss: 958.407
[49,     1] loss: 853.753
[50,     1] loss: 943.229
[51,     1] loss: 951.492
[52,     1] loss: 885.491
[53,     1] loss: 857.761
[54,     1] loss: 915.372
[55,     1] loss: 872.695
[56,     1] loss: 855.030
[57,     1] loss: 880.264
[58,     1] loss: 813.991
[59,     1] loss: 837.474
[60,     1] loss: 812.956
[61,     1] loss: 821.740
[62,     1] loss: 809.628
[63,     1] loss: 773.598
[64,     1] loss: 833.992
[65,     1] loss: 786.273
[66,     1] loss: 831.473
[67,     1] loss: 749.954
[68,     1] loss: 795.966
[69,     1] loss: 749.120
[70,     1] loss: 751.113
[71,     1] loss: 783.721
[72,     1] loss: 702.321
[73,     1] loss: 656.071
[74,     1] loss: 765.424
[75,     1] loss: 694.854
[76,     1] loss: 698.030
[77,     1] loss: 723.316
[78,     1] loss: 667.682
[79,     1] loss: 706.439
[80,     1] loss: 711.849
[81,     1] loss: 608.468
[82,     1] loss: 661.560
[83,     1] loss: 700.632
[84,     1] loss: 648.787
[85,     1] loss: 643.539
[86,     1] loss: 584.224
[87,     1] loss: 623.742
[88,     1] loss: 678.719
[89,     1] loss: 911.427
[90,     1] loss: 831.271
[91,     1] loss: 587.531
[92,     1] loss: 805.366
[93,     1] loss: 613.062
[94,     1] loss: 765.633
[95,     1] loss: 638.461
[96,     1] loss: 844.363
[97,     1] loss: 582.422
[98,     1] loss: 663.704
[99,     1] loss: 653.436
[100,     1] loss: 708.030
[101,     1] loss: 766.364
[102,     1] loss: 640.106
[103,     1] loss: 606.590
[104,     1] loss: 592.633
[105,     1] loss: 610.498
[106,     1] loss: 578.558
[107,     1] loss: 563.885
[108,     1] loss: 509.487
[109,     1] loss: 588.919
[110,     1] loss: 569.410
[111,     1] loss: 809.609
[112,     1] loss: 1520.353
[113,     1] loss: 670.241
[114,     1] loss: 989.501
[115,     1] loss: 945.688
[116,     1] loss: 872.750
[117,     1] loss: 921.192
[118,     1] loss: 931.226
[119,     1] loss: 805.917
[120,     1] loss: 896.844
[121,     1] loss: 945.101
[122,     1] loss: 830.927
[123,     1] loss: 892.354
[124,     1] loss: 811.807
Early stopping applied (best metric=0.34931400418281555)
Finished Training
Total time taken: 18.909160614013672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1331.959
[2,     1] loss: 1334.576
[3,     1] loss: 1323.849
[4,     1] loss: 1330.607
[5,     1] loss: 1324.739
[6,     1] loss: 1322.795
[7,     1] loss: 1323.356
[8,     1] loss: 1321.932
[9,     1] loss: 1321.937
[10,     1] loss: 1319.661
[11,     1] loss: 1318.751
[12,     1] loss: 1311.585
[13,     1] loss: 1301.800
[14,     1] loss: 1288.331
[15,     1] loss: 1270.690
[16,     1] loss: 1244.203
[17,     1] loss: 1225.438
[18,     1] loss: 1164.228
[19,     1] loss: 1156.203
[20,     1] loss: 1132.474
[21,     1] loss: 1151.152
[22,     1] loss: 1101.417
[23,     1] loss: 1113.042
[24,     1] loss: 1093.655
[25,     1] loss: 1113.885
[26,     1] loss: 1094.125
[27,     1] loss: 1018.665
[28,     1] loss: 1092.774
[29,     1] loss: 1051.985
[30,     1] loss: 1057.652
[31,     1] loss: 1047.094
[32,     1] loss: 1060.584
[33,     1] loss: 1009.366
[34,     1] loss: 1016.321
[35,     1] loss: 1050.042
[36,     1] loss: 988.766
[37,     1] loss: 960.917
[38,     1] loss: 987.936
[39,     1] loss: 910.383
[40,     1] loss: 951.829
[41,     1] loss: 970.201
[42,     1] loss: 963.600
[43,     1] loss: 934.356
[44,     1] loss: 975.410
[45,     1] loss: 926.426
[46,     1] loss: 891.473
[47,     1] loss: 883.870
[48,     1] loss: 923.819
[49,     1] loss: 949.076
[50,     1] loss: 935.877
[51,     1] loss: 888.535
[52,     1] loss: 872.555
[53,     1] loss: 889.529
[54,     1] loss: 810.548
[55,     1] loss: 878.124
[56,     1] loss: 855.799
[57,     1] loss: 842.451
[58,     1] loss: 992.553
[59,     1] loss: 815.973
[60,     1] loss: 874.877
[61,     1] loss: 803.867
[62,     1] loss: 839.672
[63,     1] loss: 801.563
[64,     1] loss: 900.439
[65,     1] loss: 783.925
[66,     1] loss: 843.487
[67,     1] loss: 744.105
[68,     1] loss: 824.761
[69,     1] loss: 817.078
[70,     1] loss: 724.646
[71,     1] loss: 872.756
[72,     1] loss: 675.755
[73,     1] loss: 814.884
[74,     1] loss: 727.386
[75,     1] loss: 770.681
[76,     1] loss: 800.124
[77,     1] loss: 702.356
[78,     1] loss: 705.506
[79,     1] loss: 662.088
[80,     1] loss: 665.244
[81,     1] loss: 645.302
[82,     1] loss: 636.477
[83,     1] loss: 904.845
[84,     1] loss: 1014.075
[85,     1] loss: 752.063
[86,     1] loss: 848.153
[87,     1] loss: 764.155
[88,     1] loss: 888.865
[89,     1] loss: 757.624
[90,     1] loss: 784.969
[91,     1] loss: 800.994
[92,     1] loss: 712.535
[93,     1] loss: 793.754
[94,     1] loss: 710.996
[95,     1] loss: 803.598
[96,     1] loss: 692.946
[97,     1] loss: 806.959
[98,     1] loss: 787.920
[99,     1] loss: 695.726
[100,     1] loss: 705.740
[101,     1] loss: 675.143
[102,     1] loss: 716.228
[103,     1] loss: 646.846
[104,     1] loss: 692.981
[105,     1] loss: 654.338
[106,     1] loss: 618.384
[107,     1] loss: 597.674
[108,     1] loss: 725.065
[109,     1] loss: 665.515
[110,     1] loss: 660.237
[111,     1] loss: 544.144
[112,     1] loss: 604.980
Early stopping applied (best metric=0.3498731553554535)
Finished Training
Total time taken: 15.973593711853027
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1328.922
[2,     1] loss: 1329.019
[3,     1] loss: 1327.982
[4,     1] loss: 1326.174
[5,     1] loss: 1319.823
[6,     1] loss: 1310.821
[7,     1] loss: 1302.713
[8,     1] loss: 1275.769
[9,     1] loss: 1253.593
[10,     1] loss: 1222.921
[11,     1] loss: 1189.696
[12,     1] loss: 1162.507
[13,     1] loss: 1107.196
[14,     1] loss: 1129.229
[15,     1] loss: 1106.891
[16,     1] loss: 1094.792
[17,     1] loss: 1116.606
[18,     1] loss: 1094.933
[19,     1] loss: 1074.510
[20,     1] loss: 1062.826
[21,     1] loss: 1062.879
[22,     1] loss: 1043.598
[23,     1] loss: 1013.976
[24,     1] loss: 1083.609
[25,     1] loss: 1011.493
[26,     1] loss: 1000.167
[27,     1] loss: 998.299
[28,     1] loss: 968.710
[29,     1] loss: 968.552
[30,     1] loss: 992.190
[31,     1] loss: 962.912
[32,     1] loss: 936.839
[33,     1] loss: 937.897
[34,     1] loss: 912.357
[35,     1] loss: 904.078
[36,     1] loss: 961.130
[37,     1] loss: 890.779
[38,     1] loss: 892.986
[39,     1] loss: 867.824
[40,     1] loss: 893.949
[41,     1] loss: 815.003
[42,     1] loss: 767.745
[43,     1] loss: 841.072
[44,     1] loss: 1106.861
[45,     1] loss: 912.433
[46,     1] loss: 903.071
[47,     1] loss: 853.494
[48,     1] loss: 865.916
[49,     1] loss: 855.531
[50,     1] loss: 868.844
[51,     1] loss: 828.808
[52,     1] loss: 842.945
[53,     1] loss: 856.238
[54,     1] loss: 834.300
[55,     1] loss: 793.024
[56,     1] loss: 822.078
[57,     1] loss: 881.676
[58,     1] loss: 786.119
[59,     1] loss: 749.587
[60,     1] loss: 843.562
[61,     1] loss: 813.980
[62,     1] loss: 755.028
[63,     1] loss: 730.718
[64,     1] loss: 831.815
[65,     1] loss: 777.163
[66,     1] loss: 759.437
[67,     1] loss: 704.237
[68,     1] loss: 886.477
[69,     1] loss: 825.935
[70,     1] loss: 780.832
[71,     1] loss: 893.217
[72,     1] loss: 750.821
[73,     1] loss: 786.899
[74,     1] loss: 738.829
[75,     1] loss: 805.149
[76,     1] loss: 721.582
[77,     1] loss: 749.334
[78,     1] loss: 699.416
[79,     1] loss: 708.095
[80,     1] loss: 661.542
[81,     1] loss: 710.683
[82,     1] loss: 702.704
[83,     1] loss: 621.350
[84,     1] loss: 624.987
[85,     1] loss: 642.747
[86,     1] loss: 709.183
[87,     1] loss: 684.923
[88,     1] loss: 694.834
[89,     1] loss: 566.552
[90,     1] loss: 598.395
[91,     1] loss: 630.729
[92,     1] loss: 703.408
[93,     1] loss: 919.616
[94,     1] loss: 878.620
[95,     1] loss: 659.591
[96,     1] loss: 788.019
[97,     1] loss: 708.663
[98,     1] loss: 857.094
[99,     1] loss: 675.490
[100,     1] loss: 769.277
[101,     1] loss: 668.160
[102,     1] loss: 802.047
[103,     1] loss: 643.284
[104,     1] loss: 668.603
[105,     1] loss: 616.694
[106,     1] loss: 688.553
[107,     1] loss: 573.956
[108,     1] loss: 575.186
[109,     1] loss: 737.100
[110,     1] loss: 737.082
[111,     1] loss: 558.231
[112,     1] loss: 838.583
[113,     1] loss: 710.635
[114,     1] loss: 687.050
[115,     1] loss: 645.307
[116,     1] loss: 620.023
Early stopping applied (best metric=0.3891669511795044)
Finished Training
Total time taken: 17.247678518295288
{'Hydroxylation-K Validation Accuracy': 0.7367316784869976, 'Hydroxylation-K Validation Sensitivity': 0.6548148148148148, 'Hydroxylation-K Validation Specificity': 0.7578947368421053, 'Hydroxylation-K Validation Precision': 0.41009556720083035, 'Hydroxylation-K AUC ROC': 0.7709746588693958, 'Hydroxylation-K AUC PR': 0.5861652035712365, 'Hydroxylation-K MCC': 0.355612957931749, 'Hydroxylation-K F1': 0.5008504581837915, 'Validation Loss (Hydroxylation-K)': 0.4789987802505493, 'Hydroxylation-P Validation Accuracy': 0.7909493765121905, 'Hydroxylation-P Validation Sensitivity': 0.8087830687830688, 'Hydroxylation-P Validation Specificity': 0.7871415033168737, 'Hydroxylation-P Validation Precision': 0.4562108405253143, 'Hydroxylation-P AUC ROC': 0.8558048455886254, 'Hydroxylation-P AUC PR': 0.6032372921325244, 'Hydroxylation-P MCC': 0.4920261607828643, 'Hydroxylation-P F1': 0.5801437844747193, 'Validation Loss (Hydroxylation-P)': 0.3609214186668396, 'Validation Loss (total)': 0.8399202028910319, 'TimeToTrain': 16.723291063308714}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001371158807330317,
 'learning_rate_Hydroxylation-K': 0.007807144217640626,
 'learning_rate_Hydroxylation-P': 0.009573697532159905,
 'log_base': 2.9028034110781493,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3521499859,
 'sample_weights': [1.980170941541353, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.901811798594798,
 'weight_decay_Hydroxylation-K': 7.994348013308558,
 'weight_decay_Hydroxylation-P': 6.233468448040903}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.568
[2,     1] loss: 1240.155
[3,     1] loss: 1239.934
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0067055870586162115,
 'learning_rate_Hydroxylation-K': 0.008613018299941922,
 'learning_rate_Hydroxylation-P': 0.0050587256759960205,
 'log_base': 2.018857216081334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3147622862,
 'sample_weights': [1.5665564749647352, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7587887068871005,
 'weight_decay_Hydroxylation-K': 7.722796211801937,
 'weight_decay_Hydroxylation-P': 8.001469789141508}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1410.411
[2,     1] loss: 1409.351
[3,     1] loss: 1413.218
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003681930547694958,
 'learning_rate_Hydroxylation-K': 0.009599923018522779,
 'learning_rate_Hydroxylation-P': 0.009695349970348006,
 'log_base': 2.0257240696425236,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4136887624,
 'sample_weights': [2.376324577728941, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.312021123219571,
 'weight_decay_Hydroxylation-K': 3.7139937332634485,
 'weight_decay_Hydroxylation-P': 3.195397141061767}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.668
[2,     1] loss: 1406.437
[3,     1] loss: 1407.290
[4,     1] loss: 1405.483
[5,     1] loss: 1405.435
[6,     1] loss: 1404.386
[7,     1] loss: 1404.718
[8,     1] loss: 1401.453
[9,     1] loss: 1403.480
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027251862957360286,
 'learning_rate_Hydroxylation-K': 0.008361262270403477,
 'learning_rate_Hydroxylation-P': 0.0036955624250451756,
 'log_base': 2.82826120316665,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1419017064,
 'sample_weights': [2.3648942028634647, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2325092925006285,
 'weight_decay_Hydroxylation-K': 6.267531027776744,
 'weight_decay_Hydroxylation-P': 6.493938414712086}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.079
[2,     1] loss: 1249.425
[3,     1] loss: 1242.725
[4,     1] loss: 1243.335
[5,     1] loss: 1246.788
[6,     1] loss: 1244.158
[7,     1] loss: 1245.999
[8,     1] loss: 1232.836
[9,     1] loss: 1227.424
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004766372539655439,
 'learning_rate_Hydroxylation-K': 0.009894228560435031,
 'learning_rate_Hydroxylation-P': 0.006501396554617414,
 'log_base': 2.9031507915647112,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3057278229,
 'sample_weights': [1.605755500957429, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.994193860847108,
 'weight_decay_Hydroxylation-K': 4.670102284354658,
 'weight_decay_Hydroxylation-P': 8.126991872282593}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.470
[2,     1] loss: 1238.854
[3,     1] loss: 1238.815
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003288782005904679,
 'learning_rate_Hydroxylation-K': 0.007209792400792779,
 'learning_rate_Hydroxylation-P': 0.009704324254789419,
 'log_base': 1.978294848849656,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 613113848,
 'sample_weights': [1.5663805880528014, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.584567303134167,
 'weight_decay_Hydroxylation-K': 9.347874625615642,
 'weight_decay_Hydroxylation-P': 9.597923084962558}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1421.068
[2,     1] loss: 1426.289
[3,     1] loss: 1422.701
[4,     1] loss: 1419.459
[5,     1] loss: 1425.146
[6,     1] loss: 1423.388
[7,     1] loss: 1419.383
[8,     1] loss: 1400.711
[9,     1] loss: 1382.515
[10,     1] loss: 1361.829
[11,     1] loss: 1321.425
[12,     1] loss: 1296.383
[13,     1] loss: 1276.140
[14,     1] loss: 1229.036
[15,     1] loss: 1213.785
[16,     1] loss: 1162.631
[17,     1] loss: 1174.484
[18,     1] loss: 1177.273
[19,     1] loss: 1161.881
[20,     1] loss: 1160.589
[21,     1] loss: 1158.468
[22,     1] loss: 1151.763
[23,     1] loss: 1113.326
[24,     1] loss: 1106.822
[25,     1] loss: 1103.211
[26,     1] loss: 1131.527
[27,     1] loss: 1082.392
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013081552077174206,
 'learning_rate_Hydroxylation-K': 0.008449126193608353,
 'learning_rate_Hydroxylation-P': 0.008032694896949534,
 'log_base': 2.7877051968130786,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2129915079,
 'sample_weights': [2.4470196443061956, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.387788663272993,
 'weight_decay_Hydroxylation-K': 9.596682654287033,
 'weight_decay_Hydroxylation-P': 9.001209337988275}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.090
[2,     1] loss: 1261.319
[3,     1] loss: 1252.621
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001467442241217266,
 'learning_rate_Hydroxylation-K': 0.007869697438982689,
 'learning_rate_Hydroxylation-P': 0.006325818522777348,
 'log_base': 2.347950817786624,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1398825494,
 'sample_weights': [1.6283775066026807, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.105938817153626,
 'weight_decay_Hydroxylation-K': 6.079992483577338,
 'weight_decay_Hydroxylation-P': 7.584623537429162}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1321.838
[2,     1] loss: 1322.401
[3,     1] loss: 1318.986
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008819645466064785,
 'learning_rate_Hydroxylation-K': 0.002174341743322874,
 'learning_rate_Hydroxylation-P': 0.005823294568987566,
 'log_base': 2.777838699496141,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1546973231,
 'sample_weights': [1.9558982204956938, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.034582745742699,
 'weight_decay_Hydroxylation-K': 8.045093362964197,
 'weight_decay_Hydroxylation-P': 8.722905393783197}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.701
[2,     1] loss: 1257.453
[3,     1] loss: 1253.522
[4,     1] loss: 1250.084
[5,     1] loss: 1252.556
[6,     1] loss: 1254.183
[7,     1] loss: 1251.109
[8,     1] loss: 1249.439
[9,     1] loss: 1246.938
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0052005631261951004,
 'learning_rate_Hydroxylation-K': 0.0084001945313673,
 'learning_rate_Hydroxylation-P': 0.006719477278635783,
 'log_base': 1.9744026432762896,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 193882044,
 'sample_weights': [1.6340285539039063, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.21569480450525,
 'weight_decay_Hydroxylation-K': 6.250880615049423,
 'weight_decay_Hydroxylation-P': 4.473318702853517}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1431.870
[2,     1] loss: 1425.447
[3,     1] loss: 1423.712
[4,     1] loss: 1425.598
[5,     1] loss: 1424.158
[6,     1] loss: 1414.634
[7,     1] loss: 1409.704
[8,     1] loss: 1394.113
[9,     1] loss: 1363.831
[10,     1] loss: 1325.284
[11,     1] loss: 1289.227
[12,     1] loss: 1247.902
[13,     1] loss: 1227.614
[14,     1] loss: 1192.222
[15,     1] loss: 1171.820
[16,     1] loss: 1138.479
[17,     1] loss: 1139.844
[18,     1] loss: 1119.638
[19,     1] loss: 1160.987
[20,     1] loss: 1171.037
[21,     1] loss: 1187.526
[22,     1] loss: 1113.839
[23,     1] loss: 1180.226
[24,     1] loss: 1117.536
[25,     1] loss: 1150.146
[26,     1] loss: 1099.587
[27,     1] loss: 1107.544
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004833960745958367,
 'learning_rate_Hydroxylation-K': 0.006660742417965064,
 'learning_rate_Hydroxylation-P': 0.006899483436060032,
 'log_base': 1.3739115454762851,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 550791963,
 'sample_weights': [2.4541038487585936, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.8437406864407695,
 'weight_decay_Hydroxylation-K': 7.972629776835197,
 'weight_decay_Hydroxylation-P': 2.321592757857589}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2039.284
[2,     1] loss: 2015.656
[3,     1] loss: 2012.877
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003625370523882286,
 'learning_rate_Hydroxylation-K': 0.007811668523203712,
 'learning_rate_Hydroxylation-P': 0.009491148736043373,
 'log_base': 2.9457567256298476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1920999665,
 'sample_weights': [5.255410225697932, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.521944261538033,
 'weight_decay_Hydroxylation-K': 9.901722014985644,
 'weight_decay_Hydroxylation-P': 9.99720534945723}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.974
[2,     1] loss: 1234.556
[3,     1] loss: 1237.707
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033050634930119702,
 'learning_rate_Hydroxylation-K': 0.009744622634737011,
 'learning_rate_Hydroxylation-P': 0.006110695396597883,
 'log_base': 2.9834564838139466,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3824725503,
 'sample_weights': [1.5452573981205073, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.441398768099729,
 'weight_decay_Hydroxylation-K': 8.420976147663831,
 'weight_decay_Hydroxylation-P': 6.297225862444549}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.771
[2,     1] loss: 1227.045
[3,     1] loss: 1232.218
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005300792515630197,
 'learning_rate_Hydroxylation-K': 0.0014347734610687959,
 'learning_rate_Hydroxylation-P': 0.0078333994917235,
 'log_base': 1.1293281107992077,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 278277104,
 'sample_weights': [1.5272800666284636, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.169293405853999,
 'weight_decay_Hydroxylation-K': 6.298081696023135,
 'weight_decay_Hydroxylation-P': 2.8403110488775356}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4455.173
[2,     1] loss: 4454.328
[3,     1] loss: 4450.602
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009908037281392952,
 'learning_rate_Hydroxylation-K': 0.000458806503820958,
 'learning_rate_Hydroxylation-P': 0.0020352223286712262,
 'log_base': 1.1836657717500019,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1333218527,
 'sample_weights': [13.72639236597972, 1.7158651452167197],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.122740157158472,
 'weight_decay_Hydroxylation-K': 9.111994256201365,
 'weight_decay_Hydroxylation-P': 2.428445503396468}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3214.776
[2,     1] loss: 3214.080
[3,     1] loss: 3208.217
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017836129208837225,
 'learning_rate_Hydroxylation-K': 0.009893843797674969,
 'learning_rate_Hydroxylation-P': 0.006914722051922765,
 'log_base': 1.8271355134807532,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1944785065,
 'sample_weights': [9.900846158082105, 1.23765344730428],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.941043653735723,
 'weight_decay_Hydroxylation-K': 6.709763290318029,
 'weight_decay_Hydroxylation-P': 5.607249862880823}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1500.366
[2,     1] loss: 1495.618
[3,     1] loss: 1493.874
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009817544201099747,
 'learning_rate_Hydroxylation-K': 0.004990174489384384,
 'learning_rate_Hydroxylation-P': 0.009642594628729005,
 'log_base': 1.5207394492464992,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2642679261,
 'sample_weights': [2.769713277781616, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.007120413703149,
 'weight_decay_Hydroxylation-K': 9.938101772163755,
 'weight_decay_Hydroxylation-P': 1.7466185269816374}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1759.498
[2,     1] loss: 1750.832
[3,     1] loss: 1747.273
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004849158277491688,
 'learning_rate_Hydroxylation-K': 0.009963968201705533,
 'learning_rate_Hydroxylation-P': 0.007614532948656965,
 'log_base': 1.492955604029429,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1459461938,
 'sample_weights': [3.9824816417415216, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.975305933392308,
 'weight_decay_Hydroxylation-K': 8.974418202293272,
 'weight_decay_Hydroxylation-P': 6.726406076016396}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1785.904
[2,     1] loss: 1793.165
[3,     1] loss: 1792.135
[4,     1] loss: 1790.102
[5,     1] loss: 1785.865
[6,     1] loss: 1781.095
[7,     1] loss: 1786.305
[8,     1] loss: 1783.380
[9,     1] loss: 1783.203
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004797637478693092,
 'learning_rate_Hydroxylation-K': 0.008402927742772587,
 'learning_rate_Hydroxylation-P': 0.008465507418111218,
 'log_base': 1.6098163011542188,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 863983304,
 'sample_weights': [4.165716106510719, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.362085286721772,
 'weight_decay_Hydroxylation-K': 7.0975192171552495,
 'weight_decay_Hydroxylation-P': 6.4132371357233735}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1653.332
[2,     1] loss: 1653.080
[3,     1] loss: 1645.387
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009467025917106455,
 'learning_rate_Hydroxylation-K': 0.007389475661230208,
 'learning_rate_Hydroxylation-P': 0.008328470575254779,
 'log_base': 2.463269439674455,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2112510442,
 'sample_weights': [3.506349005647086, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.228428849606111,
 'weight_decay_Hydroxylation-K': 8.525139672227235,
 'weight_decay_Hydroxylation-P': 8.762628158056623}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1300.765
[2,     1] loss: 1300.558
[3,     1] loss: 1297.368
[4,     1] loss: 1299.449
[5,     1] loss: 1298.687
[6,     1] loss: 1304.089
[7,     1] loss: 1300.182
[8,     1] loss: 1293.022
[9,     1] loss: 1294.479
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008634123924419635,
 'learning_rate_Hydroxylation-K': 0.009999657765607059,
 'learning_rate_Hydroxylation-P': 0.006055115092382672,
 'log_base': 1.3042856084947243,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3096791386,
 'sample_weights': [1.8518719657628457, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.644890986658167,
 'weight_decay_Hydroxylation-K': 9.10425541102141,
 'weight_decay_Hydroxylation-P': 1.842196693244651}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2236.573
[2,     1] loss: 2231.702
[3,     1] loss: 2243.214
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004266717318421359,
 'learning_rate_Hydroxylation-K': 0.009495131296421558,
 'learning_rate_Hydroxylation-P': 0.009105203811960661,
 'log_base': 1.634409492021366,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 570963258,
 'sample_weights': [6.2842416998078745, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8956547091615024,
 'weight_decay_Hydroxylation-K': 6.239107909625133,
 'weight_decay_Hydroxylation-P': 1.1651270524550885}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1631.134
[2,     1] loss: 1628.097
[3,     1] loss: 1624.904
[4,     1] loss: 1630.620
[5,     1] loss: 1622.664
[6,     1] loss: 1620.630
[7,     1] loss: 1626.235
[8,     1] loss: 1626.139
[9,     1] loss: 1625.039
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 5.4068337354766516e-05,
 'learning_rate_Hydroxylation-K': 0.00805157046526404,
 'learning_rate_Hydroxylation-P': 0.00626347942305043,
 'log_base': 2.9633086700062825,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3919247202,
 'sample_weights': [3.398139156953138, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7776435957483003,
 'weight_decay_Hydroxylation-K': 5.237642609141004,
 'weight_decay_Hydroxylation-P': 9.322214835699716}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.882
[2,     1] loss: 1233.990
[3,     1] loss: 1232.522
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006907338746113723,
 'learning_rate_Hydroxylation-K': 0.008500295108708218,
 'learning_rate_Hydroxylation-P': 0.007597827963961265,
 'log_base': 1.0226733600653413,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3976834988,
 'sample_weights': [1.53680682451981, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.565054212444442,
 'weight_decay_Hydroxylation-K': 6.296089555078794,
 'weight_decay_Hydroxylation-P': 6.509215475641619}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24087.404
Exploding loss, terminate run (best metric=0.5312942862510681)
Finished Training
Total time taken: 0.21399974822998047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24198.246
Exploding loss, terminate run (best metric=0.5283299088478088)
Finished Training
Total time taken: 0.20599889755249023
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24235.531
Exploding loss, terminate run (best metric=0.5322929620742798)
Finished Training
Total time taken: 0.22100090980529785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24213.566
Exploding loss, terminate run (best metric=0.5320109724998474)
Finished Training
Total time taken: 0.20699787139892578
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24263.172
Exploding loss, terminate run (best metric=0.533011794090271)
Finished Training
Total time taken: 0.21200251579284668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24411.035
Exploding loss, terminate run (best metric=0.5314264893531799)
Finished Training
Total time taken: 0.20451569557189941
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24092.672
Exploding loss, terminate run (best metric=0.5435853004455566)
Finished Training
Total time taken: 0.20600056648254395
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24226.191
Exploding loss, terminate run (best metric=0.5408927798271179)
Finished Training
Total time taken: 0.20299983024597168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24136.562
Exploding loss, terminate run (best metric=0.539903998374939)
Finished Training
Total time taken: 0.21204733848571777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24285.689
Exploding loss, terminate run (best metric=0.529032289981842)
Finished Training
Total time taken: 0.19900131225585938
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24249.109
Exploding loss, terminate run (best metric=0.5369237065315247)
Finished Training
Total time taken: 0.2109987735748291
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24362.887
Exploding loss, terminate run (best metric=0.5435861349105835)
Finished Training
Total time taken: 0.20199942588806152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24211.395
Exploding loss, terminate run (best metric=0.5272267460823059)
Finished Training
Total time taken: 0.2120037078857422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24343.889
Exploding loss, terminate run (best metric=0.5278871059417725)
Finished Training
Total time taken: 0.20300054550170898
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24187.414
Exploding loss, terminate run (best metric=0.5288064479827881)
Finished Training
Total time taken: 0.21400046348571777
{'Hydroxylation-K Validation Accuracy': 0.5194444444444444, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5740545808966862, 'Hydroxylation-K AUC PR': 0.28592303596306773, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.15623973727422005, 'Validation Loss (Hydroxylation-K)': 0.5576899607976278, 'Hydroxylation-P Validation Accuracy': 0.5219419657208602, 'Hydroxylation-P Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-P Validation Specificity': 0.5333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5919790343024669, 'Hydroxylation-P AUC PR': 0.28437477485307056, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.1405642505373356, 'Validation Loss (Hydroxylation-P)': 0.533747394879659, 'Validation Loss (total)': 1.0914373397827148, 'TimeToTrain': 0.2084378401438395}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008333272351752546,
 'learning_rate_Hydroxylation-K': 0.007152705339441662,
 'learning_rate_Hydroxylation-P': 0.00860865535342773,
 'log_base': 1.30648604497532,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 202063278,
 'sample_weights': [74.51699891797224, 9.295270719988329],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.80759524352377,
 'weight_decay_Hydroxylation-K': 5.258608524236777,
 'weight_decay_Hydroxylation-P': 7.018194851160112}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2221.364
[2,     1] loss: 2240.434
[3,     1] loss: 2228.820
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00043492428455391884,
 'learning_rate_Hydroxylation-K': 0.005771655453358524,
 'learning_rate_Hydroxylation-P': 0.0028938229799865783,
 'log_base': 2.138837103685489,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3210822698,
 'sample_weights': [6.244617805581403, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.305366013381681,
 'weight_decay_Hydroxylation-K': 2.2459435516742743,
 'weight_decay_Hydroxylation-P': 1.8843476742155207}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1371.674
[2,     1] loss: 1371.260
[3,     1] loss: 1369.343
[4,     1] loss: 1371.943
[5,     1] loss: 1373.998
[6,     1] loss: 1370.398
[7,     1] loss: 1375.949
[8,     1] loss: 1373.241
[9,     1] loss: 1373.753
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008456844229086366,
 'learning_rate_Hydroxylation-K': 0.008279174273805786,
 'learning_rate_Hydroxylation-P': 0.005374619408839728,
 'log_base': 1.1084568533423629,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1516878396,
 'sample_weights': [2.195877934836949, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.306854560550054,
 'weight_decay_Hydroxylation-K': 6.775879816713688,
 'weight_decay_Hydroxylation-P': 5.119680062248395}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5283.725
[2,     1] loss: 5257.494
[3,     1] loss: 5310.037
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006439748596194413,
 'learning_rate_Hydroxylation-K': 0.005421115039144993,
 'learning_rate_Hydroxylation-P': 0.006611646825854862,
 'log_base': 1.012343138317613,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1572278995,
 'sample_weights': [16.213092978060377, 2.0267146964384573],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.075238613049217,
 'weight_decay_Hydroxylation-K': 6.29807212413939,
 'weight_decay_Hydroxylation-P': 7.29126043725616}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44151.164
Exploding loss, terminate run (best metric=0.5337492227554321)
Finished Training
Total time taken: 0.20202946662902832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44224.008
Exploding loss, terminate run (best metric=0.5275499224662781)
Finished Training
Total time taken: 0.21700048446655273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44165.715
Exploding loss, terminate run (best metric=0.5345147252082825)
Finished Training
Total time taken: 0.21500015258789062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44267.719
Exploding loss, terminate run (best metric=0.5280367732048035)
Finished Training
Total time taken: 0.22499895095825195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44093.281
Exploding loss, terminate run (best metric=0.5290082097053528)
Finished Training
Total time taken: 0.21200013160705566
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44082.559
Exploding loss, terminate run (best metric=0.5432004332542419)
Finished Training
Total time taken: 0.20299959182739258
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44388.828
Exploding loss, terminate run (best metric=0.5345214605331421)
Finished Training
Total time taken: 0.2089979648590088
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44255.832
Exploding loss, terminate run (best metric=0.5277983546257019)
Finished Training
Total time taken: 0.20100092887878418
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44070.766
Exploding loss, terminate run (best metric=0.5354479551315308)
Finished Training
Total time taken: 0.21500062942504883
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44313.793
Exploding loss, terminate run (best metric=0.5328304171562195)
Finished Training
Total time taken: 0.2050004005432129
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44060.980
Exploding loss, terminate run (best metric=0.533111035823822)
Finished Training
Total time taken: 0.21000123023986816
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44363.375
Exploding loss, terminate run (best metric=0.5258358716964722)
Finished Training
Total time taken: 0.21000027656555176
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44133.645
Exploding loss, terminate run (best metric=0.5388049483299255)
Finished Training
Total time taken: 0.21100091934204102
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44161.184
Exploding loss, terminate run (best metric=0.5267038941383362)
Finished Training
Total time taken: 0.1960000991821289
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44111.582
Exploding loss, terminate run (best metric=0.5276464819908142)
Finished Training
Total time taken: 0.21300077438354492
{'Hydroxylation-K Validation Accuracy': 0.49134160756501183, 'Hydroxylation-K Validation Sensitivity': 0.5266666666666666, 'Hydroxylation-K Validation Specificity': 0.48771929824561405, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5952631578947368, 'Hydroxylation-K AUC PR': 0.3040511318199618, 'Hydroxylation-K MCC': 0.02585330828765672, 'Hydroxylation-K F1': 0.1894835050007464, 'Validation Loss (Hydroxylation-K)': 0.5575435280799865, 'Hydroxylation-P Validation Accuracy': 0.4861190294908888, 'Hydroxylation-P Validation Sensitivity': 0.5388359788359789, 'Hydroxylation-P Validation Specificity': 0.47560975609756095, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6262549234534122, 'Hydroxylation-P AUC PR': 0.317665549159168, 'Hydroxylation-P MCC': 0.022264174128076242, 'Hydroxylation-P F1': 0.17405588925201662, 'Validation Loss (Hydroxylation-P)': 0.5319173137346903, 'Validation Loss (total)': 1.089460833867391, 'TimeToTrain': 0.20960213343302408}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007122314768691471,
 'learning_rate_Hydroxylation-K': 0.0036288361810538298,
 'learning_rate_Hydroxylation-P': 0.006638425525798879,
 'log_base': 1.1930877226004233,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 264267859,
 'sample_weights': [136.18669254470421, 16.9879650823878],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4443069605353873,
 'weight_decay_Hydroxylation-K': 4.139646562799491,
 'weight_decay_Hydroxylation-P': 5.027720153548463}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3081.152
[2,     1] loss: 3070.898
[3,     1] loss: 3088.248
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003374170547429387,
 'learning_rate_Hydroxylation-K': 0.0035856655869406315,
 'learning_rate_Hydroxylation-P': 0.005516397580225067,
 'log_base': 1.1900445616020527,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1336300814,
 'sample_weights': [9.45620806981049, 1.1820715451147452],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.108175333541059,
 'weight_decay_Hydroxylation-K': 5.490526254820254,
 'weight_decay_Hydroxylation-P': 6.635888238891776}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3115.987
[2,     1] loss: 3106.387
[3,     1] loss: 3115.307
[4,     1] loss: 3101.729
[5,     1] loss: 3107.180
[6,     1] loss: 3116.573
[7,     1] loss: 3074.009
[8,     1] loss: 3087.808
[9,     1] loss: 3085.078
[10,     1] loss: 3052.161
[11,     1] loss: 3013.777
[12,     1] loss: 2984.573
[13,     1] loss: 2855.738
[14,     1] loss: 2802.523
[15,     1] loss: 2746.192
[16,     1] loss: 2660.100
[17,     1] loss: 2751.337
[18,     1] loss: 2545.309
[19,     1] loss: 2621.008
[20,     1] loss: 2452.785
[21,     1] loss: 2632.161
[22,     1] loss: 2685.081
[23,     1] loss: 2529.513
[24,     1] loss: 2393.821
[25,     1] loss: 2472.620
[26,     1] loss: 2588.377
[27,     1] loss: 2477.964
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012269587392613858,
 'learning_rate_Hydroxylation-K': 0.0031172269153788095,
 'learning_rate_Hydroxylation-P': 0.007441140318586413,
 'log_base': 1.6582559651464481,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1988392697,
 'sample_weights': [9.595010753334062, 1.1994225489597856],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.130368975623208,
 'weight_decay_Hydroxylation-K': 5.711090024959985,
 'weight_decay_Hydroxylation-P': 3.093987098504535}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1615.884
[2,     1] loss: 1609.836
[3,     1] loss: 1607.421
[4,     1] loss: 1601.810
[5,     1] loss: 1609.047
[6,     1] loss: 1605.904
[7,     1] loss: 1601.966
[8,     1] loss: 1602.913
[9,     1] loss: 1605.195
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008461746215542978,
 'learning_rate_Hydroxylation-K': 0.00473095934749668,
 'learning_rate_Hydroxylation-P': 0.0061506589075202974,
 'log_base': 1.4808002584312048,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 602030183,
 'sample_weights': [3.3008184398713447, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.089447215143781,
 'weight_decay_Hydroxylation-K': 5.385604725815659,
 'weight_decay_Hydroxylation-P': 8.193819628002267}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1811.898
[2,     1] loss: 1805.418
[3,     1] loss: 1810.149
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005738265401696292,
 'learning_rate_Hydroxylation-K': 0.008943748061970163,
 'learning_rate_Hydroxylation-P': 0.007259309450919114,
 'log_base': 1.0130987210793043,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2028298983,
 'sample_weights': [4.25246280882101, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.967678050888862,
 'weight_decay_Hydroxylation-K': 6.603163019141579,
 'weight_decay_Hydroxylation-P': 3.172444072626259}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41781.562
Exploding loss, terminate run (best metric=0.5338388085365295)
Finished Training
Total time taken: 0.19900131225585938
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41796.570
Exploding loss, terminate run (best metric=0.5299596786499023)
Finished Training
Total time taken: 0.21399784088134766
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41610.461
Exploding loss, terminate run (best metric=0.5274438261985779)
Finished Training
Total time taken: 0.21299982070922852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41849.559
Exploding loss, terminate run (best metric=0.5266611576080322)
Finished Training
Total time taken: 0.21400213241577148
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 41769.227
Exploding loss, terminate run (best metric=0.5438266396522522)
Finished Training
Total time taken: 0.20100069046020508
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41620.535
Exploding loss, terminate run (best metric=0.5483747124671936)
Finished Training
Total time taken: 0.22500061988830566
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41659.930
Exploding loss, terminate run (best metric=0.5370804667472839)
Finished Training
Total time taken: 0.21200156211853027
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41585.555
Exploding loss, terminate run (best metric=0.526658296585083)
Finished Training
Total time taken: 0.22600221633911133
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41634.523
Exploding loss, terminate run (best metric=0.527730405330658)
Finished Training
Total time taken: 0.20299792289733887
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 41859.543
Exploding loss, terminate run (best metric=0.5334615111351013)
Finished Training
Total time taken: 0.2290029525756836
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41596.410
Exploding loss, terminate run (best metric=0.531830906867981)
Finished Training
Total time taken: 0.23700261116027832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41580.965
Exploding loss, terminate run (best metric=0.5263240933418274)
Finished Training
Total time taken: 0.21199679374694824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41521.355
Exploding loss, terminate run (best metric=0.5333924293518066)
Finished Training
Total time taken: 0.2090005874633789
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41718.922
Exploding loss, terminate run (best metric=0.5335820913314819)
Finished Training
Total time taken: 0.21900010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 42075.203
Exploding loss, terminate run (best metric=0.5481712222099304)
Finished Training
Total time taken: 0.2520003318786621
{'Hydroxylation-K Validation Accuracy': 0.5591903073286052, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.5982456140350877, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5567446393762183, 'Hydroxylation-K AUC PR': 0.3471754953743829, 'Hydroxylation-K MCC': -0.004988477453464162, 'Hydroxylation-K F1': 0.13481116584564862, 'Validation Loss (Hydroxylation-K)': 0.5590571880340576, 'Hydroxylation-P Validation Accuracy': 0.5570384244454596, 'Hydroxylation-P Validation Sensitivity': 0.4074074074074074, 'Hydroxylation-P Validation Specificity': 0.5894308943089431, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.536484602802499, 'Hydroxylation-P AUC PR': 0.2395214082300833, 'Hydroxylation-P MCC': -0.0034017979087609643, 'Hydroxylation-P F1': 0.12813489084675528, 'Validation Loss (Hydroxylation-P)': 0.5338890830675761, 'Validation Loss (total)': 1.0929462591807046, 'TimeToTrain': 0.21766716639200848}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009287500381088996,
 'learning_rate_Hydroxylation-K': 0.0003560884450322343,
 'learning_rate_Hydroxylation-P': 0.007388156131183816,
 'log_base': 2.125527747105733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 566921444,
 'sample_weights': [128.37892271461973, 16.014021749405714],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.13194778442972,
 'weight_decay_Hydroxylation-K': 9.665209542040563,
 'weight_decay_Hydroxylation-P': 5.940242558697157}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1373.146
[2,     1] loss: 1378.078
[3,     1] loss: 1377.633
[4,     1] loss: 1374.660
[5,     1] loss: 1372.420
[6,     1] loss: 1372.592
[7,     1] loss: 1379.147
[8,     1] loss: 1367.478
[9,     1] loss: 1367.715
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005324341651470718,
 'learning_rate_Hydroxylation-K': 0.009676978632712994,
 'learning_rate_Hydroxylation-P': 0.007819602166702517,
 'log_base': 1.0314854826821636,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4227292391,
 'sample_weights': [2.214056490313171, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.781187829181146,
 'weight_decay_Hydroxylation-K': 6.569016406164423,
 'weight_decay_Hydroxylation-P': 1.0312012633771723}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17499.270
Exploding loss, terminate run (best metric=0.5358060598373413)
Finished Training
Total time taken: 0.18503189086914062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17455.992
Exploding loss, terminate run (best metric=0.5274296402931213)
Finished Training
Total time taken: 0.22300267219543457
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17504.066
Exploding loss, terminate run (best metric=0.5266621708869934)
Finished Training
Total time taken: 0.2090001106262207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17602.986
Exploding loss, terminate run (best metric=0.533334493637085)
Finished Training
Total time taken: 0.20099997520446777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17490.316
Exploding loss, terminate run (best metric=0.530235230922699)
Finished Training
Total time taken: 0.23099970817565918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17543.682
Exploding loss, terminate run (best metric=0.5364071726799011)
Finished Training
Total time taken: 0.20406174659729004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17511.221
Exploding loss, terminate run (best metric=0.5287113785743713)
Finished Training
Total time taken: 0.19199752807617188
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17507.975
Exploding loss, terminate run (best metric=0.5311633348464966)
Finished Training
Total time taken: 0.21403121948242188
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17598.145
Exploding loss, terminate run (best metric=0.52792888879776)
Finished Training
Total time taken: 0.23301196098327637
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17524.424
Exploding loss, terminate run (best metric=0.533146858215332)
Finished Training
Total time taken: 0.23400163650512695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17489.713
Exploding loss, terminate run (best metric=0.5350325107574463)
Finished Training
Total time taken: 0.21699786186218262
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17464.383
Exploding loss, terminate run (best metric=0.5270953178405762)
Finished Training
Total time taken: 0.19700217247009277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17473.723
Exploding loss, terminate run (best metric=0.5289639234542847)
Finished Training
Total time taken: 0.2559974193572998
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17624.715
Exploding loss, terminate run (best metric=0.5385305881500244)
Finished Training
Total time taken: 0.2540004253387451
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17543.055
Exploding loss, terminate run (best metric=0.5341838598251343)
Finished Training
Total time taken: 0.2040114402770996
{'Hydroxylation-K Validation Accuracy': 0.5592198581560284, 'Hydroxylation-K Validation Sensitivity': 0.4266666666666667, 'Hydroxylation-K Validation Specificity': 0.5912280701754385, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6352046783625731, 'Hydroxylation-K AUC PR': 0.3319410567376159, 'Hydroxylation-K MCC': 0.02092399960954238, 'Hydroxylation-K F1': 0.16186744290192567, 'Validation Loss (Hydroxylation-K)': 0.5586682081222534, 'Hydroxylation-P Validation Accuracy': 0.5425036292573981, 'Hydroxylation-P Validation Sensitivity': 0.4533333333333333, 'Hydroxylation-P Validation Specificity': 0.5626066137961994, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5837127885885929, 'Hydroxylation-P AUC PR': 0.2775047608909706, 'Hydroxylation-P MCC': 0.014941156556516264, 'Hydroxylation-P F1': 0.14369631312306475, 'Validation Loss (Hydroxylation-P)': 0.5316420952479045, 'Validation Loss (total)': 1.090310287475586, 'TimeToTrain': 0.21694318453470865}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004253122883507701,
 'learning_rate_Hydroxylation-K': 0.006601171787090885,
 'learning_rate_Hydroxylation-P': 0.009554925966356487,
 'log_base': 1.1134493850935518,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3249276978,
 'sample_weights': [53.89298856641388, 6.7226260572503005],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.1079679209046356,
 'weight_decay_Hydroxylation-K': 7.827819911947323,
 'weight_decay_Hydroxylation-P': 6.767310376490968}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5048.854
[2,     1] loss: 5048.798
[3,     1] loss: 5036.107
[4,     1] loss: 5042.452
[5,     1] loss: 5038.578
[6,     1] loss: 5048.893
[7,     1] loss: 5035.461
[8,     1] loss: 5020.601
[9,     1] loss: 5020.805
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025964201496146747,
 'learning_rate_Hydroxylation-K': 0.00877081518118851,
 'learning_rate_Hydroxylation-P': 0.008669761526263774,
 'log_base': 1.01220982370553,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1930014196,
 'sample_weights': [15.535086641182186, 1.9419606393878437],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.493167581462881,
 'weight_decay_Hydroxylation-K': 5.133501400008005,
 'weight_decay_Hydroxylation-P': 3.1583029531481244}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44438.984
Exploding loss, terminate run (best metric=0.5390484929084778)
Finished Training
Total time taken: 0.20399808883666992
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44775.758
Exploding loss, terminate run (best metric=0.5262879133224487)
Finished Training
Total time taken: 0.20399999618530273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44876.004
Exploding loss, terminate run (best metric=0.5276910066604614)
Finished Training
Total time taken: 0.2330036163330078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 45164.000
Exploding loss, terminate run (best metric=0.5341552495956421)
Finished Training
Total time taken: 0.2149982452392578
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44928.973
Exploding loss, terminate run (best metric=0.5301892161369324)
Finished Training
Total time taken: 0.20800065994262695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44578.359
Exploding loss, terminate run (best metric=0.5303632020950317)
Finished Training
Total time taken: 0.2110004425048828
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44811.773
Exploding loss, terminate run (best metric=0.5329269170761108)
Finished Training
Total time taken: 0.20800042152404785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44552.074
Exploding loss, terminate run (best metric=0.525955319404602)
Finished Training
Total time taken: 0.20600008964538574
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44557.316
Exploding loss, terminate run (best metric=0.5268014669418335)
Finished Training
Total time taken: 0.2180004119873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44720.238
Exploding loss, terminate run (best metric=0.5294350385665894)
Finished Training
Total time taken: 0.2050013542175293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44611.852
Exploding loss, terminate run (best metric=0.5356640815734863)
Finished Training
Total time taken: 0.20800042152404785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44694.195
Exploding loss, terminate run (best metric=0.526328444480896)
Finished Training
Total time taken: 0.20000028610229492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44765.953
Exploding loss, terminate run (best metric=0.527542769908905)
Finished Training
Total time taken: 0.2069997787475586
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44619.664
Exploding loss, terminate run (best metric=0.5329961180686951)
Finished Training
Total time taken: 0.20999884605407715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44658.211
Exploding loss, terminate run (best metric=0.5308921337127686)
Finished Training
Total time taken: 0.2069997787475586
{'Hydroxylation-K Validation Accuracy': 0.44166666666666665, 'Hydroxylation-K Validation Sensitivity': 0.6, 'Hydroxylation-K Validation Specificity': 0.4, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6428849902534114, 'Hydroxylation-K AUC PR': 0.35652369758707164, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.20221674876847293, 'Validation Loss (Hydroxylation-K)': 0.556922173500061, 'Hydroxylation-P Validation Accuracy': 0.4349603911814967, 'Hydroxylation-P Validation Sensitivity': 0.6, 'Hydroxylation-P Validation Specificity': 0.4, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6011510806460679, 'Hydroxylation-P AUC PR': 0.2952408080668138, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.18005432731554796, 'Validation Loss (Hydroxylation-P)': 0.5304184913635254, 'Validation Loss (total)': 1.0873406807581583, 'TimeToTrain': 0.20960016250610353}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006556121951389988,
 'learning_rate_Hydroxylation-K': 0.004249901698328677,
 'learning_rate_Hydroxylation-P': 0.0005678832923996097,
 'log_base': 1.4937059307085536,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 836214094,
 'sample_weights': [137.6645815752863, 17.172317362174102],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9069600837799285,
 'weight_decay_Hydroxylation-K': 4.598005756974182,
 'weight_decay_Hydroxylation-P': 0.8416636435780245}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1794.550
[2,     1] loss: 1786.998
[3,     1] loss: 1783.349
[4,     1] loss: 1782.796
[5,     1] loss: 1786.067
[6,     1] loss: 1776.009
[7,     1] loss: 1777.669
[8,     1] loss: 1795.760
[9,     1] loss: 1788.926
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017099533586708367,
 'learning_rate_Hydroxylation-K': 0.003707138962433809,
 'learning_rate_Hydroxylation-P': 0.0059006813444747565,
 'log_base': 1.3385272642010366,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3940511707,
 'sample_weights': [4.160499862098679, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.148821405146194,
 'weight_decay_Hydroxylation-K': 7.968218365801362,
 'weight_decay_Hydroxylation-P': 1.0786802602839707}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2105.086
[2,     1] loss: 2104.542
[3,     1] loss: 2114.728
[4,     1] loss: 2115.123
[5,     1] loss: 2115.656
[6,     1] loss: 2114.985
[7,     1] loss: 2109.951
[8,     1] loss: 2104.751
[9,     1] loss: 2105.947
[10,     1] loss: 2109.741
[11,     1] loss: 2109.144
[12,     1] loss: 2102.324
[13,     1] loss: 2084.756
[14,     1] loss: 2084.738
[15,     1] loss: 2066.395
[16,     1] loss: 2035.789
[17,     1] loss: 2022.793
[18,     1] loss: 2000.085
[19,     1] loss: 1964.046
[20,     1] loss: 1940.808
[21,     1] loss: 1911.777
[22,     1] loss: 1863.362
[23,     1] loss: 1851.966
[24,     1] loss: 1774.799
[25,     1] loss: 1816.669
[26,     1] loss: 1743.732
[27,     1] loss: 1833.853
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015323071018412608,
 'learning_rate_Hydroxylation-K': 0.009456222821397124,
 'learning_rate_Hydroxylation-P': 0.00748237819120223,
 'log_base': 1.2348916792613247,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1099932207,
 'sample_weights': [5.7257036615015915, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.389518994222597,
 'weight_decay_Hydroxylation-K': 4.843608389928545,
 'weight_decay_Hydroxylation-P': 2.759039331574055}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2578.596
[2,     1] loss: 2576.656
[3,     1] loss: 2581.266
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006701762571806199,
 'learning_rate_Hydroxylation-K': 0.008033769870408315,
 'learning_rate_Hydroxylation-P': 0.005934988542716381,
 'log_base': 2.190948918668944,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3170148001,
 'sample_weights': [7.912680703238235, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.07441773548779662,
 'weight_decay_Hydroxylation-K': 9.804848325844597,
 'weight_decay_Hydroxylation-P': 0.6101504231573975}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1357.104
[2,     1] loss: 1357.396
[3,     1] loss: 1358.939
[4,     1] loss: 1361.142
[5,     1] loss: 1354.715
[6,     1] loss: 1358.205
[7,     1] loss: 1357.008
[8,     1] loss: 1356.218
[9,     1] loss: 1354.756
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0060565881793380304,
 'learning_rate_Hydroxylation-K': 0.006903552127760514,
 'learning_rate_Hydroxylation-P': 0.009042932612732459,
 'log_base': 2.5597855505894516,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2567868137,
 'sample_weights': [2.1284829666892717, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.27783760557488013,
 'weight_decay_Hydroxylation-K': 6.322447897996057,
 'weight_decay_Hydroxylation-P': 3.573843664352302}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.267
[2,     1] loss: 1284.022
[3,     1] loss: 1283.511
[4,     1] loss: 1278.912
[5,     1] loss: 1281.286
[6,     1] loss: 1277.755
[7,     1] loss: 1277.970
[8,     1] loss: 1268.747
[9,     1] loss: 1252.246
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00513167222264622,
 'learning_rate_Hydroxylation-K': 0.007046473263606084,
 'learning_rate_Hydroxylation-P': 0.007925752595798155,
 'log_base': 1.6844407716642729,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2544499166,
 'sample_weights': [1.7761479235865605, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5934112415926642,
 'weight_decay_Hydroxylation-K': 9.500690254721407,
 'weight_decay_Hydroxylation-P': 3.4436231408415563}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1590.379
[2,     1] loss: 1598.877
[3,     1] loss: 1576.335
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003687018098419517,
 'learning_rate_Hydroxylation-K': 0.008372825357560676,
 'learning_rate_Hydroxylation-P': 0.00843968163420978,
 'log_base': 1.2253934522946992,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1088345035,
 'sample_weights': [3.20164077554276, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.467396517120617,
 'weight_decay_Hydroxylation-K': 4.970161160232518,
 'weight_decay_Hydroxylation-P': 0.36770686910241324}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2678.374
[2,     1] loss: 2667.684
[3,     1] loss: 2673.504
[4,     1] loss: 2664.255
[5,     1] loss: 2677.594
[6,     1] loss: 2664.222
[7,     1] loss: 2665.017
[8,     1] loss: 2661.651
[9,     1] loss: 2661.785
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00561174976819946,
 'learning_rate_Hydroxylation-K': 0.006108117205058119,
 'learning_rate_Hydroxylation-P': 0.006650772566868544,
 'log_base': 1.2838140206390438,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1734919564,
 'sample_weights': [8.21325839717235, 1.0266968506084009],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.7856562355288,
 'weight_decay_Hydroxylation-K': 7.257798302112982,
 'weight_decay_Hydroxylation-P': 8.485991633872944}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2319.614
[2,     1] loss: 2328.123
[3,     1] loss: 2315.070
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006572084309684114,
 'learning_rate_Hydroxylation-K': 0.009615758340991983,
 'learning_rate_Hydroxylation-P': 0.006972971206008146,
 'log_base': 2.800375423641306,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4270645500,
 'sample_weights': [6.682173441383088, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.928127280138588,
 'weight_decay_Hydroxylation-K': 5.9167531429418805,
 'weight_decay_Hydroxylation-P': 1.7015322209628971}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.919
[2,     1] loss: 1249.452
[3,     1] loss: 1250.186
[4,     1] loss: 1251.055
[5,     1] loss: 1249.375
[6,     1] loss: 1245.281
[7,     1] loss: 1236.655
[8,     1] loss: 1237.478
[9,     1] loss: 1217.154
[10,     1] loss: 1190.893
[11,     1] loss: 1150.656
[12,     1] loss: 1111.560
[13,     1] loss: 1090.719
[14,     1] loss: 1135.009
[15,     1] loss: 1090.400
[16,     1] loss: 1055.137
[17,     1] loss: 1037.636
[18,     1] loss: 1050.580
[19,     1] loss: 1015.925
[20,     1] loss: 986.983
[21,     1] loss: 977.046
[22,     1] loss: 985.172
[23,     1] loss: 966.057
[24,     1] loss: 977.832
[25,     1] loss: 990.635
[26,     1] loss: 937.447
[27,     1] loss: 944.414
[28,     1] loss: 926.747
[29,     1] loss: 931.623
[30,     1] loss: 926.371
[31,     1] loss: 964.633
[32,     1] loss: 982.702
[33,     1] loss: 880.140
[34,     1] loss: 908.576
[35,     1] loss: 912.334
[36,     1] loss: 918.751
[37,     1] loss: 878.887
[38,     1] loss: 876.395
[39,     1] loss: 840.535
[40,     1] loss: 857.891
[41,     1] loss: 853.351
[42,     1] loss: 810.533
[43,     1] loss: 801.486
[44,     1] loss: 759.421
[45,     1] loss: 734.993
[46,     1] loss: 755.346
[47,     1] loss: 711.660
[48,     1] loss: 709.601
[49,     1] loss: 779.252
[50,     1] loss: 2132.119
[51,     1] loss: 1041.411
[52,     1] loss: 1028.626
[53,     1] loss: 1127.849
[54,     1] loss: 1079.958
[55,     1] loss: 1127.985
[56,     1] loss: 1146.328
[57,     1] loss: 1120.715
[58,     1] loss: 1088.387
[59,     1] loss: 1056.745
[60,     1] loss: 1090.026
[61,     1] loss: 1076.499
[62,     1] loss: 1053.346
[63,     1] loss: 1070.237
[64,     1] loss: 1019.027
[65,     1] loss: 1059.965
[66,     1] loss: 1009.113
[67,     1] loss: 1004.925
[68,     1] loss: 983.696
[69,     1] loss: 971.798
[70,     1] loss: 968.405
[71,     1] loss: 951.794
[72,     1] loss: 915.468
[73,     1] loss: 883.009
[74,     1] loss: 898.125
Early stopping applied (best metric=0.3178732097148895)
Finished Training
Total time taken: 10.545029640197754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.271
[2,     1] loss: 1254.950
[3,     1] loss: 1257.173
[4,     1] loss: 1246.894
[5,     1] loss: 1249.465
[6,     1] loss: 1249.616
[7,     1] loss: 1247.334
[8,     1] loss: 1249.138
[9,     1] loss: 1244.716
[10,     1] loss: 1242.129
[11,     1] loss: 1235.464
[12,     1] loss: 1224.632
[13,     1] loss: 1193.870
[14,     1] loss: 1172.920
[15,     1] loss: 1142.758
[16,     1] loss: 1101.624
[17,     1] loss: 1117.625
[18,     1] loss: 1069.853
[19,     1] loss: 1050.068
[20,     1] loss: 1036.807
[21,     1] loss: 1065.762
[22,     1] loss: 1041.817
[23,     1] loss: 1023.864
[24,     1] loss: 1021.675
[25,     1] loss: 1025.325
[26,     1] loss: 1001.445
[27,     1] loss: 976.420
[28,     1] loss: 983.286
[29,     1] loss: 925.639
[30,     1] loss: 936.356
[31,     1] loss: 932.208
[32,     1] loss: 954.408
[33,     1] loss: 888.059
[34,     1] loss: 919.563
[35,     1] loss: 906.784
[36,     1] loss: 906.523
[37,     1] loss: 960.705
[38,     1] loss: 859.734
[39,     1] loss: 910.882
[40,     1] loss: 872.451
[41,     1] loss: 852.782
[42,     1] loss: 868.505
[43,     1] loss: 815.381
[44,     1] loss: 837.233
[45,     1] loss: 776.696
[46,     1] loss: 1034.668
[47,     1] loss: 856.402
[48,     1] loss: 890.628
[49,     1] loss: 807.554
[50,     1] loss: 918.152
[51,     1] loss: 781.592
[52,     1] loss: 858.494
[53,     1] loss: 830.591
[54,     1] loss: 832.100
[55,     1] loss: 792.614
[56,     1] loss: 743.932
[57,     1] loss: 796.227
[58,     1] loss: 738.724
[59,     1] loss: 752.012
[60,     1] loss: 722.803
[61,     1] loss: 638.963
[62,     1] loss: 693.161
[63,     1] loss: 665.655
[64,     1] loss: 832.913
[65,     1] loss: 955.563
[66,     1] loss: 913.131
[67,     1] loss: 832.347
[68,     1] loss: 807.046
[69,     1] loss: 933.961
[70,     1] loss: 828.091
[71,     1] loss: 743.223
[72,     1] loss: 876.422
[73,     1] loss: 755.828
[74,     1] loss: 765.861
[75,     1] loss: 732.119
[76,     1] loss: 705.418
[77,     1] loss: 689.145
[78,     1] loss: 637.856
[79,     1] loss: 612.413
[80,     1] loss: 621.319
[81,     1] loss: 635.762
[82,     1] loss: 755.969
[83,     1] loss: 811.024
[84,     1] loss: 654.148
[85,     1] loss: 673.600
[86,     1] loss: 647.087
[87,     1] loss: 609.031
[88,     1] loss: 661.001
[89,     1] loss: 568.645
[90,     1] loss: 597.689
[91,     1] loss: 676.369
[92,     1] loss: 1135.369
[93,     1] loss: 637.100
[94,     1] loss: 837.891
[95,     1] loss: 695.309
[96,     1] loss: 809.871
[97,     1] loss: 674.982
[98,     1] loss: 719.111
[99,     1] loss: 620.163
[100,     1] loss: 632.015
[101,     1] loss: 635.124
[102,     1] loss: 610.170
[103,     1] loss: 543.730
[104,     1] loss: 606.204
[105,     1] loss: 792.013
[106,     1] loss: 628.513
[107,     1] loss: 599.156
[108,     1] loss: 649.249
Early stopping applied (best metric=0.3587130010128021)
Finished Training
Total time taken: 15.316157102584839
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.108
[2,     1] loss: 1249.758
[3,     1] loss: 1249.164
[4,     1] loss: 1250.950
[5,     1] loss: 1248.237
[6,     1] loss: 1245.163
[7,     1] loss: 1246.488
[8,     1] loss: 1242.242
[9,     1] loss: 1237.134
[10,     1] loss: 1222.176
[11,     1] loss: 1191.011
[12,     1] loss: 1164.116
[13,     1] loss: 1151.699
[14,     1] loss: 1085.909
[15,     1] loss: 1054.131
[16,     1] loss: 1059.179
[17,     1] loss: 1029.950
[18,     1] loss: 1062.105
[19,     1] loss: 995.855
[20,     1] loss: 993.540
[21,     1] loss: 970.271
[22,     1] loss: 965.130
[23,     1] loss: 986.554
[24,     1] loss: 984.724
[25,     1] loss: 934.621
[26,     1] loss: 928.528
[27,     1] loss: 931.024
[28,     1] loss: 934.236
[29,     1] loss: 971.093
[30,     1] loss: 851.929
[31,     1] loss: 937.275
[32,     1] loss: 910.201
[33,     1] loss: 837.247
[34,     1] loss: 890.608
[35,     1] loss: 857.504
[36,     1] loss: 889.931
[37,     1] loss: 826.207
[38,     1] loss: 854.828
[39,     1] loss: 824.749
[40,     1] loss: 818.712
[41,     1] loss: 813.932
[42,     1] loss: 785.894
[43,     1] loss: 832.772
[44,     1] loss: 845.806
[45,     1] loss: 740.520
[46,     1] loss: 741.424
[47,     1] loss: 799.790
[48,     1] loss: 732.771
[49,     1] loss: 741.220
[50,     1] loss: 722.082
[51,     1] loss: 659.475
[52,     1] loss: 673.099
[53,     1] loss: 892.525
[54,     1] loss: 983.046
[55,     1] loss: 688.392
[56,     1] loss: 890.053
[57,     1] loss: 733.338
[58,     1] loss: 806.361
[59,     1] loss: 808.874
[60,     1] loss: 719.735
[61,     1] loss: 747.252
[62,     1] loss: 729.739
[63,     1] loss: 700.403
[64,     1] loss: 639.790
[65,     1] loss: 675.797
[66,     1] loss: 625.683
[67,     1] loss: 657.609
[68,     1] loss: 676.581
[69,     1] loss: 572.677
[70,     1] loss: 632.748
[71,     1] loss: 561.170
[72,     1] loss: 532.516
[73,     1] loss: 605.744
[74,     1] loss: 799.795
[75,     1] loss: 957.429
[76,     1] loss: 762.115
Early stopping applied (best metric=0.38412317633628845)
Finished Training
Total time taken: 10.899596452713013
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.494
[2,     1] loss: 1255.606
[3,     1] loss: 1247.825
[4,     1] loss: 1248.596
[5,     1] loss: 1247.289
[6,     1] loss: 1253.892
[7,     1] loss: 1251.829
[8,     1] loss: 1248.301
[9,     1] loss: 1247.951
[10,     1] loss: 1247.920
[11,     1] loss: 1245.966
[12,     1] loss: 1246.011
[13,     1] loss: 1239.942
[14,     1] loss: 1232.724
[15,     1] loss: 1222.787
[16,     1] loss: 1195.924
[17,     1] loss: 1144.151
[18,     1] loss: 1127.809
[19,     1] loss: 1094.526
[20,     1] loss: 1053.405
[21,     1] loss: 1059.886
[22,     1] loss: 1036.295
[23,     1] loss: 1077.914
[24,     1] loss: 1007.729
[25,     1] loss: 1036.465
[26,     1] loss: 1029.594
[27,     1] loss: 989.568
[28,     1] loss: 974.055
[29,     1] loss: 1012.550
[30,     1] loss: 985.239
[31,     1] loss: 1016.178
[32,     1] loss: 952.246
[33,     1] loss: 979.504
[34,     1] loss: 926.438
[35,     1] loss: 925.823
[36,     1] loss: 863.570
[37,     1] loss: 891.604
[38,     1] loss: 993.309
[39,     1] loss: 1013.417
[40,     1] loss: 863.071
[41,     1] loss: 891.728
[42,     1] loss: 905.266
[43,     1] loss: 848.578
[44,     1] loss: 867.428
[45,     1] loss: 831.838
[46,     1] loss: 799.310
[47,     1] loss: 791.556
[48,     1] loss: 787.451
[49,     1] loss: 788.555
[50,     1] loss: 772.129
[51,     1] loss: 824.367
[52,     1] loss: 1142.050
[53,     1] loss: 1152.563
[54,     1] loss: 991.583
[55,     1] loss: 942.472
[56,     1] loss: 988.671
[57,     1] loss: 1038.909
[58,     1] loss: 1017.352
[59,     1] loss: 988.556
[60,     1] loss: 954.657
[61,     1] loss: 941.187
[62,     1] loss: 953.136
[63,     1] loss: 917.452
[64,     1] loss: 902.282
[65,     1] loss: 877.880
[66,     1] loss: 896.673
[67,     1] loss: 865.271
[68,     1] loss: 875.482
[69,     1] loss: 888.404
[70,     1] loss: 806.095
[71,     1] loss: 817.682
[72,     1] loss: 828.002
[73,     1] loss: 770.412
[74,     1] loss: 763.742
[75,     1] loss: 729.414
[76,     1] loss: 836.783
[77,     1] loss: 759.470
[78,     1] loss: 681.303
[79,     1] loss: 907.166
[80,     1] loss: 702.133
[81,     1] loss: 785.618
[82,     1] loss: 782.827
[83,     1] loss: 692.103
[84,     1] loss: 795.219
[85,     1] loss: 694.606
[86,     1] loss: 662.744
[87,     1] loss: 785.765
[88,     1] loss: 710.803
[89,     1] loss: 608.778
[90,     1] loss: 695.375
[91,     1] loss: 989.528
[92,     1] loss: 857.601
[93,     1] loss: 736.529
[94,     1] loss: 757.478
[95,     1] loss: 838.913
[96,     1] loss: 674.861
[97,     1] loss: 760.880
[98,     1] loss: 658.871
[99,     1] loss: 799.959
[100,     1] loss: 631.338
[101,     1] loss: 689.427
[102,     1] loss: 733.833
[103,     1] loss: 565.184
[104,     1] loss: 670.302
[105,     1] loss: 749.583
[106,     1] loss: 547.423
[107,     1] loss: 701.736
[108,     1] loss: 684.289
[109,     1] loss: 542.971
[110,     1] loss: 665.916
[111,     1] loss: 639.242
[112,     1] loss: 508.752
[113,     1] loss: 572.730
[114,     1] loss: 541.950
[115,     1] loss: 469.042
[116,     1] loss: 516.854
Early stopping applied (best metric=0.33081942796707153)
Finished Training
Total time taken: 16.40363073348999
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1251.528
[2,     1] loss: 1252.712
[3,     1] loss: 1256.211
[4,     1] loss: 1254.641
[5,     1] loss: 1251.627
[6,     1] loss: 1251.289
[7,     1] loss: 1254.286
[8,     1] loss: 1253.452
[9,     1] loss: 1249.497
[10,     1] loss: 1250.741
[11,     1] loss: 1252.830
[12,     1] loss: 1249.350
[13,     1] loss: 1248.509
[14,     1] loss: 1247.665
[15,     1] loss: 1243.324
[16,     1] loss: 1238.674
[17,     1] loss: 1228.035
[18,     1] loss: 1215.290
[19,     1] loss: 1177.598
[20,     1] loss: 1153.777
[21,     1] loss: 1097.146
[22,     1] loss: 1049.582
[23,     1] loss: 1107.334
[24,     1] loss: 1022.377
[25,     1] loss: 1112.544
[26,     1] loss: 1029.695
[27,     1] loss: 1059.252
[28,     1] loss: 999.418
[29,     1] loss: 988.861
[30,     1] loss: 989.330
[31,     1] loss: 981.497
[32,     1] loss: 976.948
[33,     1] loss: 957.675
[34,     1] loss: 968.419
[35,     1] loss: 962.467
[36,     1] loss: 979.515
[37,     1] loss: 927.186
[38,     1] loss: 954.675
[39,     1] loss: 883.255
[40,     1] loss: 921.365
[41,     1] loss: 893.374
[42,     1] loss: 907.989
[43,     1] loss: 898.501
[44,     1] loss: 861.823
[45,     1] loss: 850.378
[46,     1] loss: 844.414
[47,     1] loss: 837.282
[48,     1] loss: 834.639
[49,     1] loss: 867.675
[50,     1] loss: 907.418
[51,     1] loss: 1078.941
[52,     1] loss: 865.252
[53,     1] loss: 956.495
[54,     1] loss: 952.029
[55,     1] loss: 848.793
[56,     1] loss: 900.631
[57,     1] loss: 876.506
[58,     1] loss: 793.010
[59,     1] loss: 835.633
[60,     1] loss: 879.074
[61,     1] loss: 848.562
[62,     1] loss: 798.229
[63,     1] loss: 827.141
[64,     1] loss: 805.068
[65,     1] loss: 776.399
[66,     1] loss: 775.443
[67,     1] loss: 775.227
[68,     1] loss: 707.460
[69,     1] loss: 829.384
[70,     1] loss: 748.170
[71,     1] loss: 647.467
[72,     1] loss: 666.253
[73,     1] loss: 743.557
[74,     1] loss: 795.402
[75,     1] loss: 819.807
[76,     1] loss: 643.582
[77,     1] loss: 786.973
[78,     1] loss: 710.435
[79,     1] loss: 673.758
[80,     1] loss: 678.778
[81,     1] loss: 610.281
[82,     1] loss: 617.827
[83,     1] loss: 744.279
[84,     1] loss: 877.438
[85,     1] loss: 634.044
[86,     1] loss: 791.267
[87,     1] loss: 655.653
[88,     1] loss: 752.727
[89,     1] loss: 601.021
[90,     1] loss: 662.750
[91,     1] loss: 627.321
[92,     1] loss: 569.299
[93,     1] loss: 754.910
[94,     1] loss: 942.283
[95,     1] loss: 661.805
[96,     1] loss: 847.765
[97,     1] loss: 585.734
[98,     1] loss: 675.915
[99,     1] loss: 641.759
[100,     1] loss: 765.549
[101,     1] loss: 568.958
[102,     1] loss: 593.318
[103,     1] loss: 662.382
[104,     1] loss: 536.846
[105,     1] loss: 519.923
[106,     1] loss: 622.434
[107,     1] loss: 804.629
[108,     1] loss: 509.381
[109,     1] loss: 551.209
Early stopping applied (best metric=0.3335895240306854)
Finished Training
Total time taken: 15.391191244125366
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.289
[2,     1] loss: 1259.575
[3,     1] loss: 1250.494
[4,     1] loss: 1255.975
[5,     1] loss: 1252.598
[6,     1] loss: 1251.464
[7,     1] loss: 1248.103
[8,     1] loss: 1247.764
[9,     1] loss: 1251.446
[10,     1] loss: 1248.062
[11,     1] loss: 1251.594
[12,     1] loss: 1247.114
[13,     1] loss: 1249.975
[14,     1] loss: 1247.938
[15,     1] loss: 1247.316
[16,     1] loss: 1247.432
[17,     1] loss: 1247.733
[18,     1] loss: 1246.764
[19,     1] loss: 1242.302
[20,     1] loss: 1238.824
[21,     1] loss: 1233.554
[22,     1] loss: 1215.783
[23,     1] loss: 1207.543
[24,     1] loss: 1190.456
[25,     1] loss: 1162.731
[26,     1] loss: 1134.652
[27,     1] loss: 1132.275
[28,     1] loss: 1107.147
[29,     1] loss: 1073.141
[30,     1] loss: 1072.125
[31,     1] loss: 1084.261
[32,     1] loss: 1063.140
[33,     1] loss: 1036.828
[34,     1] loss: 1041.943
[35,     1] loss: 995.929
[36,     1] loss: 1021.652
[37,     1] loss: 1012.189
[38,     1] loss: 973.943
[39,     1] loss: 989.191
[40,     1] loss: 994.843
[41,     1] loss: 969.759
[42,     1] loss: 962.952
[43,     1] loss: 954.822
[44,     1] loss: 933.931
[45,     1] loss: 935.385
[46,     1] loss: 922.997
[47,     1] loss: 932.547
[48,     1] loss: 879.176
[49,     1] loss: 865.935
[50,     1] loss: 912.429
[51,     1] loss: 868.960
[52,     1] loss: 827.697
[53,     1] loss: 852.999
[54,     1] loss: 781.328
[55,     1] loss: 811.479
[56,     1] loss: 863.228
[57,     1] loss: 834.223
[58,     1] loss: 774.833
[59,     1] loss: 805.849
[60,     1] loss: 896.284
[61,     1] loss: 998.392
[62,     1] loss: 770.346
[63,     1] loss: 848.045
[64,     1] loss: 803.171
[65,     1] loss: 838.842
[66,     1] loss: 792.984
[67,     1] loss: 845.556
[68,     1] loss: 769.382
[69,     1] loss: 841.641
[70,     1] loss: 761.087
[71,     1] loss: 830.451
[72,     1] loss: 721.061
[73,     1] loss: 743.241
[74,     1] loss: 719.555
[75,     1] loss: 678.979
[76,     1] loss: 658.319
[77,     1] loss: 623.279
[78,     1] loss: 603.493
[79,     1] loss: 590.493
[80,     1] loss: 687.380
[81,     1] loss: 1964.073
[82,     1] loss: 1298.101
[83,     1] loss: 1074.089
[84,     1] loss: 1061.228
[85,     1] loss: 1122.380
[86,     1] loss: 1112.004
[87,     1] loss: 1169.394
[88,     1] loss: 1153.407
[89,     1] loss: 1141.321
[90,     1] loss: 1127.122
[91,     1] loss: 1130.430
[92,     1] loss: 1140.971
[93,     1] loss: 1129.749
[94,     1] loss: 1111.623
[95,     1] loss: 1111.788
[96,     1] loss: 1087.571
[97,     1] loss: 1093.182
[98,     1] loss: 1066.523
[99,     1] loss: 1071.650
[100,     1] loss: 1043.748
[101,     1] loss: 1026.957
[102,     1] loss: 1002.764
[103,     1] loss: 1009.493
[104,     1] loss: 1013.400
[105,     1] loss: 960.172
[106,     1] loss: 940.458
[107,     1] loss: 917.856
[108,     1] loss: 924.168
[109,     1] loss: 894.297
[110,     1] loss: 870.234
[111,     1] loss: 902.767
[112,     1] loss: 883.521
[113,     1] loss: 924.620
[114,     1] loss: 867.677
[115,     1] loss: 855.383
[116,     1] loss: 917.161
[117,     1] loss: 814.940
[118,     1] loss: 858.453
[119,     1] loss: 993.549
[120,     1] loss: 1122.787
[121,     1] loss: 867.170
[122,     1] loss: 1030.137
[123,     1] loss: 972.490
[124,     1] loss: 906.162
Early stopping applied (best metric=0.3254946768283844)
Finished Training
Total time taken: 17.734233140945435
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.938
[2,     1] loss: 1248.927
[3,     1] loss: 1248.961
[4,     1] loss: 1250.317
[5,     1] loss: 1249.976
[6,     1] loss: 1243.941
[7,     1] loss: 1242.643
[8,     1] loss: 1238.662
[9,     1] loss: 1233.985
[10,     1] loss: 1205.781
[11,     1] loss: 1168.600
[12,     1] loss: 1122.570
[13,     1] loss: 1096.276
[14,     1] loss: 1057.202
[15,     1] loss: 1066.042
[16,     1] loss: 1103.373
[17,     1] loss: 995.100
[18,     1] loss: 996.019
[19,     1] loss: 1002.457
[20,     1] loss: 979.496
[21,     1] loss: 965.252
[22,     1] loss: 996.755
[23,     1] loss: 944.337
[24,     1] loss: 977.451
[25,     1] loss: 987.403
[26,     1] loss: 929.326
[27,     1] loss: 881.718
[28,     1] loss: 905.698
[29,     1] loss: 868.857
[30,     1] loss: 871.685
[31,     1] loss: 825.044
[32,     1] loss: 888.269
[33,     1] loss: 939.216
[34,     1] loss: 968.634
[35,     1] loss: 865.305
[36,     1] loss: 937.765
[37,     1] loss: 862.897
[38,     1] loss: 870.370
[39,     1] loss: 823.044
[40,     1] loss: 883.516
[41,     1] loss: 862.436
[42,     1] loss: 805.239
[43,     1] loss: 833.072
[44,     1] loss: 805.371
[45,     1] loss: 803.897
[46,     1] loss: 816.696
[47,     1] loss: 817.733
[48,     1] loss: 735.204
[49,     1] loss: 795.057
[50,     1] loss: 729.001
[51,     1] loss: 717.745
[52,     1] loss: 742.060
[53,     1] loss: 889.479
[54,     1] loss: 1107.074
[55,     1] loss: 755.567
[56,     1] loss: 923.533
[57,     1] loss: 819.158
[58,     1] loss: 824.768
[59,     1] loss: 909.453
[60,     1] loss: 783.576
[61,     1] loss: 839.299
[62,     1] loss: 832.797
[63,     1] loss: 762.318
[64,     1] loss: 822.462
[65,     1] loss: 749.283
[66,     1] loss: 710.403
[67,     1] loss: 689.467
[68,     1] loss: 668.844
[69,     1] loss: 673.697
[70,     1] loss: 762.144
[71,     1] loss: 745.460
[72,     1] loss: 644.584
[73,     1] loss: 666.659
[74,     1] loss: 665.292
[75,     1] loss: 613.549
[76,     1] loss: 588.976
[77,     1] loss: 519.091
[78,     1] loss: 509.117
[79,     1] loss: 500.913
[80,     1] loss: 1143.603
[81,     1] loss: 2757.253
[82,     1] loss: 947.947
[83,     1] loss: 989.971
[84,     1] loss: 1205.355
[85,     1] loss: 1102.342
[86,     1] loss: 1156.506
[87,     1] loss: 1172.401
[88,     1] loss: 1154.183
[89,     1] loss: 1143.579
[90,     1] loss: 1129.663
[91,     1] loss: 1135.401
[92,     1] loss: 1122.052
[93,     1] loss: 1112.758
[94,     1] loss: 1109.439
[95,     1] loss: 1101.735
[96,     1] loss: 1099.255
[97,     1] loss: 1082.944
[98,     1] loss: 1096.423
[99,     1] loss: 1078.799
[100,     1] loss: 1046.008
[101,     1] loss: 1062.136
[102,     1] loss: 1060.391
[103,     1] loss: 1036.488
[104,     1] loss: 1046.977
[105,     1] loss: 1034.009
[106,     1] loss: 1031.471
[107,     1] loss: 987.651
[108,     1] loss: 965.506
[109,     1] loss: 929.643
[110,     1] loss: 986.656
[111,     1] loss: 988.605
[112,     1] loss: 927.518
[113,     1] loss: 909.403
[114,     1] loss: 926.728
[115,     1] loss: 917.337
[116,     1] loss: 877.185
[117,     1] loss: 840.470
[118,     1] loss: 888.542
[119,     1] loss: 885.528
[120,     1] loss: 860.510
[121,     1] loss: 888.138
[122,     1] loss: 948.645
[123,     1] loss: 947.727
Early stopping applied (best metric=0.33825087547302246)
Finished Training
Total time taken: 17.775275230407715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.668
[2,     1] loss: 1272.773
[3,     1] loss: 1252.363
[4,     1] loss: 1252.932
[5,     1] loss: 1252.506
[6,     1] loss: 1252.927
[7,     1] loss: 1251.481
[8,     1] loss: 1251.477
[9,     1] loss: 1249.028
[10,     1] loss: 1249.379
[11,     1] loss: 1250.318
[12,     1] loss: 1248.350
[13,     1] loss: 1247.792
[14,     1] loss: 1248.415
[15,     1] loss: 1249.833
[16,     1] loss: 1248.502
[17,     1] loss: 1246.482
[18,     1] loss: 1245.707
[19,     1] loss: 1242.225
[20,     1] loss: 1241.349
[21,     1] loss: 1229.607
[22,     1] loss: 1222.268
[23,     1] loss: 1196.465
[24,     1] loss: 1187.663
[25,     1] loss: 1151.587
[26,     1] loss: 1137.056
[27,     1] loss: 1143.995
[28,     1] loss: 1092.734
[29,     1] loss: 1070.396
[30,     1] loss: 1088.464
[31,     1] loss: 1044.531
[32,     1] loss: 1032.154
[33,     1] loss: 973.822
[34,     1] loss: 1049.250
[35,     1] loss: 1066.835
[36,     1] loss: 961.288
[37,     1] loss: 1044.340
[38,     1] loss: 972.934
[39,     1] loss: 959.346
[40,     1] loss: 938.214
[41,     1] loss: 943.387
[42,     1] loss: 926.090
[43,     1] loss: 891.964
[44,     1] loss: 880.935
[45,     1] loss: 887.306
[46,     1] loss: 890.300
[47,     1] loss: 885.341
[48,     1] loss: 992.630
[49,     1] loss: 879.760
[50,     1] loss: 883.285
[51,     1] loss: 856.844
[52,     1] loss: 854.038
[53,     1] loss: 822.067
[54,     1] loss: 857.651
[55,     1] loss: 827.717
[56,     1] loss: 743.227
[57,     1] loss: 918.693
[58,     1] loss: 1117.710
[59,     1] loss: 785.203
[60,     1] loss: 873.399
[61,     1] loss: 837.974
[62,     1] loss: 853.167
[63,     1] loss: 824.478
[64,     1] loss: 770.962
[65,     1] loss: 779.566
[66,     1] loss: 731.562
[67,     1] loss: 729.920
[68,     1] loss: 712.225
[69,     1] loss: 663.589
[70,     1] loss: 689.163
[71,     1] loss: 665.480
[72,     1] loss: 701.020
[73,     1] loss: 996.161
[74,     1] loss: 1292.633
[75,     1] loss: 774.187
[76,     1] loss: 972.983
[77,     1] loss: 912.143
[78,     1] loss: 889.320
[79,     1] loss: 847.894
[80,     1] loss: 893.442
[81,     1] loss: 844.951
[82,     1] loss: 829.295
[83,     1] loss: 796.158
[84,     1] loss: 817.748
[85,     1] loss: 815.898
[86,     1] loss: 769.279
[87,     1] loss: 788.073
[88,     1] loss: 740.737
[89,     1] loss: 735.121
[90,     1] loss: 713.091
[91,     1] loss: 703.419
[92,     1] loss: 720.902
[93,     1] loss: 843.258
[94,     1] loss: 852.416
[95,     1] loss: 683.491
[96,     1] loss: 773.645
[97,     1] loss: 684.277
[98,     1] loss: 700.410
[99,     1] loss: 719.370
[100,     1] loss: 652.979
[101,     1] loss: 652.474
[102,     1] loss: 690.000
[103,     1] loss: 644.597
[104,     1] loss: 647.329
[105,     1] loss: 644.470
[106,     1] loss: 580.646
[107,     1] loss: 616.121
[108,     1] loss: 582.249
[109,     1] loss: 669.398
[110,     1] loss: 1278.097
[111,     1] loss: 1064.749
[112,     1] loss: 856.592
[113,     1] loss: 914.230
[114,     1] loss: 905.160
[115,     1] loss: 764.343
[116,     1] loss: 806.867
[117,     1] loss: 713.343
[118,     1] loss: 943.031
[119,     1] loss: 712.113
[120,     1] loss: 849.914
[121,     1] loss: 703.320
[122,     1] loss: 731.547
[123,     1] loss: 702.380
[124,     1] loss: 707.212
[125,     1] loss: 677.355
[126,     1] loss: 640.948
[127,     1] loss: 621.765
[128,     1] loss: 656.845
[129,     1] loss: 583.907
[130,     1] loss: 578.193
[131,     1] loss: 640.661
[132,     1] loss: 777.417
[133,     1] loss: 596.206
[134,     1] loss: 576.047
[135,     1] loss: 568.585
[136,     1] loss: 561.899
[137,     1] loss: 717.068
[138,     1] loss: 1097.473
[139,     1] loss: 1534.653
[140,     1] loss: 1035.328
[141,     1] loss: 1046.266
[142,     1] loss: 1073.298
[143,     1] loss: 1084.629
[144,     1] loss: 1123.624
[145,     1] loss: 1110.064
[146,     1] loss: 1113.790
Early stopping applied (best metric=0.31618624925613403)
Finished Training
Total time taken: 22.209213972091675
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.369
[2,     1] loss: 1253.677
[3,     1] loss: 1251.184
[4,     1] loss: 1250.848
[5,     1] loss: 1251.577
[6,     1] loss: 1250.221
[7,     1] loss: 1250.456
[8,     1] loss: 1249.645
[9,     1] loss: 1247.202
[10,     1] loss: 1249.898
[11,     1] loss: 1248.105
[12,     1] loss: 1245.499
[13,     1] loss: 1243.079
[14,     1] loss: 1237.113
[15,     1] loss: 1222.047
[16,     1] loss: 1198.928
[17,     1] loss: 1180.866
[18,     1] loss: 1167.776
[19,     1] loss: 1105.895
[20,     1] loss: 1089.047
[21,     1] loss: 1088.372
[22,     1] loss: 1042.137
[23,     1] loss: 1061.066
[24,     1] loss: 1017.988
[25,     1] loss: 992.964
[26,     1] loss: 1051.004
[27,     1] loss: 968.174
[28,     1] loss: 981.151
[29,     1] loss: 979.156
[30,     1] loss: 951.806
[31,     1] loss: 960.691
[32,     1] loss: 933.036
[33,     1] loss: 918.862
[34,     1] loss: 891.031
[35,     1] loss: 852.284
[36,     1] loss: 914.965
[37,     1] loss: 881.525
[38,     1] loss: 849.882
[39,     1] loss: 894.241
[40,     1] loss: 823.956
[41,     1] loss: 914.202
[42,     1] loss: 919.059
[43,     1] loss: 840.187
[44,     1] loss: 832.793
[45,     1] loss: 866.582
[46,     1] loss: 752.135
[47,     1] loss: 844.593
[48,     1] loss: 827.276
[49,     1] loss: 731.055
[50,     1] loss: 703.639
[51,     1] loss: 867.052
[52,     1] loss: 1519.996
[53,     1] loss: 948.075
[54,     1] loss: 1113.302
[55,     1] loss: 993.933
[56,     1] loss: 997.818
[57,     1] loss: 1011.810
[58,     1] loss: 1010.246
[59,     1] loss: 1027.266
[60,     1] loss: 960.035
[61,     1] loss: 950.544
[62,     1] loss: 938.206
[63,     1] loss: 938.181
[64,     1] loss: 948.453
[65,     1] loss: 932.604
[66,     1] loss: 901.853
[67,     1] loss: 925.857
[68,     1] loss: 901.485
[69,     1] loss: 868.098
[70,     1] loss: 861.509
[71,     1] loss: 804.664
[72,     1] loss: 860.402
[73,     1] loss: 868.244
[74,     1] loss: 832.643
[75,     1] loss: 809.637
[76,     1] loss: 855.827
[77,     1] loss: 786.353
[78,     1] loss: 749.919
[79,     1] loss: 743.094
[80,     1] loss: 753.833
[81,     1] loss: 696.599
[82,     1] loss: 707.487
[83,     1] loss: 719.614
[84,     1] loss: 709.283
[85,     1] loss: 650.155
[86,     1] loss: 676.788
Early stopping applied (best metric=0.4214937090873718)
Finished Training
Total time taken: 12.211054563522339
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1256.954
[2,     1] loss: 1253.583
[3,     1] loss: 1253.778
[4,     1] loss: 1247.683
[5,     1] loss: 1252.231
[6,     1] loss: 1245.854
[7,     1] loss: 1242.325
[8,     1] loss: 1223.760
[9,     1] loss: 1208.283
[10,     1] loss: 1171.497
[11,     1] loss: 1108.887
[12,     1] loss: 1105.417
[13,     1] loss: 1110.695
[14,     1] loss: 1034.261
[15,     1] loss: 1097.447
[16,     1] loss: 1019.267
[17,     1] loss: 1098.739
[18,     1] loss: 1042.172
[19,     1] loss: 1038.663
[20,     1] loss: 1048.434
[21,     1] loss: 1032.243
[22,     1] loss: 1007.212
[23,     1] loss: 947.380
[24,     1] loss: 987.269
[25,     1] loss: 950.139
[26,     1] loss: 967.210
[27,     1] loss: 938.943
[28,     1] loss: 928.072
[29,     1] loss: 937.909
[30,     1] loss: 970.682
[31,     1] loss: 886.387
[32,     1] loss: 942.094
[33,     1] loss: 883.136
[34,     1] loss: 852.539
[35,     1] loss: 861.783
[36,     1] loss: 859.200
[37,     1] loss: 867.047
[38,     1] loss: 814.523
[39,     1] loss: 808.547
[40,     1] loss: 826.920
[41,     1] loss: 945.136
[42,     1] loss: 1265.350
[43,     1] loss: 822.190
[44,     1] loss: 1011.242
[45,     1] loss: 1000.262
[46,     1] loss: 942.282
[47,     1] loss: 947.167
[48,     1] loss: 975.997
[49,     1] loss: 906.963
[50,     1] loss: 883.941
[51,     1] loss: 927.539
[52,     1] loss: 861.389
[53,     1] loss: 838.025
[54,     1] loss: 881.503
[55,     1] loss: 872.903
[56,     1] loss: 808.279
[57,     1] loss: 858.256
[58,     1] loss: 781.083
[59,     1] loss: 812.294
[60,     1] loss: 763.304
[61,     1] loss: 760.881
[62,     1] loss: 717.681
[63,     1] loss: 732.159
[64,     1] loss: 835.884
[65,     1] loss: 842.824
[66,     1] loss: 685.461
[67,     1] loss: 812.480
[68,     1] loss: 718.954
[69,     1] loss: 765.375
[70,     1] loss: 643.557
[71,     1] loss: 769.215
[72,     1] loss: 687.332
[73,     1] loss: 711.447
[74,     1] loss: 939.272
[75,     1] loss: 731.185
[76,     1] loss: 781.523
Early stopping applied (best metric=0.403136283159256)
Finished Training
Total time taken: 10.96417236328125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.877
[2,     1] loss: 1257.939
[3,     1] loss: 1251.473
[4,     1] loss: 1250.235
[5,     1] loss: 1249.858
[6,     1] loss: 1249.748
[7,     1] loss: 1251.052
[8,     1] loss: 1249.660
[9,     1] loss: 1248.671
[10,     1] loss: 1250.177
[11,     1] loss: 1248.805
[12,     1] loss: 1247.724
[13,     1] loss: 1249.906
[14,     1] loss: 1250.263
[15,     1] loss: 1246.632
[16,     1] loss: 1245.443
[17,     1] loss: 1244.754
[18,     1] loss: 1238.385
[19,     1] loss: 1227.863
[20,     1] loss: 1213.484
[21,     1] loss: 1182.455
[22,     1] loss: 1165.256
[23,     1] loss: 1126.259
[24,     1] loss: 1096.000
[25,     1] loss: 1038.745
[26,     1] loss: 1055.135
[27,     1] loss: 1089.847
[28,     1] loss: 1063.591
[29,     1] loss: 1032.321
[30,     1] loss: 1021.757
[31,     1] loss: 1016.073
[32,     1] loss: 1000.063
[33,     1] loss: 989.246
[34,     1] loss: 1006.327
[35,     1] loss: 1000.287
[36,     1] loss: 956.863
[37,     1] loss: 924.521
[38,     1] loss: 935.094
[39,     1] loss: 967.493
[40,     1] loss: 935.309
[41,     1] loss: 897.492
[42,     1] loss: 888.619
[43,     1] loss: 904.940
[44,     1] loss: 931.581
[45,     1] loss: 877.946
[46,     1] loss: 835.295
[47,     1] loss: 975.228
[48,     1] loss: 854.241
[49,     1] loss: 874.775
[50,     1] loss: 867.053
[51,     1] loss: 892.984
[52,     1] loss: 812.662
[53,     1] loss: 868.029
[54,     1] loss: 775.182
[55,     1] loss: 867.227
[56,     1] loss: 892.556
[57,     1] loss: 861.210
[58,     1] loss: 860.661
[59,     1] loss: 747.419
[60,     1] loss: 797.352
[61,     1] loss: 724.721
[62,     1] loss: 779.202
[63,     1] loss: 688.497
[64,     1] loss: 679.995
[65,     1] loss: 677.216
[66,     1] loss: 692.854
[67,     1] loss: 710.118
[68,     1] loss: 837.180
[69,     1] loss: 741.043
[70,     1] loss: 663.021
[71,     1] loss: 680.814
[72,     1] loss: 625.160
[73,     1] loss: 715.478
[74,     1] loss: 832.375
[75,     1] loss: 734.099
[76,     1] loss: 629.151
[77,     1] loss: 621.324
[78,     1] loss: 663.253
[79,     1] loss: 583.598
[80,     1] loss: 565.853
[81,     1] loss: 584.776
[82,     1] loss: 763.269
[83,     1] loss: 933.445
[84,     1] loss: 1023.956
[85,     1] loss: 775.837
[86,     1] loss: 812.920
[87,     1] loss: 915.454
[88,     1] loss: 790.359
[89,     1] loss: 800.148
[90,     1] loss: 838.425
[91,     1] loss: 702.869
[92,     1] loss: 781.693
[93,     1] loss: 649.664
[94,     1] loss: 740.195
[95,     1] loss: 641.803
[96,     1] loss: 659.284
[97,     1] loss: 598.882
[98,     1] loss: 568.904
[99,     1] loss: 665.871
[100,     1] loss: 668.040
[101,     1] loss: 467.795
[102,     1] loss: 813.819
[103,     1] loss: 966.734
[104,     1] loss: 641.660
[105,     1] loss: 763.561
[106,     1] loss: 707.294
[107,     1] loss: 603.323
[108,     1] loss: 661.874
[109,     1] loss: 605.411
[110,     1] loss: 502.918
[111,     1] loss: 688.363
[112,     1] loss: 646.040
[113,     1] loss: 624.612
[114,     1] loss: 469.481
[115,     1] loss: 573.432
[116,     1] loss: 542.601
[117,     1] loss: 470.873
[118,     1] loss: 675.591
[119,     1] loss: 1094.474
[120,     1] loss: 581.742
[121,     1] loss: 949.120
[122,     1] loss: 697.560
[123,     1] loss: 798.233
[124,     1] loss: 650.899
[125,     1] loss: 685.743
[126,     1] loss: 528.198
[127,     1] loss: 651.529
[128,     1] loss: 656.237
[129,     1] loss: 556.441
[130,     1] loss: 549.585
[131,     1] loss: 520.950
[132,     1] loss: 464.172
[133,     1] loss: 469.060
[134,     1] loss: 445.033
[135,     1] loss: 444.491
[136,     1] loss: 581.593
[137,     1] loss: 1662.895
[138,     1] loss: 561.789
[139,     1] loss: 851.664
[140,     1] loss: 796.955
Early stopping applied (best metric=0.3636188507080078)
Finished Training
Total time taken: 20.11779022216797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.422
[2,     1] loss: 1252.250
[3,     1] loss: 1248.701
[4,     1] loss: 1248.354
[5,     1] loss: 1246.903
[6,     1] loss: 1245.852
[7,     1] loss: 1242.485
[8,     1] loss: 1236.479
[9,     1] loss: 1222.597
[10,     1] loss: 1201.234
[11,     1] loss: 1168.915
[12,     1] loss: 1116.582
[13,     1] loss: 1114.066
[14,     1] loss: 1087.863
[15,     1] loss: 1035.042
[16,     1] loss: 1022.482
[17,     1] loss: 1028.534
[18,     1] loss: 1023.067
[19,     1] loss: 975.689
[20,     1] loss: 965.158
[21,     1] loss: 991.849
[22,     1] loss: 988.016
[23,     1] loss: 956.992
[24,     1] loss: 955.036
[25,     1] loss: 978.475
[26,     1] loss: 929.833
[27,     1] loss: 964.841
[28,     1] loss: 1046.479
[29,     1] loss: 931.425
[30,     1] loss: 1008.725
[31,     1] loss: 925.332
[32,     1] loss: 964.825
[33,     1] loss: 889.592
[34,     1] loss: 920.633
[35,     1] loss: 904.997
[36,     1] loss: 902.067
[37,     1] loss: 901.156
[38,     1] loss: 911.903
[39,     1] loss: 843.690
[40,     1] loss: 847.222
[41,     1] loss: 836.599
[42,     1] loss: 935.607
[43,     1] loss: 830.267
[44,     1] loss: 815.951
[45,     1] loss: 817.764
[46,     1] loss: 864.383
[47,     1] loss: 912.276
[48,     1] loss: 810.288
[49,     1] loss: 847.761
[50,     1] loss: 808.588
[51,     1] loss: 835.791
[52,     1] loss: 800.201
[53,     1] loss: 789.548
[54,     1] loss: 770.462
[55,     1] loss: 788.098
[56,     1] loss: 695.110
[57,     1] loss: 804.777
[58,     1] loss: 864.255
[59,     1] loss: 781.653
[60,     1] loss: 722.841
[61,     1] loss: 727.110
[62,     1] loss: 723.261
[63,     1] loss: 701.696
[64,     1] loss: 733.011
[65,     1] loss: 696.248
[66,     1] loss: 615.620
[67,     1] loss: 783.981
[68,     1] loss: 1315.484
[69,     1] loss: 658.474
[70,     1] loss: 897.707
[71,     1] loss: 827.285
[72,     1] loss: 839.775
[73,     1] loss: 942.117
[74,     1] loss: 826.786
[75,     1] loss: 893.393
[76,     1] loss: 869.284
[77,     1] loss: 789.042
[78,     1] loss: 821.107
[79,     1] loss: 751.545
[80,     1] loss: 798.215
[81,     1] loss: 698.177
[82,     1] loss: 740.147
[83,     1] loss: 721.223
[84,     1] loss: 702.923
[85,     1] loss: 740.763
[86,     1] loss: 646.257
[87,     1] loss: 696.105
[88,     1] loss: 772.097
[89,     1] loss: 783.146
[90,     1] loss: 710.347
[91,     1] loss: 739.791
[92,     1] loss: 693.869
[93,     1] loss: 758.865
[94,     1] loss: 660.128
[95,     1] loss: 732.405
[96,     1] loss: 668.935
[97,     1] loss: 605.182
[98,     1] loss: 644.127
[99,     1] loss: 528.131
[100,     1] loss: 592.456
[101,     1] loss: 853.067
[102,     1] loss: 656.477
[103,     1] loss: 575.337
[104,     1] loss: 608.438
[105,     1] loss: 583.055
[106,     1] loss: 583.976
[107,     1] loss: 868.749
[108,     1] loss: 547.454
[109,     1] loss: 987.690
[110,     1] loss: 1344.617
[111,     1] loss: 1261.385
[112,     1] loss: 1078.616
[113,     1] loss: 1151.206
[114,     1] loss: 1186.320
[115,     1] loss: 1175.277
[116,     1] loss: 1146.996
[117,     1] loss: 1132.378
[118,     1] loss: 1087.639
[119,     1] loss: 1055.737
[120,     1] loss: 1015.285
[121,     1] loss: 987.641
[122,     1] loss: 1069.402
[123,     1] loss: 1061.424
[124,     1] loss: 1016.139
[125,     1] loss: 957.788
[126,     1] loss: 898.993
[127,     1] loss: 915.399
[128,     1] loss: 862.635
[129,     1] loss: 865.961
[130,     1] loss: 815.667
[131,     1] loss: 810.214
[132,     1] loss: 787.629
[133,     1] loss: 757.869
[134,     1] loss: 819.292
[135,     1] loss: 914.766
[136,     1] loss: 783.367
[137,     1] loss: 828.211
[138,     1] loss: 780.905
[139,     1] loss: 779.952
[140,     1] loss: 877.691
[141,     1] loss: 802.023
[142,     1] loss: 814.288
[143,     1] loss: 727.924
[144,     1] loss: 741.280
[145,     1] loss: 781.205
[146,     1] loss: 677.511
[147,     1] loss: 654.713
[148,     1] loss: 733.565
[149,     1] loss: 796.444
[150,     1] loss: 1047.982
[151,     1] loss: 827.731
[152,     1] loss: 947.338
[153,     1] loss: 823.153
[154,     1] loss: 730.634
[155,     1] loss: 729.011
[156,     1] loss: 738.927
[157,     1] loss: 677.883
[158,     1] loss: 636.791
[159,     1] loss: 801.877
[160,     1] loss: 888.637
[161,     1] loss: 759.970
[162,     1] loss: 762.123
[163,     1] loss: 761.852
[164,     1] loss: 746.297
[165,     1] loss: 756.039
[166,     1] loss: 651.383
[167,     1] loss: 797.909
[168,     1] loss: 783.068
[169,     1] loss: 787.975
[170,     1] loss: 647.800
[171,     1] loss: 686.987
[172,     1] loss: 730.698
[173,     1] loss: 711.415
[174,     1] loss: 667.956
[175,     1] loss: 655.424
[176,     1] loss: 591.031
[177,     1] loss: 646.164
[178,     1] loss: 580.194
[179,     1] loss: 573.075
[180,     1] loss: 635.802
[181,     1] loss: 581.829
[182,     1] loss: 601.478
[183,     1] loss: 583.358
[184,     1] loss: 531.660
[185,     1] loss: 715.085
[186,     1] loss: 1219.708
[187,     1] loss: 1604.654
[188,     1] loss: 1458.537
Early stopping applied (best metric=0.3695977032184601)
Finished Training
Total time taken: 27.2430100440979
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.994
[2,     1] loss: 1247.842
[3,     1] loss: 1249.045
[4,     1] loss: 1244.623
[5,     1] loss: 1247.371
[6,     1] loss: 1241.057
[7,     1] loss: 1225.650
[8,     1] loss: 1200.023
[9,     1] loss: 1158.079
[10,     1] loss: 1122.484
[11,     1] loss: 1123.599
[12,     1] loss: 1064.490
[13,     1] loss: 1104.411
[14,     1] loss: 1050.298
[15,     1] loss: 1081.945
[16,     1] loss: 1003.086
[17,     1] loss: 1074.920
[18,     1] loss: 1031.878
[19,     1] loss: 1048.814
[20,     1] loss: 1028.182
[21,     1] loss: 989.879
[22,     1] loss: 985.648
[23,     1] loss: 966.777
[24,     1] loss: 966.966
[25,     1] loss: 928.603
[26,     1] loss: 922.881
[27,     1] loss: 994.425
[28,     1] loss: 931.326
[29,     1] loss: 882.592
[30,     1] loss: 908.652
[31,     1] loss: 852.753
[32,     1] loss: 1014.012
[33,     1] loss: 870.308
[34,     1] loss: 867.721
[35,     1] loss: 853.326
[36,     1] loss: 857.353
[37,     1] loss: 811.873
[38,     1] loss: 800.052
[39,     1] loss: 782.083
[40,     1] loss: 866.074
[41,     1] loss: 996.878
[42,     1] loss: 918.137
[43,     1] loss: 795.614
[44,     1] loss: 846.952
[45,     1] loss: 873.232
[46,     1] loss: 824.142
[47,     1] loss: 809.571
[48,     1] loss: 811.448
[49,     1] loss: 798.399
[50,     1] loss: 724.171
[51,     1] loss: 790.781
[52,     1] loss: 789.307
[53,     1] loss: 673.076
[54,     1] loss: 722.918
[55,     1] loss: 1081.849
[56,     1] loss: 721.813
[57,     1] loss: 953.432
[58,     1] loss: 790.227
[59,     1] loss: 831.834
[60,     1] loss: 832.492
[61,     1] loss: 774.893
[62,     1] loss: 725.094
[63,     1] loss: 794.569
[64,     1] loss: 739.990
[65,     1] loss: 756.095
[66,     1] loss: 647.113
[67,     1] loss: 650.950
[68,     1] loss: 736.083
[69,     1] loss: 608.116
[70,     1] loss: 742.095
[71,     1] loss: 782.082
[72,     1] loss: 659.385
[73,     1] loss: 658.064
[74,     1] loss: 625.767
[75,     1] loss: 672.629
[76,     1] loss: 569.293
[77,     1] loss: 657.676
[78,     1] loss: 669.527
[79,     1] loss: 517.692
[80,     1] loss: 774.589
[81,     1] loss: 1545.661
[82,     1] loss: 848.010
[83,     1] loss: 765.192
[84,     1] loss: 1071.147
[85,     1] loss: 885.220
[86,     1] loss: 941.183
[87,     1] loss: 921.925
[88,     1] loss: 871.129
[89,     1] loss: 819.590
[90,     1] loss: 834.211
[91,     1] loss: 815.942
[92,     1] loss: 808.024
[93,     1] loss: 788.464
[94,     1] loss: 751.521
[95,     1] loss: 734.604
[96,     1] loss: 694.126
[97,     1] loss: 701.476
[98,     1] loss: 701.245
[99,     1] loss: 654.466
[100,     1] loss: 588.690
[101,     1] loss: 620.713
[102,     1] loss: 919.550
[103,     1] loss: 656.782
[104,     1] loss: 586.664
[105,     1] loss: 607.006
[106,     1] loss: 567.191
[107,     1] loss: 586.849
[108,     1] loss: 511.848
[109,     1] loss: 598.703
[110,     1] loss: 1089.194
[111,     1] loss: 1826.767
[112,     1] loss: 1326.968
[113,     1] loss: 1161.302
[114,     1] loss: 1230.729
Early stopping applied (best metric=0.3494899570941925)
Finished Training
Total time taken: 16.628183841705322
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.021
[2,     1] loss: 1246.107
[3,     1] loss: 1250.635
[4,     1] loss: 1251.798
[5,     1] loss: 1246.082
[6,     1] loss: 1249.280
[7,     1] loss: 1238.655
[8,     1] loss: 1230.445
[9,     1] loss: 1195.326
[10,     1] loss: 1169.623
[11,     1] loss: 1129.892
[12,     1] loss: 1132.302
[13,     1] loss: 1071.608
[14,     1] loss: 1065.836
[15,     1] loss: 1060.637
[16,     1] loss: 1033.236
[17,     1] loss: 1039.813
[18,     1] loss: 1023.328
[19,     1] loss: 1006.528
[20,     1] loss: 991.771
[21,     1] loss: 974.621
[22,     1] loss: 994.275
[23,     1] loss: 920.171
[24,     1] loss: 904.544
[25,     1] loss: 902.635
[26,     1] loss: 909.703
[27,     1] loss: 896.544
[28,     1] loss: 918.356
[29,     1] loss: 904.715
[30,     1] loss: 998.505
[31,     1] loss: 933.270
[32,     1] loss: 888.548
[33,     1] loss: 891.268
[34,     1] loss: 860.076
[35,     1] loss: 870.880
[36,     1] loss: 907.823
[37,     1] loss: 961.457
[38,     1] loss: 805.617
[39,     1] loss: 811.036
[40,     1] loss: 876.638
[41,     1] loss: 781.573
[42,     1] loss: 928.786
[43,     1] loss: 846.296
[44,     1] loss: 806.756
[45,     1] loss: 810.649
[46,     1] loss: 802.575
[47,     1] loss: 759.335
[48,     1] loss: 770.030
[49,     1] loss: 769.131
[50,     1] loss: 728.898
[51,     1] loss: 795.500
[52,     1] loss: 786.977
[53,     1] loss: 700.642
[54,     1] loss: 717.150
[55,     1] loss: 681.420
[56,     1] loss: 703.397
[57,     1] loss: 651.869
[58,     1] loss: 631.516
[59,     1] loss: 599.320
[60,     1] loss: 941.339
[61,     1] loss: 2072.216
[62,     1] loss: 740.503
[63,     1] loss: 995.650
[64,     1] loss: 1046.187
[65,     1] loss: 1016.080
[66,     1] loss: 1019.543
[67,     1] loss: 1027.260
[68,     1] loss: 1010.946
[69,     1] loss: 972.242
[70,     1] loss: 937.097
[71,     1] loss: 969.376
[72,     1] loss: 942.721
[73,     1] loss: 949.329
[74,     1] loss: 908.050
[75,     1] loss: 905.389
[76,     1] loss: 860.433
[77,     1] loss: 999.808
[78,     1] loss: 904.424
[79,     1] loss: 932.999
[80,     1] loss: 894.337
[81,     1] loss: 922.559
[82,     1] loss: 906.793
[83,     1] loss: 874.638
[84,     1] loss: 904.398
[85,     1] loss: 896.507
[86,     1] loss: 840.132
[87,     1] loss: 907.804
[88,     1] loss: 846.361
[89,     1] loss: 831.461
[90,     1] loss: 828.139
[91,     1] loss: 839.241
[92,     1] loss: 758.094
[93,     1] loss: 794.064
[94,     1] loss: 744.932
[95,     1] loss: 717.171
[96,     1] loss: 742.566
[97,     1] loss: 814.375
[98,     1] loss: 1039.116
[99,     1] loss: 815.557
Early stopping applied (best metric=0.2800416648387909)
Finished Training
Total time taken: 14.7136390209198
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1255.950
[2,     1] loss: 1256.377
[3,     1] loss: 1251.767
[4,     1] loss: 1251.501
[5,     1] loss: 1253.878
[6,     1] loss: 1249.118
[7,     1] loss: 1249.761
[8,     1] loss: 1246.259
[9,     1] loss: 1248.120
[10,     1] loss: 1236.902
[11,     1] loss: 1231.016
[12,     1] loss: 1210.511
[13,     1] loss: 1197.334
[14,     1] loss: 1162.979
[15,     1] loss: 1123.556
[16,     1] loss: 1101.492
[17,     1] loss: 1049.320
[18,     1] loss: 1052.021
[19,     1] loss: 1024.079
[20,     1] loss: 1038.102
[21,     1] loss: 1017.674
[22,     1] loss: 985.405
[23,     1] loss: 957.347
[24,     1] loss: 970.297
[25,     1] loss: 1000.827
[26,     1] loss: 961.015
[27,     1] loss: 971.470
[28,     1] loss: 957.830
[29,     1] loss: 979.587
[30,     1] loss: 934.924
[31,     1] loss: 932.638
[32,     1] loss: 913.798
[33,     1] loss: 885.397
[34,     1] loss: 911.660
[35,     1] loss: 838.381
[36,     1] loss: 883.535
[37,     1] loss: 871.373
[38,     1] loss: 812.397
[39,     1] loss: 867.475
[40,     1] loss: 919.558
[41,     1] loss: 835.118
[42,     1] loss: 839.704
[43,     1] loss: 896.532
[44,     1] loss: 838.959
[45,     1] loss: 773.388
[46,     1] loss: 787.201
[47,     1] loss: 768.628
[48,     1] loss: 769.147
[49,     1] loss: 814.330
[50,     1] loss: 1260.738
[51,     1] loss: 999.420
[52,     1] loss: 855.633
[53,     1] loss: 823.349
[54,     1] loss: 905.108
[55,     1] loss: 934.871
[56,     1] loss: 892.579
[57,     1] loss: 903.720
[58,     1] loss: 921.282
[59,     1] loss: 849.426
[60,     1] loss: 833.632
[61,     1] loss: 902.026
[62,     1] loss: 874.344
[63,     1] loss: 794.355
[64,     1] loss: 889.408
[65,     1] loss: 827.677
[66,     1] loss: 784.363
[67,     1] loss: 745.576
[68,     1] loss: 800.583
[69,     1] loss: 758.304
[70,     1] loss: 730.671
[71,     1] loss: 704.736
[72,     1] loss: 659.760
[73,     1] loss: 705.305
[74,     1] loss: 769.332
[75,     1] loss: 885.834
[76,     1] loss: 736.295
[77,     1] loss: 662.519
[78,     1] loss: 749.622
[79,     1] loss: 668.227
[80,     1] loss: 713.000
[81,     1] loss: 680.654
[82,     1] loss: 648.856
[83,     1] loss: 647.335
[84,     1] loss: 1020.188
[85,     1] loss: 1466.676
[86,     1] loss: 815.386
[87,     1] loss: 929.111
[88,     1] loss: 1027.047
[89,     1] loss: 981.089
[90,     1] loss: 950.849
[91,     1] loss: 931.397
[92,     1] loss: 957.534
[93,     1] loss: 902.054
[94,     1] loss: 881.004
[95,     1] loss: 911.822
[96,     1] loss: 873.579
[97,     1] loss: 878.990
[98,     1] loss: 846.643
Early stopping applied (best metric=0.39251548051834106)
Finished Training
Total time taken: 14.660195112228394
{'Hydroxylation-K Validation Accuracy': 0.7029255319148936, 'Hydroxylation-K Validation Sensitivity': 0.5696296296296296, 'Hydroxylation-K Validation Specificity': 0.7368421052631579, 'Hydroxylation-K Validation Precision': 0.35176427431071394, 'Hydroxylation-K AUC ROC': 0.7639766081871345, 'Hydroxylation-K AUC PR': 0.5372338976307711, 'Hydroxylation-K MCC': 0.26299995059767317, 'Hydroxylation-K F1': 0.4263777893705884, 'Validation Loss (Hydroxylation-K)': 0.5060210704803467, 'Hydroxylation-P Validation Accuracy': 0.7815051689423549, 'Hydroxylation-P Validation Sensitivity': 0.8371957671957672, 'Hydroxylation-P Validation Specificity': 0.7696194323906429, 'Hydroxylation-P Validation Precision': 0.4465704687090932, 'Hydroxylation-P AUC ROC': 0.8663031069910465, 'Hydroxylation-P AUC PR': 0.5912109755005138, 'Hydroxylation-P MCC': 0.4941656612464308, 'Hydroxylation-P F1': 0.579713640763927, 'Validation Loss (Hydroxylation-P)': 0.35232958594957986, 'Validation Loss (total)': 0.858350662390391, 'TimeToTrain': 16.187491512298585}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004075957236657239,
 'learning_rate_Hydroxylation-K': 0.0021708061222838653,
 'learning_rate_Hydroxylation-P': 0.00902305854256916,
 'log_base': 1.185382213921747,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3171689765,
 'sample_weights': [1.6224092047640803, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.809355637541435,
 'weight_decay_Hydroxylation-K': 0.6230881299284476,
 'weight_decay_Hydroxylation-P': 2.003532357201541}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3198.172
[2,     1] loss: 3176.459
[3,     1] loss: 3186.894
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019055930550348932,
 'learning_rate_Hydroxylation-K': 0.0075553427012731616,
 'learning_rate_Hydroxylation-P': 0.0034034603049330554,
 'log_base': 1.7114857912985646,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3414575136,
 'sample_weights': [9.816485087958721, 1.2271079072979592],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6802237193253446,
 'weight_decay_Hydroxylation-K': 8.485739858537503,
 'weight_decay_Hydroxylation-P': 6.758140855882818}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1572.959
[2,     1] loss: 1562.458
[3,     1] loss: 1567.051
[4,     1] loss: 1560.484
[5,     1] loss: 1562.936
[6,     1] loss: 1560.821
[7,     1] loss: 1557.704
[8,     1] loss: 1560.498
[9,     1] loss: 1555.261
[10,     1] loss: 1554.075
[11,     1] loss: 1553.130
[12,     1] loss: 1550.578
[13,     1] loss: 1544.720
[14,     1] loss: 1539.364
[15,     1] loss: 1525.016
[16,     1] loss: 1500.953
[17,     1] loss: 1490.340
[18,     1] loss: 1447.411
[19,     1] loss: 1436.816
[20,     1] loss: 1394.859
[21,     1] loss: 1385.658
[22,     1] loss: 1342.892
[23,     1] loss: 1354.570
[24,     1] loss: 1309.910
[25,     1] loss: 1354.642
[26,     1] loss: 1294.155
[27,     1] loss: 1288.445
[28,     1] loss: 1344.545
[29,     1] loss: 1289.374
[30,     1] loss: 1303.876
[31,     1] loss: 1301.198
[32,     1] loss: 1273.876
[33,     1] loss: 1258.419
[34,     1] loss: 1242.865
[35,     1] loss: 1292.947
[36,     1] loss: 1264.906
[37,     1] loss: 1235.798
[38,     1] loss: 1237.476
[39,     1] loss: 1229.280
[40,     1] loss: 1154.136
[41,     1] loss: 1160.429
[42,     1] loss: 1189.354
[43,     1] loss: 1117.147
[44,     1] loss: 1148.802
[45,     1] loss: 1168.648
[46,     1] loss: 1130.049
[47,     1] loss: 1116.959
[48,     1] loss: 1123.152
[49,     1] loss: 1157.949
[50,     1] loss: 1127.389
[51,     1] loss: 1050.488
[52,     1] loss: 1067.895
[53,     1] loss: 1097.208
[54,     1] loss: 1093.369
[55,     1] loss: 1057.684
[56,     1] loss: 1080.084
[57,     1] loss: 1002.887
[58,     1] loss: 1013.181
[59,     1] loss: 952.211
[60,     1] loss: 978.785
[61,     1] loss: 920.178
[62,     1] loss: 993.652
[63,     1] loss: 969.280
[64,     1] loss: 889.009
[65,     1] loss: 957.173
[66,     1] loss: 856.469
[67,     1] loss: 921.806
[68,     1] loss: 896.888
[69,     1] loss: 944.421
[70,     1] loss: 852.025
[71,     1] loss: 817.852
[72,     1] loss: 842.607
[73,     1] loss: 839.028
[74,     1] loss: 868.361
[75,     1] loss: 884.787
[76,     1] loss: 862.609
[77,     1] loss: 854.880
[78,     1] loss: 810.838
[79,     1] loss: 858.530
[80,     1] loss: 801.942
[81,     1] loss: 864.580
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0067086169336296134,
 'learning_rate_Hydroxylation-K': 0.009543986465049968,
 'learning_rate_Hydroxylation-P': 0.008064973795228319,
 'log_base': 2.985954649381007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1574705791,
 'sample_weights': [3.106739087217138, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.840620619709142,
 'weight_decay_Hydroxylation-K': 5.296159526762653,
 'weight_decay_Hydroxylation-P': 4.171143194083374}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.376
[2,     1] loss: 1233.438
[3,     1] loss: 1229.609
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030213911785885775,
 'learning_rate_Hydroxylation-K': 0.008196798588730696,
 'learning_rate_Hydroxylation-P': 0.007776918977991551,
 'log_base': 2.7677667745034196,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3016674786,
 'sample_weights': [1.5261115011215156, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.061269976309969,
 'weight_decay_Hydroxylation-K': 7.671036898325236,
 'weight_decay_Hydroxylation-P': 1.1995769713948792}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.002
[2,     1] loss: 1252.470
[3,     1] loss: 1255.055
[4,     1] loss: 1254.595
[5,     1] loss: 1252.904
[6,     1] loss: 1246.992
[7,     1] loss: 1244.539
[8,     1] loss: 1240.306
[9,     1] loss: 1209.392
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030893393746341483,
 'learning_rate_Hydroxylation-K': 0.009853468809306452,
 'learning_rate_Hydroxylation-P': 0.0032509631865145125,
 'log_base': 1.5143015716519506,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2423432778,
 'sample_weights': [1.6398588210410394, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.36358796196287363,
 'weight_decay_Hydroxylation-K': 7.419238012013322,
 'weight_decay_Hydroxylation-P': 3.7893803154285304}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1754.805
[2,     1] loss: 1762.507
[3,     1] loss: 1752.514
[4,     1] loss: 1761.006
[5,     1] loss: 1759.620
[6,     1] loss: 1754.304
[7,     1] loss: 1754.196
[8,     1] loss: 1748.093
[9,     1] loss: 1753.726
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00876809391669368,
 'learning_rate_Hydroxylation-K': 0.00800132570008429,
 'learning_rate_Hydroxylation-P': 0.006851248643444898,
 'log_base': 2.542803749080126,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 392021537,
 'sample_weights': [4.02319737729265, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.874415369498972,
 'weight_decay_Hydroxylation-K': 5.513002227152511,
 'weight_decay_Hydroxylation-P': 0.07929531070877749}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.521
[2,     1] loss: 1294.135
[3,     1] loss: 1288.179
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006026545850946743,
 'learning_rate_Hydroxylation-K': 0.009022473187103275,
 'learning_rate_Hydroxylation-P': 0.003752821936661623,
 'log_base': 2.212209906018023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 354401383,
 'sample_weights': [1.788815625165095, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.263077803672849,
 'weight_decay_Hydroxylation-K': 6.778836624510679,
 'weight_decay_Hydroxylation-P': 1.9412245041463205}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1353.024
[2,     1] loss: 1357.515
[3,     1] loss: 1351.298
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006269892601081177,
 'learning_rate_Hydroxylation-K': 0.008466648533349561,
 'learning_rate_Hydroxylation-P': 0.008198382288690671,
 'log_base': 2.684896605419186,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2881017359,
 'sample_weights': [2.1025944888561883, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.741131411412965,
 'weight_decay_Hydroxylation-K': 5.181017041690216,
 'weight_decay_Hydroxylation-P': 2.6277960636730544}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.566
[2,     1] loss: 1279.263
[3,     1] loss: 1268.200
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006572384115682109,
 'learning_rate_Hydroxylation-K': 0.008264840008241634,
 'learning_rate_Hydroxylation-P': 0.008497868467981772,
 'log_base': 1.96043668014086,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3868164845,
 'sample_weights': [1.6903318988544667, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.554651699063308,
 'weight_decay_Hydroxylation-K': 3.3718173570378482,
 'weight_decay_Hydroxylation-P': 4.618895736288135}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1433.139
[2,     1] loss: 1439.293
[3,     1] loss: 1429.396
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035822815101134646,
 'learning_rate_Hydroxylation-K': 0.004965831700897542,
 'learning_rate_Hydroxylation-P': 0.007350841300197162,
 'log_base': 2.637332450824063,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 558733118,
 'sample_weights': [2.4799827402010766, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.275849648372692,
 'weight_decay_Hydroxylation-K': 9.302010620018601,
 'weight_decay_Hydroxylation-P': 2.794894057507897}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.571
[2,     1] loss: 1271.404
[3,     1] loss: 1269.508
[4,     1] loss: 1267.838
[5,     1] loss: 1267.595
[6,     1] loss: 1257.062
[7,     1] loss: 1244.618
[8,     1] loss: 1223.273
[9,     1] loss: 1193.686
[10,     1] loss: 1151.541
[11,     1] loss: 1113.295
[12,     1] loss: 1099.227
[13,     1] loss: 1046.750
[14,     1] loss: 1083.826
[15,     1] loss: 1061.381
[16,     1] loss: 1051.706
[17,     1] loss: 1051.088
[18,     1] loss: 999.043
[19,     1] loss: 1057.895
[20,     1] loss: 1054.918
[21,     1] loss: 1034.521
[22,     1] loss: 999.753
[23,     1] loss: 1007.870
[24,     1] loss: 974.825
[25,     1] loss: 991.882
[26,     1] loss: 952.355
[27,     1] loss: 930.742
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038563263545719523,
 'learning_rate_Hydroxylation-K': 0.00860204425790075,
 'learning_rate_Hydroxylation-P': 0.004779575384499714,
 'log_base': 1.9402992731041169,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2316203741,
 'sample_weights': [1.721487198272595, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.211440364931799,
 'weight_decay_Hydroxylation-K': 9.533185678192307,
 'weight_decay_Hydroxylation-P': 3.525738963214402}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1445.491
[2,     1] loss: 1439.246
[3,     1] loss: 1438.251
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009375322622438398,
 'learning_rate_Hydroxylation-K': 0.009984413431341937,
 'learning_rate_Hydroxylation-P': 0.0004008443528297392,
 'log_base': 2.894011143541472,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3245490980,
 'sample_weights': [2.5186131522481467, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.188368613748989,
 'weight_decay_Hydroxylation-K': 8.452941790575053,
 'weight_decay_Hydroxylation-P': 4.141723853427093}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.705
[2,     1] loss: 1236.071
[3,     1] loss: 1252.610
[4,     1] loss: 1245.745
[5,     1] loss: 1245.348
[6,     1] loss: 1234.282
[7,     1] loss: 1230.935
[8,     1] loss: 1221.527
[9,     1] loss: 1208.337
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031230084898689822,
 'learning_rate_Hydroxylation-K': 0.003995229287138488,
 'learning_rate_Hydroxylation-P': 0.0008792439833634536,
 'log_base': 2.584832369817398,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2917362160,
 'sample_weights': [1.5710284594768205, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.2816496370600727,
 'weight_decay_Hydroxylation-K': 0.7684167128542507,
 'weight_decay_Hydroxylation-P': 4.802027721878963}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1284.955
[2,     1] loss: 1282.441
[3,     1] loss: 1277.149
[4,     1] loss: 1278.194
[5,     1] loss: 1277.992
[6,     1] loss: 1275.829
[7,     1] loss: 1273.088
[8,     1] loss: 1265.380
[9,     1] loss: 1257.803
[10,     1] loss: 1244.951
[11,     1] loss: 1220.323
[12,     1] loss: 1180.986
[13,     1] loss: 1160.128
[14,     1] loss: 1128.776
[15,     1] loss: 1090.577
[16,     1] loss: 1101.603
[17,     1] loss: 1086.045
[18,     1] loss: 1034.245
[19,     1] loss: 1061.341
[20,     1] loss: 1068.398
[21,     1] loss: 1037.717
[22,     1] loss: 1054.529
[23,     1] loss: 1052.016
[24,     1] loss: 1014.684
[25,     1] loss: 1007.403
[26,     1] loss: 1013.038
[27,     1] loss: 994.710
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004798692997117725,
 'learning_rate_Hydroxylation-K': 0.009880584641840984,
 'learning_rate_Hydroxylation-P': 0.00792101087649023,
 'log_base': 1.145663285278176,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1213423573,
 'sample_weights': [1.757936513764125, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.660401004971295,
 'weight_decay_Hydroxylation-K': 6.538767268413575,
 'weight_decay_Hydroxylation-P': 0.12770526939603233}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3984.706
[2,     1] loss: 3994.583
[3,     1] loss: 3977.933
[4,     1] loss: 3982.192
[5,     1] loss: 3987.712
[6,     1] loss: 3982.037
[7,     1] loss: 3959.986
[8,     1] loss: 3965.191
[9,     1] loss: 3953.135
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007707813165859841,
 'learning_rate_Hydroxylation-K': 0.0078058810822874925,
 'learning_rate_Hydroxylation-P': 0.005469857910917976,
 'log_base': 1.0666933944388783,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1326603169,
 'sample_weights': [12.276783457220493, 1.5346570510272841],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.7465286563082,
 'weight_decay_Hydroxylation-K': 6.339086089941479,
 'weight_decay_Hydroxylation-P': 7.185879041438429}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8388.314
[2,     1] loss: 8431.730
[3,     1] loss: 8400.996
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027667542595833767,
 'learning_rate_Hydroxylation-K': 0.0014062663496967132,
 'learning_rate_Hydroxylation-P': 0.009685727016057041,
 'log_base': 1.0844293080805778,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 610318385,
 'sample_weights': [25.857351718905807, 3.232293480990944],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.597071162312378,
 'weight_decay_Hydroxylation-K': 6.780225978762616,
 'weight_decay_Hydroxylation-P': 1.7367724315442659}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6707.909
[2,     1] loss: 6710.400
[3,     1] loss: 6693.595
[4,     1] loss: 6693.375
[5,     1] loss: 6670.049
[6,     1] loss: 6675.707
[7,     1] loss: 6669.008
[8,     1] loss: 6668.165
[9,     1] loss: 6651.212
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006147614467336719,
 'learning_rate_Hydroxylation-K': 0.009511025892750668,
 'learning_rate_Hydroxylation-P': 0.008767016058130414,
 'log_base': 2.0456178704940773,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3271739838,
 'sample_weights': [20.59671234285391, 2.574688226365264],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.761682493663633,
 'weight_decay_Hydroxylation-K': 8.406521023507846,
 'weight_decay_Hydroxylation-P': 3.290196010266696}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1405.927
[2,     1] loss: 1401.504
[3,     1] loss: 1402.700
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004317149114438767,
 'learning_rate_Hydroxylation-K': 0.004895711729000146,
 'learning_rate_Hydroxylation-P': 0.008227364866336363,
 'log_base': 1.3042204317415425,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2810182936,
 'sample_weights': [2.3326022424945116, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.912054063345607,
 'weight_decay_Hydroxylation-K': 4.112151374712038,
 'weight_decay_Hydroxylation-P': 3.65311075692366}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2234.668
[2,     1] loss: 2236.191
[3,     1] loss: 2234.793
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005289713407315794,
 'learning_rate_Hydroxylation-K': 0.005003140289424401,
 'learning_rate_Hydroxylation-P': 0.008894933608513692,
 'log_base': 1.0191944559181763,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3869213128,
 'sample_weights': [6.285424051563348, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.816808487251455,
 'weight_decay_Hydroxylation-K': 3.40404269972663,
 'weight_decay_Hydroxylation-P': 3.51041629149754}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28587.678
Exploding loss, terminate run (best metric=0.5317962765693665)
Finished Training
Total time taken: 0.19995856285095215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28646.980
Exploding loss, terminate run (best metric=0.5305339694023132)
Finished Training
Total time taken: 0.22899985313415527
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28518.943
Exploding loss, terminate run (best metric=0.5387582182884216)
Finished Training
Total time taken: 0.20400071144104004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28568.578
Exploding loss, terminate run (best metric=0.5292471647262573)
Finished Training
Total time taken: 0.2089998722076416
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28290.559
Exploding loss, terminate run (best metric=0.532312273979187)
Finished Training
Total time taken: 0.20800042152404785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28658.918
Exploding loss, terminate run (best metric=0.5332278609275818)
Finished Training
Total time taken: 0.19299888610839844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28594.701
Exploding loss, terminate run (best metric=0.5266236066818237)
Finished Training
Total time taken: 0.21000099182128906
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28595.234
Exploding loss, terminate run (best metric=0.5450583696365356)
Finished Training
Total time taken: 0.2089993953704834
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28565.578
Exploding loss, terminate run (best metric=0.526199460029602)
Finished Training
Total time taken: 0.19499945640563965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28540.930
Exploding loss, terminate run (best metric=0.5280251502990723)
Finished Training
Total time taken: 0.19000029563903809
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28554.229
Exploding loss, terminate run (best metric=0.5350033044815063)
Finished Training
Total time taken: 0.22600245475769043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28520.949
Exploding loss, terminate run (best metric=0.5284456610679626)
Finished Training
Total time taken: 0.23999857902526855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28577.932
Exploding loss, terminate run (best metric=0.5266361236572266)
Finished Training
Total time taken: 0.23200035095214844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28759.639
Exploding loss, terminate run (best metric=0.5465924143791199)
Finished Training
Total time taken: 0.23488640785217285
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28520.789
Exploding loss, terminate run (best metric=0.5290986895561218)
Finished Training
Total time taken: 0.21303629875183105
{'Hydroxylation-K Validation Accuracy': 0.4266548463356974, 'Hydroxylation-K Validation Sensitivity': 0.6444444444444445, 'Hydroxylation-K Validation Specificity': 0.3736842105263158, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6130409356725146, 'Hydroxylation-K AUC PR': 0.3486611415372395, 'Hydroxylation-K MCC': 0.014347700918474546, 'Hydroxylation-K F1': 0.22732348111658457, 'Validation Loss (Hydroxylation-K)': 0.5582394599914551, 'Hydroxylation-P Validation Accuracy': 0.4262480077153444, 'Hydroxylation-P Validation Sensitivity': 0.6247619047619047, 'Hydroxylation-P Validation Specificity': 0.38333333333333336, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5804279534980568, 'Hydroxylation-P AUC PR': 0.2594870237680403, 'Hydroxylation-P MCC': 0.0069311403302958876, 'Hydroxylation-P F1': 0.20001153821435755, 'Validation Loss (Hydroxylation-P)': 0.5325039029121399, 'Validation Loss (total)': 1.090743358929952, 'TimeToTrain': 0.21285883585611978}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004310224649918674,
 'learning_rate_Hydroxylation-K': 0.006723682605128567,
 'learning_rate_Hydroxylation-P': 0.00760456188275416,
 'log_base': 1.1113431027287923,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1302574111,
 'sample_weights': [87.8724903222551, 10.961238351587957],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.724861130710424,
 'weight_decay_Hydroxylation-K': 0.9891601844978379,
 'weight_decay_Hydroxylation-P': 4.480325174861967}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5122.770
[2,     1] loss: 5142.001
[3,     1] loss: 5149.240
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003808851764096272,
 'learning_rate_Hydroxylation-K': 0.005354700597822178,
 'learning_rate_Hydroxylation-P': 0.006332772261782926,
 'log_base': 2.3082669920709127,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3609187341,
 'sample_weights': [15.813720122812635, 1.976791166351725],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.46224015081097,
 'weight_decay_Hydroxylation-K': 5.0932596207909375,
 'weight_decay_Hydroxylation-P': 9.689246500642927}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.749
[2,     1] loss: 1329.927
[3,     1] loss: 1326.999
[4,     1] loss: 1327.935
[5,     1] loss: 1326.454
[6,     1] loss: 1321.171
[7,     1] loss: 1319.304
[8,     1] loss: 1305.480
[9,     1] loss: 1296.028
[10,     1] loss: 1261.750
[11,     1] loss: 1217.953
[12,     1] loss: 1197.060
[13,     1] loss: 1198.776
[14,     1] loss: 1139.620
[15,     1] loss: 1179.723
[16,     1] loss: 1158.439
[17,     1] loss: 1175.742
[18,     1] loss: 1153.705
[19,     1] loss: 1116.451
[20,     1] loss: 1106.089
[21,     1] loss: 1147.295
[22,     1] loss: 1106.529
[23,     1] loss: 1083.123
[24,     1] loss: 1082.381
[25,     1] loss: 1114.131
[26,     1] loss: 1054.110
[27,     1] loss: 1059.962
[28,     1] loss: 1023.369
[29,     1] loss: 1074.053
[30,     1] loss: 994.905
[31,     1] loss: 987.930
[32,     1] loss: 990.665
[33,     1] loss: 1023.080
[34,     1] loss: 976.972
[35,     1] loss: 956.379
[36,     1] loss: 964.768
[37,     1] loss: 932.073
[38,     1] loss: 947.849
[39,     1] loss: 941.634
[40,     1] loss: 969.971
[41,     1] loss: 912.695
[42,     1] loss: 902.203
[43,     1] loss: 953.231
[44,     1] loss: 913.397
[45,     1] loss: 910.397
[46,     1] loss: 858.462
[47,     1] loss: 803.348
[48,     1] loss: 831.139
[49,     1] loss: 880.993
[50,     1] loss: 862.563
[51,     1] loss: 1006.803
[52,     1] loss: 785.095
[53,     1] loss: 871.794
[54,     1] loss: 839.076
[55,     1] loss: 909.105
[56,     1] loss: 849.578
[57,     1] loss: 897.027
[58,     1] loss: 789.865
[59,     1] loss: 871.556
[60,     1] loss: 770.013
[61,     1] loss: 823.532
[62,     1] loss: 738.596
[63,     1] loss: 715.367
[64,     1] loss: 728.658
[65,     1] loss: 715.262
[66,     1] loss: 773.648
[67,     1] loss: 879.609
[68,     1] loss: 976.758
[69,     1] loss: 684.345
[70,     1] loss: 857.732
[71,     1] loss: 770.649
[72,     1] loss: 912.110
[73,     1] loss: 763.743
[74,     1] loss: 804.658
[75,     1] loss: 771.689
[76,     1] loss: 717.313
[77,     1] loss: 750.971
[78,     1] loss: 718.236
[79,     1] loss: 721.095
[80,     1] loss: 802.370
[81,     1] loss: 697.612
[82,     1] loss: 678.316
[83,     1] loss: 732.002
[84,     1] loss: 619.469
[85,     1] loss: 681.999
[86,     1] loss: 635.217
[87,     1] loss: 623.188
[88,     1] loss: 731.592
[89,     1] loss: 681.058
[90,     1] loss: 608.292
[91,     1] loss: 619.861
[92,     1] loss: 553.175
[93,     1] loss: 555.192
[94,     1] loss: 564.121
[95,     1] loss: 583.686
[96,     1] loss: 853.310
[97,     1] loss: 1548.689
[98,     1] loss: 679.929
[99,     1] loss: 1003.585
[100,     1] loss: 1030.625
[101,     1] loss: 947.031
[102,     1] loss: 992.198
[103,     1] loss: 995.527
[104,     1] loss: 999.094
[105,     1] loss: 879.676
[106,     1] loss: 897.255
[107,     1] loss: 1010.967
[108,     1] loss: 947.063
[109,     1] loss: 881.636
[110,     1] loss: 884.659
[111,     1] loss: 880.242
[112,     1] loss: 871.326
Early stopping applied (best metric=0.2890581786632538)
Finished Training
Total time taken: 19.166084051132202
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.100
[2,     1] loss: 1334.410
[3,     1] loss: 1329.216
[4,     1] loss: 1326.004
[5,     1] loss: 1329.427
[6,     1] loss: 1323.734
[7,     1] loss: 1321.171
[8,     1] loss: 1319.197
[9,     1] loss: 1300.424
[10,     1] loss: 1282.960
[11,     1] loss: 1250.506
[12,     1] loss: 1218.996
[13,     1] loss: 1195.386
[14,     1] loss: 1140.744
[15,     1] loss: 1163.724
[16,     1] loss: 1093.942
[17,     1] loss: 1102.063
[18,     1] loss: 1098.406
[19,     1] loss: 1062.092
[20,     1] loss: 1073.622
[21,     1] loss: 1053.764
[22,     1] loss: 1061.452
[23,     1] loss: 1007.897
[24,     1] loss: 1034.102
[25,     1] loss: 1025.953
[26,     1] loss: 959.098
[27,     1] loss: 979.289
[28,     1] loss: 977.544
[29,     1] loss: 949.598
[30,     1] loss: 995.626
[31,     1] loss: 992.544
[32,     1] loss: 913.141
[33,     1] loss: 944.364
[34,     1] loss: 883.334
[35,     1] loss: 902.689
[36,     1] loss: 852.938
[37,     1] loss: 884.248
[38,     1] loss: 909.093
[39,     1] loss: 852.266
[40,     1] loss: 830.489
[41,     1] loss: 806.743
[42,     1] loss: 831.826
[43,     1] loss: 921.274
[44,     1] loss: 886.776
[45,     1] loss: 810.057
[46,     1] loss: 815.165
[47,     1] loss: 851.336
[48,     1] loss: 774.307
[49,     1] loss: 793.553
[50,     1] loss: 736.742
[51,     1] loss: 740.257
[52,     1] loss: 717.263
[53,     1] loss: 801.416
[54,     1] loss: 828.824
[55,     1] loss: 778.989
[56,     1] loss: 710.120
[57,     1] loss: 845.882
[58,     1] loss: 692.668
[59,     1] loss: 721.999
[60,     1] loss: 683.875
[61,     1] loss: 791.513
[62,     1] loss: 709.374
[63,     1] loss: 698.223
[64,     1] loss: 807.388
[65,     1] loss: 639.554
[66,     1] loss: 648.337
[67,     1] loss: 714.158
[68,     1] loss: 612.447
[69,     1] loss: 731.449
[70,     1] loss: 745.819
[71,     1] loss: 620.224
[72,     1] loss: 737.526
[73,     1] loss: 619.772
[74,     1] loss: 637.905
[75,     1] loss: 615.855
[76,     1] loss: 573.265
[77,     1] loss: 654.136
[78,     1] loss: 654.980
[79,     1] loss: 550.126
[80,     1] loss: 536.647
[81,     1] loss: 705.454
[82,     1] loss: 674.489
[83,     1] loss: 535.127
[84,     1] loss: 538.186
[85,     1] loss: 566.875
Early stopping applied (best metric=0.39599373936653137)
Finished Training
Total time taken: 12.46853494644165
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1335.843
[2,     1] loss: 1330.359
[3,     1] loss: 1328.630
[4,     1] loss: 1328.217
[5,     1] loss: 1323.201
[6,     1] loss: 1323.407
[7,     1] loss: 1306.703
[8,     1] loss: 1290.734
[9,     1] loss: 1264.855
[10,     1] loss: 1222.427
[11,     1] loss: 1186.704
[12,     1] loss: 1172.043
[13,     1] loss: 1117.458
[14,     1] loss: 1112.220
[15,     1] loss: 1096.151
[16,     1] loss: 1120.700
[17,     1] loss: 1105.276
[18,     1] loss: 1021.754
[19,     1] loss: 1060.081
[20,     1] loss: 1029.721
[21,     1] loss: 1025.442
[22,     1] loss: 1017.335
[23,     1] loss: 1031.888
[24,     1] loss: 979.728
[25,     1] loss: 992.566
[26,     1] loss: 939.485
[27,     1] loss: 973.920
[28,     1] loss: 924.336
[29,     1] loss: 944.717
[30,     1] loss: 913.223
[31,     1] loss: 1011.062
[32,     1] loss: 983.816
[33,     1] loss: 913.896
[34,     1] loss: 1022.616
[35,     1] loss: 955.011
[36,     1] loss: 906.120
[37,     1] loss: 903.016
[38,     1] loss: 913.065
[39,     1] loss: 906.100
[40,     1] loss: 926.683
[41,     1] loss: 852.601
[42,     1] loss: 918.367
[43,     1] loss: 863.905
[44,     1] loss: 814.960
[45,     1] loss: 834.869
[46,     1] loss: 823.761
[47,     1] loss: 829.046
[48,     1] loss: 854.114
[49,     1] loss: 831.198
[50,     1] loss: 819.415
[51,     1] loss: 764.339
[52,     1] loss: 781.696
[53,     1] loss: 832.771
[54,     1] loss: 832.047
[55,     1] loss: 769.428
[56,     1] loss: 809.990
[57,     1] loss: 784.218
[58,     1] loss: 759.624
[59,     1] loss: 732.615
[60,     1] loss: 742.494
[61,     1] loss: 711.986
[62,     1] loss: 746.301
[63,     1] loss: 752.744
[64,     1] loss: 926.129
[65,     1] loss: 1197.624
[66,     1] loss: 705.684
[67,     1] loss: 990.063
[68,     1] loss: 844.377
[69,     1] loss: 848.535
[70,     1] loss: 906.809
[71,     1] loss: 849.523
[72,     1] loss: 801.674
[73,     1] loss: 836.465
[74,     1] loss: 783.508
[75,     1] loss: 771.114
[76,     1] loss: 808.449
[77,     1] loss: 725.622
[78,     1] loss: 685.510
[79,     1] loss: 728.791
[80,     1] loss: 731.822
[81,     1] loss: 767.151
[82,     1] loss: 687.135
[83,     1] loss: 678.469
[84,     1] loss: 714.501
[85,     1] loss: 662.015
[86,     1] loss: 748.334
[87,     1] loss: 681.720
[88,     1] loss: 608.230
[89,     1] loss: 742.746
[90,     1] loss: 740.017
[91,     1] loss: 586.564
[92,     1] loss: 601.809
[93,     1] loss: 603.894
[94,     1] loss: 576.528
[95,     1] loss: 589.797
[96,     1] loss: 807.842
[97,     1] loss: 1395.805
[98,     1] loss: 1236.270
[99,     1] loss: 1151.367
[100,     1] loss: 898.928
[101,     1] loss: 978.078
[102,     1] loss: 1141.715
[103,     1] loss: 1110.253
[104,     1] loss: 1052.913
[105,     1] loss: 1026.555
[106,     1] loss: 1041.327
Early stopping applied (best metric=0.4056681990623474)
Finished Training
Total time taken: 15.991798877716064
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1333.470
[2,     1] loss: 1328.036
[3,     1] loss: 1328.479
[4,     1] loss: 1329.393
[5,     1] loss: 1324.878
[6,     1] loss: 1324.305
[7,     1] loss: 1330.062
[8,     1] loss: 1325.150
[9,     1] loss: 1319.852
[10,     1] loss: 1314.450
[11,     1] loss: 1302.061
[12,     1] loss: 1289.381
[13,     1] loss: 1269.512
[14,     1] loss: 1247.845
[15,     1] loss: 1217.375
[16,     1] loss: 1183.878
[17,     1] loss: 1167.998
[18,     1] loss: 1177.536
[19,     1] loss: 1117.128
[20,     1] loss: 1166.461
[21,     1] loss: 1045.098
[22,     1] loss: 1112.037
[23,     1] loss: 1050.349
[24,     1] loss: 1066.070
[25,     1] loss: 1061.955
[26,     1] loss: 1061.823
[27,     1] loss: 1074.193
[28,     1] loss: 1053.776
[29,     1] loss: 1017.683
[30,     1] loss: 1025.442
[31,     1] loss: 1016.733
[32,     1] loss: 1011.239
[33,     1] loss: 967.215
[34,     1] loss: 953.795
[35,     1] loss: 876.801
[36,     1] loss: 976.405
[37,     1] loss: 1030.975
[38,     1] loss: 945.693
[39,     1] loss: 901.590
[40,     1] loss: 935.802
[41,     1] loss: 920.396
[42,     1] loss: 936.413
[43,     1] loss: 877.230
[44,     1] loss: 893.243
[45,     1] loss: 844.497
[46,     1] loss: 868.987
[47,     1] loss: 880.734
[48,     1] loss: 873.546
[49,     1] loss: 896.425
[50,     1] loss: 835.353
[51,     1] loss: 800.685
[52,     1] loss: 817.447
[53,     1] loss: 810.623
[54,     1] loss: 803.929
[55,     1] loss: 800.807
[56,     1] loss: 951.789
[57,     1] loss: 987.416
[58,     1] loss: 781.889
[59,     1] loss: 893.776
[60,     1] loss: 789.732
[61,     1] loss: 818.823
[62,     1] loss: 777.146
[63,     1] loss: 811.774
[64,     1] loss: 787.303
[65,     1] loss: 732.596
[66,     1] loss: 737.890
[67,     1] loss: 701.935
[68,     1] loss: 716.729
[69,     1] loss: 705.329
[70,     1] loss: 773.155
[71,     1] loss: 849.311
[72,     1] loss: 793.752
[73,     1] loss: 694.961
[74,     1] loss: 898.181
[75,     1] loss: 706.266
[76,     1] loss: 857.887
[77,     1] loss: 709.825
[78,     1] loss: 858.731
[79,     1] loss: 664.923
[80,     1] loss: 825.804
[81,     1] loss: 653.112
[82,     1] loss: 785.116
[83,     1] loss: 633.520
[84,     1] loss: 748.678
[85,     1] loss: 610.701
[86,     1] loss: 716.516
[87,     1] loss: 601.671
[88,     1] loss: 664.632
[89,     1] loss: 607.315
[90,     1] loss: 683.321
[91,     1] loss: 576.974
[92,     1] loss: 611.861
[93,     1] loss: 640.944
[94,     1] loss: 605.742
[95,     1] loss: 507.654
[96,     1] loss: 616.278
[97,     1] loss: 730.899
[98,     1] loss: 532.960
[99,     1] loss: 598.410
[100,     1] loss: 686.776
[101,     1] loss: 566.948
[102,     1] loss: 526.155
[103,     1] loss: 605.099
[104,     1] loss: 505.374
[105,     1] loss: 544.472
[106,     1] loss: 651.140
[107,     1] loss: 523.230
[108,     1] loss: 554.321
[109,     1] loss: 683.656
[110,     1] loss: 506.695
[111,     1] loss: 753.106
[112,     1] loss: 780.116
[113,     1] loss: 679.280
[114,     1] loss: 663.876
Early stopping applied (best metric=0.3560227155685425)
Finished Training
Total time taken: 17.407659769058228
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1332.141
[2,     1] loss: 1328.788
[3,     1] loss: 1335.476
[4,     1] loss: 1326.339
[5,     1] loss: 1327.040
[6,     1] loss: 1329.232
[7,     1] loss: 1326.118
[8,     1] loss: 1323.596
[9,     1] loss: 1316.621
[10,     1] loss: 1300.755
[11,     1] loss: 1272.170
[12,     1] loss: 1242.073
[13,     1] loss: 1212.198
[14,     1] loss: 1166.855
[15,     1] loss: 1141.930
[16,     1] loss: 1121.039
[17,     1] loss: 1060.460
[18,     1] loss: 1065.688
[19,     1] loss: 1037.181
[20,     1] loss: 1081.190
[21,     1] loss: 1057.326
[22,     1] loss: 1097.643
[23,     1] loss: 1047.098
[24,     1] loss: 1124.140
[25,     1] loss: 1094.565
[26,     1] loss: 1062.314
[27,     1] loss: 992.566
[28,     1] loss: 1031.749
[29,     1] loss: 984.638
[30,     1] loss: 987.311
[31,     1] loss: 960.920
[32,     1] loss: 929.245
[33,     1] loss: 1007.702
[34,     1] loss: 919.094
[35,     1] loss: 928.446
[36,     1] loss: 913.112
[37,     1] loss: 879.846
[38,     1] loss: 879.090
[39,     1] loss: 940.812
[40,     1] loss: 886.694
[41,     1] loss: 870.554
[42,     1] loss: 833.260
[43,     1] loss: 882.683
[44,     1] loss: 813.999
[45,     1] loss: 835.538
[46,     1] loss: 786.412
[47,     1] loss: 877.660
[48,     1] loss: 1015.310
[49,     1] loss: 999.188
[50,     1] loss: 781.089
[51,     1] loss: 841.187
[52,     1] loss: 820.968
[53,     1] loss: 803.140
[54,     1] loss: 785.307
[55,     1] loss: 762.388
[56,     1] loss: 768.826
[57,     1] loss: 722.732
[58,     1] loss: 768.011
[59,     1] loss: 770.349
[60,     1] loss: 739.515
[61,     1] loss: 728.490
[62,     1] loss: 728.054
[63,     1] loss: 672.702
[64,     1] loss: 667.664
[65,     1] loss: 675.573
[66,     1] loss: 660.185
[67,     1] loss: 643.995
[68,     1] loss: 1052.777
[69,     1] loss: 1302.953
[70,     1] loss: 736.783
Early stopping applied (best metric=0.42475658655166626)
Finished Training
Total time taken: 12.350021839141846
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1328.290
[2,     1] loss: 1330.952
[3,     1] loss: 1332.958
[4,     1] loss: 1325.026
[5,     1] loss: 1325.658
[6,     1] loss: 1325.077
[7,     1] loss: 1309.631
[8,     1] loss: 1300.191
[9,     1] loss: 1267.388
[10,     1] loss: 1227.489
[11,     1] loss: 1190.262
[12,     1] loss: 1181.666
[13,     1] loss: 1116.879
[14,     1] loss: 1128.797
[15,     1] loss: 1114.448
[16,     1] loss: 1109.125
[17,     1] loss: 1064.962
[18,     1] loss: 1139.297
[19,     1] loss: 1045.429
[20,     1] loss: 1090.915
[21,     1] loss: 1069.317
[22,     1] loss: 1042.912
[23,     1] loss: 1050.804
[24,     1] loss: 1028.161
[25,     1] loss: 1028.527
[26,     1] loss: 1033.022
[27,     1] loss: 1017.313
[28,     1] loss: 958.688
[29,     1] loss: 975.811
[30,     1] loss: 928.190
[31,     1] loss: 924.899
[32,     1] loss: 972.851
[33,     1] loss: 950.563
[34,     1] loss: 984.738
[35,     1] loss: 933.863
[36,     1] loss: 919.499
[37,     1] loss: 926.000
[38,     1] loss: 897.310
[39,     1] loss: 938.123
[40,     1] loss: 867.163
[41,     1] loss: 882.925
[42,     1] loss: 1052.164
[43,     1] loss: 1054.137
[44,     1] loss: 876.917
[45,     1] loss: 982.485
[46,     1] loss: 965.766
[47,     1] loss: 903.481
[48,     1] loss: 935.989
[49,     1] loss: 916.939
[50,     1] loss: 890.137
[51,     1] loss: 949.900
[52,     1] loss: 824.600
[53,     1] loss: 859.351
[54,     1] loss: 888.932
[55,     1] loss: 816.941
[56,     1] loss: 885.875
[57,     1] loss: 798.325
[58,     1] loss: 827.839
[59,     1] loss: 747.336
[60,     1] loss: 770.072
[61,     1] loss: 765.082
[62,     1] loss: 755.978
[63,     1] loss: 687.410
[64,     1] loss: 684.001
[65,     1] loss: 748.171
[66,     1] loss: 913.416
[67,     1] loss: 1243.084
[68,     1] loss: 749.253
[69,     1] loss: 937.392
[70,     1] loss: 850.778
[71,     1] loss: 825.075
[72,     1] loss: 888.252
[73,     1] loss: 855.422
[74,     1] loss: 777.364
[75,     1] loss: 822.598
[76,     1] loss: 818.735
[77,     1] loss: 799.739
[78,     1] loss: 743.223
[79,     1] loss: 723.390
[80,     1] loss: 818.860
[81,     1] loss: 698.747
[82,     1] loss: 758.979
[83,     1] loss: 658.986
[84,     1] loss: 681.750
[85,     1] loss: 625.030
[86,     1] loss: 602.264
[87,     1] loss: 662.292
[88,     1] loss: 890.519
[89,     1] loss: 550.592
[90,     1] loss: 727.130
[91,     1] loss: 717.885
[92,     1] loss: 695.724
[93,     1] loss: 673.634
[94,     1] loss: 726.867
[95,     1] loss: 608.822
[96,     1] loss: 631.890
[97,     1] loss: 589.638
[98,     1] loss: 644.317
[99,     1] loss: 618.078
[100,     1] loss: 532.015
[101,     1] loss: 581.166
[102,     1] loss: 765.975
[103,     1] loss: 547.387
[104,     1] loss: 601.272
[105,     1] loss: 698.133
[106,     1] loss: 529.145
[107,     1] loss: 660.157
[108,     1] loss: 569.844
[109,     1] loss: 499.485
[110,     1] loss: 705.371
[111,     1] loss: 789.526
Early stopping applied (best metric=0.3953857421875)
Finished Training
Total time taken: 17.302144050598145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.129
[2,     1] loss: 1330.688
[3,     1] loss: 1327.414
[4,     1] loss: 1331.214
[5,     1] loss: 1327.322
[6,     1] loss: 1325.073
[7,     1] loss: 1332.240
[8,     1] loss: 1326.531
[9,     1] loss: 1317.839
[10,     1] loss: 1309.569
[11,     1] loss: 1294.828
[12,     1] loss: 1261.834
[13,     1] loss: 1229.979
[14,     1] loss: 1189.643
[15,     1] loss: 1171.266
[16,     1] loss: 1112.015
[17,     1] loss: 1097.651
[18,     1] loss: 1100.793
[19,     1] loss: 1072.052
[20,     1] loss: 1109.707
[21,     1] loss: 1098.212
[22,     1] loss: 1090.797
[23,     1] loss: 1054.030
[24,     1] loss: 1027.361
[25,     1] loss: 1059.875
[26,     1] loss: 1055.281
[27,     1] loss: 1035.102
[28,     1] loss: 1029.916
[29,     1] loss: 1028.006
[30,     1] loss: 997.376
[31,     1] loss: 973.708
[32,     1] loss: 959.951
[33,     1] loss: 983.539
[34,     1] loss: 979.517
[35,     1] loss: 937.144
[36,     1] loss: 964.536
[37,     1] loss: 935.267
[38,     1] loss: 960.169
[39,     1] loss: 895.216
[40,     1] loss: 912.716
[41,     1] loss: 944.426
[42,     1] loss: 858.342
[43,     1] loss: 937.909
[44,     1] loss: 903.083
[45,     1] loss: 897.465
[46,     1] loss: 942.345
[47,     1] loss: 892.435
[48,     1] loss: 920.486
[49,     1] loss: 890.857
[50,     1] loss: 956.743
[51,     1] loss: 871.129
[52,     1] loss: 957.934
[53,     1] loss: 824.402
[54,     1] loss: 889.474
[55,     1] loss: 845.031
[56,     1] loss: 906.285
[57,     1] loss: 828.117
[58,     1] loss: 826.577
[59,     1] loss: 831.472
[60,     1] loss: 797.592
[61,     1] loss: 791.784
[62,     1] loss: 733.129
[63,     1] loss: 793.084
[64,     1] loss: 808.688
[65,     1] loss: 773.289
[66,     1] loss: 742.669
[67,     1] loss: 687.252
[68,     1] loss: 752.865
[69,     1] loss: 963.503
[70,     1] loss: 742.468
Early stopping applied (best metric=0.3886638283729553)
Finished Training
Total time taken: 10.692623853683472
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.569
[2,     1] loss: 1333.679
[3,     1] loss: 1330.134
[4,     1] loss: 1325.759
[5,     1] loss: 1322.393
[6,     1] loss: 1321.385
[7,     1] loss: 1307.798
[8,     1] loss: 1289.945
[9,     1] loss: 1251.575
[10,     1] loss: 1209.627
[11,     1] loss: 1210.357
[12,     1] loss: 1140.493
[13,     1] loss: 1153.052
[14,     1] loss: 1113.958
[15,     1] loss: 1106.822
[16,     1] loss: 1103.100
[17,     1] loss: 1163.513
[18,     1] loss: 1071.455
[19,     1] loss: 1063.629
[20,     1] loss: 1054.074
[21,     1] loss: 1055.050
[22,     1] loss: 1054.123
[23,     1] loss: 1068.474
[24,     1] loss: 1025.269
[25,     1] loss: 998.984
[26,     1] loss: 972.530
[27,     1] loss: 965.301
[28,     1] loss: 941.331
[29,     1] loss: 981.053
[30,     1] loss: 997.948
[31,     1] loss: 972.080
[32,     1] loss: 933.046
[33,     1] loss: 922.505
[34,     1] loss: 918.598
[35,     1] loss: 960.181
[36,     1] loss: 910.264
[37,     1] loss: 894.539
[38,     1] loss: 954.687
[39,     1] loss: 928.141
[40,     1] loss: 909.897
[41,     1] loss: 927.552
[42,     1] loss: 855.658
[43,     1] loss: 880.388
[44,     1] loss: 884.718
[45,     1] loss: 1008.346
[46,     1] loss: 862.127
[47,     1] loss: 901.900
[48,     1] loss: 889.330
[49,     1] loss: 840.518
[50,     1] loss: 855.482
[51,     1] loss: 872.217
[52,     1] loss: 833.901
[53,     1] loss: 872.227
[54,     1] loss: 824.152
[55,     1] loss: 771.160
[56,     1] loss: 897.036
[57,     1] loss: 781.644
[58,     1] loss: 771.214
[59,     1] loss: 814.282
[60,     1] loss: 738.760
[61,     1] loss: 879.269
[62,     1] loss: 759.214
[63,     1] loss: 760.887
[64,     1] loss: 849.578
[65,     1] loss: 732.622
[66,     1] loss: 751.617
[67,     1] loss: 698.106
[68,     1] loss: 675.830
[69,     1] loss: 713.083
[70,     1] loss: 690.053
[71,     1] loss: 640.329
[72,     1] loss: 670.302
[73,     1] loss: 759.060
[74,     1] loss: 833.041
[75,     1] loss: 634.235
[76,     1] loss: 674.940
[77,     1] loss: 693.425
[78,     1] loss: 626.365
[79,     1] loss: 692.794
[80,     1] loss: 636.821
[81,     1] loss: 651.361
[82,     1] loss: 712.412
[83,     1] loss: 595.207
[84,     1] loss: 710.111
[85,     1] loss: 641.598
[86,     1] loss: 714.491
[87,     1] loss: 917.181
[88,     1] loss: 639.567
[89,     1] loss: 979.138
[90,     1] loss: 646.262
[91,     1] loss: 841.357
[92,     1] loss: 706.968
[93,     1] loss: 707.170
[94,     1] loss: 747.708
[95,     1] loss: 640.967
[96,     1] loss: 761.212
[97,     1] loss: 591.664
[98,     1] loss: 804.480
[99,     1] loss: 571.511
[100,     1] loss: 672.842
[101,     1] loss: 655.627
[102,     1] loss: 730.864
[103,     1] loss: 613.186
[104,     1] loss: 655.442
[105,     1] loss: 564.850
[106,     1] loss: 628.447
[107,     1] loss: 578.988
[108,     1] loss: 574.910
[109,     1] loss: 513.456
[110,     1] loss: 505.164
[111,     1] loss: 452.407
[112,     1] loss: 538.825
[113,     1] loss: 732.449
[114,     1] loss: 1007.681
[115,     1] loss: 598.721
[116,     1] loss: 673.944
[117,     1] loss: 610.315
[118,     1] loss: 740.743
[119,     1] loss: 551.982
[120,     1] loss: 756.591
[121,     1] loss: 639.911
[122,     1] loss: 600.963
[123,     1] loss: 722.927
[124,     1] loss: 628.549
[125,     1] loss: 601.214
[126,     1] loss: 608.986
[127,     1] loss: 518.883
[128,     1] loss: 615.399
[129,     1] loss: 534.027
[130,     1] loss: 598.814
[131,     1] loss: 722.024
[132,     1] loss: 522.116
Early stopping applied (best metric=0.2951253056526184)
Finished Training
Total time taken: 19.790202140808105
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.945
[2,     1] loss: 1328.925
[3,     1] loss: 1331.329
[4,     1] loss: 1326.651
[5,     1] loss: 1330.027
[6,     1] loss: 1327.368
[7,     1] loss: 1325.992
[8,     1] loss: 1320.293
[9,     1] loss: 1316.333
[10,     1] loss: 1304.975
[11,     1] loss: 1289.112
[12,     1] loss: 1260.698
[13,     1] loss: 1238.184
[14,     1] loss: 1205.510
[15,     1] loss: 1158.698
[16,     1] loss: 1179.292
[17,     1] loss: 1128.679
[18,     1] loss: 1138.695
[19,     1] loss: 1120.488
[20,     1] loss: 1110.596
[21,     1] loss: 1083.771
[22,     1] loss: 1064.871
[23,     1] loss: 1137.162
[24,     1] loss: 1047.478
[25,     1] loss: 1078.381
[26,     1] loss: 1045.394
[27,     1] loss: 1034.690
[28,     1] loss: 1013.334
[29,     1] loss: 1065.582
[30,     1] loss: 1037.539
[31,     1] loss: 1015.509
[32,     1] loss: 985.612
[33,     1] loss: 980.059
[34,     1] loss: 1011.804
[35,     1] loss: 1027.356
[36,     1] loss: 981.711
[37,     1] loss: 1004.022
[38,     1] loss: 939.016
[39,     1] loss: 988.441
[40,     1] loss: 985.580
[41,     1] loss: 969.235
[42,     1] loss: 932.648
[43,     1] loss: 1005.184
[44,     1] loss: 992.427
[45,     1] loss: 916.427
[46,     1] loss: 996.384
[47,     1] loss: 925.324
[48,     1] loss: 957.577
[49,     1] loss: 939.146
[50,     1] loss: 934.711
[51,     1] loss: 893.019
[52,     1] loss: 891.225
[53,     1] loss: 845.399
[54,     1] loss: 891.701
[55,     1] loss: 921.449
[56,     1] loss: 885.144
[57,     1] loss: 862.675
[58,     1] loss: 839.950
[59,     1] loss: 847.085
[60,     1] loss: 825.225
[61,     1] loss: 815.121
[62,     1] loss: 783.033
[63,     1] loss: 817.643
[64,     1] loss: 1067.604
[65,     1] loss: 1208.531
[66,     1] loss: 842.669
[67,     1] loss: 972.083
[68,     1] loss: 967.617
[69,     1] loss: 893.357
[70,     1] loss: 912.357
[71,     1] loss: 988.361
[72,     1] loss: 933.975
[73,     1] loss: 851.012
[74,     1] loss: 964.300
[75,     1] loss: 957.252
[76,     1] loss: 860.222
[77,     1] loss: 903.270
[78,     1] loss: 838.310
[79,     1] loss: 861.628
[80,     1] loss: 876.563
[81,     1] loss: 804.519
[82,     1] loss: 864.423
[83,     1] loss: 770.738
[84,     1] loss: 804.401
[85,     1] loss: 808.146
[86,     1] loss: 743.235
[87,     1] loss: 814.402
[88,     1] loss: 747.049
[89,     1] loss: 705.801
[90,     1] loss: 783.161
[91,     1] loss: 984.642
[92,     1] loss: 728.354
[93,     1] loss: 1081.531
[94,     1] loss: 715.419
[95,     1] loss: 953.107
[96,     1] loss: 813.499
[97,     1] loss: 836.612
[98,     1] loss: 893.342
[99,     1] loss: 761.303
[100,     1] loss: 799.730
[101,     1] loss: 755.401
[102,     1] loss: 804.015
[103,     1] loss: 715.172
[104,     1] loss: 754.072
[105,     1] loss: 675.207
[106,     1] loss: 773.077
[107,     1] loss: 703.670
[108,     1] loss: 761.354
[109,     1] loss: 836.196
[110,     1] loss: 636.036
[111,     1] loss: 671.526
[112,     1] loss: 625.146
[113,     1] loss: 639.115
[114,     1] loss: 702.391
[115,     1] loss: 622.261
[116,     1] loss: 606.826
[117,     1] loss: 608.262
[118,     1] loss: 552.441
[119,     1] loss: 892.744
[120,     1] loss: 1804.816
[121,     1] loss: 678.335
[122,     1] loss: 956.168
[123,     1] loss: 864.106
[124,     1] loss: 878.059
[125,     1] loss: 917.369
[126,     1] loss: 836.814
[127,     1] loss: 923.631
[128,     1] loss: 884.967
[129,     1] loss: 809.290
[130,     1] loss: 871.116
[131,     1] loss: 815.669
[132,     1] loss: 815.038
[133,     1] loss: 774.020
[134,     1] loss: 779.746
[135,     1] loss: 796.860
[136,     1] loss: 749.695
[137,     1] loss: 723.986
[138,     1] loss: 779.180
[139,     1] loss: 806.891
[140,     1] loss: 733.073
[141,     1] loss: 710.634
[142,     1] loss: 717.111
[143,     1] loss: 659.301
[144,     1] loss: 701.101
[145,     1] loss: 765.434
[146,     1] loss: 617.692
[147,     1] loss: 630.447
[148,     1] loss: 696.531
[149,     1] loss: 826.935
[150,     1] loss: 1155.742
[151,     1] loss: 607.502
[152,     1] loss: 1021.689
[153,     1] loss: 673.696
[154,     1] loss: 899.967
[155,     1] loss: 687.361
[156,     1] loss: 835.159
[157,     1] loss: 740.464
[158,     1] loss: 717.627
[159,     1] loss: 660.349
[160,     1] loss: 670.454
[161,     1] loss: 614.964
[162,     1] loss: 662.795
[163,     1] loss: 589.364
[164,     1] loss: 649.889
[165,     1] loss: 902.565
Early stopping applied (best metric=0.3389561176300049)
Finished Training
Total time taken: 25.640618085861206
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1333.099
[2,     1] loss: 1335.768
[3,     1] loss: 1329.342
[4,     1] loss: 1328.827
[5,     1] loss: 1331.664
[6,     1] loss: 1330.063
[7,     1] loss: 1325.387
[8,     1] loss: 1326.332
[9,     1] loss: 1320.406
[10,     1] loss: 1311.886
[11,     1] loss: 1296.964
[12,     1] loss: 1264.938
[13,     1] loss: 1246.725
[14,     1] loss: 1203.722
[15,     1] loss: 1176.671
[16,     1] loss: 1153.573
[17,     1] loss: 1123.410
[18,     1] loss: 1132.713
[19,     1] loss: 1113.815
[20,     1] loss: 1123.395
[21,     1] loss: 1052.534
[22,     1] loss: 1065.708
[23,     1] loss: 1013.265
[24,     1] loss: 1097.611
[25,     1] loss: 1111.272
[26,     1] loss: 1033.675
[27,     1] loss: 1057.088
[28,     1] loss: 1050.735
[29,     1] loss: 1021.182
[30,     1] loss: 1036.976
[31,     1] loss: 1011.090
[32,     1] loss: 1042.608
[33,     1] loss: 999.254
[34,     1] loss: 930.271
[35,     1] loss: 949.596
[36,     1] loss: 945.682
[37,     1] loss: 970.482
[38,     1] loss: 920.461
[39,     1] loss: 914.405
[40,     1] loss: 886.398
[41,     1] loss: 1006.711
[42,     1] loss: 940.377
[43,     1] loss: 881.829
[44,     1] loss: 871.158
[45,     1] loss: 857.154
[46,     1] loss: 848.050
[47,     1] loss: 862.464
[48,     1] loss: 841.425
[49,     1] loss: 848.355
[50,     1] loss: 958.010
[51,     1] loss: 1111.439
[52,     1] loss: 909.822
[53,     1] loss: 831.518
[54,     1] loss: 835.934
[55,     1] loss: 866.944
[56,     1] loss: 838.383
[57,     1] loss: 851.229
[58,     1] loss: 800.940
[59,     1] loss: 783.604
[60,     1] loss: 805.709
[61,     1] loss: 735.920
[62,     1] loss: 775.714
[63,     1] loss: 759.212
[64,     1] loss: 734.721
[65,     1] loss: 696.053
[66,     1] loss: 747.150
[67,     1] loss: 732.853
[68,     1] loss: 766.327
[69,     1] loss: 765.586
[70,     1] loss: 716.905
[71,     1] loss: 689.373
[72,     1] loss: 691.894
[73,     1] loss: 669.773
[74,     1] loss: 891.816
[75,     1] loss: 1453.992
[76,     1] loss: 665.786
[77,     1] loss: 1034.460
[78,     1] loss: 895.967
[79,     1] loss: 840.474
[80,     1] loss: 935.860
[81,     1] loss: 965.315
[82,     1] loss: 872.421
[83,     1] loss: 835.893
[84,     1] loss: 840.426
[85,     1] loss: 871.825
[86,     1] loss: 804.004
[87,     1] loss: 836.750
[88,     1] loss: 776.455
[89,     1] loss: 759.150
[90,     1] loss: 781.344
[91,     1] loss: 710.133
[92,     1] loss: 726.971
[93,     1] loss: 671.789
[94,     1] loss: 679.469
[95,     1] loss: 596.815
[96,     1] loss: 597.973
[97,     1] loss: 607.556
[98,     1] loss: 602.690
[99,     1] loss: 712.818
[100,     1] loss: 919.090
[101,     1] loss: 1091.003
[102,     1] loss: 643.545
[103,     1] loss: 927.666
[104,     1] loss: 865.025
[105,     1] loss: 752.812
[106,     1] loss: 909.490
[107,     1] loss: 853.562
[108,     1] loss: 697.270
[109,     1] loss: 862.408
[110,     1] loss: 704.049
[111,     1] loss: 776.448
[112,     1] loss: 725.736
[113,     1] loss: 668.584
[114,     1] loss: 731.333
[115,     1] loss: 709.950
[116,     1] loss: 734.669
[117,     1] loss: 700.685
[118,     1] loss: 592.220
[119,     1] loss: 583.407
Early stopping applied (best metric=0.3501174747943878)
Finished Training
Total time taken: 17.806732416152954
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.767
[2,     1] loss: 1326.027
[3,     1] loss: 1327.068
[4,     1] loss: 1328.406
[5,     1] loss: 1327.711
[6,     1] loss: 1321.282
[7,     1] loss: 1319.659
[8,     1] loss: 1312.669
[9,     1] loss: 1286.825
[10,     1] loss: 1262.407
[11,     1] loss: 1214.714
[12,     1] loss: 1190.748
[13,     1] loss: 1154.912
[14,     1] loss: 1117.199
[15,     1] loss: 1105.034
[16,     1] loss: 1106.072
[17,     1] loss: 1157.567
[18,     1] loss: 1080.977
[19,     1] loss: 1089.148
[20,     1] loss: 1023.982
[21,     1] loss: 1043.545
[22,     1] loss: 1031.011
[23,     1] loss: 1032.471
[24,     1] loss: 1011.127
[25,     1] loss: 999.418
[26,     1] loss: 962.802
[27,     1] loss: 1003.218
[28,     1] loss: 940.043
[29,     1] loss: 999.113
[30,     1] loss: 1072.979
[31,     1] loss: 1010.642
[32,     1] loss: 945.452
[33,     1] loss: 955.335
[34,     1] loss: 929.272
[35,     1] loss: 947.376
[36,     1] loss: 937.609
[37,     1] loss: 926.025
[38,     1] loss: 876.612
[39,     1] loss: 857.436
[40,     1] loss: 881.998
[41,     1] loss: 865.175
[42,     1] loss: 931.260
[43,     1] loss: 950.673
[44,     1] loss: 864.100
[45,     1] loss: 889.110
[46,     1] loss: 927.867
[47,     1] loss: 815.049
[48,     1] loss: 940.673
[49,     1] loss: 829.957
[50,     1] loss: 880.601
[51,     1] loss: 845.395
[52,     1] loss: 801.139
[53,     1] loss: 828.745
[54,     1] loss: 777.510
[55,     1] loss: 818.319
[56,     1] loss: 757.673
[57,     1] loss: 799.840
[58,     1] loss: 860.613
[59,     1] loss: 730.287
[60,     1] loss: 799.337
[61,     1] loss: 748.744
[62,     1] loss: 719.348
[63,     1] loss: 731.151
[64,     1] loss: 722.664
[65,     1] loss: 662.388
[66,     1] loss: 750.190
[67,     1] loss: 991.124
[68,     1] loss: 749.024
[69,     1] loss: 666.512
[70,     1] loss: 751.003
[71,     1] loss: 683.768
[72,     1] loss: 832.332
[73,     1] loss: 710.919
[74,     1] loss: 706.742
[75,     1] loss: 712.578
[76,     1] loss: 652.576
[77,     1] loss: 653.632
[78,     1] loss: 615.886
[79,     1] loss: 601.624
[80,     1] loss: 615.852
[81,     1] loss: 599.459
[82,     1] loss: 634.832
[83,     1] loss: 686.017
[84,     1] loss: 701.477
[85,     1] loss: 592.883
[86,     1] loss: 567.520
[87,     1] loss: 599.002
[88,     1] loss: 613.980
[89,     1] loss: 629.242
[90,     1] loss: 577.677
Early stopping applied (best metric=0.3923313319683075)
Finished Training
Total time taken: 13.693109035491943
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1331.419
[2,     1] loss: 1328.523
[3,     1] loss: 1328.040
[4,     1] loss: 1325.632
[5,     1] loss: 1327.012
[6,     1] loss: 1324.523
[7,     1] loss: 1321.759
[8,     1] loss: 1313.408
[9,     1] loss: 1301.568
[10,     1] loss: 1274.495
[11,     1] loss: 1252.735
[12,     1] loss: 1192.803
[13,     1] loss: 1173.008
[14,     1] loss: 1124.254
[15,     1] loss: 1114.282
[16,     1] loss: 1104.080
[17,     1] loss: 1091.735
[18,     1] loss: 1056.258
[19,     1] loss: 1053.265
[20,     1] loss: 1094.272
[21,     1] loss: 1080.342
[22,     1] loss: 1031.458
[23,     1] loss: 1033.437
[24,     1] loss: 1034.237
[25,     1] loss: 967.935
[26,     1] loss: 965.003
[27,     1] loss: 1007.411
[28,     1] loss: 974.449
[29,     1] loss: 977.181
[30,     1] loss: 967.573
[31,     1] loss: 940.767
[32,     1] loss: 906.522
[33,     1] loss: 878.402
[34,     1] loss: 901.882
[35,     1] loss: 917.710
[36,     1] loss: 967.386
[37,     1] loss: 965.927
[38,     1] loss: 899.501
[39,     1] loss: 878.848
[40,     1] loss: 872.088
[41,     1] loss: 873.048
[42,     1] loss: 783.955
[43,     1] loss: 857.026
[44,     1] loss: 841.274
[45,     1] loss: 898.542
[46,     1] loss: 798.522
[47,     1] loss: 807.015
[48,     1] loss: 856.032
[49,     1] loss: 818.137
[50,     1] loss: 758.644
[51,     1] loss: 806.378
[52,     1] loss: 756.505
[53,     1] loss: 816.769
[54,     1] loss: 862.251
[55,     1] loss: 751.583
[56,     1] loss: 752.356
[57,     1] loss: 771.933
[58,     1] loss: 742.485
[59,     1] loss: 760.695
[60,     1] loss: 737.686
[61,     1] loss: 708.818
[62,     1] loss: 904.741
[63,     1] loss: 690.700
[64,     1] loss: 755.995
[65,     1] loss: 815.748
[66,     1] loss: 708.044
[67,     1] loss: 844.283
[68,     1] loss: 648.181
[69,     1] loss: 728.671
[70,     1] loss: 649.739
[71,     1] loss: 762.464
[72,     1] loss: 674.509
[73,     1] loss: 722.017
[74,     1] loss: 876.683
[75,     1] loss: 627.562
[76,     1] loss: 810.707
[77,     1] loss: 657.973
[78,     1] loss: 804.946
[79,     1] loss: 660.601
[80,     1] loss: 696.613
[81,     1] loss: 644.650
[82,     1] loss: 676.374
[83,     1] loss: 590.316
[84,     1] loss: 618.890
[85,     1] loss: 675.766
[86,     1] loss: 703.080
[87,     1] loss: 615.541
[88,     1] loss: 607.966
[89,     1] loss: 668.242
[90,     1] loss: 550.529
[91,     1] loss: 589.957
Early stopping applied (best metric=0.40632498264312744)
Finished Training
Total time taken: 13.125529527664185
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.395
[2,     1] loss: 1327.004
[3,     1] loss: 1328.677
[4,     1] loss: 1329.796
[5,     1] loss: 1327.278
[6,     1] loss: 1320.914
[7,     1] loss: 1320.236
[8,     1] loss: 1308.069
[9,     1] loss: 1303.180
[10,     1] loss: 1272.454
[11,     1] loss: 1247.493
[12,     1] loss: 1207.032
[13,     1] loss: 1186.902
[14,     1] loss: 1167.354
[15,     1] loss: 1160.945
[16,     1] loss: 1108.335
[17,     1] loss: 1104.047
[18,     1] loss: 1065.152
[19,     1] loss: 1082.606
[20,     1] loss: 1101.199
[21,     1] loss: 1081.299
[22,     1] loss: 1121.511
[23,     1] loss: 1080.229
[24,     1] loss: 1109.027
[25,     1] loss: 1010.349
[26,     1] loss: 1095.628
[27,     1] loss: 1050.060
[28,     1] loss: 1079.252
[29,     1] loss: 1031.069
[30,     1] loss: 1042.221
[31,     1] loss: 974.402
[32,     1] loss: 1014.826
[33,     1] loss: 1042.158
[34,     1] loss: 1058.976
[35,     1] loss: 986.165
[36,     1] loss: 1019.742
[37,     1] loss: 993.325
[38,     1] loss: 947.438
[39,     1] loss: 984.308
[40,     1] loss: 950.350
[41,     1] loss: 975.176
[42,     1] loss: 949.120
[43,     1] loss: 912.563
[44,     1] loss: 879.755
[45,     1] loss: 889.198
[46,     1] loss: 876.921
[47,     1] loss: 889.601
[48,     1] loss: 836.219
[49,     1] loss: 860.460
[50,     1] loss: 1037.139
[51,     1] loss: 862.917
[52,     1] loss: 866.742
[53,     1] loss: 880.456
[54,     1] loss: 838.379
[55,     1] loss: 876.262
[56,     1] loss: 823.336
[57,     1] loss: 893.363
[58,     1] loss: 777.235
[59,     1] loss: 899.864
[60,     1] loss: 912.508
[61,     1] loss: 798.684
[62,     1] loss: 826.324
[63,     1] loss: 758.327
[64,     1] loss: 770.356
[65,     1] loss: 744.229
[66,     1] loss: 759.598
[67,     1] loss: 826.265
[68,     1] loss: 959.980
[69,     1] loss: 844.868
[70,     1] loss: 700.392
[71,     1] loss: 814.316
[72,     1] loss: 780.943
[73,     1] loss: 752.277
[74,     1] loss: 735.284
[75,     1] loss: 735.044
[76,     1] loss: 754.184
[77,     1] loss: 720.942
[78,     1] loss: 661.996
[79,     1] loss: 773.102
[80,     1] loss: 619.484
[81,     1] loss: 702.122
[82,     1] loss: 753.551
[83,     1] loss: 611.972
[84,     1] loss: 753.516
[85,     1] loss: 910.666
[86,     1] loss: 721.330
[87,     1] loss: 798.344
[88,     1] loss: 741.710
[89,     1] loss: 752.070
[90,     1] loss: 796.496
[91,     1] loss: 653.671
[92,     1] loss: 698.024
[93,     1] loss: 662.244
[94,     1] loss: 657.268
[95,     1] loss: 571.950
[96,     1] loss: 640.332
Early stopping applied (best metric=0.2649374008178711)
Finished Training
Total time taken: 14.363018035888672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1328.722
[2,     1] loss: 1326.690
[3,     1] loss: 1327.099
[4,     1] loss: 1331.676
[5,     1] loss: 1325.883
[6,     1] loss: 1319.853
[7,     1] loss: 1317.749
[8,     1] loss: 1300.238
[9,     1] loss: 1283.204
[10,     1] loss: 1257.737
[11,     1] loss: 1218.209
[12,     1] loss: 1190.548
[13,     1] loss: 1145.666
[14,     1] loss: 1148.264
[15,     1] loss: 1132.755
[16,     1] loss: 1232.839
[17,     1] loss: 1125.053
[18,     1] loss: 1075.910
[19,     1] loss: 1055.873
[20,     1] loss: 1089.752
[21,     1] loss: 1086.106
[22,     1] loss: 1096.828
[23,     1] loss: 1074.418
[24,     1] loss: 1042.638
[25,     1] loss: 998.321
[26,     1] loss: 1002.768
[27,     1] loss: 1000.633
[28,     1] loss: 970.556
[29,     1] loss: 1089.440
[30,     1] loss: 1058.282
[31,     1] loss: 951.843
[32,     1] loss: 949.796
[33,     1] loss: 933.196
[34,     1] loss: 905.158
[35,     1] loss: 929.106
[36,     1] loss: 940.352
[37,     1] loss: 933.537
[38,     1] loss: 889.370
[39,     1] loss: 914.175
[40,     1] loss: 884.849
[41,     1] loss: 884.906
[42,     1] loss: 850.124
[43,     1] loss: 821.969
[44,     1] loss: 820.873
[45,     1] loss: 844.425
[46,     1] loss: 785.129
[47,     1] loss: 899.024
[48,     1] loss: 1288.565
[49,     1] loss: 947.188
[50,     1] loss: 943.586
[51,     1] loss: 859.217
[52,     1] loss: 971.696
[53,     1] loss: 964.880
[54,     1] loss: 921.502
[55,     1] loss: 929.743
[56,     1] loss: 935.918
[57,     1] loss: 884.865
[58,     1] loss: 876.224
[59,     1] loss: 874.140
[60,     1] loss: 895.948
[61,     1] loss: 865.216
[62,     1] loss: 821.682
[63,     1] loss: 821.146
[64,     1] loss: 834.176
[65,     1] loss: 817.284
[66,     1] loss: 728.685
[67,     1] loss: 764.143
[68,     1] loss: 732.192
[69,     1] loss: 781.014
[70,     1] loss: 718.076
[71,     1] loss: 730.066
[72,     1] loss: 807.681
[73,     1] loss: 752.416
[74,     1] loss: 666.485
[75,     1] loss: 658.113
[76,     1] loss: 707.996
[77,     1] loss: 701.261
[78,     1] loss: 642.643
[79,     1] loss: 691.558
[80,     1] loss: 829.515
[81,     1] loss: 961.070
[82,     1] loss: 618.928
[83,     1] loss: 851.517
[84,     1] loss: 681.294
[85,     1] loss: 821.142
[86,     1] loss: 761.864
[87,     1] loss: 751.867
[88,     1] loss: 736.809
[89,     1] loss: 693.671
[90,     1] loss: 669.712
[91,     1] loss: 639.313
[92,     1] loss: 655.407
[93,     1] loss: 732.385
[94,     1] loss: 601.716
[95,     1] loss: 855.293
[96,     1] loss: 638.877
[97,     1] loss: 701.557
[98,     1] loss: 623.922
[99,     1] loss: 812.448
[100,     1] loss: 620.502
[101,     1] loss: 724.863
[102,     1] loss: 576.135
[103,     1] loss: 767.662
[104,     1] loss: 651.491
[105,     1] loss: 664.718
[106,     1] loss: 548.184
[107,     1] loss: 601.275
[108,     1] loss: 655.339
[109,     1] loss: 551.031
[110,     1] loss: 799.716
[111,     1] loss: 853.254
[112,     1] loss: 558.733
[113,     1] loss: 791.390
[114,     1] loss: 549.583
[115,     1] loss: 652.995
Early stopping applied (best metric=0.3653974235057831)
Finished Training
Total time taken: 17.210023641586304
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1329.120
[2,     1] loss: 1331.022
[3,     1] loss: 1328.607
[4,     1] loss: 1327.915
[5,     1] loss: 1325.459
[6,     1] loss: 1315.917
[7,     1] loss: 1297.965
[8,     1] loss: 1271.945
[9,     1] loss: 1226.448
[10,     1] loss: 1161.334
[11,     1] loss: 1120.130
[12,     1] loss: 1117.087
[13,     1] loss: 1078.998
[14,     1] loss: 1112.850
[15,     1] loss: 1013.368
[16,     1] loss: 1088.686
[17,     1] loss: 1149.537
[18,     1] loss: 1055.453
[19,     1] loss: 1108.410
[20,     1] loss: 1050.815
[21,     1] loss: 1057.335
[22,     1] loss: 1049.023
[23,     1] loss: 1042.065
[24,     1] loss: 1009.924
[25,     1] loss: 1028.358
[26,     1] loss: 1062.971
[27,     1] loss: 933.609
[28,     1] loss: 995.134
[29,     1] loss: 944.515
[30,     1] loss: 953.101
[31,     1] loss: 902.665
[32,     1] loss: 956.690
[33,     1] loss: 896.761
[34,     1] loss: 978.578
[35,     1] loss: 894.297
[36,     1] loss: 875.453
[37,     1] loss: 878.547
[38,     1] loss: 901.600
[39,     1] loss: 880.088
[40,     1] loss: 808.460
[41,     1] loss: 804.625
[42,     1] loss: 799.191
[43,     1] loss: 808.330
[44,     1] loss: 784.199
[45,     1] loss: 783.627
[46,     1] loss: 764.252
[47,     1] loss: 871.645
[48,     1] loss: 1156.196
[49,     1] loss: 976.585
[50,     1] loss: 915.137
[51,     1] loss: 856.794
[52,     1] loss: 936.054
[53,     1] loss: 956.142
[54,     1] loss: 903.240
[55,     1] loss: 838.636
[56,     1] loss: 851.474
[57,     1] loss: 908.263
[58,     1] loss: 823.522
[59,     1] loss: 847.071
[60,     1] loss: 881.408
[61,     1] loss: 801.458
[62,     1] loss: 809.854
[63,     1] loss: 809.212
[64,     1] loss: 827.837
[65,     1] loss: 735.430
[66,     1] loss: 784.689
[67,     1] loss: 729.659
[68,     1] loss: 720.623
[69,     1] loss: 737.801
[70,     1] loss: 715.015
[71,     1] loss: 729.032
[72,     1] loss: 651.236
[73,     1] loss: 665.691
[74,     1] loss: 609.301
[75,     1] loss: 659.974
[76,     1] loss: 603.557
[77,     1] loss: 744.091
[78,     1] loss: 1044.845
[79,     1] loss: 621.963
[80,     1] loss: 930.134
[81,     1] loss: 711.161
[82,     1] loss: 781.883
[83,     1] loss: 838.477
Early stopping applied (best metric=0.4273606836795807)
Finished Training
Total time taken: 12.510148048400879
{'Hydroxylation-K Validation Accuracy': 0.7353427895981087, 'Hydroxylation-K Validation Sensitivity': 0.6674074074074074, 'Hydroxylation-K Validation Specificity': 0.7526315789473684, 'Hydroxylation-K Validation Precision': 0.4050129222342845, 'Hydroxylation-K AUC ROC': 0.7906432748538011, 'Hydroxylation-K AUC PR': 0.5946120134495438, 'Hydroxylation-K MCC': 0.35875733137024235, 'Hydroxylation-K F1': 0.49908644312153083, 'Validation Loss (Hydroxylation-K)': 0.46844481229782103, 'Hydroxylation-P Validation Accuracy': 0.7949457557145999, 'Hydroxylation-P Validation Sensitivity': 0.7878306878306879, 'Hydroxylation-P Validation Specificity': 0.796471145693052, 'Hydroxylation-P Validation Precision': 0.4571215337504374, 'Hydroxylation-P AUC ROC': 0.8457577466951999, 'Hydroxylation-P AUC PR': 0.5883681601752605, 'Hydroxylation-P MCC': 0.4851618737814225, 'Hydroxylation-P F1': 0.5772338846880744, 'Validation Loss (Hydroxylation-P)': 0.3664066473642985, 'Validation Loss (total)': 0.8348514676094055, 'TimeToTrain': 15.96788322130839}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006544884918726599,
 'learning_rate_Hydroxylation-K': 0.0009831943802377093,
 'learning_rate_Hydroxylation-P': 0.009425027022209305,
 'log_base': 1.157342432188707,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3176169659,
 'sample_weights': [1.9972354850623895, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.221680519411757,
 'weight_decay_Hydroxylation-K': 4.4139173065504735,
 'weight_decay_Hydroxylation-P': 3.3985396436183417}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3684.093
[2,     1] loss: 3752.061
[3,     1] loss: 3694.399
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003073175569677168,
 'learning_rate_Hydroxylation-K': 0.00566492503447987,
 'learning_rate_Hydroxylation-P': 0.005270328185010355,
 'log_base': 2.24550492056081,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3444482841,
 'sample_weights': [11.424653519223023, 1.4281367053442193],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6345780557172667,
 'weight_decay_Hydroxylation-K': 5.779937988434541,
 'weight_decay_Hydroxylation-P': 1.20121875729691}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.288
[2,     1] loss: 1344.452
[3,     1] loss: 1343.884
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009681580325898817,
 'learning_rate_Hydroxylation-K': 0.003961751777773506,
 'learning_rate_Hydroxylation-P': 0.005996619115348183,
 'log_base': 2.413189769299644,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 808074554,
 'sample_weights': [2.063766100805949, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6059282347562407,
 'weight_decay_Hydroxylation-K': 4.801297527586503,
 'weight_decay_Hydroxylation-P': 5.267585500765547}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1306.543
[2,     1] loss: 1311.517
[3,     1] loss: 1313.283
[4,     1] loss: 1308.211
[5,     1] loss: 1306.488
[6,     1] loss: 1306.371
[7,     1] loss: 1306.871
[8,     1] loss: 1305.644
[9,     1] loss: 1303.269
[10,     1] loss: 1303.536
[11,     1] loss: 1296.310
[12,     1] loss: 1280.959
[13,     1] loss: 1263.857
[14,     1] loss: 1241.311
[15,     1] loss: 1187.175
[16,     1] loss: 1132.814
[17,     1] loss: 1128.476
[18,     1] loss: 1145.733
[19,     1] loss: 1113.128
[20,     1] loss: 1084.080
[21,     1] loss: 1091.401
[22,     1] loss: 1058.142
[23,     1] loss: 1092.031
[24,     1] loss: 1009.347
[25,     1] loss: 1060.100
[26,     1] loss: 1051.401
[27,     1] loss: 1049.262
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006360765691445885,
 'learning_rate_Hydroxylation-K': 0.006516333480126604,
 'learning_rate_Hydroxylation-P': 0.009534796183902763,
 'log_base': 1.0827758177499827,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 943726160,
 'sample_weights': [1.895049924741754, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.58798735915205,
 'weight_decay_Hydroxylation-K': 2.2012499448954443,
 'weight_decay_Hydroxylation-P': 1.4506844123203484}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6864.090
[2,     1] loss: 6870.353
[3,     1] loss: 6804.504
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006476413132814382,
 'learning_rate_Hydroxylation-K': 0.00911601125811122,
 'learning_rate_Hydroxylation-P': 0.00937251379909801,
 'log_base': 1.0936399794288651,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1812983227,
 'sample_weights': [20.991905904917623, 2.624089324678599],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.057646758301136,
 'weight_decay_Hydroxylation-K': 8.761712029993621,
 'weight_decay_Hydroxylation-P': 5.998915734187861}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6058.173
[2,     1] loss: 6051.961
[3,     1] loss: 6058.951
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025258690180902403,
 'learning_rate_Hydroxylation-K': 0.0006097036168843835,
 'learning_rate_Hydroxylation-P': 0.006693687746035769,
 'log_base': 2.6013800393945106,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2543795300,
 'sample_weights': [18.650586406152215, 2.3314131127043023],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.695788216416884,
 'weight_decay_Hydroxylation-K': 6.852128577365738,
 'weight_decay_Hydroxylation-P': 3.378050400486977}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.395
[2,     1] loss: 1276.512
[3,     1] loss: 1276.905
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006809834542094453,
 'learning_rate_Hydroxylation-K': 0.004880511073179823,
 'learning_rate_Hydroxylation-P': 0.006328480261230904,
 'log_base': 1.3021888604522227,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3559357083,
 'sample_weights': [1.7462025650767041, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.465854488703801,
 'weight_decay_Hydroxylation-K': 4.6938834529137505,
 'weight_decay_Hydroxylation-P': 0.9754258815429289}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2252.239
[2,     1] loss: 2252.321
[3,     1] loss: 2255.483
[4,     1] loss: 2241.281
[5,     1] loss: 2235.190
[6,     1] loss: 2239.936
[7,     1] loss: 2239.357
[8,     1] loss: 2242.490
[9,     1] loss: 2238.780
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005403431111715874,
 'learning_rate_Hydroxylation-K': 0.005288415891147725,
 'learning_rate_Hydroxylation-P': 0.007963458952208338,
 'log_base': 1.8837654805369843,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2253807739,
 'sample_weights': [6.322532565190513, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.533670713871457,
 'weight_decay_Hydroxylation-K': 4.088220010404109,
 'weight_decay_Hydroxylation-P': 4.158991192133727}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1466.848
[2,     1] loss: 1467.646
[3,     1] loss: 1463.429
[4,     1] loss: 1460.384
[5,     1] loss: 1461.832
[6,     1] loss: 1455.982
[7,     1] loss: 1443.196
[8,     1] loss: 1426.625
[9,     1] loss: 1406.557
[10,     1] loss: 1350.398
[11,     1] loss: 1298.009
[12,     1] loss: 1241.132
[13,     1] loss: 1230.445
[14,     1] loss: 1325.812
[15,     1] loss: 1219.900
[16,     1] loss: 1310.013
[17,     1] loss: 1126.288
[18,     1] loss: 1215.629
[19,     1] loss: 1161.323
[20,     1] loss: 1192.364
[21,     1] loss: 1158.270
[22,     1] loss: 1213.603
[23,     1] loss: 1133.356
[24,     1] loss: 1137.708
[25,     1] loss: 1129.950
[26,     1] loss: 1112.625
[27,     1] loss: 1143.500
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022077238102640702,
 'learning_rate_Hydroxylation-K': 0.0003452768788484717,
 'learning_rate_Hydroxylation-P': 0.004451572775734908,
 'log_base': 2.103273487339521,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 883283272,
 'sample_weights': [2.636215293646697, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.557241945300847,
 'weight_decay_Hydroxylation-K': 3.7162652351800136,
 'weight_decay_Hydroxylation-P': 8.75070289955846}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1382.733
[2,     1] loss: 1384.033
[3,     1] loss: 1378.508
[4,     1] loss: 1383.745
[5,     1] loss: 1380.067
[6,     1] loss: 1383.358
[7,     1] loss: 1376.633
[8,     1] loss: 1375.953
[9,     1] loss: 1370.322
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005109849965134268,
 'learning_rate_Hydroxylation-K': 0.004560767328874323,
 'learning_rate_Hydroxylation-P': 0.009511652524314237,
 'log_base': 1.0157328378715598,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2675403095,
 'sample_weights': [2.245399490759364, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.868896926892472,
 'weight_decay_Hydroxylation-K': 3.6645475398533636,
 'weight_decay_Hydroxylation-P': 2.8173602793331343}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34733.547
Exploding loss, terminate run (best metric=0.5315613150596619)
Finished Training
Total time taken: 0.20000028610229492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34843.703
Exploding loss, terminate run (best metric=0.5281009078025818)
Finished Training
Total time taken: 0.25899648666381836
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34700.648
Exploding loss, terminate run (best metric=0.5270997285842896)
Finished Training
Total time taken: 0.21500086784362793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34770.605
Exploding loss, terminate run (best metric=0.5316047072410583)
Finished Training
Total time taken: 0.2350015640258789
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 34814.746
Exploding loss, terminate run (best metric=0.5321505665779114)
Finished Training
Total time taken: 0.2050004005432129
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34890.262
Exploding loss, terminate run (best metric=0.5414043068885803)
Finished Training
Total time taken: 0.23200225830078125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34756.414
Exploding loss, terminate run (best metric=0.5256488919258118)
Finished Training
Total time taken: 0.2200019359588623
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34665.539
Exploding loss, terminate run (best metric=0.5262751579284668)
Finished Training
Total time taken: 0.2570013999938965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34819.305
Exploding loss, terminate run (best metric=0.528649091720581)
Finished Training
Total time taken: 0.21400117874145508
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 34865.941
Exploding loss, terminate run (best metric=0.5275845527648926)
Finished Training
Total time taken: 0.2420048713684082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34667.660
Exploding loss, terminate run (best metric=0.54084312915802)
Finished Training
Total time taken: 0.23600077629089355
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34786.406
Exploding loss, terminate run (best metric=0.5303477048873901)
Finished Training
Total time taken: 0.2279980182647705
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34738.234
Exploding loss, terminate run (best metric=0.5304226279258728)
Finished Training
Total time taken: 0.23400044441223145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 34706.688
Exploding loss, terminate run (best metric=0.5271948575973511)
Finished Training
Total time taken: 0.23400044441223145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 34833.883
Exploding loss, terminate run (best metric=0.530168354511261)
Finished Training
Total time taken: 0.20300030708312988
{'Hydroxylation-K Validation Accuracy': 0.4805555555555555, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6137037037037038, 'Hydroxylation-K AUC PR': 0.31914864597585907, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1792282430213465, 'Validation Loss (Hydroxylation-K)': 0.5578403989473979, 'Hydroxylation-P Validation Accuracy': 0.4790681352892408, 'Hydroxylation-P Validation Sensitivity': 0.540952380952381, 'Hydroxylation-P Validation Specificity': 0.46625766871165647, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5879806517682472, 'Hydroxylation-P AUC PR': 0.2859877089095382, 'Hydroxylation-P MCC': 0.01753082780428111, 'Hydroxylation-P F1': 0.17335904834559088, 'Validation Loss (Hydroxylation-P)': 0.5306037267049154, 'Validation Loss (total)': 1.088444137573242, 'TimeToTrain': 0.22760074933369953}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003260082891479857,
 'learning_rate_Hydroxylation-K': 0.0040069050230007765,
 'learning_rate_Hydroxylation-P': 0.009724153523988819,
 'log_base': 1.3622119953372414,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2576728356,
 'sample_weights': [107.02389739355655, 13.350190080473418],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.252087883075667,
 'weight_decay_Hydroxylation-K': 0.8282252942834996,
 'weight_decay_Hydroxylation-P': 2.2410882999058046}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2048.990
[2,     1] loss: 2047.194
[3,     1] loss: 2049.212
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014834890795073447,
 'learning_rate_Hydroxylation-K': 0.009681665122151345,
 'learning_rate_Hydroxylation-P': 0.00930996808663421,
 'log_base': 1.0370660920118233,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 863538965,
 'sample_weights': [5.4008087123653254, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.162558735397823,
 'weight_decay_Hydroxylation-K': 2.8910912051718074,
 'weight_decay_Hydroxylation-P': 5.19084815493611}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14931.200
[2,     1] loss: 14890.357
[3,     1] loss: 14849.012
[4,     1] loss: 14901.592
[5,     1] loss: 14874.104
[6,     1] loss: 14891.119
[7,     1] loss: 14892.359
[8,     1] loss: 14911.926
[9,     1] loss: 14851.106
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004241863206033026,
 'learning_rate_Hydroxylation-K': 0.0028664233252990775,
 'learning_rate_Hydroxylation-P': 0.00894331386470474,
 'log_base': 2.9662656259403204,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 210002544,
 'sample_weights': [45.8692904068961, 5.733882184519578],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.994150219014533,
 'weight_decay_Hydroxylation-K': 9.851133611772703,
 'weight_decay_Hydroxylation-P': 2.931080569225003}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.013
[2,     1] loss: 1233.173
[3,     1] loss: 1232.408
[4,     1] loss: 1229.716
[5,     1] loss: 1228.366
[6,     1] loss: 1218.833
[7,     1] loss: 1210.355
[8,     1] loss: 1194.052
[9,     1] loss: 1158.513
[10,     1] loss: 1129.123
[11,     1] loss: 1069.734
[12,     1] loss: 1060.103
[13,     1] loss: 1032.694
[14,     1] loss: 982.376
[15,     1] loss: 1059.310
[16,     1] loss: 995.755
[17,     1] loss: 996.604
[18,     1] loss: 949.144
[19,     1] loss: 969.297
[20,     1] loss: 937.173
[21,     1] loss: 1008.296
[22,     1] loss: 927.967
[23,     1] loss: 980.728
[24,     1] loss: 926.334
[25,     1] loss: 952.826
[26,     1] loss: 914.998
[27,     1] loss: 958.597
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0043382817390605,
 'learning_rate_Hydroxylation-K': 0.003355235000844211,
 'learning_rate_Hydroxylation-P': 0.006518847801394585,
 'log_base': 2.5834437585648473,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1536328149,
 'sample_weights': [1.5353971470941568, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.620934767917005,
 'weight_decay_Hydroxylation-K': 9.548646326005118,
 'weight_decay_Hydroxylation-P': 1.9417239116413496}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.008
[2,     1] loss: 1280.059
[3,     1] loss: 1277.138
[4,     1] loss: 1287.192
[5,     1] loss: 1279.647
[6,     1] loss: 1282.281
[7,     1] loss: 1276.435
[8,     1] loss: 1272.863
[9,     1] loss: 1273.635
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009571864392400408,
 'learning_rate_Hydroxylation-K': 0.006466988943635256,
 'learning_rate_Hydroxylation-P': 0.006182632892891691,
 'log_base': 2.740569507231584,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 73362952,
 'sample_weights': [1.7589317943431764, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.153818916858853,
 'weight_decay_Hydroxylation-K': 9.262461641700787,
 'weight_decay_Hydroxylation-P': 7.496835627024563}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.469
[2,     1] loss: 1260.633
[3,     1] loss: 1251.395
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00571863095637587,
 'learning_rate_Hydroxylation-K': 0.0037571695206209343,
 'learning_rate_Hydroxylation-P': 0.007895205561403565,
 'log_base': 1.1032266303805955,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1170580918,
 'sample_weights': [1.6559213110301776, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.076611772018843,
 'weight_decay_Hydroxylation-K': 4.403850860870036,
 'weight_decay_Hydroxylation-P': 1.676626670303515}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5526.668
[2,     1] loss: 5538.103
[3,     1] loss: 5492.932
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034124844139715406,
 'learning_rate_Hydroxylation-K': 0.004521536578052138,
 'learning_rate_Hydroxylation-P': 0.0077766025343202734,
 'log_base': 1.018162786140889,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1916023619,
 'sample_weights': [16.99365811416066, 2.124289097263931],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.392528195571332,
 'weight_decay_Hydroxylation-K': 4.2608910699976805,
 'weight_decay_Hydroxylation-P': 3.6569217314673805}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30146.166
Exploding loss, terminate run (best metric=0.5320100784301758)
Finished Training
Total time taken: 0.20799732208251953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30067.348
Exploding loss, terminate run (best metric=0.5273703932762146)
Finished Training
Total time taken: 0.19500136375427246
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30004.402
Exploding loss, terminate run (best metric=0.5267296433448792)
Finished Training
Total time taken: 0.24399924278259277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30198.541
Exploding loss, terminate run (best metric=0.5277047157287598)
Finished Training
Total time taken: 0.20999932289123535
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 30182.893
Exploding loss, terminate run (best metric=0.5340345501899719)
Finished Training
Total time taken: 0.22700047492980957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30151.908
Exploding loss, terminate run (best metric=0.5330339670181274)
Finished Training
Total time taken: 0.221998929977417
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30111.570
Exploding loss, terminate run (best metric=0.5326402187347412)
Finished Training
Total time taken: 0.2260143756866455
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30157.672
Exploding loss, terminate run (best metric=0.5288075804710388)
Finished Training
Total time taken: 0.20499897003173828
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30084.975
Exploding loss, terminate run (best metric=0.5268111228942871)
Finished Training
Total time taken: 0.22499895095825195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 30243.252
Exploding loss, terminate run (best metric=0.5291256904602051)
Finished Training
Total time taken: 0.20000100135803223
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30166.033
Exploding loss, terminate run (best metric=0.5350351929664612)
Finished Training
Total time taken: 0.2369990348815918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29859.320
Exploding loss, terminate run (best metric=0.5408425331115723)
Finished Training
Total time taken: 0.2030029296875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30351.613
Exploding loss, terminate run (best metric=0.526451826095581)
Finished Training
Total time taken: 0.2049999237060547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30239.535
Exploding loss, terminate run (best metric=0.5275969505310059)
Finished Training
Total time taken: 0.2200028896331787
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 30286.227
Exploding loss, terminate run (best metric=0.5275497436523438)
Finished Training
Total time taken: 0.2690005302429199
{'Hydroxylation-K Validation Accuracy': 0.35939716312056735, 'Hydroxylation-K Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-K Validation Specificity': 0.26666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6058284600389864, 'Hydroxylation-K AUC PR': 0.34095214912746336, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.24507389162561577, 'Validation Loss (Hydroxylation-K)': 0.5566198468208313, 'Hydroxylation-P Validation Accuracy': 0.3500826692384481, 'Hydroxylation-P Validation Sensitivity': 0.7314285714285714, 'Hydroxylation-P Validation Specificity': 0.2682926829268293, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.628035604435799, 'Hydroxylation-P AUC PR': 0.32270894943293, 'Hydroxylation-P MCC': -0.0019084119076789258, 'Hydroxylation-P F1': 0.22019254051045314, 'Validation Loss (Hydroxylation-P)': 0.5303829471270244, 'Validation Loss (total)': 1.0870028018951416, 'TimeToTrain': 0.21973435084025064}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002982790753724096,
 'learning_rate_Hydroxylation-K': 0.006524127593304244,
 'learning_rate_Hydroxylation-P': 0.005881767701064681,
 'log_base': 2.758840621608357,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3299269201,
 'sample_weights': [92.81660449603895, 11.577968498845244],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.111188080206655,
 'weight_decay_Hydroxylation-K': 5.518958501678714,
 'weight_decay_Hydroxylation-P': 7.615634507412132}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.953
[2,     1] loss: 1260.967
[3,     1] loss: 1250.676
[4,     1] loss: 1254.580
[5,     1] loss: 1251.886
[6,     1] loss: 1247.731
[7,     1] loss: 1241.561
[8,     1] loss: 1227.037
[9,     1] loss: 1205.014
[10,     1] loss: 1167.785
[11,     1] loss: 1154.187
[12,     1] loss: 1105.430
[13,     1] loss: 1095.263
[14,     1] loss: 1060.849
[15,     1] loss: 1044.146
[16,     1] loss: 1057.785
[17,     1] loss: 1084.373
[18,     1] loss: 1029.808
[19,     1] loss: 1008.688
[20,     1] loss: 974.308
[21,     1] loss: 1015.051
[22,     1] loss: 959.548
[23,     1] loss: 1029.793
[24,     1] loss: 948.679
[25,     1] loss: 952.571
[26,     1] loss: 961.798
[27,     1] loss: 951.813
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00021950782337639687,
 'learning_rate_Hydroxylation-K': 0.005233200887227519,
 'learning_rate_Hydroxylation-P': 0.009117677637969928,
 'log_base': 2.273934562786614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3973354003,
 'sample_weights': [1.6450786656874163, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.2326650759820685,
 'weight_decay_Hydroxylation-K': 9.07542591566774,
 'weight_decay_Hydroxylation-P': 2.0353850287340034}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1342.098
[2,     1] loss: 1340.464
[3,     1] loss: 1335.403
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0045352919461461106,
 'learning_rate_Hydroxylation-K': 0.005577852765672716,
 'learning_rate_Hydroxylation-P': 0.009568486984435192,
 'log_base': 1.856557416328528,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3290565719,
 'sample_weights': [2.0321601174146435, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.900621496818188,
 'weight_decay_Hydroxylation-K': 6.627240301044655,
 'weight_decay_Hydroxylation-P': 9.304260618689508}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1484.493
[2,     1] loss: 1479.051
[3,     1] loss: 1479.501
[4,     1] loss: 1477.754
[5,     1] loss: 1474.399
[6,     1] loss: 1473.215
[7,     1] loss: 1472.318
[8,     1] loss: 1472.160
[9,     1] loss: 1470.032
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024336328380662607,
 'learning_rate_Hydroxylation-K': 0.0053564286821192,
 'learning_rate_Hydroxylation-P': 0.008775606685027887,
 'log_base': 1.0579518582499923,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 94598505,
 'sample_weights': [2.6982036572780443, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.816276591889952,
 'weight_decay_Hydroxylation-K': 4.568063257965721,
 'weight_decay_Hydroxylation-P': 2.345304403289831}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9611.797
[2,     1] loss: 9645.385
[3,     1] loss: 9571.191
[4,     1] loss: 9572.204
[5,     1] loss: 9643.809
[6,     1] loss: 9582.524
[7,     1] loss: 9574.618
[8,     1] loss: 9579.190
[9,     1] loss: 9585.762
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024477377801897255,
 'learning_rate_Hydroxylation-K': 0.007646723148073642,
 'learning_rate_Hydroxylation-P': 0.009578124040830907,
 'log_base': 2.3402265201004155,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 746889748,
 'sample_weights': [29.634298236165023, 3.7044299835416785],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6397250078246257,
 'weight_decay_Hydroxylation-K': 9.105823667924035,
 'weight_decay_Hydroxylation-P': 2.471751149648496}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1321.590
[2,     1] loss: 1323.487
[3,     1] loss: 1316.586
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006762192014930543,
 'learning_rate_Hydroxylation-K': 0.003269661679157352,
 'learning_rate_Hydroxylation-P': 0.002682625543854308,
 'log_base': 2.443588675098797,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 652279420,
 'sample_weights': [1.9634785159186738, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.605562494085657,
 'weight_decay_Hydroxylation-K': 3.41616183443824,
 'weight_decay_Hydroxylation-P': 9.66288927271469}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1304.453
[2,     1] loss: 1307.067
[3,     1] loss: 1304.452
[4,     1] loss: 1307.199
[5,     1] loss: 1301.000
[6,     1] loss: 1301.420
[7,     1] loss: 1302.031
[8,     1] loss: 1300.893
[9,     1] loss: 1299.580
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005292670026304169,
 'learning_rate_Hydroxylation-K': 0.0031422884225141817,
 'learning_rate_Hydroxylation-P': 0.00990939392816845,
 'log_base': 1.0858850150999257,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2475338788,
 'sample_weights': [1.8684985439028354, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.807574097618236,
 'weight_decay_Hydroxylation-K': 4.067224193483922,
 'weight_decay_Hydroxylation-P': 3.545964861027413}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6593.458
[2,     1] loss: 6566.171
[3,     1] loss: 6550.155
[4,     1] loss: 6537.935
[5,     1] loss: 6592.611
[6,     1] loss: 6545.808
[7,     1] loss: 6568.927
[8,     1] loss: 6552.975
[9,     1] loss: 6512.154
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006928211227279253,
 'learning_rate_Hydroxylation-K': 0.008185516740004147,
 'learning_rate_Hydroxylation-P': 0.007391075728944766,
 'log_base': 2.773576669826248,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3242126438,
 'sample_weights': [20.26137905153373, 2.5327699501521224],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.698074975346691,
 'weight_decay_Hydroxylation-K': 9.5779941375908,
 'weight_decay_Hydroxylation-P': 9.67763518788799}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.690
[2,     1] loss: 1268.441
[3,     1] loss: 1251.091
[4,     1] loss: 1255.991
[5,     1] loss: 1249.604
[6,     1] loss: 1254.077
[7,     1] loss: 1251.508
[8,     1] loss: 1252.501
[9,     1] loss: 1253.402
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007250225975073473,
 'learning_rate_Hydroxylation-K': 0.003364392087050919,
 'learning_rate_Hydroxylation-P': 0.009780251357831171,
 'log_base': 1.0221723546558827,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2463354414,
 'sample_weights': [1.6364880360169338, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.53044244282513,
 'weight_decay_Hydroxylation-K': 8.356693190008286,
 'weight_decay_Hydroxylation-P': 6.783595392689398}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24709.633
Exploding loss, terminate run (best metric=0.5329840779304504)
Finished Training
Total time taken: 0.21300268173217773
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24744.490
Exploding loss, terminate run (best metric=0.5280948281288147)
Finished Training
Total time taken: 0.23400139808654785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24676.117
Exploding loss, terminate run (best metric=0.5391442179679871)
Finished Training
Total time taken: 0.22100067138671875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24638.000
Exploding loss, terminate run (best metric=0.5311420559883118)
Finished Training
Total time taken: 0.21500039100646973
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24722.355
Exploding loss, terminate run (best metric=0.5299527049064636)
Finished Training
Total time taken: 0.2160036563873291
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24643.535
Exploding loss, terminate run (best metric=0.5319331884384155)
Finished Training
Total time taken: 0.22400426864624023
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24816.621
Exploding loss, terminate run (best metric=0.5269753932952881)
Finished Training
Total time taken: 0.24299883842468262
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24720.082
Exploding loss, terminate run (best metric=0.5331946015357971)
Finished Training
Total time taken: 0.23000311851501465
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24722.969
Exploding loss, terminate run (best metric=0.5443413257598877)
Finished Training
Total time taken: 0.22499752044677734
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24743.828
Exploding loss, terminate run (best metric=0.5392577648162842)
Finished Training
Total time taken: 0.22800064086914062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24678.928
Exploding loss, terminate run (best metric=0.5427131056785583)
Finished Training
Total time taken: 0.20399856567382812
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24710.375
Exploding loss, terminate run (best metric=0.528526246547699)
Finished Training
Total time taken: 0.2050004005432129
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24673.625
Exploding loss, terminate run (best metric=0.5421592593193054)
Finished Training
Total time taken: 0.20499944686889648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24662.855
Exploding loss, terminate run (best metric=0.5456703901290894)
Finished Training
Total time taken: 0.21000003814697266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24766.758
Exploding loss, terminate run (best metric=0.5407248735427856)
Finished Training
Total time taken: 0.21599817276000977
{'Hydroxylation-K Validation Accuracy': 0.561968085106383, 'Hydroxylation-K Validation Sensitivity': 0.4066666666666667, 'Hydroxylation-K Validation Specificity': 0.6, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6107407407407407, 'Hydroxylation-K AUC PR': 0.32204337856520593, 'Hydroxylation-K MCC': 0.018956214323163814, 'Hydroxylation-K F1': 0.14693237796686073, 'Validation Loss (Hydroxylation-K)': 0.5603415807088216, 'Hydroxylation-P Validation Accuracy': 0.5654901950831599, 'Hydroxylation-P Validation Sensitivity': 0.40925925925925927, 'Hydroxylation-P Validation Specificity': 0.5991869918699188, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5650958039707479, 'Hydroxylation-P AUC PR': 0.27580185272819857, 'Hydroxylation-P MCC': 0.01765669803986899, 'Hydroxylation-P F1': 0.1356435510952099, 'Validation Loss (Hydroxylation-P)': 0.5357876022656759, 'Validation Loss (total)': 1.0961291948954264, 'TimeToTrain': 0.21926732063293458}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023899643948985306,
 'learning_rate_Hydroxylation-K': 0.0008340438264558209,
 'learning_rate_Hydroxylation-P': 0.007585480045496167,
 'log_base': 1.338391892713706,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 527995128,
 'sample_weights': [76.18204470666596, 9.50296898470403],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.175106699205012,
 'weight_decay_Hydroxylation-K': 7.811641411567966,
 'weight_decay_Hydroxylation-P': 2.3069567203542594}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2120.040
[2,     1] loss: 2114.056
[3,     1] loss: 2120.531
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00559614877212391,
 'learning_rate_Hydroxylation-K': 0.00476078592066368,
 'learning_rate_Hydroxylation-P': 0.006179936327369473,
 'log_base': 2.452622044827836,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1183654209,
 'sample_weights': [5.727690482380682, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.836702516169315,
 'weight_decay_Hydroxylation-K': 8.731834653106729,
 'weight_decay_Hydroxylation-P': 6.0706155166452715}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1304.513
[2,     1] loss: 1300.817
[3,     1] loss: 1299.972
[4,     1] loss: 1299.904
[5,     1] loss: 1297.323
[6,     1] loss: 1290.255
[7,     1] loss: 1284.856
[8,     1] loss: 1259.554
[9,     1] loss: 1226.340
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004034333809668519,
 'learning_rate_Hydroxylation-K': 0.006943563566868807,
 'learning_rate_Hydroxylation-P': 0.00980589392870056,
 'log_base': 1.383396451612116,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 100132206,
 'sample_weights': [1.8608135387845237, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.141268535665844,
 'weight_decay_Hydroxylation-K': 3.381404107492516,
 'weight_decay_Hydroxylation-P': 5.339319569691465}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1989.678
[2,     1] loss: 1995.694
[3,     1] loss: 1995.571
[4,     1] loss: 1988.922
[5,     1] loss: 1994.762
[6,     1] loss: 1992.471
[7,     1] loss: 1988.301
[8,     1] loss: 1975.305
[9,     1] loss: 1971.914
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0056703246487623486,
 'learning_rate_Hydroxylation-K': 0.005177700182344571,
 'learning_rate_Hydroxylation-P': 0.007603593470237037,
 'log_base': 1.2276711472915618,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3715429497,
 'sample_weights': [5.144002420648074, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.93417903360322,
 'weight_decay_Hydroxylation-K': 4.1679455146481015,
 'weight_decay_Hydroxylation-P': 3.956132290005343}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2656.911
[2,     1] loss: 2649.721
[3,     1] loss: 2647.117
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038009240528310098,
 'learning_rate_Hydroxylation-K': 0.0032704507458544,
 'learning_rate_Hydroxylation-P': 0.009542470076188561,
 'log_base': 2.6844482094267734,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3141728620,
 'sample_weights': [8.138900636523294, 1.0174017724573274],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.718286144935874,
 'weight_decay_Hydroxylation-K': 3.773532639057182,
 'weight_decay_Hydroxylation-P': 8.493687604677511}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.189
[2,     1] loss: 1262.191
[3,     1] loss: 1262.307
[4,     1] loss: 1257.146
[5,     1] loss: 1250.720
[6,     1] loss: 1232.788
[7,     1] loss: 1220.155
[8,     1] loss: 1182.643
[9,     1] loss: 1121.637
[10,     1] loss: 1104.475
[11,     1] loss: 1082.838
[12,     1] loss: 1033.569
[13,     1] loss: 1030.517
[14,     1] loss: 1007.414
[15,     1] loss: 1052.753
[16,     1] loss: 1018.117
[17,     1] loss: 1018.026
[18,     1] loss: 1030.884
[19,     1] loss: 974.487
[20,     1] loss: 986.120
[21,     1] loss: 987.160
[22,     1] loss: 919.720
[23,     1] loss: 936.046
[24,     1] loss: 944.493
[25,     1] loss: 906.912
[26,     1] loss: 893.500
[27,     1] loss: 987.046
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013634461947326245,
 'learning_rate_Hydroxylation-K': 0.009382347391880397,
 'learning_rate_Hydroxylation-P': 0.009061441530387295,
 'log_base': 1.1382191824796508,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3728678108,
 'sample_weights': [1.6906178002102783, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.514903152584732,
 'weight_decay_Hydroxylation-K': 7.147481855499301,
 'weight_decay_Hydroxylation-P': 4.587015982941869}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4172.456
[2,     1] loss: 4206.887
[3,     1] loss: 4168.914
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010427468295252326,
 'learning_rate_Hydroxylation-K': 0.004907716440831522,
 'learning_rate_Hydroxylation-P': 0.0016975691846165333,
 'log_base': 2.945531904927577,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3144130887,
 'sample_weights': [12.894945923339263, 1.6119303360546937],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.77185279206189,
 'weight_decay_Hydroxylation-K': 5.208421844463359,
 'weight_decay_Hydroxylation-P': 8.988592611753488}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.952
[2,     1] loss: 1235.201
[3,     1] loss: 1236.366
[4,     1] loss: 1235.547
[5,     1] loss: 1237.097
[6,     1] loss: 1235.612
[7,     1] loss: 1233.458
[8,     1] loss: 1234.524
[9,     1] loss: 1232.331
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005768059560253565,
 'learning_rate_Hydroxylation-K': 0.005639192373520931,
 'learning_rate_Hydroxylation-P': 0.00940016468717491,
 'log_base': 1.0707679944404833,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2677578571,
 'sample_weights': [1.5453665714880507, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.333821836390729,
 'weight_decay_Hydroxylation-K': 4.699786447679814,
 'weight_decay_Hydroxylation-P': 4.933022769456796}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8000.592
[2,     1] loss: 7936.417
[3,     1] loss: 7914.828
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006261432448558634,
 'learning_rate_Hydroxylation-K': 0.004007081711808811,
 'learning_rate_Hydroxylation-P': 0.0055626801584173075,
 'log_base': 1.0417069266814443,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 241857826,
 'sample_weights': [24.415579446397793, 3.0520650040702475],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.473338360401086,
 'weight_decay_Hydroxylation-K': 4.434673091408644,
 'weight_decay_Hydroxylation-P': 7.536629508007606}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13207.137
[2,     1] loss: 13285.854
[3,     1] loss: 13175.557
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007071015552924723,
 'learning_rate_Hydroxylation-K': 0.004621855434110048,
 'learning_rate_Hydroxylation-P': 0.0037972944110509555,
 'log_base': 1.3054771876657958,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3942017625,
 'sample_weights': [40.85699609741702, 5.107321259121654],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.245045120684075,
 'weight_decay_Hydroxylation-K': 8.2434255271773,
 'weight_decay_Hydroxylation-P': 5.7048919359888055}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2234.709
[2,     1] loss: 2245.830
[3,     1] loss: 2230.501
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005394440050271128,
 'learning_rate_Hydroxylation-K': 0.007039273911168284,
 'learning_rate_Hydroxylation-P': 0.009472466350548193,
 'log_base': 2.2870597215795696,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4080939551,
 'sample_weights': [6.2627140943524084, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.995337901669586,
 'weight_decay_Hydroxylation-K': 8.5001872052573,
 'weight_decay_Hydroxylation-P': 3.6758844946590354}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1336.481
[2,     1] loss: 1339.535
[3,     1] loss: 1331.284
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003452764088040401,
 'learning_rate_Hydroxylation-K': 0.0054764817905345485,
 'learning_rate_Hydroxylation-P': 0.004663464847641269,
 'log_base': 1.2367795636498498,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 318470055,
 'sample_weights': [2.0180221023098905, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.973918303442335,
 'weight_decay_Hydroxylation-K': 3.588365873991922,
 'weight_decay_Hydroxylation-P': 2.504617403127834}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2599.445
[2,     1] loss: 2569.600
[3,     1] loss: 2573.647
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007339911961128381,
 'learning_rate_Hydroxylation-K': 0.00984739259578181,
 'learning_rate_Hydroxylation-P': 0.007091459840418678,
 'log_base': 2.798662373304095,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1448983152,
 'sample_weights': [7.855801009151583, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.517706892752251,
 'weight_decay_Hydroxylation-K': 6.744012642056296,
 'weight_decay_Hydroxylation-P': 2.4329212443139614}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.348
[2,     1] loss: 1250.121
[3,     1] loss: 1248.432
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007192088462775112,
 'learning_rate_Hydroxylation-K': 0.003179908612578878,
 'learning_rate_Hydroxylation-P': 0.0024938086783488377,
 'log_base': 2.2360443265248917,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 682609939,
 'sample_weights': [1.6221705362418448, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.4867670929795409,
 'weight_decay_Hydroxylation-K': 6.945376340557015,
 'weight_decay_Hydroxylation-P': 2.37776236402258}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1346.077
[2,     1] loss: 1345.454
[3,     1] loss: 1348.674
[4,     1] loss: 1347.987
[5,     1] loss: 1353.222
[6,     1] loss: 1342.996
[7,     1] loss: 1341.628
[8,     1] loss: 1341.448
[9,     1] loss: 1334.982
[10,     1] loss: 1328.894
[11,     1] loss: 1320.246
[12,     1] loss: 1294.408
[13,     1] loss: 1278.807
[14,     1] loss: 1241.453
[15,     1] loss: 1205.121
[16,     1] loss: 1173.103
[17,     1] loss: 1155.900
[18,     1] loss: 1136.971
[19,     1] loss: 1128.651
[20,     1] loss: 1086.096
[21,     1] loss: 1104.087
[22,     1] loss: 1101.774
[23,     1] loss: 1088.228
[24,     1] loss: 1056.357
[25,     1] loss: 1063.315
[26,     1] loss: 1073.610
[27,     1] loss: 1075.087
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006699983505899527,
 'learning_rate_Hydroxylation-K': 0.002373332763826423,
 'learning_rate_Hydroxylation-P': 0.007733508323444905,
 'log_base': 1.239072095312973,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4160908007,
 'sample_weights': [2.074593965632449, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.558432611144096,
 'weight_decay_Hydroxylation-K': 6.523547058053328,
 'weight_decay_Hydroxylation-P': 0.1386174121619278}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2553.561
[2,     1] loss: 2543.533
[3,     1] loss: 2559.013
[4,     1] loss: 2541.548
[5,     1] loss: 2564.394
[6,     1] loss: 2548.261
[7,     1] loss: 2556.578
[8,     1] loss: 2548.157
[9,     1] loss: 2547.251
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018307077299247148,
 'learning_rate_Hydroxylation-K': 0.003444330169605564,
 'learning_rate_Hydroxylation-P': 0.004343695553810088,
 'log_base': 2.6536100410079504,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1337791050,
 'sample_weights': [7.7879334989722775, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.387033740865734,
 'weight_decay_Hydroxylation-K': 1.2083318249228232,
 'weight_decay_Hydroxylation-P': 9.836120601140705}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.291
[2,     1] loss: 1270.151
[3,     1] loss: 1268.132
[4,     1] loss: 1268.918
[5,     1] loss: 1267.362
[6,     1] loss: 1266.785
[7,     1] loss: 1267.345
[8,     1] loss: 1264.010
[9,     1] loss: 1262.143
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007393582587202109,
 'learning_rate_Hydroxylation-K': 0.008100630074457544,
 'learning_rate_Hydroxylation-P': 0.006061311801611251,
 'log_base': 1.3621990932657702,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1385838933,
 'sample_weights': [1.710633504296057, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.940573038936359,
 'weight_decay_Hydroxylation-K': 7.331095977758878,
 'weight_decay_Hydroxylation-P': 0.49739681648276823}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2038.701
[2,     1] loss: 2051.203
[3,     1] loss: 2101.162
[4,     1] loss: 2038.874
[5,     1] loss: 2050.151
[6,     1] loss: 2045.836
[7,     1] loss: 2053.549
[8,     1] loss: 2048.169
[9,     1] loss: 2045.800
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004376952875337508,
 'learning_rate_Hydroxylation-K': 0.004578165118533348,
 'learning_rate_Hydroxylation-P': 0.0030200042149694565,
 'log_base': 2.425545020642986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1826144888,
 'sample_weights': [5.40097420400714, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.547709445897663,
 'weight_decay_Hydroxylation-K': 9.52231299139895,
 'weight_decay_Hydroxylation-P': 6.111762328764754}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1305.849
[2,     1] loss: 1307.443
[3,     1] loss: 1303.303
[4,     1] loss: 1304.714
[5,     1] loss: 1304.687
[6,     1] loss: 1301.116
[7,     1] loss: 1297.320
[8,     1] loss: 1288.693
[9,     1] loss: 1284.115
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006020236992171718,
 'learning_rate_Hydroxylation-K': 0.008926461448835573,
 'learning_rate_Hydroxylation-P': 0.0076170697560962826,
 'log_base': 1.1913244899989208,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 611489512,
 'sample_weights': [1.8841277261791531, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.200739942839466,
 'weight_decay_Hydroxylation-K': 2.9066528683112076,
 'weight_decay_Hydroxylation-P': 7.465318702758444}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3110.108
[2,     1] loss: 3110.485
[3,     1] loss: 3092.482
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005274667734883542,
 'learning_rate_Hydroxylation-K': 0.00987877872286132,
 'learning_rate_Hydroxylation-P': 0.008016397629000591,
 'log_base': 1.3400827393486172,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1359354038,
 'sample_weights': [9.536094734055096, 1.1920577628397089],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.08791223698444,
 'weight_decay_Hydroxylation-K': 8.495077339518796,
 'weight_decay_Hydroxylation-P': 1.3627028234561676}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2113.984
[2,     1] loss: 2116.343
[3,     1] loss: 2106.738
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0055344409186879485,
 'learning_rate_Hydroxylation-K': 0.008766229345914872,
 'learning_rate_Hydroxylation-P': 0.00476557030161168,
 'log_base': 2.7004477752453124,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3207589722,
 'sample_weights': [5.702987065502013, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.002456059964011,
 'weight_decay_Hydroxylation-K': 4.1177917674915605,
 'weight_decay_Hydroxylation-P': 2.3964871846024005}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.143
[2,     1] loss: 1262.127
[3,     1] loss: 1259.935
[4,     1] loss: 1256.599
[5,     1] loss: 1257.092
[6,     1] loss: 1243.220
[7,     1] loss: 1221.119
[8,     1] loss: 1171.514
[9,     1] loss: 1166.577
[10,     1] loss: 1083.861
[11,     1] loss: 1086.499
[12,     1] loss: 1028.239
[13,     1] loss: 1085.896
[14,     1] loss: 986.434
[15,     1] loss: 1067.351
[16,     1] loss: 929.683
[17,     1] loss: 1070.692
[18,     1] loss: 981.134
[19,     1] loss: 973.172
[20,     1] loss: 1029.382
[21,     1] loss: 977.687
[22,     1] loss: 1008.940
[23,     1] loss: 1018.614
[24,     1] loss: 901.462
[25,     1] loss: 956.070
[26,     1] loss: 927.065
[27,     1] loss: 898.548
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001845416157163358,
 'learning_rate_Hydroxylation-K': 0.00569563570488678,
 'learning_rate_Hydroxylation-P': 0.004351973282999005,
 'log_base': 2.465277771385651,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3434468433,
 'sample_weights': [1.6805048996090417, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.20241719959513205,
 'weight_decay_Hydroxylation-K': 9.818177753397947,
 'weight_decay_Hydroxylation-P': 5.764588539052644}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.526
[2,     1] loss: 1303.433
[3,     1] loss: 1295.664
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0063204606244240905,
 'learning_rate_Hydroxylation-K': 0.006402529839562906,
 'learning_rate_Hydroxylation-P': 0.008105177053045306,
 'log_base': 1.7403600315416796,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3696993003,
 'sample_weights': [1.8501993184219745, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.40210221980128,
 'weight_decay_Hydroxylation-K': 1.3004028480772876,
 'weight_decay_Hydroxylation-P': 9.700122222429835}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1546.250
[2,     1] loss: 1539.667
[3,     1] loss: 1547.234
[4,     1] loss: 1544.222
[5,     1] loss: 1540.132
[6,     1] loss: 1545.405
[7,     1] loss: 1543.132
[8,     1] loss: 1541.340
[9,     1] loss: 1540.097
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005418647133024736,
 'learning_rate_Hydroxylation-K': 0.0034753554333861103,
 'learning_rate_Hydroxylation-P': 0.006215756905109375,
 'log_base': 1.1293509398176105,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3693234351,
 'sample_weights': [3.0129349058180797, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.740365691863786,
 'weight_decay_Hydroxylation-K': 1.061046522053271,
 'weight_decay_Hydroxylation-P': 2.7329626633438346}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4462.988
[2,     1] loss: 4431.244
[3,     1] loss: 4488.025
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005561369606792831,
 'learning_rate_Hydroxylation-K': 0.006332102808885514,
 'learning_rate_Hydroxylation-P': 0.006577940859866898,
 'log_base': 2.626641445971626,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1748979003,
 'sample_weights': [13.724111332195195, 1.7155800050093315],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.929301526810371,
 'weight_decay_Hydroxylation-K': 9.792207777005498,
 'weight_decay_Hydroxylation-P': 2.822277128354326}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.542
[2,     1] loss: 1270.984
[3,     1] loss: 1273.906
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009321444919301383,
 'learning_rate_Hydroxylation-K': 0.007211194953254508,
 'learning_rate_Hydroxylation-P': 0.006542169588600348,
 'log_base': 2.6031314397144376,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2439030354,
 'sample_weights': [1.7287281266905379, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.572331292484306,
 'weight_decay_Hydroxylation-K': 9.875812740995938,
 'weight_decay_Hydroxylation-P': 9.648052026193191}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1278.648
[2,     1] loss: 1281.605
[3,     1] loss: 1281.011
[4,     1] loss: 1274.464
[5,     1] loss: 1278.070
[6,     1] loss: 1274.567
[7,     1] loss: 1274.001
[8,     1] loss: 1271.677
[9,     1] loss: 1274.833
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00598911712707368,
 'learning_rate_Hydroxylation-K': 0.0012913751487406112,
 'learning_rate_Hydroxylation-P': 0.009126154748690407,
 'log_base': 1.852161326388157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4286096301,
 'sample_weights': [1.7449741434601964, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.371155352032186,
 'weight_decay_Hydroxylation-K': 5.82702788401483,
 'weight_decay_Hydroxylation-P': 7.716829394312741}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1483.847
[2,     1] loss: 1484.961
[3,     1] loss: 1478.369
[4,     1] loss: 1477.844
[5,     1] loss: 1478.465
[6,     1] loss: 1478.886
[7,     1] loss: 1475.296
[8,     1] loss: 1476.659
[9,     1] loss: 1467.742
[10,     1] loss: 1470.233
[11,     1] loss: 1460.283
[12,     1] loss: 1450.368
[13,     1] loss: 1433.260
[14,     1] loss: 1410.543
[15,     1] loss: 1364.157
[16,     1] loss: 1327.357
[17,     1] loss: 1341.722
[18,     1] loss: 1270.120
[19,     1] loss: 1305.149
[20,     1] loss: 1222.808
[21,     1] loss: 1270.432
[22,     1] loss: 1243.801
[23,     1] loss: 1204.157
[24,     1] loss: 1284.610
[25,     1] loss: 1233.453
[26,     1] loss: 1173.830
[27,     1] loss: 1191.300
[28,     1] loss: 1200.595
[29,     1] loss: 1159.662
[30,     1] loss: 1136.234
[31,     1] loss: 1155.118
[32,     1] loss: 1095.273
[33,     1] loss: 1062.455
[34,     1] loss: 1100.689
[35,     1] loss: 1101.869
[36,     1] loss: 1136.942
[37,     1] loss: 1216.733
[38,     1] loss: 1046.967
[39,     1] loss: 1172.045
[40,     1] loss: 1087.243
[41,     1] loss: 1071.520
[42,     1] loss: 1176.603
[43,     1] loss: 1045.488
[44,     1] loss: 1188.885
[45,     1] loss: 995.775
[46,     1] loss: 1117.467
[47,     1] loss: 1131.337
[48,     1] loss: 997.573
[49,     1] loss: 1091.494
[50,     1] loss: 966.729
[51,     1] loss: 975.814
[52,     1] loss: 932.730
[53,     1] loss: 906.333
[54,     1] loss: 949.181
[55,     1] loss: 878.980
[56,     1] loss: 909.463
[57,     1] loss: 872.911
[58,     1] loss: 928.492
[59,     1] loss: 964.908
[60,     1] loss: 819.796
[61,     1] loss: 883.323
[62,     1] loss: 868.410
[63,     1] loss: 984.639
[64,     1] loss: 803.134
[65,     1] loss: 983.358
[66,     1] loss: 789.878
[67,     1] loss: 954.487
[68,     1] loss: 805.538
[69,     1] loss: 923.670
[70,     1] loss: 999.487
[71,     1] loss: 809.314
[72,     1] loss: 932.156
[73,     1] loss: 778.578
[74,     1] loss: 868.468
[75,     1] loss: 763.699
[76,     1] loss: 886.005
[77,     1] loss: 703.217
[78,     1] loss: 805.377
[79,     1] loss: 668.894
[80,     1] loss: 685.074
[81,     1] loss: 694.743
[82,     1] loss: 717.922
[83,     1] loss: 709.068
[84,     1] loss: 593.180
[85,     1] loss: 622.962
[86,     1] loss: 656.997
[87,     1] loss: 705.936
[88,     1] loss: 660.401
[89,     1] loss: 623.419
[90,     1] loss: 555.200
[91,     1] loss: 557.038
[92,     1] loss: 595.497
[93,     1] loss: 709.163
[94,     1] loss: 1094.775
[95,     1] loss: 815.033
[96,     1] loss: 660.103
[97,     1] loss: 741.764
[98,     1] loss: 658.183
[99,     1] loss: 678.028
[100,     1] loss: 612.092
[101,     1] loss: 611.129
[102,     1] loss: 608.163
[103,     1] loss: 571.282
[104,     1] loss: 502.548
[105,     1] loss: 578.160
[106,     1] loss: 880.853
[107,     1] loss: 762.084
[108,     1] loss: 547.441
[109,     1] loss: 560.970
[110,     1] loss: 595.224
[111,     1] loss: 482.003
[112,     1] loss: 534.559
[113,     1] loss: 512.523
[114,     1] loss: 539.587
[115,     1] loss: 535.320
[116,     1] loss: 522.014
[117,     1] loss: 654.795
[118,     1] loss: 1069.757
[119,     1] loss: 702.500
[120,     1] loss: 645.672
[121,     1] loss: 739.732
[122,     1] loss: 609.771
[123,     1] loss: 774.264
[124,     1] loss: 598.928
[125,     1] loss: 613.670
[126,     1] loss: 746.383
[127,     1] loss: 511.815
[128,     1] loss: 644.039
[129,     1] loss: 585.294
[130,     1] loss: 605.240
[131,     1] loss: 625.439
[132,     1] loss: 550.297
[133,     1] loss: 527.711
[134,     1] loss: 552.762
[135,     1] loss: 496.752
[136,     1] loss: 682.435
[137,     1] loss: 2442.000
[138,     1] loss: 590.573
[139,     1] loss: 1420.965
[140,     1] loss: 996.327
[141,     1] loss: 1183.787
[142,     1] loss: 1263.262
[143,     1] loss: 1175.700
[144,     1] loss: 1000.928
[145,     1] loss: 1028.340
[146,     1] loss: 928.853
Early stopping applied (best metric=0.3434107005596161)
Finished Training
Total time taken: 21.703551054000854
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1485.448
[2,     1] loss: 1480.002
[3,     1] loss: 1480.744
[4,     1] loss: 1484.890
[5,     1] loss: 1480.487
[6,     1] loss: 1474.237
[7,     1] loss: 1476.162
[8,     1] loss: 1475.472
[9,     1] loss: 1478.517
[10,     1] loss: 1478.200
[11,     1] loss: 1470.577
[12,     1] loss: 1471.456
[13,     1] loss: 1467.494
[14,     1] loss: 1457.566
[15,     1] loss: 1444.544
[16,     1] loss: 1417.676
[17,     1] loss: 1381.002
[18,     1] loss: 1349.971
[19,     1] loss: 1299.793
[20,     1] loss: 1276.924
[21,     1] loss: 1296.094
[22,     1] loss: 1303.125
[23,     1] loss: 1245.241
[24,     1] loss: 1255.936
[25,     1] loss: 1226.332
[26,     1] loss: 1226.699
[27,     1] loss: 1207.148
[28,     1] loss: 1276.481
[29,     1] loss: 1243.400
[30,     1] loss: 1200.514
[31,     1] loss: 1188.846
[32,     1] loss: 1166.924
[33,     1] loss: 1148.627
[34,     1] loss: 1164.463
[35,     1] loss: 1100.216
[36,     1] loss: 1128.734
[37,     1] loss: 1080.552
[38,     1] loss: 1126.404
[39,     1] loss: 1193.831
[40,     1] loss: 1043.204
[41,     1] loss: 1087.189
[42,     1] loss: 1072.995
[43,     1] loss: 1129.523
[44,     1] loss: 1076.328
[45,     1] loss: 1037.038
[46,     1] loss: 1070.337
[47,     1] loss: 1059.745
[48,     1] loss: 1044.742
[49,     1] loss: 1022.893
[50,     1] loss: 956.819
[51,     1] loss: 977.115
[52,     1] loss: 948.321
[53,     1] loss: 989.406
[54,     1] loss: 1020.433
[55,     1] loss: 1187.590
[56,     1] loss: 1022.930
[57,     1] loss: 1005.824
[58,     1] loss: 980.345
[59,     1] loss: 977.776
[60,     1] loss: 883.039
[61,     1] loss: 986.778
[62,     1] loss: 900.047
[63,     1] loss: 938.373
[64,     1] loss: 878.759
[65,     1] loss: 887.652
[66,     1] loss: 814.885
[67,     1] loss: 820.617
[68,     1] loss: 842.151
[69,     1] loss: 768.339
[70,     1] loss: 714.861
[71,     1] loss: 836.072
[72,     1] loss: 1021.209
[73,     1] loss: 1047.327
[74,     1] loss: 833.540
[75,     1] loss: 811.933
[76,     1] loss: 759.598
[77,     1] loss: 812.344
[78,     1] loss: 745.359
[79,     1] loss: 801.841
[80,     1] loss: 733.811
[81,     1] loss: 671.383
[82,     1] loss: 700.539
[83,     1] loss: 796.735
[84,     1] loss: 720.714
[85,     1] loss: 679.444
[86,     1] loss: 684.874
[87,     1] loss: 934.765
[88,     1] loss: 649.320
[89,     1] loss: 698.211
[90,     1] loss: 681.401
[91,     1] loss: 671.261
[92,     1] loss: 769.803
[93,     1] loss: 670.763
[94,     1] loss: 606.543
[95,     1] loss: 643.393
[96,     1] loss: 696.087
[97,     1] loss: 613.969
[98,     1] loss: 817.757
[99,     1] loss: 1168.063
[100,     1] loss: 664.983
[101,     1] loss: 976.493
[102,     1] loss: 757.551
Early stopping applied (best metric=0.3561847507953644)
Finished Training
Total time taken: 15.418139457702637
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1482.519
[2,     1] loss: 1486.974
[3,     1] loss: 1481.540
[4,     1] loss: 1481.477
[5,     1] loss: 1482.239
[6,     1] loss: 1478.781
[7,     1] loss: 1479.040
[8,     1] loss: 1478.153
[9,     1] loss: 1478.175
[10,     1] loss: 1477.036
[11,     1] loss: 1477.852
[12,     1] loss: 1479.565
[13,     1] loss: 1483.173
[14,     1] loss: 1474.825
[15,     1] loss: 1476.641
[16,     1] loss: 1475.633
[17,     1] loss: 1473.488
[18,     1] loss: 1470.458
[19,     1] loss: 1466.628
[20,     1] loss: 1451.444
[21,     1] loss: 1440.648
[22,     1] loss: 1411.121
[23,     1] loss: 1389.233
[24,     1] loss: 1360.448
[25,     1] loss: 1329.548
[26,     1] loss: 1322.678
[27,     1] loss: 1302.215
[28,     1] loss: 1321.410
[29,     1] loss: 1267.589
[30,     1] loss: 1266.105
[31,     1] loss: 1218.224
[32,     1] loss: 1213.148
[33,     1] loss: 1192.841
[34,     1] loss: 1188.719
[35,     1] loss: 1253.945
[36,     1] loss: 1142.569
[37,     1] loss: 1196.797
[38,     1] loss: 1136.289
[39,     1] loss: 1142.570
[40,     1] loss: 1141.409
[41,     1] loss: 1074.960
[42,     1] loss: 1091.225
[43,     1] loss: 1106.016
[44,     1] loss: 1134.284
[45,     1] loss: 1047.466
[46,     1] loss: 1031.215
[47,     1] loss: 1076.524
[48,     1] loss: 1520.730
[49,     1] loss: 1243.495
[50,     1] loss: 1181.293
[51,     1] loss: 1096.314
[52,     1] loss: 1188.774
[53,     1] loss: 1225.201
[54,     1] loss: 1167.464
[55,     1] loss: 1134.620
[56,     1] loss: 1136.699
[57,     1] loss: 1168.684
[58,     1] loss: 1146.738
[59,     1] loss: 1120.476
[60,     1] loss: 1108.958
[61,     1] loss: 1042.920
[62,     1] loss: 1053.756
[63,     1] loss: 991.692
[64,     1] loss: 1025.279
[65,     1] loss: 1020.582
[66,     1] loss: 1005.855
[67,     1] loss: 996.650
[68,     1] loss: 997.612
[69,     1] loss: 970.281
[70,     1] loss: 1003.894
[71,     1] loss: 955.112
[72,     1] loss: 951.552
[73,     1] loss: 896.331
[74,     1] loss: 908.839
[75,     1] loss: 913.016
[76,     1] loss: 858.741
[77,     1] loss: 832.432
[78,     1] loss: 782.219
[79,     1] loss: 829.300
[80,     1] loss: 914.910
[81,     1] loss: 1319.127
[82,     1] loss: 819.955
[83,     1] loss: 944.521
[84,     1] loss: 904.706
[85,     1] loss: 960.897
[86,     1] loss: 831.129
[87,     1] loss: 874.897
[88,     1] loss: 757.458
[89,     1] loss: 822.204
[90,     1] loss: 706.326
[91,     1] loss: 852.016
[92,     1] loss: 778.139
[93,     1] loss: 731.140
[94,     1] loss: 692.951
[95,     1] loss: 641.903
[96,     1] loss: 743.425
[97,     1] loss: 603.311
[98,     1] loss: 568.043
[99,     1] loss: 636.818
[100,     1] loss: 666.065
[101,     1] loss: 1225.591
[102,     1] loss: 771.947
[103,     1] loss: 646.294
[104,     1] loss: 746.783
[105,     1] loss: 661.064
[106,     1] loss: 864.948
[107,     1] loss: 794.551
[108,     1] loss: 621.740
[109,     1] loss: 861.004
[110,     1] loss: 662.543
[111,     1] loss: 786.904
[112,     1] loss: 670.340
[113,     1] loss: 689.244
[114,     1] loss: 641.943
Early stopping applied (best metric=0.37721604108810425)
Finished Training
Total time taken: 16.54107165336609
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1480.824
[2,     1] loss: 1479.711
[3,     1] loss: 1480.972
[4,     1] loss: 1478.988
[5,     1] loss: 1476.404
[6,     1] loss: 1472.414
[7,     1] loss: 1472.872
[8,     1] loss: 1457.300
[9,     1] loss: 1447.712
[10,     1] loss: 1419.065
[11,     1] loss: 1380.038
[12,     1] loss: 1333.591
[13,     1] loss: 1277.511
[14,     1] loss: 1313.534
[15,     1] loss: 1307.230
[16,     1] loss: 1241.341
[17,     1] loss: 1322.392
[18,     1] loss: 1223.129
[19,     1] loss: 1322.110
[20,     1] loss: 1238.256
[21,     1] loss: 1264.778
[22,     1] loss: 1212.560
[23,     1] loss: 1207.379
[24,     1] loss: 1202.956
[25,     1] loss: 1151.345
[26,     1] loss: 1117.875
[27,     1] loss: 1135.292
[28,     1] loss: 1109.986
[29,     1] loss: 1154.439
[30,     1] loss: 1045.522
[31,     1] loss: 1090.146
[32,     1] loss: 1101.824
[33,     1] loss: 1067.623
[34,     1] loss: 1053.897
[35,     1] loss: 1040.528
[36,     1] loss: 998.889
[37,     1] loss: 1038.048
[38,     1] loss: 1008.203
[39,     1] loss: 1031.854
[40,     1] loss: 997.621
[41,     1] loss: 974.383
[42,     1] loss: 1078.052
[43,     1] loss: 1181.117
[44,     1] loss: 980.647
[45,     1] loss: 1038.467
[46,     1] loss: 899.931
[47,     1] loss: 964.436
[48,     1] loss: 1012.655
[49,     1] loss: 1047.209
[50,     1] loss: 878.249
[51,     1] loss: 847.372
[52,     1] loss: 926.072
[53,     1] loss: 892.344
[54,     1] loss: 813.104
[55,     1] loss: 788.354
[56,     1] loss: 861.870
[57,     1] loss: 1235.242
[58,     1] loss: 1366.207
[59,     1] loss: 945.485
[60,     1] loss: 987.324
[61,     1] loss: 1082.721
[62,     1] loss: 1088.665
[63,     1] loss: 1016.901
[64,     1] loss: 1016.499
[65,     1] loss: 1043.973
[66,     1] loss: 958.562
[67,     1] loss: 898.890
[68,     1] loss: 949.015
[69,     1] loss: 890.532
[70,     1] loss: 886.128
[71,     1] loss: 786.506
[72,     1] loss: 826.491
[73,     1] loss: 833.418
[74,     1] loss: 730.638
[75,     1] loss: 825.048
[76,     1] loss: 745.416
[77,     1] loss: 714.517
[78,     1] loss: 708.030
[79,     1] loss: 714.601
[80,     1] loss: 729.240
[81,     1] loss: 675.725
[82,     1] loss: 609.447
[83,     1] loss: 713.357
[84,     1] loss: 793.406
[85,     1] loss: 1237.317
[86,     1] loss: 638.018
[87,     1] loss: 1133.885
[88,     1] loss: 818.649
[89,     1] loss: 982.051
[90,     1] loss: 965.379
Early stopping applied (best metric=0.3980255722999573)
Finished Training
Total time taken: 14.051568746566772
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1484.025
[2,     1] loss: 1482.434
[3,     1] loss: 1479.919
[4,     1] loss: 1485.824
[5,     1] loss: 1481.683
[6,     1] loss: 1480.844
[7,     1] loss: 1480.219
[8,     1] loss: 1475.737
[9,     1] loss: 1476.793
[10,     1] loss: 1471.211
[11,     1] loss: 1467.156
[12,     1] loss: 1462.358
[13,     1] loss: 1444.526
[14,     1] loss: 1426.920
[15,     1] loss: 1384.731
[16,     1] loss: 1358.364
[17,     1] loss: 1313.302
[18,     1] loss: 1302.316
[19,     1] loss: 1295.035
[20,     1] loss: 1282.880
[21,     1] loss: 1287.452
[22,     1] loss: 1246.496
[23,     1] loss: 1240.221
[24,     1] loss: 1259.908
[25,     1] loss: 1181.925
[26,     1] loss: 1184.828
[27,     1] loss: 1197.889
[28,     1] loss: 1186.702
[29,     1] loss: 1122.584
[30,     1] loss: 1141.236
[31,     1] loss: 1123.323
[32,     1] loss: 1190.972
[33,     1] loss: 1249.566
[34,     1] loss: 1117.270
[35,     1] loss: 1128.757
[36,     1] loss: 1117.147
[37,     1] loss: 1120.845
[38,     1] loss: 1100.090
[39,     1] loss: 1116.344
[40,     1] loss: 1075.269
[41,     1] loss: 1095.190
[42,     1] loss: 1004.922
[43,     1] loss: 1078.192
[44,     1] loss: 1019.615
[45,     1] loss: 976.039
[46,     1] loss: 983.235
[47,     1] loss: 984.548
[48,     1] loss: 1051.057
[49,     1] loss: 1034.348
[50,     1] loss: 879.864
[51,     1] loss: 939.297
[52,     1] loss: 993.882
[53,     1] loss: 903.120
[54,     1] loss: 904.337
[55,     1] loss: 885.289
[56,     1] loss: 830.165
[57,     1] loss: 978.859
[58,     1] loss: 1116.526
[59,     1] loss: 752.802
[60,     1] loss: 895.507
[61,     1] loss: 810.573
[62,     1] loss: 961.155
[63,     1] loss: 776.283
[64,     1] loss: 921.434
[65,     1] loss: 843.505
[66,     1] loss: 894.300
[67,     1] loss: 713.891
[68,     1] loss: 926.012
[69,     1] loss: 717.207
[70,     1] loss: 893.152
[71,     1] loss: 698.444
[72,     1] loss: 750.690
[73,     1] loss: 751.692
[74,     1] loss: 638.977
[75,     1] loss: 767.756
[76,     1] loss: 694.021
[77,     1] loss: 644.897
[78,     1] loss: 709.439
[79,     1] loss: 667.697
[80,     1] loss: 650.641
[81,     1] loss: 1065.232
[82,     1] loss: 959.929
[83,     1] loss: 811.997
[84,     1] loss: 795.964
[85,     1] loss: 985.783
[86,     1] loss: 732.758
[87,     1] loss: 954.282
[88,     1] loss: 701.225
[89,     1] loss: 840.066
[90,     1] loss: 809.402
[91,     1] loss: 724.187
[92,     1] loss: 799.274
[93,     1] loss: 651.774
Early stopping applied (best metric=0.3635944426059723)
Finished Training
Total time taken: 14.360531568527222
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1473.805
[2,     1] loss: 1484.458
[3,     1] loss: 1479.801
[4,     1] loss: 1476.225
[5,     1] loss: 1474.622
[6,     1] loss: 1475.670
[7,     1] loss: 1474.967
[8,     1] loss: 1479.481
[9,     1] loss: 1474.026
[10,     1] loss: 1466.166
[11,     1] loss: 1456.073
[12,     1] loss: 1441.361
[13,     1] loss: 1420.966
[14,     1] loss: 1383.713
[15,     1] loss: 1345.905
[16,     1] loss: 1317.382
[17,     1] loss: 1251.581
[18,     1] loss: 1279.875
[19,     1] loss: 1248.763
[20,     1] loss: 1341.030
[21,     1] loss: 1220.188
[22,     1] loss: 1259.733
[23,     1] loss: 1208.577
[24,     1] loss: 1208.394
[25,     1] loss: 1209.520
[26,     1] loss: 1174.977
[27,     1] loss: 1173.252
[28,     1] loss: 1150.926
[29,     1] loss: 1116.159
[30,     1] loss: 1112.527
[31,     1] loss: 1073.002
[32,     1] loss: 1102.606
[33,     1] loss: 1070.346
[34,     1] loss: 1096.501
[35,     1] loss: 1020.098
[36,     1] loss: 1046.071
[37,     1] loss: 1133.927
[38,     1] loss: 1116.857
[39,     1] loss: 1012.726
[40,     1] loss: 1023.381
[41,     1] loss: 949.727
[42,     1] loss: 1049.033
[43,     1] loss: 912.039
[44,     1] loss: 1064.719
[45,     1] loss: 1049.041
[46,     1] loss: 912.941
[47,     1] loss: 1087.629
[48,     1] loss: 909.010
[49,     1] loss: 963.490
[50,     1] loss: 922.957
[51,     1] loss: 881.377
[52,     1] loss: 801.168
[53,     1] loss: 815.804
[54,     1] loss: 945.561
[55,     1] loss: 857.979
[56,     1] loss: 850.391
[57,     1] loss: 756.738
[58,     1] loss: 701.085
[59,     1] loss: 741.018
[60,     1] loss: 755.334
[61,     1] loss: 700.496
[62,     1] loss: 1178.552
[63,     1] loss: 1273.423
[64,     1] loss: 805.352
[65,     1] loss: 968.687
[66,     1] loss: 977.180
[67,     1] loss: 922.131
[68,     1] loss: 959.681
[69,     1] loss: 1011.192
[70,     1] loss: 907.373
[71,     1] loss: 903.561
[72,     1] loss: 933.045
[73,     1] loss: 789.011
[74,     1] loss: 861.273
[75,     1] loss: 816.954
[76,     1] loss: 774.252
[77,     1] loss: 746.361
[78,     1] loss: 794.035
[79,     1] loss: 752.458
[80,     1] loss: 800.423
[81,     1] loss: 664.193
[82,     1] loss: 792.063
[83,     1] loss: 611.368
[84,     1] loss: 820.823
[85,     1] loss: 687.291
Early stopping applied (best metric=0.407450407743454)
Finished Training
Total time taken: 12.556121587753296
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1482.644
[2,     1] loss: 1482.176
[3,     1] loss: 1480.893
[4,     1] loss: 1479.724
[5,     1] loss: 1478.693
[6,     1] loss: 1481.420
[7,     1] loss: 1480.704
[8,     1] loss: 1479.724
[9,     1] loss: 1474.565
[10,     1] loss: 1480.826
[11,     1] loss: 1481.888
[12,     1] loss: 1476.416
[13,     1] loss: 1479.441
[14,     1] loss: 1477.385
[15,     1] loss: 1477.551
[16,     1] loss: 1476.673
[17,     1] loss: 1477.485
[18,     1] loss: 1476.638
[19,     1] loss: 1475.884
[20,     1] loss: 1475.019
[21,     1] loss: 1472.296
[22,     1] loss: 1467.260
[23,     1] loss: 1463.133
[24,     1] loss: 1449.705
[25,     1] loss: 1432.803
[26,     1] loss: 1410.178
[27,     1] loss: 1373.432
[28,     1] loss: 1361.889
[29,     1] loss: 1321.187
[30,     1] loss: 1253.593
[31,     1] loss: 1252.355
[32,     1] loss: 1245.427
[33,     1] loss: 1276.518
[34,     1] loss: 1255.640
[35,     1] loss: 1228.221
[36,     1] loss: 1197.942
[37,     1] loss: 1193.960
[38,     1] loss: 1215.702
[39,     1] loss: 1230.045
[40,     1] loss: 1203.171
[41,     1] loss: 1145.102
[42,     1] loss: 1160.691
[43,     1] loss: 1126.302
[44,     1] loss: 1099.699
[45,     1] loss: 1089.381
[46,     1] loss: 1106.675
[47,     1] loss: 1015.662
[48,     1] loss: 1001.210
[49,     1] loss: 1032.155
[50,     1] loss: 1065.662
[51,     1] loss: 993.207
[52,     1] loss: 1040.947
[53,     1] loss: 1088.461
[54,     1] loss: 1025.626
[55,     1] loss: 992.786
[56,     1] loss: 933.591
[57,     1] loss: 1027.848
[58,     1] loss: 942.167
[59,     1] loss: 968.449
[60,     1] loss: 905.079
[61,     1] loss: 887.676
[62,     1] loss: 957.786
[63,     1] loss: 886.122
[64,     1] loss: 867.433
[65,     1] loss: 900.793
[66,     1] loss: 1025.012
[67,     1] loss: 847.589
[68,     1] loss: 915.783
[69,     1] loss: 934.311
[70,     1] loss: 782.075
[71,     1] loss: 898.462
[72,     1] loss: 781.489
[73,     1] loss: 878.696
[74,     1] loss: 823.595
[75,     1] loss: 858.651
[76,     1] loss: 831.235
[77,     1] loss: 736.389
[78,     1] loss: 893.220
[79,     1] loss: 757.132
[80,     1] loss: 801.731
[81,     1] loss: 814.307
[82,     1] loss: 725.790
[83,     1] loss: 738.140
[84,     1] loss: 665.459
[85,     1] loss: 751.382
[86,     1] loss: 750.337
[87,     1] loss: 676.743
[88,     1] loss: 848.167
[89,     1] loss: 747.407
[90,     1] loss: 663.028
[91,     1] loss: 673.791
[92,     1] loss: 591.061
[93,     1] loss: 608.106
[94,     1] loss: 739.873
[95,     1] loss: 1084.782
[96,     1] loss: 1129.209
[97,     1] loss: 600.468
[98,     1] loss: 944.183
[99,     1] loss: 756.466
[100,     1] loss: 795.317
[101,     1] loss: 854.849
[102,     1] loss: 711.670
[103,     1] loss: 815.633
[104,     1] loss: 712.962
[105,     1] loss: 817.667
[106,     1] loss: 706.049
[107,     1] loss: 699.092
[108,     1] loss: 742.834
[109,     1] loss: 673.109
[110,     1] loss: 625.770
[111,     1] loss: 605.778
[112,     1] loss: 620.744
[113,     1] loss: 595.169
[114,     1] loss: 554.817
[115,     1] loss: 602.939
[116,     1] loss: 614.181
[117,     1] loss: 570.626
[118,     1] loss: 989.304
[119,     1] loss: 1402.626
[120,     1] loss: 909.324
[121,     1] loss: 1004.259
[122,     1] loss: 908.436
[123,     1] loss: 943.914
[124,     1] loss: 1012.124
[125,     1] loss: 785.965
[126,     1] loss: 931.525
[127,     1] loss: 802.088
[128,     1] loss: 802.027
[129,     1] loss: 847.329
[130,     1] loss: 752.061
[131,     1] loss: 743.693
[132,     1] loss: 795.886
[133,     1] loss: 693.689
[134,     1] loss: 726.873
[135,     1] loss: 674.474
[136,     1] loss: 742.426
[137,     1] loss: 664.306
[138,     1] loss: 714.120
[139,     1] loss: 710.078
[140,     1] loss: 562.284
[141,     1] loss: 731.123
[142,     1] loss: 627.389
[143,     1] loss: 580.863
[144,     1] loss: 740.499
[145,     1] loss: 596.180
[146,     1] loss: 555.642
[147,     1] loss: 599.315
[148,     1] loss: 604.430
Early stopping applied (best metric=0.3279263377189636)
Finished Training
Total time taken: 24.15665888786316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1480.216
[2,     1] loss: 1482.558
[3,     1] loss: 1478.489
[4,     1] loss: 1478.664
[5,     1] loss: 1486.405
[6,     1] loss: 1478.207
[7,     1] loss: 1474.134
[8,     1] loss: 1477.485
[9,     1] loss: 1467.937
[10,     1] loss: 1475.186
[11,     1] loss: 1460.372
[12,     1] loss: 1450.568
[13,     1] loss: 1422.304
[14,     1] loss: 1390.932
[15,     1] loss: 1347.523
[16,     1] loss: 1304.233
[17,     1] loss: 1269.377
[18,     1] loss: 1246.570
[19,     1] loss: 1262.718
[20,     1] loss: 1199.627
[21,     1] loss: 1195.447
[22,     1] loss: 1189.275
[23,     1] loss: 1239.185
[24,     1] loss: 1152.352
[25,     1] loss: 1230.964
[26,     1] loss: 1120.012
[27,     1] loss: 1080.161
[28,     1] loss: 1040.047
[29,     1] loss: 1105.156
[30,     1] loss: 1122.607
[31,     1] loss: 1077.766
[32,     1] loss: 1064.643
[33,     1] loss: 1134.226
[34,     1] loss: 1077.382
[35,     1] loss: 1054.804
[36,     1] loss: 1083.218
[37,     1] loss: 980.295
[38,     1] loss: 1006.057
[39,     1] loss: 996.798
[40,     1] loss: 944.112
[41,     1] loss: 923.040
[42,     1] loss: 963.077
[43,     1] loss: 898.588
[44,     1] loss: 918.730
[45,     1] loss: 912.990
[46,     1] loss: 893.728
[47,     1] loss: 794.621
[48,     1] loss: 834.368
[49,     1] loss: 849.365
[50,     1] loss: 957.548
[51,     1] loss: 1036.592
[52,     1] loss: 1002.294
[53,     1] loss: 800.743
[54,     1] loss: 872.607
[55,     1] loss: 790.199
[56,     1] loss: 870.061
[57,     1] loss: 777.475
[58,     1] loss: 824.107
[59,     1] loss: 757.148
[60,     1] loss: 725.557
[61,     1] loss: 734.022
[62,     1] loss: 694.164
[63,     1] loss: 762.008
[64,     1] loss: 1279.103
[65,     1] loss: 915.562
[66,     1] loss: 757.126
[67,     1] loss: 868.993
[68,     1] loss: 865.233
[69,     1] loss: 797.267
[70,     1] loss: 833.703
[71,     1] loss: 777.524
[72,     1] loss: 765.472
[73,     1] loss: 665.748
[74,     1] loss: 713.445
[75,     1] loss: 687.581
[76,     1] loss: 622.659
[77,     1] loss: 569.445
[78,     1] loss: 656.253
[79,     1] loss: 705.954
[80,     1] loss: 729.671
[81,     1] loss: 698.126
[82,     1] loss: 620.095
[83,     1] loss: 585.349
[84,     1] loss: 605.634
[85,     1] loss: 563.308
[86,     1] loss: 634.193
[87,     1] loss: 594.188
[88,     1] loss: 640.540
[89,     1] loss: 854.843
[90,     1] loss: 2203.704
[91,     1] loss: 753.766
[92,     1] loss: 1197.613
[93,     1] loss: 1054.627
[94,     1] loss: 1080.717
[95,     1] loss: 1118.649
[96,     1] loss: 1139.432
[97,     1] loss: 1060.687
[98,     1] loss: 963.083
[99,     1] loss: 883.151
[100,     1] loss: 956.406
[101,     1] loss: 895.030
[102,     1] loss: 922.083
[103,     1] loss: 862.196
[104,     1] loss: 868.316
[105,     1] loss: 786.907
[106,     1] loss: 849.826
[107,     1] loss: 779.018
Early stopping applied (best metric=0.3583439886569977)
Finished Training
Total time taken: 16.602067470550537
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1478.320
[2,     1] loss: 1477.849
[3,     1] loss: 1478.191
[4,     1] loss: 1481.665
[5,     1] loss: 1474.242
[6,     1] loss: 1475.569
[7,     1] loss: 1467.552
[8,     1] loss: 1466.408
[9,     1] loss: 1452.227
[10,     1] loss: 1433.269
[11,     1] loss: 1423.555
[12,     1] loss: 1367.909
[13,     1] loss: 1345.953
[14,     1] loss: 1324.359
[15,     1] loss: 1267.464
[16,     1] loss: 1287.229
[17,     1] loss: 1281.943
[18,     1] loss: 1287.575
[19,     1] loss: 1236.659
[20,     1] loss: 1267.899
[21,     1] loss: 1224.642
[22,     1] loss: 1213.637
[23,     1] loss: 1242.744
[24,     1] loss: 1214.475
[25,     1] loss: 1209.336
[26,     1] loss: 1194.259
[27,     1] loss: 1148.199
[28,     1] loss: 1124.796
[29,     1] loss: 1148.125
[30,     1] loss: 1083.497
[31,     1] loss: 1079.479
[32,     1] loss: 1063.653
[33,     1] loss: 1092.037
[34,     1] loss: 1079.044
[35,     1] loss: 1048.565
[36,     1] loss: 1161.755
[37,     1] loss: 1111.895
[38,     1] loss: 1038.790
[39,     1] loss: 1037.063
[40,     1] loss: 965.243
[41,     1] loss: 935.581
[42,     1] loss: 971.773
[43,     1] loss: 964.333
[44,     1] loss: 1057.722
[45,     1] loss: 1300.684
[46,     1] loss: 998.922
[47,     1] loss: 1102.819
[48,     1] loss: 1008.687
[49,     1] loss: 1105.810
[50,     1] loss: 1053.588
[51,     1] loss: 1017.540
[52,     1] loss: 1040.899
[53,     1] loss: 994.117
[54,     1] loss: 956.758
[55,     1] loss: 935.308
[56,     1] loss: 888.327
[57,     1] loss: 968.476
[58,     1] loss: 901.736
[59,     1] loss: 887.719
[60,     1] loss: 963.242
[61,     1] loss: 803.118
[62,     1] loss: 937.467
[63,     1] loss: 878.991
[64,     1] loss: 804.005
[65,     1] loss: 793.529
[66,     1] loss: 737.875
[67,     1] loss: 703.198
[68,     1] loss: 717.442
[69,     1] loss: 784.833
[70,     1] loss: 662.951
[71,     1] loss: 877.567
[72,     1] loss: 2534.347
[73,     1] loss: 809.122
[74,     1] loss: 1482.884
[75,     1] loss: 1094.806
[76,     1] loss: 1176.737
[77,     1] loss: 1268.155
[78,     1] loss: 1289.134
[79,     1] loss: 1277.022
[80,     1] loss: 1226.004
[81,     1] loss: 1140.634
[82,     1] loss: 1111.815
[83,     1] loss: 1138.818
[84,     1] loss: 1177.669
[85,     1] loss: 1053.245
[86,     1] loss: 1109.389
[87,     1] loss: 1097.118
[88,     1] loss: 1110.894
[89,     1] loss: 1040.444
[90,     1] loss: 1082.507
[91,     1] loss: 989.809
[92,     1] loss: 977.123
[93,     1] loss: 996.562
[94,     1] loss: 976.989
[95,     1] loss: 937.466
[96,     1] loss: 861.359
[97,     1] loss: 847.283
[98,     1] loss: 842.648
[99,     1] loss: 802.466
[100,     1] loss: 807.170
[101,     1] loss: 951.570
[102,     1] loss: 1886.423
[103,     1] loss: 1023.786
[104,     1] loss: 1073.905
[105,     1] loss: 1144.757
[106,     1] loss: 1093.609
[107,     1] loss: 1122.814
[108,     1] loss: 1095.013
[109,     1] loss: 984.705
[110,     1] loss: 1135.045
[111,     1] loss: 979.055
[112,     1] loss: 1033.947
[113,     1] loss: 903.138
[114,     1] loss: 1054.615
[115,     1] loss: 899.000
[116,     1] loss: 994.153
[117,     1] loss: 863.559
[118,     1] loss: 1024.762
[119,     1] loss: 890.390
[120,     1] loss: 957.692
[121,     1] loss: 765.012
[122,     1] loss: 912.018
[123,     1] loss: 764.366
[124,     1] loss: 917.422
[125,     1] loss: 797.182
[126,     1] loss: 850.054
[127,     1] loss: 709.956
[128,     1] loss: 705.615
[129,     1] loss: 713.246
[130,     1] loss: 731.174
[131,     1] loss: 889.445
[132,     1] loss: 826.841
[133,     1] loss: 887.197
[134,     1] loss: 676.815
[135,     1] loss: 769.266
[136,     1] loss: 702.429
[137,     1] loss: 714.601
[138,     1] loss: 688.757
[139,     1] loss: 699.545
[140,     1] loss: 758.004
[141,     1] loss: 799.772
[142,     1] loss: 855.301
[143,     1] loss: 632.737
[144,     1] loss: 710.255
[145,     1] loss: 889.876
[146,     1] loss: 663.808
Early stopping applied (best metric=0.33087337017059326)
Finished Training
Total time taken: 21.886793851852417
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1482.376
[2,     1] loss: 1485.420
[3,     1] loss: 1478.333
[4,     1] loss: 1477.143
[5,     1] loss: 1464.478
[6,     1] loss: 1471.628
[7,     1] loss: 1446.676
[8,     1] loss: 1418.481
[9,     1] loss: 1346.122
[10,     1] loss: 1289.699
[11,     1] loss: 1301.372
[12,     1] loss: 1257.877
[13,     1] loss: 1208.550
[14,     1] loss: 1165.001
[15,     1] loss: 1211.537
[16,     1] loss: 1245.039
[17,     1] loss: 1144.365
[18,     1] loss: 1158.651
[19,     1] loss: 1140.257
[20,     1] loss: 1156.684
[21,     1] loss: 1106.620
[22,     1] loss: 1142.226
[23,     1] loss: 1117.065
[24,     1] loss: 1085.831
[25,     1] loss: 1081.057
[26,     1] loss: 1082.224
[27,     1] loss: 1068.033
[28,     1] loss: 1037.851
[29,     1] loss: 1015.809
[30,     1] loss: 1052.350
[31,     1] loss: 1018.286
[32,     1] loss: 1050.268
[33,     1] loss: 1100.695
[34,     1] loss: 1056.101
[35,     1] loss: 992.122
[36,     1] loss: 992.963
[37,     1] loss: 930.990
[38,     1] loss: 991.290
[39,     1] loss: 921.135
[40,     1] loss: 885.391
[41,     1] loss: 963.430
[42,     1] loss: 1245.886
[43,     1] loss: 1074.486
[44,     1] loss: 983.012
[45,     1] loss: 951.224
[46,     1] loss: 1053.298
[47,     1] loss: 935.884
[48,     1] loss: 962.231
[49,     1] loss: 1019.709
[50,     1] loss: 934.498
[51,     1] loss: 957.411
[52,     1] loss: 921.868
[53,     1] loss: 970.324
[54,     1] loss: 880.743
[55,     1] loss: 891.404
[56,     1] loss: 845.891
[57,     1] loss: 894.588
[58,     1] loss: 776.905
[59,     1] loss: 798.633
[60,     1] loss: 811.545
[61,     1] loss: 723.832
[62,     1] loss: 694.086
[63,     1] loss: 704.706
[64,     1] loss: 722.680
[65,     1] loss: 827.880
[66,     1] loss: 1521.264
[67,     1] loss: 1326.499
[68,     1] loss: 1056.408
[69,     1] loss: 967.686
[70,     1] loss: 1079.562
[71,     1] loss: 1160.603
[72,     1] loss: 1166.655
[73,     1] loss: 1109.329
[74,     1] loss: 1066.797
[75,     1] loss: 1028.661
[76,     1] loss: 1057.210
[77,     1] loss: 1054.678
[78,     1] loss: 1022.175
[79,     1] loss: 1027.591
[80,     1] loss: 996.959
[81,     1] loss: 938.913
[82,     1] loss: 941.129
[83,     1] loss: 950.364
[84,     1] loss: 895.274
[85,     1] loss: 835.307
[86,     1] loss: 831.257
[87,     1] loss: 862.901
[88,     1] loss: 872.769
[89,     1] loss: 774.195
[90,     1] loss: 772.297
[91,     1] loss: 682.745
[92,     1] loss: 698.169
[93,     1] loss: 650.404
Early stopping applied (best metric=0.40686535835266113)
Finished Training
Total time taken: 14.64561676979065
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1482.099
[2,     1] loss: 1477.322
[3,     1] loss: 1483.666
[4,     1] loss: 1480.331
[5,     1] loss: 1478.671
[6,     1] loss: 1477.444
[7,     1] loss: 1473.095
[8,     1] loss: 1468.448
[9,     1] loss: 1461.923
[10,     1] loss: 1448.094
[11,     1] loss: 1422.285
[12,     1] loss: 1391.639
[13,     1] loss: 1350.593
[14,     1] loss: 1288.544
[15,     1] loss: 1273.079
[16,     1] loss: 1246.230
[17,     1] loss: 1287.963
[18,     1] loss: 1306.498
[19,     1] loss: 1220.265
[20,     1] loss: 1248.813
[21,     1] loss: 1218.118
[22,     1] loss: 1234.215
[23,     1] loss: 1256.523
[24,     1] loss: 1230.621
[25,     1] loss: 1189.196
[26,     1] loss: 1208.439
[27,     1] loss: 1170.028
[28,     1] loss: 1206.024
[29,     1] loss: 1132.717
[30,     1] loss: 1167.287
[31,     1] loss: 1133.876
[32,     1] loss: 1091.370
[33,     1] loss: 1096.024
[34,     1] loss: 1125.158
[35,     1] loss: 1187.958
[36,     1] loss: 1045.089
[37,     1] loss: 1058.705
[38,     1] loss: 1022.523
[39,     1] loss: 1053.573
[40,     1] loss: 997.597
[41,     1] loss: 998.500
[42,     1] loss: 1034.013
[43,     1] loss: 1000.684
[44,     1] loss: 1239.607
[45,     1] loss: 1255.788
[46,     1] loss: 1011.535
[47,     1] loss: 1063.992
[48,     1] loss: 1077.681
[49,     1] loss: 991.413
[50,     1] loss: 1065.988
[51,     1] loss: 1080.144
[52,     1] loss: 974.808
[53,     1] loss: 977.052
[54,     1] loss: 973.078
[55,     1] loss: 975.100
[56,     1] loss: 950.399
[57,     1] loss: 872.873
[58,     1] loss: 896.045
[59,     1] loss: 869.152
[60,     1] loss: 892.420
[61,     1] loss: 932.643
[62,     1] loss: 827.547
[63,     1] loss: 834.133
[64,     1] loss: 927.519
[65,     1] loss: 1389.781
[66,     1] loss: 848.115
[67,     1] loss: 1148.054
[68,     1] loss: 859.283
[69,     1] loss: 994.026
[70,     1] loss: 1016.748
[71,     1] loss: 834.162
[72,     1] loss: 933.803
[73,     1] loss: 806.403
[74,     1] loss: 876.471
[75,     1] loss: 783.635
[76,     1] loss: 883.749
[77,     1] loss: 732.091
[78,     1] loss: 810.254
[79,     1] loss: 739.019
[80,     1] loss: 853.579
[81,     1] loss: 967.785
[82,     1] loss: 699.778
[83,     1] loss: 966.227
[84,     1] loss: 730.631
[85,     1] loss: 799.407
[86,     1] loss: 699.503
[87,     1] loss: 725.280
[88,     1] loss: 626.043
[89,     1] loss: 739.193
[90,     1] loss: 654.185
[91,     1] loss: 592.386
[92,     1] loss: 671.422
[93,     1] loss: 596.512
[94,     1] loss: 597.591
[95,     1] loss: 566.015
[96,     1] loss: 787.978
[97,     1] loss: 1017.052
[98,     1] loss: 580.561
[99,     1] loss: 955.059
[100,     1] loss: 680.024
[101,     1] loss: 970.952
[102,     1] loss: 597.442
[103,     1] loss: 1007.338
[104,     1] loss: 665.882
[105,     1] loss: 914.283
[106,     1] loss: 703.231
[107,     1] loss: 693.504
[108,     1] loss: 708.964
[109,     1] loss: 684.696
[110,     1] loss: 715.092
[111,     1] loss: 621.439
[112,     1] loss: 819.483
[113,     1] loss: 750.149
[114,     1] loss: 755.513
[115,     1] loss: 638.260
[116,     1] loss: 777.210
[117,     1] loss: 582.696
[118,     1] loss: 786.671
[119,     1] loss: 570.711
[120,     1] loss: 695.850
[121,     1] loss: 796.943
[122,     1] loss: 644.364
[123,     1] loss: 895.592
[124,     1] loss: 595.692
[125,     1] loss: 832.067
[126,     1] loss: 608.065
[127,     1] loss: 751.298
[128,     1] loss: 579.236
[129,     1] loss: 696.932
[130,     1] loss: 617.132
[131,     1] loss: 576.865
[132,     1] loss: 590.293
[133,     1] loss: 562.141
[134,     1] loss: 669.235
Early stopping applied (best metric=0.3479066789150238)
Finished Training
Total time taken: 21.683152675628662
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1483.907
[2,     1] loss: 1494.265
[3,     1] loss: 1477.865
[4,     1] loss: 1479.575
[5,     1] loss: 1477.467
[6,     1] loss: 1481.494
[7,     1] loss: 1480.857
[8,     1] loss: 1477.674
[9,     1] loss: 1478.529
[10,     1] loss: 1476.893
[11,     1] loss: 1480.730
[12,     1] loss: 1477.472
[13,     1] loss: 1479.038
[14,     1] loss: 1475.766
[15,     1] loss: 1476.968
[16,     1] loss: 1472.262
[17,     1] loss: 1468.884
[18,     1] loss: 1463.301
[19,     1] loss: 1449.458
[20,     1] loss: 1428.752
[21,     1] loss: 1396.479
[22,     1] loss: 1357.246
[23,     1] loss: 1332.679
[24,     1] loss: 1295.423
[25,     1] loss: 1266.761
[26,     1] loss: 1244.029
[27,     1] loss: 1309.919
[28,     1] loss: 1300.794
[29,     1] loss: 1271.738
[30,     1] loss: 1202.046
[31,     1] loss: 1215.813
[32,     1] loss: 1193.601
[33,     1] loss: 1209.389
[34,     1] loss: 1198.653
[35,     1] loss: 1164.740
[36,     1] loss: 1162.220
[37,     1] loss: 1157.220
[38,     1] loss: 1187.701
[39,     1] loss: 1204.054
[40,     1] loss: 1220.952
[41,     1] loss: 1151.865
[42,     1] loss: 1195.173
[43,     1] loss: 1198.562
[44,     1] loss: 1112.081
[45,     1] loss: 1137.708
[46,     1] loss: 1073.002
[47,     1] loss: 1073.852
[48,     1] loss: 1100.708
[49,     1] loss: 1054.669
[50,     1] loss: 1022.408
[51,     1] loss: 1015.864
[52,     1] loss: 996.823
[53,     1] loss: 973.385
[54,     1] loss: 1013.497
[55,     1] loss: 958.047
[56,     1] loss: 972.881
[57,     1] loss: 937.780
[58,     1] loss: 875.749
[59,     1] loss: 929.060
[60,     1] loss: 977.458
[61,     1] loss: 953.075
[62,     1] loss: 884.552
[63,     1] loss: 870.077
[64,     1] loss: 1000.630
[65,     1] loss: 1084.476
[66,     1] loss: 834.323
[67,     1] loss: 948.643
[68,     1] loss: 808.312
[69,     1] loss: 949.624
[70,     1] loss: 789.291
[71,     1] loss: 1036.980
[72,     1] loss: 774.285
[73,     1] loss: 918.578
[74,     1] loss: 737.332
[75,     1] loss: 926.731
[76,     1] loss: 858.688
[77,     1] loss: 812.430
[78,     1] loss: 780.711
[79,     1] loss: 700.918
[80,     1] loss: 700.670
[81,     1] loss: 728.322
[82,     1] loss: 825.345
[83,     1] loss: 703.885
[84,     1] loss: 648.527
[85,     1] loss: 682.938
[86,     1] loss: 820.486
[87,     1] loss: 567.995
[88,     1] loss: 674.028
[89,     1] loss: 656.396
[90,     1] loss: 541.200
[91,     1] loss: 774.169
[92,     1] loss: 803.889
[93,     1] loss: 580.318
[94,     1] loss: 655.514
[95,     1] loss: 754.810
[96,     1] loss: 602.842
[97,     1] loss: 867.285
[98,     1] loss: 800.982
Early stopping applied (best metric=0.40671342611312866)
Finished Training
Total time taken: 14.500530004501343
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1475.718
[2,     1] loss: 1498.431
[3,     1] loss: 1477.827
[4,     1] loss: 1483.919
[5,     1] loss: 1488.562
[6,     1] loss: 1479.126
[7,     1] loss: 1480.648
[8,     1] loss: 1480.192
[9,     1] loss: 1478.685
[10,     1] loss: 1477.693
[11,     1] loss: 1472.178
[12,     1] loss: 1477.024
[13,     1] loss: 1472.232
[14,     1] loss: 1467.668
[15,     1] loss: 1461.696
[16,     1] loss: 1452.243
[17,     1] loss: 1430.844
[18,     1] loss: 1410.660
[19,     1] loss: 1379.704
[20,     1] loss: 1334.918
[21,     1] loss: 1312.071
[22,     1] loss: 1268.207
[23,     1] loss: 1289.979
[24,     1] loss: 1217.890
[25,     1] loss: 1254.219
[26,     1] loss: 1255.258
[27,     1] loss: 1199.338
[28,     1] loss: 1185.654
[29,     1] loss: 1237.022
[30,     1] loss: 1174.206
[31,     1] loss: 1188.981
[32,     1] loss: 1092.456
[33,     1] loss: 1190.974
[34,     1] loss: 1146.933
[35,     1] loss: 1118.752
[36,     1] loss: 1064.013
[37,     1] loss: 1120.660
[38,     1] loss: 1063.219
[39,     1] loss: 1168.630
[40,     1] loss: 1375.713
[41,     1] loss: 1074.299
[42,     1] loss: 1130.231
[43,     1] loss: 1062.330
[44,     1] loss: 1140.399
[45,     1] loss: 1063.260
[46,     1] loss: 1044.617
[47,     1] loss: 1031.741
[48,     1] loss: 994.053
[49,     1] loss: 1049.605
[50,     1] loss: 941.075
[51,     1] loss: 1071.870
[52,     1] loss: 1141.725
[53,     1] loss: 982.244
[54,     1] loss: 1008.475
[55,     1] loss: 933.410
[56,     1] loss: 1001.153
[57,     1] loss: 941.939
[58,     1] loss: 928.572
[59,     1] loss: 955.565
[60,     1] loss: 973.803
[61,     1] loss: 792.740
[62,     1] loss: 854.268
[63,     1] loss: 988.480
[64,     1] loss: 1040.993
[65,     1] loss: 852.957
[66,     1] loss: 987.072
[67,     1] loss: 875.873
[68,     1] loss: 948.836
[69,     1] loss: 887.253
[70,     1] loss: 955.730
[71,     1] loss: 812.731
[72,     1] loss: 791.663
[73,     1] loss: 863.276
[74,     1] loss: 746.921
[75,     1] loss: 711.055
[76,     1] loss: 870.537
[77,     1] loss: 715.522
[78,     1] loss: 673.859
[79,     1] loss: 661.741
[80,     1] loss: 712.382
[81,     1] loss: 956.418
[82,     1] loss: 972.061
[83,     1] loss: 871.927
[84,     1] loss: 807.449
[85,     1] loss: 728.080
[86,     1] loss: 940.604
[87,     1] loss: 677.977
[88,     1] loss: 834.129
[89,     1] loss: 690.441
[90,     1] loss: 874.212
[91,     1] loss: 723.569
[92,     1] loss: 681.772
[93,     1] loss: 674.711
[94,     1] loss: 676.938
[95,     1] loss: 778.232
[96,     1] loss: 628.141
[97,     1] loss: 635.674
[98,     1] loss: 705.188
[99,     1] loss: 664.403
[100,     1] loss: 561.424
[101,     1] loss: 677.945
[102,     1] loss: 695.187
[103,     1] loss: 805.143
[104,     1] loss: 616.071
[105,     1] loss: 617.129
[106,     1] loss: 709.480
[107,     1] loss: 610.824
[108,     1] loss: 667.709
[109,     1] loss: 845.254
[110,     1] loss: 587.927
[111,     1] loss: 567.327
[112,     1] loss: 649.802
[113,     1] loss: 563.360
[114,     1] loss: 480.975
Early stopping applied (best metric=0.3063720464706421)
Finished Training
Total time taken: 16.865545749664307
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1479.168
[2,     1] loss: 1476.178
[3,     1] loss: 1483.092
[4,     1] loss: 1483.332
[5,     1] loss: 1478.580
[6,     1] loss: 1472.580
[7,     1] loss: 1467.789
[8,     1] loss: 1453.167
[9,     1] loss: 1430.291
[10,     1] loss: 1403.475
[11,     1] loss: 1370.776
[12,     1] loss: 1304.714
[13,     1] loss: 1305.852
[14,     1] loss: 1246.305
[15,     1] loss: 1202.922
[16,     1] loss: 1220.390
[17,     1] loss: 1299.524
[18,     1] loss: 1215.137
[19,     1] loss: 1271.433
[20,     1] loss: 1166.350
[21,     1] loss: 1214.742
[22,     1] loss: 1185.577
[23,     1] loss: 1180.565
[24,     1] loss: 1155.983
[25,     1] loss: 1127.557
[26,     1] loss: 1215.407
[27,     1] loss: 1114.422
[28,     1] loss: 1165.399
[29,     1] loss: 1123.845
[30,     1] loss: 1079.828
[31,     1] loss: 1088.768
[32,     1] loss: 1052.377
[33,     1] loss: 986.185
[34,     1] loss: 1079.652
[35,     1] loss: 1232.990
[36,     1] loss: 1020.311
[37,     1] loss: 1086.945
[38,     1] loss: 1072.942
[39,     1] loss: 1147.876
[40,     1] loss: 1028.807
[41,     1] loss: 1014.576
[42,     1] loss: 1085.183
[43,     1] loss: 926.299
[44,     1] loss: 999.418
[45,     1] loss: 950.234
[46,     1] loss: 948.726
[47,     1] loss: 938.820
[48,     1] loss: 959.783
[49,     1] loss: 907.666
[50,     1] loss: 896.813
[51,     1] loss: 845.017
[52,     1] loss: 885.771
[53,     1] loss: 887.258
[54,     1] loss: 977.301
[55,     1] loss: 1091.708
[56,     1] loss: 904.833
[57,     1] loss: 955.927
[58,     1] loss: 953.928
[59,     1] loss: 940.263
[60,     1] loss: 847.111
[61,     1] loss: 913.656
[62,     1] loss: 810.895
[63,     1] loss: 1059.499
[64,     1] loss: 840.743
[65,     1] loss: 827.798
[66,     1] loss: 799.726
[67,     1] loss: 756.165
[68,     1] loss: 743.872
[69,     1] loss: 739.373
[70,     1] loss: 698.787
[71,     1] loss: 687.102
[72,     1] loss: 665.692
[73,     1] loss: 775.751
[74,     1] loss: 980.582
[75,     1] loss: 828.561
[76,     1] loss: 643.519
[77,     1] loss: 751.500
[78,     1] loss: 682.260
[79,     1] loss: 697.281
[80,     1] loss: 585.091
Early stopping applied (best metric=0.39089882373809814)
Finished Training
Total time taken: 11.829218864440918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1484.809
[2,     1] loss: 1487.312
[3,     1] loss: 1482.220
[4,     1] loss: 1478.446
[5,     1] loss: 1481.771
[6,     1] loss: 1483.140
[7,     1] loss: 1479.932
[8,     1] loss: 1479.098
[9,     1] loss: 1477.916
[10,     1] loss: 1468.732
[11,     1] loss: 1466.582
[12,     1] loss: 1457.211
[13,     1] loss: 1445.118
[14,     1] loss: 1423.112
[15,     1] loss: 1382.411
[16,     1] loss: 1345.833
[17,     1] loss: 1287.650
[18,     1] loss: 1278.544
[19,     1] loss: 1273.648
[20,     1] loss: 1230.590
[21,     1] loss: 1331.694
[22,     1] loss: 1251.473
[23,     1] loss: 1227.556
[24,     1] loss: 1194.267
[25,     1] loss: 1193.035
[26,     1] loss: 1181.823
[27,     1] loss: 1186.336
[28,     1] loss: 1173.789
[29,     1] loss: 1140.144
[30,     1] loss: 1136.086
[31,     1] loss: 1182.737
[32,     1] loss: 1088.326
[33,     1] loss: 1041.712
[34,     1] loss: 1145.002
[35,     1] loss: 1097.760
[36,     1] loss: 1088.435
[37,     1] loss: 1030.464
[38,     1] loss: 1061.233
[39,     1] loss: 988.371
[40,     1] loss: 965.379
[41,     1] loss: 981.269
[42,     1] loss: 940.118
[43,     1] loss: 1069.242
[44,     1] loss: 1470.971
[45,     1] loss: 910.888
[46,     1] loss: 1221.264
[47,     1] loss: 1007.489
[48,     1] loss: 1036.951
[49,     1] loss: 1143.317
[50,     1] loss: 1066.898
[51,     1] loss: 1012.391
[52,     1] loss: 1107.010
[53,     1] loss: 1044.785
[54,     1] loss: 1000.792
[55,     1] loss: 1001.070
[56,     1] loss: 948.216
[57,     1] loss: 935.591
[58,     1] loss: 937.384
[59,     1] loss: 898.777
[60,     1] loss: 898.807
[61,     1] loss: 803.127
[62,     1] loss: 961.983
[63,     1] loss: 879.719
[64,     1] loss: 851.793
[65,     1] loss: 853.351
[66,     1] loss: 778.704
[67,     1] loss: 787.547
[68,     1] loss: 754.896
[69,     1] loss: 700.491
[70,     1] loss: 748.780
[71,     1] loss: 733.919
[72,     1] loss: 1044.178
[73,     1] loss: 950.729
[74,     1] loss: 747.877
[75,     1] loss: 755.521
[76,     1] loss: 763.230
[77,     1] loss: 708.766
[78,     1] loss: 712.154
[79,     1] loss: 659.924
[80,     1] loss: 746.421
[81,     1] loss: 749.799
[82,     1] loss: 697.765
[83,     1] loss: 665.068
[84,     1] loss: 739.422
[85,     1] loss: 900.558
[86,     1] loss: 759.773
[87,     1] loss: 746.177
[88,     1] loss: 720.017
[89,     1] loss: 685.166
[90,     1] loss: 698.011
[91,     1] loss: 656.086
Early stopping applied (best metric=0.3691699206829071)
Finished Training
Total time taken: 14.040232181549072
{'Hydroxylation-K Validation Accuracy': 0.7408096926713948, 'Hydroxylation-K Validation Sensitivity': 0.7518518518518519, 'Hydroxylation-K Validation Specificity': 0.7385964912280701, 'Hydroxylation-K Validation Precision': 0.4227199907076068, 'Hydroxylation-K AUC ROC': 0.8248343079922027, 'Hydroxylation-K AUC PR': 0.62232814388012, 'Hydroxylation-K MCC': 0.4115482617676779, 'Hydroxylation-K F1': 0.5380819258468499, 'Validation Loss (Hydroxylation-K)': 0.43982964754104614, 'Hydroxylation-P Validation Accuracy': 0.7839222543694906, 'Hydroxylation-P Validation Sensitivity': 0.8160317460317461, 'Hydroxylation-P Validation Specificity': 0.7769340116714051, 'Hydroxylation-P Validation Precision': 0.4456714642031354, 'Hydroxylation-P AUC ROC': 0.8505673005897456, 'Hydroxylation-P AUC PR': 0.5856451233772937, 'Hydroxylation-P MCC': 0.48535435697026047, 'Hydroxylation-P F1': 0.5740585065339346, 'Validation Loss (Hydroxylation-P)': 0.36606345772743226, 'Validation Loss (total)': 0.8058930953343709, 'TimeToTrain': 16.722720034917195}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002740168187479192,
 'learning_rate_Hydroxylation-K': 0.00035433288991184426,
 'learning_rate_Hydroxylation-P': 0.009460559730286294,
 'log_base': 1.9415414514411604,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 490652669,
 'sample_weights': [2.710590980322978, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.001583867874306,
 'weight_decay_Hydroxylation-K': 7.674778235759737,
 'weight_decay_Hydroxylation-P': 6.451420155408517}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1442.428
[2,     1] loss: 1441.386
[3,     1] loss: 1436.767
[4,     1] loss: 1441.158
[5,     1] loss: 1438.608
[6,     1] loss: 1429.832
[7,     1] loss: 1430.625
[8,     1] loss: 1427.145
[9,     1] loss: 1425.491
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035724393397114076,
 'learning_rate_Hydroxylation-K': 0.009439143958655426,
 'learning_rate_Hydroxylation-P': 0.009588615490531978,
 'log_base': 1.8330973286733714,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1324891875,
 'sample_weights': [2.516183699663825, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.720112212375425,
 'weight_decay_Hydroxylation-K': 5.281500193757589,
 'weight_decay_Hydroxylation-P': 3.8170819048842524}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1489.223
[2,     1] loss: 1497.834
[3,     1] loss: 1492.961
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033370323692459136,
 'learning_rate_Hydroxylation-K': 0.0028193784352861383,
 'learning_rate_Hydroxylation-P': 0.007224729234734839,
 'log_base': 2.9376046219906415,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1902478219,
 'sample_weights': [2.754824559892822, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6779688351868556,
 'weight_decay_Hydroxylation-K': 9.165293628861217,
 'weight_decay_Hydroxylation-P': 2.0170310918028744}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.752
[2,     1] loss: 1241.113
[3,     1] loss: 1234.568
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007067273454500233,
 'learning_rate_Hydroxylation-K': 0.008802674175089956,
 'learning_rate_Hydroxylation-P': 0.007956979511457308,
 'log_base': 1.0110186961706034,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1769450192,
 'sample_weights': [1.549231325353621, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.280330699361112,
 'weight_decay_Hydroxylation-K': 5.183265278354018,
 'weight_decay_Hydroxylation-P': 2.050383994949531}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49410.156
Exploding loss, terminate run (best metric=0.530687689781189)
Finished Training
Total time taken: 0.19899964332580566
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49602.480
Exploding loss, terminate run (best metric=0.5359732508659363)
Finished Training
Total time taken: 0.23800206184387207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49463.406
Exploding loss, terminate run (best metric=0.5289424061775208)
Finished Training
Total time taken: 0.20499682426452637
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49470.934
Exploding loss, terminate run (best metric=0.5324937105178833)
Finished Training
Total time taken: 0.21799826622009277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 49792.336
Exploding loss, terminate run (best metric=0.5531883835792542)
Finished Training
Total time taken: 0.21700167655944824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49483.715
Exploding loss, terminate run (best metric=0.546392560005188)
Finished Training
Total time taken: 0.18999910354614258
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49246.758
Exploding loss, terminate run (best metric=0.5279183983802795)
Finished Training
Total time taken: 0.21700119972229004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49425.832
Exploding loss, terminate run (best metric=0.5309662818908691)
Finished Training
Total time taken: 0.22600221633911133
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49600.398
Exploding loss, terminate run (best metric=0.5286992788314819)
Finished Training
Total time taken: 0.20599937438964844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 49478.855
Exploding loss, terminate run (best metric=0.5343670845031738)
Finished Training
Total time taken: 0.2010033130645752
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49549.273
Exploding loss, terminate run (best metric=0.5338296294212341)
Finished Training
Total time taken: 0.21400189399719238
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49499.945
Exploding loss, terminate run (best metric=0.5289154648780823)
Finished Training
Total time taken: 0.21499919891357422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49554.930
Exploding loss, terminate run (best metric=0.5258073210716248)
Finished Training
Total time taken: 0.21799969673156738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 49703.984
Exploding loss, terminate run (best metric=0.5268091559410095)
Finished Training
Total time taken: 0.20000028610229492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 49581.922
Exploding loss, terminate run (best metric=0.5353930592536926)
Finished Training
Total time taken: 0.23200273513793945
{'Hydroxylation-K Validation Accuracy': 0.5286347517730496, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5421052631578948, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6203703703703703, 'Hydroxylation-K AUC PR': 0.35488456766034865, 'Hydroxylation-K MCC': 0.011661857142355327, 'Hydroxylation-K F1': 0.15996839855005113, 'Validation Loss (Hydroxylation-K)': 0.5578020493189494, 'Hydroxylation-P Validation Accuracy': 0.5235007360032485, 'Hydroxylation-P Validation Sensitivity': 0.46476190476190476, 'Hydroxylation-P Validation Specificity': 0.5365853658536586, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.560849925618742, 'Hydroxylation-P AUC PR': 0.25353746516240205, 'Hydroxylation-P MCC': 0.002468384034727649, 'Hydroxylation-P F1': 0.14020223091409534, 'Validation Loss (Hydroxylation-P)': 0.5333589116732279, 'Validation Loss (total)': 1.0911609729131062, 'TimeToTrain': 0.21306716601053874}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027792399317944087,
 'learning_rate_Hydroxylation-K': 0.00047299020527739893,
 'learning_rate_Hydroxylation-P': 0.0017577026752101763,
 'log_base': 1.2229564858594693,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3204407758,
 'sample_weights': [152.4562495349101, 19.01743404804911],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.297194594922719,
 'weight_decay_Hydroxylation-K': 9.265034710034096,
 'weight_decay_Hydroxylation-P': 1.7314284302580762}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2703.825
[2,     1] loss: 2681.323
[3,     1] loss: 2695.194
[4,     1] loss: 2685.593
[5,     1] loss: 2699.086
[6,     1] loss: 2691.376
[7,     1] loss: 2674.002
[8,     1] loss: 2678.806
[9,     1] loss: 2664.977
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004367016384750137,
 'learning_rate_Hydroxylation-K': 0.00046279817011693133,
 'learning_rate_Hydroxylation-P': 0.0020383124301954405,
 'log_base': 2.9640351277046113,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1662364299,
 'sample_weights': [8.29449277935303, 1.0368515395653122],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.502254563344845,
 'weight_decay_Hydroxylation-K': 4.418920989499112,
 'weight_decay_Hydroxylation-P': 2.5938352156227222}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.059
[2,     1] loss: 1231.363
[3,     1] loss: 1232.072
[4,     1] loss: 1232.146
[5,     1] loss: 1231.798
[6,     1] loss: 1229.430
[7,     1] loss: 1233.592
[8,     1] loss: 1226.708
[9,     1] loss: 1225.170
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009933477002445868,
 'learning_rate_Hydroxylation-K': 0.004124929203659164,
 'learning_rate_Hydroxylation-P': 0.004002312203001739,
 'log_base': 1.4143621188266247,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 729130985,
 'sample_weights': [1.536460128263935, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.160760662901398,
 'weight_decay_Hydroxylation-K': 4.410642031072212,
 'weight_decay_Hydroxylation-P': 7.9131983618111255}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1933.887
[2,     1] loss: 1936.072
[3,     1] loss: 1918.287
[4,     1] loss: 1936.627
[5,     1] loss: 1926.638
[6,     1] loss: 1926.483
[7,     1] loss: 1927.348
[8,     1] loss: 1922.026
[9,     1] loss: 1923.603
[10,     1] loss: 1920.619
[11,     1] loss: 1922.164
[12,     1] loss: 1922.372
[13,     1] loss: 1923.872
[14,     1] loss: 1925.019
[15,     1] loss: 1921.368
[16,     1] loss: 1923.202
[17,     1] loss: 1921.184
[18,     1] loss: 1923.242
[19,     1] loss: 1921.722
[20,     1] loss: 1922.918
[21,     1] loss: 1922.530
[22,     1] loss: 1921.957
[23,     1] loss: 1921.878
[24,     1] loss: 1922.274
[25,     1] loss: 1921.049
[26,     1] loss: 1922.472
[27,     1] loss: 1925.049
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005233477960817615,
 'learning_rate_Hydroxylation-K': 0.005556379046884349,
 'learning_rate_Hydroxylation-P': 0.009727436404548826,
 'log_base': 2.9270210215867793,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 268447660,
 'sample_weights': [4.815535203992549, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.167136731922527,
 'weight_decay_Hydroxylation-K': 9.3575713896065,
 'weight_decay_Hydroxylation-P': 3.7516672462555976}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.605
[2,     1] loss: 1234.657
[3,     1] loss: 1238.349
[4,     1] loss: 1236.371
[5,     1] loss: 1235.628
[6,     1] loss: 1235.848
[7,     1] loss: 1235.014
[8,     1] loss: 1232.733
[9,     1] loss: 1233.338
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006355917029059402,
 'learning_rate_Hydroxylation-K': 0.008916613731008174,
 'learning_rate_Hydroxylation-P': 0.007486079824521654,
 'log_base': 1.1303281121982298,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 650262393,
 'sample_weights': [1.5544377739347615, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.626849908630744,
 'weight_decay_Hydroxylation-K': 3.292524550309407,
 'weight_decay_Hydroxylation-P': 2.487601214747573}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4424.315
[2,     1] loss: 4434.460
[3,     1] loss: 4429.247
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031603199526542445,
 'learning_rate_Hydroxylation-K': 0.002009728555855244,
 'learning_rate_Hydroxylation-P': 0.00935822671370086,
 'log_base': 2.7075724302133883,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4117217150,
 'sample_weights': [13.627222351422295, 1.7034684158436428],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0190049232635734,
 'weight_decay_Hydroxylation-K': 7.898268509661948,
 'weight_decay_Hydroxylation-P': 3.9174930055184016}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.941
[2,     1] loss: 1262.956
[3,     1] loss: 1258.833
[4,     1] loss: 1259.243
[5,     1] loss: 1260.237
[6,     1] loss: 1257.432
[7,     1] loss: 1254.793
[8,     1] loss: 1247.569
[9,     1] loss: 1242.123
[10,     1] loss: 1233.735
[11,     1] loss: 1211.851
[12,     1] loss: 1185.359
[13,     1] loss: 1160.946
[14,     1] loss: 1135.606
[15,     1] loss: 1098.400
[16,     1] loss: 1081.708
[17,     1] loss: 1087.972
[18,     1] loss: 1023.596
[19,     1] loss: 1018.320
[20,     1] loss: 1043.483
[21,     1] loss: 1022.626
[22,     1] loss: 981.333
[23,     1] loss: 1064.363
[24,     1] loss: 972.683
[25,     1] loss: 1012.023
[26,     1] loss: 997.391
[27,     1] loss: 993.633
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008470728940903796,
 'learning_rate_Hydroxylation-K': 0.007466896452977249,
 'learning_rate_Hydroxylation-P': 0.008811003754056787,
 'log_base': 1.0464316403067313,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1593259107,
 'sample_weights': [1.67605947393866, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.394299710374609,
 'weight_decay_Hydroxylation-K': 5.936379177636181,
 'weight_decay_Hydroxylation-P': 3.9626695202846127}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12009.053
[2,     1] loss: 11859.375
[3,     1] loss: 12019.836
[4,     1] loss: 11963.105
[5,     1] loss: 11952.732
[6,     1] loss: 11941.355
[7,     1] loss: 11934.621
[8,     1] loss: 11923.587
[9,     1] loss: 11936.186
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007378285381188704,
 'learning_rate_Hydroxylation-K': 0.009465339962874067,
 'learning_rate_Hydroxylation-P': 0.008726317742686066,
 'log_base': 1.178728110724702,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3629244603,
 'sample_weights': [36.78326813099356, 4.598085646273741],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.569585357348842,
 'weight_decay_Hydroxylation-K': 6.2558228781972085,
 'weight_decay_Hydroxylation-P': 9.981083479469685}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3281.452
[2,     1] loss: 3325.836
[3,     1] loss: 3309.800
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002927308875346344,
 'learning_rate_Hydroxylation-K': 0.0020682994627962657,
 'learning_rate_Hydroxylation-P': 0.009133923483484461,
 'log_base': 2.0772646584798977,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 86766182,
 'sample_weights': [10.152541419433009, 1.2691165670121656],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.487952446150935,
 'weight_decay_Hydroxylation-K': 8.91056204585781,
 'weight_decay_Hydroxylation-P': 6.188325902770764}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1388.801
[2,     1] loss: 1391.976
[3,     1] loss: 1389.227
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035604972638204535,
 'learning_rate_Hydroxylation-K': 0.005956816739070586,
 'learning_rate_Hydroxylation-P': 0.009841314672693642,
 'log_base': 1.816107424402763,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 974432910,
 'sample_weights': [2.2836176336018674, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.617796173424576,
 'weight_decay_Hydroxylation-K': 8.768260195904496,
 'weight_decay_Hydroxylation-P': 3.795819075443193}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1501.535
[2,     1] loss: 1495.509
[3,     1] loss: 1497.042
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008307468678121484,
 'learning_rate_Hydroxylation-K': 0.007825981641447736,
 'learning_rate_Hydroxylation-P': 0.0070253603308600525,
 'log_base': 1.0615028843544045,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 649988511,
 'sample_weights': [2.7978145223737894, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.938952557964666,
 'weight_decay_Hydroxylation-K': 6.696515927980206,
 'weight_decay_Hydroxylation-P': 1.1808616792166287}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9099.467
[2,     1] loss: 9110.353
[3,     1] loss: 9085.166
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007408560316737369,
 'learning_rate_Hydroxylation-K': 0.004378814842705383,
 'learning_rate_Hydroxylation-P': 0.009214462894249533,
 'log_base': 2.4484119312130055,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1622026979,
 'sample_weights': [27.970562542719264, 3.496455010138477],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.653476304663718,
 'weight_decay_Hydroxylation-K': 3.1716774671204404,
 'weight_decay_Hydroxylation-P': 7.675661349356421}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1303.361
[2,     1] loss: 1302.918
[3,     1] loss: 1303.426
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038090823805332803,
 'learning_rate_Hydroxylation-K': 0.0019352870326242886,
 'learning_rate_Hydroxylation-P': 0.0015908770248653247,
 'log_base': 1.0839789545759357,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 784613850,
 'sample_weights': [1.8643838227090137, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.281827165671369,
 'weight_decay_Hydroxylation-K': 8.41320058137895,
 'weight_decay_Hydroxylation-P': 8.211115561486109}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6720.049
[2,     1] loss: 6720.188
[3,     1] loss: 6710.166
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008909711283936621,
 'learning_rate_Hydroxylation-K': 0.005997852735845138,
 'learning_rate_Hydroxylation-P': 0.0094571659761031,
 'log_base': 1.5259829526864803,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2630629746,
 'sample_weights': [20.702808101242596, 2.5879507070681815],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.6475693207677935,
 'weight_decay_Hydroxylation-K': 7.196884387798196,
 'weight_decay_Hydroxylation-P': 6.918126747062675}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1745.164
[2,     1] loss: 1754.644
[3,     1] loss: 1740.190
[4,     1] loss: 1732.496
[5,     1] loss: 1748.418
[6,     1] loss: 1744.946
[7,     1] loss: 1742.266
[8,     1] loss: 1739.979
[9,     1] loss: 1737.615
[10,     1] loss: 1736.533
[11,     1] loss: 1735.575
[12,     1] loss: 1735.719
[13,     1] loss: 1731.055
[14,     1] loss: 1733.888
[15,     1] loss: 1720.911
[16,     1] loss: 1706.072
[17,     1] loss: 1687.536
[18,     1] loss: 1628.028
[19,     1] loss: 1576.406
[20,     1] loss: 1520.340
[21,     1] loss: 1553.642
[22,     1] loss: 1475.708
[23,     1] loss: 1421.584
[24,     1] loss: 1405.719
[25,     1] loss: 1472.312
[26,     1] loss: 1326.624
[27,     1] loss: 1375.788
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002980123596627941,
 'learning_rate_Hydroxylation-K': 0.0020483350196036213,
 'learning_rate_Hydroxylation-P': 0.009441778897099459,
 'log_base': 2.484865585119137,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2889716952,
 'sample_weights': [3.950047414641852, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.516272658585847,
 'weight_decay_Hydroxylation-K': 7.026469821843534,
 'weight_decay_Hydroxylation-P': 3.615521692320331}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.902
[2,     1] loss: 1292.802
[3,     1] loss: 1294.611
[4,     1] loss: 1292.949
[5,     1] loss: 1296.406
[6,     1] loss: 1292.708
[7,     1] loss: 1288.262
[8,     1] loss: 1284.558
[9,     1] loss: 1277.804
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00367894583514416,
 'learning_rate_Hydroxylation-K': 0.0036153988675838786,
 'learning_rate_Hydroxylation-P': 0.007581115737820891,
 'log_base': 1.3332617487411285,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 641897219,
 'sample_weights': [1.834112384504829, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.038101318303815,
 'weight_decay_Hydroxylation-K': 3.0362252850913474,
 'weight_decay_Hydroxylation-P': 6.03163792888378}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2127.988
[2,     1] loss: 2127.675
[3,     1] loss: 2140.344
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034941856930742555,
 'learning_rate_Hydroxylation-K': 0.007476464847407542,
 'learning_rate_Hydroxylation-P': 0.005227176645537539,
 'log_base': 1.337343442371,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3876246090,
 'sample_weights': [5.804166934181327, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.16766739048753,
 'weight_decay_Hydroxylation-K': 6.789172620877528,
 'weight_decay_Hydroxylation-P': 5.346596922163002}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2114.318
[2,     1] loss: 2119.860
[3,     1] loss: 2115.914
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035055076230628033,
 'learning_rate_Hydroxylation-K': 0.0009639279415347554,
 'learning_rate_Hydroxylation-P': 0.009849655585965685,
 'log_base': 2.844632020995035,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1387272418,
 'sample_weights': [5.74313205184614, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.534943520370624,
 'weight_decay_Hydroxylation-K': 7.258772537609746,
 'weight_decay_Hydroxylation-P': 5.4591507887082935}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.534
[2,     1] loss: 1250.077
[3,     1] loss: 1241.269
[4,     1] loss: 1243.696
[5,     1] loss: 1244.467
[6,     1] loss: 1239.908
[7,     1] loss: 1239.219
[8,     1] loss: 1235.967
[9,     1] loss: 1233.043
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050776496057473005,
 'learning_rate_Hydroxylation-K': 0.005016664836521483,
 'learning_rate_Hydroxylation-P': 0.007747717831382448,
 'log_base': 1.0648119316291116,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1621799454,
 'sample_weights': [1.5968904776135806, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.70488789445401,
 'weight_decay_Hydroxylation-K': 7.767222301403212,
 'weight_decay_Hydroxylation-P': 6.37228081834042}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8669.133
[2,     1] loss: 8623.315
[3,     1] loss: 8608.590
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00694625314833373,
 'learning_rate_Hydroxylation-K': 0.0003014929174944091,
 'learning_rate_Hydroxylation-P': 0.0027405447067602754,
 'log_base': 1.6925851449882918,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1891973650,
 'sample_weights': [26.584254308998776, 3.3231598051534075],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.306742572498667,
 'weight_decay_Hydroxylation-K': 8.949303647175315,
 'weight_decay_Hydroxylation-P': 1.3440263921040199}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1583.787
[2,     1] loss: 1586.131
[3,     1] loss: 1577.501
[4,     1] loss: 1577.288
[5,     1] loss: 1573.341
[6,     1] loss: 1583.516
[7,     1] loss: 1578.120
[8,     1] loss: 1577.694
[9,     1] loss: 1574.797
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005707703387486738,
 'learning_rate_Hydroxylation-K': 0.009515760764653765,
 'learning_rate_Hydroxylation-P': 0.007467642815119175,
 'log_base': 2.5902268387857164,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2608200918,
 'sample_weights': [3.1722961340949123, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.636258433092182,
 'weight_decay_Hydroxylation-K': 2.233997762708839,
 'weight_decay_Hydroxylation-P': 0.014080444385738966}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1281.502
[2,     1] loss: 1279.554
[3,     1] loss: 1278.620
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006164076285301436,
 'learning_rate_Hydroxylation-K': 0.0088546446614791,
 'learning_rate_Hydroxylation-P': 0.009022354546872143,
 'log_base': 1.0153079637457323,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3101331434,
 'sample_weights': [1.754085758615657, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.20431610952996,
 'weight_decay_Hydroxylation-K': 7.605393541956801,
 'weight_decay_Hydroxylation-P': 2.6739654475674657}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35707.430
Exploding loss, terminate run (best metric=0.5297570824623108)
Finished Training
Total time taken: 0.2040109634399414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35797.473
Exploding loss, terminate run (best metric=0.5338547825813293)
Finished Training
Total time taken: 0.22900176048278809
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35729.723
Exploding loss, terminate run (best metric=0.529059886932373)
Finished Training
Total time taken: 0.2090001106262207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35954.094
Exploding loss, terminate run (best metric=0.5268769860267639)
Finished Training
Total time taken: 0.22499728202819824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35728.566
Exploding loss, terminate run (best metric=0.5337862968444824)
Finished Training
Total time taken: 0.23099946975708008
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35599.109
Exploding loss, terminate run (best metric=0.5312666296958923)
Finished Training
Total time taken: 0.2180030345916748
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35769.375
Exploding loss, terminate run (best metric=0.5295534133911133)
Finished Training
Total time taken: 0.24399709701538086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35823.547
Exploding loss, terminate run (best metric=0.5280686616897583)
Finished Training
Total time taken: 0.2220008373260498
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35574.359
Exploding loss, terminate run (best metric=0.5393952131271362)
Finished Training
Total time taken: 0.24200057983398438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35715.984
Exploding loss, terminate run (best metric=0.5407350659370422)
Finished Training
Total time taken: 0.23600172996520996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35735.305
Exploding loss, terminate run (best metric=0.5329093337059021)
Finished Training
Total time taken: 0.23100876808166504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36018.914
Exploding loss, terminate run (best metric=0.5263469219207764)
Finished Training
Total time taken: 0.2070012092590332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35766.812
Exploding loss, terminate run (best metric=0.5331390500068665)
Finished Training
Total time taken: 0.21799707412719727
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 35653.148
Exploding loss, terminate run (best metric=0.5317656993865967)
Finished Training
Total time taken: 0.208998441696167
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 35678.234
Exploding loss, terminate run (best metric=0.5283690094947815)
Finished Training
Total time taken: 0.20699858665466309
{'Hydroxylation-K Validation Accuracy': 0.4814125295508274, 'Hydroxylation-K Validation Sensitivity': 0.5266666666666666, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.627270955165692, 'Hydroxylation-K AUC PR': 0.31785028522583625, 'Hydroxylation-K MCC': -0.0071828575111990535, 'Hydroxylation-K F1': 0.19276699185421295, 'Validation Loss (Hydroxylation-K)': 0.5577330907185872, 'Hydroxylation-P Validation Accuracy': 0.48284195387712975, 'Hydroxylation-P Validation Sensitivity': 0.5295238095238095, 'Hydroxylation-P Validation Specificity': 0.4719512195121951, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5966084145430995, 'Hydroxylation-P AUC PR': 0.29374563866225134, 'Hydroxylation-P MCC': 0.003459546031424619, 'Hydroxylation-P F1': 0.17627222860368222, 'Validation Loss (Hydroxylation-P)': 0.531658935546875, 'Validation Loss (total)': 1.0893920183181762, 'TimeToTrain': 0.22213446299235026}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004588344998901337,
 'learning_rate_Hydroxylation-K': 0.007526062307437537,
 'learning_rate_Hydroxylation-P': 0.005751503994965546,
 'log_base': 1.639819149493318,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1870521300,
 'sample_weights': [109.97129039496468, 13.717848685410605],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7227402321464214,
 'weight_decay_Hydroxylation-K': 6.0943765936893755,
 'weight_decay_Hydroxylation-P': 2.8429353442101974}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1621.290
[2,     1] loss: 1626.879
[3,     1] loss: 1624.977
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004688108018206282,
 'learning_rate_Hydroxylation-K': 0.002966176691729208,
 'learning_rate_Hydroxylation-P': 0.009376024435949683,
 'log_base': 2.7382124765955433,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4086789411,
 'sample_weights': [3.3754357763380636, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.701158813954854,
 'weight_decay_Hydroxylation-K': 4.2413284689202895,
 'weight_decay_Hydroxylation-P': 3.7940226022123067}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.006
[2,     1] loss: 1261.638
[3,     1] loss: 1258.313
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00983547817533542,
 'learning_rate_Hydroxylation-K': 0.0053646635794705145,
 'learning_rate_Hydroxylation-P': 0.007534733481874135,
 'log_base': 2.843001752852005,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1109821106,
 'sample_weights': [1.6573357681814034, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.022134886085404,
 'weight_decay_Hydroxylation-K': 3.363401934016382,
 'weight_decay_Hydroxylation-P': 1.215572706817769}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.814
[2,     1] loss: 1250.227
[3,     1] loss: 1247.245
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005150543750302502,
 'learning_rate_Hydroxylation-K': 0.0003223827651280647,
 'learning_rate_Hydroxylation-P': 0.00890598542574457,
 'log_base': 1.1996752114487295,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2539173759,
 'sample_weights': [1.597766619142156, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.644019363520375,
 'weight_decay_Hydroxylation-K': 3.9142708875479566,
 'weight_decay_Hydroxylation-P': 1.0501914432708883}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2968.565
[2,     1] loss: 2977.991
[3,     1] loss: 2975.509
[4,     1] loss: 2970.512
[5,     1] loss: 2971.657
[6,     1] loss: 2976.990
[7,     1] loss: 2972.029
[8,     1] loss: 2967.907
[9,     1] loss: 2955.354
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006247679259305609,
 'learning_rate_Hydroxylation-K': 0.002329295568118945,
 'learning_rate_Hydroxylation-P': 0.007528002771251532,
 'log_base': 2.6777629032469417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2844660810,
 'sample_weights': [9.170201775506749, 1.1463193810629089],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4174930295440493,
 'weight_decay_Hydroxylation-K': 8.390559379597931,
 'weight_decay_Hydroxylation-P': 4.409790016442793}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.363
[2,     1] loss: 1271.217
[3,     1] loss: 1267.652
[4,     1] loss: 1262.834
[5,     1] loss: 1262.595
[6,     1] loss: 1264.510
[7,     1] loss: 1257.864
[8,     1] loss: 1263.400
[9,     1] loss: 1255.672
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0064770426099733215,
 'learning_rate_Hydroxylation-K': 0.00035703687802162397,
 'learning_rate_Hydroxylation-P': 0.009504963377324952,
 'log_base': 2.9373719931657147,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2941481462,
 'sample_weights': [1.6948976141248868, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.096963972998438,
 'weight_decay_Hydroxylation-K': 9.183339821187522,
 'weight_decay_Hydroxylation-P': 3.8564664159709503}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.751
[2,     1] loss: 1237.699
[3,     1] loss: 1233.496
[4,     1] loss: 1237.970
[5,     1] loss: 1235.160
[6,     1] loss: 1231.164
[7,     1] loss: 1229.730
[8,     1] loss: 1230.382
[9,     1] loss: 1231.498
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010759389110986317,
 'learning_rate_Hydroxylation-K': 0.0011674789456343794,
 'learning_rate_Hydroxylation-P': 0.009027679229325023,
 'log_base': 2.988865657428196,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4077785509,
 'sample_weights': [1.5493451877196545, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.40348097958663,
 'weight_decay_Hydroxylation-K': 9.899901964291157,
 'weight_decay_Hydroxylation-P': 2.9988086362421673}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.363
[2,     1] loss: 1232.306
[3,     1] loss: 1228.975
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005630353628126012,
 'learning_rate_Hydroxylation-K': 0.0010284385263173918,
 'learning_rate_Hydroxylation-P': 0.0010918184024047036,
 'log_base': 1.406222176330236,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 605603777,
 'sample_weights': [1.524753304010842, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.814665307702376,
 'weight_decay_Hydroxylation-K': 5.391073427102299,
 'weight_decay_Hydroxylation-P': 0.7887385148876036}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1945.497
[2,     1] loss: 1945.913
[3,     1] loss: 1936.287
[4,     1] loss: 1944.385
[5,     1] loss: 1944.272
[6,     1] loss: 1946.082
[7,     1] loss: 1938.307
[8,     1] loss: 1933.665
[9,     1] loss: 1936.119
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011325377894686674,
 'learning_rate_Hydroxylation-K': 0.004853910758606894,
 'learning_rate_Hydroxylation-P': 0.0065034380099258365,
 'log_base': 2.8146286206901046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1483601335,
 'sample_weights': [4.897066125584814, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8972786755689013,
 'weight_decay_Hydroxylation-K': 9.254450182547675,
 'weight_decay_Hydroxylation-P': 5.1424072796178875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.954
[2,     1] loss: 1248.885
[3,     1] loss: 1250.444
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0058979265775872996,
 'learning_rate_Hydroxylation-K': 0.007460978337055965,
 'learning_rate_Hydroxylation-P': 0.009224222677584637,
 'log_base': 1.7280582163670224,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2624260809,
 'sample_weights': [1.6132530216915233, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.800394280448055,
 'weight_decay_Hydroxylation-K': 5.11578829384586,
 'weight_decay_Hydroxylation-P': 1.7830813523417708}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1561.262
[2,     1] loss: 1547.445
[3,     1] loss: 1550.759
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0075260459726829605,
 'learning_rate_Hydroxylation-K': 0.004455306599296688,
 'learning_rate_Hydroxylation-P': 0.007765677392290419,
 'log_base': 1.0709729608987508,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 516373394,
 'sample_weights': [3.052007592686699, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.323897551305359,
 'weight_decay_Hydroxylation-K': 6.2956953062605585,
 'weight_decay_Hydroxylation-P': 7.657390305583749}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7894.372
[2,     1] loss: 7890.587
[3,     1] loss: 7890.045
[4,     1] loss: 7906.109
[5,     1] loss: 7890.718
[6,     1] loss: 7878.651
[7,     1] loss: 7873.868
[8,     1] loss: 7857.662
[9,     1] loss: 7867.105
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004266800563004384,
 'learning_rate_Hydroxylation-K': 0.006906990109469869,
 'learning_rate_Hydroxylation-P': 0.00850252205078275,
 'log_base': 1.1064283685281273,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3539173514,
 'sample_weights': [24.34742498492707, 3.043545368188464],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.276671141065775,
 'weight_decay_Hydroxylation-K': 6.790186534218978,
 'weight_decay_Hydroxylation-P': 2.2468630158323712}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5373.782
[2,     1] loss: 5362.561
[3,     1] loss: 5379.137
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020707280178974595,
 'learning_rate_Hydroxylation-K': 0.0019057683160033597,
 'learning_rate_Hydroxylation-P': 0.00025790470764554914,
 'log_base': 1.8751271964934706,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1471410014,
 'sample_weights': [16.506726646754597, 2.06342031901874],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.802372113403371,
 'weight_decay_Hydroxylation-K': 6.493683975857693,
 'weight_decay_Hydroxylation-P': 2.9676308848634294}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1474.579
[2,     1] loss: 1470.427
[3,     1] loss: 1470.194
[4,     1] loss: 1471.782
[5,     1] loss: 1467.557
[6,     1] loss: 1466.249
[7,     1] loss: 1465.294
[8,     1] loss: 1465.099
[9,     1] loss: 1464.712
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005278610846157171,
 'learning_rate_Hydroxylation-K': 0.0020047476458421613,
 'learning_rate_Hydroxylation-P': 0.009895027563130397,
 'log_base': 1.0288182450469048,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1905153626,
 'sample_weights': [2.6554884110249524, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.594909983889494,
 'weight_decay_Hydroxylation-K': 7.684162659017257,
 'weight_decay_Hydroxylation-P': 0.1348574552114914}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19068.412
Exploding loss, terminate run (best metric=0.5399930477142334)
Finished Training
Total time taken: 0.21601080894470215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19014.379
Exploding loss, terminate run (best metric=0.5285484790802002)
Finished Training
Total time taken: 0.23500323295593262
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19097.680
Exploding loss, terminate run (best metric=0.5319938063621521)
Finished Training
Total time taken: 0.20999836921691895
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19089.289
Exploding loss, terminate run (best metric=0.5314348340034485)
Finished Training
Total time taken: 0.22300243377685547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19038.256
Exploding loss, terminate run (best metric=0.531326949596405)
Finished Training
Total time taken: 0.19100189208984375
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19045.455
Exploding loss, terminate run (best metric=0.5346923470497131)
Finished Training
Total time taken: 0.20999860763549805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19006.428
Exploding loss, terminate run (best metric=0.5276967883110046)
Finished Training
Total time taken: 0.20900225639343262
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19032.420
Exploding loss, terminate run (best metric=0.5276729464530945)
Finished Training
Total time taken: 0.22699975967407227
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19173.340
Exploding loss, terminate run (best metric=0.5274937748908997)
Finished Training
Total time taken: 0.20399999618530273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19094.229
Exploding loss, terminate run (best metric=0.5307851433753967)
Finished Training
Total time taken: 0.23000001907348633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19167.711
Exploding loss, terminate run (best metric=0.5364231467247009)
Finished Training
Total time taken: 0.20199847221374512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19055.203
Exploding loss, terminate run (best metric=0.526371955871582)
Finished Training
Total time taken: 0.2239973545074463
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19062.430
Exploding loss, terminate run (best metric=0.5377697348594666)
Finished Training
Total time taken: 0.207000732421875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19169.602
Exploding loss, terminate run (best metric=0.528741180896759)
Finished Training
Total time taken: 0.221998929977417
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19050.594
Exploding loss, terminate run (best metric=0.5321109890937805)
Finished Training
Total time taken: 0.20099854469299316
{'Hydroxylation-K Validation Accuracy': 0.5605791962174941, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.6, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6639961013645224, 'Hydroxylation-K AUC PR': 0.37747405210815366, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.13481116584564862, 'Validation Loss (Hydroxylation-K)': 0.5562580386797588, 'Hydroxylation-P Validation Accuracy': 0.5648273691690777, 'Hydroxylation-P Validation Sensitivity': 0.3980952380952381, 'Hydroxylation-P Validation Specificity': 0.6016260162601627, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5644519894512413, 'Hydroxylation-P AUC PR': 0.24445303086578685, 'Hydroxylation-P MCC': -0.0006780756818135361, 'Hydroxylation-P F1': 0.11951131514450293, 'Validation Loss (Hydroxylation-P)': 0.5315370082855224, 'Validation Loss (total)': 1.0877950509389243, 'TimeToTrain': 0.21406742731730144}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00801605024896817,
 'learning_rate_Hydroxylation-K': 0.00967451962297855,
 'learning_rate_Hydroxylation-P': 0.008433956477472358,
 'log_base': 1.1390505692535013,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4241788774,
 'sample_weights': [58.80443456096552, 7.335281166941642],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.14336220329768,
 'weight_decay_Hydroxylation-K': 5.052658182338303,
 'weight_decay_Hydroxylation-P': 5.452751315857312}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4175.269
[2,     1] loss: 4172.283
[3,     1] loss: 4163.130
[4,     1] loss: 4134.824
[5,     1] loss: 4178.922
[6,     1] loss: 4148.354
[7,     1] loss: 4165.562
[8,     1] loss: 4157.393
[9,     1] loss: 4160.946
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004222353097709865,
 'learning_rate_Hydroxylation-K': 0.00961097054428426,
 'learning_rate_Hydroxylation-P': 0.004761974214147816,
 'log_base': 1.152512209377149,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1754685649,
 'sample_weights': [12.822628391364102, 1.602890296312582],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.278694398699466,
 'weight_decay_Hydroxylation-K': 4.248064661827577,
 'weight_decay_Hydroxylation-P': 1.4052808153370853}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3813.051
[2,     1] loss: 3814.991
[3,     1] loss: 3800.156
[4,     1] loss: 3817.563
[5,     1] loss: 3801.497
[6,     1] loss: 3802.518
[7,     1] loss: 3793.301
[8,     1] loss: 3804.499
[9,     1] loss: 3779.141
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007741682349318786,
 'learning_rate_Hydroxylation-K': 0.008886277815068703,
 'learning_rate_Hydroxylation-P': 0.008659845414591135,
 'log_base': 1.1320653411874524,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3963234824,
 'sample_weights': [11.761272706135149, 1.470215724698587],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.493506833951757,
 'weight_decay_Hydroxylation-K': 6.9909813664099545,
 'weight_decay_Hydroxylation-P': 0.9539281776970574}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4364.006
[2,     1] loss: 4370.850
[3,     1] loss: 4377.887
[4,     1] loss: 4371.647
[5,     1] loss: 4381.072
[6,     1] loss: 4356.882
[7,     1] loss: 4355.333
[8,     1] loss: 4366.564
[9,     1] loss: 4371.024
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007002420200277061,
 'learning_rate_Hydroxylation-K': 0.007911953213256714,
 'learning_rate_Hydroxylation-P': 0.008178032504210876,
 'log_base': 1.1693631129603959,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3855416539,
 'sample_weights': [13.4585081488187, 1.682378328074621],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.861605871162306,
 'weight_decay_Hydroxylation-K': 6.003917337239924,
 'weight_decay_Hydroxylation-P': 2.028835663724605}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3471.861
[2,     1] loss: 3457.877
[3,     1] loss: 3467.122
[4,     1] loss: 3470.032
[5,     1] loss: 3460.643
[6,     1] loss: 3457.257
[7,     1] loss: 3468.981
[8,     1] loss: 3452.716
[9,     1] loss: 3455.225
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037632228315613836,
 'learning_rate_Hydroxylation-K': 0.007182312899682403,
 'learning_rate_Hydroxylation-P': 0.006427302275431509,
 'log_base': 1.0566597022278117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 754603587,
 'sample_weights': [10.670146498840245, 1.3338196944663414],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.265990128960755,
 'weight_decay_Hydroxylation-K': 8.94136946174502,
 'weight_decay_Hydroxylation-P': 4.065635141911165}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9830.154
[2,     1] loss: 9811.438
[3,     1] loss: 9810.073
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004049325820105391,
 'learning_rate_Hydroxylation-K': 0.0012175597834658215,
 'learning_rate_Hydroxylation-P': 0.009238416830123853,
 'log_base': 1.034120806193796,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4031811741,
 'sample_weights': [30.291437320021082, 3.786575533478228],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.387920978277924,
 'weight_decay_Hydroxylation-K': 6.860275640446207,
 'weight_decay_Hydroxylation-P': 1.343075319181014}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16106.247
Exploding loss, terminate run (best metric=0.5314863920211792)
Finished Training
Total time taken: 0.19299983978271484
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16141.392
Exploding loss, terminate run (best metric=0.5314857959747314)
Finished Training
Total time taken: 0.2110002040863037
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16283.245
Exploding loss, terminate run (best metric=0.5353332161903381)
Finished Training
Total time taken: 0.20599794387817383
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16152.149
Exploding loss, terminate run (best metric=0.5264552235603333)
Finished Training
Total time taken: 0.1979994773864746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16247.854
Exploding loss, terminate run (best metric=0.5355085730552673)
Finished Training
Total time taken: 0.21100234985351562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16123.744
Exploding loss, terminate run (best metric=0.5368159413337708)
Finished Training
Total time taken: 0.21500110626220703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16143.727
Exploding loss, terminate run (best metric=0.5296651721000671)
Finished Training
Total time taken: 0.21900010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16287.505
Exploding loss, terminate run (best metric=0.5279533863067627)
Finished Training
Total time taken: 0.23199987411499023
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16269.730
Exploding loss, terminate run (best metric=0.528693437576294)
Finished Training
Total time taken: 0.21103620529174805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16015.281
Exploding loss, terminate run (best metric=0.5459965467453003)
Finished Training
Total time taken: 0.22999930381774902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16169.723
Exploding loss, terminate run (best metric=0.5331107974052429)
Finished Training
Total time taken: 0.24800324440002441
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16186.311
Exploding loss, terminate run (best metric=0.5272186994552612)
Finished Training
Total time taken: 0.2149975299835205
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16128.234
Exploding loss, terminate run (best metric=0.534178614616394)
Finished Training
Total time taken: 0.2070000171661377
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16115.992
Exploding loss, terminate run (best metric=0.5365573167800903)
Finished Training
Total time taken: 0.21600031852722168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16237.237
Exploding loss, terminate run (best metric=0.5282108187675476)
Finished Training
Total time taken: 0.24000024795532227
{'Hydroxylation-K Validation Accuracy': 0.535047281323877, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5526315789473685, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.656140350877193, 'Hydroxylation-K AUC PR': 0.37435241227787003, 'Hydroxylation-K MCC': 0.01793425289496568, 'Hydroxylation-K F1': 0.1614778325123153, 'Validation Loss (Hydroxylation-K)': 0.5573284308115641, 'Hydroxylation-P Validation Accuracy': 0.5409190058034279, 'Hydroxylation-P Validation Sensitivity': 0.45142857142857146, 'Hydroxylation-P Validation Specificity': 0.5597560975609757, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.623556988750577, 'Hydroxylation-P AUC PR': 0.30937760812643395, 'Hydroxylation-P MCC': 0.012057353512827683, 'Hydroxylation-P F1': 0.14270065518167083, 'Validation Loss (Hydroxylation-P)': 0.5325779954592387, 'Validation Loss (total)': 1.0899064222971597, 'TimeToTrain': 0.21680251757303873}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004151142067703583,
 'learning_rate_Hydroxylation-K': 0.00832490786401157,
 'learning_rate_Hydroxylation-P': 0.007756814633421957,
 'log_base': 1.0856605091567366,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 356851273,
 'sample_weights': [49.794387840578395, 6.211365487541649],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.221925589523975,
 'weight_decay_Hydroxylation-K': 5.233310040054324,
 'weight_decay_Hydroxylation-P': 2.8373581731542696}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6612.398
[2,     1] loss: 6591.348
[3,     1] loss: 6585.333
[4,     1] loss: 6610.719
[5,     1] loss: 6587.916
[6,     1] loss: 6581.949
[7,     1] loss: 6577.241
[8,     1] loss: 6607.815
[9,     1] loss: 6572.367
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006990266343204009,
 'learning_rate_Hydroxylation-K': 0.009661156203847778,
 'learning_rate_Hydroxylation-P': 0.008053333961834592,
 'log_base': 1.4435999251405784,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3059520586,
 'sample_weights': [20.31235279156076, 2.5391419131196455],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.967733371556255,
 'weight_decay_Hydroxylation-K': 4.82973385469294,
 'weight_decay_Hydroxylation-P': 2.4018785719054576}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1872.143
[2,     1] loss: 1875.055
[3,     1] loss: 1871.183
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007164825915927698,
 'learning_rate_Hydroxylation-K': 0.007796919194740235,
 'learning_rate_Hydroxylation-P': 0.0088933513258192,
 'log_base': 1.1518117888582349,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 812450649,
 'sample_weights': [4.547157518862486, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.023784297098272,
 'weight_decay_Hydroxylation-K': 7.73854168017878,
 'weight_decay_Hydroxylation-P': 2.29179045053785}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3841.984
[2,     1] loss: 3833.263
[3,     1] loss: 3853.285
[4,     1] loss: 3819.897
[5,     1] loss: 3838.606
[6,     1] loss: 3800.718
[7,     1] loss: 3802.527
[8,     1] loss: 3797.504
[9,     1] loss: 3766.349
[10,     1] loss: 3766.820
[11,     1] loss: 3690.659
[12,     1] loss: 3587.993
[13,     1] loss: 3494.068
[14,     1] loss: 3366.646
[15,     1] loss: 3421.610
[16,     1] loss: 3101.620
[17,     1] loss: 3420.015
[18,     1] loss: 3155.719
[19,     1] loss: 3457.398
[20,     1] loss: 3458.894
[21,     1] loss: 3230.438
[22,     1] loss: 3309.056
[23,     1] loss: 3269.097
[24,     1] loss: 3182.333
[25,     1] loss: 3106.040
[26,     1] loss: 3168.528
[27,     1] loss: 2846.095
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00575796775189369,
 'learning_rate_Hydroxylation-K': 0.00993465545769547,
 'learning_rate_Hydroxylation-P': 0.0072390106304090025,
 'log_base': 1.072897540874688,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1124683715,
 'sample_weights': [11.811860568053396, 1.476539451044316],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.232593245468328,
 'weight_decay_Hydroxylation-K': 5.343489659595217,
 'weight_decay_Hydroxylation-P': 2.1519809144857547}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7726.535
[2,     1] loss: 7659.786
[3,     1] loss: 7666.283
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026924014989339567,
 'learning_rate_Hydroxylation-K': 0.00220348061122088,
 'learning_rate_Hydroxylation-P': 0.00612616542196275,
 'log_base': 2.1782324968819875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 191696594,
 'sample_weights': [23.726160687432852, 2.965884339302249],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3524976351088833,
 'weight_decay_Hydroxylation-K': 7.2880724464986475,
 'weight_decay_Hydroxylation-P': 9.855439072829284}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1363.546
[2,     1] loss: 1364.716
[3,     1] loss: 1360.558
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00583639789955389,
 'learning_rate_Hydroxylation-K': 0.00038304818831798836,
 'learning_rate_Hydroxylation-P': 0.006667884367602314,
 'log_base': 2.228179093147854,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2748504723,
 'sample_weights': [2.1443977214616385, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.651221795870686,
 'weight_decay_Hydroxylation-K': 6.664087615367794,
 'weight_decay_Hydroxylation-P': 8.163991901199179}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.290
[2,     1] loss: 1349.640
[3,     1] loss: 1346.032
[4,     1] loss: 1345.853
[5,     1] loss: 1345.121
[6,     1] loss: 1343.759
[7,     1] loss: 1340.918
[8,     1] loss: 1331.319
[9,     1] loss: 1307.498
[10,     1] loss: 1284.364
[11,     1] loss: 1239.758
[12,     1] loss: 1205.501
[13,     1] loss: 1200.312
[14,     1] loss: 1141.168
[15,     1] loss: 1156.998
[16,     1] loss: 1144.204
[17,     1] loss: 1245.714
[18,     1] loss: 1083.965
[19,     1] loss: 1203.327
[20,     1] loss: 1094.478
[21,     1] loss: 1102.037
[22,     1] loss: 1140.366
[23,     1] loss: 1094.986
[24,     1] loss: 1099.246
[25,     1] loss: 1054.066
[26,     1] loss: 1086.134
[27,     1] loss: 1026.412
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005876713315517656,
 'learning_rate_Hydroxylation-K': 0.008565497954244258,
 'learning_rate_Hydroxylation-P': 0.005936761645015491,
 'log_base': 1.0188610364993165,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3582430923,
 'sample_weights': [2.083718203512652, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.984823849034914,
 'weight_decay_Hydroxylation-K': 5.773944484581208,
 'weight_decay_Hydroxylation-P': 4.844695622184188}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28947.930
Exploding loss, terminate run (best metric=0.531511664390564)
Finished Training
Total time taken: 0.21000003814697266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28989.758
Exploding loss, terminate run (best metric=0.5320846438407898)
Finished Training
Total time taken: 0.22400236129760742
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28912.172
Exploding loss, terminate run (best metric=0.5315982103347778)
Finished Training
Total time taken: 0.20599794387817383
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29033.068
Exploding loss, terminate run (best metric=0.5256963968276978)
Finished Training
Total time taken: 0.21900177001953125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29026.688
Exploding loss, terminate run (best metric=0.5353723168373108)
Finished Training
Total time taken: 0.20199918746948242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29075.297
Exploding loss, terminate run (best metric=0.5386049747467041)
Finished Training
Total time taken: 0.212996244430542
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29030.398
Exploding loss, terminate run (best metric=0.5285544991493225)
Finished Training
Total time taken: 0.22700285911560059
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29088.570
Exploding loss, terminate run (best metric=0.5287678837776184)
Finished Training
Total time taken: 0.24500226974487305
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28729.641
Exploding loss, terminate run (best metric=0.5571638941764832)
Finished Training
Total time taken: 0.22299909591674805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28956.660
Exploding loss, terminate run (best metric=0.5292855501174927)
Finished Training
Total time taken: 0.2160022258758545
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28948.617
Exploding loss, terminate run (best metric=0.5317160487174988)
Finished Training
Total time taken: 0.24399709701538086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29047.438
Exploding loss, terminate run (best metric=0.5335716605186462)
Finished Training
Total time taken: 0.2160038948059082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29011.070
Exploding loss, terminate run (best metric=0.5271158814430237)
Finished Training
Total time taken: 0.21500039100646973
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28990.910
Exploding loss, terminate run (best metric=0.532767117023468)
Finished Training
Total time taken: 0.23199868202209473
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28965.660
Exploding loss, terminate run (best metric=0.5361143946647644)
Finished Training
Total time taken: 0.22800397872924805
{'Hydroxylation-K Validation Accuracy': 0.5258569739952719, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5385964912280702, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6286939571150097, 'Hydroxylation-K AUC PR': 0.33293431592374173, 'Hydroxylation-K MCC': 0.00883021571376696, 'Hydroxylation-K F1': 0.15905359008807285, 'Validation Loss (Hydroxylation-K)': 0.5581305623054504, 'Hydroxylation-P Validation Accuracy': 0.532208788047984, 'Hydroxylation-P Validation Sensitivity': 0.45714285714285713, 'Hydroxylation-P Validation Specificity': 0.5479674796747968, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.630224881579812, 'Hydroxylation-P AUC PR': 0.32391881153349306, 'Hydroxylation-P MCC': 0.004810442185115577, 'Hydroxylation-P F1': 0.14126102683508143, 'Validation Loss (Hydroxylation-P)': 0.5333283424377442, 'Validation Loss (total)': 1.0914589166641235, 'TimeToTrain': 0.22133386929829915}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004170598487721396,
 'learning_rate_Hydroxylation-K': 0.0055687685220371305,
 'learning_rate_Hydroxylation-P': 0.00955486246674971,
 'log_base': 1.044003984795744,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 501831408,
 'sample_weights': [89.41119778092822, 11.15317713864212],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.937507735865992,
 'weight_decay_Hydroxylation-K': 4.568318666721303,
 'weight_decay_Hydroxylation-P': 3.630912214051547}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12550.922
[2,     1] loss: 12582.934
[3,     1] loss: 12614.010
[4,     1] loss: 12592.429
[5,     1] loss: 12620.553
[6,     1] loss: 12557.509
[7,     1] loss: 12544.829
[8,     1] loss: 12561.107
[9,     1] loss: 12527.889
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023757396382193216,
 'learning_rate_Hydroxylation-K': 0.0024802315032391026,
 'learning_rate_Hydroxylation-P': 0.007438758680323892,
 'log_base': 1.1586710304587533,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3411398206,
 'sample_weights': [38.76718465390451, 4.846084765191501],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.277737778757187,
 'weight_decay_Hydroxylation-K': 0.27845049153530144,
 'weight_decay_Hydroxylation-P': 9.803336490305623}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3725.506
[2,     1] loss: 3686.726
[3,     1] loss: 3674.359
[4,     1] loss: 3681.330
[5,     1] loss: 3695.161
[6,     1] loss: 3692.906
[7,     1] loss: 3665.712
[8,     1] loss: 3673.786
[9,     1] loss: 3661.647
[10,     1] loss: 3654.594
[11,     1] loss: 3653.540
[12,     1] loss: 3650.083
[13,     1] loss: 3652.799
[14,     1] loss: 3632.539
[15,     1] loss: 3617.336
[16,     1] loss: 3578.364
[17,     1] loss: 3521.559
[18,     1] loss: 3480.631
[19,     1] loss: 3418.177
[20,     1] loss: 3357.343
[21,     1] loss: 3307.950
[22,     1] loss: 3263.971
[23,     1] loss: 3100.582
[24,     1] loss: 3091.391
[25,     1] loss: 3019.771
[26,     1] loss: 3038.773
[27,     1] loss: 3060.496
[28,     1] loss: 3090.202
[29,     1] loss: 3048.357
[30,     1] loss: 2874.712
[31,     1] loss: 2831.779
[32,     1] loss: 2708.111
[33,     1] loss: 2954.874
[34,     1] loss: 2881.580
[35,     1] loss: 2827.552
[36,     1] loss: 2700.161
[37,     1] loss: 2690.975
[38,     1] loss: 2680.842
[39,     1] loss: 2636.265
[40,     1] loss: 2723.843
[41,     1] loss: 2738.025
[42,     1] loss: 2598.521
[43,     1] loss: 2669.969
[44,     1] loss: 2433.814
[45,     1] loss: 2473.074
[46,     1] loss: 2550.699
[47,     1] loss: 2464.966
[48,     1] loss: 2281.853
[49,     1] loss: 2463.999
[50,     1] loss: 2244.501
[51,     1] loss: 2419.823
[52,     1] loss: 1993.910
[53,     1] loss: 2207.408
[54,     1] loss: 2367.328
[55,     1] loss: 2255.142
[56,     1] loss: 2123.107
[57,     1] loss: 2599.251
[58,     1] loss: 2095.180
[59,     1] loss: 2506.870
[60,     1] loss: 1950.386
[61,     1] loss: 2336.002
[62,     1] loss: 2120.863
[63,     1] loss: 1856.512
[64,     1] loss: 2301.021
[65,     1] loss: 2156.566
[66,     1] loss: 1982.389
[67,     1] loss: 2093.111
[68,     1] loss: 1990.745
[69,     1] loss: 1944.022
[70,     1] loss: 1896.627
[71,     1] loss: 1864.043
[72,     1] loss: 2024.410
[73,     1] loss: 2028.244
[74,     1] loss: 2079.550
[75,     1] loss: 1819.480
[76,     1] loss: 2202.116
[77,     1] loss: 1710.618
[78,     1] loss: 1927.981
[79,     1] loss: 1776.970
[80,     1] loss: 2000.026
[81,     1] loss: 1899.389
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003119968092638428,
 'learning_rate_Hydroxylation-K': 0.0004912731172386287,
 'learning_rate_Hydroxylation-P': 0.009853290373665028,
 'log_base': 1.6155932201412484,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2028264123,
 'sample_weights': [11.335651362484132, 1.4170110071617983],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.767603093382711,
 'weight_decay_Hydroxylation-K': 6.66145025187327,
 'weight_decay_Hydroxylation-P': 0.12541212367968535}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1642.135
[2,     1] loss: 1644.884
[3,     1] loss: 1637.506
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032298863128246255,
 'learning_rate_Hydroxylation-K': 0.00487112415741548,
 'learning_rate_Hydroxylation-P': 0.006832730176898208,
 'log_base': 1.7956706844795103,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1654396117,
 'sample_weights': [3.4801656504344893, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.368291117790886,
 'weight_decay_Hydroxylation-K': 7.386197697078794,
 'weight_decay_Hydroxylation-P': 8.256535153781085}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1509.588
[2,     1] loss: 1515.424
[3,     1] loss: 1508.920
[4,     1] loss: 1509.035
[5,     1] loss: 1509.667
[6,     1] loss: 1510.521
[7,     1] loss: 1505.370
[8,     1] loss: 1509.726
[9,     1] loss: 1510.031
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021528674021950495,
 'learning_rate_Hydroxylation-K': 0.002112358232903966,
 'learning_rate_Hydroxylation-P': 0.00770331891942131,
 'log_base': 1.106135821049736,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3682089028,
 'sample_weights': [2.8519033130352387, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.779925341251912,
 'weight_decay_Hydroxylation-K': 3.2190487663013263,
 'weight_decay_Hydroxylation-P': 3.7510539659628876}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5372.413
[2,     1] loss: 5369.255
[3,     1] loss: 5364.137
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005114781195762357,
 'learning_rate_Hydroxylation-K': 0.00530545616351219,
 'learning_rate_Hydroxylation-P': 0.005511646420563923,
 'log_base': 1.9306394259937825,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 622666643,
 'sample_weights': [16.549999725468144, 2.0688296622395286],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.742844509431661,
 'weight_decay_Hydroxylation-K': 5.127052100208133,
 'weight_decay_Hydroxylation-P': 9.274074772872002}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1450.351
[2,     1] loss: 1445.089
[3,     1] loss: 1444.615
[4,     1] loss: 1446.655
[5,     1] loss: 1446.328
[6,     1] loss: 1440.680
[7,     1] loss: 1442.479
[8,     1] loss: 1442.344
[9,     1] loss: 1440.548
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007573421777614684,
 'learning_rate_Hydroxylation-K': 0.009039208879640432,
 'learning_rate_Hydroxylation-P': 0.006969255463860795,
 'log_base': 1.2332263796875866,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1820687821,
 'sample_weights': [2.5377213010345216, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.444762030219744,
 'weight_decay_Hydroxylation-K': 8.219920012675356,
 'weight_decay_Hydroxylation-P': 4.233372327089338}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2583.448
[2,     1] loss: 2589.192
[3,     1] loss: 2581.756
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0069977792739499755,
 'learning_rate_Hydroxylation-K': 0.008286767853338813,
 'learning_rate_Hydroxylation-P': 0.004398348272964253,
 'log_base': 1.2289474026197356,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3143105984,
 'sample_weights': [7.9636159976130205, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.631072427373411,
 'weight_decay_Hydroxylation-K': 5.262652761720932,
 'weight_decay_Hydroxylation-P': 4.180960548180197}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2637.270
[2,     1] loss: 2628.900
[3,     1] loss: 2641.758
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034659749667969343,
 'learning_rate_Hydroxylation-K': 0.0026928016450674074,
 'learning_rate_Hydroxylation-P': 0.007960904065008082,
 'log_base': 1.3197871555013219,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1498851866,
 'sample_weights': [8.097880664377938, 1.0122740784074018],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.690412981852896,
 'weight_decay_Hydroxylation-K': 8.207117311855328,
 'weight_decay_Hydroxylation-P': 2.5462688652707337}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2185.998
[2,     1] loss: 2181.463
[3,     1] loss: 2183.799
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009728162157335823,
 'learning_rate_Hydroxylation-K': 0.002515098219779473,
 'learning_rate_Hydroxylation-P': 0.00111587930291911,
 'log_base': 1.184136578876561,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 277987863,
 'sample_weights': [6.016651432160472, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.729661663702284,
 'weight_decay_Hydroxylation-K': 0.9183224615113794,
 'weight_decay_Hydroxylation-P': 3.031678133437403}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3209.615
[2,     1] loss: 3195.997
[3,     1] loss: 3214.206
[4,     1] loss: 3194.873
[5,     1] loss: 3203.507
[6,     1] loss: 3198.623
[7,     1] loss: 3194.651
[8,     1] loss: 3192.329
[9,     1] loss: 3211.365
[10,     1] loss: 3197.802
[11,     1] loss: 3181.523
[12,     1] loss: 3185.565
[13,     1] loss: 3182.348
[14,     1] loss: 3160.930
[15,     1] loss: 3132.263
[16,     1] loss: 3112.744
[17,     1] loss: 2964.996
[18,     1] loss: 2859.475
[19,     1] loss: 2753.717
[20,     1] loss: 2768.971
[21,     1] loss: 2886.927
[22,     1] loss: 2832.778
[23,     1] loss: 2956.928
[24,     1] loss: 2610.583
[25,     1] loss: 2810.368
[26,     1] loss: 2636.956
[27,     1] loss: 2673.039
[28,     1] loss: 2778.033
[29,     1] loss: 2530.848
[30,     1] loss: 2704.702
[31,     1] loss: 2492.522
[32,     1] loss: 2516.205
[33,     1] loss: 2373.757
[34,     1] loss: 2453.644
[35,     1] loss: 2348.484
[36,     1] loss: 2214.967
[37,     1] loss: 2191.959
[38,     1] loss: 2544.652
[39,     1] loss: 2143.585
[40,     1] loss: 2907.856
[41,     1] loss: 2201.510
[42,     1] loss: 2607.051
[43,     1] loss: 2337.046
[44,     1] loss: 2275.422
[45,     1] loss: 2456.332
[46,     1] loss: 2225.815
[47,     1] loss: 2286.648
[48,     1] loss: 2248.071
[49,     1] loss: 2070.622
[50,     1] loss: 2023.466
[51,     1] loss: 1933.076
[52,     1] loss: 2011.643
[53,     1] loss: 1994.653
[54,     1] loss: 1832.293
[55,     1] loss: 1606.548
[56,     1] loss: 1848.487
[57,     1] loss: 1614.351
[58,     1] loss: 1576.372
[59,     1] loss: 2033.756
[60,     1] loss: 4101.948
[61,     1] loss: 2228.886
[62,     1] loss: 2086.416
[63,     1] loss: 2717.321
[64,     1] loss: 2257.960
[65,     1] loss: 2560.210
[66,     1] loss: 2481.966
[67,     1] loss: 2230.332
[68,     1] loss: 2187.679
[69,     1] loss: 2301.884
[70,     1] loss: 2069.490
[71,     1] loss: 1992.731
[72,     1] loss: 1881.279
[73,     1] loss: 2102.987
[74,     1] loss: 2019.687
[75,     1] loss: 2046.770
[76,     1] loss: 1786.329
[77,     1] loss: 1969.915
[78,     1] loss: 1799.223
Early stopping applied (best metric=0.4686596691608429)
Finished Training
Total time taken: 11.809710025787354
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3213.579
[2,     1] loss: 3208.475
[3,     1] loss: 3196.099
[4,     1] loss: 3186.465
[5,     1] loss: 3172.633
[6,     1] loss: 3243.646
[7,     1] loss: 3222.005
[8,     1] loss: 3206.545
[9,     1] loss: 3205.175
[10,     1] loss: 3203.639
[11,     1] loss: 3199.827
[12,     1] loss: 3205.267
[13,     1] loss: 3200.133
[14,     1] loss: 3199.739
[15,     1] loss: 3196.987
[16,     1] loss: 3197.823
[17,     1] loss: 3189.923
[18,     1] loss: 3185.234
[19,     1] loss: 3169.834
[20,     1] loss: 3160.114
[21,     1] loss: 3108.130
[22,     1] loss: 3057.425
[23,     1] loss: 3009.998
[24,     1] loss: 2898.349
[25,     1] loss: 2809.635
[26,     1] loss: 2872.440
[27,     1] loss: 2591.279
[28,     1] loss: 2809.631
[29,     1] loss: 2704.053
[30,     1] loss: 2584.806
[31,     1] loss: 2740.391
[32,     1] loss: 2536.178
[33,     1] loss: 2686.263
[34,     1] loss: 2430.367
[35,     1] loss: 2490.992
[36,     1] loss: 2585.811
[37,     1] loss: 2303.467
[38,     1] loss: 2365.607
[39,     1] loss: 2285.349
[40,     1] loss: 2327.891
[41,     1] loss: 2319.758
[42,     1] loss: 2167.838
[43,     1] loss: 2160.664
[44,     1] loss: 1913.577
[45,     1] loss: 2049.885
[46,     1] loss: 2272.610
[47,     1] loss: 1925.631
[48,     1] loss: 2170.754
[49,     1] loss: 2006.041
[50,     1] loss: 1941.838
[51,     1] loss: 2088.115
[52,     1] loss: 1803.041
[53,     1] loss: 1770.080
[54,     1] loss: 2079.104
[55,     1] loss: 3514.051
[56,     1] loss: 3847.267
[57,     1] loss: 3536.062
[58,     1] loss: 3040.176
[59,     1] loss: 2666.829
[60,     1] loss: 2648.608
[61,     1] loss: 2939.088
[62,     1] loss: 2971.659
[63,     1] loss: 2895.169
[64,     1] loss: 2944.354
[65,     1] loss: 2948.275
[66,     1] loss: 2919.295
[67,     1] loss: 2794.270
[68,     1] loss: 2832.941
[69,     1] loss: 2787.698
[70,     1] loss: 2788.728
[71,     1] loss: 2693.011
[72,     1] loss: 2591.047
[73,     1] loss: 2577.309
[74,     1] loss: 2570.833
[75,     1] loss: 2418.922
[76,     1] loss: 2518.542
[77,     1] loss: 2366.003
Early stopping applied (best metric=0.4508788585662842)
Finished Training
Total time taken: 11.506087303161621
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3232.302
[2,     1] loss: 3220.675
[3,     1] loss: 3207.924
[4,     1] loss: 3198.692
[5,     1] loss: 3204.029
[6,     1] loss: 3209.026
[7,     1] loss: 3213.630
[8,     1] loss: 3198.842
[9,     1] loss: 3193.541
[10,     1] loss: 3206.821
[11,     1] loss: 3205.520
[12,     1] loss: 3200.337
[13,     1] loss: 3202.320
[14,     1] loss: 3202.228
[15,     1] loss: 3206.333
[16,     1] loss: 3198.798
[17,     1] loss: 3200.503
[18,     1] loss: 3211.985
[19,     1] loss: 3201.633
[20,     1] loss: 3200.558
[21,     1] loss: 3198.720
[22,     1] loss: 3199.109
[23,     1] loss: 3196.745
[24,     1] loss: 3196.831
[25,     1] loss: 3185.100
[26,     1] loss: 3187.831
[27,     1] loss: 3159.039
[28,     1] loss: 3101.861
[29,     1] loss: 3091.228
[30,     1] loss: 3016.217
[31,     1] loss: 2876.564
[32,     1] loss: 2928.639
[33,     1] loss: 2774.611
[34,     1] loss: 2724.005
[35,     1] loss: 2838.733
[36,     1] loss: 3187.864
[37,     1] loss: 2765.804
[38,     1] loss: 2875.462
[39,     1] loss: 3011.904
[40,     1] loss: 2728.902
[41,     1] loss: 2781.159
[42,     1] loss: 2791.652
[43,     1] loss: 2565.451
[44,     1] loss: 2671.974
[45,     1] loss: 2586.035
[46,     1] loss: 2542.426
[47,     1] loss: 2376.556
[48,     1] loss: 2385.984
[49,     1] loss: 2321.310
[50,     1] loss: 2505.721
[51,     1] loss: 2331.041
[52,     1] loss: 2274.461
[53,     1] loss: 2194.763
[54,     1] loss: 2280.049
[55,     1] loss: 2517.685
[56,     1] loss: 1842.031
[57,     1] loss: 2737.065
[58,     1] loss: 3365.204
[59,     1] loss: 2755.117
[60,     1] loss: 2443.530
[61,     1] loss: 2661.957
[62,     1] loss: 2844.785
[63,     1] loss: 2650.757
[64,     1] loss: 2629.729
[65,     1] loss: 2573.431
[66,     1] loss: 2427.170
[67,     1] loss: 2393.165
[68,     1] loss: 2236.341
[69,     1] loss: 2314.971
[70,     1] loss: 2092.269
[71,     1] loss: 3049.192
[72,     1] loss: 2681.621
[73,     1] loss: 2806.260
[74,     1] loss: 2210.903
[75,     1] loss: 2620.190
[76,     1] loss: 2381.572
[77,     1] loss: 2404.310
[78,     1] loss: 2548.350
[79,     1] loss: 2562.098
[80,     1] loss: 2308.411
[81,     1] loss: 2278.104
[82,     1] loss: 2224.649
[83,     1] loss: 2010.127
[84,     1] loss: 2085.199
[85,     1] loss: 2067.872
[86,     1] loss: 2727.053
[87,     1] loss: 1938.872
[88,     1] loss: 2106.170
[89,     1] loss: 1957.512
[90,     1] loss: 2190.635
[91,     1] loss: 1929.822
[92,     1] loss: 2229.529
[93,     1] loss: 2029.278
[94,     1] loss: 1683.376
[95,     1] loss: 1883.099
Early stopping applied (best metric=0.431343138217926)
Finished Training
Total time taken: 13.724643230438232
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3196.548
[2,     1] loss: 3210.150
[3,     1] loss: 3196.708
[4,     1] loss: 3194.608
[5,     1] loss: 3203.517
[6,     1] loss: 3184.781
[7,     1] loss: 3222.680
[8,     1] loss: 3208.490
[9,     1] loss: 3200.767
[10,     1] loss: 3190.167
[11,     1] loss: 3183.421
[12,     1] loss: 3175.308
[13,     1] loss: 3160.482
[14,     1] loss: 3141.612
[15,     1] loss: 3101.610
[16,     1] loss: 3038.231
[17,     1] loss: 2994.341
[18,     1] loss: 2979.355
[19,     1] loss: 2825.295
[20,     1] loss: 2709.246
[21,     1] loss: 2900.492
[22,     1] loss: 2577.866
[23,     1] loss: 2593.438
[24,     1] loss: 2603.555
[25,     1] loss: 2579.163
[26,     1] loss: 2493.048
[27,     1] loss: 2497.817
[28,     1] loss: 2737.779
[29,     1] loss: 2440.236
[30,     1] loss: 2360.859
[31,     1] loss: 2210.457
[32,     1] loss: 2403.129
[33,     1] loss: 2457.098
[34,     1] loss: 2313.556
[35,     1] loss: 2501.678
[36,     1] loss: 2454.814
[37,     1] loss: 2063.991
[38,     1] loss: 2550.646
[39,     1] loss: 2119.900
[40,     1] loss: 2012.732
[41,     1] loss: 2014.393
[42,     1] loss: 1863.141
[43,     1] loss: 2148.401
[44,     1] loss: 2204.936
[45,     1] loss: 2175.262
[46,     1] loss: 2136.095
[47,     1] loss: 1904.755
[48,     1] loss: 2335.102
[49,     1] loss: 2116.404
[50,     1] loss: 1879.069
[51,     1] loss: 2324.427
[52,     1] loss: 1784.437
[53,     1] loss: 1923.574
[54,     1] loss: 1769.937
[55,     1] loss: 1752.358
[56,     1] loss: 2405.975
[57,     1] loss: 3843.561
[58,     1] loss: 2093.853
[59,     1] loss: 2374.754
[60,     1] loss: 2812.364
[61,     1] loss: 2561.503
[62,     1] loss: 2591.486
[63,     1] loss: 2515.666
[64,     1] loss: 2410.259
[65,     1] loss: 2251.609
[66,     1] loss: 2356.299
[67,     1] loss: 2163.603
[68,     1] loss: 2256.368
[69,     1] loss: 2137.644
[70,     1] loss: 2452.301
[71,     1] loss: 2202.251
[72,     1] loss: 1943.256
Early stopping applied (best metric=0.428492933511734)
Finished Training
Total time taken: 12.260627508163452
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3218.083
[2,     1] loss: 3263.129
[3,     1] loss: 3211.981
[4,     1] loss: 3204.986
[5,     1] loss: 3222.542
[6,     1] loss: 3204.240
[7,     1] loss: 3212.196
[8,     1] loss: 3209.054
[9,     1] loss: 3203.686
[10,     1] loss: 3203.404
[11,     1] loss: 3202.584
[12,     1] loss: 3203.525
[13,     1] loss: 3202.701
[14,     1] loss: 3202.080
[15,     1] loss: 3203.103
[16,     1] loss: 3199.315
[17,     1] loss: 3204.926
[18,     1] loss: 3204.341
[19,     1] loss: 3198.147
[20,     1] loss: 3198.849
[21,     1] loss: 3202.275
[22,     1] loss: 3198.891
[23,     1] loss: 3194.459
[24,     1] loss: 3184.011
[25,     1] loss: 3174.425
[26,     1] loss: 3145.331
[27,     1] loss: 3086.580
[28,     1] loss: 3027.179
[29,     1] loss: 2958.018
[30,     1] loss: 2927.864
[31,     1] loss: 2880.542
[32,     1] loss: 2669.043
[33,     1] loss: 2623.093
[34,     1] loss: 2679.818
[35,     1] loss: 2670.631
[36,     1] loss: 2864.202
[37,     1] loss: 2544.539
[38,     1] loss: 2720.405
[39,     1] loss: 2591.471
[40,     1] loss: 2533.189
[41,     1] loss: 2482.986
[42,     1] loss: 2482.215
[43,     1] loss: 2235.563
[44,     1] loss: 2414.383
[45,     1] loss: 2432.948
[46,     1] loss: 3458.435
[47,     1] loss: 2254.924
[48,     1] loss: 2884.900
[49,     1] loss: 2334.667
[50,     1] loss: 2549.488
[51,     1] loss: 2638.027
[52,     1] loss: 2572.197
[53,     1] loss: 2490.332
[54,     1] loss: 2397.544
[55,     1] loss: 2434.638
[56,     1] loss: 2414.301
[57,     1] loss: 2253.654
[58,     1] loss: 2399.965
[59,     1] loss: 2175.587
[60,     1] loss: 2138.671
[61,     1] loss: 2027.736
[62,     1] loss: 2126.131
[63,     1] loss: 1957.210
[64,     1] loss: 1938.978
[65,     1] loss: 1832.676
[66,     1] loss: 2603.854
[67,     1] loss: 4637.648
[68,     1] loss: 2536.039
[69,     1] loss: 2462.197
[70,     1] loss: 2837.536
[71,     1] loss: 2713.980
[72,     1] loss: 2806.059
[73,     1] loss: 2840.726
[74,     1] loss: 2789.403
[75,     1] loss: 2686.222
[76,     1] loss: 2723.294
[77,     1] loss: 2593.986
[78,     1] loss: 2750.350
[79,     1] loss: 2415.626
[80,     1] loss: 2440.034
[81,     1] loss: 2312.877
[82,     1] loss: 2432.229
[83,     1] loss: 2212.947
Early stopping applied (best metric=0.4160572290420532)
Finished Training
Total time taken: 12.268720149993896
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3207.064
[2,     1] loss: 3209.161
[3,     1] loss: 3215.312
[4,     1] loss: 3198.874
[5,     1] loss: 3217.680
[6,     1] loss: 3204.766
[7,     1] loss: 3200.075
[8,     1] loss: 3207.080
[9,     1] loss: 3202.025
[10,     1] loss: 3213.677
[11,     1] loss: 3211.390
[12,     1] loss: 3202.743
[13,     1] loss: 3199.619
[14,     1] loss: 3202.574
[15,     1] loss: 3200.420
[16,     1] loss: 3198.979
[17,     1] loss: 3199.361
[18,     1] loss: 3194.229
[19,     1] loss: 3193.474
[20,     1] loss: 3186.154
[21,     1] loss: 3177.369
[22,     1] loss: 3141.383
[23,     1] loss: 3120.072
[24,     1] loss: 3082.780
[25,     1] loss: 2984.592
[26,     1] loss: 2952.990
[27,     1] loss: 2937.340
[28,     1] loss: 2852.101
[29,     1] loss: 2804.254
[30,     1] loss: 2822.940
[31,     1] loss: 2655.386
[32,     1] loss: 2799.141
[33,     1] loss: 2722.337
[34,     1] loss: 2609.916
[35,     1] loss: 2625.682
[36,     1] loss: 2715.912
[37,     1] loss: 2600.474
[38,     1] loss: 2449.478
[39,     1] loss: 2486.799
[40,     1] loss: 2449.423
[41,     1] loss: 2636.760
[42,     1] loss: 2451.715
[43,     1] loss: 2315.234
[44,     1] loss: 2636.762
[45,     1] loss: 2428.982
[46,     1] loss: 2362.458
[47,     1] loss: 2540.749
[48,     1] loss: 2236.861
[49,     1] loss: 2155.943
[50,     1] loss: 2062.931
[51,     1] loss: 2064.128
[52,     1] loss: 2210.294
[53,     1] loss: 3887.260
[54,     1] loss: 3151.158
[55,     1] loss: 2900.691
[56,     1] loss: 2760.646
[57,     1] loss: 2864.227
[58,     1] loss: 2897.339
[59,     1] loss: 2958.861
[60,     1] loss: 2983.449
[61,     1] loss: 2834.523
[62,     1] loss: 2736.380
[63,     1] loss: 2786.662
[64,     1] loss: 2635.788
[65,     1] loss: 2620.385
[66,     1] loss: 2578.272
[67,     1] loss: 2633.248
[68,     1] loss: 2581.442
[69,     1] loss: 2375.537
[70,     1] loss: 2507.806
[71,     1] loss: 2404.535
[72,     1] loss: 2415.413
[73,     1] loss: 2562.983
[74,     1] loss: 2180.507
[75,     1] loss: 2414.696
[76,     1] loss: 2467.114
[77,     1] loss: 2493.016
[78,     1] loss: 2150.315
[79,     1] loss: 2186.660
[80,     1] loss: 2220.950
[81,     1] loss: 2097.700
[82,     1] loss: 2601.513
Early stopping applied (best metric=0.45607954263687134)
Finished Training
Total time taken: 13.411821603775024
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3228.135
[2,     1] loss: 3225.496
[3,     1] loss: 3236.720
[4,     1] loss: 3215.730
[5,     1] loss: 3203.688
[6,     1] loss: 3201.279
[7,     1] loss: 3198.799
[8,     1] loss: 3198.787
[9,     1] loss: 3227.836
[10,     1] loss: 3223.920
[11,     1] loss: 3209.861
[12,     1] loss: 3205.235
[13,     1] loss: 3206.456
[14,     1] loss: 3199.420
[15,     1] loss: 3198.821
[16,     1] loss: 3200.697
[17,     1] loss: 3205.870
[18,     1] loss: 3197.269
[19,     1] loss: 3199.615
[20,     1] loss: 3204.110
[21,     1] loss: 3203.212
[22,     1] loss: 3203.812
[23,     1] loss: 3208.058
[24,     1] loss: 3203.952
[25,     1] loss: 3201.419
[26,     1] loss: 3201.544
[27,     1] loss: 3203.418
[28,     1] loss: 3201.488
[29,     1] loss: 3202.394
[30,     1] loss: 3202.524
[31,     1] loss: 3200.317
[32,     1] loss: 3198.099
[33,     1] loss: 3199.517
[34,     1] loss: 3193.963
[35,     1] loss: 3184.543
[36,     1] loss: 3177.828
[37,     1] loss: 3136.949
[38,     1] loss: 3073.282
[39,     1] loss: 3006.876
[40,     1] loss: 3075.730
[41,     1] loss: 3063.756
[42,     1] loss: 2894.166
[43,     1] loss: 2828.331
[44,     1] loss: 2749.529
[45,     1] loss: 2763.578
[46,     1] loss: 2783.124
[47,     1] loss: 2752.786
[48,     1] loss: 2663.671
[49,     1] loss: 2606.162
[50,     1] loss: 2389.677
[51,     1] loss: 2476.368
[52,     1] loss: 2490.109
[53,     1] loss: 2541.407
[54,     1] loss: 2662.616
[55,     1] loss: 2255.477
[56,     1] loss: 2374.840
[57,     1] loss: 2180.619
[58,     1] loss: 2416.494
[59,     1] loss: 2145.527
[60,     1] loss: 2099.079
[61,     1] loss: 2238.590
[62,     1] loss: 2095.428
[63,     1] loss: 2001.675
[64,     1] loss: 2009.109
[65,     1] loss: 1865.771
[66,     1] loss: 2049.873
[67,     1] loss: 3856.319
[68,     1] loss: 3142.467
[69,     1] loss: 2823.159
[70,     1] loss: 2838.192
[71,     1] loss: 2975.890
[72,     1] loss: 2988.496
[73,     1] loss: 2887.694
[74,     1] loss: 2701.410
[75,     1] loss: 2878.595
[76,     1] loss: 2585.703
[77,     1] loss: 2562.600
[78,     1] loss: 2390.248
[79,     1] loss: 2354.449
[80,     1] loss: 2175.973
[81,     1] loss: 2024.620
[82,     1] loss: 1981.767
[83,     1] loss: 2215.302
[84,     1] loss: 2861.395
[85,     1] loss: 2149.574
[86,     1] loss: 2248.531
[87,     1] loss: 2143.870
[88,     1] loss: 2068.448
[89,     1] loss: 1958.496
[90,     1] loss: 2163.931
[91,     1] loss: 2043.940
[92,     1] loss: 1918.277
[93,     1] loss: 1645.971
[94,     1] loss: 1661.455
[95,     1] loss: 1867.362
[96,     1] loss: 1729.122
[97,     1] loss: 1734.219
[98,     1] loss: 1805.790
[99,     1] loss: 1954.292
[100,     1] loss: 2741.580
[101,     1] loss: 1744.521
[102,     1] loss: 2326.493
[103,     1] loss: 1906.137
[104,     1] loss: 2059.353
Early stopping applied (best metric=0.3907686769962311)
Finished Training
Total time taken: 15.23015308380127
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3208.188
[2,     1] loss: 3196.928
[3,     1] loss: 3203.600
[4,     1] loss: 3226.745
[5,     1] loss: 3211.328
[6,     1] loss: 3199.716
[7,     1] loss: 3198.604
[8,     1] loss: 3221.261
[9,     1] loss: 3203.835
[10,     1] loss: 3198.003
[11,     1] loss: 3201.673
[12,     1] loss: 3202.637
[13,     1] loss: 3201.928
[14,     1] loss: 3202.204
[15,     1] loss: 3201.909
[16,     1] loss: 3200.741
[17,     1] loss: 3204.377
[18,     1] loss: 3201.189
[19,     1] loss: 3202.875
[20,     1] loss: 3193.016
[21,     1] loss: 3200.596
[22,     1] loss: 3195.219
[23,     1] loss: 3195.869
[24,     1] loss: 3207.683
[25,     1] loss: 3180.413
[26,     1] loss: 3160.673
[27,     1] loss: 3140.207
[28,     1] loss: 3087.612
[29,     1] loss: 3011.486
[30,     1] loss: 2924.578
[31,     1] loss: 2891.021
[32,     1] loss: 2833.587
[33,     1] loss: 2886.163
[34,     1] loss: 2807.356
[35,     1] loss: 2724.766
[36,     1] loss: 2840.376
[37,     1] loss: 2671.784
[38,     1] loss: 2670.428
[39,     1] loss: 2520.394
[40,     1] loss: 2594.580
[41,     1] loss: 2669.845
[42,     1] loss: 2599.038
[43,     1] loss: 2447.444
[44,     1] loss: 2388.589
[45,     1] loss: 2495.740
[46,     1] loss: 2400.687
[47,     1] loss: 2316.534
[48,     1] loss: 2485.069
[49,     1] loss: 2154.559
[50,     1] loss: 2266.263
[51,     1] loss: 2524.448
[52,     1] loss: 3261.491
[53,     1] loss: 2167.960
[54,     1] loss: 2717.253
[55,     1] loss: 2451.090
[56,     1] loss: 2525.802
[57,     1] loss: 2587.506
[58,     1] loss: 2370.261
[59,     1] loss: 2383.742
[60,     1] loss: 2186.892
[61,     1] loss: 2414.843
[62,     1] loss: 2000.132
[63,     1] loss: 1987.802
[64,     1] loss: 2208.604
[65,     1] loss: 2003.616
[66,     1] loss: 1965.211
[67,     1] loss: 2102.376
[68,     1] loss: 1993.306
[69,     1] loss: 2031.472
[70,     1] loss: 1735.585
[71,     1] loss: 2252.855
[72,     1] loss: 3884.021
[73,     1] loss: 2023.661
[74,     1] loss: 2593.539
[75,     1] loss: 2336.876
[76,     1] loss: 2481.539
[77,     1] loss: 2422.836
[78,     1] loss: 2181.864
[79,     1] loss: 2317.264
[80,     1] loss: 2132.144
[81,     1] loss: 2197.135
[82,     1] loss: 2123.928
[83,     1] loss: 2484.695
[84,     1] loss: 2001.081
[85,     1] loss: 1940.857
[86,     1] loss: 2033.370
Early stopping applied (best metric=0.40719473361968994)
Finished Training
Total time taken: 12.227593183517456
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3207.028
[2,     1] loss: 3199.261
[3,     1] loss: 3220.431
[4,     1] loss: 3204.137
[5,     1] loss: 3192.941
[6,     1] loss: 3202.571
[7,     1] loss: 3195.544
[8,     1] loss: 3206.267
[9,     1] loss: 3198.983
[10,     1] loss: 3207.122
[11,     1] loss: 3186.110
[12,     1] loss: 3186.797
[13,     1] loss: 3191.548
[14,     1] loss: 3154.715
[15,     1] loss: 3136.421
[16,     1] loss: 3083.417
[17,     1] loss: 3003.104
[18,     1] loss: 2840.183
[19,     1] loss: 2766.653
[20,     1] loss: 2796.395
[21,     1] loss: 2559.989
[22,     1] loss: 2612.812
[23,     1] loss: 2400.255
[24,     1] loss: 2726.831
[25,     1] loss: 2677.758
[26,     1] loss: 2449.830
[27,     1] loss: 2573.830
[28,     1] loss: 2655.451
[29,     1] loss: 2531.693
[30,     1] loss: 2552.133
[31,     1] loss: 2480.135
[32,     1] loss: 2379.717
[33,     1] loss: 2562.737
[34,     1] loss: 2262.230
[35,     1] loss: 2745.628
[36,     1] loss: 2325.826
[37,     1] loss: 2239.176
[38,     1] loss: 2186.111
[39,     1] loss: 2232.723
[40,     1] loss: 2139.133
[41,     1] loss: 1938.864
[42,     1] loss: 1878.674
[43,     1] loss: 1712.155
[44,     1] loss: 1848.441
[45,     1] loss: 2176.634
[46,     1] loss: 4062.147
[47,     1] loss: 2922.412
[48,     1] loss: 3023.233
[49,     1] loss: 2184.593
[50,     1] loss: 2581.374
[51,     1] loss: 2697.112
[52,     1] loss: 2766.240
[53,     1] loss: 2718.266
[54,     1] loss: 2758.451
[55,     1] loss: 2686.416
[56,     1] loss: 2683.432
[57,     1] loss: 2573.573
[58,     1] loss: 2494.667
[59,     1] loss: 2465.256
[60,     1] loss: 2415.835
[61,     1] loss: 2400.618
[62,     1] loss: 2228.549
[63,     1] loss: 2191.992
[64,     1] loss: 2232.457
[65,     1] loss: 2079.669
[66,     1] loss: 2093.346
[67,     1] loss: 1967.670
[68,     1] loss: 1959.324
[69,     1] loss: 1776.969
[70,     1] loss: 1818.186
[71,     1] loss: 2559.792
[72,     1] loss: 3419.834
[73,     1] loss: 2463.966
[74,     1] loss: 2242.953
[75,     1] loss: 3051.564
[76,     1] loss: 2375.713
[77,     1] loss: 2401.928
[78,     1] loss: 2524.513
[79,     1] loss: 2373.135
[80,     1] loss: 2207.998
[81,     1] loss: 2119.029
[82,     1] loss: 2109.112
[83,     1] loss: 2088.316
[84,     1] loss: 2053.065
[85,     1] loss: 1969.354
[86,     1] loss: 1925.626
[87,     1] loss: 2173.216
[88,     1] loss: 1971.363
[89,     1] loss: 1771.796
[90,     1] loss: 1938.239
[91,     1] loss: 1728.476
[92,     1] loss: 2017.007
[93,     1] loss: 3029.934
[94,     1] loss: 1980.697
[95,     1] loss: 2507.136
[96,     1] loss: 2037.982
[97,     1] loss: 2370.366
[98,     1] loss: 2393.428
[99,     1] loss: 2167.435
[100,     1] loss: 2023.137
[101,     1] loss: 2076.410
[102,     1] loss: 1980.952
[103,     1] loss: 1983.713
[104,     1] loss: 1758.218
[105,     1] loss: 2016.833
[106,     1] loss: 2231.868
[107,     1] loss: 1823.947
[108,     1] loss: 2309.573
[109,     1] loss: 1979.775
[110,     1] loss: 2245.380
[111,     1] loss: 1906.030
Early stopping applied (best metric=0.45777156949043274)
Finished Training
Total time taken: 15.809653997421265
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3216.967
[2,     1] loss: 3219.541
[3,     1] loss: 3213.987
[4,     1] loss: 3238.270
[5,     1] loss: 3199.239
[6,     1] loss: 3219.284
[7,     1] loss: 3211.414
[8,     1] loss: 3204.339
[9,     1] loss: 3203.532
[10,     1] loss: 3209.227
[11,     1] loss: 3206.479
[12,     1] loss: 3205.456
[13,     1] loss: 3204.904
[14,     1] loss: 3219.318
[15,     1] loss: 3202.758
[16,     1] loss: 3207.070
[17,     1] loss: 3203.227
[18,     1] loss: 3203.828
[19,     1] loss: 3202.401
[20,     1] loss: 3201.660
[21,     1] loss: 3202.347
[22,     1] loss: 3203.868
[23,     1] loss: 3202.152
[24,     1] loss: 3199.490
[25,     1] loss: 3198.688
[26,     1] loss: 3202.022
[27,     1] loss: 3197.355
[28,     1] loss: 3192.052
[29,     1] loss: 3178.574
[30,     1] loss: 3151.617
[31,     1] loss: 3118.699
[32,     1] loss: 3082.560
[33,     1] loss: 3006.618
[34,     1] loss: 2957.934
[35,     1] loss: 2881.928
[36,     1] loss: 2852.047
[37,     1] loss: 2827.362
[38,     1] loss: 2703.290
[39,     1] loss: 2682.525
[40,     1] loss: 2855.550
[41,     1] loss: 2729.922
[42,     1] loss: 2688.602
[43,     1] loss: 2713.465
[44,     1] loss: 2590.845
[45,     1] loss: 2642.396
[46,     1] loss: 2710.157
[47,     1] loss: 2887.359
[48,     1] loss: 2593.237
[49,     1] loss: 2730.812
[50,     1] loss: 2473.434
[51,     1] loss: 2576.974
[52,     1] loss: 2589.497
[53,     1] loss: 2484.070
[54,     1] loss: 2530.505
[55,     1] loss: 2331.841
[56,     1] loss: 2532.792
[57,     1] loss: 2172.490
[58,     1] loss: 2412.042
[59,     1] loss: 2416.603
[60,     1] loss: 2246.875
[61,     1] loss: 2424.086
[62,     1] loss: 2359.863
[63,     1] loss: 1986.761
[64,     1] loss: 2114.321
[65,     1] loss: 1966.943
[66,     1] loss: 2045.224
[67,     1] loss: 2848.933
[68,     1] loss: 3403.468
[69,     1] loss: 2732.152
[70,     1] loss: 2886.900
[71,     1] loss: 2950.650
[72,     1] loss: 2899.352
[73,     1] loss: 2878.193
[74,     1] loss: 2787.814
[75,     1] loss: 2715.082
[76,     1] loss: 2665.090
[77,     1] loss: 2509.299
[78,     1] loss: 2464.590
[79,     1] loss: 2510.366
[80,     1] loss: 2524.251
[81,     1] loss: 3184.181
[82,     1] loss: 2224.403
[83,     1] loss: 2551.629
[84,     1] loss: 2279.686
[85,     1] loss: 2424.685
[86,     1] loss: 2409.006
[87,     1] loss: 2343.159
[88,     1] loss: 2147.754
[89,     1] loss: 2268.729
[90,     1] loss: 2420.218
[91,     1] loss: 2045.764
[92,     1] loss: 2160.633
[93,     1] loss: 1876.352
[94,     1] loss: 1855.138
[95,     1] loss: 1696.388
[96,     1] loss: 2002.849
[97,     1] loss: 3961.524
[98,     1] loss: 3446.382
[99,     1] loss: 2725.508
[100,     1] loss: 3070.089
[101,     1] loss: 3151.869
[102,     1] loss: 3152.787
[103,     1] loss: 3169.881
[104,     1] loss: 3167.933
[105,     1] loss: 3159.739
[106,     1] loss: 3159.329
[107,     1] loss: 3144.262
Early stopping applied (best metric=0.38591936230659485)
Finished Training
Total time taken: 15.198585033416748
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3189.378
[2,     1] loss: 3254.301
[3,     1] loss: 3199.599
[4,     1] loss: 3206.882
[5,     1] loss: 3202.659
[6,     1] loss: 3210.186
[7,     1] loss: 3203.662
[8,     1] loss: 3207.958
[9,     1] loss: 3203.250
[10,     1] loss: 3196.322
[11,     1] loss: 3195.124
[12,     1] loss: 3194.596
[13,     1] loss: 3196.084
[14,     1] loss: 3189.568
[15,     1] loss: 3185.077
[16,     1] loss: 3176.194
[17,     1] loss: 3150.350
[18,     1] loss: 3112.295
[19,     1] loss: 3052.312
[20,     1] loss: 3002.067
[21,     1] loss: 2957.319
[22,     1] loss: 2827.658
[23,     1] loss: 2752.976
[24,     1] loss: 2800.747
[25,     1] loss: 2763.283
[26,     1] loss: 2645.774
[27,     1] loss: 2589.385
[28,     1] loss: 3058.985
[29,     1] loss: 2626.025
[30,     1] loss: 2921.477
[31,     1] loss: 2785.327
[32,     1] loss: 2687.204
[33,     1] loss: 2756.238
[34,     1] loss: 2619.746
[35,     1] loss: 2571.599
[36,     1] loss: 2603.234
[37,     1] loss: 2582.722
[38,     1] loss: 2414.148
[39,     1] loss: 2414.420
[40,     1] loss: 2426.735
[41,     1] loss: 2288.153
[42,     1] loss: 2424.974
[43,     1] loss: 2378.835
[44,     1] loss: 2103.380
[45,     1] loss: 2157.502
[46,     1] loss: 2235.343
[47,     1] loss: 1940.434
[48,     1] loss: 2083.363
[49,     1] loss: 2213.929
[50,     1] loss: 2301.759
[51,     1] loss: 2298.984
[52,     1] loss: 1953.242
[53,     1] loss: 2180.258
[54,     1] loss: 1896.028
[55,     1] loss: 2198.725
[56,     1] loss: 1734.482
[57,     1] loss: 1850.795
[58,     1] loss: 1915.055
[59,     1] loss: 1812.935
[60,     1] loss: 1731.115
[61,     1] loss: 2768.532
[62,     1] loss: 4567.267
[63,     1] loss: 2895.982
[64,     1] loss: 2979.466
[65,     1] loss: 3081.740
[66,     1] loss: 3110.797
[67,     1] loss: 3157.390
[68,     1] loss: 3061.538
[69,     1] loss: 3117.883
[70,     1] loss: 3035.621
[71,     1] loss: 3073.958
[72,     1] loss: 2979.240
[73,     1] loss: 2847.977
[74,     1] loss: 2840.150
[75,     1] loss: 2777.091
[76,     1] loss: 2789.990
[77,     1] loss: 2610.120
[78,     1] loss: 2668.604
[79,     1] loss: 2630.645
[80,     1] loss: 2517.258
[81,     1] loss: 2429.358
[82,     1] loss: 2584.029
[83,     1] loss: 3886.724
[84,     1] loss: 3021.149
[85,     1] loss: 2546.393
[86,     1] loss: 2585.187
[87,     1] loss: 2774.957
[88,     1] loss: 2657.378
[89,     1] loss: 2562.490
[90,     1] loss: 2665.348
[91,     1] loss: 2546.818
[92,     1] loss: 2592.835
[93,     1] loss: 2561.300
[94,     1] loss: 2459.117
[95,     1] loss: 2520.939
[96,     1] loss: 2272.086
[97,     1] loss: 2105.055
[98,     1] loss: 2197.591
Early stopping applied (best metric=0.46009066700935364)
Finished Training
Total time taken: 13.898078918457031
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3215.478
[2,     1] loss: 3217.053
[3,     1] loss: 3191.924
[4,     1] loss: 3268.938
[5,     1] loss: 3199.542
[6,     1] loss: 3202.566
[7,     1] loss: 3203.604
[8,     1] loss: 3202.829
[9,     1] loss: 3197.607
[10,     1] loss: 3200.048
[11,     1] loss: 3198.484
[12,     1] loss: 3198.556
[13,     1] loss: 3201.324
[14,     1] loss: 3200.952
[15,     1] loss: 3204.307
[16,     1] loss: 3202.002
[17,     1] loss: 3200.948
[18,     1] loss: 3200.747
[19,     1] loss: 3203.033
[20,     1] loss: 3195.524
[21,     1] loss: 3199.192
[22,     1] loss: 3201.262
[23,     1] loss: 3205.438
[24,     1] loss: 3198.906
[25,     1] loss: 3197.766
[26,     1] loss: 3199.852
[27,     1] loss: 3197.756
[28,     1] loss: 3196.855
[29,     1] loss: 3206.091
[30,     1] loss: 3196.278
[31,     1] loss: 3184.142
[32,     1] loss: 3185.580
[33,     1] loss: 3178.965
[34,     1] loss: 3158.494
[35,     1] loss: 3122.589
[36,     1] loss: 3053.537
[37,     1] loss: 2952.684
[38,     1] loss: 2889.848
[39,     1] loss: 2927.946
[40,     1] loss: 3115.647
[41,     1] loss: 2907.994
[42,     1] loss: 2854.224
[43,     1] loss: 2649.005
[44,     1] loss: 2706.741
[45,     1] loss: 2663.805
[46,     1] loss: 2633.140
[47,     1] loss: 2708.045
[48,     1] loss: 2487.495
[49,     1] loss: 2410.268
[50,     1] loss: 2400.413
[51,     1] loss: 2219.986
[52,     1] loss: 2344.896
[53,     1] loss: 2387.997
[54,     1] loss: 3660.939
[55,     1] loss: 2338.146
[56,     1] loss: 3065.429
[57,     1] loss: 2535.860
[58,     1] loss: 2613.857
[59,     1] loss: 2599.560
[60,     1] loss: 2624.084
[61,     1] loss: 2548.333
[62,     1] loss: 2432.435
[63,     1] loss: 2425.497
[64,     1] loss: 2356.403
[65,     1] loss: 2319.543
[66,     1] loss: 2123.523
[67,     1] loss: 2289.397
[68,     1] loss: 2286.023
[69,     1] loss: 2058.116
[70,     1] loss: 2438.151
[71,     1] loss: 2510.406
[72,     1] loss: 2273.946
[73,     1] loss: 2511.003
[74,     1] loss: 2111.224
[75,     1] loss: 2251.842
[76,     1] loss: 2059.052
[77,     1] loss: 2344.303
[78,     1] loss: 2066.000
[79,     1] loss: 2155.037
[80,     1] loss: 1947.863
[81,     1] loss: 1739.099
[82,     1] loss: 1990.140
[83,     1] loss: 1858.787
[84,     1] loss: 2413.996
[85,     1] loss: 2089.909
[86,     1] loss: 1949.394
[87,     1] loss: 1980.330
[88,     1] loss: 1818.103
[89,     1] loss: 1962.979
[90,     1] loss: 1724.734
[91,     1] loss: 1758.298
[92,     1] loss: 1840.695
[93,     1] loss: 1652.402
[94,     1] loss: 1723.296
[95,     1] loss: 3693.913
[96,     1] loss: 4646.183
[97,     1] loss: 4322.558
[98,     1] loss: 3307.364
[99,     1] loss: 2981.342
[100,     1] loss: 2978.876
[101,     1] loss: 3036.214
[102,     1] loss: 3071.016
[103,     1] loss: 3048.181
[104,     1] loss: 3026.339
Early stopping applied (best metric=0.4230751097202301)
Finished Training
Total time taken: 14.740983724594116
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3233.776
[2,     1] loss: 3230.544
[3,     1] loss: 3220.587
[4,     1] loss: 3209.324
[5,     1] loss: 3205.703
[6,     1] loss: 3204.570
[7,     1] loss: 3202.557
[8,     1] loss: 3205.983
[9,     1] loss: 3199.609
[10,     1] loss: 3218.386
[11,     1] loss: 3201.892
[12,     1] loss: 3202.255
[13,     1] loss: 3211.199
[14,     1] loss: 3202.800
[15,     1] loss: 3202.483
[16,     1] loss: 3202.944
[17,     1] loss: 3202.808
[18,     1] loss: 3199.407
[19,     1] loss: 3201.251
[20,     1] loss: 3199.815
[21,     1] loss: 3202.551
[22,     1] loss: 3202.009
[23,     1] loss: 3199.076
[24,     1] loss: 3198.755
[25,     1] loss: 3193.733
[26,     1] loss: 3192.201
[27,     1] loss: 3184.498
[28,     1] loss: 3168.865
[29,     1] loss: 3139.769
[30,     1] loss: 3102.958
[31,     1] loss: 3078.564
[32,     1] loss: 2962.788
[33,     1] loss: 2893.903
[34,     1] loss: 2769.486
[35,     1] loss: 2923.665
[36,     1] loss: 2828.968
[37,     1] loss: 2653.607
[38,     1] loss: 2656.720
[39,     1] loss: 2567.405
[40,     1] loss: 2595.904
[41,     1] loss: 2674.818
[42,     1] loss: 2627.832
[43,     1] loss: 2561.385
[44,     1] loss: 2579.122
[45,     1] loss: 2598.768
[46,     1] loss: 2544.752
[47,     1] loss: 2745.022
[48,     1] loss: 2365.513
[49,     1] loss: 2811.861
[50,     1] loss: 2572.495
[51,     1] loss: 2988.978
[52,     1] loss: 2657.572
[53,     1] loss: 2659.597
[54,     1] loss: 2587.370
[55,     1] loss: 2543.225
[56,     1] loss: 2578.986
[57,     1] loss: 2532.827
[58,     1] loss: 2520.294
[59,     1] loss: 2280.121
[60,     1] loss: 2323.554
[61,     1] loss: 2227.749
[62,     1] loss: 2127.933
[63,     1] loss: 2074.509
[64,     1] loss: 1991.439
[65,     1] loss: 2648.180
[66,     1] loss: 3521.364
[67,     1] loss: 2812.985
[68,     1] loss: 3041.611
[69,     1] loss: 2935.009
[70,     1] loss: 2920.654
[71,     1] loss: 2876.553
[72,     1] loss: 2815.499
[73,     1] loss: 3151.022
[74,     1] loss: 2825.144
[75,     1] loss: 2808.665
[76,     1] loss: 2805.375
[77,     1] loss: 2705.230
[78,     1] loss: 2656.900
[79,     1] loss: 2644.283
[80,     1] loss: 2404.592
[81,     1] loss: 2437.417
[82,     1] loss: 2505.671
[83,     1] loss: 2470.083
[84,     1] loss: 2271.065
[85,     1] loss: 2309.043
[86,     1] loss: 2037.254
Early stopping applied (best metric=0.41323328018188477)
Finished Training
Total time taken: 12.302073001861572
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3202.385
[2,     1] loss: 3218.376
[3,     1] loss: 3230.929
[4,     1] loss: 3209.909
[5,     1] loss: 3197.441
[6,     1] loss: 3209.311
[7,     1] loss: 3206.867
[8,     1] loss: 3199.230
[9,     1] loss: 3205.610
[10,     1] loss: 3205.076
[11,     1] loss: 3201.448
[12,     1] loss: 3206.669
[13,     1] loss: 3196.754
[14,     1] loss: 3206.454
[15,     1] loss: 3202.028
[16,     1] loss: 3199.940
[17,     1] loss: 3200.088
[18,     1] loss: 3198.068
[19,     1] loss: 3199.852
[20,     1] loss: 3199.027
[21,     1] loss: 3199.453
[22,     1] loss: 3193.981
[23,     1] loss: 3186.799
[24,     1] loss: 3177.756
[25,     1] loss: 3161.780
[26,     1] loss: 3115.726
[27,     1] loss: 3071.295
[28,     1] loss: 3030.912
[29,     1] loss: 2980.183
[30,     1] loss: 2933.149
[31,     1] loss: 2890.142
[32,     1] loss: 2864.351
[33,     1] loss: 2718.585
[34,     1] loss: 2764.758
[35,     1] loss: 3030.974
[36,     1] loss: 2794.280
[37,     1] loss: 2661.710
[38,     1] loss: 2730.608
[39,     1] loss: 2599.854
[40,     1] loss: 2650.117
[41,     1] loss: 2515.452
[42,     1] loss: 2567.503
[43,     1] loss: 2520.173
[44,     1] loss: 2379.311
[45,     1] loss: 2399.987
[46,     1] loss: 2314.387
[47,     1] loss: 2034.594
[48,     1] loss: 2948.473
[49,     1] loss: 5342.890
[50,     1] loss: 2396.440
[51,     1] loss: 2955.323
[52,     1] loss: 3283.717
[53,     1] loss: 2944.067
[54,     1] loss: 2953.633
[55,     1] loss: 3025.911
[56,     1] loss: 3070.113
[57,     1] loss: 3095.620
[58,     1] loss: 3131.089
[59,     1] loss: 3111.263
[60,     1] loss: 3030.041
[61,     1] loss: 2966.015
[62,     1] loss: 2937.524
[63,     1] loss: 2833.303
[64,     1] loss: 2822.768
[65,     1] loss: 2631.691
[66,     1] loss: 2574.129
[67,     1] loss: 2744.812
[68,     1] loss: 2430.240
[69,     1] loss: 2626.786
[70,     1] loss: 2580.963
[71,     1] loss: 2485.218
[72,     1] loss: 2515.632
[73,     1] loss: 2444.118
[74,     1] loss: 2335.726
[75,     1] loss: 2592.958
[76,     1] loss: 3259.524
[77,     1] loss: 2215.084
[78,     1] loss: 2758.334
[79,     1] loss: 2577.927
[80,     1] loss: 2597.689
[81,     1] loss: 2663.779
[82,     1] loss: 2634.731
[83,     1] loss: 2299.755
[84,     1] loss: 2354.552
[85,     1] loss: 2205.062
[86,     1] loss: 2276.113
[87,     1] loss: 2204.479
[88,     1] loss: 2272.499
[89,     1] loss: 3044.581
[90,     1] loss: 2062.381
[91,     1] loss: 2409.715
[92,     1] loss: 2308.520
[93,     1] loss: 2364.612
[94,     1] loss: 2343.684
[95,     1] loss: 2387.602
Early stopping applied (best metric=0.3633159101009369)
Finished Training
Total time taken: 13.397017240524292
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3193.750
[2,     1] loss: 3227.174
[3,     1] loss: 3208.852
[4,     1] loss: 3229.918
[5,     1] loss: 3202.999
[6,     1] loss: 3219.712
[7,     1] loss: 3195.297
[8,     1] loss: 3204.016
[9,     1] loss: 3199.141
[10,     1] loss: 3202.317
[11,     1] loss: 3200.106
[12,     1] loss: 3202.424
[13,     1] loss: 3206.361
[14,     1] loss: 3203.500
[15,     1] loss: 3198.658
[16,     1] loss: 3197.175
[17,     1] loss: 3191.957
[18,     1] loss: 3189.974
[19,     1] loss: 3189.156
[20,     1] loss: 3161.452
[21,     1] loss: 3144.147
[22,     1] loss: 3099.537
[23,     1] loss: 3026.482
[24,     1] loss: 2996.967
[25,     1] loss: 2936.747
[26,     1] loss: 2885.361
[27,     1] loss: 2745.515
[28,     1] loss: 2814.632
[29,     1] loss: 2696.582
[30,     1] loss: 2874.028
[31,     1] loss: 2719.094
[32,     1] loss: 2801.441
[33,     1] loss: 2576.949
[34,     1] loss: 2723.607
[35,     1] loss: 2591.109
[36,     1] loss: 2600.259
[37,     1] loss: 2548.292
[38,     1] loss: 2502.440
[39,     1] loss: 2417.846
[40,     1] loss: 2296.573
[41,     1] loss: 2441.020
[42,     1] loss: 2502.107
[43,     1] loss: 2267.692
[44,     1] loss: 2370.050
[45,     1] loss: 2303.125
[46,     1] loss: 2220.338
[47,     1] loss: 2726.473
[48,     1] loss: 2329.604
[49,     1] loss: 2357.475
[50,     1] loss: 2292.518
[51,     1] loss: 2273.239
[52,     1] loss: 2182.275
[53,     1] loss: 2032.413
[54,     1] loss: 2284.017
[55,     1] loss: 2017.868
[56,     1] loss: 2046.416
[57,     1] loss: 2188.892
[58,     1] loss: 2064.505
[59,     1] loss: 1951.386
[60,     1] loss: 2151.156
[61,     1] loss: 2007.918
[62,     1] loss: 1764.658
[63,     1] loss: 2014.086
[64,     1] loss: 3880.337
[65,     1] loss: 1889.894
[66,     1] loss: 2472.610
[67,     1] loss: 2090.938
[68,     1] loss: 2472.283
[69,     1] loss: 2549.698
[70,     1] loss: 2319.246
[71,     1] loss: 2432.781
[72,     1] loss: 2240.530
[73,     1] loss: 2244.953
[74,     1] loss: 2191.866
[75,     1] loss: 2120.238
[76,     1] loss: 1992.452
[77,     1] loss: 2014.458
[78,     1] loss: 2001.130
[79,     1] loss: 2239.532
[80,     1] loss: 4808.257
[81,     1] loss: 3734.349
[82,     1] loss: 2727.698
[83,     1] loss: 2619.058
[84,     1] loss: 2892.403
[85,     1] loss: 2798.097
[86,     1] loss: 2825.551
[87,     1] loss: 2863.682
[88,     1] loss: 2817.521
[89,     1] loss: 2717.476
[90,     1] loss: 2659.449
[91,     1] loss: 2687.699
[92,     1] loss: 2713.781
Early stopping applied (best metric=0.4417295753955841)
Finished Training
Total time taken: 13.034015655517578
{'Hydroxylation-K Validation Accuracy': 0.6892434988179669, 'Hydroxylation-K Validation Sensitivity': 0.78, 'Hydroxylation-K Validation Specificity': 0.6666666666666666, 'Hydroxylation-K Validation Precision': 0.376212002875753, 'Hydroxylation-K AUC ROC': 0.8033918128654971, 'Hydroxylation-K AUC PR': 0.5637941500233, 'Hydroxylation-K MCC': 0.3654334902725268, 'Hydroxylation-K F1': 0.5049127970794638, 'Validation Loss (Hydroxylation-K)': 0.4562282105286916, 'Hydroxylation-P Validation Accuracy': 0.7095716630289495, 'Hydroxylation-P Validation Sensitivity': 0.7730687830687831, 'Hydroxylation-P Validation Specificity': 0.6959723676991371, 'Hydroxylation-P Validation Precision': 0.3575126931244337, 'Hydroxylation-P AUC ROC': 0.7952788772665699, 'Hydroxylation-P AUC PR': 0.48903471161257095, 'Hydroxylation-P MCC': 0.370453696432721, 'Hydroxylation-P F1': 0.4862226114274382, 'Validation Loss (Hydroxylation-P)': 0.42630735039711, 'Validation Loss (total)': 0.8825355609258015, 'TimeToTrain': 13.387984244028727}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026614308498215293,
 'learning_rate_Hydroxylation-K': 0.008501248161638598,
 'learning_rate_Hydroxylation-P': 0.0069092373989997164,
 'log_base': 1.0327750119283428,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 672665317,
 'sample_weights': [9.884877515317264, 1.2330423107879482],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.472392017717693,
 'weight_decay_Hydroxylation-K': 5.798399973567736,
 'weight_decay_Hydroxylation-P': 3.955444815817737}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16820.438
Exploding loss, terminate run (best metric=0.5316930413246155)
Finished Training
Total time taken: 0.2220020294189453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16948.752
Exploding loss, terminate run (best metric=0.5274002552032471)
Finished Training
Total time taken: 0.19800019264221191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16846.426
Exploding loss, terminate run (best metric=0.5287338495254517)
Finished Training
Total time taken: 0.2240002155303955
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16831.299
Exploding loss, terminate run (best metric=0.528016984462738)
Finished Training
Total time taken: 0.20100021362304688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16752.570
Exploding loss, terminate run (best metric=0.5281033515930176)
Finished Training
Total time taken: 0.21500015258789062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16802.984
Exploding loss, terminate run (best metric=0.5320238471031189)
Finished Training
Total time taken: 0.19900035858154297
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16782.664
Exploding loss, terminate run (best metric=0.5350415110588074)
Finished Training
Total time taken: 0.21500468254089355
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16798.621
Exploding loss, terminate run (best metric=0.5273924469947815)
Finished Training
Total time taken: 0.20000028610229492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17003.445
Exploding loss, terminate run (best metric=0.5314030647277832)
Finished Training
Total time taken: 0.20499825477600098
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16830.350
Exploding loss, terminate run (best metric=0.5290470123291016)
Finished Training
Total time taken: 0.20600104331970215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16941.850
Exploding loss, terminate run (best metric=0.533051609992981)
Finished Training
Total time taken: 0.2109978199005127
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16861.344
Exploding loss, terminate run (best metric=0.5322904586791992)
Finished Training
Total time taken: 0.19800019264221191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16815.594
Exploding loss, terminate run (best metric=0.5276126265525818)
Finished Training
Total time taken: 0.20299911499023438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16798.246
Exploding loss, terminate run (best metric=0.5302479267120361)
Finished Training
Total time taken: 0.20400094985961914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16804.543
Exploding loss, terminate run (best metric=0.5287285447120667)
Finished Training
Total time taken: 0.21199703216552734
{'Hydroxylation-K Validation Accuracy': 0.36164302600472814, 'Hydroxylation-K Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-K Validation Specificity': 0.26666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5996881091617934, 'Hydroxylation-K AUC PR': 0.29563723496659405, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.2466338259441708, 'Validation Loss (Hydroxylation-K)': 0.5563376347223917, 'Hydroxylation-P Validation Accuracy': 0.34974549515253034, 'Hydroxylation-P Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-P Validation Specificity': 0.26666666666666666, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6141347706056669, 'Hydroxylation-P AUC PR': 0.27852478721391055, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.22081788577751335, 'Validation Loss (Hydroxylation-P)': 0.5300524353981018, 'Validation Loss (total)': 1.0863900899887085, 'TimeToTrain': 0.20753350257873535}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003202683954401252,
 'learning_rate_Hydroxylation-K': 0.0030498631751918245,
 'learning_rate_Hydroxylation-P': 0.008744355293098745,
 'log_base': 1.01736890390119,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2302377489,
 'sample_weights': [51.80509749660652, 6.462181957118611],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.829108021814696,
 'weight_decay_Hydroxylation-K': 4.679093254840632,
 'weight_decay_Hydroxylation-P': 0.3465864257668526}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31490.254
Exploding loss, terminate run (best metric=0.5361074805259705)
Finished Training
Total time taken: 0.1960005760192871
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31391.779
Exploding loss, terminate run (best metric=0.5313509702682495)
Finished Training
Total time taken: 0.21700310707092285
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31517.939
Exploding loss, terminate run (best metric=0.5294537544250488)
Finished Training
Total time taken: 0.2050018310546875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31653.916
Exploding loss, terminate run (best metric=0.5283116698265076)
Finished Training
Total time taken: 0.20800089836120605
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31499.072
Exploding loss, terminate run (best metric=0.5285266041755676)
Finished Training
Total time taken: 0.1979994773864746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31445.535
Exploding loss, terminate run (best metric=0.5316769480705261)
Finished Training
Total time taken: 0.22099876403808594
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31637.250
Exploding loss, terminate run (best metric=0.5290303230285645)
Finished Training
Total time taken: 0.1979987621307373
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31665.375
Exploding loss, terminate run (best metric=0.5303359031677246)
Finished Training
Total time taken: 0.22100019454956055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31470.578
Exploding loss, terminate run (best metric=0.529485821723938)
Finished Training
Total time taken: 0.20599985122680664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31544.275
Exploding loss, terminate run (best metric=0.5283136963844299)
Finished Training
Total time taken: 0.18899750709533691
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31451.387
Exploding loss, terminate run (best metric=0.5319822430610657)
Finished Training
Total time taken: 0.22099900245666504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31522.633
Exploding loss, terminate run (best metric=0.528064489364624)
Finished Training
Total time taken: 0.22500014305114746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31304.125
Exploding loss, terminate run (best metric=0.5366859436035156)
Finished Training
Total time taken: 0.20199990272521973
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31534.770
Exploding loss, terminate run (best metric=0.5299670696258545)
Finished Training
Total time taken: 0.20600271224975586
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31479.902
Exploding loss, terminate run (best metric=0.5298765301704407)
Finished Training
Total time taken: 0.20799827575683594
{'Hydroxylation-K Validation Accuracy': 0.5542257683215129, 'Hydroxylation-K Validation Sensitivity': 0.44814814814814813, 'Hydroxylation-K Validation Specificity': 0.5824561403508772, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6755165692007797, 'Hydroxylation-K AUC PR': 0.3903135350929229, 'Hydroxylation-K MCC': 0.042713679928607313, 'Hydroxylation-K F1': 0.18416032243618452, 'Validation Loss (Hydroxylation-K)': 0.555734658241272, 'Hydroxylation-P Validation Accuracy': 0.5638298563524694, 'Hydroxylation-P Validation Sensitivity': 0.44571428571428573, 'Hydroxylation-P Validation Specificity': 0.5886128983989226, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6393917877390868, 'Hydroxylation-P AUC PR': 0.33311230114716833, 'Hydroxylation-P MCC': 0.05273541685980245, 'Hydroxylation-P F1': 0.17530077970864735, 'Validation Loss (Hydroxylation-P)': 0.5306112964948019, 'Validation Loss (total)': 1.086345942815145, 'TimeToTrain': 0.20806673367818196}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00363341514352351,
 'learning_rate_Hydroxylation-K': 0.007621603946697772,
 'learning_rate_Hydroxylation-P': 0.0063925052739444085,
 'log_base': 1.2655331313794358,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 24486308,
 'sample_weights': [97.02102451716812, 12.102428996240679],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.852345502459437,
 'weight_decay_Hydroxylation-K': 4.588919530654081,
 'weight_decay_Hydroxylation-P': 3.686770106783071}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2425.274
[2,     1] loss: 2397.228
[3,     1] loss: 2401.575
[4,     1] loss: 2405.982
[5,     1] loss: 2405.607
[6,     1] loss: 2399.011
[7,     1] loss: 2396.306
[8,     1] loss: 2410.984
[9,     1] loss: 2397.530
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003548668401790981,
 'learning_rate_Hydroxylation-K': 0.0033375144260452093,
 'learning_rate_Hydroxylation-P': 0.009539065231856332,
 'log_base': 1.0834618335444037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1339979577,
 'sample_weights': [7.0891267963789035, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.552028758863294,
 'weight_decay_Hydroxylation-K': 5.553739791580111,
 'weight_decay_Hydroxylation-P': 1.4576008507692098}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6759.618
[2,     1] loss: 6758.459
[3,     1] loss: 6764.909
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009207014305310665,
 'learning_rate_Hydroxylation-K': 0.0034083853874546507,
 'learning_rate_Hydroxylation-P': 0.008901401171734475,
 'log_base': 1.2579042509650349,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2953002701,
 'sample_weights': [20.82604460820873, 2.6033558638845884],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.499788165091484,
 'weight_decay_Hydroxylation-K': 3.5684203173836857,
 'weight_decay_Hydroxylation-P': 3.2879440626409204}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2442.528
[2,     1] loss: 2436.588
[3,     1] loss: 2464.114
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005878731192957194,
 'learning_rate_Hydroxylation-K': 0.004643480752786225,
 'learning_rate_Hydroxylation-P': 0.0033512891217425426,
 'log_base': 2.6204041810381122,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1184607953,
 'sample_weights': [7.275940991103084, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.1090660285971925,
 'weight_decay_Hydroxylation-K': 2.93524186725258,
 'weight_decay_Hydroxylation-P': 8.559132566284235}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1276.406
[2,     1] loss: 1271.882
[3,     1] loss: 1274.724
[4,     1] loss: 1272.389
[5,     1] loss: 1272.297
[6,     1] loss: 1275.383
[7,     1] loss: 1265.628
[8,     1] loss: 1268.214
[9,     1] loss: 1262.285
[10,     1] loss: 1245.694
[11,     1] loss: 1221.290
[12,     1] loss: 1179.088
[13,     1] loss: 1166.606
[14,     1] loss: 1115.850
[15,     1] loss: 1118.997
[16,     1] loss: 1117.848
[17,     1] loss: 1058.526
[18,     1] loss: 1105.170
[19,     1] loss: 1025.498
[20,     1] loss: 1078.682
[21,     1] loss: 1092.835
[22,     1] loss: 989.691
[23,     1] loss: 1016.571
[24,     1] loss: 979.052
[25,     1] loss: 984.384
[26,     1] loss: 960.218
[27,     1] loss: 923.092
[28,     1] loss: 939.367
[29,     1] loss: 935.106
[30,     1] loss: 944.573
[31,     1] loss: 923.917
[32,     1] loss: 942.361
[33,     1] loss: 892.287
[34,     1] loss: 873.642
[35,     1] loss: 869.806
[36,     1] loss: 901.948
[37,     1] loss: 845.229
[38,     1] loss: 835.358
[39,     1] loss: 1037.751
[40,     1] loss: 1086.371
[41,     1] loss: 840.151
[42,     1] loss: 967.989
[43,     1] loss: 951.789
[44,     1] loss: 836.424
[45,     1] loss: 956.470
[46,     1] loss: 879.186
[47,     1] loss: 878.043
[48,     1] loss: 935.133
[49,     1] loss: 870.381
[50,     1] loss: 881.323
[51,     1] loss: 856.301
[52,     1] loss: 771.067
[53,     1] loss: 802.897
[54,     1] loss: 753.783
[55,     1] loss: 806.101
[56,     1] loss: 746.112
[57,     1] loss: 835.790
[58,     1] loss: 682.281
[59,     1] loss: 801.739
[60,     1] loss: 710.223
[61,     1] loss: 823.958
[62,     1] loss: 671.669
[63,     1] loss: 801.754
[64,     1] loss: 641.367
[65,     1] loss: 680.442
[66,     1] loss: 614.194
[67,     1] loss: 579.001
[68,     1] loss: 738.364
[69,     1] loss: 747.926
[70,     1] loss: 589.442
[71,     1] loss: 664.612
[72,     1] loss: 579.986
[73,     1] loss: 548.444
[74,     1] loss: 591.589
[75,     1] loss: 552.726
[76,     1] loss: 545.085
[77,     1] loss: 489.769
[78,     1] loss: 548.126
[79,     1] loss: 968.704
[80,     1] loss: 1315.640
[81,     1] loss: 697.997
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007144572124281281,
 'learning_rate_Hydroxylation-K': 0.004199462800120829,
 'learning_rate_Hydroxylation-P': 0.008742072768609908,
 'log_base': 1.0734163364671059,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3447606207,
 'sample_weights': [1.7329945290114375, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.873223671578074,
 'weight_decay_Hydroxylation-K': 0.525134577820749,
 'weight_decay_Hydroxylation-P': 5.343964629213169}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7676.448
[2,     1] loss: 7625.326
[3,     1] loss: 7708.444
[4,     1] loss: 7645.948
[5,     1] loss: 7630.656
[6,     1] loss: 7611.988
[7,     1] loss: 7655.982
[8,     1] loss: 7637.619
[9,     1] loss: 7623.993
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007111665397015495,
 'learning_rate_Hydroxylation-K': 0.008509804503198791,
 'learning_rate_Hydroxylation-P': 0.005565977853234963,
 'log_base': 1.1627287224240217,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4080494946,
 'sample_weights': [23.564262223310795, 2.9456462516646216],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.921576945794277,
 'weight_decay_Hydroxylation-K': 4.963020820219013,
 'weight_decay_Hydroxylation-P': 7.849887695831696}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3601.007
[2,     1] loss: 3589.164
[3,     1] loss: 3588.478
[4,     1] loss: 3598.344
[5,     1] loss: 3597.886
[6,     1] loss: 3599.140
[7,     1] loss: 3593.391
[8,     1] loss: 3592.503
[9,     1] loss: 3591.842
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005545870739731195,
 'learning_rate_Hydroxylation-K': 0.009749029635801865,
 'learning_rate_Hydroxylation-P': 0.00554816858864574,
 'log_base': 1.1381090693429918,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3557963103,
 'sample_weights': [11.072810849097054, 1.3841546772794078],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.033291072220328,
 'weight_decay_Hydroxylation-K': 7.504027503942365,
 'weight_decay_Hydroxylation-P': 5.716409412394454}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4205.998
[2,     1] loss: 4186.564
[3,     1] loss: 4203.281
[4,     1] loss: 4175.820
[5,     1] loss: 4180.121
[6,     1] loss: 4179.199
[7,     1] loss: 4174.328
[8,     1] loss: 4179.781
[9,     1] loss: 4167.288
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0075200418711012336,
 'learning_rate_Hydroxylation-K': 0.00320950213355149,
 'learning_rate_Hydroxylation-P': 0.008567601959770335,
 'log_base': 1.426578410631729,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2195476074,
 'sample_weights': [12.90458923755819, 1.613135796769458],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.68854667116406,
 'weight_decay_Hydroxylation-K': 9.18531725877113,
 'weight_decay_Hydroxylation-P': 0.6262308968560781}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1903.687
[2,     1] loss: 1898.658
[3,     1] loss: 1899.318
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002771132791472619,
 'learning_rate_Hydroxylation-K': 0.006668077577680718,
 'learning_rate_Hydroxylation-P': 0.005920684238287539,
 'log_base': 2.110226875915261,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2356910645,
 'sample_weights': [4.698965652180214, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.432784265921349,
 'weight_decay_Hydroxylation-K': 2.087807695528369,
 'weight_decay_Hydroxylation-P': 9.870417902738707}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1382.615
[2,     1] loss: 1380.040
[3,     1] loss: 1381.733
[4,     1] loss: 1380.099
[5,     1] loss: 1378.616
[6,     1] loss: 1378.214
[7,     1] loss: 1377.318
[8,     1] loss: 1372.533
[9,     1] loss: 1365.564
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00039639511218694367,
 'learning_rate_Hydroxylation-K': 0.0051660234146362425,
 'learning_rate_Hydroxylation-P': 0.0027027810496037374,
 'log_base': 2.2138267869567847,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1736637046,
 'sample_weights': [2.2354757410719714, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7758345023784825,
 'weight_decay_Hydroxylation-K': 2.571703718627121,
 'weight_decay_Hydroxylation-P': 4.23153344944606}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.937
[2,     1] loss: 1355.195
[3,     1] loss: 1348.357
[4,     1] loss: 1355.070
[5,     1] loss: 1349.620
[6,     1] loss: 1349.420
[7,     1] loss: 1345.941
[8,     1] loss: 1346.186
[9,     1] loss: 1358.912
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004306731339139823,
 'learning_rate_Hydroxylation-K': 0.00043120309082060864,
 'learning_rate_Hydroxylation-P': 0.006854563761612482,
 'log_base': 1.152113917485149,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 374092574,
 'sample_weights': [2.1006614837258097, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.938678923469167,
 'weight_decay_Hydroxylation-K': 7.17987811630176,
 'weight_decay_Hydroxylation-P': 1.3675390436171564}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3833.226
[2,     1] loss: 3830.625
[3,     1] loss: 3836.583
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006549004380981236,
 'learning_rate_Hydroxylation-K': 0.003835617818306445,
 'learning_rate_Hydroxylation-P': 0.0013059511306027143,
 'log_base': 2.0671636883571516,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3129520842,
 'sample_weights': [11.78998228508513, 1.4738045603183685],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.030755316056428894,
 'weight_decay_Hydroxylation-K': 7.1276848425807255,
 'weight_decay_Hydroxylation-P': 9.62642248160401}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1406.435
[2,     1] loss: 1399.620
[3,     1] loss: 1395.242
[4,     1] loss: 1391.568
[5,     1] loss: 1393.498
[6,     1] loss: 1392.311
[7,     1] loss: 1392.899
[8,     1] loss: 1388.515
[9,     1] loss: 1387.505
[10,     1] loss: 1385.434
[11,     1] loss: 1387.178
[12,     1] loss: 1374.750
[13,     1] loss: 1368.698
[14,     1] loss: 1354.286
[15,     1] loss: 1332.309
[16,     1] loss: 1305.334
[17,     1] loss: 1267.288
[18,     1] loss: 1303.334
[19,     1] loss: 1241.898
[20,     1] loss: 1251.169
[21,     1] loss: 1238.583
[22,     1] loss: 1201.679
[23,     1] loss: 1217.535
[24,     1] loss: 1170.675
[25,     1] loss: 1208.408
[26,     1] loss: 1178.510
[27,     1] loss: 1176.148
[28,     1] loss: 1162.618
[29,     1] loss: 1179.312
[30,     1] loss: 1122.801
[31,     1] loss: 1127.560
[32,     1] loss: 1098.610
[33,     1] loss: 1157.938
[34,     1] loss: 1152.079
[35,     1] loss: 1140.816
[36,     1] loss: 1087.392
[37,     1] loss: 1135.604
[38,     1] loss: 1082.971
[39,     1] loss: 1105.315
[40,     1] loss: 1125.947
[41,     1] loss: 1153.033
[42,     1] loss: 1049.898
[43,     1] loss: 1082.690
[44,     1] loss: 1073.474
[45,     1] loss: 1075.340
[46,     1] loss: 1083.649
[47,     1] loss: 1124.181
[48,     1] loss: 1054.451
[49,     1] loss: 1050.248
[50,     1] loss: 1053.027
[51,     1] loss: 1068.325
[52,     1] loss: 1061.226
[53,     1] loss: 1069.457
[54,     1] loss: 1046.932
[55,     1] loss: 1028.081
[56,     1] loss: 999.315
[57,     1] loss: 1071.757
[58,     1] loss: 989.670
[59,     1] loss: 1045.497
[60,     1] loss: 1036.739
[61,     1] loss: 1011.529
[62,     1] loss: 1027.741
[63,     1] loss: 1018.987
[64,     1] loss: 1068.454
[65,     1] loss: 1054.023
[66,     1] loss: 1026.825
[67,     1] loss: 1004.787
[68,     1] loss: 1003.163
[69,     1] loss: 1052.182
[70,     1] loss: 971.695
[71,     1] loss: 1029.470
[72,     1] loss: 978.238
[73,     1] loss: 987.324
[74,     1] loss: 1000.425
[75,     1] loss: 1003.785
[76,     1] loss: 942.363
[77,     1] loss: 975.674
[78,     1] loss: 1023.224
[79,     1] loss: 955.001
[80,     1] loss: 999.308
[81,     1] loss: 997.455
[82,     1] loss: 982.063
[83,     1] loss: 964.491
[84,     1] loss: 958.131
[85,     1] loss: 1047.324
[86,     1] loss: 989.076
[87,     1] loss: 957.774
[88,     1] loss: 936.275
[89,     1] loss: 938.783
[90,     1] loss: 979.205
[91,     1] loss: 1007.488
[92,     1] loss: 963.829
[93,     1] loss: 905.589
[94,     1] loss: 965.692
[95,     1] loss: 924.423
[96,     1] loss: 998.300
[97,     1] loss: 946.254
[98,     1] loss: 934.986
[99,     1] loss: 914.905
[100,     1] loss: 955.307
[101,     1] loss: 916.785
[102,     1] loss: 948.215
[103,     1] loss: 939.546
[104,     1] loss: 949.590
[105,     1] loss: 910.432
[106,     1] loss: 891.094
[107,     1] loss: 932.022
[108,     1] loss: 918.150
[109,     1] loss: 916.475
[110,     1] loss: 898.314
[111,     1] loss: 910.888
[112,     1] loss: 856.425
[113,     1] loss: 929.765
[114,     1] loss: 917.467
[115,     1] loss: 872.041
[116,     1] loss: 911.292
[117,     1] loss: 932.311
[118,     1] loss: 873.226
[119,     1] loss: 845.011
[120,     1] loss: 870.882
[121,     1] loss: 871.206
[122,     1] loss: 882.441
[123,     1] loss: 905.993
[124,     1] loss: 850.021
[125,     1] loss: 834.615
[126,     1] loss: 858.202
[127,     1] loss: 795.254
[128,     1] loss: 800.968
[129,     1] loss: 841.172
[130,     1] loss: 852.323
[131,     1] loss: 796.476
[132,     1] loss: 955.573
[133,     1] loss: 823.294
[134,     1] loss: 814.312
[135,     1] loss: 779.094
[136,     1] loss: 830.607
[137,     1] loss: 898.771
[138,     1] loss: 787.717
[139,     1] loss: 808.273
[140,     1] loss: 743.701
[141,     1] loss: 897.526
[142,     1] loss: 832.983
[143,     1] loss: 818.671
[144,     1] loss: 847.689
[145,     1] loss: 855.221
[146,     1] loss: 819.202
[147,     1] loss: 820.719
[148,     1] loss: 873.918
[149,     1] loss: 814.694
[150,     1] loss: 787.722
[151,     1] loss: 790.668
[152,     1] loss: 868.740
[153,     1] loss: 813.204
[154,     1] loss: 808.715
[155,     1] loss: 826.432
[156,     1] loss: 808.524
[157,     1] loss: 704.894
Early stopping applied (best metric=0.309461385011673)
Finished Training
Total time taken: 22.15203046798706
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1406.340
[2,     1] loss: 1397.099
[3,     1] loss: 1394.480
[4,     1] loss: 1392.768
[5,     1] loss: 1395.063
[6,     1] loss: 1394.349
[7,     1] loss: 1393.527
[8,     1] loss: 1391.866
[9,     1] loss: 1391.073
[10,     1] loss: 1392.753
[11,     1] loss: 1388.160
[12,     1] loss: 1391.832
[13,     1] loss: 1391.469
[14,     1] loss: 1389.099
[15,     1] loss: 1385.675
[16,     1] loss: 1388.709
[17,     1] loss: 1386.763
[18,     1] loss: 1384.761
[19,     1] loss: 1375.625
[20,     1] loss: 1375.664
[21,     1] loss: 1366.967
[22,     1] loss: 1347.954
[23,     1] loss: 1331.205
[24,     1] loss: 1297.528
[25,     1] loss: 1287.394
[26,     1] loss: 1248.555
[27,     1] loss: 1245.051
[28,     1] loss: 1257.370
[29,     1] loss: 1271.435
[30,     1] loss: 1189.405
[31,     1] loss: 1188.498
[32,     1] loss: 1147.577
[33,     1] loss: 1115.866
[34,     1] loss: 1170.861
[35,     1] loss: 1163.604
[36,     1] loss: 1166.845
[37,     1] loss: 1132.839
[38,     1] loss: 1141.960
[39,     1] loss: 1162.188
[40,     1] loss: 1151.198
[41,     1] loss: 1113.824
[42,     1] loss: 1130.970
[43,     1] loss: 1111.042
[44,     1] loss: 1100.573
[45,     1] loss: 1130.562
[46,     1] loss: 1053.983
[47,     1] loss: 1066.260
[48,     1] loss: 1084.091
[49,     1] loss: 1106.187
[50,     1] loss: 1087.390
[51,     1] loss: 1067.968
[52,     1] loss: 1091.824
[53,     1] loss: 1076.333
[54,     1] loss: 1075.678
[55,     1] loss: 1091.093
[56,     1] loss: 1076.780
[57,     1] loss: 1059.800
[58,     1] loss: 1079.672
[59,     1] loss: 1012.245
[60,     1] loss: 1062.336
[61,     1] loss: 1081.825
[62,     1] loss: 1072.857
[63,     1] loss: 1006.923
[64,     1] loss: 1022.809
[65,     1] loss: 1033.953
[66,     1] loss: 1058.962
[67,     1] loss: 1025.032
[68,     1] loss: 1024.395
[69,     1] loss: 1035.514
[70,     1] loss: 1030.352
[71,     1] loss: 1000.218
[72,     1] loss: 1037.813
[73,     1] loss: 1008.529
[74,     1] loss: 996.243
[75,     1] loss: 981.348
[76,     1] loss: 1082.278
[77,     1] loss: 1017.439
[78,     1] loss: 1013.178
[79,     1] loss: 969.637
[80,     1] loss: 992.075
[81,     1] loss: 1009.909
[82,     1] loss: 1033.288
[83,     1] loss: 973.078
[84,     1] loss: 979.234
[85,     1] loss: 967.539
[86,     1] loss: 988.747
[87,     1] loss: 991.931
[88,     1] loss: 959.448
[89,     1] loss: 1034.406
[90,     1] loss: 936.271
[91,     1] loss: 906.675
[92,     1] loss: 934.142
[93,     1] loss: 922.889
[94,     1] loss: 938.125
[95,     1] loss: 1019.783
[96,     1] loss: 933.255
[97,     1] loss: 975.924
[98,     1] loss: 948.099
[99,     1] loss: 945.225
[100,     1] loss: 949.189
[101,     1] loss: 986.367
[102,     1] loss: 961.260
[103,     1] loss: 904.969
[104,     1] loss: 909.945
[105,     1] loss: 954.220
[106,     1] loss: 935.780
[107,     1] loss: 994.710
[108,     1] loss: 930.134
[109,     1] loss: 964.966
[110,     1] loss: 871.745
[111,     1] loss: 964.031
[112,     1] loss: 926.076
[113,     1] loss: 918.836
[114,     1] loss: 897.206
[115,     1] loss: 903.294
[116,     1] loss: 964.078
[117,     1] loss: 882.799
[118,     1] loss: 927.564
[119,     1] loss: 962.794
[120,     1] loss: 852.827
[121,     1] loss: 889.181
[122,     1] loss: 886.605
[123,     1] loss: 920.596
[124,     1] loss: 910.278
[125,     1] loss: 920.444
[126,     1] loss: 935.715
[127,     1] loss: 927.644
[128,     1] loss: 843.923
[129,     1] loss: 919.782
[130,     1] loss: 849.864
[131,     1] loss: 914.047
[132,     1] loss: 913.552
[133,     1] loss: 837.861
[134,     1] loss: 872.711
[135,     1] loss: 829.021
[136,     1] loss: 837.054
[137,     1] loss: 825.042
[138,     1] loss: 832.486
[139,     1] loss: 849.433
[140,     1] loss: 817.211
[141,     1] loss: 862.831
[142,     1] loss: 891.400
[143,     1] loss: 839.080
[144,     1] loss: 825.696
[145,     1] loss: 852.748
[146,     1] loss: 789.606
[147,     1] loss: 789.338
[148,     1] loss: 776.070
[149,     1] loss: 800.483
[150,     1] loss: 800.594
[151,     1] loss: 864.041
[152,     1] loss: 855.014
[153,     1] loss: 770.471
[154,     1] loss: 848.846
[155,     1] loss: 834.441
[156,     1] loss: 753.196
[157,     1] loss: 738.274
[158,     1] loss: 794.414
[159,     1] loss: 834.992
[160,     1] loss: 850.170
Early stopping applied (best metric=0.37135928869247437)
Finished Training
Total time taken: 22.62203097343445
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.059
[2,     1] loss: 1392.790
[3,     1] loss: 1398.174
[4,     1] loss: 1396.466
[5,     1] loss: 1391.258
[6,     1] loss: 1392.738
[7,     1] loss: 1386.078
[8,     1] loss: 1379.117
[9,     1] loss: 1366.948
[10,     1] loss: 1359.243
[11,     1] loss: 1330.018
[12,     1] loss: 1306.340
[13,     1] loss: 1291.814
[14,     1] loss: 1232.791
[15,     1] loss: 1222.977
[16,     1] loss: 1197.861
[17,     1] loss: 1179.002
[18,     1] loss: 1139.852
[19,     1] loss: 1180.993
[20,     1] loss: 1188.048
[21,     1] loss: 1146.667
[22,     1] loss: 1127.473
[23,     1] loss: 1126.977
[24,     1] loss: 1148.432
[25,     1] loss: 1083.692
[26,     1] loss: 1122.625
[27,     1] loss: 1132.724
[28,     1] loss: 1113.376
[29,     1] loss: 1059.594
[30,     1] loss: 1063.670
[31,     1] loss: 1091.718
[32,     1] loss: 1122.140
[33,     1] loss: 1068.893
[34,     1] loss: 1065.517
[35,     1] loss: 1116.731
[36,     1] loss: 1105.761
[37,     1] loss: 1085.890
[38,     1] loss: 1031.157
[39,     1] loss: 1097.611
[40,     1] loss: 998.887
[41,     1] loss: 1014.705
[42,     1] loss: 1030.223
[43,     1] loss: 1040.377
[44,     1] loss: 1020.063
[45,     1] loss: 1047.757
[46,     1] loss: 1022.330
[47,     1] loss: 1054.094
[48,     1] loss: 1036.336
[49,     1] loss: 1054.898
[50,     1] loss: 1004.542
[51,     1] loss: 1053.733
[52,     1] loss: 1040.915
[53,     1] loss: 1037.351
[54,     1] loss: 1009.452
[55,     1] loss: 976.101
[56,     1] loss: 1005.134
[57,     1] loss: 959.209
[58,     1] loss: 991.565
[59,     1] loss: 1030.021
[60,     1] loss: 1002.458
[61,     1] loss: 976.584
[62,     1] loss: 997.473
[63,     1] loss: 1040.935
[64,     1] loss: 993.354
[65,     1] loss: 979.265
[66,     1] loss: 970.015
[67,     1] loss: 967.546
[68,     1] loss: 1036.808
[69,     1] loss: 938.068
[70,     1] loss: 1004.086
[71,     1] loss: 977.502
[72,     1] loss: 922.016
[73,     1] loss: 979.233
[74,     1] loss: 968.489
[75,     1] loss: 961.662
[76,     1] loss: 1001.106
[77,     1] loss: 976.116
[78,     1] loss: 944.457
[79,     1] loss: 996.887
Early stopping applied (best metric=0.38763296604156494)
Finished Training
Total time taken: 11.152015447616577
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.617
[2,     1] loss: 1412.308
[3,     1] loss: 1392.014
[4,     1] loss: 1389.690
[5,     1] loss: 1396.226
[6,     1] loss: 1392.672
[7,     1] loss: 1392.452
[8,     1] loss: 1393.447
[9,     1] loss: 1391.816
[10,     1] loss: 1396.917
[11,     1] loss: 1389.343
[12,     1] loss: 1397.423
[13,     1] loss: 1396.135
[14,     1] loss: 1388.706
[15,     1] loss: 1393.171
[16,     1] loss: 1393.213
[17,     1] loss: 1389.058
[18,     1] loss: 1392.665
[19,     1] loss: 1391.464
[20,     1] loss: 1391.396
[21,     1] loss: 1391.738
[22,     1] loss: 1393.404
[23,     1] loss: 1390.219
[24,     1] loss: 1393.463
[25,     1] loss: 1389.363
[26,     1] loss: 1390.889
[27,     1] loss: 1391.381
[28,     1] loss: 1392.558
[29,     1] loss: 1391.585
[30,     1] loss: 1389.320
[31,     1] loss: 1394.109
[32,     1] loss: 1391.786
[33,     1] loss: 1391.830
[34,     1] loss: 1388.844
[35,     1] loss: 1391.545
[36,     1] loss: 1383.910
[37,     1] loss: 1387.598
[38,     1] loss: 1384.103
[39,     1] loss: 1386.380
[40,     1] loss: 1379.903
[41,     1] loss: 1378.383
[42,     1] loss: 1369.486
[43,     1] loss: 1362.439
[44,     1] loss: 1348.852
[45,     1] loss: 1350.132
[46,     1] loss: 1308.815
[47,     1] loss: 1311.092
[48,     1] loss: 1273.418
[49,     1] loss: 1264.037
[50,     1] loss: 1238.705
[51,     1] loss: 1269.444
[52,     1] loss: 1219.417
[53,     1] loss: 1273.360
[54,     1] loss: 1229.546
[55,     1] loss: 1206.981
[56,     1] loss: 1227.569
[57,     1] loss: 1189.301
[58,     1] loss: 1210.921
[59,     1] loss: 1198.642
[60,     1] loss: 1155.037
[61,     1] loss: 1166.992
[62,     1] loss: 1171.091
[63,     1] loss: 1178.229
[64,     1] loss: 1162.275
[65,     1] loss: 1210.807
[66,     1] loss: 1211.805
[67,     1] loss: 1159.074
[68,     1] loss: 1181.001
[69,     1] loss: 1171.901
[70,     1] loss: 1142.042
[71,     1] loss: 1146.710
[72,     1] loss: 1169.579
[73,     1] loss: 1080.283
[74,     1] loss: 1156.928
[75,     1] loss: 1150.874
[76,     1] loss: 1154.461
[77,     1] loss: 1141.944
[78,     1] loss: 1162.657
[79,     1] loss: 1158.792
[80,     1] loss: 1127.942
[81,     1] loss: 1080.968
[82,     1] loss: 1138.427
[83,     1] loss: 1119.554
[84,     1] loss: 1127.541
[85,     1] loss: 1113.309
[86,     1] loss: 1094.539
[87,     1] loss: 1177.655
[88,     1] loss: 1164.676
[89,     1] loss: 1142.431
[90,     1] loss: 1129.505
[91,     1] loss: 1047.210
[92,     1] loss: 1069.318
[93,     1] loss: 1116.243
[94,     1] loss: 1119.271
[95,     1] loss: 1082.432
[96,     1] loss: 1079.165
[97,     1] loss: 1056.183
[98,     1] loss: 1120.462
[99,     1] loss: 1041.414
[100,     1] loss: 1104.973
[101,     1] loss: 1087.285
[102,     1] loss: 1108.374
[103,     1] loss: 1095.436
[104,     1] loss: 1026.002
[105,     1] loss: 1053.255
[106,     1] loss: 1084.227
[107,     1] loss: 1087.904
[108,     1] loss: 1085.261
[109,     1] loss: 993.507
[110,     1] loss: 1076.622
[111,     1] loss: 1064.872
[112,     1] loss: 1069.964
[113,     1] loss: 1056.226
[114,     1] loss: 1035.770
[115,     1] loss: 1101.113
[116,     1] loss: 1074.391
[117,     1] loss: 1023.878
[118,     1] loss: 1026.718
[119,     1] loss: 1021.859
[120,     1] loss: 1036.968
[121,     1] loss: 1011.885
[122,     1] loss: 1011.644
[123,     1] loss: 1112.187
[124,     1] loss: 1099.697
[125,     1] loss: 1086.369
[126,     1] loss: 1034.831
[127,     1] loss: 1079.510
[128,     1] loss: 1070.933
[129,     1] loss: 1045.722
[130,     1] loss: 1071.905
[131,     1] loss: 1015.113
[132,     1] loss: 1052.952
[133,     1] loss: 1038.788
[134,     1] loss: 995.216
[135,     1] loss: 1027.411
[136,     1] loss: 1031.273
[137,     1] loss: 1020.359
[138,     1] loss: 991.946
[139,     1] loss: 1012.732
[140,     1] loss: 1004.509
[141,     1] loss: 1043.951
[142,     1] loss: 1030.214
[143,     1] loss: 1013.062
[144,     1] loss: 1006.741
[145,     1] loss: 1016.109
[146,     1] loss: 998.357
[147,     1] loss: 1070.355
[148,     1] loss: 1010.180
[149,     1] loss: 1043.329
[150,     1] loss: 1036.498
[151,     1] loss: 1057.866
[152,     1] loss: 1083.594
[153,     1] loss: 989.165
[154,     1] loss: 1023.287
[155,     1] loss: 1095.764
[156,     1] loss: 953.743
[157,     1] loss: 1040.922
[158,     1] loss: 1015.232
[159,     1] loss: 996.522
[160,     1] loss: 1031.241
[161,     1] loss: 1004.892
[162,     1] loss: 985.645
[163,     1] loss: 1027.965
[164,     1] loss: 1013.389
[165,     1] loss: 976.412
[166,     1] loss: 977.520
[167,     1] loss: 1006.036
[168,     1] loss: 985.261
[169,     1] loss: 949.516
[170,     1] loss: 1012.069
[171,     1] loss: 989.603
[172,     1] loss: 926.084
[173,     1] loss: 1021.333
[174,     1] loss: 1013.663
[175,     1] loss: 990.351
[176,     1] loss: 976.398
[177,     1] loss: 1016.403
[178,     1] loss: 985.999
[179,     1] loss: 957.455
[180,     1] loss: 1008.215
[181,     1] loss: 984.965
[182,     1] loss: 905.795
[183,     1] loss: 946.842
[184,     1] loss: 992.286
[185,     1] loss: 964.448
[186,     1] loss: 993.079
[187,     1] loss: 973.832
[188,     1] loss: 1020.679
[189,     1] loss: 949.493
[190,     1] loss: 948.917
[191,     1] loss: 971.071
[192,     1] loss: 958.979
[193,     1] loss: 1015.629
[194,     1] loss: 974.765
[195,     1] loss: 923.285
[196,     1] loss: 975.085
[197,     1] loss: 1035.911
[198,     1] loss: 960.730
[199,     1] loss: 963.504
[200,     1] loss: 989.876
Finished Training
Total time taken: 28.058038234710693
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1403.210
[2,     1] loss: 1391.089
[3,     1] loss: 1393.034
[4,     1] loss: 1397.677
[5,     1] loss: 1394.427
[6,     1] loss: 1389.979
[7,     1] loss: 1392.764
[8,     1] loss: 1398.165
[9,     1] loss: 1387.812
[10,     1] loss: 1367.785
[11,     1] loss: 1359.130
[12,     1] loss: 1336.479
[13,     1] loss: 1315.817
[14,     1] loss: 1261.200
[15,     1] loss: 1212.055
[16,     1] loss: 1149.852
[17,     1] loss: 1270.729
[18,     1] loss: 1197.764
[19,     1] loss: 1219.333
[20,     1] loss: 1201.130
[21,     1] loss: 1152.896
[22,     1] loss: 1172.631
[23,     1] loss: 1147.277
[24,     1] loss: 1170.081
[25,     1] loss: 1165.316
[26,     1] loss: 1111.777
[27,     1] loss: 1136.014
[28,     1] loss: 1132.739
[29,     1] loss: 1151.464
[30,     1] loss: 1099.566
[31,     1] loss: 1104.922
[32,     1] loss: 1056.961
[33,     1] loss: 1149.907
[34,     1] loss: 1107.856
[35,     1] loss: 1097.187
[36,     1] loss: 1076.345
[37,     1] loss: 1067.382
[38,     1] loss: 1052.458
[39,     1] loss: 1030.315
[40,     1] loss: 1095.314
[41,     1] loss: 1056.571
[42,     1] loss: 1044.832
[43,     1] loss: 999.612
[44,     1] loss: 1007.783
[45,     1] loss: 1014.844
[46,     1] loss: 1011.338
[47,     1] loss: 971.214
[48,     1] loss: 1021.152
[49,     1] loss: 988.677
[50,     1] loss: 963.697
[51,     1] loss: 992.812
[52,     1] loss: 1034.300
[53,     1] loss: 997.002
[54,     1] loss: 976.825
[55,     1] loss: 995.939
[56,     1] loss: 982.120
[57,     1] loss: 941.631
[58,     1] loss: 974.834
[59,     1] loss: 978.630
[60,     1] loss: 963.765
[61,     1] loss: 953.603
[62,     1] loss: 951.549
[63,     1] loss: 938.867
[64,     1] loss: 928.123
[65,     1] loss: 935.418
[66,     1] loss: 862.536
[67,     1] loss: 914.970
[68,     1] loss: 956.244
[69,     1] loss: 919.196
[70,     1] loss: 947.521
[71,     1] loss: 973.216
[72,     1] loss: 924.546
[73,     1] loss: 951.392
[74,     1] loss: 938.012
[75,     1] loss: 911.314
[76,     1] loss: 884.789
[77,     1] loss: 940.861
[78,     1] loss: 847.165
[79,     1] loss: 879.188
[80,     1] loss: 893.318
[81,     1] loss: 860.850
[82,     1] loss: 812.960
[83,     1] loss: 893.263
[84,     1] loss: 841.755
Early stopping applied (best metric=0.4076699912548065)
Finished Training
Total time taken: 12.691017389297485
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.649
[2,     1] loss: 1392.490
[3,     1] loss: 1398.136
[4,     1] loss: 1384.355
[5,     1] loss: 1397.626
[6,     1] loss: 1384.784
[7,     1] loss: 1382.646
[8,     1] loss: 1384.577
[9,     1] loss: 1368.440
[10,     1] loss: 1343.635
[11,     1] loss: 1313.612
[12,     1] loss: 1288.161
[13,     1] loss: 1225.129
[14,     1] loss: 1229.001
[15,     1] loss: 1243.369
[16,     1] loss: 1231.877
[17,     1] loss: 1234.354
[18,     1] loss: 1123.931
[19,     1] loss: 1172.012
[20,     1] loss: 1173.158
[21,     1] loss: 1178.878
[22,     1] loss: 1156.273
[23,     1] loss: 1164.045
[24,     1] loss: 1098.215
[25,     1] loss: 1135.578
[26,     1] loss: 1115.637
[27,     1] loss: 1128.858
[28,     1] loss: 1035.039
[29,     1] loss: 1056.527
[30,     1] loss: 1104.265
[31,     1] loss: 1007.941
[32,     1] loss: 1053.979
[33,     1] loss: 1066.026
[34,     1] loss: 1047.167
[35,     1] loss: 1062.137
[36,     1] loss: 1094.085
[37,     1] loss: 1058.116
[38,     1] loss: 1065.249
[39,     1] loss: 1011.706
[40,     1] loss: 1092.335
[41,     1] loss: 1075.973
[42,     1] loss: 1071.548
[43,     1] loss: 1063.759
[44,     1] loss: 1030.380
[45,     1] loss: 1081.015
[46,     1] loss: 1064.580
[47,     1] loss: 1045.441
[48,     1] loss: 1030.755
[49,     1] loss: 1040.746
[50,     1] loss: 989.665
[51,     1] loss: 987.382
[52,     1] loss: 1030.530
[53,     1] loss: 1007.569
[54,     1] loss: 1026.015
[55,     1] loss: 1088.008
[56,     1] loss: 963.148
[57,     1] loss: 1053.335
[58,     1] loss: 1065.470
[59,     1] loss: 1051.607
[60,     1] loss: 960.672
[61,     1] loss: 987.167
[62,     1] loss: 994.045
[63,     1] loss: 1000.637
[64,     1] loss: 993.394
[65,     1] loss: 954.712
[66,     1] loss: 1003.090
[67,     1] loss: 955.188
[68,     1] loss: 998.237
[69,     1] loss: 1015.676
[70,     1] loss: 980.253
[71,     1] loss: 966.178
[72,     1] loss: 941.350
[73,     1] loss: 976.579
[74,     1] loss: 1016.068
[75,     1] loss: 922.008
[76,     1] loss: 862.813
[77,     1] loss: 1013.268
[78,     1] loss: 980.094
[79,     1] loss: 1002.925
[80,     1] loss: 941.599
[81,     1] loss: 1021.773
[82,     1] loss: 1004.570
[83,     1] loss: 964.986
[84,     1] loss: 906.839
[85,     1] loss: 931.602
[86,     1] loss: 968.737
[87,     1] loss: 911.055
[88,     1] loss: 936.020
[89,     1] loss: 987.376
[90,     1] loss: 971.838
[91,     1] loss: 932.871
[92,     1] loss: 959.519
[93,     1] loss: 952.512
[94,     1] loss: 898.805
[95,     1] loss: 869.569
[96,     1] loss: 885.148
[97,     1] loss: 918.270
[98,     1] loss: 867.493
[99,     1] loss: 926.508
[100,     1] loss: 829.234
[101,     1] loss: 929.689
[102,     1] loss: 867.307
[103,     1] loss: 910.914
[104,     1] loss: 839.790
[105,     1] loss: 852.739
[106,     1] loss: 877.461
[107,     1] loss: 840.708
[108,     1] loss: 867.665
[109,     1] loss: 892.381
[110,     1] loss: 882.128
[111,     1] loss: 886.225
[112,     1] loss: 877.137
[113,     1] loss: 881.769
[114,     1] loss: 844.326
[115,     1] loss: 850.157
[116,     1] loss: 897.961
[117,     1] loss: 917.988
[118,     1] loss: 822.937
[119,     1] loss: 906.398
[120,     1] loss: 877.756
[121,     1] loss: 895.846
[122,     1] loss: 828.034
[123,     1] loss: 904.084
[124,     1] loss: 852.223
[125,     1] loss: 826.273
[126,     1] loss: 796.266
[127,     1] loss: 821.219
[128,     1] loss: 862.579
[129,     1] loss: 833.185
[130,     1] loss: 857.002
[131,     1] loss: 804.848
[132,     1] loss: 805.596
[133,     1] loss: 834.921
[134,     1] loss: 840.352
Early stopping applied (best metric=0.3711657226085663)
Finished Training
Total time taken: 19.55502700805664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.342
[2,     1] loss: 1398.992
[3,     1] loss: 1393.337
[4,     1] loss: 1391.654
[5,     1] loss: 1397.669
[6,     1] loss: 1389.891
[7,     1] loss: 1387.652
[8,     1] loss: 1383.502
[9,     1] loss: 1379.037
[10,     1] loss: 1366.695
[11,     1] loss: 1354.197
[12,     1] loss: 1341.907
[13,     1] loss: 1319.592
[14,     1] loss: 1290.328
[15,     1] loss: 1249.584
[16,     1] loss: 1270.411
[17,     1] loss: 1201.263
[18,     1] loss: 1207.478
[19,     1] loss: 1169.583
[20,     1] loss: 1186.202
[21,     1] loss: 1183.498
[22,     1] loss: 1108.480
[23,     1] loss: 1076.921
[24,     1] loss: 1161.633
[25,     1] loss: 1143.256
[26,     1] loss: 1117.348
[27,     1] loss: 1158.266
[28,     1] loss: 1113.890
[29,     1] loss: 1080.801
[30,     1] loss: 1111.222
[31,     1] loss: 1084.747
[32,     1] loss: 1075.779
[33,     1] loss: 1064.369
[34,     1] loss: 1066.036
[35,     1] loss: 1092.266
[36,     1] loss: 1061.766
[37,     1] loss: 1067.964
[38,     1] loss: 1086.837
[39,     1] loss: 1047.715
[40,     1] loss: 1113.240
[41,     1] loss: 1078.042
[42,     1] loss: 1047.058
[43,     1] loss: 1024.774
[44,     1] loss: 1006.049
[45,     1] loss: 1061.867
[46,     1] loss: 1023.159
[47,     1] loss: 965.083
[48,     1] loss: 1022.151
[49,     1] loss: 978.890
[50,     1] loss: 974.948
[51,     1] loss: 1027.525
[52,     1] loss: 1032.708
[53,     1] loss: 960.194
[54,     1] loss: 943.188
[55,     1] loss: 1026.618
[56,     1] loss: 1020.602
[57,     1] loss: 961.445
[58,     1] loss: 963.028
[59,     1] loss: 943.218
[60,     1] loss: 962.019
[61,     1] loss: 919.551
[62,     1] loss: 951.001
[63,     1] loss: 924.090
[64,     1] loss: 1037.404
[65,     1] loss: 940.059
[66,     1] loss: 977.587
[67,     1] loss: 944.460
[68,     1] loss: 899.620
[69,     1] loss: 876.305
[70,     1] loss: 933.394
[71,     1] loss: 916.224
[72,     1] loss: 941.876
[73,     1] loss: 884.376
[74,     1] loss: 867.695
[75,     1] loss: 913.357
[76,     1] loss: 888.430
[77,     1] loss: 874.024
[78,     1] loss: 895.764
[79,     1] loss: 936.526
[80,     1] loss: 887.388
[81,     1] loss: 883.642
[82,     1] loss: 903.802
[83,     1] loss: 848.143
[84,     1] loss: 837.725
[85,     1] loss: 897.438
Early stopping applied (best metric=0.39927443861961365)
Finished Training
Total time taken: 12.437020063400269
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.132
[2,     1] loss: 1394.699
[3,     1] loss: 1388.186
[4,     1] loss: 1398.863
[5,     1] loss: 1395.216
[6,     1] loss: 1390.068
[7,     1] loss: 1390.339
[8,     1] loss: 1391.928
[9,     1] loss: 1394.702
[10,     1] loss: 1390.728
[11,     1] loss: 1391.362
[12,     1] loss: 1385.017
[13,     1] loss: 1388.243
[14,     1] loss: 1388.487
[15,     1] loss: 1383.741
[16,     1] loss: 1377.515
[17,     1] loss: 1372.433
[18,     1] loss: 1355.546
[19,     1] loss: 1349.464
[20,     1] loss: 1330.590
[21,     1] loss: 1300.810
[22,     1] loss: 1264.413
[23,     1] loss: 1267.516
[24,     1] loss: 1236.531
[25,     1] loss: 1234.386
[26,     1] loss: 1215.564
[27,     1] loss: 1234.666
[28,     1] loss: 1208.453
[29,     1] loss: 1191.841
[30,     1] loss: 1183.719
[31,     1] loss: 1164.777
[32,     1] loss: 1172.059
[33,     1] loss: 1148.680
[34,     1] loss: 1168.570
[35,     1] loss: 1145.361
[36,     1] loss: 1157.129
[37,     1] loss: 1176.102
[38,     1] loss: 1181.145
[39,     1] loss: 1168.040
[40,     1] loss: 1125.741
[41,     1] loss: 1166.649
[42,     1] loss: 1087.257
[43,     1] loss: 1131.917
[44,     1] loss: 1163.338
[45,     1] loss: 1130.700
[46,     1] loss: 1114.606
[47,     1] loss: 1121.798
[48,     1] loss: 1110.902
[49,     1] loss: 1073.402
[50,     1] loss: 1076.676
[51,     1] loss: 1139.030
[52,     1] loss: 1107.462
[53,     1] loss: 1071.680
[54,     1] loss: 1079.355
[55,     1] loss: 1087.509
[56,     1] loss: 1077.730
[57,     1] loss: 1087.631
[58,     1] loss: 1048.998
[59,     1] loss: 1082.157
[60,     1] loss: 1154.289
[61,     1] loss: 1072.948
[62,     1] loss: 1044.803
[63,     1] loss: 1120.281
[64,     1] loss: 1066.591
[65,     1] loss: 1087.245
[66,     1] loss: 1011.825
[67,     1] loss: 1039.528
[68,     1] loss: 1065.874
[69,     1] loss: 1084.713
[70,     1] loss: 1021.555
[71,     1] loss: 1033.112
[72,     1] loss: 1083.640
[73,     1] loss: 985.178
[74,     1] loss: 980.956
[75,     1] loss: 1025.130
[76,     1] loss: 1043.880
[77,     1] loss: 1011.896
[78,     1] loss: 1028.075
[79,     1] loss: 1013.771
[80,     1] loss: 1007.076
[81,     1] loss: 1045.154
[82,     1] loss: 1055.711
[83,     1] loss: 995.502
[84,     1] loss: 1033.167
[85,     1] loss: 1018.069
[86,     1] loss: 985.495
[87,     1] loss: 1009.396
[88,     1] loss: 943.827
[89,     1] loss: 1023.922
[90,     1] loss: 1000.350
[91,     1] loss: 988.995
[92,     1] loss: 985.173
[93,     1] loss: 1002.256
[94,     1] loss: 985.152
[95,     1] loss: 986.976
[96,     1] loss: 958.801
[97,     1] loss: 1000.387
[98,     1] loss: 1020.480
[99,     1] loss: 1022.501
[100,     1] loss: 970.038
[101,     1] loss: 989.000
[102,     1] loss: 939.547
[103,     1] loss: 959.869
[104,     1] loss: 991.497
[105,     1] loss: 965.802
[106,     1] loss: 983.512
[107,     1] loss: 993.021
[108,     1] loss: 933.897
[109,     1] loss: 935.151
[110,     1] loss: 969.484
[111,     1] loss: 958.405
Early stopping applied (best metric=0.3695787489414215)
Finished Training
Total time taken: 16.160022258758545
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.224
[2,     1] loss: 1398.307
[3,     1] loss: 1393.299
[4,     1] loss: 1393.851
[5,     1] loss: 1394.726
[6,     1] loss: 1390.557
[7,     1] loss: 1392.679
[8,     1] loss: 1389.660
[9,     1] loss: 1391.258
[10,     1] loss: 1396.014
[11,     1] loss: 1391.646
[12,     1] loss: 1392.840
[13,     1] loss: 1389.818
[14,     1] loss: 1386.112
[15,     1] loss: 1379.863
[16,     1] loss: 1384.439
[17,     1] loss: 1382.992
[18,     1] loss: 1372.891
[19,     1] loss: 1369.102
[20,     1] loss: 1350.688
[21,     1] loss: 1333.091
[22,     1] loss: 1324.987
[23,     1] loss: 1279.694
[24,     1] loss: 1265.841
[25,     1] loss: 1222.327
[26,     1] loss: 1230.183
[27,     1] loss: 1210.899
[28,     1] loss: 1220.333
[29,     1] loss: 1204.389
[30,     1] loss: 1167.185
[31,     1] loss: 1180.196
[32,     1] loss: 1163.842
[33,     1] loss: 1160.303
[34,     1] loss: 1192.946
[35,     1] loss: 1161.449
[36,     1] loss: 1196.547
[37,     1] loss: 1151.009
[38,     1] loss: 1103.224
[39,     1] loss: 1137.730
[40,     1] loss: 1150.431
[41,     1] loss: 1120.894
[42,     1] loss: 1076.817
[43,     1] loss: 1086.515
[44,     1] loss: 1091.053
[45,     1] loss: 1052.841
[46,     1] loss: 1075.949
[47,     1] loss: 1138.355
[48,     1] loss: 1057.195
[49,     1] loss: 1065.014
[50,     1] loss: 1084.010
[51,     1] loss: 1056.640
[52,     1] loss: 1096.930
[53,     1] loss: 1059.161
[54,     1] loss: 1105.831
[55,     1] loss: 1054.244
[56,     1] loss: 1072.011
[57,     1] loss: 1051.984
[58,     1] loss: 1067.151
[59,     1] loss: 1087.281
[60,     1] loss: 987.882
[61,     1] loss: 1044.167
[62,     1] loss: 1077.811
[63,     1] loss: 1055.953
[64,     1] loss: 1025.407
[65,     1] loss: 1025.300
[66,     1] loss: 971.603
[67,     1] loss: 1048.854
[68,     1] loss: 1069.154
[69,     1] loss: 1043.896
[70,     1] loss: 973.303
[71,     1] loss: 987.853
[72,     1] loss: 1018.936
[73,     1] loss: 1003.770
[74,     1] loss: 991.098
[75,     1] loss: 1003.439
[76,     1] loss: 1044.368
[77,     1] loss: 1009.814
[78,     1] loss: 993.939
[79,     1] loss: 1010.606
[80,     1] loss: 968.155
[81,     1] loss: 1017.134
[82,     1] loss: 995.130
[83,     1] loss: 1020.469
[84,     1] loss: 989.821
[85,     1] loss: 963.217
[86,     1] loss: 1003.230
[87,     1] loss: 1024.818
[88,     1] loss: 1003.430
[89,     1] loss: 950.518
[90,     1] loss: 1052.835
[91,     1] loss: 1026.421
[92,     1] loss: 988.811
[93,     1] loss: 1044.465
[94,     1] loss: 1055.895
[95,     1] loss: 988.930
[96,     1] loss: 956.642
[97,     1] loss: 965.985
[98,     1] loss: 987.444
[99,     1] loss: 973.076
Early stopping applied (best metric=0.3825528025627136)
Finished Training
Total time taken: 14.202020406723022
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1400.668
[2,     1] loss: 1398.599
[3,     1] loss: 1391.803
[4,     1] loss: 1397.276
[5,     1] loss: 1395.446
[6,     1] loss: 1394.661
[7,     1] loss: 1396.981
[8,     1] loss: 1394.189
[9,     1] loss: 1395.181
[10,     1] loss: 1392.588
[11,     1] loss: 1396.134
[12,     1] loss: 1400.023
[13,     1] loss: 1393.957
[14,     1] loss: 1387.955
[15,     1] loss: 1381.868
[16,     1] loss: 1377.961
[17,     1] loss: 1373.562
[18,     1] loss: 1368.086
[19,     1] loss: 1360.821
[20,     1] loss: 1339.507
[21,     1] loss: 1320.524
[22,     1] loss: 1315.570
[23,     1] loss: 1250.356
[24,     1] loss: 1290.118
[25,     1] loss: 1228.183
[26,     1] loss: 1185.404
[27,     1] loss: 1180.965
[28,     1] loss: 1176.039
[29,     1] loss: 1147.724
[30,     1] loss: 1118.681
[31,     1] loss: 1150.004
[32,     1] loss: 1155.146
[33,     1] loss: 1149.516
[34,     1] loss: 1138.236
[35,     1] loss: 1099.953
[36,     1] loss: 1118.177
[37,     1] loss: 1129.860
[38,     1] loss: 1130.124
[39,     1] loss: 1092.152
[40,     1] loss: 1116.360
[41,     1] loss: 1173.406
[42,     1] loss: 1064.198
[43,     1] loss: 1102.935
[44,     1] loss: 1073.059
[45,     1] loss: 1137.666
[46,     1] loss: 1097.031
[47,     1] loss: 1109.270
[48,     1] loss: 1106.664
[49,     1] loss: 1076.912
[50,     1] loss: 1066.341
[51,     1] loss: 1080.057
[52,     1] loss: 1042.757
[53,     1] loss: 1031.805
[54,     1] loss: 1012.276
[55,     1] loss: 1054.625
[56,     1] loss: 1059.889
[57,     1] loss: 1083.741
[58,     1] loss: 1035.595
[59,     1] loss: 1072.544
[60,     1] loss: 1021.330
[61,     1] loss: 1021.278
[62,     1] loss: 1059.236
[63,     1] loss: 1085.018
[64,     1] loss: 1037.376
[65,     1] loss: 1033.941
[66,     1] loss: 1059.923
[67,     1] loss: 1081.191
[68,     1] loss: 1036.083
[69,     1] loss: 995.227
[70,     1] loss: 1050.467
[71,     1] loss: 1031.800
[72,     1] loss: 1072.279
[73,     1] loss: 1074.469
[74,     1] loss: 1045.581
[75,     1] loss: 1004.884
[76,     1] loss: 956.096
[77,     1] loss: 961.396
[78,     1] loss: 985.524
[79,     1] loss: 1026.825
[80,     1] loss: 984.927
[81,     1] loss: 1004.094
[82,     1] loss: 986.285
[83,     1] loss: 973.393
[84,     1] loss: 979.107
[85,     1] loss: 994.850
[86,     1] loss: 993.341
[87,     1] loss: 1015.678
[88,     1] loss: 968.923
[89,     1] loss: 932.038
[90,     1] loss: 982.601
[91,     1] loss: 1027.299
[92,     1] loss: 970.541
[93,     1] loss: 975.114
[94,     1] loss: 947.633
[95,     1] loss: 985.327
[96,     1] loss: 967.215
[97,     1] loss: 906.519
[98,     1] loss: 944.626
[99,     1] loss: 975.278
[100,     1] loss: 929.985
[101,     1] loss: 990.742
[102,     1] loss: 949.865
[103,     1] loss: 968.341
[104,     1] loss: 882.436
[105,     1] loss: 922.204
[106,     1] loss: 882.593
[107,     1] loss: 1012.440
[108,     1] loss: 936.007
[109,     1] loss: 880.958
[110,     1] loss: 968.831
[111,     1] loss: 972.154
[112,     1] loss: 929.640
[113,     1] loss: 942.972
[114,     1] loss: 916.321
[115,     1] loss: 909.636
[116,     1] loss: 934.002
Early stopping applied (best metric=0.37957215309143066)
Finished Training
Total time taken: 16.790024042129517
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.469
[2,     1] loss: 1399.748
[3,     1] loss: 1408.544
[4,     1] loss: 1393.307
[5,     1] loss: 1388.635
[6,     1] loss: 1391.211
[7,     1] loss: 1391.131
[8,     1] loss: 1396.316
[9,     1] loss: 1391.952
[10,     1] loss: 1392.839
[11,     1] loss: 1392.638
[12,     1] loss: 1393.308
[13,     1] loss: 1394.584
[14,     1] loss: 1389.757
[15,     1] loss: 1386.233
[16,     1] loss: 1384.435
[17,     1] loss: 1383.641
[18,     1] loss: 1378.933
[19,     1] loss: 1374.804
[20,     1] loss: 1366.200
[21,     1] loss: 1354.119
[22,     1] loss: 1344.593
[23,     1] loss: 1306.229
[24,     1] loss: 1282.987
[25,     1] loss: 1258.798
[26,     1] loss: 1259.457
[27,     1] loss: 1203.820
[28,     1] loss: 1166.336
[29,     1] loss: 1191.427
[30,     1] loss: 1182.705
[31,     1] loss: 1167.786
[32,     1] loss: 1169.346
[33,     1] loss: 1145.824
[34,     1] loss: 1123.952
[35,     1] loss: 1117.843
[36,     1] loss: 1157.329
[37,     1] loss: 1103.499
[38,     1] loss: 1108.095
[39,     1] loss: 1135.414
[40,     1] loss: 1097.928
[41,     1] loss: 1097.820
[42,     1] loss: 1117.077
[43,     1] loss: 1092.397
[44,     1] loss: 1124.088
[45,     1] loss: 1078.139
[46,     1] loss: 1072.049
[47,     1] loss: 1087.205
[48,     1] loss: 1083.078
[49,     1] loss: 1073.274
[50,     1] loss: 1045.802
[51,     1] loss: 1026.690
[52,     1] loss: 1095.688
[53,     1] loss: 1033.632
[54,     1] loss: 1073.992
[55,     1] loss: 1046.327
[56,     1] loss: 1053.011
[57,     1] loss: 1003.714
[58,     1] loss: 1016.281
[59,     1] loss: 1052.426
[60,     1] loss: 1029.250
[61,     1] loss: 1013.734
[62,     1] loss: 1016.652
[63,     1] loss: 993.747
[64,     1] loss: 1010.118
[65,     1] loss: 1054.536
[66,     1] loss: 1029.485
[67,     1] loss: 1014.681
[68,     1] loss: 1044.071
[69,     1] loss: 1024.032
[70,     1] loss: 1034.651
[71,     1] loss: 993.072
[72,     1] loss: 971.212
[73,     1] loss: 1008.596
[74,     1] loss: 961.754
[75,     1] loss: 973.172
[76,     1] loss: 1012.176
[77,     1] loss: 1001.968
[78,     1] loss: 1000.926
[79,     1] loss: 975.750
[80,     1] loss: 990.287
[81,     1] loss: 966.890
[82,     1] loss: 1006.834
[83,     1] loss: 978.145
[84,     1] loss: 967.065
[85,     1] loss: 973.462
[86,     1] loss: 1020.688
[87,     1] loss: 985.222
[88,     1] loss: 960.174
[89,     1] loss: 965.558
[90,     1] loss: 949.248
[91,     1] loss: 1001.610
[92,     1] loss: 968.961
[93,     1] loss: 971.929
[94,     1] loss: 906.455
[95,     1] loss: 940.955
[96,     1] loss: 962.626
[97,     1] loss: 930.632
[98,     1] loss: 963.224
[99,     1] loss: 896.508
[100,     1] loss: 933.297
[101,     1] loss: 916.254
[102,     1] loss: 931.650
[103,     1] loss: 965.918
[104,     1] loss: 876.797
[105,     1] loss: 958.913
[106,     1] loss: 965.543
[107,     1] loss: 931.166
[108,     1] loss: 939.401
[109,     1] loss: 963.564
[110,     1] loss: 929.552
[111,     1] loss: 922.770
[112,     1] loss: 930.274
[113,     1] loss: 893.415
[114,     1] loss: 950.239
[115,     1] loss: 932.960
[116,     1] loss: 912.419
[117,     1] loss: 861.511
[118,     1] loss: 876.632
[119,     1] loss: 880.330
Early stopping applied (best metric=0.4223259687423706)
Finished Training
Total time taken: 17.49202275276184
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.769
[2,     1] loss: 1404.557
[3,     1] loss: 1390.582
[4,     1] loss: 1395.806
[5,     1] loss: 1391.271
[6,     1] loss: 1393.180
[7,     1] loss: 1393.488
[8,     1] loss: 1391.156
[9,     1] loss: 1392.896
[10,     1] loss: 1393.276
[11,     1] loss: 1393.694
[12,     1] loss: 1390.910
[13,     1] loss: 1391.028
[14,     1] loss: 1385.305
[15,     1] loss: 1393.744
[16,     1] loss: 1396.196
[17,     1] loss: 1385.641
[18,     1] loss: 1381.940
[19,     1] loss: 1393.357
[20,     1] loss: 1386.839
[21,     1] loss: 1386.677
[22,     1] loss: 1380.182
[23,     1] loss: 1375.943
[24,     1] loss: 1365.852
[25,     1] loss: 1351.048
[26,     1] loss: 1340.606
[27,     1] loss: 1344.011
[28,     1] loss: 1324.792
[29,     1] loss: 1276.688
[30,     1] loss: 1243.344
[31,     1] loss: 1216.402
[32,     1] loss: 1202.942
[33,     1] loss: 1220.097
[34,     1] loss: 1210.551
[35,     1] loss: 1206.444
[36,     1] loss: 1116.980
[37,     1] loss: 1172.049
[38,     1] loss: 1166.851
[39,     1] loss: 1197.450
[40,     1] loss: 1160.516
[41,     1] loss: 1123.686
[42,     1] loss: 1153.554
[43,     1] loss: 1160.775
[44,     1] loss: 1132.688
[45,     1] loss: 1105.262
[46,     1] loss: 1122.443
[47,     1] loss: 1120.658
[48,     1] loss: 1147.706
[49,     1] loss: 1103.121
[50,     1] loss: 1081.187
[51,     1] loss: 1107.919
[52,     1] loss: 1077.803
[53,     1] loss: 1108.856
[54,     1] loss: 1077.832
[55,     1] loss: 1101.210
[56,     1] loss: 1074.041
[57,     1] loss: 1067.215
[58,     1] loss: 1092.578
[59,     1] loss: 1049.238
[60,     1] loss: 1097.251
[61,     1] loss: 1043.673
[62,     1] loss: 1006.574
[63,     1] loss: 1073.582
[64,     1] loss: 1068.096
[65,     1] loss: 1004.337
[66,     1] loss: 1022.294
[67,     1] loss: 1051.314
[68,     1] loss: 1092.208
[69,     1] loss: 997.364
[70,     1] loss: 1067.114
[71,     1] loss: 1023.975
[72,     1] loss: 997.359
[73,     1] loss: 1032.266
[74,     1] loss: 1024.362
[75,     1] loss: 1031.134
[76,     1] loss: 1036.247
[77,     1] loss: 993.686
[78,     1] loss: 1009.144
[79,     1] loss: 1009.468
[80,     1] loss: 982.261
[81,     1] loss: 1000.191
[82,     1] loss: 999.023
[83,     1] loss: 1008.626
[84,     1] loss: 1025.472
[85,     1] loss: 1001.918
[86,     1] loss: 988.545
[87,     1] loss: 985.085
[88,     1] loss: 1035.261
[89,     1] loss: 1000.282
[90,     1] loss: 1013.835
[91,     1] loss: 968.827
[92,     1] loss: 1003.719
[93,     1] loss: 964.728
[94,     1] loss: 1002.669
[95,     1] loss: 930.741
[96,     1] loss: 980.242
[97,     1] loss: 974.399
[98,     1] loss: 1029.650
[99,     1] loss: 951.453
[100,     1] loss: 1007.951
[101,     1] loss: 918.432
[102,     1] loss: 958.347
[103,     1] loss: 941.688
[104,     1] loss: 1020.610
Early stopping applied (best metric=0.40417107939720154)
Finished Training
Total time taken: 15.086020469665527
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.324
[2,     1] loss: 1398.001
[3,     1] loss: 1394.258
[4,     1] loss: 1391.761
[5,     1] loss: 1387.077
[6,     1] loss: 1384.856
[7,     1] loss: 1396.636
[8,     1] loss: 1394.314
[9,     1] loss: 1404.336
[10,     1] loss: 1398.129
[11,     1] loss: 1392.182
[12,     1] loss: 1390.174
[13,     1] loss: 1392.952
[14,     1] loss: 1391.677
[15,     1] loss: 1395.822
[16,     1] loss: 1390.121
[17,     1] loss: 1385.264
[18,     1] loss: 1387.533
[19,     1] loss: 1389.277
[20,     1] loss: 1388.451
[21,     1] loss: 1384.830
[22,     1] loss: 1379.462
[23,     1] loss: 1378.042
[24,     1] loss: 1378.506
[25,     1] loss: 1371.670
[26,     1] loss: 1357.259
[27,     1] loss: 1346.744
[28,     1] loss: 1338.348
[29,     1] loss: 1305.838
[30,     1] loss: 1303.827
[31,     1] loss: 1244.108
[32,     1] loss: 1218.720
[33,     1] loss: 1236.366
[34,     1] loss: 1206.520
[35,     1] loss: 1231.226
[36,     1] loss: 1158.399
[37,     1] loss: 1179.555
[38,     1] loss: 1172.580
[39,     1] loss: 1175.276
[40,     1] loss: 1179.145
[41,     1] loss: 1190.814
[42,     1] loss: 1181.843
[43,     1] loss: 1162.803
[44,     1] loss: 1173.934
[45,     1] loss: 1194.392
[46,     1] loss: 1108.292
[47,     1] loss: 1079.685
[48,     1] loss: 1149.455
[49,     1] loss: 1159.806
[50,     1] loss: 1113.100
[51,     1] loss: 1090.019
[52,     1] loss: 1163.884
[53,     1] loss: 1147.351
[54,     1] loss: 1110.058
[55,     1] loss: 1099.636
[56,     1] loss: 1106.150
[57,     1] loss: 1107.750
[58,     1] loss: 1116.672
[59,     1] loss: 1100.892
[60,     1] loss: 1078.632
[61,     1] loss: 1084.443
[62,     1] loss: 1075.157
[63,     1] loss: 1055.754
[64,     1] loss: 1083.323
[65,     1] loss: 1055.715
[66,     1] loss: 1084.688
[67,     1] loss: 1076.550
[68,     1] loss: 1062.489
[69,     1] loss: 1068.540
[70,     1] loss: 1047.957
[71,     1] loss: 1070.538
[72,     1] loss: 1033.268
[73,     1] loss: 1069.241
[74,     1] loss: 1014.601
[75,     1] loss: 1025.732
[76,     1] loss: 1066.727
[77,     1] loss: 991.590
[78,     1] loss: 1061.633
[79,     1] loss: 1040.104
[80,     1] loss: 1071.735
[81,     1] loss: 977.143
[82,     1] loss: 1038.246
[83,     1] loss: 1023.562
[84,     1] loss: 1005.254
[85,     1] loss: 1055.002
[86,     1] loss: 1012.693
[87,     1] loss: 999.141
[88,     1] loss: 1000.757
[89,     1] loss: 971.119
[90,     1] loss: 1050.759
[91,     1] loss: 1023.306
[92,     1] loss: 1005.148
[93,     1] loss: 1068.009
[94,     1] loss: 979.922
[95,     1] loss: 972.701
[96,     1] loss: 1023.851
[97,     1] loss: 968.307
[98,     1] loss: 954.576
[99,     1] loss: 975.578
[100,     1] loss: 1034.122
[101,     1] loss: 995.152
[102,     1] loss: 1003.898
[103,     1] loss: 956.892
[104,     1] loss: 922.407
[105,     1] loss: 940.948
[106,     1] loss: 970.191
[107,     1] loss: 979.435
[108,     1] loss: 952.987
[109,     1] loss: 917.262
[110,     1] loss: 945.226
[111,     1] loss: 951.195
[112,     1] loss: 977.256
[113,     1] loss: 924.689
[114,     1] loss: 875.257
[115,     1] loss: 1007.522
[116,     1] loss: 905.581
[117,     1] loss: 878.550
[118,     1] loss: 902.441
[119,     1] loss: 930.625
[120,     1] loss: 903.966
[121,     1] loss: 939.591
[122,     1] loss: 903.580
[123,     1] loss: 870.540
[124,     1] loss: 897.738
[125,     1] loss: 899.918
[126,     1] loss: 934.589
[127,     1] loss: 902.589
[128,     1] loss: 905.847
Early stopping applied (best metric=0.3694628179073334)
Finished Training
Total time taken: 18.352023124694824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1395.139
[2,     1] loss: 1393.745
[3,     1] loss: 1392.721
[4,     1] loss: 1389.997
[5,     1] loss: 1391.430
[6,     1] loss: 1390.473
[7,     1] loss: 1390.541
[8,     1] loss: 1387.746
[9,     1] loss: 1386.652
[10,     1] loss: 1383.718
[11,     1] loss: 1375.078
[12,     1] loss: 1368.933
[13,     1] loss: 1355.404
[14,     1] loss: 1348.325
[15,     1] loss: 1312.726
[16,     1] loss: 1292.285
[17,     1] loss: 1242.639
[18,     1] loss: 1207.256
[19,     1] loss: 1211.897
[20,     1] loss: 1194.553
[21,     1] loss: 1208.104
[22,     1] loss: 1191.101
[23,     1] loss: 1092.106
[24,     1] loss: 1160.922
[25,     1] loss: 1180.084
[26,     1] loss: 1119.456
[27,     1] loss: 1137.738
[28,     1] loss: 1111.332
[29,     1] loss: 1116.267
[30,     1] loss: 1130.704
[31,     1] loss: 1180.164
[32,     1] loss: 1082.588
[33,     1] loss: 1118.175
[34,     1] loss: 1044.740
[35,     1] loss: 1084.914
[36,     1] loss: 1082.477
[37,     1] loss: 1093.950
[38,     1] loss: 1066.310
[39,     1] loss: 1076.810
[40,     1] loss: 1087.444
[41,     1] loss: 1084.374
[42,     1] loss: 1075.599
[43,     1] loss: 1091.018
[44,     1] loss: 1035.453
[45,     1] loss: 1044.851
[46,     1] loss: 1049.521
[47,     1] loss: 1072.015
[48,     1] loss: 1086.749
[49,     1] loss: 1074.781
[50,     1] loss: 1050.192
[51,     1] loss: 1075.701
[52,     1] loss: 1043.333
[53,     1] loss: 1021.578
[54,     1] loss: 1021.565
[55,     1] loss: 1079.738
[56,     1] loss: 1016.152
[57,     1] loss: 1033.173
[58,     1] loss: 1031.221
[59,     1] loss: 1002.771
[60,     1] loss: 941.218
[61,     1] loss: 971.610
[62,     1] loss: 1039.720
[63,     1] loss: 1015.622
[64,     1] loss: 979.232
[65,     1] loss: 1055.879
[66,     1] loss: 997.855
[67,     1] loss: 1049.563
[68,     1] loss: 1005.192
[69,     1] loss: 1015.097
[70,     1] loss: 1013.980
[71,     1] loss: 995.975
[72,     1] loss: 1038.761
[73,     1] loss: 1056.511
[74,     1] loss: 975.810
[75,     1] loss: 965.508
[76,     1] loss: 1019.965
[77,     1] loss: 962.210
[78,     1] loss: 955.928
[79,     1] loss: 939.021
[80,     1] loss: 992.900
[81,     1] loss: 944.769
[82,     1] loss: 957.630
[83,     1] loss: 975.467
[84,     1] loss: 947.650
[85,     1] loss: 968.651
[86,     1] loss: 934.377
[87,     1] loss: 969.333
[88,     1] loss: 974.949
[89,     1] loss: 976.211
[90,     1] loss: 928.071
[91,     1] loss: 948.699
[92,     1] loss: 891.064
[93,     1] loss: 928.621
[94,     1] loss: 948.265
[95,     1] loss: 969.207
[96,     1] loss: 951.324
[97,     1] loss: 914.290
[98,     1] loss: 945.870
[99,     1] loss: 936.995
[100,     1] loss: 930.503
[101,     1] loss: 918.568
[102,     1] loss: 943.082
[103,     1] loss: 892.069
[104,     1] loss: 971.253
[105,     1] loss: 956.783
[106,     1] loss: 927.508
[107,     1] loss: 910.494
[108,     1] loss: 921.717
[109,     1] loss: 890.085
[110,     1] loss: 926.123
[111,     1] loss: 925.815
[112,     1] loss: 883.833
[113,     1] loss: 866.694
[114,     1] loss: 938.495
[115,     1] loss: 925.810
[116,     1] loss: 858.478
[117,     1] loss: 885.311
[118,     1] loss: 891.897
[119,     1] loss: 884.794
[120,     1] loss: 865.828
[121,     1] loss: 916.268
[122,     1] loss: 843.876
[123,     1] loss: 861.182
[124,     1] loss: 887.652
[125,     1] loss: 871.333
[126,     1] loss: 851.053
Early stopping applied (best metric=0.3461342751979828)
Finished Training
Total time taken: 17.788021326065063
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1396.021
[2,     1] loss: 1394.252
[3,     1] loss: 1392.180
[4,     1] loss: 1398.582
[5,     1] loss: 1394.494
[6,     1] loss: 1395.925
[7,     1] loss: 1390.549
[8,     1] loss: 1389.547
[9,     1] loss: 1382.990
[10,     1] loss: 1391.357
[11,     1] loss: 1370.816
[12,     1] loss: 1360.581
[13,     1] loss: 1336.289
[14,     1] loss: 1326.593
[15,     1] loss: 1270.734
[16,     1] loss: 1250.294
[17,     1] loss: 1237.929
[18,     1] loss: 1210.949
[19,     1] loss: 1149.451
[20,     1] loss: 1169.429
[21,     1] loss: 1134.719
[22,     1] loss: 1124.532
[23,     1] loss: 1122.508
[24,     1] loss: 1098.461
[25,     1] loss: 1096.609
[26,     1] loss: 1137.315
[27,     1] loss: 1064.194
[28,     1] loss: 1104.874
[29,     1] loss: 1144.251
[30,     1] loss: 1078.298
[31,     1] loss: 1097.749
[32,     1] loss: 1083.407
[33,     1] loss: 1095.698
[34,     1] loss: 1104.746
[35,     1] loss: 1053.473
[36,     1] loss: 1032.309
[37,     1] loss: 1070.932
[38,     1] loss: 1060.025
[39,     1] loss: 1079.944
[40,     1] loss: 1024.636
[41,     1] loss: 1052.302
[42,     1] loss: 1060.005
[43,     1] loss: 1056.665
[44,     1] loss: 1068.195
[45,     1] loss: 1019.899
[46,     1] loss: 1017.502
[47,     1] loss: 1004.751
[48,     1] loss: 974.578
[49,     1] loss: 1007.452
[50,     1] loss: 977.721
[51,     1] loss: 995.811
[52,     1] loss: 983.140
[53,     1] loss: 1026.759
[54,     1] loss: 1032.709
[55,     1] loss: 1062.775
[56,     1] loss: 1043.489
[57,     1] loss: 1043.128
[58,     1] loss: 993.136
[59,     1] loss: 1013.449
[60,     1] loss: 1000.973
[61,     1] loss: 992.162
[62,     1] loss: 1036.319
[63,     1] loss: 1009.049
[64,     1] loss: 1031.982
[65,     1] loss: 976.337
[66,     1] loss: 1006.287
[67,     1] loss: 962.428
[68,     1] loss: 1005.769
[69,     1] loss: 921.395
[70,     1] loss: 956.791
[71,     1] loss: 974.674
[72,     1] loss: 920.026
[73,     1] loss: 946.578
[74,     1] loss: 939.644
[75,     1] loss: 952.505
[76,     1] loss: 962.305
[77,     1] loss: 945.216
[78,     1] loss: 966.147
[79,     1] loss: 955.234
[80,     1] loss: 946.890
[81,     1] loss: 930.540
[82,     1] loss: 954.234
[83,     1] loss: 886.437
[84,     1] loss: 882.966
[85,     1] loss: 865.236
[86,     1] loss: 939.231
[87,     1] loss: 957.568
[88,     1] loss: 940.365
[89,     1] loss: 903.357
[90,     1] loss: 910.156
[91,     1] loss: 913.728
[92,     1] loss: 927.710
[93,     1] loss: 902.919
[94,     1] loss: 896.437
[95,     1] loss: 899.965
[96,     1] loss: 903.953
[97,     1] loss: 988.712
[98,     1] loss: 879.013
[99,     1] loss: 899.899
Early stopping applied (best metric=0.37808874249458313)
Finished Training
Total time taken: 14.102020502090454
{'Hydroxylation-K Validation Accuracy': 0.774468085106383, 'Hydroxylation-K Validation Sensitivity': 0.6296296296296297, 'Hydroxylation-K Validation Specificity': 0.8105263157894737, 'Hydroxylation-K Validation Precision': 0.46853774708263873, 'Hydroxylation-K AUC ROC': 0.8101169590643275, 'Hydroxylation-K AUC PR': 0.5948910170897098, 'Hydroxylation-K MCC': 0.40045588142068583, 'Hydroxylation-K F1': 0.5305776475497931, 'Validation Loss (Hydroxylation-K)': 0.45964617729187013, 'Hydroxylation-P Validation Accuracy': 0.8033429267549871, 'Hydroxylation-P Validation Sensitivity': 0.7558201058201058, 'Hydroxylation-P Validation Specificity': 0.8135742431043942, 'Hydroxylation-P Validation Precision': 0.47156562716063305, 'Hydroxylation-P AUC ROC': 0.8434535902910884, 'Hydroxylation-P AUC PR': 0.5736501021786601, 'Hydroxylation-P MCC': 0.48350057026640597, 'Hydroxylation-P F1': 0.5783798830676454, 'Validation Loss (Hydroxylation-P)': 0.3704227089881897, 'Validation Loss (total)': 0.8300688942273458, 'TimeToTrain': 17.242623631159464}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0053670958629597485,
 'learning_rate_Hydroxylation-K': 0.0031233879265189744,
 'learning_rate_Hydroxylation-P': 0.002232331065283185,
 'log_base': 2.8892149682402337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2416600137,
 'sample_weights': [2.3006518507861418, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.362497996669975,
 'weight_decay_Hydroxylation-K': 7.168226520549792,
 'weight_decay_Hydroxylation-P': 0.20043945971095223}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.479
[2,     1] loss: 1241.770
[3,     1] loss: 1241.494
[4,     1] loss: 1241.937
[5,     1] loss: 1242.146
[6,     1] loss: 1237.010
[7,     1] loss: 1237.934
[8,     1] loss: 1238.071
[9,     1] loss: 1232.793
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005442090499961779,
 'learning_rate_Hydroxylation-K': 0.009423358184228882,
 'learning_rate_Hydroxylation-P': 0.007548201980063512,
 'log_base': 1.2720894482404015,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 65273293,
 'sample_weights': [1.5734844678877828, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.51870690177729,
 'weight_decay_Hydroxylation-K': 5.995433597740891,
 'weight_decay_Hydroxylation-P': 6.472820100355576}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2374.073
[2,     1] loss: 2376.111
[3,     1] loss: 2370.502
[4,     1] loss: 2368.866
[5,     1] loss: 2369.085
[6,     1] loss: 2362.360
[7,     1] loss: 2355.622
[8,     1] loss: 2348.065
[9,     1] loss: 2330.317
[10,     1] loss: 2305.474
[11,     1] loss: 2256.343
[12,     1] loss: 2180.762
[13,     1] loss: 2159.087
[14,     1] loss: 2054.753
[15,     1] loss: 2096.346
[16,     1] loss: 2031.539
[17,     1] loss: 1935.409
[18,     1] loss: 1971.382
[19,     1] loss: 2020.225
[20,     1] loss: 2024.448
[21,     1] loss: 1829.296
[22,     1] loss: 1889.129
[23,     1] loss: 1905.668
[24,     1] loss: 1965.288
[25,     1] loss: 1798.104
[26,     1] loss: 1877.356
[27,     1] loss: 1788.464
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0044870810723213524,
 'learning_rate_Hydroxylation-K': 0.009228602876862124,
 'learning_rate_Hydroxylation-P': 0.006968918449836879,
 'log_base': 1.5231593730250537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2675374297,
 'sample_weights': [6.93691395794862, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.902655352883141,
 'weight_decay_Hydroxylation-K': 4.557831258749915,
 'weight_decay_Hydroxylation-P': 2.665408068456318}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1751.877
[2,     1] loss: 1748.402
[3,     1] loss: 1749.011
[4,     1] loss: 1736.965
[5,     1] loss: 1760.808
[6,     1] loss: 1747.021
[7,     1] loss: 1749.115
[8,     1] loss: 1741.774
[9,     1] loss: 1745.581
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005761177239653763,
 'learning_rate_Hydroxylation-K': 0.009986831432406348,
 'learning_rate_Hydroxylation-P': 0.006852961281003387,
 'log_base': 2.1720850829643927,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3903385472,
 'sample_weights': [3.96743313778906, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.060833125349486,
 'weight_decay_Hydroxylation-K': 8.180986797177416,
 'weight_decay_Hydroxylation-P': 2.5971523826899863}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1364.096
[2,     1] loss: 1364.020
[3,     1] loss: 1359.012
[4,     1] loss: 1359.364
[5,     1] loss: 1353.776
[6,     1] loss: 1357.072
[7,     1] loss: 1347.501
[8,     1] loss: 1330.318
[9,     1] loss: 1310.259
[10,     1] loss: 1251.248
[11,     1] loss: 1221.003
[12,     1] loss: 1172.935
[13,     1] loss: 1129.886
[14,     1] loss: 1162.634
[15,     1] loss: 1100.840
[16,     1] loss: 1089.973
[17,     1] loss: 1059.722
[18,     1] loss: 1077.035
[19,     1] loss: 1084.538
[20,     1] loss: 1123.841
[21,     1] loss: 1096.440
[22,     1] loss: 1056.117
[23,     1] loss: 1063.943
[24,     1] loss: 1075.296
[25,     1] loss: 1078.774
[26,     1] loss: 1063.915
[27,     1] loss: 1036.619
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009228447139694612,
 'learning_rate_Hydroxylation-K': 0.0018310568681106521,
 'learning_rate_Hydroxylation-P': 0.007391100800069761,
 'log_base': 1.2915943816150377,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4285338477,
 'sample_weights': [2.1522107656616942, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.562625713002088,
 'weight_decay_Hydroxylation-K': 4.779137108239289,
 'weight_decay_Hydroxylation-P': 0.732040504465465}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2270.624
[2,     1] loss: 2265.273
[3,     1] loss: 2286.311
[4,     1] loss: 2303.795
[5,     1] loss: 2299.410
[6,     1] loss: 2281.020
[7,     1] loss: 2285.237
[8,     1] loss: 2282.630
[9,     1] loss: 2294.888
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006303825353327336,
 'learning_rate_Hydroxylation-K': 0.004654627004460178,
 'learning_rate_Hydroxylation-P': 0.006776280249605069,
 'log_base': 2.5054063984754884,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1532594154,
 'sample_weights': [6.524386610548601, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.801969307007619,
 'weight_decay_Hydroxylation-K': 9.16448578971259,
 'weight_decay_Hydroxylation-P': 7.828043764967292}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1290.892
[2,     1] loss: 1298.145
[3,     1] loss: 1289.362
[4,     1] loss: 1288.921
[5,     1] loss: 1286.331
[6,     1] loss: 1283.293
[7,     1] loss: 1269.154
[8,     1] loss: 1246.581
[9,     1] loss: 1208.976
[10,     1] loss: 1183.635
[11,     1] loss: 1130.278
[12,     1] loss: 1100.351
[13,     1] loss: 1160.850
[14,     1] loss: 1061.549
[15,     1] loss: 1094.119
[16,     1] loss: 1057.666
[17,     1] loss: 1056.859
[18,     1] loss: 1024.937
[19,     1] loss: 1037.970
[20,     1] loss: 977.831
[21,     1] loss: 1031.999
[22,     1] loss: 1057.972
[23,     1] loss: 990.012
[24,     1] loss: 1026.113
[25,     1] loss: 1015.829
[26,     1] loss: 991.282
[27,     1] loss: 987.078
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004792619435846183,
 'learning_rate_Hydroxylation-K': 0.00955878863395214,
 'learning_rate_Hydroxylation-P': 0.008963566282474474,
 'log_base': 1.1113575002259388,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2010935548,
 'sample_weights': [1.8176726105787389, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.915984179902821,
 'weight_decay_Hydroxylation-K': 6.984512466362437,
 'weight_decay_Hydroxylation-P': 1.165199103362752}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5138.639
[2,     1] loss: 5201.799
[3,     1] loss: 5113.156
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00219167826602414,
 'learning_rate_Hydroxylation-K': 0.006558943965509631,
 'learning_rate_Hydroxylation-P': 0.007301350874904619,
 'log_base': 1.0667722936131634,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3278461698,
 'sample_weights': [15.8117797767181, 1.9765486137461674],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.737664362792622,
 'weight_decay_Hydroxylation-K': 6.118377933829868,
 'weight_decay_Hydroxylation-P': 4.920286015424319}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8447.533
[2,     1] loss: 8383.283
[3,     1] loss: 8366.355
[4,     1] loss: 8424.916
[5,     1] loss: 8377.555
[6,     1] loss: 8369.934
[7,     1] loss: 8353.459
[8,     1] loss: 8360.708
[9,     1] loss: 8391.781
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001264470430833454,
 'learning_rate_Hydroxylation-K': 0.005218754996350559,
 'learning_rate_Hydroxylation-P': 0.0031530478407074437,
 'log_base': 2.441973961705427,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1232605784,
 'sample_weights': [25.82776369201954, 3.2285948351489595],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.6159113245145775,
 'weight_decay_Hydroxylation-K': 5.377179984562549,
 'weight_decay_Hydroxylation-P': 2.4688826196938205}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1304.844
[2,     1] loss: 1299.817
[3,     1] loss: 1305.689
[4,     1] loss: 1303.473
[5,     1] loss: 1299.682
[6,     1] loss: 1302.002
[7,     1] loss: 1300.513
[8,     1] loss: 1300.965
[9,     1] loss: 1301.084
[10,     1] loss: 1299.468
[11,     1] loss: 1297.627
[12,     1] loss: 1298.553
[13,     1] loss: 1296.711
[14,     1] loss: 1292.107
[15,     1] loss: 1292.463
[16,     1] loss: 1285.289
[17,     1] loss: 1278.004
[18,     1] loss: 1263.811
[19,     1] loss: 1257.035
[20,     1] loss: 1232.120
[21,     1] loss: 1214.587
[22,     1] loss: 1197.950
[23,     1] loss: 1183.306
[24,     1] loss: 1151.633
[25,     1] loss: 1116.738
[26,     1] loss: 1112.050
[27,     1] loss: 1109.329
[28,     1] loss: 1071.996
[29,     1] loss: 1120.090
[30,     1] loss: 1013.048
[31,     1] loss: 1029.206
[32,     1] loss: 1070.322
[33,     1] loss: 1050.538
[34,     1] loss: 981.459
[35,     1] loss: 1000.302
[36,     1] loss: 978.740
[37,     1] loss: 976.798
[38,     1] loss: 1021.210
[39,     1] loss: 1049.271
[40,     1] loss: 949.533
[41,     1] loss: 990.972
[42,     1] loss: 975.858
[43,     1] loss: 991.925
[44,     1] loss: 961.168
[45,     1] loss: 931.641
[46,     1] loss: 921.738
[47,     1] loss: 905.683
[48,     1] loss: 899.915
[49,     1] loss: 888.130
[50,     1] loss: 893.789
[51,     1] loss: 924.076
[52,     1] loss: 820.619
[53,     1] loss: 889.587
[54,     1] loss: 847.664
[55,     1] loss: 875.624
[56,     1] loss: 816.232
[57,     1] loss: 852.547
[58,     1] loss: 876.113
[59,     1] loss: 854.047
[60,     1] loss: 831.488
[61,     1] loss: 814.340
[62,     1] loss: 821.451
[63,     1] loss: 783.748
[64,     1] loss: 790.774
[65,     1] loss: 749.570
[66,     1] loss: 719.732
[67,     1] loss: 767.170
[68,     1] loss: 791.124
[69,     1] loss: 754.622
[70,     1] loss: 748.323
[71,     1] loss: 729.377
[72,     1] loss: 660.427
[73,     1] loss: 689.710
[74,     1] loss: 663.130
[75,     1] loss: 719.141
[76,     1] loss: 734.892
[77,     1] loss: 718.438
[78,     1] loss: 693.998
[79,     1] loss: 748.518
[80,     1] loss: 645.157
[81,     1] loss: 659.404
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004868430395950669,
 'learning_rate_Hydroxylation-K': 0.008728212785265876,
 'learning_rate_Hydroxylation-P': 0.00636787859614851,
 'log_base': 2.2066693825259867,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 204095176,
 'sample_weights': [1.869881938866014, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.175838790825388,
 'weight_decay_Hydroxylation-K': 5.713246087438028,
 'weight_decay_Hydroxylation-P': 1.636335379203099}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1361.776
[2,     1] loss: 1356.703
[3,     1] loss: 1354.373
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004732335366968172,
 'learning_rate_Hydroxylation-K': 0.005358358674671911,
 'learning_rate_Hydroxylation-P': 0.008013341054511217,
 'log_base': 1.0296355181980432,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1953993669,
 'sample_weights': [2.109256143229988, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.985671026740357,
 'weight_decay_Hydroxylation-K': 2.389795523846103,
 'weight_decay_Hydroxylation-P': 2.0133411688727145}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18568.213
Exploding loss, terminate run (best metric=0.5312169790267944)
Finished Training
Total time taken: 0.205000638961792
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18531.141
Exploding loss, terminate run (best metric=0.5274301767349243)
Finished Training
Total time taken: 0.21699833869934082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18584.754
Exploding loss, terminate run (best metric=0.5383222103118896)
Finished Training
Total time taken: 0.21000051498413086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18757.426
Exploding loss, terminate run (best metric=0.5430976152420044)
Finished Training
Total time taken: 0.20600199699401855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18628.148
Exploding loss, terminate run (best metric=0.5341386795043945)
Finished Training
Total time taken: 0.21300220489501953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18703.957
Exploding loss, terminate run (best metric=0.5389764904975891)
Finished Training
Total time taken: 0.19900059700012207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18588.445
Exploding loss, terminate run (best metric=0.5281667709350586)
Finished Training
Total time taken: 0.20299983024597168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18818.617
Exploding loss, terminate run (best metric=0.5265399217605591)
Finished Training
Total time taken: 0.20699572563171387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18683.340
Exploding loss, terminate run (best metric=0.5305396318435669)
Finished Training
Total time taken: 0.20399951934814453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18661.355
Exploding loss, terminate run (best metric=0.5287498235702515)
Finished Training
Total time taken: 0.20300006866455078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18557.576
Exploding loss, terminate run (best metric=0.5327072739601135)
Finished Training
Total time taken: 0.20799970626831055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18571.746
Exploding loss, terminate run (best metric=0.5324280858039856)
Finished Training
Total time taken: 0.21100234985351562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18638.922
Exploding loss, terminate run (best metric=0.5340924859046936)
Finished Training
Total time taken: 0.20999717712402344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18745.602
Exploding loss, terminate run (best metric=0.5350528359413147)
Finished Training
Total time taken: 0.2050013542175293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18556.273
Exploding loss, terminate run (best metric=0.5377158522605896)
Finished Training
Total time taken: 0.2140028476715088
{'Hydroxylation-K Validation Accuracy': 0.5961583924349881, 'Hydroxylation-K Validation Sensitivity': 0.35333333333333333, 'Hydroxylation-K Validation Specificity': 0.6543859649122807, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6173489278752436, 'Hydroxylation-K AUC PR': 0.3445813245114293, 'Hydroxylation-K MCC': 0.007719298245614035, 'Hydroxylation-K F1': 0.13338259441707717, 'Validation Loss (Hydroxylation-K)': 0.5574837883313497, 'Hydroxylation-P Validation Accuracy': 0.6030396088185033, 'Hydroxylation-P Validation Sensitivity': 0.3537037037037037, 'Hydroxylation-P Validation Specificity': 0.6565040650406504, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5725644373665472, 'Hydroxylation-P AUC PR': 0.28070445881195194, 'Hydroxylation-P MCC': 0.01020776874435411, 'Hydroxylation-P F1': 0.12056702555356809, 'Validation Loss (Hydroxylation-P)': 0.5332783222198486, 'Validation Loss (total)': 1.0907621065775552, 'TimeToTrain': 0.20766685803731283}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023265725714255794,
 'learning_rate_Hydroxylation-K': 0.003025797922540206,
 'learning_rate_Hydroxylation-P': 0.008643609798832129,
 'log_base': 1.010616478188669,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1024332426,
 'sample_weights': [57.205572937536964, 7.135838733008838],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.209945224066244,
 'weight_decay_Hydroxylation-K': 2.031684250913573,
 'weight_decay_Hydroxylation-P': 1.1820642798162446}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51195.160
Exploding loss, terminate run (best metric=0.534094512462616)
Finished Training
Total time taken: 0.20399951934814453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51487.664
Exploding loss, terminate run (best metric=0.5285586714744568)
Finished Training
Total time taken: 0.21300029754638672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51379.078
Exploding loss, terminate run (best metric=0.5296634435653687)
Finished Training
Total time taken: 0.2310009002685547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51362.570
Exploding loss, terminate run (best metric=0.5285918116569519)
Finished Training
Total time taken: 0.19999980926513672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 51278.668
Exploding loss, terminate run (best metric=0.5284795165061951)
Finished Training
Total time taken: 0.21999764442443848
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51362.812
Exploding loss, terminate run (best metric=0.5348281264305115)
Finished Training
Total time taken: 0.1990032196044922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51160.762
Exploding loss, terminate run (best metric=0.5265591144561768)
Finished Training
Total time taken: 0.20199847221374512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51340.746
Exploding loss, terminate run (best metric=0.5274766087532043)
Finished Training
Total time taken: 0.1940011978149414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51554.359
Exploding loss, terminate run (best metric=0.5283774137496948)
Finished Training
Total time taken: 0.21900010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 51208.805
Exploding loss, terminate run (best metric=0.5306621789932251)
Finished Training
Total time taken: 0.2010021209716797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51482.164
Exploding loss, terminate run (best metric=0.5317897796630859)
Finished Training
Total time taken: 0.21899795532226562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51326.750
Exploding loss, terminate run (best metric=0.5269139409065247)
Finished Training
Total time taken: 0.2070004940032959
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51685.820
Exploding loss, terminate run (best metric=0.5306561589241028)
Finished Training
Total time taken: 0.20599818229675293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51696.758
Exploding loss, terminate run (best metric=0.5269420146942139)
Finished Training
Total time taken: 0.21600008010864258
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 51208.922
Exploding loss, terminate run (best metric=0.5296370983123779)
Finished Training
Total time taken: 0.2239973545074463
{'Hydroxylation-K Validation Accuracy': 0.42523640661938533, 'Hydroxylation-K Validation Sensitivity': 0.6444444444444445, 'Hydroxylation-K Validation Specificity': 0.3719298245614035, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6532943469785575, 'Hydroxylation-K AUC PR': 0.3594190652198945, 'Hydroxylation-K MCC': 0.01291200332550006, 'Hydroxylation-K F1': 0.22646326606282113, 'Validation Loss (Hydroxylation-K)': 0.5561341087023417, 'Hydroxylation-P Validation Accuracy': 0.42357010642437776, 'Hydroxylation-P Validation Sensitivity': 0.6457142857142857, 'Hydroxylation-P Validation Specificity': 0.37642276422764226, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5926487714227128, 'Hydroxylation-P AUC PR': 0.2721952456998511, 'Hydroxylation-P MCC': 0.01712284139722497, 'Hydroxylation-P F1': 0.20740475466597533, 'Validation Loss (Hydroxylation-P)': 0.5295486927032471, 'Validation Loss (total)': 1.0856828292210896, 'TimeToTrain': 0.21033315658569335}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038388667948674815,
 'learning_rate_Hydroxylation-K': 0.00476423945101803,
 'learning_rate_Hydroxylation-P': 0.007048045771008569,
 'log_base': 1.0785908628529899,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1585312371,
 'sample_weights': [158.20070232311085, 19.733998651828927],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.819111194133741,
 'weight_decay_Hydroxylation-K': 2.849083036883534,
 'weight_decay_Hydroxylation-P': 1.9222203242555838}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7172.185
[2,     1] loss: 7181.486
[3,     1] loss: 7106.470
[4,     1] loss: 7218.391
[5,     1] loss: 7165.344
[6,     1] loss: 7153.162
[7,     1] loss: 7151.487
[8,     1] loss: 7174.101
[9,     1] loss: 7182.574
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004519395580506035,
 'learning_rate_Hydroxylation-K': 0.009694253225809303,
 'learning_rate_Hydroxylation-P': 0.009631833842136816,
 'log_base': 1.8591352455383132,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3535950852,
 'sample_weights': [22.06640145244754, 2.758406442355246],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.687422236361492,
 'weight_decay_Hydroxylation-K': 7.075408450595703,
 'weight_decay_Hydroxylation-P': 2.4234244320058136}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1482.940
[2,     1] loss: 1476.788
[3,     1] loss: 1474.927
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016983322107709791,
 'learning_rate_Hydroxylation-K': 0.008718649569770944,
 'learning_rate_Hydroxylation-P': 0.0003887176711736724,
 'log_base': 1.7925925486465206,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3133164588,
 'sample_weights': [2.6921662650450435, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.438919098608846,
 'weight_decay_Hydroxylation-K': 8.792774297943174,
 'weight_decay_Hydroxylation-P': 4.289514288529263}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1528.309
[2,     1] loss: 1514.092
[3,     1] loss: 1508.106
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002689500379238102,
 'learning_rate_Hydroxylation-K': 0.002196814838316543,
 'learning_rate_Hydroxylation-P': 0.009471782958591095,
 'log_base': 1.1738681943869431,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3324574537,
 'sample_weights': [2.8602864445500518, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.014126744408047,
 'weight_decay_Hydroxylation-K': 1.725109999789919,
 'weight_decay_Hydroxylation-P': 1.9228917853027856}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3400.400
[2,     1] loss: 3379.967
[3,     1] loss: 3381.621
[4,     1] loss: 3372.260
[5,     1] loss: 3365.916
[6,     1] loss: 3401.719
[7,     1] loss: 3359.162
[8,     1] loss: 3353.069
[9,     1] loss: 3357.148
[10,     1] loss: 3360.533
[11,     1] loss: 3368.236
[12,     1] loss: 3326.201
[13,     1] loss: 3311.175
[14,     1] loss: 3285.996
[15,     1] loss: 3212.638
[16,     1] loss: 3112.147
[17,     1] loss: 3109.192
[18,     1] loss: 3026.207
[19,     1] loss: 2868.221
[20,     1] loss: 2986.368
[21,     1] loss: 2773.689
[22,     1] loss: 2831.283
[23,     1] loss: 3096.137
[24,     1] loss: 2797.159
[25,     1] loss: 2640.094
[26,     1] loss: 2660.428
[27,     1] loss: 2648.037
[28,     1] loss: 2741.224
[29,     1] loss: 2785.067
[30,     1] loss: 2777.378
[31,     1] loss: 2671.796
[32,     1] loss: 2656.416
[33,     1] loss: 2738.576
[34,     1] loss: 2415.325
[35,     1] loss: 2509.065
[36,     1] loss: 2357.387
[37,     1] loss: 2507.665
[38,     1] loss: 2223.707
[39,     1] loss: 2416.183
[40,     1] loss: 2304.928
[41,     1] loss: 2243.269
[42,     1] loss: 2313.787
[43,     1] loss: 2172.652
[44,     1] loss: 2341.273
[45,     1] loss: 2324.678
[46,     1] loss: 2191.396
[47,     1] loss: 2036.723
[48,     1] loss: 2156.427
[49,     1] loss: 2146.515
[50,     1] loss: 1880.550
[51,     1] loss: 1988.537
[52,     1] loss: 1982.325
[53,     1] loss: 1842.123
[54,     1] loss: 1954.301
[55,     1] loss: 1947.300
[56,     1] loss: 1798.351
[57,     1] loss: 2356.077
[58,     1] loss: 1899.590
[59,     1] loss: 1807.433
[60,     1] loss: 1792.228
[61,     1] loss: 1963.143
[62,     1] loss: 1783.485
[63,     1] loss: 1809.421
[64,     1] loss: 1851.319
[65,     1] loss: 1816.366
[66,     1] loss: 1863.971
[67,     1] loss: 1743.046
[68,     1] loss: 1841.191
[69,     1] loss: 1612.966
[70,     1] loss: 1661.900
[71,     1] loss: 1737.413
[72,     1] loss: 1741.320
[73,     1] loss: 1639.743
[74,     1] loss: 1426.448
[75,     1] loss: 1677.766
[76,     1] loss: 1594.884
[77,     1] loss: 1543.355
[78,     1] loss: 1764.827
[79,     1] loss: 1463.086
[80,     1] loss: 1589.211
[81,     1] loss: 1523.853
[82,     1] loss: 2165.176
[83,     1] loss: 1647.539
[84,     1] loss: 1851.815
[85,     1] loss: 1663.375
[86,     1] loss: 1808.870
[87,     1] loss: 1424.826
[88,     1] loss: 2067.729
[89,     1] loss: 1575.870
[90,     1] loss: 1700.375
[91,     1] loss: 2074.296
[92,     1] loss: 1619.158
[93,     1] loss: 1909.293
[94,     1] loss: 1803.572
[95,     1] loss: 1530.239
[96,     1] loss: 1848.726
[97,     1] loss: 1785.464
[98,     1] loss: 1562.364
[99,     1] loss: 1936.502
[100,     1] loss: 1342.081
[101,     1] loss: 1619.795
[102,     1] loss: 1783.695
[103,     1] loss: 1321.846
[104,     1] loss: 1748.952
[105,     1] loss: 1455.490
Early stopping applied (best metric=0.31112661957740784)
Finished Training
Total time taken: 14.893020629882812
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3385.669
[2,     1] loss: 3367.949
[3,     1] loss: 3403.884
[4,     1] loss: 3371.732
[5,     1] loss: 3384.053
[6,     1] loss: 3368.762
[7,     1] loss: 3367.766
[8,     1] loss: 3360.363
[9,     1] loss: 3369.187
[10,     1] loss: 3365.591
[11,     1] loss: 3370.472
[12,     1] loss: 3346.073
[13,     1] loss: 3349.300
[14,     1] loss: 3324.257
[15,     1] loss: 3287.374
[16,     1] loss: 3252.253
[17,     1] loss: 3219.235
[18,     1] loss: 3153.785
[19,     1] loss: 3120.580
[20,     1] loss: 2998.964
[21,     1] loss: 2892.577
[22,     1] loss: 2879.495
[23,     1] loss: 2901.493
[24,     1] loss: 2773.800
[25,     1] loss: 2678.260
[26,     1] loss: 2693.561
[27,     1] loss: 2727.900
[28,     1] loss: 2744.107
[29,     1] loss: 2656.964
[30,     1] loss: 2693.328
[31,     1] loss: 2610.067
[32,     1] loss: 2441.795
[33,     1] loss: 2364.292
[34,     1] loss: 2617.868
[35,     1] loss: 2424.469
[36,     1] loss: 2605.621
[37,     1] loss: 2457.213
[38,     1] loss: 2441.339
[39,     1] loss: 2262.546
[40,     1] loss: 2458.699
[41,     1] loss: 2143.996
[42,     1] loss: 2379.165
[43,     1] loss: 2092.922
[44,     1] loss: 2229.266
[45,     1] loss: 2106.091
[46,     1] loss: 2301.726
[47,     1] loss: 2239.707
[48,     1] loss: 1828.771
[49,     1] loss: 2286.139
[50,     1] loss: 1815.646
[51,     1] loss: 2135.803
[52,     1] loss: 1810.093
[53,     1] loss: 1984.552
[54,     1] loss: 1921.667
[55,     1] loss: 1948.431
[56,     1] loss: 1859.813
[57,     1] loss: 1904.338
[58,     1] loss: 1869.103
[59,     1] loss: 1970.124
[60,     1] loss: 1690.366
[61,     1] loss: 1808.546
[62,     1] loss: 1604.953
[63,     1] loss: 1865.334
[64,     1] loss: 1839.742
[65,     1] loss: 1650.424
[66,     1] loss: 1629.445
[67,     1] loss: 1678.541
[68,     1] loss: 1578.159
[69,     1] loss: 1535.105
[70,     1] loss: 1564.327
[71,     1] loss: 1769.349
[72,     1] loss: 1503.092
[73,     1] loss: 1565.771
[74,     1] loss: 1589.437
[75,     1] loss: 1787.872
[76,     1] loss: 1548.552
[77,     1] loss: 1523.708
[78,     1] loss: 1392.662
[79,     1] loss: 1527.699
[80,     1] loss: 1502.015
[81,     1] loss: 1601.280
[82,     1] loss: 1729.421
[83,     1] loss: 1466.497
[84,     1] loss: 1547.417
[85,     1] loss: 1557.244
[86,     1] loss: 1708.195
[87,     1] loss: 2004.908
[88,     1] loss: 1435.812
[89,     1] loss: 1502.688
[90,     1] loss: 1747.928
[91,     1] loss: 1461.127
Early stopping applied (best metric=0.4732131063938141)
Finished Training
Total time taken: 13.52605414390564
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3380.427
[2,     1] loss: 3378.720
[3,     1] loss: 3397.580
[4,     1] loss: 3382.507
[5,     1] loss: 3370.933
[6,     1] loss: 3374.836
[7,     1] loss: 3374.331
[8,     1] loss: 3366.965
[9,     1] loss: 3379.057
[10,     1] loss: 3360.648
[11,     1] loss: 3374.757
[12,     1] loss: 3327.377
[13,     1] loss: 3319.629
[14,     1] loss: 3290.501
[15,     1] loss: 3264.480
[16,     1] loss: 3206.712
[17,     1] loss: 3216.850
[18,     1] loss: 3137.868
[19,     1] loss: 3032.774
[20,     1] loss: 3081.695
[21,     1] loss: 3058.134
[22,     1] loss: 2962.971
[23,     1] loss: 2892.505
[24,     1] loss: 2895.499
[25,     1] loss: 2893.627
[26,     1] loss: 2893.145
[27,     1] loss: 3005.737
[28,     1] loss: 2854.498
[29,     1] loss: 2931.910
[30,     1] loss: 2715.627
[31,     1] loss: 2809.504
[32,     1] loss: 2896.190
[33,     1] loss: 2652.076
[34,     1] loss: 2697.063
[35,     1] loss: 2721.595
[36,     1] loss: 2679.029
[37,     1] loss: 2774.953
[38,     1] loss: 2665.054
[39,     1] loss: 2500.621
[40,     1] loss: 2487.633
[41,     1] loss: 3058.660
[42,     1] loss: 2536.780
[43,     1] loss: 2666.302
[44,     1] loss: 2618.418
[45,     1] loss: 2417.439
[46,     1] loss: 2415.223
[47,     1] loss: 2486.626
[48,     1] loss: 2360.346
[49,     1] loss: 2436.182
[50,     1] loss: 2373.408
[51,     1] loss: 2383.975
[52,     1] loss: 2259.922
[53,     1] loss: 2311.096
[54,     1] loss: 2255.196
[55,     1] loss: 1987.988
[56,     1] loss: 2210.028
[57,     1] loss: 2051.685
[58,     1] loss: 2073.429
[59,     1] loss: 2045.690
[60,     1] loss: 1790.846
[61,     1] loss: 1945.860
[62,     1] loss: 1937.512
[63,     1] loss: 1837.094
[64,     1] loss: 2102.082
[65,     1] loss: 1858.331
[66,     1] loss: 1754.554
[67,     1] loss: 2023.832
[68,     1] loss: 1893.988
[69,     1] loss: 1923.728
[70,     1] loss: 1833.094
[71,     1] loss: 1871.324
[72,     1] loss: 1658.019
[73,     1] loss: 1940.444
[74,     1] loss: 1931.960
[75,     1] loss: 1750.039
[76,     1] loss: 2047.457
[77,     1] loss: 1655.727
[78,     1] loss: 1931.253
[79,     1] loss: 1766.217
[80,     1] loss: 1943.517
[81,     1] loss: 1594.411
[82,     1] loss: 2151.365
[83,     1] loss: 1667.198
[84,     1] loss: 2000.357
[85,     1] loss: 1664.886
[86,     1] loss: 1761.344
[87,     1] loss: 1493.497
[88,     1] loss: 1655.021
[89,     1] loss: 1715.074
[90,     1] loss: 1598.642
[91,     1] loss: 2145.062
[92,     1] loss: 1650.033
[93,     1] loss: 2157.442
[94,     1] loss: 1590.488
[95,     1] loss: 1802.814
[96,     1] loss: 1632.860
[97,     1] loss: 1476.443
[98,     1] loss: 1597.119
Early stopping applied (best metric=0.4427777826786041)
Finished Training
Total time taken: 14.192084074020386
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3388.527
[2,     1] loss: 3385.835
[3,     1] loss: 3369.956
[4,     1] loss: 3370.399
[5,     1] loss: 3375.050
[6,     1] loss: 3370.840
[7,     1] loss: 3381.315
[8,     1] loss: 3365.977
[9,     1] loss: 3372.329
[10,     1] loss: 3372.831
[11,     1] loss: 3349.703
[12,     1] loss: 3346.668
[13,     1] loss: 3334.624
[14,     1] loss: 3293.398
[15,     1] loss: 3263.057
[16,     1] loss: 3190.118
[17,     1] loss: 3145.450
[18,     1] loss: 3037.171
[19,     1] loss: 3077.557
[20,     1] loss: 2994.540
[21,     1] loss: 2973.800
[22,     1] loss: 2818.895
[23,     1] loss: 2908.411
[24,     1] loss: 2929.605
[25,     1] loss: 2821.274
[26,     1] loss: 2686.256
[27,     1] loss: 2788.844
[28,     1] loss: 2589.292
[29,     1] loss: 2755.590
[30,     1] loss: 2507.717
[31,     1] loss: 2572.386
[32,     1] loss: 2641.314
[33,     1] loss: 2546.069
[34,     1] loss: 2590.827
[35,     1] loss: 2436.508
[36,     1] loss: 2367.528
[37,     1] loss: 2283.424
[38,     1] loss: 2236.016
[39,     1] loss: 2366.451
[40,     1] loss: 2149.331
[41,     1] loss: 2303.641
[42,     1] loss: 2131.642
[43,     1] loss: 2152.377
[44,     1] loss: 2272.212
[45,     1] loss: 2060.948
[46,     1] loss: 2174.831
[47,     1] loss: 2102.298
[48,     1] loss: 1975.116
[49,     1] loss: 1995.262
[50,     1] loss: 1870.565
[51,     1] loss: 1952.916
[52,     1] loss: 1958.776
[53,     1] loss: 1774.786
[54,     1] loss: 1927.351
[55,     1] loss: 2004.787
[56,     1] loss: 1917.705
[57,     1] loss: 1912.675
[58,     1] loss: 1738.588
[59,     1] loss: 2170.368
[60,     1] loss: 1903.788
[61,     1] loss: 1811.143
[62,     1] loss: 1569.413
[63,     1] loss: 1689.834
[64,     1] loss: 1686.071
[65,     1] loss: 1663.526
[66,     1] loss: 1573.090
[67,     1] loss: 1659.891
[68,     1] loss: 1393.427
[69,     1] loss: 1596.410
[70,     1] loss: 1573.844
[71,     1] loss: 1470.855
[72,     1] loss: 1541.997
[73,     1] loss: 1536.656
[74,     1] loss: 1455.617
[75,     1] loss: 1749.036
[76,     1] loss: 2254.967
[77,     1] loss: 1429.205
[78,     1] loss: 1775.744
[79,     1] loss: 1437.683
[80,     1] loss: 1511.889
[81,     1] loss: 1538.080
[82,     1] loss: 1577.305
[83,     1] loss: 1418.695
[84,     1] loss: 1488.857
[85,     1] loss: 1416.176
[86,     1] loss: 1352.504
Early stopping applied (best metric=0.43426769971847534)
Finished Training
Total time taken: 12.819051027297974
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3378.041
[2,     1] loss: 3375.171
[3,     1] loss: 3386.969
[4,     1] loss: 3372.485
[5,     1] loss: 3383.309
[6,     1] loss: 3376.784
[7,     1] loss: 3362.632
[8,     1] loss: 3355.390
[9,     1] loss: 3362.657
[10,     1] loss: 3350.673
[11,     1] loss: 3318.765
[12,     1] loss: 3319.439
[13,     1] loss: 3290.047
[14,     1] loss: 3233.464
[15,     1] loss: 3167.991
[16,     1] loss: 3175.076
[17,     1] loss: 3085.874
[18,     1] loss: 3027.197
[19,     1] loss: 2919.038
[20,     1] loss: 2891.918
[21,     1] loss: 2857.158
[22,     1] loss: 2800.015
[23,     1] loss: 2877.885
[24,     1] loss: 2821.360
[25,     1] loss: 2895.090
[26,     1] loss: 2656.694
[27,     1] loss: 2564.076
[28,     1] loss: 2557.726
[29,     1] loss: 2525.781
[30,     1] loss: 2561.429
[31,     1] loss: 2554.677
[32,     1] loss: 2478.858
[33,     1] loss: 2501.634
[34,     1] loss: 2538.554
[35,     1] loss: 2298.093
[36,     1] loss: 2379.619
[37,     1] loss: 2294.585
[38,     1] loss: 2415.485
[39,     1] loss: 2209.552
[40,     1] loss: 2041.059
[41,     1] loss: 2357.658
[42,     1] loss: 2265.473
[43,     1] loss: 2017.419
[44,     1] loss: 2207.519
[45,     1] loss: 2029.783
[46,     1] loss: 2349.402
[47,     1] loss: 2182.646
[48,     1] loss: 2234.408
[49,     1] loss: 2040.649
[50,     1] loss: 2207.034
[51,     1] loss: 2082.304
[52,     1] loss: 2054.312
[53,     1] loss: 1833.231
[54,     1] loss: 1902.627
[55,     1] loss: 2053.320
[56,     1] loss: 1933.462
[57,     1] loss: 2257.856
[58,     1] loss: 1841.501
[59,     1] loss: 1833.399
[60,     1] loss: 1918.826
[61,     1] loss: 1837.605
[62,     1] loss: 2091.935
[63,     1] loss: 1644.231
[64,     1] loss: 1866.119
[65,     1] loss: 1884.430
[66,     1] loss: 1730.890
[67,     1] loss: 1967.575
[68,     1] loss: 1801.521
[69,     1] loss: 1652.087
[70,     1] loss: 1767.447
[71,     1] loss: 1673.514
[72,     1] loss: 1666.876
[73,     1] loss: 1564.473
[74,     1] loss: 1504.626
[75,     1] loss: 1580.818
[76,     1] loss: 1497.779
[77,     1] loss: 1521.152
[78,     1] loss: 1533.159
[79,     1] loss: 1480.156
[80,     1] loss: 1463.188
[81,     1] loss: 1415.986
[82,     1] loss: 1422.733
[83,     1] loss: 1270.317
[84,     1] loss: 1537.688
[85,     1] loss: 1910.798
[86,     1] loss: 1897.318
[87,     1] loss: 1524.653
[88,     1] loss: 1573.336
[89,     1] loss: 1807.164
[90,     1] loss: 1556.609
[91,     1] loss: 1808.543
[92,     1] loss: 1467.869
[93,     1] loss: 1624.942
[94,     1] loss: 1525.735
[95,     1] loss: 1466.490
[96,     1] loss: 1397.388
[97,     1] loss: 1482.009
[98,     1] loss: 1509.833
[99,     1] loss: 1413.240
[100,     1] loss: 1536.476
[101,     1] loss: 1287.188
[102,     1] loss: 1260.651
[103,     1] loss: 1399.402
[104,     1] loss: 1290.922
[105,     1] loss: 1348.357
[106,     1] loss: 1480.475
[107,     1] loss: 1461.314
[108,     1] loss: 1351.750
[109,     1] loss: 1358.079
[110,     1] loss: 1468.901
[111,     1] loss: 1362.517
[112,     1] loss: 1458.069
[113,     1] loss: 1428.114
[114,     1] loss: 1319.883
[115,     1] loss: 1412.115
[116,     1] loss: 1356.889
[117,     1] loss: 1352.686
[118,     1] loss: 1382.891
[119,     1] loss: 1280.084
[120,     1] loss: 1322.204
[121,     1] loss: 1317.608
[122,     1] loss: 1488.584
[123,     1] loss: 1404.094
[124,     1] loss: 1256.217
[125,     1] loss: 1521.677
[126,     1] loss: 1579.906
[127,     1] loss: 1129.662
[128,     1] loss: 1570.829
[129,     1] loss: 1347.084
[130,     1] loss: 1344.078
[131,     1] loss: 1192.900
[132,     1] loss: 1374.931
[133,     1] loss: 1425.939
[134,     1] loss: 1337.653
[135,     1] loss: 1211.942
[136,     1] loss: 1158.012
[137,     1] loss: 1281.521
[138,     1] loss: 1198.106
[139,     1] loss: 1363.102
[140,     1] loss: 1287.132
Early stopping applied (best metric=0.4229087829589844)
Finished Training
Total time taken: 21.324822902679443
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3379.172
[2,     1] loss: 3373.794
[3,     1] loss: 3388.756
[4,     1] loss: 3373.842
[5,     1] loss: 3392.749
[6,     1] loss: 3366.485
[7,     1] loss: 3364.770
[8,     1] loss: 3366.022
[9,     1] loss: 3351.668
[10,     1] loss: 3358.458
[11,     1] loss: 3346.251
[12,     1] loss: 3319.899
[13,     1] loss: 3307.014
[14,     1] loss: 3233.068
[15,     1] loss: 3217.595
[16,     1] loss: 3130.279
[17,     1] loss: 3043.241
[18,     1] loss: 3069.249
[19,     1] loss: 2996.136
[20,     1] loss: 2820.249
[21,     1] loss: 2858.552
[22,     1] loss: 2861.307
[23,     1] loss: 2918.881
[24,     1] loss: 2835.360
[25,     1] loss: 2798.699
[26,     1] loss: 2834.043
[27,     1] loss: 2622.456
[28,     1] loss: 2655.131
[29,     1] loss: 2562.372
[30,     1] loss: 2547.887
[31,     1] loss: 2899.922
[32,     1] loss: 2504.597
[33,     1] loss: 2697.999
[34,     1] loss: 2551.755
[35,     1] loss: 2364.695
[36,     1] loss: 2540.220
[37,     1] loss: 2338.364
[38,     1] loss: 2439.988
[39,     1] loss: 2496.487
[40,     1] loss: 2322.159
[41,     1] loss: 2476.067
[42,     1] loss: 2145.748
[43,     1] loss: 2185.517
[44,     1] loss: 2113.596
[45,     1] loss: 1989.241
[46,     1] loss: 2223.331
[47,     1] loss: 2123.887
[48,     1] loss: 2088.297
[49,     1] loss: 2597.124
[50,     1] loss: 1954.135
[51,     1] loss: 2176.964
[52,     1] loss: 2018.404
[53,     1] loss: 2136.418
[54,     1] loss: 2056.374
[55,     1] loss: 2131.563
[56,     1] loss: 2051.872
[57,     1] loss: 2124.947
[58,     1] loss: 1847.102
[59,     1] loss: 1892.431
[60,     1] loss: 1972.438
[61,     1] loss: 2276.176
[62,     1] loss: 1961.278
[63,     1] loss: 1696.694
[64,     1] loss: 1751.555
[65,     1] loss: 2133.317
[66,     1] loss: 1709.600
[67,     1] loss: 1726.276
[68,     1] loss: 1838.222
[69,     1] loss: 1812.856
[70,     1] loss: 1798.355
[71,     1] loss: 1681.896
[72,     1] loss: 1622.870
[73,     1] loss: 1617.482
[74,     1] loss: 1786.713
[75,     1] loss: 1551.719
[76,     1] loss: 1579.170
[77,     1] loss: 1617.728
[78,     1] loss: 1682.209
[79,     1] loss: 2054.343
[80,     1] loss: 1614.093
[81,     1] loss: 1784.240
[82,     1] loss: 1969.304
[83,     1] loss: 1491.843
[84,     1] loss: 1907.025
[85,     1] loss: 1853.151
[86,     1] loss: 1569.600
[87,     1] loss: 1760.313
[88,     1] loss: 1590.296
[89,     1] loss: 1668.461
[90,     1] loss: 1658.215
[91,     1] loss: 1462.723
[92,     1] loss: 1645.228
[93,     1] loss: 1425.655
[94,     1] loss: 1322.332
[95,     1] loss: 1225.314
[96,     1] loss: 1388.550
[97,     1] loss: 1377.136
[98,     1] loss: 1245.969
[99,     1] loss: 1285.331
[100,     1] loss: 1231.348
[101,     1] loss: 1171.414
[102,     1] loss: 1335.474
[103,     1] loss: 1194.974
[104,     1] loss: 1379.578
[105,     1] loss: 1594.542
[106,     1] loss: 1229.310
[107,     1] loss: 1286.220
[108,     1] loss: 1314.285
[109,     1] loss: 1253.996
[110,     1] loss: 1527.005
[111,     1] loss: 1183.615
[112,     1] loss: 1378.015
[113,     1] loss: 1167.369
[114,     1] loss: 1543.609
[115,     1] loss: 1288.289
[116,     1] loss: 1410.402
[117,     1] loss: 1622.870
[118,     1] loss: 1270.008
[119,     1] loss: 1308.091
[120,     1] loss: 1228.924
[121,     1] loss: 1428.912
[122,     1] loss: 1194.047
[123,     1] loss: 1244.358
[124,     1] loss: 1286.580
[125,     1] loss: 1234.937
[126,     1] loss: 1184.749
[127,     1] loss: 1274.870
[128,     1] loss: 1117.741
[129,     1] loss: 1218.349
[130,     1] loss: 1084.758
[131,     1] loss: 1248.198
[132,     1] loss: 1218.910
[133,     1] loss: 1239.534
[134,     1] loss: 1233.169
[135,     1] loss: 1152.933
[136,     1] loss: 1143.523
[137,     1] loss: 1149.331
[138,     1] loss: 1129.280
Early stopping applied (best metric=0.40800443291664124)
Finished Training
Total time taken: 22.223907470703125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3380.131
[2,     1] loss: 3363.912
[3,     1] loss: 3366.077
[4,     1] loss: 3395.175
[5,     1] loss: 3384.127
[6,     1] loss: 3377.173
[7,     1] loss: 3382.992
[8,     1] loss: 3366.817
[9,     1] loss: 3374.272
[10,     1] loss: 3365.948
[11,     1] loss: 3354.298
[12,     1] loss: 3362.425
[13,     1] loss: 3345.543
[14,     1] loss: 3323.969
[15,     1] loss: 3306.595
[16,     1] loss: 3289.150
[17,     1] loss: 3242.008
[18,     1] loss: 3180.673
[19,     1] loss: 3190.522
[20,     1] loss: 3128.164
[21,     1] loss: 3004.928
[22,     1] loss: 2985.033
[23,     1] loss: 3043.581
[24,     1] loss: 2902.590
[25,     1] loss: 2948.291
[26,     1] loss: 3000.984
[27,     1] loss: 2874.754
[28,     1] loss: 2862.842
[29,     1] loss: 2846.882
[30,     1] loss: 2771.032
[31,     1] loss: 2758.530
[32,     1] loss: 2878.731
[33,     1] loss: 2849.297
[34,     1] loss: 2716.237
[35,     1] loss: 2867.551
[36,     1] loss: 2941.711
[37,     1] loss: 2707.733
[38,     1] loss: 2628.918
[39,     1] loss: 2543.128
[40,     1] loss: 2672.591
[41,     1] loss: 2516.618
[42,     1] loss: 2331.645
[43,     1] loss: 2233.975
[44,     1] loss: 2412.243
[45,     1] loss: 2317.393
[46,     1] loss: 2288.106
[47,     1] loss: 2253.106
[48,     1] loss: 2099.022
[49,     1] loss: 2238.112
[50,     1] loss: 2402.465
[51,     1] loss: 2215.375
[52,     1] loss: 2148.827
[53,     1] loss: 2067.255
[54,     1] loss: 2141.938
[55,     1] loss: 2195.040
[56,     1] loss: 2002.500
[57,     1] loss: 1992.167
[58,     1] loss: 2012.284
[59,     1] loss: 2054.607
[60,     1] loss: 2003.653
[61,     1] loss: 1877.190
[62,     1] loss: 1728.762
[63,     1] loss: 1834.271
[64,     1] loss: 1938.986
[65,     1] loss: 1816.916
[66,     1] loss: 1638.542
[67,     1] loss: 1749.319
[68,     1] loss: 1852.232
[69,     1] loss: 1683.694
[70,     1] loss: 1695.739
[71,     1] loss: 1707.849
[72,     1] loss: 1712.985
[73,     1] loss: 1989.077
[74,     1] loss: 2189.076
[75,     1] loss: 1568.953
[76,     1] loss: 1842.102
[77,     1] loss: 1751.989
[78,     1] loss: 1795.175
[79,     1] loss: 1762.415
[80,     1] loss: 2237.624
[81,     1] loss: 1858.549
[82,     1] loss: 1774.381
[83,     1] loss: 1835.574
[84,     1] loss: 1625.460
[85,     1] loss: 1780.496
[86,     1] loss: 1652.471
Early stopping applied (best metric=0.40396639704704285)
Finished Training
Total time taken: 13.393093824386597
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3407.599
[2,     1] loss: 3396.888
[3,     1] loss: 3377.807
[4,     1] loss: 3362.066
[5,     1] loss: 3380.845
[6,     1] loss: 3385.307
[7,     1] loss: 3381.055
[8,     1] loss: 3388.177
[9,     1] loss: 3370.268
[10,     1] loss: 3374.427
[11,     1] loss: 3379.130
[12,     1] loss: 3369.178
[13,     1] loss: 3364.905
[14,     1] loss: 3361.190
[15,     1] loss: 3336.354
[16,     1] loss: 3319.332
[17,     1] loss: 3303.084
[18,     1] loss: 3249.195
[19,     1] loss: 3211.898
[20,     1] loss: 3172.573
[21,     1] loss: 3143.527
[22,     1] loss: 3098.727
[23,     1] loss: 3065.743
[24,     1] loss: 2956.222
[25,     1] loss: 2868.199
[26,     1] loss: 2864.270
[27,     1] loss: 2926.132
[28,     1] loss: 2765.456
[29,     1] loss: 2681.539
[30,     1] loss: 2607.215
[31,     1] loss: 2878.824
[32,     1] loss: 2671.426
[33,     1] loss: 2664.000
[34,     1] loss: 2579.169
[35,     1] loss: 2504.884
[36,     1] loss: 2531.541
[37,     1] loss: 2846.558
[38,     1] loss: 2440.906
[39,     1] loss: 2674.524
[40,     1] loss: 2331.324
[41,     1] loss: 2443.943
[42,     1] loss: 2300.436
[43,     1] loss: 2572.797
[44,     1] loss: 2438.187
[45,     1] loss: 2334.629
[46,     1] loss: 2243.530
[47,     1] loss: 2262.835
[48,     1] loss: 2209.280
[49,     1] loss: 2294.385
[50,     1] loss: 2226.115
[51,     1] loss: 2250.404
[52,     1] loss: 2052.499
[53,     1] loss: 1984.727
[54,     1] loss: 2140.706
[55,     1] loss: 1926.395
[56,     1] loss: 2163.102
[57,     1] loss: 1892.034
[58,     1] loss: 1777.828
[59,     1] loss: 1915.844
[60,     1] loss: 2014.210
[61,     1] loss: 3101.814
[62,     1] loss: 2191.061
[63,     1] loss: 1816.251
[64,     1] loss: 2207.682
[65,     1] loss: 2024.838
[66,     1] loss: 1911.676
[67,     1] loss: 1906.123
[68,     1] loss: 1942.536
[69,     1] loss: 1899.463
[70,     1] loss: 2114.474
[71,     1] loss: 1696.169
[72,     1] loss: 1977.599
[73,     1] loss: 1823.038
[74,     1] loss: 1946.140
[75,     1] loss: 1852.516
[76,     1] loss: 1761.860
[77,     1] loss: 1957.594
[78,     1] loss: 1582.201
[79,     1] loss: 1980.115
[80,     1] loss: 1550.797
[81,     1] loss: 2021.865
[82,     1] loss: 1779.539
[83,     1] loss: 1576.713
[84,     1] loss: 1733.039
[85,     1] loss: 1514.215
[86,     1] loss: 1719.045
[87,     1] loss: 1695.290
[88,     1] loss: 1726.607
[89,     1] loss: 1520.231
[90,     1] loss: 1481.468
[91,     1] loss: 1698.000
[92,     1] loss: 1406.646
[93,     1] loss: 1666.537
[94,     1] loss: 1680.844
[95,     1] loss: 1597.382
[96,     1] loss: 1565.896
[97,     1] loss: 1594.477
[98,     1] loss: 1435.800
[99,     1] loss: 1543.846
[100,     1] loss: 1349.648
[101,     1] loss: 1515.542
[102,     1] loss: 1364.390
[103,     1] loss: 1410.483
[104,     1] loss: 1333.889
[105,     1] loss: 1429.861
[106,     1] loss: 1294.490
[107,     1] loss: 1318.552
[108,     1] loss: 1438.621
[109,     1] loss: 1518.449
[110,     1] loss: 1415.303
[111,     1] loss: 1561.445
[112,     1] loss: 1731.295
[113,     1] loss: 1346.175
[114,     1] loss: 1488.000
[115,     1] loss: 1280.701
[116,     1] loss: 1519.735
[117,     1] loss: 1822.746
[118,     1] loss: 1463.467
[119,     1] loss: 1642.770
[120,     1] loss: 1283.143
[121,     1] loss: 1456.354
[122,     1] loss: 1300.045
Early stopping applied (best metric=0.337365984916687)
Finished Training
Total time taken: 18.218131065368652
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3390.850
[2,     1] loss: 3373.985
[3,     1] loss: 3381.497
[4,     1] loss: 3360.656
[5,     1] loss: 3368.713
[6,     1] loss: 3357.484
[7,     1] loss: 3375.376
[8,     1] loss: 3380.211
[9,     1] loss: 3374.651
[10,     1] loss: 3377.649
[11,     1] loss: 3366.062
[12,     1] loss: 3354.368
[13,     1] loss: 3331.010
[14,     1] loss: 3314.250
[15,     1] loss: 3293.172
[16,     1] loss: 3263.357
[17,     1] loss: 3254.324
[18,     1] loss: 3122.768
[19,     1] loss: 2990.960
[20,     1] loss: 3097.987
[21,     1] loss: 3007.391
[22,     1] loss: 2937.369
[23,     1] loss: 3023.266
[24,     1] loss: 3033.695
[25,     1] loss: 2956.408
[26,     1] loss: 2848.783
[27,     1] loss: 2964.709
[28,     1] loss: 2706.945
[29,     1] loss: 2720.517
[30,     1] loss: 2699.939
[31,     1] loss: 2798.372
[32,     1] loss: 2618.897
[33,     1] loss: 2607.654
[34,     1] loss: 2594.686
[35,     1] loss: 2462.898
[36,     1] loss: 2661.099
[37,     1] loss: 2405.038
[38,     1] loss: 2297.890
[39,     1] loss: 2339.235
[40,     1] loss: 2530.108
[41,     1] loss: 2264.753
[42,     1] loss: 2614.601
[43,     1] loss: 2356.545
[44,     1] loss: 2688.849
[45,     1] loss: 2179.311
[46,     1] loss: 2355.046
[47,     1] loss: 2290.577
[48,     1] loss: 2361.367
[49,     1] loss: 2242.260
[50,     1] loss: 2183.845
[51,     1] loss: 2097.465
[52,     1] loss: 2278.609
[53,     1] loss: 1961.775
[54,     1] loss: 2093.886
[55,     1] loss: 1855.594
[56,     1] loss: 2017.189
[57,     1] loss: 1908.852
[58,     1] loss: 1939.791
[59,     1] loss: 1828.054
[60,     1] loss: 1931.599
[61,     1] loss: 1799.576
[62,     1] loss: 1883.019
[63,     1] loss: 1714.683
[64,     1] loss: 2103.415
[65,     1] loss: 1691.567
[66,     1] loss: 1750.034
[67,     1] loss: 1705.474
[68,     1] loss: 1797.736
[69,     1] loss: 1754.554
[70,     1] loss: 1883.783
[71,     1] loss: 1601.274
[72,     1] loss: 1902.438
[73,     1] loss: 1571.536
[74,     1] loss: 1901.293
[75,     1] loss: 1476.145
[76,     1] loss: 1665.972
[77,     1] loss: 1511.141
[78,     1] loss: 1730.712
[79,     1] loss: 1505.157
[80,     1] loss: 1651.160
[81,     1] loss: 1699.114
[82,     1] loss: 1692.855
[83,     1] loss: 1617.090
[84,     1] loss: 1826.307
[85,     1] loss: 1762.331
[86,     1] loss: 1922.232
[87,     1] loss: 1554.647
[88,     1] loss: 1979.377
[89,     1] loss: 1526.190
[90,     1] loss: 1918.916
[91,     1] loss: 1574.810
[92,     1] loss: 1636.469
[93,     1] loss: 1595.788
[94,     1] loss: 1567.236
[95,     1] loss: 1754.936
[96,     1] loss: 1480.316
[97,     1] loss: 1581.443
[98,     1] loss: 1497.903
[99,     1] loss: 1551.933
Early stopping applied (best metric=0.42457345128059387)
Finished Training
Total time taken: 14.840535402297974
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3408.725
[2,     1] loss: 3384.787
[3,     1] loss: 3379.767
[4,     1] loss: 3382.914
[5,     1] loss: 3385.280
[6,     1] loss: 3380.794
[7,     1] loss: 3370.767
[8,     1] loss: 3389.133
[9,     1] loss: 3376.524
[10,     1] loss: 3396.882
[11,     1] loss: 3375.885
[12,     1] loss: 3375.382
[13,     1] loss: 3381.727
[14,     1] loss: 3379.818
[15,     1] loss: 3372.586
[16,     1] loss: 3385.624
[17,     1] loss: 3382.917
[18,     1] loss: 3372.445
[19,     1] loss: 3369.915
[20,     1] loss: 3369.334
[21,     1] loss: 3365.130
[22,     1] loss: 3354.988
[23,     1] loss: 3342.938
[24,     1] loss: 3320.496
[25,     1] loss: 3297.213
[26,     1] loss: 3256.032
[27,     1] loss: 3230.754
[28,     1] loss: 3172.521
[29,     1] loss: 3062.782
[30,     1] loss: 3061.115
[31,     1] loss: 3035.592
[32,     1] loss: 2831.359
[33,     1] loss: 2884.515
[34,     1] loss: 2890.555
[35,     1] loss: 2732.348
[36,     1] loss: 2707.928
[37,     1] loss: 2682.146
[38,     1] loss: 2721.292
[39,     1] loss: 2919.537
[40,     1] loss: 2561.994
[41,     1] loss: 2676.204
[42,     1] loss: 2571.223
[43,     1] loss: 2818.111
[44,     1] loss: 2686.809
[45,     1] loss: 2511.318
[46,     1] loss: 2708.221
[47,     1] loss: 2513.834
[48,     1] loss: 2410.359
[49,     1] loss: 2449.855
[50,     1] loss: 2380.245
[51,     1] loss: 2228.227
[52,     1] loss: 2505.874
[53,     1] loss: 2224.587
[54,     1] loss: 2223.246
[55,     1] loss: 2127.595
[56,     1] loss: 2107.615
[57,     1] loss: 2241.111
[58,     1] loss: 1885.890
[59,     1] loss: 2170.707
[60,     1] loss: 2289.156
[61,     1] loss: 2157.715
[62,     1] loss: 2253.104
[63,     1] loss: 2056.816
[64,     1] loss: 1967.836
[65,     1] loss: 1976.760
[66,     1] loss: 1799.164
[67,     1] loss: 1981.412
[68,     1] loss: 1858.986
[69,     1] loss: 1861.771
[70,     1] loss: 2056.383
[71,     1] loss: 1841.803
[72,     1] loss: 1857.032
[73,     1] loss: 1802.576
[74,     1] loss: 1705.536
[75,     1] loss: 1866.375
[76,     1] loss: 1743.427
[77,     1] loss: 1825.200
[78,     1] loss: 1573.993
[79,     1] loss: 1923.365
[80,     1] loss: 1937.013
[81,     1] loss: 1973.351
[82,     1] loss: 2192.206
[83,     1] loss: 1810.778
[84,     1] loss: 1868.099
[85,     1] loss: 1801.608
[86,     1] loss: 2079.362
[87,     1] loss: 1810.056
[88,     1] loss: 1898.923
[89,     1] loss: 1620.530
[90,     1] loss: 1671.488
[91,     1] loss: 1661.897
[92,     1] loss: 1642.808
[93,     1] loss: 1739.463
[94,     1] loss: 1508.721
[95,     1] loss: 1484.691
[96,     1] loss: 1628.448
[97,     1] loss: 1451.978
[98,     1] loss: 1754.043
[99,     1] loss: 1649.120
[100,     1] loss: 1724.721
[101,     1] loss: 1896.692
[102,     1] loss: 1606.700
[103,     1] loss: 2053.777
[104,     1] loss: 1677.192
[105,     1] loss: 1671.282
[106,     1] loss: 1902.278
[107,     1] loss: 1635.390
[108,     1] loss: 1885.617
[109,     1] loss: 1549.922
[110,     1] loss: 1796.219
[111,     1] loss: 1793.159
[112,     1] loss: 1571.706
[113,     1] loss: 1635.021
[114,     1] loss: 1340.685
[115,     1] loss: 1545.307
[116,     1] loss: 1418.161
[117,     1] loss: 1460.931
[118,     1] loss: 1280.753
[119,     1] loss: 1536.233
[120,     1] loss: 1511.718
[121,     1] loss: 1301.954
[122,     1] loss: 1449.580
[123,     1] loss: 1286.866
[124,     1] loss: 1632.313
[125,     1] loss: 1263.532
[126,     1] loss: 1543.068
[127,     1] loss: 1592.482
[128,     1] loss: 1434.361
[129,     1] loss: 1523.275
[130,     1] loss: 1321.333
[131,     1] loss: 1352.761
[132,     1] loss: 1498.605
Early stopping applied (best metric=0.36470523476600647)
Finished Training
Total time taken: 19.800543785095215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3385.932
[2,     1] loss: 3379.281
[3,     1] loss: 3384.808
[4,     1] loss: 3365.425
[5,     1] loss: 3410.344
[6,     1] loss: 3389.790
[7,     1] loss: 3369.946
[8,     1] loss: 3379.151
[9,     1] loss: 3375.200
[10,     1] loss: 3367.642
[11,     1] loss: 3356.230
[12,     1] loss: 3365.534
[13,     1] loss: 3354.426
[14,     1] loss: 3346.528
[15,     1] loss: 3341.471
[16,     1] loss: 3319.826
[17,     1] loss: 3309.182
[18,     1] loss: 3262.157
[19,     1] loss: 3236.163
[20,     1] loss: 3136.684
[21,     1] loss: 3163.730
[22,     1] loss: 3025.919
[23,     1] loss: 2980.850
[24,     1] loss: 2916.434
[25,     1] loss: 3049.044
[26,     1] loss: 2874.957
[27,     1] loss: 2878.615
[28,     1] loss: 2841.632
[29,     1] loss: 2807.539
[30,     1] loss: 2652.243
[31,     1] loss: 2823.708
[32,     1] loss: 2666.601
[33,     1] loss: 2617.786
[34,     1] loss: 2560.347
[35,     1] loss: 2555.584
[36,     1] loss: 2581.114
[37,     1] loss: 2550.020
[38,     1] loss: 2443.526
[39,     1] loss: 2414.991
[40,     1] loss: 2422.478
[41,     1] loss: 2511.561
[42,     1] loss: 2602.921
[43,     1] loss: 2405.286
[44,     1] loss: 2417.453
[45,     1] loss: 2323.147
[46,     1] loss: 2136.717
[47,     1] loss: 2132.332
[48,     1] loss: 2073.810
[49,     1] loss: 2082.246
[50,     1] loss: 2213.296
[51,     1] loss: 2366.340
[52,     1] loss: 2460.650
[53,     1] loss: 2012.580
[54,     1] loss: 2054.644
[55,     1] loss: 2171.377
[56,     1] loss: 1830.648
[57,     1] loss: 1936.698
[58,     1] loss: 1834.578
[59,     1] loss: 2065.852
[60,     1] loss: 1799.128
[61,     1] loss: 2094.284
[62,     1] loss: 1915.734
[63,     1] loss: 1817.944
[64,     1] loss: 1676.012
[65,     1] loss: 1996.926
[66,     1] loss: 1804.050
[67,     1] loss: 2004.533
[68,     1] loss: 1906.843
[69,     1] loss: 1960.449
[70,     1] loss: 1807.220
[71,     1] loss: 1785.711
[72,     1] loss: 1907.935
[73,     1] loss: 1713.854
[74,     1] loss: 1619.527
[75,     1] loss: 1762.734
[76,     1] loss: 1714.064
[77,     1] loss: 1604.624
[78,     1] loss: 1554.340
[79,     1] loss: 1613.406
[80,     1] loss: 1543.180
[81,     1] loss: 1516.535
Early stopping applied (best metric=0.43338659405708313)
Finished Training
Total time taken: 12.449229001998901
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3397.232
[2,     1] loss: 3374.667
[3,     1] loss: 3375.813
[4,     1] loss: 3376.193
[5,     1] loss: 3377.093
[6,     1] loss: 3381.512
[7,     1] loss: 3368.382
[8,     1] loss: 3373.176
[9,     1] loss: 3367.450
[10,     1] loss: 3361.036
[11,     1] loss: 3359.014
[12,     1] loss: 3346.863
[13,     1] loss: 3320.270
[14,     1] loss: 3296.934
[15,     1] loss: 3248.323
[16,     1] loss: 3181.267
[17,     1] loss: 3132.005
[18,     1] loss: 3083.984
[19,     1] loss: 3034.782
[20,     1] loss: 2918.133
[21,     1] loss: 3043.894
[22,     1] loss: 2791.884
[23,     1] loss: 2821.708
[24,     1] loss: 2830.643
[25,     1] loss: 2834.067
[26,     1] loss: 2819.934
[27,     1] loss: 2555.061
[28,     1] loss: 2756.259
[29,     1] loss: 2956.816
[30,     1] loss: 2671.340
[31,     1] loss: 2676.699
[32,     1] loss: 2816.332
[33,     1] loss: 2714.341
[34,     1] loss: 2668.477
[35,     1] loss: 2606.973
[36,     1] loss: 2475.732
[37,     1] loss: 2432.885
[38,     1] loss: 2486.818
[39,     1] loss: 2570.977
[40,     1] loss: 2373.283
[41,     1] loss: 2850.531
[42,     1] loss: 2275.030
[43,     1] loss: 2667.670
[44,     1] loss: 2369.891
[45,     1] loss: 2212.898
[46,     1] loss: 2467.880
[47,     1] loss: 2141.076
[48,     1] loss: 2188.592
[49,     1] loss: 2312.997
[50,     1] loss: 2022.545
[51,     1] loss: 2051.900
[52,     1] loss: 2019.297
[53,     1] loss: 2088.329
[54,     1] loss: 1834.840
[55,     1] loss: 2001.143
[56,     1] loss: 2016.325
[57,     1] loss: 2041.489
[58,     1] loss: 1895.026
[59,     1] loss: 2065.092
[60,     1] loss: 1785.077
[61,     1] loss: 1715.068
[62,     1] loss: 1981.066
[63,     1] loss: 1846.861
[64,     1] loss: 1812.044
[65,     1] loss: 1893.975
[66,     1] loss: 1910.748
[67,     1] loss: 1712.943
[68,     1] loss: 1636.584
[69,     1] loss: 1641.850
[70,     1] loss: 1835.169
[71,     1] loss: 1470.772
[72,     1] loss: 1710.073
[73,     1] loss: 1802.349
[74,     1] loss: 1662.864
[75,     1] loss: 1621.938
[76,     1] loss: 1441.586
[77,     1] loss: 1615.730
[78,     1] loss: 1526.386
[79,     1] loss: 1800.782
[80,     1] loss: 1654.673
[81,     1] loss: 1819.094
[82,     1] loss: 1845.151
[83,     1] loss: 1827.380
[84,     1] loss: 1518.793
[85,     1] loss: 1949.794
[86,     1] loss: 1600.794
[87,     1] loss: 1718.770
[88,     1] loss: 1638.422
[89,     1] loss: 1509.041
[90,     1] loss: 1685.108
[91,     1] loss: 1441.603
Early stopping applied (best metric=0.42423051595687866)
Finished Training
Total time taken: 14.177384853363037
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3368.902
[2,     1] loss: 3410.805
[3,     1] loss: 3392.351
[4,     1] loss: 3372.814
[5,     1] loss: 3379.435
[6,     1] loss: 3366.181
[7,     1] loss: 3376.779
[8,     1] loss: 3380.470
[9,     1] loss: 3356.403
[10,     1] loss: 3388.556
[11,     1] loss: 3372.711
[12,     1] loss: 3362.708
[13,     1] loss: 3360.774
[14,     1] loss: 3357.395
[15,     1] loss: 3347.993
[16,     1] loss: 3349.671
[17,     1] loss: 3333.531
[18,     1] loss: 3299.930
[19,     1] loss: 3237.898
[20,     1] loss: 3248.820
[21,     1] loss: 3146.292
[22,     1] loss: 3136.603
[23,     1] loss: 3130.667
[24,     1] loss: 3012.502
[25,     1] loss: 3070.380
[26,     1] loss: 3001.700
[27,     1] loss: 3067.811
[28,     1] loss: 2955.877
[29,     1] loss: 2785.115
[30,     1] loss: 2843.620
[31,     1] loss: 2678.148
[32,     1] loss: 2612.961
[33,     1] loss: 2647.123
[34,     1] loss: 2641.676
[35,     1] loss: 2477.525
[36,     1] loss: 2537.009
[37,     1] loss: 2427.853
[38,     1] loss: 2491.471
[39,     1] loss: 2527.787
[40,     1] loss: 2615.502
[41,     1] loss: 2226.275
[42,     1] loss: 2221.104
[43,     1] loss: 2469.312
[44,     1] loss: 2321.594
[45,     1] loss: 2343.760
[46,     1] loss: 2357.913
[47,     1] loss: 2550.434
[48,     1] loss: 2188.150
[49,     1] loss: 2371.487
[50,     1] loss: 2204.200
[51,     1] loss: 2374.448
[52,     1] loss: 2323.232
[53,     1] loss: 2018.041
[54,     1] loss: 2031.451
[55,     1] loss: 2203.817
[56,     1] loss: 2030.241
[57,     1] loss: 2297.045
[58,     1] loss: 1902.213
[59,     1] loss: 2196.606
[60,     1] loss: 2034.187
[61,     1] loss: 1935.865
[62,     1] loss: 2197.015
[63,     1] loss: 2021.118
[64,     1] loss: 2079.195
[65,     1] loss: 1998.038
[66,     1] loss: 1856.955
[67,     1] loss: 1831.355
[68,     1] loss: 1815.560
[69,     1] loss: 1875.783
[70,     1] loss: 1822.993
[71,     1] loss: 1598.239
[72,     1] loss: 1969.210
[73,     1] loss: 1832.643
[74,     1] loss: 2364.356
[75,     1] loss: 1606.833
[76,     1] loss: 2036.900
[77,     1] loss: 1733.269
[78,     1] loss: 1875.885
[79,     1] loss: 1814.242
[80,     1] loss: 1734.841
[81,     1] loss: 1879.445
[82,     1] loss: 1512.012
[83,     1] loss: 1988.061
Early stopping applied (best metric=0.4474177360534668)
Finished Training
Total time taken: 13.176124095916748
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3383.181
[2,     1] loss: 3389.206
[3,     1] loss: 3386.372
[4,     1] loss: 3379.787
[5,     1] loss: 3383.091
[6,     1] loss: 3373.768
[7,     1] loss: 3363.456
[8,     1] loss: 3379.917
[9,     1] loss: 3362.126
[10,     1] loss: 3353.349
[11,     1] loss: 3344.329
[12,     1] loss: 3329.566
[13,     1] loss: 3297.458
[14,     1] loss: 3256.746
[15,     1] loss: 3192.995
[16,     1] loss: 3149.597
[17,     1] loss: 3100.103
[18,     1] loss: 3060.099
[19,     1] loss: 2975.021
[20,     1] loss: 2941.283
[21,     1] loss: 2987.975
[22,     1] loss: 3076.810
[23,     1] loss: 3023.988
[24,     1] loss: 2817.353
[25,     1] loss: 2960.267
[26,     1] loss: 2842.606
[27,     1] loss: 2895.887
[28,     1] loss: 2805.471
[29,     1] loss: 2738.643
[30,     1] loss: 2735.406
[31,     1] loss: 2735.528
[32,     1] loss: 2652.922
[33,     1] loss: 2903.440
[34,     1] loss: 2630.081
[35,     1] loss: 2556.160
[36,     1] loss: 2642.439
[37,     1] loss: 2604.335
[38,     1] loss: 2461.476
[39,     1] loss: 2508.717
[40,     1] loss: 2562.049
[41,     1] loss: 2343.410
[42,     1] loss: 2478.792
[43,     1] loss: 2400.071
[44,     1] loss: 2369.114
[45,     1] loss: 2265.598
[46,     1] loss: 2402.044
[47,     1] loss: 2244.053
[48,     1] loss: 2089.049
[49,     1] loss: 2242.794
[50,     1] loss: 2092.387
[51,     1] loss: 2124.573
[52,     1] loss: 2063.050
[53,     1] loss: 1839.329
[54,     1] loss: 2519.845
[55,     1] loss: 2366.355
[56,     1] loss: 1879.240
[57,     1] loss: 2083.676
[58,     1] loss: 1915.432
[59,     1] loss: 1871.815
[60,     1] loss: 2112.958
[61,     1] loss: 1788.005
[62,     1] loss: 1946.249
[63,     1] loss: 2022.181
[64,     1] loss: 1927.380
[65,     1] loss: 1765.936
[66,     1] loss: 1851.853
[67,     1] loss: 1855.460
[68,     1] loss: 1637.802
[69,     1] loss: 1775.510
[70,     1] loss: 1749.234
[71,     1] loss: 1758.338
[72,     1] loss: 1622.282
[73,     1] loss: 1536.981
[74,     1] loss: 1548.997
[75,     1] loss: 1720.835
[76,     1] loss: 1570.059
Early stopping applied (best metric=0.41720205545425415)
Finished Training
Total time taken: 12.857059240341187
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3376.394
[2,     1] loss: 3414.969
[3,     1] loss: 3379.952
[4,     1] loss: 3389.350
[5,     1] loss: 3372.924
[6,     1] loss: 3376.566
[7,     1] loss: 3375.261
[8,     1] loss: 3370.283
[9,     1] loss: 3379.878
[10,     1] loss: 3380.521
[11,     1] loss: 3366.402
[12,     1] loss: 3353.362
[13,     1] loss: 3327.596
[14,     1] loss: 3315.705
[15,     1] loss: 3255.301
[16,     1] loss: 3211.688
[17,     1] loss: 3153.672
[18,     1] loss: 3112.473
[19,     1] loss: 3032.645
[20,     1] loss: 3016.329
[21,     1] loss: 2891.670
[22,     1] loss: 2998.590
[23,     1] loss: 2725.219
[24,     1] loss: 2641.026
[25,     1] loss: 2867.650
[26,     1] loss: 2549.000
[27,     1] loss: 2610.531
[28,     1] loss: 2764.487
[29,     1] loss: 2589.738
[30,     1] loss: 2671.068
[31,     1] loss: 2423.055
[32,     1] loss: 2491.944
[33,     1] loss: 2420.292
[34,     1] loss: 2434.962
[35,     1] loss: 2262.442
[36,     1] loss: 2515.773
[37,     1] loss: 2564.836
[38,     1] loss: 2302.376
[39,     1] loss: 2335.011
[40,     1] loss: 2115.545
[41,     1] loss: 2269.596
[42,     1] loss: 2153.035
[43,     1] loss: 2174.465
[44,     1] loss: 2053.828
[45,     1] loss: 2093.073
[46,     1] loss: 1979.900
[47,     1] loss: 1814.820
[48,     1] loss: 1788.482
[49,     1] loss: 1973.835
[50,     1] loss: 1926.378
[51,     1] loss: 1634.485
[52,     1] loss: 2065.813
[53,     1] loss: 1795.263
[54,     1] loss: 1839.512
[55,     1] loss: 1859.406
[56,     1] loss: 1665.318
[57,     1] loss: 1689.060
[58,     1] loss: 1672.726
[59,     1] loss: 1617.048
[60,     1] loss: 1792.461
[61,     1] loss: 1645.597
[62,     1] loss: 1669.451
[63,     1] loss: 1548.534
[64,     1] loss: 1604.313
[65,     1] loss: 1642.772
[66,     1] loss: 1621.417
[67,     1] loss: 1385.766
[68,     1] loss: 1542.627
[69,     1] loss: 1458.308
[70,     1] loss: 1360.065
[71,     1] loss: 1373.908
[72,     1] loss: 1553.804
[73,     1] loss: 1390.445
[74,     1] loss: 1461.504
[75,     1] loss: 1272.910
[76,     1] loss: 1318.735
[77,     1] loss: 1260.024
[78,     1] loss: 1544.508
[79,     1] loss: 1385.941
[80,     1] loss: 1279.199
Early stopping applied (best metric=0.3639219403266907)
Finished Training
Total time taken: 12.901079416275024
{'Hydroxylation-K Validation Accuracy': 0.7253250591016548, 'Hydroxylation-K Validation Sensitivity': 0.7903703703703704, 'Hydroxylation-K Validation Specificity': 0.7087719298245614, 'Hydroxylation-K Validation Precision': 0.4152025422118301, 'Hydroxylation-K AUC ROC': 0.8225925925925925, 'Hydroxylation-K AUC PR': 0.6151196889892572, 'Hydroxylation-K MCC': 0.4152090755631114, 'Hydroxylation-K F1': 0.5413810151805125, 'Validation Loss (Hydroxylation-K)': 0.4398394147555033, 'Hydroxylation-P Validation Accuracy': 0.7514500617565945, 'Hydroxylation-P Validation Sensitivity': 0.7517460317460317, 'Hydroxylation-P Validation Specificity': 0.7513566761434486, 'Hydroxylation-P Validation Precision': 0.3980614443333347, 'Hydroxylation-P AUC ROC': 0.8113870758704665, 'Hydroxylation-P AUC PR': 0.537810916344311, 'Hydroxylation-P MCC': 0.4085237873840921, 'Hydroxylation-P F1': 0.5185427599725252, 'Validation Loss (Hydroxylation-P)': 0.4072712222735087, 'Validation Loss (total)': 0.847110637029012, 'TimeToTrain': 15.386141395568847}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009615606345273349,
 'learning_rate_Hydroxylation-K': 0.0006962806158653857,
 'learning_rate_Hydroxylation-P': 0.009765991001766033,
 'log_base': 2.153702912632243,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2955719262,
 'sample_weights': [10.421928987809318, 1.3000342575902775],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.621132716834985,
 'weight_decay_Hydroxylation-K': 5.609941283626089,
 'weight_decay_Hydroxylation-P': 8.480715770329725}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1372.538
[2,     1] loss: 1364.853
[3,     1] loss: 1375.634
[4,     1] loss: 1368.010
[5,     1] loss: 1373.675
[6,     1] loss: 1363.549
[7,     1] loss: 1363.608
[8,     1] loss: 1365.628
[9,     1] loss: 1362.982
[10,     1] loss: 1364.106
[11,     1] loss: 1360.586
[12,     1] loss: 1357.519
[13,     1] loss: 1340.681
[14,     1] loss: 1325.210
[15,     1] loss: 1296.987
[16,     1] loss: 1267.239
[17,     1] loss: 1201.559
[18,     1] loss: 1166.941
[19,     1] loss: 1159.929
[20,     1] loss: 1080.327
[21,     1] loss: 1168.977
[22,     1] loss: 1124.706
[23,     1] loss: 1068.506
[24,     1] loss: 1122.767
[25,     1] loss: 1085.166
[26,     1] loss: 1080.734
[27,     1] loss: 1020.361
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038939108224259537,
 'learning_rate_Hydroxylation-K': 0.0016161718302781292,
 'learning_rate_Hydroxylation-P': 0.008999075735225374,
 'log_base': 1.0739890902041422,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4097730371,
 'sample_weights': [2.1760529915078863, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.350661585928885,
 'weight_decay_Hydroxylation-K': 8.704983368662237,
 'weight_decay_Hydroxylation-P': 1.432041535280896}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7616.678
[2,     1] loss: 7582.109
[3,     1] loss: 7582.853
[4,     1] loss: 7610.942
[5,     1] loss: 7593.993
[6,     1] loss: 7585.807
[7,     1] loss: 7571.941
[8,     1] loss: 7551.873
[9,     1] loss: 7548.640
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017848530526547225,
 'learning_rate_Hydroxylation-K': 0.002747067418027052,
 'learning_rate_Hydroxylation-P': 0.008449605910860733,
 'log_base': 1.1210198222925745,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 846821189,
 'sample_weights': [23.388161077991153, 2.923632760484257],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.998684613064105,
 'weight_decay_Hydroxylation-K': 2.384584504202707,
 'weight_decay_Hydroxylation-P': 3.359257258135777}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4752.249
[2,     1] loss: 4760.138
[3,     1] loss: 4739.218
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004363747117475194,
 'learning_rate_Hydroxylation-K': 0.00733220339736817,
 'learning_rate_Hydroxylation-P': 0.003967713966047211,
 'log_base': 2.786850849229185,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2934303027,
 'sample_weights': [14.613623028193306, 1.8267732504544052],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.854772285702087,
 'weight_decay_Hydroxylation-K': 5.165813158541274,
 'weight_decay_Hydroxylation-P': 7.66515279303065}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.249
[2,     1] loss: 1255.309
[3,     1] loss: 1252.382
[4,     1] loss: 1252.658
[5,     1] loss: 1250.670
[6,     1] loss: 1251.774
[7,     1] loss: 1250.374
[8,     1] loss: 1250.870
[9,     1] loss: 1248.760
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0043247770589576445,
 'learning_rate_Hydroxylation-K': 0.004061021209530877,
 'learning_rate_Hydroxylation-P': 0.008180057359002539,
 'log_base': 1.053786240725794,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2130598827,
 'sample_weights': [1.6288644996124617, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.736767317848999,
 'weight_decay_Hydroxylation-K': 5.3877432060825825,
 'weight_decay_Hydroxylation-P': 0.21668779495299834}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10329.511
[2,     1] loss: 10282.037
[3,     1] loss: 10293.660
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010303637272537334,
 'learning_rate_Hydroxylation-K': 0.0033521431224839407,
 'learning_rate_Hydroxylation-P': 0.00898118408665134,
 'log_base': 1.2272106017704438,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2579833829,
 'sample_weights': [31.86591327031989, 3.9833926091575935],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.237272659573653,
 'weight_decay_Hydroxylation-K': 1.9358022897035185,
 'weight_decay_Hydroxylation-P': 0.9286764938633031}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2627.188
[2,     1] loss: 2680.597
[3,     1] loss: 2652.749
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00945948161191285,
 'learning_rate_Hydroxylation-K': 0.003778641332334477,
 'learning_rate_Hydroxylation-P': 0.0018411231635928494,
 'log_base': 2.099016841728165,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2122579758,
 'sample_weights': [8.153815764829181, 1.0192662353193926],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.40535308848779045,
 'weight_decay_Hydroxylation-K': 7.407205768795661,
 'weight_decay_Hydroxylation-P': 8.572201875397196}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1380.612
[2,     1] loss: 1390.707
[3,     1] loss: 1383.054
[4,     1] loss: 1377.545
[5,     1] loss: 1391.832
[6,     1] loss: 1381.064
[7,     1] loss: 1382.348
[8,     1] loss: 1379.921
[9,     1] loss: 1375.745
[10,     1] loss: 1380.652
[11,     1] loss: 1384.095
[12,     1] loss: 1370.956
[13,     1] loss: 1377.115
[14,     1] loss: 1366.414
[15,     1] loss: 1364.422
[16,     1] loss: 1359.131
[17,     1] loss: 1348.996
[18,     1] loss: 1333.471
[19,     1] loss: 1299.147
[20,     1] loss: 1275.153
[21,     1] loss: 1258.277
[22,     1] loss: 1232.030
[23,     1] loss: 1201.355
[24,     1] loss: 1222.728
[25,     1] loss: 1231.974
[26,     1] loss: 1200.911
[27,     1] loss: 1177.389
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006719426530860162,
 'learning_rate_Hydroxylation-K': 0.0008154192062881454,
 'learning_rate_Hydroxylation-P': 0.009956446766888873,
 'log_base': 1.1749050991545291,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2511209364,
 'sample_weights': [2.2515344568754103, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.593624692923812,
 'weight_decay_Hydroxylation-K': 6.490570391125989,
 'weight_decay_Hydroxylation-P': 1.4531720487683726}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3377.451
[2,     1] loss: 3373.972
[3,     1] loss: 3365.510
[4,     1] loss: 3360.560
[5,     1] loss: 3363.241
[6,     1] loss: 3356.975
[7,     1] loss: 3364.804
[8,     1] loss: 3360.342
[9,     1] loss: 3361.723
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002944129966881787,
 'learning_rate_Hydroxylation-K': 0.00043923960041472305,
 'learning_rate_Hydroxylation-P': 0.005856551822314451,
 'log_base': 1.6790117763568415,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2305685369,
 'sample_weights': [10.357158058448233, 1.2946946322208936],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.550980485392243,
 'weight_decay_Hydroxylation-K': 1.9412251369433333,
 'weight_decay_Hydroxylation-P': 1.7831844599726885}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1590.893
[2,     1] loss: 1587.235
[3,     1] loss: 1583.869
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038345943207013142,
 'learning_rate_Hydroxylation-K': 0.008129663416920493,
 'learning_rate_Hydroxylation-P': 0.008829990564138008,
 'log_base': 2.960462311121873,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3986619920,
 'sample_weights': [3.2215858291990944, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.958910012042153,
 'weight_decay_Hydroxylation-K': 8.093246443856215,
 'weight_decay_Hydroxylation-P': 6.936807284954619}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.222
[2,     1] loss: 1234.596
[3,     1] loss: 1230.729
[4,     1] loss: 1234.360
[5,     1] loss: 1231.280
[6,     1] loss: 1227.785
[7,     1] loss: 1218.231
[8,     1] loss: 1205.360
[9,     1] loss: 1179.809
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005911387065380814,
 'learning_rate_Hydroxylation-K': 0.004204901099171581,
 'learning_rate_Hydroxylation-P': 0.009001745010009447,
 'log_base': 1.1071013358236306,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2792876965,
 'sample_weights': [1.538167556902202, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.505714158037525,
 'weight_decay_Hydroxylation-K': 1.2090805617933444,
 'weight_decay_Hydroxylation-P': 1.4509933004131845}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5337.250
[2,     1] loss: 5318.716
[3,     1] loss: 5333.545
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014116661959867067,
 'learning_rate_Hydroxylation-K': 0.003566012009498916,
 'learning_rate_Hydroxylation-P': 0.008421599804550725,
 'log_base': 1.0170270026832497,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 264236805,
 'sample_weights': [16.408079237492363, 2.051088917824169],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.350413292728098,
 'weight_decay_Hydroxylation-K': 2.4853376275039687,
 'weight_decay_Hydroxylation-P': 0.3852739443895603}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32068.164
Exploding loss, terminate run (best metric=0.5318751931190491)
Finished Training
Total time taken: 0.19600248336791992
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32101.555
Exploding loss, terminate run (best metric=0.5292217135429382)
Finished Training
Total time taken: 0.23100042343139648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32184.441
Exploding loss, terminate run (best metric=0.5268405079841614)
Finished Training
Total time taken: 0.20000028610229492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32199.527
Exploding loss, terminate run (best metric=0.5286741852760315)
Finished Training
Total time taken: 0.21851587295532227
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32076.330
Exploding loss, terminate run (best metric=0.5333835482597351)
Finished Training
Total time taken: 0.20399832725524902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32126.053
Exploding loss, terminate run (best metric=0.5370266437530518)
Finished Training
Total time taken: 0.23100042343139648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32090.799
Exploding loss, terminate run (best metric=0.5288276672363281)
Finished Training
Total time taken: 0.20099973678588867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32008.506
Exploding loss, terminate run (best metric=0.534671425819397)
Finished Training
Total time taken: 0.21400189399719238
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32147.705
Exploding loss, terminate run (best metric=0.5288240909576416)
Finished Training
Total time taken: 0.22900104522705078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32221.492
Exploding loss, terminate run (best metric=0.5277188420295715)
Finished Training
Total time taken: 0.25400233268737793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31996.176
Exploding loss, terminate run (best metric=0.5337889790534973)
Finished Training
Total time taken: 0.2070026397705078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32120.039
Exploding loss, terminate run (best metric=0.5289252400398254)
Finished Training
Total time taken: 0.2290036678314209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32067.148
Exploding loss, terminate run (best metric=0.5269911289215088)
Finished Training
Total time taken: 0.20100069046020508
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32192.152
Exploding loss, terminate run (best metric=0.5291245579719543)
Finished Training
Total time taken: 0.22400188446044922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32113.629
Exploding loss, terminate run (best metric=0.5298554301261902)
Finished Training
Total time taken: 0.19899821281433105
{'Hydroxylation-K Validation Accuracy': 0.3955673758865248, 'Hydroxylation-K Validation Sensitivity': 0.6918518518518518, 'Hydroxylation-K Validation Specificity': 0.32280701754385965, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5999220272904483, 'Hydroxylation-K AUC PR': 0.33140355989435744, 'Hydroxylation-K MCC': 0.017089382760563463, 'Hydroxylation-K F1': 0.2626135244066279, 'Validation Loss (Hydroxylation-K)': 0.5566532929738363, 'Hydroxylation-P Validation Accuracy': 0.38772421027697407, 'Hydroxylation-P Validation Sensitivity': 0.6913227513227513, 'Hydroxylation-P Validation Specificity': 0.3231707317073171, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6102609202691501, 'Hydroxylation-P AUC PR': 0.26160457731078685, 'Hydroxylation-P MCC': 0.023739493940089958, 'Hydroxylation-P F1': 0.2261127279271077, 'Validation Loss (Hydroxylation-P)': 0.5303832769393921, 'Validation Loss (total)': 1.0870365699132283, 'TimeToTrain': 0.2159019947052002}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016009893492035675,
 'learning_rate_Hydroxylation-K': 0.0039761974138619385,
 'learning_rate_Hydroxylation-P': 0.0057259576848457325,
 'log_base': 1.0124111152735424,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1700946265,
 'sample_weights': [98.95252235534734, 12.343364562106819],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.303857696977708,
 'weight_decay_Hydroxylation-K': 2.234210145381544,
 'weight_decay_Hydroxylation-P': 2.9360327620153623}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44078.035
Exploding loss, terminate run (best metric=0.5387071371078491)
Finished Training
Total time taken: 0.18899989128112793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43792.148
Exploding loss, terminate run (best metric=0.5271692276000977)
Finished Training
Total time taken: 0.1939997673034668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44040.160
Exploding loss, terminate run (best metric=0.528126060962677)
Finished Training
Total time taken: 0.20000052452087402
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44005.656
Exploding loss, terminate run (best metric=0.528108537197113)
Finished Training
Total time taken: 0.22100090980529785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44117.531
Exploding loss, terminate run (best metric=0.5324255228042603)
Finished Training
Total time taken: 0.2199997901916504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44141.668
Exploding loss, terminate run (best metric=0.537253201007843)
Finished Training
Total time taken: 0.22400164604187012
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44054.961
Exploding loss, terminate run (best metric=0.5280103087425232)
Finished Training
Total time taken: 0.23400378227233887
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43995.820
Exploding loss, terminate run (best metric=0.5277560353279114)
Finished Training
Total time taken: 0.24000096321105957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43920.516
Exploding loss, terminate run (best metric=0.5316433906555176)
Finished Training
Total time taken: 0.22800302505493164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 43927.414
Exploding loss, terminate run (best metric=0.5277602672576904)
Finished Training
Total time taken: 0.231003999710083
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43689.941
Exploding loss, terminate run (best metric=0.5321083664894104)
Finished Training
Total time taken: 0.21399998664855957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44033.262
Exploding loss, terminate run (best metric=0.5279297828674316)
Finished Training
Total time taken: 0.22300267219543457
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43810.570
Exploding loss, terminate run (best metric=0.5358186364173889)
Finished Training
Total time taken: 0.23400211334228516
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 43910.562
Exploding loss, terminate run (best metric=0.5268828272819519)
Finished Training
Total time taken: 0.26403331756591797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44320.004
Exploding loss, terminate run (best metric=0.5283267498016357)
Finished Training
Total time taken: 0.24699831008911133
{'Hydroxylation-K Validation Accuracy': 0.400531914893617, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6563937621832359, 'Hydroxylation-K AUC PR': 0.33907999192258104, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22364532019704436, 'Validation Loss (Hydroxylation-K)': 0.5567813833554586, 'Hydroxylation-P Validation Accuracy': 0.3917443107794867, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5661300448821023, 'Hydroxylation-P AUC PR': 0.23675142103930727, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.1999973472585679, 'Validation Loss (Hydroxylation-P)': 0.5305350701014201, 'Validation Loss (total)': 1.0873164733250935, 'TimeToTrain': 0.2242033799489339}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002849702155170532,
 'learning_rate_Hydroxylation-K': 0.005047924401153934,
 'learning_rate_Hydroxylation-P': 0.0041449686026147825,
 'log_base': 1.4480870386871685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3523302220,
 'sample_weights': [135.44534055549417, 16.895488633542957],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.043202299667941,
 'weight_decay_Hydroxylation-K': 2.8215583242862334,
 'weight_decay_Hydroxylation-P': 1.7533446327419726}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1860.346
[2,     1] loss: 1856.718
[3,     1] loss: 1854.572
[4,     1] loss: 1858.015
[5,     1] loss: 1855.054
[6,     1] loss: 1857.925
[7,     1] loss: 1843.856
[8,     1] loss: 1846.808
[9,     1] loss: 1827.988
[10,     1] loss: 1802.112
[11,     1] loss: 1776.154
[12,     1] loss: 1722.692
[13,     1] loss: 1675.228
[14,     1] loss: 1648.376
[15,     1] loss: 1614.123
[16,     1] loss: 1632.381
[17,     1] loss: 1577.917
[18,     1] loss: 1536.874
[19,     1] loss: 1586.082
[20,     1] loss: 1547.767
[21,     1] loss: 1553.971
[22,     1] loss: 1473.731
[23,     1] loss: 1507.636
[24,     1] loss: 1531.286
[25,     1] loss: 1606.092
[26,     1] loss: 1494.239
[27,     1] loss: 1563.223
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003621120111788773,
 'learning_rate_Hydroxylation-K': 0.0005977400569305122,
 'learning_rate_Hydroxylation-P': 0.009470588502713585,
 'log_base': 1.0791888167104111,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2946279854,
 'sample_weights': [4.509042266637739, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.517978781504253,
 'weight_decay_Hydroxylation-K': 3.3331370071184536,
 'weight_decay_Hydroxylation-P': 1.3341320586155763}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7124.704
[2,     1] loss: 7093.037
[3,     1] loss: 7126.580
[4,     1] loss: 7104.147
[5,     1] loss: 7096.168
[6,     1] loss: 7078.170
[7,     1] loss: 7080.040
[8,     1] loss: 7106.029
[9,     1] loss: 7050.074
[10,     1] loss: 7019.820
[11,     1] loss: 6939.766
[12,     1] loss: 6908.715
[13,     1] loss: 6810.439
[14,     1] loss: 6627.920
[15,     1] loss: 6479.034
[16,     1] loss: 6157.331
[17,     1] loss: 6035.861
[18,     1] loss: 6060.782
[19,     1] loss: 5794.015
[20,     1] loss: 6013.736
[21,     1] loss: 5960.239
[22,     1] loss: 5739.675
[23,     1] loss: 6016.003
[24,     1] loss: 5978.541
[25,     1] loss: 5627.411
[26,     1] loss: 5517.876
[27,     1] loss: 5718.551
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035338621445724105,
 'learning_rate_Hydroxylation-K': 0.005554601119252004,
 'learning_rate_Hydroxylation-P': 0.007847979171369727,
 'log_base': 1.0167402487079986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1158285362,
 'sample_weights': [21.905924727722056, 2.738346078988579],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.752876512776702,
 'weight_decay_Hydroxylation-K': 1.720930038860612,
 'weight_decay_Hydroxylation-P': 0.7526690672285925}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32828.098
Exploding loss, terminate run (best metric=0.5334756970405579)
Finished Training
Total time taken: 0.22754359245300293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33060.039
Exploding loss, terminate run (best metric=0.5394082069396973)
Finished Training
Total time taken: 0.21199870109558105
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32914.492
Exploding loss, terminate run (best metric=0.5277577042579651)
Finished Training
Total time taken: 0.21900248527526855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32722.027
Exploding loss, terminate run (best metric=0.5265257358551025)
Finished Training
Total time taken: 0.21799921989440918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32603.047
Exploding loss, terminate run (best metric=0.5320299863815308)
Finished Training
Total time taken: 0.22299981117248535
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32769.223
Exploding loss, terminate run (best metric=0.5329576134681702)
Finished Training
Total time taken: 0.2220001220703125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32715.355
Exploding loss, terminate run (best metric=0.5270947813987732)
Finished Training
Total time taken: 0.2570004463195801
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32750.006
Exploding loss, terminate run (best metric=0.5297130942344666)
Finished Training
Total time taken: 0.2715282440185547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32588.891
Exploding loss, terminate run (best metric=0.5291538834571838)
Finished Training
Total time taken: 0.29051780700683594
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32689.090
Exploding loss, terminate run (best metric=0.5284736156463623)
Finished Training
Total time taken: 0.23846650123596191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32574.633
Exploding loss, terminate run (best metric=0.5336486101150513)
Finished Training
Total time taken: 0.25999999046325684
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32542.695
Exploding loss, terminate run (best metric=0.5321272611618042)
Finished Training
Total time taken: 0.22301030158996582
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32541.578
Exploding loss, terminate run (best metric=0.5305878520011902)
Finished Training
Total time taken: 0.291034460067749
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32709.980
Exploding loss, terminate run (best metric=0.530911386013031)
Finished Training
Total time taken: 0.24029326438903809
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32685.805
Exploding loss, terminate run (best metric=0.5339899659156799)
Finished Training
Total time taken: 0.2550015449523926
{'Hydroxylation-K Validation Accuracy': 0.6227836879432624, 'Hydroxylation-K Validation Sensitivity': 0.33111111111111113, 'Hydroxylation-K Validation Specificity': 0.6982456140350877, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.7005847953216374, 'Hydroxylation-K AUC PR': 0.42322054708645634, 'Hydroxylation-K MCC': 0.027850528179993925, 'Hydroxylation-K F1': 0.13817001772900483, 'Validation Loss (Hydroxylation-K)': 0.5557663242022196, 'Hydroxylation-P Validation Accuracy': 0.6212305635923726, 'Hydroxylation-P Validation Sensitivity': 0.3385185185185185, 'Hydroxylation-P Validation Specificity': 0.682520325203252, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6541388973997833, 'Hydroxylation-P AUC PR': 0.33274260305511205, 'Hydroxylation-P MCC': 0.022946946220995648, 'Hydroxylation-P F1': 0.1240582990915979, 'Validation Loss (Hydroxylation-P)': 0.5311903595924378, 'Validation Loss (total)': 1.0869566679000855, 'TimeToTrain': 0.24322643280029296}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011738070222563138,
 'learning_rate_Hydroxylation-K': 0.005812694419902091,
 'learning_rate_Hydroxylation-P': 0.00732364684131412,
 'log_base': 1.3801552173731733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 419319463,
 'sample_weights': [100.633311189855, 12.553026618640796],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.77982866897886,
 'weight_decay_Hydroxylation-K': 1.8530333822073828,
 'weight_decay_Hydroxylation-P': 0.16275769463562612}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1998.393
[2,     1] loss: 1999.764
[3,     1] loss: 2000.704
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003556709877433322,
 'learning_rate_Hydroxylation-K': 0.0030208686130484526,
 'learning_rate_Hydroxylation-P': 0.005471800910714156,
 'log_base': 2.8897407076335524,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1125166075,
 'sample_weights': [5.181452615813579, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.802578549432047,
 'weight_decay_Hydroxylation-K': 6.611651451481806,
 'weight_decay_Hydroxylation-P': 7.569803970820101}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.484
[2,     1] loss: 1240.744
[3,     1] loss: 1238.712
[4,     1] loss: 1240.153
[5,     1] loss: 1237.465
[6,     1] loss: 1238.103
[7,     1] loss: 1232.156
[8,     1] loss: 1223.873
[9,     1] loss: 1212.354
[10,     1] loss: 1196.069
[11,     1] loss: 1165.840
[12,     1] loss: 1104.699
[13,     1] loss: 1109.225
[14,     1] loss: 1063.432
[15,     1] loss: 1037.378
[16,     1] loss: 1076.209
[17,     1] loss: 1028.962
[18,     1] loss: 1048.482
[19,     1] loss: 1026.926
[20,     1] loss: 985.243
[21,     1] loss: 1013.745
[22,     1] loss: 951.763
[23,     1] loss: 925.241
[24,     1] loss: 921.678
[25,     1] loss: 969.634
[26,     1] loss: 932.272
[27,     1] loss: 935.510
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005765799195327907,
 'learning_rate_Hydroxylation-K': 0.009104270089720316,
 'learning_rate_Hydroxylation-P': 0.006349142420669477,
 'log_base': 2.899169169133329,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1245636381,
 'sample_weights': [1.573214675328169, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.14883523012587,
 'weight_decay_Hydroxylation-K': 5.972217579954714,
 'weight_decay_Hydroxylation-P': 0.24730372867946548}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.154
[2,     1] loss: 1239.723
[3,     1] loss: 1234.274
[4,     1] loss: 1237.970
[5,     1] loss: 1234.190
[6,     1] loss: 1230.644
[7,     1] loss: 1214.445
[8,     1] loss: 1189.979
[9,     1] loss: 1174.913
[10,     1] loss: 1119.685
[11,     1] loss: 1076.368
[12,     1] loss: 1053.846
[13,     1] loss: 1045.996
[14,     1] loss: 995.317
[15,     1] loss: 1069.885
[16,     1] loss: 933.148
[17,     1] loss: 978.213
[18,     1] loss: 979.440
[19,     1] loss: 964.315
[20,     1] loss: 962.803
[21,     1] loss: 968.201
[22,     1] loss: 1015.054
[23,     1] loss: 973.611
[24,     1] loss: 940.701
[25,     1] loss: 895.205
[26,     1] loss: 879.168
[27,     1] loss: 901.507
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011920977048037727,
 'learning_rate_Hydroxylation-K': 0.003791995885363636,
 'learning_rate_Hydroxylation-P': 0.00874830087928502,
 'log_base': 1.2903157801180223,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3815420873,
 'sample_weights': [1.568400214346073, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.011966822879046,
 'weight_decay_Hydroxylation-K': 4.127001975410694,
 'weight_decay_Hydroxylation-P': 0.5635351183145637}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2292.168
[2,     1] loss: 2286.629
[3,     1] loss: 2288.972
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018999197528321815,
 'learning_rate_Hydroxylation-K': 0.00284701692742188,
 'learning_rate_Hydroxylation-P': 0.007889968003753304,
 'log_base': 1.032595718915805,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1384706946,
 'sample_weights': [6.549738839629041, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.572946590985149,
 'weight_decay_Hydroxylation-K': 1.7535535095179724,
 'weight_decay_Hydroxylation-P': 0.891483849481465}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16905.055
Exploding loss, terminate run (best metric=0.5326846837997437)
Finished Training
Total time taken: 0.23497653007507324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16840.559
Exploding loss, terminate run (best metric=0.5315225720405579)
Finished Training
Total time taken: 0.2310023307800293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16998.863
Exploding loss, terminate run (best metric=0.5284500122070312)
Finished Training
Total time taken: 0.2349998950958252
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16883.320
Exploding loss, terminate run (best metric=0.5360414981842041)
Finished Training
Total time taken: 0.2180013656616211
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16935.477
Exploding loss, terminate run (best metric=0.5299776792526245)
Finished Training
Total time taken: 0.21899962425231934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17018.715
Exploding loss, terminate run (best metric=0.5344330072402954)
Finished Training
Total time taken: 0.22052454948425293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16932.400
Exploding loss, terminate run (best metric=0.53106689453125)
Finished Training
Total time taken: 0.24855852127075195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17023.037
Exploding loss, terminate run (best metric=0.5316336154937744)
Finished Training
Total time taken: 0.26000070571899414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16904.365
Exploding loss, terminate run (best metric=0.530086100101471)
Finished Training
Total time taken: 0.25751757621765137
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16992.371
Exploding loss, terminate run (best metric=0.5298345685005188)
Finished Training
Total time taken: 0.22903752326965332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16966.025
Exploding loss, terminate run (best metric=0.5319268703460693)
Finished Training
Total time taken: 0.23399829864501953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16889.217
Exploding loss, terminate run (best metric=0.5268340110778809)
Finished Training
Total time taken: 0.24300050735473633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16838.238
Exploding loss, terminate run (best metric=0.5329958200454712)
Finished Training
Total time taken: 0.21600031852722168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16917.652
Exploding loss, terminate run (best metric=0.5336437225341797)
Finished Training
Total time taken: 0.24486923217773438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17083.277
Exploding loss, terminate run (best metric=0.5345157980918884)
Finished Training
Total time taken: 0.23000025749206543
{'Hydroxylation-K Validation Accuracy': 0.5605791962174941, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.6, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6350487329434698, 'Hydroxylation-K AUC PR': 0.3418232262869585, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.13481116584564862, 'Validation Loss (Hydroxylation-K)': 0.556721834341685, 'Hydroxylation-P Validation Accuracy': 0.5651580461228702, 'Hydroxylation-P Validation Sensitivity': 0.4, 'Hydroxylation-P Validation Specificity': 0.6, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.582615051457263, 'Hydroxylation-P AUC PR': 0.27763285169557633, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.12062123059431565, 'Validation Loss (Hydroxylation-P)': 0.5317097902297974, 'Validation Loss (total)': 1.0884316285451254, 'TimeToTrain': 0.23476581573486327}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003421069964925891,
 'learning_rate_Hydroxylation-K': 0.006051166488464911,
 'learning_rate_Hydroxylation-P': 0.00928308406466347,
 'log_base': 1.3448245865287098,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 884982463,
 'sample_weights': [52.08550591766155, 6.497160179855951],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.224131644862339,
 'weight_decay_Hydroxylation-K': 2.6937171816051926,
 'weight_decay_Hydroxylation-P': 1.319753913088078}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2100.540
[2,     1] loss: 2096.647
[3,     1] loss: 2093.933
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004126377715782675,
 'learning_rate_Hydroxylation-K': 0.005108699569460078,
 'learning_rate_Hydroxylation-P': 0.008625663668006487,
 'log_base': 1.0279194152727742,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 619485078,
 'sample_weights': [5.634992717657469, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.720279821142301,
 'weight_decay_Hydroxylation-K': 1.2778345544267244,
 'weight_decay_Hydroxylation-P': 1.8016865407899578}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19702.852
Exploding loss, terminate run (best metric=0.5364482998847961)
Finished Training
Total time taken: 0.21114897727966309
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19653.252
Exploding loss, terminate run (best metric=0.5268804430961609)
Finished Training
Total time taken: 0.25500059127807617
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19705.309
Exploding loss, terminate run (best metric=0.527042031288147)
Finished Training
Total time taken: 0.22899913787841797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19638.164
Exploding loss, terminate run (best metric=0.5262858867645264)
Finished Training
Total time taken: 0.23200249671936035
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19737.801
Exploding loss, terminate run (best metric=0.5275477170944214)
Finished Training
Total time taken: 0.2220005989074707
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19680.988
Exploding loss, terminate run (best metric=0.534663200378418)
Finished Training
Total time taken: 0.2770717144012451
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19659.963
Exploding loss, terminate run (best metric=0.535557210445404)
Finished Training
Total time taken: 0.21899819374084473
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19630.039
Exploding loss, terminate run (best metric=0.5271417498588562)
Finished Training
Total time taken: 0.2609996795654297
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19787.672
Exploding loss, terminate run (best metric=0.5297184586524963)
Finished Training
Total time taken: 0.24499797821044922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19833.875
Exploding loss, terminate run (best metric=0.5347402691841125)
Finished Training
Total time taken: 0.2735166549682617
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19836.201
Exploding loss, terminate run (best metric=0.5304331183433533)
Finished Training
Total time taken: 0.24399876594543457
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19615.373
Exploding loss, terminate run (best metric=0.5260239839553833)
Finished Training
Total time taken: 0.2435460090637207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19667.801
Exploding loss, terminate run (best metric=0.5272406339645386)
Finished Training
Total time taken: 0.23761343955993652
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 19683.148
Exploding loss, terminate run (best metric=0.5325090289115906)
Finished Training
Total time taken: 0.23900055885314941
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 19736.889
Exploding loss, terminate run (best metric=0.528971254825592)
Finished Training
Total time taken: 0.22599577903747559
{'Hydroxylation-K Validation Accuracy': 0.44166666666666665, 'Hydroxylation-K Validation Sensitivity': 0.6, 'Hydroxylation-K Validation Specificity': 0.4, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6074658869395712, 'Hydroxylation-K AUC PR': 0.3476870262749296, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.20221674876847293, 'Validation Loss (Hydroxylation-K)': 0.5570969700813293, 'Hydroxylation-P Validation Accuracy': 0.4349603911814967, 'Hydroxylation-P Validation Sensitivity': 0.6, 'Hydroxylation-P Validation Specificity': 0.4, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5893476340109589, 'Hydroxylation-P AUC PR': 0.2826573623177676, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.18005432731554796, 'Validation Loss (Hydroxylation-P)': 0.5300802191098531, 'Validation Loss (total)': 1.0871771971384685, 'TimeToTrain': 0.24099270502726236}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 6.709064196695584e-05,
 'learning_rate_Hydroxylation-K': 0.004588697258015938,
 'learning_rate_Hydroxylation-P': 0.0077995909464005754,
 'log_base': 1.0298854615368007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4270665906,
 'sample_weights': [60.6709242231614, 7.5681075952478665],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.991361923481083,
 'weight_decay_Hydroxylation-K': 0.7378592214089716,
 'weight_decay_Hydroxylation-P': 0.007366574432919548}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18411.543
Exploding loss, terminate run (best metric=0.5337885022163391)
Finished Training
Total time taken: 0.20599961280822754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18461.863
Exploding loss, terminate run (best metric=0.5307932496070862)
Finished Training
Total time taken: 0.2429978847503662
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18477.996
Exploding loss, terminate run (best metric=0.5271347761154175)
Finished Training
Total time taken: 0.21900010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18398.916
Exploding loss, terminate run (best metric=0.5288439989089966)
Finished Training
Total time taken: 0.2460002899169922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18341.691
Exploding loss, terminate run (best metric=0.5285577774047852)
Finished Training
Total time taken: 0.22999858856201172
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18438.492
Exploding loss, terminate run (best metric=0.5362193584442139)
Finished Training
Total time taken: 0.24000072479248047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18402.371
Exploding loss, terminate run (best metric=0.527263343334198)
Finished Training
Total time taken: 0.24599957466125488
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18435.674
Exploding loss, terminate run (best metric=0.5275742411613464)
Finished Training
Total time taken: 0.219038724899292
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18403.670
Exploding loss, terminate run (best metric=0.5344640612602234)
Finished Training
Total time taken: 0.22699761390686035
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18353.039
Exploding loss, terminate run (best metric=0.528584897518158)
Finished Training
Total time taken: 0.25551867485046387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18303.051
Exploding loss, terminate run (best metric=0.5385379791259766)
Finished Training
Total time taken: 0.22451448440551758
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18510.133
Exploding loss, terminate run (best metric=0.5273323655128479)
Finished Training
Total time taken: 0.212083101272583
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18616.934
Exploding loss, terminate run (best metric=0.537722110748291)
Finished Training
Total time taken: 0.23499846458435059
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18465.350
Exploding loss, terminate run (best metric=0.5290987491607666)
Finished Training
Total time taken: 0.2400815486907959
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18495.684
Exploding loss, terminate run (best metric=0.5289701223373413)
Finished Training
Total time taken: 0.24822258949279785
{'Hydroxylation-K Validation Accuracy': 0.4191193853427896, 'Hydroxylation-K Validation Sensitivity': 0.6266666666666667, 'Hydroxylation-K Validation Specificity': 0.3701754385964912, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5509551656920078, 'Hydroxylation-K AUC PR': 0.28719631398080087, 'Hydroxylation-K MCC': -0.0025852224477649374, 'Hydroxylation-K F1': 0.21630118120663172, 'Validation Loss (Hydroxylation-K)': 0.5571578860282898, 'Hydroxylation-P Validation Accuracy': 0.41419608141718695, 'Hydroxylation-P Validation Sensitivity': 0.6333333333333333, 'Hydroxylation-P Validation Specificity': 0.3678861788617886, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5118209830360062, 'Hydroxylation-P AUC PR': 0.20045898682280203, 'Hydroxylation-P MCC': 0.0009374645253290549, 'Hydroxylation-P F1': 0.1977890693499458, 'Validation Loss (Hydroxylation-P)': 0.5309923688570658, 'Validation Loss (total)': 1.0881502469380697, 'TimeToTrain': 0.23276346524556477}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018593284445110423,
 'learning_rate_Hydroxylation-K': 0.0029649102839785562,
 'learning_rate_Hydroxylation-P': 0.009355575034173403,
 'log_base': 1.1810749394058082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 227159322,
 'sample_weights': [56.73405971935884, 7.077022045881608],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.079297086010419,
 'weight_decay_Hydroxylation-K': 2.175261489200782,
 'weight_decay_Hydroxylation-P': 0.11290551121834058}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3261.204
[2,     1] loss: 3261.642
[3,     1] loss: 3263.597
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 9.987031709641873e-05,
 'learning_rate_Hydroxylation-K': 0.0027332196879573923,
 'learning_rate_Hydroxylation-P': 0.004334584348771769,
 'log_base': 1.5602359453270243,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3809252439,
 'sample_weights': [10.031204769540345, 1.253948901478708],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.651473306612186,
 'weight_decay_Hydroxylation-K': 2.9253169341345755,
 'weight_decay_Hydroxylation-P': 5.636160874265651}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1702.812
[2,     1] loss: 1700.056
[3,     1] loss: 1708.491
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005572609551478699,
 'learning_rate_Hydroxylation-K': 0.005666906831159968,
 'learning_rate_Hydroxylation-P': 0.009330785565188982,
 'log_base': 1.0526946134104882,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1169890917,
 'sample_weights': [3.752931824797136, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.859595785518586,
 'weight_decay_Hydroxylation-K': 0.92774168986161,
 'weight_decay_Hydroxylation-P': 2.677846868465652}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10534.223
[2,     1] loss: 10565.778
[3,     1] loss: 10541.429
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003931543082068502,
 'learning_rate_Hydroxylation-K': 0.0015735613204571775,
 'learning_rate_Hydroxylation-P': 0.009358968776907178,
 'log_base': 1.1232141719589424,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1333069695,
 'sample_weights': [32.50905394935498, 4.063788291082176],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.033966767647811,
 'weight_decay_Hydroxylation-K': 2.2193935748137266,
 'weight_decay_Hydroxylation-P': 0.9773353645559304}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4696.610
[2,     1] loss: 4661.937
[3,     1] loss: 4660.295
[4,     1] loss: 4651.035
[5,     1] loss: 4681.716
[6,     1] loss: 4643.336
[7,     1] loss: 4642.716
[8,     1] loss: 4633.340
[9,     1] loss: 4634.498
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017698768818636597,
 'learning_rate_Hydroxylation-K': 0.004255949479666902,
 'learning_rate_Hydroxylation-P': 0.004287210905206355,
 'log_base': 1.0577079159364624,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3888881668,
 'sample_weights': [14.367676532657088, 1.796028754156548],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.43039452653127,
 'weight_decay_Hydroxylation-K': 0.670559692631677,
 'weight_decay_Hydroxylation-P': 2.956793099210441}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9655.010
[2,     1] loss: 9740.764
[3,     1] loss: 9618.337
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002012770560844932,
 'learning_rate_Hydroxylation-K': 0.0018918034334770091,
 'learning_rate_Hydroxylation-P': 0.008132108228787991,
 'log_base': 1.0502223540578628,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2321089677,
 'sample_weights': [29.75610471785012, 3.7196563803116662],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.783156904955316,
 'weight_decay_Hydroxylation-K': 3.6787378780517654,
 'weight_decay_Hydroxylation-P': 0.35061416823664937}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11088.224
[2,     1] loss: 11052.547
[3,     1] loss: 11071.369
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0058950917532889675,
 'learning_rate_Hydroxylation-K': 0.0038804599767807155,
 'learning_rate_Hydroxylation-P': 0.009522464300472792,
 'log_base': 1.2541047329639947,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3724333208,
 'sample_weights': [34.068942045883674, 4.25878181479244],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.9573554004016795,
 'weight_decay_Hydroxylation-K': 7.703891262973415,
 'weight_decay_Hydroxylation-P': 1.9590000940220191}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2469.943
[2,     1] loss: 2471.691
[3,     1] loss: 2488.847
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001225513401790215,
 'learning_rate_Hydroxylation-K': 0.00021111716301627087,
 'learning_rate_Hydroxylation-P': 0.008731960875865443,
 'log_base': 1.1733693676404786,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 795301512,
 'sample_weights': [7.373150393183931, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.469656367707744,
 'weight_decay_Hydroxylation-K': 0.5994200719004559,
 'weight_decay_Hydroxylation-P': 2.8192893823820175}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3380.999
[2,     1] loss: 3412.977
[3,     1] loss: 3392.435
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022314947833280466,
 'learning_rate_Hydroxylation-K': 0.004698485367667881,
 'learning_rate_Hydroxylation-P': 0.007628648691415312,
 'log_base': 1.044882520692894,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2953431112,
 'sample_weights': [10.441889493294932, 1.305286469601121],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.092151788131002,
 'weight_decay_Hydroxylation-K': 0.8472292032458171,
 'weight_decay_Hydroxylation-P': 0.9825577258965389}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12403.256
[2,     1] loss: 12372.863
[3,     1] loss: 12376.615
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004207620894925988,
 'learning_rate_Hydroxylation-K': 0.0035893702117169644,
 'learning_rate_Hydroxylation-P': 0.007304781239069939,
 'log_base': 1.1149473874179774,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1874926047,
 'sample_weights': [38.02445575160802, 4.75324007579196],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.386178696119857,
 'weight_decay_Hydroxylation-K': 2.690429067138582,
 'weight_decay_Hydroxylation-P': 6.175824284699912}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4989.422
[2,     1] loss: 4998.538
[3,     1] loss: 5004.813
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005663337414475408,
 'learning_rate_Hydroxylation-K': 0.005515448308929827,
 'learning_rate_Hydroxylation-P': 0.0069292794156932905,
 'log_base': 1.1155054865005922,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4224397379,
 'sample_weights': [15.343128735755187, 1.9179649768358],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.8808847123563024,
 'weight_decay_Hydroxylation-K': 0.41347292635321753,
 'weight_decay_Hydroxylation-P': 0.3307780178216275}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4961.555
[2,     1] loss: 4950.184
[3,     1] loss: 4986.842
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020334085433714377,
 'learning_rate_Hydroxylation-K': 0.0017657073349071494,
 'learning_rate_Hydroxylation-P': 0.001798643935663625,
 'log_base': 1.8716106691104168,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 888616333,
 'sample_weights': [15.272884349870127, 1.9091840903381903],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.29783724430036596,
 'weight_decay_Hydroxylation-K': 6.649402777059851,
 'weight_decay_Hydroxylation-P': 9.63713241953682}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1472.454
[2,     1] loss: 1466.421
[3,     1] loss: 1470.229
[4,     1] loss: 1472.007
[5,     1] loss: 1469.889
[6,     1] loss: 1468.689
[7,     1] loss: 1467.251
[8,     1] loss: 1461.708
[9,     1] loss: 1455.837
[10,     1] loss: 1449.261
[11,     1] loss: 1440.041
[12,     1] loss: 1423.358
[13,     1] loss: 1403.078
[14,     1] loss: 1377.106
[15,     1] loss: 1345.401
[16,     1] loss: 1315.099
[17,     1] loss: 1289.504
[18,     1] loss: 1240.342
[19,     1] loss: 1290.992
[20,     1] loss: 1269.953
[21,     1] loss: 1264.606
[22,     1] loss: 1183.541
[23,     1] loss: 1201.401
[24,     1] loss: 1208.029
[25,     1] loss: 1221.560
[26,     1] loss: 1190.024
[27,     1] loss: 1251.832
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004017420794606386,
 'learning_rate_Hydroxylation-K': 0.003704807123326397,
 'learning_rate_Hydroxylation-P': 0.008045717711014484,
 'log_base': 1.074284878046665,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2350301232,
 'sample_weights': [2.6634409658016422, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.626251822215795,
 'weight_decay_Hydroxylation-K': 1.1886529447624472,
 'weight_decay_Hydroxylation-P': 3.1709742548099458}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7541.146
[2,     1] loss: 7577.993
[3,     1] loss: 7552.794
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014769683378538395,
 'learning_rate_Hydroxylation-K': 0.004013147967125555,
 'learning_rate_Hydroxylation-P': 0.00754154590602025,
 'log_base': 1.0887702812313316,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3570051724,
 'sample_weights': [23.298279859315766, 2.912397175326649],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.10820156615641,
 'weight_decay_Hydroxylation-K': 1.8652872737827804,
 'weight_decay_Hydroxylation-P': 0.6396358223007745}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6438.562
[2,     1] loss: 6406.983
[3,     1] loss: 6374.543
[4,     1] loss: 6342.222
[5,     1] loss: 6335.546
[6,     1] loss: 6389.493
[7,     1] loss: 6360.824
[8,     1] loss: 6325.125
[9,     1] loss: 6359.927
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017439366709252925,
 'learning_rate_Hydroxylation-K': 0.009708051195214841,
 'learning_rate_Hydroxylation-P': 0.006469857368365356,
 'log_base': 1.2823833887722766,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 596945659,
 'sample_weights': [19.629220351403198, 2.45374706847435],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.93079907252295,
 'weight_decay_Hydroxylation-K': 4.962414828067391,
 'weight_decay_Hydroxylation-P': 4.644313709135072}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2320.833
[2,     1] loss: 2321.269
[3,     1] loss: 2327.345
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028070464718343915,
 'learning_rate_Hydroxylation-K': 0.0005052999759260169,
 'learning_rate_Hydroxylation-P': 0.004554035613369787,
 'log_base': 2.816429106070658,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2819952793,
 'sample_weights': [6.712128782092026, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.394781871421719,
 'weight_decay_Hydroxylation-K': 9.609250975603764,
 'weight_decay_Hydroxylation-P': 9.118330979110235}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.518
[2,     1] loss: 1248.354
[3,     1] loss: 1248.095
[4,     1] loss: 1250.404
[5,     1] loss: 1247.848
[6,     1] loss: 1248.834
[7,     1] loss: 1248.219
[8,     1] loss: 1246.091
[9,     1] loss: 1246.313
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036321578533756874,
 'learning_rate_Hydroxylation-K': 0.009480347909307126,
 'learning_rate_Hydroxylation-P': 0.00579229304990063,
 'log_base': 1.7357357758032335,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2057974116,
 'sample_weights': [1.6122567112607638, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.3194369599264673,
 'weight_decay_Hydroxylation-K': 8.219057656221052,
 'weight_decay_Hydroxylation-P': 0.8643036303699141}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1550.278
[2,     1] loss: 1547.036
[3,     1] loss: 1538.388
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012286652603392138,
 'learning_rate_Hydroxylation-K': 0.0027291106809495353,
 'learning_rate_Hydroxylation-P': 0.005862263261600623,
 'log_base': 1.133032975348959,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4091968189,
 'sample_weights': [3.027472033778776, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.908587978466514,
 'weight_decay_Hydroxylation-K': 1.3002091260415958,
 'weight_decay_Hydroxylation-P': 3.3820482417593736}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4336.986
[2,     1] loss: 4354.300
[3,     1] loss: 4330.936
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007841706097744969,
 'learning_rate_Hydroxylation-K': 0.007331736533563437,
 'learning_rate_Hydroxylation-P': 0.004421808308616743,
 'log_base': 2.744199062932572,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1628378120,
 'sample_weights': [13.366442992351695, 1.6708697401763046],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.775143501502082,
 'weight_decay_Hydroxylation-K': 5.029753192892217,
 'weight_decay_Hydroxylation-P': 3.462339587328689}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.763
[2,     1] loss: 1259.350
[3,     1] loss: 1254.592
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009870954323825327,
 'learning_rate_Hydroxylation-K': 0.0057460480695708,
 'learning_rate_Hydroxylation-P': 0.00684511423215361,
 'log_base': 2.953147867480584,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1842683403,
 'sample_weights': [1.6537502940836435, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.9924367358648905,
 'weight_decay_Hydroxylation-K': 8.125410528008928,
 'weight_decay_Hydroxylation-P': 9.692840176072359}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.996
[2,     1] loss: 1238.133
[3,     1] loss: 1237.606
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002885486621806575,
 'learning_rate_Hydroxylation-K': 0.005245004010795987,
 'learning_rate_Hydroxylation-P': 0.006351785663578695,
 'log_base': 1.6563165735775325,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2700332551,
 'sample_weights': [1.541681425164227, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.336163255871305,
 'weight_decay_Hydroxylation-K': 1.1643068789912019,
 'weight_decay_Hydroxylation-P': 1.7698675305516738}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1610.680
[2,     1] loss: 1604.159
[3,     1] loss: 1606.201
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005304473558261388,
 'learning_rate_Hydroxylation-K': 0.005913940919644263,
 'learning_rate_Hydroxylation-P': 0.0061683669498019044,
 'log_base': 1.0633511873412325,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4009863187,
 'sample_weights': [3.3084734483703317, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9546409814817087,
 'weight_decay_Hydroxylation-K': 6.029673815818537,
 'weight_decay_Hydroxylation-P': 7.5625256880509415}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8816.260
[2,     1] loss: 8824.039
[3,     1] loss: 8829.418
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004735520812870162,
 'learning_rate_Hydroxylation-K': 0.006034486777741361,
 'learning_rate_Hydroxylation-P': 0.0015071421141056415,
 'log_base': 1.139245476935419,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1481051520,
 'sample_weights': [27.178376407121632, 3.397427928414889],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.9718086334704,
 'weight_decay_Hydroxylation-K': 2.7346983836239285,
 'weight_decay_Hydroxylation-P': 9.995102668822668}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4153.719
[2,     1] loss: 4188.352
[3,     1] loss: 4145.286
[4,     1] loss: 4147.785
[5,     1] loss: 4154.683
[6,     1] loss: 4152.693
[7,     1] loss: 4145.858
[8,     1] loss: 4140.476
[9,     1] loss: 4148.589
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004892110046391627,
 'learning_rate_Hydroxylation-K': 0.004456685207203122,
 'learning_rate_Hydroxylation-P': 0.008664411706941363,
 'log_base': 1.170161130478565,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 355964987,
 'sample_weights': [12.805799290638438, 1.6007865776813066],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.82491494465475,
 'weight_decay_Hydroxylation-K': 3.4497794257949774,
 'weight_decay_Hydroxylation-P': 0.0834879596092235}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3456.033
[2,     1] loss: 3436.917
[3,     1] loss: 3463.645
[4,     1] loss: 3443.625
[5,     1] loss: 3441.134
[6,     1] loss: 3451.455
[7,     1] loss: 3439.860
[8,     1] loss: 3456.277
[9,     1] loss: 3434.502
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024016015857072845,
 'learning_rate_Hydroxylation-K': 0.000598381864695897,
 'learning_rate_Hydroxylation-P': 0.005309260766685565,
 'log_base': 1.7659545097854308,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 932819313,
 'sample_weights': [10.62382373192061, 1.3280291255340124],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.783825035039868,
 'weight_decay_Hydroxylation-K': 6.377630108317587,
 'weight_decay_Hydroxylation-P': 1.48300975674438}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1530.050
[2,     1] loss: 1528.400
[3,     1] loss: 1526.997
[4,     1] loss: 1523.358
[5,     1] loss: 1525.878
[6,     1] loss: 1529.400
[7,     1] loss: 1515.749
[8,     1] loss: 1512.137
[9,     1] loss: 1512.047
[10,     1] loss: 1496.852
[11,     1] loss: 1476.620
[12,     1] loss: 1457.580
[13,     1] loss: 1429.653
[14,     1] loss: 1405.402
[15,     1] loss: 1372.748
[16,     1] loss: 1343.417
[17,     1] loss: 1360.913
[18,     1] loss: 1308.050
[19,     1] loss: 1279.265
[20,     1] loss: 1321.378
[21,     1] loss: 1303.935
[22,     1] loss: 1306.185
[23,     1] loss: 1279.522
[24,     1] loss: 1271.784
[25,     1] loss: 1234.877
[26,     1] loss: 1252.638
[27,     1] loss: 1202.230
[28,     1] loss: 1247.410
[29,     1] loss: 1244.908
[30,     1] loss: 1249.381
[31,     1] loss: 1273.608
[32,     1] loss: 1253.983
[33,     1] loss: 1231.755
[34,     1] loss: 1239.311
[35,     1] loss: 1225.730
[36,     1] loss: 1192.906
[37,     1] loss: 1166.892
[38,     1] loss: 1189.198
[39,     1] loss: 1155.994
[40,     1] loss: 1124.912
[41,     1] loss: 1139.523
[42,     1] loss: 1115.311
[43,     1] loss: 1139.382
[44,     1] loss: 1070.750
[45,     1] loss: 1053.257
[46,     1] loss: 1105.143
[47,     1] loss: 1030.680
[48,     1] loss: 1006.188
[49,     1] loss: 996.411
[50,     1] loss: 1053.480
[51,     1] loss: 1025.553
[52,     1] loss: 1184.638
[53,     1] loss: 1033.636
[54,     1] loss: 1050.381
[55,     1] loss: 1065.891
[56,     1] loss: 1017.957
[57,     1] loss: 1012.234
[58,     1] loss: 980.995
[59,     1] loss: 937.333
[60,     1] loss: 909.479
[61,     1] loss: 920.880
[62,     1] loss: 928.706
[63,     1] loss: 894.184
[64,     1] loss: 855.462
[65,     1] loss: 860.356
[66,     1] loss: 962.270
[67,     1] loss: 853.372
[68,     1] loss: 985.073
[69,     1] loss: 880.785
[70,     1] loss: 1049.792
[71,     1] loss: 787.831
[72,     1] loss: 1126.369
[73,     1] loss: 819.943
[74,     1] loss: 920.186
[75,     1] loss: 1019.122
[76,     1] loss: 827.150
[77,     1] loss: 952.480
[78,     1] loss: 899.103
[79,     1] loss: 824.721
[80,     1] loss: 929.242
[81,     1] loss: 814.631
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035165368313106837,
 'learning_rate_Hydroxylation-K': 0.008240694689365684,
 'learning_rate_Hydroxylation-P': 0.00851105193027209,
 'log_base': 1.0718038591120416,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 762627929,
 'sample_weights': [2.935587411545563, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.360442001337989,
 'weight_decay_Hydroxylation-K': 0.24364817079903123,
 'weight_decay_Hydroxylation-P': 1.8252195619598524}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7824.378
[2,     1] loss: 7821.148
[3,     1] loss: 7749.787
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029250404114666406,
 'learning_rate_Hydroxylation-K': 0.0002333988301103096,
 'learning_rate_Hydroxylation-P': 0.006753312816597228,
 'log_base': 1.7940456116192751,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4146376649,
 'sample_weights': [24.075122991944365, 3.009506307794847],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.402186109763586,
 'weight_decay_Hydroxylation-K': 8.779181623707784,
 'weight_decay_Hydroxylation-P': 6.547834667970014}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1510.467
[2,     1] loss: 1509.310
[3,     1] loss: 1508.104
[4,     1] loss: 1516.338
[5,     1] loss: 1510.825
[6,     1] loss: 1508.427
[7,     1] loss: 1510.289
[8,     1] loss: 1507.439
[9,     1] loss: 1509.784
[10,     1] loss: 1501.560
[11,     1] loss: 1501.211
[12,     1] loss: 1497.082
[13,     1] loss: 1484.548
[14,     1] loss: 1476.912
[15,     1] loss: 1458.261
[16,     1] loss: 1431.472
[17,     1] loss: 1434.673
[18,     1] loss: 1388.195
[19,     1] loss: 1331.200
[20,     1] loss: 1360.951
[21,     1] loss: 1286.357
[22,     1] loss: 1297.094
[23,     1] loss: 1261.958
[24,     1] loss: 1237.028
[25,     1] loss: 1258.905
[26,     1] loss: 1219.672
[27,     1] loss: 1279.062
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00882220842347052,
 'learning_rate_Hydroxylation-K': 0.00026320184682479715,
 'learning_rate_Hydroxylation-P': 0.00954756868709302,
 'log_base': 2.1629088040558186,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3067672134,
 'sample_weights': [2.856321183121789, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.45042054588261227,
 'weight_decay_Hydroxylation-K': 9.787368461749592,
 'weight_decay_Hydroxylation-P': 9.064060108795273}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1368.818
[2,     1] loss: 1379.475
[3,     1] loss: 1369.715
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008836601057008838,
 'learning_rate_Hydroxylation-K': 0.0003740661490427887,
 'learning_rate_Hydroxylation-P': 0.007716823771916521,
 'log_base': 1.8996120229266371,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3341828117,
 'sample_weights': [2.1640216803702375, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.51665588272854,
 'weight_decay_Hydroxylation-K': 6.9713097534109565,
 'weight_decay_Hydroxylation-P': 2.575600532934936}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1453.927
[2,     1] loss: 1463.025
[3,     1] loss: 1458.830
[4,     1] loss: 1454.134
[5,     1] loss: 1457.963
[6,     1] loss: 1451.176
[7,     1] loss: 1447.123
[8,     1] loss: 1439.981
[9,     1] loss: 1435.806
[10,     1] loss: 1406.456
[11,     1] loss: 1366.016
[12,     1] loss: 1352.416
[13,     1] loss: 1330.728
[14,     1] loss: 1276.886
[15,     1] loss: 1327.431
[16,     1] loss: 1229.935
[17,     1] loss: 1292.653
[18,     1] loss: 1202.686
[19,     1] loss: 1192.497
[20,     1] loss: 1239.854
[21,     1] loss: 1174.337
[22,     1] loss: 1212.399
[23,     1] loss: 1164.285
[24,     1] loss: 1186.346
[25,     1] loss: 1139.283
[26,     1] loss: 1108.920
[27,     1] loss: 1197.953
[28,     1] loss: 1114.499
[29,     1] loss: 1147.459
[30,     1] loss: 1123.266
[31,     1] loss: 1085.551
[32,     1] loss: 1050.331
[33,     1] loss: 1077.710
[34,     1] loss: 1065.916
[35,     1] loss: 1014.326
[36,     1] loss: 1120.629
[37,     1] loss: 1167.706
[38,     1] loss: 1078.303
[39,     1] loss: 1057.940
[40,     1] loss: 1010.347
[41,     1] loss: 1040.309
[42,     1] loss: 990.260
[43,     1] loss: 1043.153
[44,     1] loss: 993.157
[45,     1] loss: 1056.183
[46,     1] loss: 977.770
[47,     1] loss: 959.865
[48,     1] loss: 952.366
[49,     1] loss: 910.149
[50,     1] loss: 920.559
[51,     1] loss: 870.691
[52,     1] loss: 1082.302
[53,     1] loss: 1635.370
[54,     1] loss: 849.211
[55,     1] loss: 1310.942
[56,     1] loss: 974.318
[57,     1] loss: 1052.758
[58,     1] loss: 1160.290
[59,     1] loss: 1133.441
[60,     1] loss: 1056.425
[61,     1] loss: 1000.763
[62,     1] loss: 1027.511
[63,     1] loss: 933.680
[64,     1] loss: 909.918
[65,     1] loss: 968.438
[66,     1] loss: 898.328
[67,     1] loss: 881.078
[68,     1] loss: 944.617
[69,     1] loss: 803.928
[70,     1] loss: 940.778
[71,     1] loss: 763.234
[72,     1] loss: 937.126
[73,     1] loss: 858.871
[74,     1] loss: 846.695
[75,     1] loss: 698.213
[76,     1] loss: 832.041
Early stopping applied (best metric=0.3577744662761688)
Finished Training
Total time taken: 12.396098852157593
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1459.543
[2,     1] loss: 1464.969
[3,     1] loss: 1458.526
[4,     1] loss: 1463.426
[5,     1] loss: 1457.068
[6,     1] loss: 1464.642
[7,     1] loss: 1459.542
[8,     1] loss: 1457.490
[9,     1] loss: 1456.335
[10,     1] loss: 1452.502
[11,     1] loss: 1454.915
[12,     1] loss: 1456.939
[13,     1] loss: 1454.433
[14,     1] loss: 1453.258
[15,     1] loss: 1449.940
[16,     1] loss: 1443.143
[17,     1] loss: 1443.564
[18,     1] loss: 1437.416
[19,     1] loss: 1414.336
[20,     1] loss: 1397.184
[21,     1] loss: 1364.769
[22,     1] loss: 1339.308
[23,     1] loss: 1315.002
[24,     1] loss: 1240.406
[25,     1] loss: 1244.073
[26,     1] loss: 1312.886
[27,     1] loss: 1248.093
[28,     1] loss: 1275.245
[29,     1] loss: 1176.846
[30,     1] loss: 1194.250
[31,     1] loss: 1228.252
[32,     1] loss: 1187.405
[33,     1] loss: 1147.414
[34,     1] loss: 1133.076
[35,     1] loss: 1186.873
[36,     1] loss: 1141.992
[37,     1] loss: 1136.802
[38,     1] loss: 1122.219
[39,     1] loss: 1102.649
[40,     1] loss: 1092.513
[41,     1] loss: 1057.289
[42,     1] loss: 1097.190
[43,     1] loss: 1052.765
[44,     1] loss: 1104.399
[45,     1] loss: 1096.950
[46,     1] loss: 1088.290
[47,     1] loss: 990.813
[48,     1] loss: 961.100
[49,     1] loss: 928.076
[50,     1] loss: 907.903
[51,     1] loss: 925.958
[52,     1] loss: 812.493
[53,     1] loss: 870.289
[54,     1] loss: 882.021
[55,     1] loss: 2180.068
[56,     1] loss: 905.395
[57,     1] loss: 1139.339
[58,     1] loss: 1010.368
[59,     1] loss: 1136.673
[60,     1] loss: 1176.033
[61,     1] loss: 1049.258
[62,     1] loss: 1078.848
[63,     1] loss: 1065.217
[64,     1] loss: 1091.760
[65,     1] loss: 1019.190
[66,     1] loss: 1037.703
[67,     1] loss: 1017.497
[68,     1] loss: 910.820
[69,     1] loss: 997.435
[70,     1] loss: 893.408
[71,     1] loss: 891.807
[72,     1] loss: 869.676
[73,     1] loss: 899.599
[74,     1] loss: 834.515
[75,     1] loss: 755.676
[76,     1] loss: 875.626
[77,     1] loss: 759.595
[78,     1] loss: 728.815
[79,     1] loss: 811.475
[80,     1] loss: 767.157
[81,     1] loss: 661.267
[82,     1] loss: 839.407
[83,     1] loss: 906.489
[84,     1] loss: 606.345
[85,     1] loss: 788.604
[86,     1] loss: 875.905
[87,     1] loss: 657.349
[88,     1] loss: 822.997
[89,     1] loss: 677.149
[90,     1] loss: 782.890
[91,     1] loss: 733.082
[92,     1] loss: 640.001
Early stopping applied (best metric=0.3215266466140747)
Finished Training
Total time taken: 15.432167053222656
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1455.291
[2,     1] loss: 1468.343
[3,     1] loss: 1458.302
[4,     1] loss: 1459.379
[5,     1] loss: 1463.418
[6,     1] loss: 1456.198
[7,     1] loss: 1453.501
[8,     1] loss: 1461.015
[9,     1] loss: 1456.092
[10,     1] loss: 1457.824
[11,     1] loss: 1454.441
[12,     1] loss: 1455.399
[13,     1] loss: 1458.298
[14,     1] loss: 1456.375
[15,     1] loss: 1452.357
[16,     1] loss: 1451.815
[17,     1] loss: 1450.672
[18,     1] loss: 1450.814
[19,     1] loss: 1447.130
[20,     1] loss: 1431.241
[21,     1] loss: 1432.682
[22,     1] loss: 1400.595
[23,     1] loss: 1376.380
[24,     1] loss: 1341.458
[25,     1] loss: 1304.269
[26,     1] loss: 1311.623
[27,     1] loss: 1251.895
[28,     1] loss: 1342.596
[29,     1] loss: 1230.614
[30,     1] loss: 1191.282
[31,     1] loss: 1255.837
[32,     1] loss: 1219.919
[33,     1] loss: 1212.604
[34,     1] loss: 1212.931
[35,     1] loss: 1174.458
[36,     1] loss: 1180.151
[37,     1] loss: 1143.147
[38,     1] loss: 1147.333
[39,     1] loss: 1083.471
[40,     1] loss: 1169.487
[41,     1] loss: 1078.921
[42,     1] loss: 1066.375
[43,     1] loss: 1046.022
[44,     1] loss: 1071.733
[45,     1] loss: 1073.604
[46,     1] loss: 1055.643
[47,     1] loss: 1021.244
[48,     1] loss: 1035.980
[49,     1] loss: 1017.048
[50,     1] loss: 1072.803
[51,     1] loss: 977.493
[52,     1] loss: 1080.193
[53,     1] loss: 1199.860
[54,     1] loss: 1008.154
[55,     1] loss: 1072.438
[56,     1] loss: 994.285
[57,     1] loss: 992.956
[58,     1] loss: 950.735
[59,     1] loss: 949.393
[60,     1] loss: 949.744
[61,     1] loss: 995.890
[62,     1] loss: 951.186
[63,     1] loss: 841.065
[64,     1] loss: 936.193
[65,     1] loss: 800.530
[66,     1] loss: 835.121
[67,     1] loss: 995.687
[68,     1] loss: 816.822
[69,     1] loss: 843.064
[70,     1] loss: 832.109
[71,     1] loss: 824.199
[72,     1] loss: 733.221
[73,     1] loss: 840.971
[74,     1] loss: 1098.808
[75,     1] loss: 1405.999
[76,     1] loss: 800.529
[77,     1] loss: 1096.163
[78,     1] loss: 997.968
[79,     1] loss: 946.837
[80,     1] loss: 1022.225
[81,     1] loss: 946.157
[82,     1] loss: 918.774
[83,     1] loss: 838.215
[84,     1] loss: 905.236
[85,     1] loss: 798.543
[86,     1] loss: 854.668
[87,     1] loss: 868.391
[88,     1] loss: 743.290
[89,     1] loss: 732.244
[90,     1] loss: 684.846
[91,     1] loss: 636.974
[92,     1] loss: 764.254
[93,     1] loss: 1062.014
[94,     1] loss: 1164.704
[95,     1] loss: 694.807
[96,     1] loss: 1017.083
[97,     1] loss: 736.010
[98,     1] loss: 786.592
[99,     1] loss: 929.646
[100,     1] loss: 693.183
[101,     1] loss: 818.374
[102,     1] loss: 633.399
[103,     1] loss: 741.331
[104,     1] loss: 692.932
[105,     1] loss: 623.556
[106,     1] loss: 590.214
[107,     1] loss: 597.409
[108,     1] loss: 660.535
[109,     1] loss: 696.886
[110,     1] loss: 631.425
[111,     1] loss: 519.700
[112,     1] loss: 530.923
[113,     1] loss: 514.697
Early stopping applied (best metric=0.3145929276943207)
Finished Training
Total time taken: 18.29263210296631
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1454.107
[2,     1] loss: 1462.025
[3,     1] loss: 1458.545
[4,     1] loss: 1453.234
[5,     1] loss: 1461.387
[6,     1] loss: 1444.587
[7,     1] loss: 1452.221
[8,     1] loss: 1438.189
[9,     1] loss: 1424.370
[10,     1] loss: 1408.643
[11,     1] loss: 1372.949
[12,     1] loss: 1361.530
[13,     1] loss: 1280.918
[14,     1] loss: 1264.494
[15,     1] loss: 1311.696
[16,     1] loss: 1187.656
[17,     1] loss: 1261.435
[18,     1] loss: 1175.240
[19,     1] loss: 1183.844
[20,     1] loss: 1172.369
[21,     1] loss: 1162.450
[22,     1] loss: 1248.621
[23,     1] loss: 1105.636
[24,     1] loss: 1195.803
[25,     1] loss: 1157.150
[26,     1] loss: 1193.501
[27,     1] loss: 1175.525
[28,     1] loss: 1137.463
[29,     1] loss: 1171.087
[30,     1] loss: 1102.945
[31,     1] loss: 1129.641
[32,     1] loss: 1136.607
[33,     1] loss: 1073.534
[34,     1] loss: 993.516
[35,     1] loss: 1056.388
[36,     1] loss: 1034.258
[37,     1] loss: 984.945
[38,     1] loss: 1030.833
[39,     1] loss: 1073.346
[40,     1] loss: 995.002
[41,     1] loss: 1049.210
[42,     1] loss: 1139.908
[43,     1] loss: 1000.476
[44,     1] loss: 1127.752
[45,     1] loss: 952.662
[46,     1] loss: 973.617
[47,     1] loss: 948.354
[48,     1] loss: 925.676
[49,     1] loss: 927.230
[50,     1] loss: 879.359
[51,     1] loss: 1052.942
[52,     1] loss: 866.030
[53,     1] loss: 994.196
[54,     1] loss: 1030.458
[55,     1] loss: 848.384
[56,     1] loss: 960.268
[57,     1] loss: 855.013
[58,     1] loss: 1001.086
[59,     1] loss: 796.401
[60,     1] loss: 860.614
[61,     1] loss: 784.584
[62,     1] loss: 889.701
[63,     1] loss: 821.171
[64,     1] loss: 672.195
[65,     1] loss: 840.431
[66,     1] loss: 719.081
[67,     1] loss: 675.098
[68,     1] loss: 681.998
[69,     1] loss: 784.998
[70,     1] loss: 1166.748
[71,     1] loss: 1023.794
[72,     1] loss: 755.696
[73,     1] loss: 924.779
[74,     1] loss: 860.520
[75,     1] loss: 740.059
[76,     1] loss: 883.326
[77,     1] loss: 742.247
[78,     1] loss: 729.053
[79,     1] loss: 611.241
[80,     1] loss: 769.226
[81,     1] loss: 660.504
[82,     1] loss: 579.337
[83,     1] loss: 635.948
[84,     1] loss: 520.971
[85,     1] loss: 564.068
[86,     1] loss: 536.896
[87,     1] loss: 530.896
[88,     1] loss: 538.195
[89,     1] loss: 630.827
[90,     1] loss: 929.831
[91,     1] loss: 724.627
[92,     1] loss: 520.464
[93,     1] loss: 675.024
[94,     1] loss: 540.356
[95,     1] loss: 579.469
[96,     1] loss: 767.193
[97,     1] loss: 483.905
[98,     1] loss: 895.440
[99,     1] loss: 1388.339
[100,     1] loss: 797.093
[101,     1] loss: 848.518
Early stopping applied (best metric=0.3676663339138031)
Finished Training
Total time taken: 15.504709482192993
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1465.120
[2,     1] loss: 1467.593
[3,     1] loss: 1464.809
[4,     1] loss: 1460.314
[5,     1] loss: 1460.580
[6,     1] loss: 1460.097
[7,     1] loss: 1456.488
[8,     1] loss: 1458.294
[9,     1] loss: 1458.049
[10,     1] loss: 1457.107
[11,     1] loss: 1457.515
[12,     1] loss: 1456.281
[13,     1] loss: 1458.493
[14,     1] loss: 1459.077
[15,     1] loss: 1458.599
[16,     1] loss: 1455.566
[17,     1] loss: 1449.568
[18,     1] loss: 1446.784
[19,     1] loss: 1445.153
[20,     1] loss: 1431.538
[21,     1] loss: 1422.777
[22,     1] loss: 1388.885
[23,     1] loss: 1360.672
[24,     1] loss: 1350.912
[25,     1] loss: 1325.042
[26,     1] loss: 1273.812
[27,     1] loss: 1249.807
[28,     1] loss: 1281.345
[29,     1] loss: 1240.119
[30,     1] loss: 1201.088
[31,     1] loss: 1189.259
[32,     1] loss: 1201.472
[33,     1] loss: 1158.744
[34,     1] loss: 1191.662
[35,     1] loss: 1156.976
[36,     1] loss: 1202.430
[37,     1] loss: 1145.469
[38,     1] loss: 1160.148
[39,     1] loss: 1082.325
[40,     1] loss: 1100.175
[41,     1] loss: 1075.226
[42,     1] loss: 1118.496
[43,     1] loss: 1159.235
[44,     1] loss: 1066.508
[45,     1] loss: 1167.152
[46,     1] loss: 1060.531
[47,     1] loss: 1089.345
[48,     1] loss: 991.689
[49,     1] loss: 1105.695
[50,     1] loss: 989.345
[51,     1] loss: 1023.017
[52,     1] loss: 939.341
[53,     1] loss: 1005.413
[54,     1] loss: 1053.822
[55,     1] loss: 997.631
[56,     1] loss: 1056.278
[57,     1] loss: 1015.633
[58,     1] loss: 924.353
[59,     1] loss: 1082.634
[60,     1] loss: 936.879
[61,     1] loss: 915.113
[62,     1] loss: 849.464
[63,     1] loss: 852.975
[64,     1] loss: 878.498
[65,     1] loss: 968.318
[66,     1] loss: 841.070
[67,     1] loss: 847.201
[68,     1] loss: 896.602
[69,     1] loss: 803.389
[70,     1] loss: 867.040
[71,     1] loss: 1078.835
[72,     1] loss: 961.407
[73,     1] loss: 823.721
[74,     1] loss: 815.595
[75,     1] loss: 775.619
[76,     1] loss: 854.422
[77,     1] loss: 798.157
[78,     1] loss: 721.213
[79,     1] loss: 1056.284
[80,     1] loss: 1069.764
[81,     1] loss: 975.439
[82,     1] loss: 871.851
[83,     1] loss: 1111.231
[84,     1] loss: 907.953
[85,     1] loss: 969.287
[86,     1] loss: 1008.307
[87,     1] loss: 836.866
[88,     1] loss: 801.714
[89,     1] loss: 851.612
[90,     1] loss: 798.488
[91,     1] loss: 866.186
[92,     1] loss: 692.162
[93,     1] loss: 796.567
[94,     1] loss: 672.239
[95,     1] loss: 741.266
[96,     1] loss: 701.999
[97,     1] loss: 629.952
[98,     1] loss: 646.438
[99,     1] loss: 597.715
Early stopping applied (best metric=0.36499539017677307)
Finished Training
Total time taken: 15.306827306747437
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1455.813
[2,     1] loss: 1458.288
[3,     1] loss: 1457.978
[4,     1] loss: 1461.777
[5,     1] loss: 1454.960
[6,     1] loss: 1459.732
[7,     1] loss: 1454.440
[8,     1] loss: 1453.505
[9,     1] loss: 1456.876
[10,     1] loss: 1455.393
[11,     1] loss: 1453.385
[12,     1] loss: 1450.079
[13,     1] loss: 1446.702
[14,     1] loss: 1439.616
[15,     1] loss: 1422.887
[16,     1] loss: 1412.325
[17,     1] loss: 1394.910
[18,     1] loss: 1347.219
[19,     1] loss: 1322.412
[20,     1] loss: 1260.135
[21,     1] loss: 1239.365
[22,     1] loss: 1173.073
[23,     1] loss: 1233.835
[24,     1] loss: 1178.001
[25,     1] loss: 1187.936
[26,     1] loss: 1154.288
[27,     1] loss: 1138.336
[28,     1] loss: 1175.554
[29,     1] loss: 1158.102
[30,     1] loss: 1161.957
[31,     1] loss: 1152.617
[32,     1] loss: 1078.709
[33,     1] loss: 1131.647
[34,     1] loss: 1181.557
[35,     1] loss: 1064.659
[36,     1] loss: 1089.579
[37,     1] loss: 1067.041
[38,     1] loss: 1069.897
[39,     1] loss: 1058.534
[40,     1] loss: 1037.768
[41,     1] loss: 1108.816
[42,     1] loss: 1053.292
[43,     1] loss: 1106.148
[44,     1] loss: 999.610
[45,     1] loss: 1053.154
[46,     1] loss: 1132.231
[47,     1] loss: 964.281
[48,     1] loss: 1025.961
[49,     1] loss: 967.227
[50,     1] loss: 1051.931
[51,     1] loss: 976.333
[52,     1] loss: 971.089
[53,     1] loss: 886.676
[54,     1] loss: 995.615
[55,     1] loss: 843.195
[56,     1] loss: 920.272
[57,     1] loss: 858.734
[58,     1] loss: 875.950
[59,     1] loss: 963.015
[60,     1] loss: 804.144
[61,     1] loss: 872.475
[62,     1] loss: 970.544
[63,     1] loss: 905.042
[64,     1] loss: 807.646
[65,     1] loss: 836.645
[66,     1] loss: 810.626
[67,     1] loss: 810.520
[68,     1] loss: 806.880
[69,     1] loss: 745.173
[70,     1] loss: 749.195
[71,     1] loss: 738.772
[72,     1] loss: 925.227
[73,     1] loss: 1741.625
[74,     1] loss: 849.349
[75,     1] loss: 1181.556
[76,     1] loss: 928.253
[77,     1] loss: 1103.697
[78,     1] loss: 1144.067
[79,     1] loss: 1127.703
[80,     1] loss: 1036.401
[81,     1] loss: 1060.691
[82,     1] loss: 1020.355
[83,     1] loss: 1031.076
[84,     1] loss: 984.782
[85,     1] loss: 925.920
[86,     1] loss: 952.222
[87,     1] loss: 833.613
[88,     1] loss: 986.153
[89,     1] loss: 750.897
[90,     1] loss: 888.160
[91,     1] loss: 807.416
[92,     1] loss: 785.817
[93,     1] loss: 690.618
[94,     1] loss: 803.191
[95,     1] loss: 662.287
[96,     1] loss: 723.093
[97,     1] loss: 621.815
[98,     1] loss: 641.336
[99,     1] loss: 625.128
[100,     1] loss: 782.285
[101,     1] loss: 914.021
[102,     1] loss: 1673.757
[103,     1] loss: 760.336
[104,     1] loss: 1232.872
[105,     1] loss: 907.538
[106,     1] loss: 1008.854
[107,     1] loss: 1040.982
[108,     1] loss: 975.459
[109,     1] loss: 908.662
[110,     1] loss: 1085.192
[111,     1] loss: 930.800
[112,     1] loss: 973.440
[113,     1] loss: 891.390
[114,     1] loss: 912.815
[115,     1] loss: 847.276
[116,     1] loss: 879.173
[117,     1] loss: 856.572
[118,     1] loss: 785.929
[119,     1] loss: 794.810
[120,     1] loss: 718.028
[121,     1] loss: 705.182
[122,     1] loss: 686.985
[123,     1] loss: 722.317
[124,     1] loss: 680.047
[125,     1] loss: 710.884
[126,     1] loss: 600.357
[127,     1] loss: 626.969
[128,     1] loss: 987.806
[129,     1] loss: 2233.782
[130,     1] loss: 807.711
[131,     1] loss: 1253.120
[132,     1] loss: 1099.487
[133,     1] loss: 1125.456
[134,     1] loss: 1122.130
[135,     1] loss: 1041.280
[136,     1] loss: 988.752
[137,     1] loss: 1013.290
[138,     1] loss: 994.800
[139,     1] loss: 1042.900
[140,     1] loss: 948.262
[141,     1] loss: 892.926
[142,     1] loss: 907.368
[143,     1] loss: 1035.931
[144,     1] loss: 881.749
[145,     1] loss: 957.811
[146,     1] loss: 869.913
[147,     1] loss: 832.318
[148,     1] loss: 822.029
[149,     1] loss: 870.862
[150,     1] loss: 792.687
[151,     1] loss: 781.464
[152,     1] loss: 773.327
Early stopping applied (best metric=0.3500324487686157)
Finished Training
Total time taken: 23.331424713134766
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1459.345
[2,     1] loss: 1459.031
[3,     1] loss: 1454.386
[4,     1] loss: 1454.794
[5,     1] loss: 1459.279
[6,     1] loss: 1456.523
[7,     1] loss: 1454.974
[8,     1] loss: 1454.428
[9,     1] loss: 1450.556
[10,     1] loss: 1448.572
[11,     1] loss: 1448.224
[12,     1] loss: 1438.496
[13,     1] loss: 1428.195
[14,     1] loss: 1409.795
[15,     1] loss: 1375.146
[16,     1] loss: 1336.319
[17,     1] loss: 1291.644
[18,     1] loss: 1271.510
[19,     1] loss: 1278.045
[20,     1] loss: 1275.669
[21,     1] loss: 1192.879
[22,     1] loss: 1279.186
[23,     1] loss: 1205.757
[24,     1] loss: 1182.218
[25,     1] loss: 1152.433
[26,     1] loss: 1160.250
[27,     1] loss: 1141.018
[28,     1] loss: 1156.453
[29,     1] loss: 1175.471
[30,     1] loss: 1145.829
[31,     1] loss: 1186.811
[32,     1] loss: 1125.981
[33,     1] loss: 1158.644
[34,     1] loss: 1077.001
[35,     1] loss: 1109.732
[36,     1] loss: 1097.407
[37,     1] loss: 1042.682
[38,     1] loss: 1054.587
[39,     1] loss: 1076.341
[40,     1] loss: 989.773
[41,     1] loss: 1057.589
[42,     1] loss: 1049.271
[43,     1] loss: 1038.559
[44,     1] loss: 1003.271
[45,     1] loss: 974.533
[46,     1] loss: 1029.626
[47,     1] loss: 1007.455
[48,     1] loss: 1001.664
[49,     1] loss: 1038.036
[50,     1] loss: 959.469
[51,     1] loss: 996.062
[52,     1] loss: 1054.426
[53,     1] loss: 920.335
[54,     1] loss: 911.979
[55,     1] loss: 876.308
[56,     1] loss: 994.107
[57,     1] loss: 920.874
[58,     1] loss: 829.467
[59,     1] loss: 861.337
[60,     1] loss: 828.117
[61,     1] loss: 773.481
[62,     1] loss: 816.246
[63,     1] loss: 779.893
[64,     1] loss: 748.320
[65,     1] loss: 715.107
[66,     1] loss: 921.579
[67,     1] loss: 2057.736
[68,     1] loss: 905.597
[69,     1] loss: 1142.170
[70,     1] loss: 971.340
[71,     1] loss: 1106.181
[72,     1] loss: 1156.104
[73,     1] loss: 1156.148
[74,     1] loss: 1075.100
[75,     1] loss: 1013.681
[76,     1] loss: 1064.098
[77,     1] loss: 978.615
[78,     1] loss: 932.309
[79,     1] loss: 959.206
[80,     1] loss: 873.865
[81,     1] loss: 938.801
[82,     1] loss: 849.604
[83,     1] loss: 902.215
[84,     1] loss: 805.431
[85,     1] loss: 849.951
[86,     1] loss: 770.368
[87,     1] loss: 788.812
[88,     1] loss: 801.815
[89,     1] loss: 701.380
[90,     1] loss: 736.474
[91,     1] loss: 797.583
[92,     1] loss: 813.285
[93,     1] loss: 737.357
[94,     1] loss: 649.742
[95,     1] loss: 714.237
[96,     1] loss: 838.508
[97,     1] loss: 1336.760
[98,     1] loss: 617.006
[99,     1] loss: 1160.233
[100,     1] loss: 796.899
[101,     1] loss: 1090.550
[102,     1] loss: 862.360
[103,     1] loss: 802.436
[104,     1] loss: 912.669
Early stopping applied (best metric=0.3735181987285614)
Finished Training
Total time taken: 16.139886617660522
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1456.000
[2,     1] loss: 1462.072
[3,     1] loss: 1467.921
[4,     1] loss: 1456.543
[5,     1] loss: 1454.792
[6,     1] loss: 1455.962
[7,     1] loss: 1453.212
[8,     1] loss: 1451.817
[9,     1] loss: 1450.870
[10,     1] loss: 1451.260
[11,     1] loss: 1444.338
[12,     1] loss: 1427.344
[13,     1] loss: 1416.895
[14,     1] loss: 1372.997
[15,     1] loss: 1341.215
[16,     1] loss: 1335.583
[17,     1] loss: 1238.080
[18,     1] loss: 1279.961
[19,     1] loss: 1194.845
[20,     1] loss: 1217.199
[21,     1] loss: 1214.587
[22,     1] loss: 1242.294
[23,     1] loss: 1261.013
[24,     1] loss: 1185.428
[25,     1] loss: 1262.813
[26,     1] loss: 1228.735
[27,     1] loss: 1154.660
[28,     1] loss: 1244.466
[29,     1] loss: 1159.639
[30,     1] loss: 1109.288
[31,     1] loss: 1150.016
[32,     1] loss: 1144.416
[33,     1] loss: 1173.068
[34,     1] loss: 1131.190
[35,     1] loss: 1093.339
[36,     1] loss: 1148.836
[37,     1] loss: 1128.104
[38,     1] loss: 1045.278
[39,     1] loss: 1078.947
[40,     1] loss: 1016.496
[41,     1] loss: 1081.516
[42,     1] loss: 1023.394
[43,     1] loss: 996.584
[44,     1] loss: 945.833
[45,     1] loss: 996.788
[46,     1] loss: 966.411
[47,     1] loss: 976.908
[48,     1] loss: 931.078
[49,     1] loss: 1007.104
[50,     1] loss: 1632.263
[51,     1] loss: 846.235
[52,     1] loss: 1223.364
[53,     1] loss: 1011.542
[54,     1] loss: 1046.132
[55,     1] loss: 1161.788
[56,     1] loss: 1057.695
[57,     1] loss: 995.070
[58,     1] loss: 1056.467
[59,     1] loss: 1079.866
[60,     1] loss: 927.920
[61,     1] loss: 949.860
[62,     1] loss: 916.887
[63,     1] loss: 878.104
[64,     1] loss: 830.729
[65,     1] loss: 847.398
[66,     1] loss: 897.940
[67,     1] loss: 827.671
[68,     1] loss: 928.559
[69,     1] loss: 774.058
[70,     1] loss: 897.602
[71,     1] loss: 747.663
[72,     1] loss: 829.297
[73,     1] loss: 754.893
[74,     1] loss: 816.932
[75,     1] loss: 642.221
[76,     1] loss: 806.804
[77,     1] loss: 844.530
[78,     1] loss: 601.818
[79,     1] loss: 844.927
[80,     1] loss: 699.121
[81,     1] loss: 672.809
[82,     1] loss: 711.453
[83,     1] loss: 567.296
Early stopping applied (best metric=0.3747091293334961)
Finished Training
Total time taken: 13.41167426109314
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1462.066
[2,     1] loss: 1460.443
[3,     1] loss: 1459.589
[4,     1] loss: 1463.650
[5,     1] loss: 1456.406
[6,     1] loss: 1455.110
[7,     1] loss: 1452.689
[8,     1] loss: 1451.056
[9,     1] loss: 1451.259
[10,     1] loss: 1447.063
[11,     1] loss: 1443.682
[12,     1] loss: 1433.648
[13,     1] loss: 1414.170
[14,     1] loss: 1377.577
[15,     1] loss: 1354.960
[16,     1] loss: 1318.595
[17,     1] loss: 1270.857
[18,     1] loss: 1235.605
[19,     1] loss: 1207.914
[20,     1] loss: 1210.979
[21,     1] loss: 1217.878
[22,     1] loss: 1210.100
[23,     1] loss: 1252.771
[24,     1] loss: 1175.984
[25,     1] loss: 1235.775
[26,     1] loss: 1178.341
[27,     1] loss: 1191.378
[28,     1] loss: 1151.507
[29,     1] loss: 1089.027
[30,     1] loss: 1138.763
[31,     1] loss: 1038.151
[32,     1] loss: 1164.927
[33,     1] loss: 1106.047
[34,     1] loss: 1091.050
[35,     1] loss: 1052.400
[36,     1] loss: 1052.885
[37,     1] loss: 1043.836
[38,     1] loss: 1056.164
[39,     1] loss: 973.032
[40,     1] loss: 1101.719
[41,     1] loss: 1047.801
[42,     1] loss: 1054.132
[43,     1] loss: 969.842
[44,     1] loss: 1013.093
[45,     1] loss: 940.394
[46,     1] loss: 1038.105
[47,     1] loss: 919.317
[48,     1] loss: 1085.119
[49,     1] loss: 1048.968
[50,     1] loss: 931.483
[51,     1] loss: 1010.193
[52,     1] loss: 904.108
[53,     1] loss: 1027.243
[54,     1] loss: 884.166
[55,     1] loss: 981.887
[56,     1] loss: 854.716
[57,     1] loss: 934.060
[58,     1] loss: 936.321
[59,     1] loss: 881.390
[60,     1] loss: 928.295
[61,     1] loss: 815.795
[62,     1] loss: 858.107
[63,     1] loss: 809.498
[64,     1] loss: 858.661
[65,     1] loss: 932.239
[66,     1] loss: 871.665
[67,     1] loss: 771.935
[68,     1] loss: 748.947
[69,     1] loss: 888.762
[70,     1] loss: 765.536
[71,     1] loss: 746.192
[72,     1] loss: 930.094
[73,     1] loss: 690.271
[74,     1] loss: 828.116
[75,     1] loss: 903.911
[76,     1] loss: 775.861
[77,     1] loss: 750.961
[78,     1] loss: 805.715
[79,     1] loss: 755.048
[80,     1] loss: 704.429
[81,     1] loss: 611.712
[82,     1] loss: 671.902
[83,     1] loss: 878.569
[84,     1] loss: 709.798
[85,     1] loss: 634.244
[86,     1] loss: 724.621
[87,     1] loss: 626.772
[88,     1] loss: 1089.450
[89,     1] loss: 715.012
[90,     1] loss: 930.949
[91,     1] loss: 669.659
[92,     1] loss: 896.887
[93,     1] loss: 672.579
[94,     1] loss: 705.633
[95,     1] loss: 661.123
[96,     1] loss: 602.157
[97,     1] loss: 632.238
[98,     1] loss: 567.284
[99,     1] loss: 552.413
[100,     1] loss: 581.689
[101,     1] loss: 567.704
[102,     1] loss: 570.894
[103,     1] loss: 595.186
[104,     1] loss: 664.735
Early stopping applied (best metric=0.36709970235824585)
Finished Training
Total time taken: 16.229687929153442
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1459.134
[2,     1] loss: 1460.267
[3,     1] loss: 1467.347
[4,     1] loss: 1455.540
[5,     1] loss: 1463.990
[6,     1] loss: 1458.373
[7,     1] loss: 1458.449
[8,     1] loss: 1455.517
[9,     1] loss: 1457.114
[10,     1] loss: 1454.866
[11,     1] loss: 1449.510
[12,     1] loss: 1452.544
[13,     1] loss: 1432.781
[14,     1] loss: 1419.911
[15,     1] loss: 1398.468
[16,     1] loss: 1371.977
[17,     1] loss: 1332.538
[18,     1] loss: 1270.220
[19,     1] loss: 1267.130
[20,     1] loss: 1233.572
[21,     1] loss: 1238.883
[22,     1] loss: 1167.836
[23,     1] loss: 1182.862
[24,     1] loss: 1197.820
[25,     1] loss: 1122.482
[26,     1] loss: 1230.402
[27,     1] loss: 1172.464
[28,     1] loss: 1119.237
[29,     1] loss: 1166.069
[30,     1] loss: 1114.109
[31,     1] loss: 1080.762
[32,     1] loss: 1112.847
[33,     1] loss: 1131.602
[34,     1] loss: 1089.661
[35,     1] loss: 1101.366
[36,     1] loss: 1069.742
[37,     1] loss: 1078.635
[38,     1] loss: 1091.457
[39,     1] loss: 1026.292
[40,     1] loss: 1041.484
[41,     1] loss: 1035.814
[42,     1] loss: 964.761
[43,     1] loss: 991.604
[44,     1] loss: 998.878
[45,     1] loss: 995.458
[46,     1] loss: 1073.886
[47,     1] loss: 933.821
[48,     1] loss: 916.986
[49,     1] loss: 889.489
[50,     1] loss: 854.572
[51,     1] loss: 797.499
[52,     1] loss: 798.900
[53,     1] loss: 863.921
[54,     1] loss: 1277.241
[55,     1] loss: 2054.645
[56,     1] loss: 879.319
[57,     1] loss: 1171.018
[58,     1] loss: 1259.707
[59,     1] loss: 1188.510
[60,     1] loss: 1201.608
[61,     1] loss: 1177.260
[62,     1] loss: 1157.050
[63,     1] loss: 1149.608
[64,     1] loss: 1136.068
[65,     1] loss: 1110.151
[66,     1] loss: 1081.510
[67,     1] loss: 1073.103
[68,     1] loss: 976.441
[69,     1] loss: 1031.559
[70,     1] loss: 985.984
[71,     1] loss: 945.354
[72,     1] loss: 953.605
[73,     1] loss: 948.125
[74,     1] loss: 915.095
[75,     1] loss: 874.311
[76,     1] loss: 887.172
[77,     1] loss: 831.815
[78,     1] loss: 798.656
[79,     1] loss: 778.289
[80,     1] loss: 759.993
[81,     1] loss: 720.941
[82,     1] loss: 731.230
[83,     1] loss: 840.941
[84,     1] loss: 948.215
[85,     1] loss: 1206.104
[86,     1] loss: 702.192
[87,     1] loss: 985.198
[88,     1] loss: 760.009
Early stopping applied (best metric=0.4240425229072571)
Finished Training
Total time taken: 13.739884614944458
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1455.141
[2,     1] loss: 1472.604
[3,     1] loss: 1454.302
[4,     1] loss: 1458.703
[5,     1] loss: 1457.241
[6,     1] loss: 1459.032
[7,     1] loss: 1459.725
[8,     1] loss: 1454.636
[9,     1] loss: 1455.634
[10,     1] loss: 1458.568
[11,     1] loss: 1455.431
[12,     1] loss: 1455.348
[13,     1] loss: 1453.029
[14,     1] loss: 1456.172
[15,     1] loss: 1460.072
[16,     1] loss: 1458.121
[17,     1] loss: 1456.314
[18,     1] loss: 1453.071
[19,     1] loss: 1452.604
[20,     1] loss: 1451.643
[21,     1] loss: 1452.572
[22,     1] loss: 1446.580
[23,     1] loss: 1440.435
[24,     1] loss: 1428.172
[25,     1] loss: 1415.362
[26,     1] loss: 1390.288
[27,     1] loss: 1369.528
[28,     1] loss: 1323.526
[29,     1] loss: 1334.023
[30,     1] loss: 1259.243
[31,     1] loss: 1219.133
[32,     1] loss: 1230.670
[33,     1] loss: 1222.128
[34,     1] loss: 1251.365
[35,     1] loss: 1259.746
[36,     1] loss: 1198.849
[37,     1] loss: 1187.912
[38,     1] loss: 1201.484
[39,     1] loss: 1160.568
[40,     1] loss: 1185.466
[41,     1] loss: 1161.901
[42,     1] loss: 1131.018
[43,     1] loss: 1158.840
[44,     1] loss: 1108.003
[45,     1] loss: 1112.981
[46,     1] loss: 1085.744
[47,     1] loss: 1147.935
[48,     1] loss: 1183.089
[49,     1] loss: 1181.835
[50,     1] loss: 1096.306
[51,     1] loss: 1107.464
[52,     1] loss: 1134.516
[53,     1] loss: 1022.196
[54,     1] loss: 1126.865
[55,     1] loss: 1041.314
[56,     1] loss: 1090.135
[57,     1] loss: 1003.997
[58,     1] loss: 1017.454
[59,     1] loss: 993.901
[60,     1] loss: 1070.476
[61,     1] loss: 969.219
[62,     1] loss: 1014.891
[63,     1] loss: 932.227
[64,     1] loss: 902.252
[65,     1] loss: 913.645
[66,     1] loss: 848.751
[67,     1] loss: 802.914
[68,     1] loss: 819.041
[69,     1] loss: 899.305
[70,     1] loss: 2435.142
[71,     1] loss: 1006.614
[72,     1] loss: 1417.176
[73,     1] loss: 1206.961
[74,     1] loss: 1120.911
[75,     1] loss: 1242.557
[76,     1] loss: 1297.047
[77,     1] loss: 1331.895
[78,     1] loss: 1313.428
[79,     1] loss: 1288.428
[80,     1] loss: 1241.174
[81,     1] loss: 1214.112
[82,     1] loss: 1219.776
[83,     1] loss: 1207.427
[84,     1] loss: 1199.461
[85,     1] loss: 1154.629
[86,     1] loss: 1116.406
[87,     1] loss: 1083.977
[88,     1] loss: 1143.359
[89,     1] loss: 1036.671
[90,     1] loss: 1098.299
[91,     1] loss: 1021.232
[92,     1] loss: 1025.099
[93,     1] loss: 953.905
[94,     1] loss: 986.594
[95,     1] loss: 904.935
[96,     1] loss: 953.666
[97,     1] loss: 925.791
[98,     1] loss: 839.762
[99,     1] loss: 933.862
[100,     1] loss: 1030.136
[101,     1] loss: 996.669
[102,     1] loss: 868.706
[103,     1] loss: 934.031
[104,     1] loss: 765.499
[105,     1] loss: 909.218
[106,     1] loss: 802.531
[107,     1] loss: 883.996
[108,     1] loss: 836.734
[109,     1] loss: 721.514
[110,     1] loss: 1059.892
[111,     1] loss: 1041.260
[112,     1] loss: 825.287
[113,     1] loss: 1193.198
[114,     1] loss: 742.103
[115,     1] loss: 949.739
[116,     1] loss: 828.176
[117,     1] loss: 933.758
[118,     1] loss: 820.520
[119,     1] loss: 762.537
[120,     1] loss: 741.621
[121,     1] loss: 699.563
[122,     1] loss: 778.373
[123,     1] loss: 777.495
[124,     1] loss: 659.107
[125,     1] loss: 916.465
[126,     1] loss: 689.945
[127,     1] loss: 765.490
[128,     1] loss: 674.018
[129,     1] loss: 681.225
[130,     1] loss: 721.035
[131,     1] loss: 615.780
[132,     1] loss: 764.188
[133,     1] loss: 587.667
[134,     1] loss: 649.981
[135,     1] loss: 778.849
[136,     1] loss: 704.587
[137,     1] loss: 567.760
[138,     1] loss: 598.755
[139,     1] loss: 641.136
[140,     1] loss: 496.943
[141,     1] loss: 499.192
[142,     1] loss: 591.093
[143,     1] loss: 1001.343
[144,     1] loss: 3315.130
[145,     1] loss: 1740.498
[146,     1] loss: 1367.324
[147,     1] loss: 1416.397
[148,     1] loss: 1424.801
[149,     1] loss: 1416.048
[150,     1] loss: 1386.747
[151,     1] loss: 1401.933
[152,     1] loss: 1363.123
[153,     1] loss: 1347.931
[154,     1] loss: 1426.437
[155,     1] loss: 1428.047
[156,     1] loss: 1449.102
[157,     1] loss: 1453.986
[158,     1] loss: 1451.589
Early stopping applied (best metric=0.31953367590904236)
Finished Training
Total time taken: 24.392590761184692
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1453.102
[2,     1] loss: 1480.258
[3,     1] loss: 1451.621
[4,     1] loss: 1485.034
[5,     1] loss: 1462.750
[6,     1] loss: 1455.733
[7,     1] loss: 1454.923
[8,     1] loss: 1457.701
[9,     1] loss: 1456.971
[10,     1] loss: 1453.602
[11,     1] loss: 1459.018
[12,     1] loss: 1457.683
[13,     1] loss: 1453.967
[14,     1] loss: 1457.319
[15,     1] loss: 1456.302
[16,     1] loss: 1457.791
[17,     1] loss: 1454.917
[18,     1] loss: 1455.315
[19,     1] loss: 1453.872
[20,     1] loss: 1454.969
[21,     1] loss: 1453.919
[22,     1] loss: 1455.065
[23,     1] loss: 1452.394
[24,     1] loss: 1453.800
[25,     1] loss: 1454.009
[26,     1] loss: 1451.039
[27,     1] loss: 1443.088
[28,     1] loss: 1431.778
[29,     1] loss: 1429.881
[30,     1] loss: 1398.869
[31,     1] loss: 1378.696
[32,     1] loss: 1331.764
[33,     1] loss: 1300.715
[34,     1] loss: 1300.869
[35,     1] loss: 1315.771
[36,     1] loss: 1249.994
[37,     1] loss: 1221.788
[38,     1] loss: 1266.789
[39,     1] loss: 1175.251
[40,     1] loss: 1260.994
[41,     1] loss: 1146.289
[42,     1] loss: 1177.654
[43,     1] loss: 1136.973
[44,     1] loss: 1201.282
[45,     1] loss: 1115.201
[46,     1] loss: 1106.568
[47,     1] loss: 1058.921
[48,     1] loss: 1162.611
[49,     1] loss: 1074.936
[50,     1] loss: 1067.216
[51,     1] loss: 1171.840
[52,     1] loss: 1089.149
[53,     1] loss: 1098.479
[54,     1] loss: 1083.811
[55,     1] loss: 1045.247
[56,     1] loss: 1028.989
[57,     1] loss: 1017.523
[58,     1] loss: 924.496
[59,     1] loss: 969.968
[60,     1] loss: 990.207
[61,     1] loss: 1006.972
[62,     1] loss: 1031.375
[63,     1] loss: 1013.876
[64,     1] loss: 905.937
[65,     1] loss: 892.030
[66,     1] loss: 986.867
[67,     1] loss: 897.918
[68,     1] loss: 946.459
[69,     1] loss: 1130.016
[70,     1] loss: 893.719
[71,     1] loss: 934.556
[72,     1] loss: 916.158
[73,     1] loss: 844.267
[74,     1] loss: 948.071
[75,     1] loss: 865.192
[76,     1] loss: 955.515
[77,     1] loss: 885.259
[78,     1] loss: 874.667
[79,     1] loss: 917.078
[80,     1] loss: 779.824
[81,     1] loss: 896.873
[82,     1] loss: 775.985
[83,     1] loss: 731.308
[84,     1] loss: 862.039
[85,     1] loss: 892.717
[86,     1] loss: 710.938
[87,     1] loss: 792.812
[88,     1] loss: 746.646
[89,     1] loss: 687.585
[90,     1] loss: 763.575
[91,     1] loss: 831.913
[92,     1] loss: 695.010
[93,     1] loss: 928.344
[94,     1] loss: 1243.491
[95,     1] loss: 722.527
[96,     1] loss: 915.591
[97,     1] loss: 834.965
[98,     1] loss: 777.450
[99,     1] loss: 823.919
[100,     1] loss: 751.704
[101,     1] loss: 876.261
[102,     1] loss: 630.346
[103,     1] loss: 717.816
[104,     1] loss: 665.083
[105,     1] loss: 669.096
[106,     1] loss: 619.507
[107,     1] loss: 551.117
[108,     1] loss: 623.497
[109,     1] loss: 699.205
[110,     1] loss: 1033.705
[111,     1] loss: 1830.303
[112,     1] loss: 787.968
[113,     1] loss: 1038.796
[114,     1] loss: 1069.397
[115,     1] loss: 967.591
[116,     1] loss: 1039.716
[117,     1] loss: 1021.437
[118,     1] loss: 895.461
[119,     1] loss: 1012.042
[120,     1] loss: 904.419
[121,     1] loss: 915.044
[122,     1] loss: 839.994
[123,     1] loss: 878.085
[124,     1] loss: 740.178
[125,     1] loss: 741.706
[126,     1] loss: 725.703
[127,     1] loss: 638.274
[128,     1] loss: 620.382
[129,     1] loss: 569.362
[130,     1] loss: 584.284
[131,     1] loss: 867.652
[132,     1] loss: 2291.817
[133,     1] loss: 2260.474
[134,     1] loss: 1627.514
[135,     1] loss: 1342.288
[136,     1] loss: 1440.821
[137,     1] loss: 1454.406
[138,     1] loss: 1456.717
[139,     1] loss: 1455.959
[140,     1] loss: 1456.145
[141,     1] loss: 1456.310
[142,     1] loss: 1455.848
[143,     1] loss: 1456.639
[144,     1] loss: 1456.613
[145,     1] loss: 1456.216
[146,     1] loss: 1456.439
[147,     1] loss: 1455.663
[148,     1] loss: 1456.192
[149,     1] loss: 1456.475
[150,     1] loss: 1455.573
[151,     1] loss: 1454.262
[152,     1] loss: 1454.257
[153,     1] loss: 1454.414
[154,     1] loss: 1452.396
[155,     1] loss: 1452.969
[156,     1] loss: 1449.882
[157,     1] loss: 1448.292
[158,     1] loss: 1446.213
[159,     1] loss: 1434.122
[160,     1] loss: 1434.351
[161,     1] loss: 1429.774
[162,     1] loss: 1397.464
[163,     1] loss: 1488.090
[164,     1] loss: 1444.705
[165,     1] loss: 1456.310
[166,     1] loss: 1456.568
[167,     1] loss: 1456.125
Early stopping applied (best metric=0.34576165676116943)
Finished Training
Total time taken: 26.764328718185425
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1462.398
[2,     1] loss: 1462.974
[3,     1] loss: 1458.870
[4,     1] loss: 1456.672
[5,     1] loss: 1453.755
[6,     1] loss: 1454.205
[7,     1] loss: 1454.740
[8,     1] loss: 1456.168
[9,     1] loss: 1455.017
[10,     1] loss: 1458.862
[11,     1] loss: 1450.261
[12,     1] loss: 1452.260
[13,     1] loss: 1449.731
[14,     1] loss: 1437.996
[15,     1] loss: 1439.400
[16,     1] loss: 1415.770
[17,     1] loss: 1392.088
[18,     1] loss: 1341.002
[19,     1] loss: 1313.391
[20,     1] loss: 1299.333
[21,     1] loss: 1254.441
[22,     1] loss: 1240.222
[23,     1] loss: 1268.248
[24,     1] loss: 1178.678
[25,     1] loss: 1224.723
[26,     1] loss: 1143.861
[27,     1] loss: 1122.543
[28,     1] loss: 1110.675
[29,     1] loss: 1159.304
[30,     1] loss: 1120.973
[31,     1] loss: 1111.266
[32,     1] loss: 1096.084
[33,     1] loss: 1100.853
[34,     1] loss: 1102.090
[35,     1] loss: 1047.898
[36,     1] loss: 1067.161
[37,     1] loss: 1076.608
[38,     1] loss: 997.309
[39,     1] loss: 1096.613
[40,     1] loss: 1091.314
[41,     1] loss: 988.147
[42,     1] loss: 1100.889
[43,     1] loss: 1017.928
[44,     1] loss: 1167.801
[45,     1] loss: 985.830
[46,     1] loss: 1097.770
[47,     1] loss: 989.472
[48,     1] loss: 1006.073
[49,     1] loss: 959.390
[50,     1] loss: 1047.691
[51,     1] loss: 981.808
[52,     1] loss: 949.511
[53,     1] loss: 925.023
[54,     1] loss: 923.312
[55,     1] loss: 914.369
[56,     1] loss: 912.345
[57,     1] loss: 843.901
[58,     1] loss: 894.649
[59,     1] loss: 857.275
[60,     1] loss: 943.318
[61,     1] loss: 961.512
[62,     1] loss: 785.530
[63,     1] loss: 955.227
[64,     1] loss: 876.510
[65,     1] loss: 855.278
[66,     1] loss: 863.013
[67,     1] loss: 739.492
[68,     1] loss: 896.254
[69,     1] loss: 701.177
[70,     1] loss: 748.848
[71,     1] loss: 939.853
[72,     1] loss: 936.936
[73,     1] loss: 702.863
[74,     1] loss: 829.537
[75,     1] loss: 733.633
[76,     1] loss: 675.139
[77,     1] loss: 719.285
Early stopping applied (best metric=0.4511997103691101)
Finished Training
Total time taken: 12.569970846176147
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1459.419
[2,     1] loss: 1484.912
[3,     1] loss: 1460.924
[4,     1] loss: 1456.346
[5,     1] loss: 1454.318
[6,     1] loss: 1455.065
[7,     1] loss: 1458.758
[8,     1] loss: 1458.195
[9,     1] loss: 1452.783
[10,     1] loss: 1455.848
[11,     1] loss: 1457.541
[12,     1] loss: 1460.006
[13,     1] loss: 1454.680
[14,     1] loss: 1456.928
[15,     1] loss: 1452.953
[16,     1] loss: 1455.862
[17,     1] loss: 1453.198
[18,     1] loss: 1451.672
[19,     1] loss: 1455.807
[20,     1] loss: 1453.872
[21,     1] loss: 1452.131
[22,     1] loss: 1449.788
[23,     1] loss: 1446.442
[24,     1] loss: 1442.138
[25,     1] loss: 1437.308
[26,     1] loss: 1426.893
[27,     1] loss: 1400.208
[28,     1] loss: 1378.459
[29,     1] loss: 1375.335
[30,     1] loss: 1337.128
[31,     1] loss: 1335.633
[32,     1] loss: 1282.558
[33,     1] loss: 1242.002
[34,     1] loss: 1236.867
[35,     1] loss: 1194.561
[36,     1] loss: 1233.503
[37,     1] loss: 1196.264
[38,     1] loss: 1190.374
[39,     1] loss: 1229.925
[40,     1] loss: 1252.503
[41,     1] loss: 1185.863
[42,     1] loss: 1233.548
[43,     1] loss: 1155.997
[44,     1] loss: 1143.589
[45,     1] loss: 1178.810
[46,     1] loss: 1137.244
[47,     1] loss: 1137.905
[48,     1] loss: 1102.878
[49,     1] loss: 1102.661
[50,     1] loss: 1046.051
[51,     1] loss: 1076.293
[52,     1] loss: 1087.711
[53,     1] loss: 962.414
[54,     1] loss: 1024.749
[55,     1] loss: 984.699
[56,     1] loss: 991.428
[57,     1] loss: 936.538
[58,     1] loss: 1066.902
[59,     1] loss: 1023.606
[60,     1] loss: 926.873
[61,     1] loss: 939.566
[62,     1] loss: 936.696
[63,     1] loss: 881.629
[64,     1] loss: 959.818
[65,     1] loss: 849.523
[66,     1] loss: 851.557
[67,     1] loss: 787.587
[68,     1] loss: 969.840
[69,     1] loss: 1843.024
[70,     1] loss: 1143.876
[71,     1] loss: 1188.187
[72,     1] loss: 905.572
[73,     1] loss: 1109.721
[74,     1] loss: 1223.180
[75,     1] loss: 1212.323
[76,     1] loss: 1119.768
[77,     1] loss: 1102.799
[78,     1] loss: 1141.347
[79,     1] loss: 1127.010
[80,     1] loss: 1108.586
[81,     1] loss: 1030.840
[82,     1] loss: 1033.789
[83,     1] loss: 1046.879
[84,     1] loss: 1013.101
[85,     1] loss: 905.793
[86,     1] loss: 932.904
[87,     1] loss: 916.581
[88,     1] loss: 901.495
[89,     1] loss: 892.849
[90,     1] loss: 848.829
[91,     1] loss: 820.500
[92,     1] loss: 831.274
[93,     1] loss: 743.128
[94,     1] loss: 843.458
[95,     1] loss: 850.923
[96,     1] loss: 794.683
[97,     1] loss: 694.415
[98,     1] loss: 710.600
[99,     1] loss: 776.671
[100,     1] loss: 1150.939
[101,     1] loss: 1556.658
[102,     1] loss: 779.291
[103,     1] loss: 1133.391
[104,     1] loss: 881.942
[105,     1] loss: 1050.414
[106,     1] loss: 999.092
[107,     1] loss: 897.629
[108,     1] loss: 994.896
[109,     1] loss: 861.443
[110,     1] loss: 888.991
[111,     1] loss: 916.466
[112,     1] loss: 853.250
[113,     1] loss: 842.168
[114,     1] loss: 821.593
[115,     1] loss: 903.700
[116,     1] loss: 802.689
[117,     1] loss: 791.033
[118,     1] loss: 786.366
[119,     1] loss: 717.577
[120,     1] loss: 770.064
[121,     1] loss: 814.176
[122,     1] loss: 653.315
[123,     1] loss: 711.249
[124,     1] loss: 694.175
[125,     1] loss: 668.537
[126,     1] loss: 592.797
[127,     1] loss: 694.810
[128,     1] loss: 975.682
[129,     1] loss: 1790.371
[130,     1] loss: 716.429
[131,     1] loss: 1146.740
[132,     1] loss: 864.971
[133,     1] loss: 975.578
[134,     1] loss: 988.924
[135,     1] loss: 810.469
[136,     1] loss: 1036.371
[137,     1] loss: 812.045
Early stopping applied (best metric=0.3323681652545929)
Finished Training
Total time taken: 20.926773071289062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1467.651
[2,     1] loss: 1468.750
[3,     1] loss: 1464.801
[4,     1] loss: 1457.032
[5,     1] loss: 1458.386
[6,     1] loss: 1457.565
[7,     1] loss: 1458.495
[8,     1] loss: 1455.578
[9,     1] loss: 1452.992
[10,     1] loss: 1457.314
[11,     1] loss: 1447.572
[12,     1] loss: 1453.333
[13,     1] loss: 1450.585
[14,     1] loss: 1444.372
[15,     1] loss: 1434.657
[16,     1] loss: 1422.463
[17,     1] loss: 1406.107
[18,     1] loss: 1386.897
[19,     1] loss: 1351.597
[20,     1] loss: 1314.567
[21,     1] loss: 1310.031
[22,     1] loss: 1269.680
[23,     1] loss: 1239.344
[24,     1] loss: 1232.634
[25,     1] loss: 1216.855
[26,     1] loss: 1217.329
[27,     1] loss: 1193.148
[28,     1] loss: 1183.738
[29,     1] loss: 1198.606
[30,     1] loss: 1211.086
[31,     1] loss: 1181.150
[32,     1] loss: 1192.497
[33,     1] loss: 1141.404
[34,     1] loss: 1161.303
[35,     1] loss: 1150.945
[36,     1] loss: 1149.498
[37,     1] loss: 1081.438
[38,     1] loss: 1113.150
[39,     1] loss: 1044.267
[40,     1] loss: 1081.304
[41,     1] loss: 1113.227
[42,     1] loss: 1115.925
[43,     1] loss: 996.121
[44,     1] loss: 1135.584
[45,     1] loss: 998.721
[46,     1] loss: 1114.117
[47,     1] loss: 1039.138
[48,     1] loss: 1094.766
[49,     1] loss: 960.579
[50,     1] loss: 1086.400
[51,     1] loss: 974.815
[52,     1] loss: 971.478
[53,     1] loss: 878.744
[54,     1] loss: 979.834
[55,     1] loss: 950.004
[56,     1] loss: 872.652
[57,     1] loss: 893.136
[58,     1] loss: 1155.668
[59,     1] loss: 1158.664
[60,     1] loss: 945.951
[61,     1] loss: 1007.511
[62,     1] loss: 962.682
[63,     1] loss: 951.949
[64,     1] loss: 951.656
[65,     1] loss: 918.809
[66,     1] loss: 943.158
[67,     1] loss: 856.356
[68,     1] loss: 886.737
[69,     1] loss: 794.647
[70,     1] loss: 868.668
[71,     1] loss: 740.734
[72,     1] loss: 771.180
[73,     1] loss: 689.361
[74,     1] loss: 715.731
[75,     1] loss: 661.792
[76,     1] loss: 734.403
[77,     1] loss: 744.536
[78,     1] loss: 994.061
[79,     1] loss: 852.571
[80,     1] loss: 713.575
[81,     1] loss: 834.543
[82,     1] loss: 652.973
[83,     1] loss: 740.662
[84,     1] loss: 644.283
[85,     1] loss: 638.197
[86,     1] loss: 733.316
[87,     1] loss: 756.931
[88,     1] loss: 644.406
[89,     1] loss: 539.437
[90,     1] loss: 575.428
[91,     1] loss: 572.705
[92,     1] loss: 577.912
[93,     1] loss: 635.076
[94,     1] loss: 1146.420
[95,     1] loss: 1397.800
[96,     1] loss: 697.254
[97,     1] loss: 1069.293
[98,     1] loss: 987.813
[99,     1] loss: 901.534
[100,     1] loss: 896.371
Early stopping applied (best metric=0.38138893246650696)
Finished Training
Total time taken: 15.669832944869995
{'Hydroxylation-K Validation Accuracy': 0.7760342789598109, 'Hydroxylation-K Validation Sensitivity': 0.7496296296296296, 'Hydroxylation-K Validation Specificity': 0.7824561403508772, 'Hydroxylation-K Validation Precision': 0.4770646945646946, 'Hydroxylation-K AUC ROC': 0.8246588693957115, 'Hydroxylation-K AUC PR': 0.6223152294240853, 'Hydroxylation-K MCC': 0.4634511982076514, 'Hydroxylation-K F1': 0.5757262361900043, 'Validation Loss (Hydroxylation-K)': 0.4262571354707082, 'Hydroxylation-P Validation Accuracy': 0.787930781855405, 'Hydroxylation-P Validation Sensitivity': 0.8123280423280423, 'Hydroxylation-P Validation Specificity': 0.7826624769315178, 'Hydroxylation-P Validation Precision': 0.4501523383530114, 'Hydroxylation-P AUC ROC': 0.8509194239985053, 'Hydroxylation-P AUC PR': 0.5897002550892166, 'Hydroxylation-P MCC': 0.4885230999194928, 'Hydroxylation-P F1': 0.5772523762824687, 'Validation Loss (Hydroxylation-P)': 0.3630806605021159, 'Validation Loss (total)': 0.7893377979596455, 'TimeToTrain': 17.340565951665244}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007021595651775408,
 'learning_rate_Hydroxylation-K': 0.00464487102433142,
 'learning_rate_Hydroxylation-P': 0.0038300482892735382,
 'log_base': 2.9925527392022375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1707168465,
 'sample_weights': [2.603728520341821, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.073669035768703,
 'weight_decay_Hydroxylation-K': 1.0081231184235047,
 'weight_decay_Hydroxylation-P': 4.934683646973215}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.068
[2,     1] loss: 1236.331
[3,     1] loss: 1237.237
[4,     1] loss: 1232.737
[5,     1] loss: 1227.160
[6,     1] loss: 1228.740
[7,     1] loss: 1227.148
[8,     1] loss: 1226.699
[9,     1] loss: 1225.722
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008992848236361503,
 'learning_rate_Hydroxylation-K': 0.0051118135127711135,
 'learning_rate_Hydroxylation-P': 0.007455980491924808,
 'log_base': 1.2355756068721642,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4096273376,
 'sample_weights': [1.5230383699257644, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.829448832365783,
 'weight_decay_Hydroxylation-K': 3.943731077010936,
 'weight_decay_Hydroxylation-P': 3.5490360421945217}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2573.138
[2,     1] loss: 2570.965
[3,     1] loss: 2572.315
[4,     1] loss: 2573.615
[5,     1] loss: 2573.840
[6,     1] loss: 2566.159
[7,     1] loss: 2570.917
[8,     1] loss: 2567.339
[9,     1] loss: 2561.789
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008966168758231998,
 'learning_rate_Hydroxylation-K': 5.5221236094380474e-05,
 'learning_rate_Hydroxylation-P': 0.00656180284789776,
 'log_base': 1.840418274741036,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4138889649,
 'sample_weights': [7.891969827643434, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.799576364834386,
 'weight_decay_Hydroxylation-K': 4.271671987527961,
 'weight_decay_Hydroxylation-P': 3.7473267890819244}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1492.335
[2,     1] loss: 1486.107
[3,     1] loss: 1485.978
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0063402753919975545,
 'learning_rate_Hydroxylation-K': 4.719167946906546e-05,
 'learning_rate_Hydroxylation-P': 0.008352084293046517,
 'log_base': 1.2095891789938782,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 916409384,
 'sample_weights': [2.7368240388080376, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.677480908049198,
 'weight_decay_Hydroxylation-K': 7.073503643285349,
 'weight_decay_Hydroxylation-P': 0.8003546135115118}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2848.117
[2,     1] loss: 2850.354
[3,     1] loss: 2885.390
[4,     1] loss: 2855.352
[5,     1] loss: 2854.427
[6,     1] loss: 2856.837
[7,     1] loss: 2848.016
[8,     1] loss: 2841.316
[9,     1] loss: 2840.515
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007055854064788857,
 'learning_rate_Hydroxylation-K': 0.005264488828638429,
 'learning_rate_Hydroxylation-P': 0.009284929410947051,
 'log_base': 1.3340403138596066,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3032999264,
 'sample_weights': [8.77357735941294, 1.0967394191055322],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.57251977443803,
 'weight_decay_Hydroxylation-K': 3.432670007496924,
 'weight_decay_Hydroxylation-P': 0.2272123621165748}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2135.677
[2,     1] loss: 2123.081
[3,     1] loss: 2133.969
[4,     1] loss: 2122.484
[5,     1] loss: 2140.158
[6,     1] loss: 2135.833
[7,     1] loss: 2133.466
[8,     1] loss: 2129.975
[9,     1] loss: 2129.583
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004521990739296919,
 'learning_rate_Hydroxylation-K': 0.007018023041046064,
 'learning_rate_Hydroxylation-P': 0.006954753744745362,
 'log_base': 1.2472185035281016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3135970103,
 'sample_weights': [5.7924103723287965, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.747502100570381,
 'weight_decay_Hydroxylation-K': 1.9800491385515773,
 'weight_decay_Hydroxylation-P': 0.08954760018176677}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2505.806
[2,     1] loss: 2510.275
[3,     1] loss: 2504.245
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007747646532616171,
 'learning_rate_Hydroxylation-K': 0.0011029202272808416,
 'learning_rate_Hydroxylation-P': 0.007226600142907228,
 'log_base': 1.9095741887120499,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 920306058,
 'sample_weights': [7.556917988083515, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.78407621711016,
 'weight_decay_Hydroxylation-K': 7.902352008235837,
 'weight_decay_Hydroxylation-P': 2.968146967698752}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1454.697
[2,     1] loss: 1456.732
[3,     1] loss: 1452.255
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00035740041306912633,
 'learning_rate_Hydroxylation-K': 0.0021421057433956524,
 'learning_rate_Hydroxylation-P': 0.0063791935945776545,
 'log_base': 1.339132002729194,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 501954156,
 'sample_weights': [2.5807606147505715, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.262584699840267,
 'weight_decay_Hydroxylation-K': 1.4639773990139162,
 'weight_decay_Hydroxylation-P': 0.3586928222108915}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2132.357
[2,     1] loss: 2140.976
[3,     1] loss: 2119.527
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009009801695219467,
 'learning_rate_Hydroxylation-K': 0.00029913147057106487,
 'learning_rate_Hydroxylation-P': 0.006581395492710937,
 'log_base': 2.1906367733432663,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2229170079,
 'sample_weights': [5.716847283314654, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.282903509869676,
 'weight_decay_Hydroxylation-K': 5.316817961130564,
 'weight_decay_Hydroxylation-P': 2.2123894572796554}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1359.132
[2,     1] loss: 1364.802
[3,     1] loss: 1362.408
[4,     1] loss: 1359.928
[5,     1] loss: 1357.007
[6,     1] loss: 1356.963
[7,     1] loss: 1357.760
[8,     1] loss: 1357.708
[9,     1] loss: 1355.063
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029887505464766943,
 'learning_rate_Hydroxylation-K': 0.008452446322096017,
 'learning_rate_Hydroxylation-P': 0.008017557364020624,
 'log_base': 1.8563577624316991,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4205486001,
 'sample_weights': [2.1288696924896273, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.8108522823760256,
 'weight_decay_Hydroxylation-K': 9.839457694662366,
 'weight_decay_Hydroxylation-P': 2.6487261650543164}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1481.490
[2,     1] loss: 1478.047
[3,     1] loss: 1479.978
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036423314006980415,
 'learning_rate_Hydroxylation-K': 0.0037859119334239175,
 'learning_rate_Hydroxylation-P': 0.003511071248450786,
 'log_base': 1.613024771656719,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1511648342,
 'sample_weights': [2.698672736367721, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.448902382600911,
 'weight_decay_Hydroxylation-K': 3.6704463534151284,
 'weight_decay_Hydroxylation-P': 1.289613534736255}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1650.477
[2,     1] loss: 1643.555
[3,     1] loss: 1644.498
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008829892119455843,
 'learning_rate_Hydroxylation-K': 0.000474602964786464,
 'learning_rate_Hydroxylation-P': 0.005902882343519574,
 'log_base': 1.498240951391071,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1817121537,
 'sample_weights': [3.4917468973889343, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7333955027266845,
 'weight_decay_Hydroxylation-K': 7.420156104218654,
 'weight_decay_Hydroxylation-P': 3.5844647469898314}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1785.919
[2,     1] loss: 1785.024
[3,     1] loss: 1775.729
[4,     1] loss: 1779.205
[5,     1] loss: 1780.072
[6,     1] loss: 1791.837
[7,     1] loss: 1778.344
[8,     1] loss: 1774.066
[9,     1] loss: 1778.013
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003966591555978217,
 'learning_rate_Hydroxylation-K': 0.007514493966448729,
 'learning_rate_Hydroxylation-P': 0.006120948299441988,
 'log_base': 1.9608897177660314,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2498139225,
 'sample_weights': [4.129303325245833, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.839297591574569,
 'weight_decay_Hydroxylation-K': 0.10572394371882154,
 'weight_decay_Hydroxylation-P': 2.4327630340986097}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1432.091
[2,     1] loss: 1430.323
[3,     1] loss: 1432.772
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004001775243856212,
 'learning_rate_Hydroxylation-K': 0.0050563251165208906,
 'learning_rate_Hydroxylation-P': 0.0018705339161525468,
 'log_base': 1.4187828345157705,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1608451738,
 'sample_weights': [2.4791317826151555, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.384607456395411,
 'weight_decay_Hydroxylation-K': 2.5300759957396117,
 'weight_decay_Hydroxylation-P': 3.694206182504013}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1920.623
[2,     1] loss: 1917.187
[3,     1] loss: 1908.175
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00388832998690218,
 'learning_rate_Hydroxylation-K': 0.004889615472352867,
 'learning_rate_Hydroxylation-P': 0.009663246271392213,
 'log_base': 1.1830559762390573,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3251672476,
 'sample_weights': [4.77257367713682, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.914905412209025,
 'weight_decay_Hydroxylation-K': 2.9117801046711955,
 'weight_decay_Hydroxylation-P': 3.829076788275658}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3227.604
[2,     1] loss: 3222.477
[3,     1] loss: 3219.901
[4,     1] loss: 3232.864
[5,     1] loss: 3222.231
[6,     1] loss: 3209.861
[7,     1] loss: 3219.634
[8,     1] loss: 3209.355
[9,     1] loss: 3204.857
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002107803432775704,
 'learning_rate_Hydroxylation-K': 0.003468026121333145,
 'learning_rate_Hydroxylation-P': 0.008063116190729568,
 'log_base': 1.0262737984316925,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3393618161,
 'sample_weights': [9.931196899012496, 1.241447436074631],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.68252069177438,
 'weight_decay_Hydroxylation-K': 5.776497120088514,
 'weight_decay_Hydroxylation-P': 4.329663548097069}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20818.266
Exploding loss, terminate run (best metric=0.5381367802619934)
Finished Training
Total time taken: 0.2200007438659668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20910.631
Exploding loss, terminate run (best metric=0.5305668115615845)
Finished Training
Total time taken: 0.2090003490447998
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20997.805
Exploding loss, terminate run (best metric=0.526907742023468)
Finished Training
Total time taken: 0.20499944686889648
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20928.789
Exploding loss, terminate run (best metric=0.5281285643577576)
Finished Training
Total time taken: 0.20200014114379883
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20932.986
Exploding loss, terminate run (best metric=0.5309038162231445)
Finished Training
Total time taken: 0.21100330352783203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21002.750
Exploding loss, terminate run (best metric=0.5318353772163391)
Finished Training
Total time taken: 0.23000192642211914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20880.658
Exploding loss, terminate run (best metric=0.5340445041656494)
Finished Training
Total time taken: 0.22699689865112305
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20875.916
Exploding loss, terminate run (best metric=0.5332034230232239)
Finished Training
Total time taken: 0.22099828720092773
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20875.961
Exploding loss, terminate run (best metric=0.5280364155769348)
Finished Training
Total time taken: 0.21000266075134277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20912.434
Exploding loss, terminate run (best metric=0.5297409296035767)
Finished Training
Total time taken: 0.22700119018554688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20937.848
Exploding loss, terminate run (best metric=0.5324390530586243)
Finished Training
Total time taken: 0.21300196647644043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20893.717
Exploding loss, terminate run (best metric=0.5284531712532043)
Finished Training
Total time taken: 0.2219984531402588
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20950.912
Exploding loss, terminate run (best metric=0.5281394124031067)
Finished Training
Total time taken: 0.2299976348876953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 20923.496
Exploding loss, terminate run (best metric=0.5316159725189209)
Finished Training
Total time taken: 0.21199703216552734
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 20876.410
Exploding loss, terminate run (best metric=0.5341436266899109)
Finished Training
Total time taken: 0.22500252723693848
{'Hydroxylation-K Validation Accuracy': 0.4847222222222222, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.47192982456140353, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6200584795321638, 'Hydroxylation-K AUC PR': 0.37142286387626317, 'Hydroxylation-K MCC': 0.00883021571376696, 'Hydroxylation-K F1': 0.18048216151664428, 'Validation Loss (Hydroxylation-K)': 0.5562532782554627, 'Hydroxylation-P Validation Accuracy': 0.48899439960746494, 'Hydroxylation-P Validation Sensitivity': 0.5295238095238095, 'Hydroxylation-P Validation Specificity': 0.4800837947029777, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6161267266229484, 'Hydroxylation-P AUC PR': 0.279980562450271, 'Hydroxylation-P MCC': 0.01150555384457138, 'Hydroxylation-P F1': 0.16256498840251032, 'Validation Loss (Hydroxylation-P)': 0.5310863733291626, 'Validation Loss (total)': 1.0873396396636963, 'TimeToTrain': 0.21760017077128094}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016524798294215246,
 'learning_rate_Hydroxylation-K': 0.005818569334705203,
 'learning_rate_Hydroxylation-P': 0.005485198689613996,
 'log_base': 1.0909073356499084,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2713000068,
 'sample_weights': [64.41909232486393, 8.035655104074701],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.6735236812391046,
 'weight_decay_Hydroxylation-K': 3.260083855738228,
 'weight_decay_Hydroxylation-P': 0.3549546365374554}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6269.350
[2,     1] loss: 6253.137
[3,     1] loss: 6200.818
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009709559948128071,
 'learning_rate_Hydroxylation-K': 0.002698322173004657,
 'learning_rate_Hydroxylation-P': 0.004912958870144823,
 'log_base': 1.2804055793368632,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1803431233,
 'sample_weights': [19.18684747451868, 2.3984483286163325],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.85682656457372,
 'weight_decay_Hydroxylation-K': 6.972807654105023,
 'weight_decay_Hydroxylation-P': 4.014880751795657}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2333.588
[2,     1] loss: 2326.146
[3,     1] loss: 2333.061
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013359907039598535,
 'learning_rate_Hydroxylation-K': 0.0034553896798986956,
 'learning_rate_Hydroxylation-P': 0.006577592211225517,
 'log_base': 1.117512088777031,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3854395451,
 'sample_weights': [6.754042298987785, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.303610090868119,
 'weight_decay_Hydroxylation-K': 5.660933588265535,
 'weight_decay_Hydroxylation-P': 3.198206895333219}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4889.589
[2,     1] loss: 4879.461
[3,     1] loss: 4877.798
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009506654782984795,
 'learning_rate_Hydroxylation-K': 0.0009136812481810938,
 'learning_rate_Hydroxylation-P': 0.00914206669887848,
 'log_base': 2.051970664016249,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 583509065,
 'sample_weights': [15.025832984453869, 1.878301479984825],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.510468843523472,
 'weight_decay_Hydroxylation-K': 5.843461979790695,
 'weight_decay_Hydroxylation-P': 1.559691689511763}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.589
[2,     1] loss: 1398.900
[3,     1] loss: 1406.633
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00011923538988813575,
 'learning_rate_Hydroxylation-K': 0.0005038933194940718,
 'learning_rate_Hydroxylation-P': 0.006047349892650709,
 'log_base': 1.908365123141118,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 583707184,
 'sample_weights': [2.3225399027428386, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.96844551919546,
 'weight_decay_Hydroxylation-K': 7.960508579046944,
 'weight_decay_Hydroxylation-P': 2.0258594103925702}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1467.745
[2,     1] loss: 1463.773
[3,     1] loss: 1454.793
[4,     1] loss: 1461.189
[5,     1] loss: 1458.206
[6,     1] loss: 1459.614
[7,     1] loss: 1447.823
[8,     1] loss: 1447.333
[9,     1] loss: 1461.813
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007214068416669458,
 'learning_rate_Hydroxylation-K': 0.00423104706101482,
 'learning_rate_Hydroxylation-P': 0.009320300905285721,
 'log_base': 1.0248285815404634,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2108763344,
 'sample_weights': [2.5832899132160745, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.49569921908169,
 'weight_decay_Hydroxylation-K': 4.765884803759715,
 'weight_decay_Hydroxylation-P': 1.99960325043507}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22040.160
Exploding loss, terminate run (best metric=0.5583255887031555)
Finished Training
Total time taken: 0.2109999656677246
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22060.885
Exploding loss, terminate run (best metric=0.5361914038658142)
Finished Training
Total time taken: 0.22300124168395996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22140.746
Exploding loss, terminate run (best metric=0.5303407311439514)
Finished Training
Total time taken: 0.2759981155395508
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22192.711
Exploding loss, terminate run (best metric=0.5353825092315674)
Finished Training
Total time taken: 0.2869994640350342
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22120.947
Exploding loss, terminate run (best metric=0.5278871655464172)
Finished Training
Total time taken: 0.287001371383667
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22159.582
Exploding loss, terminate run (best metric=0.5392730832099915)
Finished Training
Total time taken: 0.23800086975097656
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22089.418
Exploding loss, terminate run (best metric=0.5277087092399597)
Finished Training
Total time taken: 0.24700021743774414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22147.369
Exploding loss, terminate run (best metric=0.5306152701377869)
Finished Training
Total time taken: 0.23400139808654785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22051.887
Exploding loss, terminate run (best metric=0.5272521376609802)
Finished Training
Total time taken: 0.2630016803741455
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22166.691
Exploding loss, terminate run (best metric=0.5398893356323242)
Finished Training
Total time taken: 0.2629995346069336
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22325.162
Exploding loss, terminate run (best metric=0.5321508646011353)
Finished Training
Total time taken: 0.28600287437438965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22072.324
Exploding loss, terminate run (best metric=0.5297846794128418)
Finished Training
Total time taken: 0.2799999713897705
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22118.020
Exploding loss, terminate run (best metric=0.535873532295227)
Finished Training
Total time taken: 0.274003267288208
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22089.377
Exploding loss, terminate run (best metric=0.5294067859649658)
Finished Training
Total time taken: 0.25800323486328125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22033.705
Exploding loss, terminate run (best metric=0.5303483009338379)
Finished Training
Total time taken: 0.283001184463501
{'Hydroxylation-K Validation Accuracy': 0.6383569739952718, 'Hydroxylation-K Validation Sensitivity': 0.26666666666666666, 'Hydroxylation-K Validation Specificity': 0.7333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.56953216374269, 'Hydroxylation-K AUC PR': 0.29605139800297536, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.08883415435139573, 'Validation Loss (Hydroxylation-K)': 0.5577552239100139, 'Hydroxylation-P Validation Accuracy': 0.6512573642623893, 'Hydroxylation-P Validation Sensitivity': 0.2704761904761905, 'Hydroxylation-P Validation Specificity': 0.7329268292682927, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5523731282959923, 'Hydroxylation-P AUC PR': 0.24686679925047608, 'Hydroxylation-P MCC': 0.010632410901567404, 'Hydroxylation-P F1': 0.08727117909982689, 'Validation Loss (Hydroxylation-P)': 0.5340286731719971, 'Validation Loss (total)': 1.091783881187439, 'TimeToTrain': 0.26066762606302896}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009155019429879891,
 'learning_rate_Hydroxylation-K': 0.007406659055761128,
 'learning_rate_Hydroxylation-P': 0.00527202082953204,
 'log_base': 1.580085764994152,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3393465498,
 'sample_weights': [68.1205684194708, 8.497378239232225],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.307652583713347,
 'weight_decay_Hydroxylation-K': 3.2708396936509363,
 'weight_decay_Hydroxylation-P': 1.3179888214796487}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1679.877
[2,     1] loss: 1680.989
[3,     1] loss: 1673.700
[4,     1] loss: 1677.773
[5,     1] loss: 1675.043
[6,     1] loss: 1673.006
[7,     1] loss: 1678.135
[8,     1] loss: 1678.575
[9,     1] loss: 1677.297
[10,     1] loss: 1675.370
[11,     1] loss: 1669.405
[12,     1] loss: 1672.091
[13,     1] loss: 1664.291
[14,     1] loss: 1663.555
[15,     1] loss: 1653.108
[16,     1] loss: 1648.369
[17,     1] loss: 1640.479
[18,     1] loss: 1638.655
[19,     1] loss: 1624.903
[20,     1] loss: 1615.942
[21,     1] loss: 1590.291
[22,     1] loss: 1578.349
[23,     1] loss: 1564.876
[24,     1] loss: 1538.086
[25,     1] loss: 1523.388
[26,     1] loss: 1520.163
[27,     1] loss: 1485.972
[28,     1] loss: 1485.498
[29,     1] loss: 1423.476
[30,     1] loss: 1492.187
[31,     1] loss: 1451.182
[32,     1] loss: 1437.099
[33,     1] loss: 1407.695
[34,     1] loss: 1422.065
[35,     1] loss: 1394.262
[36,     1] loss: 1353.328
[37,     1] loss: 1334.196
[38,     1] loss: 1407.947
[39,     1] loss: 1359.691
[40,     1] loss: 1395.725
[41,     1] loss: 1365.102
[42,     1] loss: 1383.853
[43,     1] loss: 1359.572
[44,     1] loss: 1369.884
[45,     1] loss: 1377.225
[46,     1] loss: 1431.873
[47,     1] loss: 1338.969
[48,     1] loss: 1371.960
[49,     1] loss: 1330.453
[50,     1] loss: 1295.117
[51,     1] loss: 1295.023
[52,     1] loss: 1307.139
[53,     1] loss: 1289.531
[54,     1] loss: 1286.298
[55,     1] loss: 1256.504
[56,     1] loss: 1284.800
[57,     1] loss: 1283.867
[58,     1] loss: 1284.798
[59,     1] loss: 1298.683
[60,     1] loss: 1264.626
[61,     1] loss: 1243.436
[62,     1] loss: 1294.120
[63,     1] loss: 1163.796
[64,     1] loss: 1160.050
[65,     1] loss: 1238.609
[66,     1] loss: 1161.514
[67,     1] loss: 1201.790
[68,     1] loss: 1204.320
[69,     1] loss: 1171.196
[70,     1] loss: 1148.896
[71,     1] loss: 1154.853
[72,     1] loss: 1093.538
[73,     1] loss: 1127.996
[74,     1] loss: 1137.917
[75,     1] loss: 1112.826
[76,     1] loss: 1122.894
[77,     1] loss: 1114.145
[78,     1] loss: 1107.461
[79,     1] loss: 1078.492
[80,     1] loss: 1099.918
[81,     1] loss: 1040.763
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0040149605166972585,
 'learning_rate_Hydroxylation-K': 0.007385992071022357,
 'learning_rate_Hydroxylation-P': 0.00480904049771536,
 'log_base': 1.299972544382292,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2027246678,
 'sample_weights': [3.6492225505761287, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.453850957123807,
 'weight_decay_Hydroxylation-K': 1.508185355417829,
 'weight_decay_Hydroxylation-P': 3.265434161845996}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2254.726
[2,     1] loss: 2256.294
[3,     1] loss: 2247.541
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003161492179784767,
 'learning_rate_Hydroxylation-K': 0.0029763297406193326,
 'learning_rate_Hydroxylation-P': 0.007602773417536441,
 'log_base': 1.3014939249635888,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3484423910,
 'sample_weights': [6.363585945271134, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.628662546039492,
 'weight_decay_Hydroxylation-K': 2.562785513724916,
 'weight_decay_Hydroxylation-P': 2.1793795996493985}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2247.046
[2,     1] loss: 2242.491
[3,     1] loss: 2239.379
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004270266919430991,
 'learning_rate_Hydroxylation-K': 0.00370167920233401,
 'learning_rate_Hydroxylation-P': 0.009114765955127052,
 'log_base': 2.2003110863979547,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 245426590,
 'sample_weights': [6.3353404030441745, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.0004399385381815,
 'weight_decay_Hydroxylation-K': 6.194584080311423,
 'weight_decay_Hydroxylation-P': 2.4079390929756173}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1364.738
[2,     1] loss: 1357.551
[3,     1] loss: 1356.752
[4,     1] loss: 1356.245
[5,     1] loss: 1356.911
[6,     1] loss: 1354.788
[7,     1] loss: 1353.268
[8,     1] loss: 1351.270
[9,     1] loss: 1351.264
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003989894489428955,
 'learning_rate_Hydroxylation-K': 0.0031375485052382336,
 'learning_rate_Hydroxylation-P': 0.007400181159934932,
 'log_base': 1.4558621627312456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1352779816,
 'sample_weights': [2.1169741145145133, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.164544997449743,
 'weight_decay_Hydroxylation-K': 4.732250143241553,
 'weight_decay_Hydroxylation-P': 3.964435305961292}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1849.843
[2,     1] loss: 1850.965
[3,     1] loss: 1841.479
[4,     1] loss: 1838.550
[5,     1] loss: 1844.293
[6,     1] loss: 1839.260
[7,     1] loss: 1840.469
[8,     1] loss: 1840.788
[9,     1] loss: 1833.353
[10,     1] loss: 1813.264
[11,     1] loss: 1804.084
[12,     1] loss: 1776.048
[13,     1] loss: 1757.974
[14,     1] loss: 1680.936
[15,     1] loss: 1625.992
[16,     1] loss: 1638.424
[17,     1] loss: 1614.517
[18,     1] loss: 1586.190
[19,     1] loss: 1623.936
[20,     1] loss: 1581.687
[21,     1] loss: 1521.098
[22,     1] loss: 1552.171
[23,     1] loss: 1595.429
[24,     1] loss: 1517.648
[25,     1] loss: 1470.925
[26,     1] loss: 1505.414
[27,     1] loss: 1490.754
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00186163013961171,
 'learning_rate_Hydroxylation-K': 0.00426591345458204,
 'learning_rate_Hydroxylation-P': 0.007359290548019833,
 'log_base': 1.1049603088395612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1578507866,
 'sample_weights': [4.444757205682746, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.468494561624697,
 'weight_decay_Hydroxylation-K': 1.0094195659999818,
 'weight_decay_Hydroxylation-P': 1.0560835404959696}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5432.925
[2,     1] loss: 5442.833
[3,     1] loss: 5427.929
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00037019078280299284,
 'learning_rate_Hydroxylation-K': 0.006499503750812016,
 'learning_rate_Hydroxylation-P': 0.0032420011458776303,
 'log_base': 1.0838018753848315,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4136520629,
 'sample_weights': [16.72630935736024, 2.090869215245417],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2587893783647397,
 'weight_decay_Hydroxylation-K': 9.725649903825747,
 'weight_decay_Hydroxylation-P': 9.03252354922255}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6730.935
[2,     1] loss: 6706.755
[3,     1] loss: 6749.151
[4,     1] loss: 6728.659
[5,     1] loss: 6747.552
[6,     1] loss: 6737.647
[7,     1] loss: 6731.939
[8,     1] loss: 6736.623
[9,     1] loss: 6705.636
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005889780482861029,
 'learning_rate_Hydroxylation-K': 0.007146147757165008,
 'learning_rate_Hydroxylation-P': 0.006237069963180797,
 'log_base': 1.1267160738441138,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 530072299,
 'sample_weights': [20.744837173910703, 2.5932045435427766],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.628176947377948,
 'weight_decay_Hydroxylation-K': 0.5156498947320036,
 'weight_decay_Hydroxylation-P': 0.04190581162176369}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4546.729
[2,     1] loss: 4562.433
[3,     1] loss: 4538.454
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00749539394389658,
 'learning_rate_Hydroxylation-K': 0.00018502941761003007,
 'learning_rate_Hydroxylation-P': 0.006201853886211248,
 'log_base': 1.8003491602060588,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1120056159,
 'sample_weights': [13.992802902834658, 1.7491677452240475],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.646116286480379,
 'weight_decay_Hydroxylation-K': 9.183204859348288,
 'weight_decay_Hydroxylation-P': 9.010644993330192}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1508.970
[2,     1] loss: 1511.636
[3,     1] loss: 1504.481
[4,     1] loss: 1509.427
[5,     1] loss: 1508.914
[6,     1] loss: 1508.530
[7,     1] loss: 1510.934
[8,     1] loss: 1503.085
[9,     1] loss: 1502.668
[10,     1] loss: 1510.012
[11,     1] loss: 1505.779
[12,     1] loss: 1503.810
[13,     1] loss: 1502.755
[14,     1] loss: 1500.771
[15,     1] loss: 1504.060
[16,     1] loss: 1505.653
[17,     1] loss: 1500.170
[18,     1] loss: 1500.434
[19,     1] loss: 1487.188
[20,     1] loss: 1479.462
[21,     1] loss: 1468.813
[22,     1] loss: 1444.786
[23,     1] loss: 1425.882
[24,     1] loss: 1403.942
[25,     1] loss: 1344.766
[26,     1] loss: 1323.458
[27,     1] loss: 1316.130
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004112088261263461,
 'learning_rate_Hydroxylation-K': 0.0020475463403382063,
 'learning_rate_Hydroxylation-P': 0.009553605496698318,
 'log_base': 1.0883413049060173,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2035456324,
 'sample_weights': [2.8392825874338166, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.461485627613616,
 'weight_decay_Hydroxylation-K': 4.643649402439147,
 'weight_decay_Hydroxylation-P': 5.926072416441504}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6389.514
[2,     1] loss: 6389.776
[3,     1] loss: 6395.436
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009502086150601712,
 'learning_rate_Hydroxylation-K': 0.0006167075877794768,
 'learning_rate_Hydroxylation-P': 0.007144659197291017,
 'log_base': 1.9726666927964998,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1444064536,
 'sample_weights': [19.72059676453526, 2.4651695601391537],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8042925117797175,
 'weight_decay_Hydroxylation-K': 7.4809884581002715,
 'weight_decay_Hydroxylation-P': 4.476232811232874}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1427.945
[2,     1] loss: 1427.739
[3,     1] loss: 1432.742
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001668902346732044,
 'learning_rate_Hydroxylation-K': 0.003960844302607197,
 'learning_rate_Hydroxylation-P': 0.008809752139831015,
 'log_base': 1.1446436203758872,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4188313878,
 'sample_weights': [2.457281225891622, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.969812438475891,
 'weight_decay_Hydroxylation-K': 8.289966751069,
 'weight_decay_Hydroxylation-P': 3.5222965693799875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4000.911
[2,     1] loss: 4004.025
[3,     1] loss: 4006.277
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002818001785438538,
 'learning_rate_Hydroxylation-K': 0.0019894378092710904,
 'learning_rate_Hydroxylation-P': 0.009228338130997665,
 'log_base': 2.038730953539337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3343337628,
 'sample_weights': [12.35770133221847, 1.5447721750622163],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.000042958254479,
 'weight_decay_Hydroxylation-K': 3.216557054493573,
 'weight_decay_Hydroxylation-P': 5.238193173780852}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1410.834
[2,     1] loss: 1405.123
[3,     1] loss: 1403.224
[4,     1] loss: 1397.987
[5,     1] loss: 1399.841
[6,     1] loss: 1400.084
[7,     1] loss: 1394.184
[8,     1] loss: 1385.776
[9,     1] loss: 1373.600
[10,     1] loss: 1359.683
[11,     1] loss: 1316.390
[12,     1] loss: 1288.693
[13,     1] loss: 1273.644
[14,     1] loss: 1227.378
[15,     1] loss: 1211.748
[16,     1] loss: 1215.132
[17,     1] loss: 1164.892
[18,     1] loss: 1202.012
[19,     1] loss: 1178.099
[20,     1] loss: 1222.140
[21,     1] loss: 1150.797
[22,     1] loss: 1154.062
[23,     1] loss: 1173.031
[24,     1] loss: 1137.338
[25,     1] loss: 1171.930
[26,     1] loss: 1159.226
[27,     1] loss: 1130.181
[28,     1] loss: 1158.613
[29,     1] loss: 1132.438
[30,     1] loss: 1085.111
[31,     1] loss: 1114.562
[32,     1] loss: 1100.681
[33,     1] loss: 1086.010
[34,     1] loss: 1047.133
[35,     1] loss: 1050.085
[36,     1] loss: 1058.536
[37,     1] loss: 1038.035
[38,     1] loss: 1020.577
[39,     1] loss: 1008.636
[40,     1] loss: 993.103
[41,     1] loss: 996.148
[42,     1] loss: 964.770
[43,     1] loss: 1014.666
[44,     1] loss: 972.357
[45,     1] loss: 973.226
[46,     1] loss: 951.381
[47,     1] loss: 954.942
[48,     1] loss: 884.689
[49,     1] loss: 923.333
[50,     1] loss: 899.850
[51,     1] loss: 864.530
[52,     1] loss: 854.716
[53,     1] loss: 831.730
[54,     1] loss: 909.706
[55,     1] loss: 1002.681
[56,     1] loss: 920.775
[57,     1] loss: 879.087
[58,     1] loss: 921.155
[59,     1] loss: 857.942
[60,     1] loss: 941.262
[61,     1] loss: 852.197
[62,     1] loss: 903.304
[63,     1] loss: 837.293
[64,     1] loss: 861.260
[65,     1] loss: 821.814
[66,     1] loss: 830.838
[67,     1] loss: 733.783
[68,     1] loss: 852.202
[69,     1] loss: 715.428
[70,     1] loss: 783.056
[71,     1] loss: 775.832
[72,     1] loss: 682.824
[73,     1] loss: 765.084
[74,     1] loss: 684.216
[75,     1] loss: 695.365
[76,     1] loss: 684.565
[77,     1] loss: 594.218
[78,     1] loss: 774.101
[79,     1] loss: 763.638
[80,     1] loss: 660.774
[81,     1] loss: 629.616
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008949648369298062,
 'learning_rate_Hydroxylation-K': 0.006874052727231774,
 'learning_rate_Hydroxylation-P': 0.0008222724960577675,
 'log_base': 2.959784790772018,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3063095887,
 'sample_weights': [2.3436454030235785, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.417110459256741,
 'weight_decay_Hydroxylation-K': 9.594667416684894,
 'weight_decay_Hydroxylation-P': 8.45604612639845}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.177
[2,     1] loss: 1242.322
[3,     1] loss: 1236.441
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004266895533569357,
 'learning_rate_Hydroxylation-K': 0.005992480863459807,
 'learning_rate_Hydroxylation-P': 0.008254313516581884,
 'log_base': 1.0887302189394708,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4066930797,
 'sample_weights': [1.5384920009144611, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.981797702635116,
 'weight_decay_Hydroxylation-K': 2.559010899613551,
 'weight_decay_Hydroxylation-P': 1.5795755415830595}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6370.979
[2,     1] loss: 6397.189
[3,     1] loss: 6367.725
[4,     1] loss: 6366.766
[5,     1] loss: 6409.421
[6,     1] loss: 6358.411
[7,     1] loss: 6356.400
[8,     1] loss: 6369.221
[9,     1] loss: 6337.607
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005759089349318401,
 'learning_rate_Hydroxylation-K': 0.001193571323338128,
 'learning_rate_Hydroxylation-P': 0.0048597273855720095,
 'log_base': 1.957135095964224,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3851760340,
 'sample_weights': [19.637716653163814, 2.454809147108385],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4288213711581337,
 'weight_decay_Hydroxylation-K': 7.22164061890447,
 'weight_decay_Hydroxylation-P': 2.501093799296573}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1435.006
[2,     1] loss: 1433.296
[3,     1] loss: 1428.604
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002386245447652234,
 'learning_rate_Hydroxylation-K': 0.00840629815605409,
 'learning_rate_Hydroxylation-P': 0.0059214667417454535,
 'log_base': 2.6586293286018465,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2246874651,
 'sample_weights': [2.4862078924292574, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7802317787393909,
 'weight_decay_Hydroxylation-K': 9.483639523176567,
 'weight_decay_Hydroxylation-P': 2.9020463250112294}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.869
[2,     1] loss: 1266.469
[3,     1] loss: 1267.830
[4,     1] loss: 1268.441
[5,     1] loss: 1265.537
[6,     1] loss: 1259.923
[7,     1] loss: 1256.555
[8,     1] loss: 1251.084
[9,     1] loss: 1238.910
[10,     1] loss: 1209.982
[11,     1] loss: 1196.818
[12,     1] loss: 1176.820
[13,     1] loss: 1158.716
[14,     1] loss: 1111.207
[15,     1] loss: 1105.959
[16,     1] loss: 1051.567
[17,     1] loss: 1038.160
[18,     1] loss: 1050.182
[19,     1] loss: 1069.297
[20,     1] loss: 1051.073
[21,     1] loss: 1050.235
[22,     1] loss: 1016.828
[23,     1] loss: 1059.612
[24,     1] loss: 1026.845
[25,     1] loss: 1056.733
[26,     1] loss: 990.925
[27,     1] loss: 991.705
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002440093386628495,
 'learning_rate_Hydroxylation-K': 0.0026189789926268645,
 'learning_rate_Hydroxylation-P': 0.00770134771231123,
 'log_base': 1.1879383611007457,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2168263156,
 'sample_weights': [1.7073275506662324, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.821148601427726,
 'weight_decay_Hydroxylation-K': 6.153490766060877,
 'weight_decay_Hydroxylation-P': 5.92420798123624}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3148.123
[2,     1] loss: 3149.845
[3,     1] loss: 3148.729
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002841622786783241,
 'learning_rate_Hydroxylation-K': 0.0024446612155988017,
 'learning_rate_Hydroxylation-P': 0.00705865436797584,
 'log_base': 2.8418207930800308,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2617055691,
 'sample_weights': [9.69370336630725, 1.2117596008359048],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.1003155855862765,
 'weight_decay_Hydroxylation-K': 1.700493019042372,
 'weight_decay_Hydroxylation-P': 1.425761679464146}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.519
[2,     1] loss: 1243.986
[3,     1] loss: 1248.191
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002517828448233804,
 'learning_rate_Hydroxylation-K': 0.0062774040628192735,
 'learning_rate_Hydroxylation-P': 0.0037511584400814575,
 'log_base': 2.6928957433102214,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2813075860,
 'sample_weights': [1.5984022075590074, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6849157424885255,
 'weight_decay_Hydroxylation-K': 8.713722679161041,
 'weight_decay_Hydroxylation-P': 1.4246021103070303}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.538
[2,     1] loss: 1264.685
[3,     1] loss: 1262.602
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008519772732432642,
 'learning_rate_Hydroxylation-K': 0.0018977286953380463,
 'learning_rate_Hydroxylation-P': 0.009059025455913396,
 'log_base': 2.9520298607783797,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2232691361,
 'sample_weights': [1.6852557349391444, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8820032636534623,
 'weight_decay_Hydroxylation-K': 2.8349756744536654,
 'weight_decay_Hydroxylation-P': 9.719224469463072}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.722
[2,     1] loss: 1236.151
[3,     1] loss: 1234.484
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0068203000177873944,
 'learning_rate_Hydroxylation-K': 0.0008598545696666148,
 'learning_rate_Hydroxylation-P': 0.006812940555924935,
 'log_base': 1.176123470341822,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1207111254,
 'sample_weights': [1.5422207010106446, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.247403954771372,
 'weight_decay_Hydroxylation-K': 5.3596123465697465,
 'weight_decay_Hydroxylation-P': 0.8134875954032348}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3346.075
[2,     1] loss: 3356.415
[3,     1] loss: 3350.054
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004967598140740328,
 'learning_rate_Hydroxylation-K': 0.0030826249955439497,
 'learning_rate_Hydroxylation-P': 0.006587047611427189,
 'log_base': 1.1174736848990034,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1254367481,
 'sample_weights': [10.29098553662138, 1.2864227483386288],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.728234183443924,
 'weight_decay_Hydroxylation-K': 1.505239260588119,
 'weight_decay_Hydroxylation-P': 0.095024685179408}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4868.052
[2,     1] loss: 4872.099
[3,     1] loss: 4882.604
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005047170890191883,
 'learning_rate_Hydroxylation-K': 0.008038851410461838,
 'learning_rate_Hydroxylation-P': 0.00962395469901188,
 'log_base': 1.0774107273368132,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1875709635,
 'sample_weights': [15.030482097949141, 1.8788826415595494],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.9600831124632,
 'weight_decay_Hydroxylation-K': 8.140605611033006,
 'weight_decay_Hydroxylation-P': 4.424363027275389}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7283.143
[2,     1] loss: 7236.090
[3,     1] loss: 7240.103
[4,     1] loss: 7305.716
[5,     1] loss: 7296.113
[6,     1] loss: 7229.714
[7,     1] loss: 7273.887
[8,     1] loss: 7248.933
[9,     1] loss: 7277.965
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005642261968301093,
 'learning_rate_Hydroxylation-K': 0.005375316606547363,
 'learning_rate_Hydroxylation-P': 0.008349331971889893,
 'log_base': 1.0680854422283823,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2044235537,
 'sample_weights': [22.39039357546518, 2.7989070179171525],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.787599583170989,
 'weight_decay_Hydroxylation-K': 4.779924793472515,
 'weight_decay_Hydroxylation-P': 5.784621146514775}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8265.681
[2,     1] loss: 8242.479
[3,     1] loss: 8206.006
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0022666625201487206,
 'learning_rate_Hydroxylation-K': 0.008609802754383611,
 'learning_rate_Hydroxylation-P': 0.006667773647437982,
 'log_base': 2.2874448356811925,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 686647298,
 'sample_weights': [25.345383977190657, 3.16829504789489],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.903499004901474,
 'weight_decay_Hydroxylation-K': 9.736123642181875,
 'weight_decay_Hydroxylation-P': 1.715476330732732}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1331.922
[2,     1] loss: 1331.004
[3,     1] loss: 1334.631
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034348432964386123,
 'learning_rate_Hydroxylation-K': 0.009855761708622785,
 'learning_rate_Hydroxylation-P': 0.0043248111937174635,
 'log_base': 2.768605501139149,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 759979588,
 'sample_weights': [2.017611456710194, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.3594580961326097,
 'weight_decay_Hydroxylation-K': 9.038432966458203,
 'weight_decay_Hydroxylation-P': 4.249622843993693}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.378
[2,     1] loss: 1252.527
[3,     1] loss: 1253.356
[4,     1] loss: 1249.880
[5,     1] loss: 1248.173
[6,     1] loss: 1246.355
[7,     1] loss: 1238.464
[8,     1] loss: 1228.569
[9,     1] loss: 1207.449
[10,     1] loss: 1175.164
[11,     1] loss: 1117.021
[12,     1] loss: 1108.310
[13,     1] loss: 1077.013
[14,     1] loss: 1083.733
[15,     1] loss: 1021.245
[16,     1] loss: 1103.503
[17,     1] loss: 1005.962
[18,     1] loss: 1042.093
[19,     1] loss: 1002.104
[20,     1] loss: 1012.721
[21,     1] loss: 1000.389
[22,     1] loss: 1024.129
[23,     1] loss: 986.195
[24,     1] loss: 932.331
[25,     1] loss: 982.273
[26,     1] loss: 1004.819
[27,     1] loss: 952.289
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030330753361233476,
 'learning_rate_Hydroxylation-K': 0.008833824256258645,
 'learning_rate_Hydroxylation-P': 0.008604201974416748,
 'log_base': 1.168453199492326,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2577095066,
 'sample_weights': [1.639370913870211, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.016416570169469,
 'weight_decay_Hydroxylation-K': 6.896214634843751,
 'weight_decay_Hydroxylation-P': 3.757511299452215}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3479.206
[2,     1] loss: 3482.148
[3,     1] loss: 3464.472
[4,     1] loss: 3487.262
[5,     1] loss: 3490.138
[6,     1] loss: 3475.792
[7,     1] loss: 3471.393
[8,     1] loss: 3459.835
[9,     1] loss: 3467.743
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002411250393395278,
 'learning_rate_Hydroxylation-K': 0.008113946066242276,
 'learning_rate_Hydroxylation-P': 0.007487090999327279,
 'log_base': 1.0759561256682517,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2010432555,
 'sample_weights': [10.72349902547417, 1.3404890172147783],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.49630682689519,
 'weight_decay_Hydroxylation-K': 5.798000367610388,
 'weight_decay_Hydroxylation-P': 3.6701668577774353}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7462.255
[2,     1] loss: 7421.323
[3,     1] loss: 7424.148
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037817754121851174,
 'learning_rate_Hydroxylation-K': 0.000995700325880331,
 'learning_rate_Hydroxylation-P': 0.004731310797977472,
 'log_base': 2.0401496858036685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 999131149,
 'sample_weights': [22.80358310327722, 2.850557698608636],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.226290591941742,
 'weight_decay_Hydroxylation-K': 6.204083062622573,
 'weight_decay_Hydroxylation-P': 1.5526205933411799}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1403.338
[2,     1] loss: 1404.947
[3,     1] loss: 1403.201
[4,     1] loss: 1393.964
[5,     1] loss: 1401.865
[6,     1] loss: 1396.338
[7,     1] loss: 1379.040
[8,     1] loss: 1365.759
[9,     1] loss: 1335.411
[10,     1] loss: 1294.705
[11,     1] loss: 1258.168
[12,     1] loss: 1255.067
[13,     1] loss: 1227.880
[14,     1] loss: 1166.962
[15,     1] loss: 1181.555
[16,     1] loss: 1180.079
[17,     1] loss: 1179.782
[18,     1] loss: 1173.750
[19,     1] loss: 1137.545
[20,     1] loss: 1180.336
[21,     1] loss: 1130.151
[22,     1] loss: 1138.664
[23,     1] loss: 1119.297
[24,     1] loss: 1077.445
[25,     1] loss: 1093.053
[26,     1] loss: 1044.067
[27,     1] loss: 1058.172
[28,     1] loss: 996.582
[29,     1] loss: 1001.152
[30,     1] loss: 1036.000
[31,     1] loss: 1062.169
[32,     1] loss: 1002.802
[33,     1] loss: 1064.137
[34,     1] loss: 984.726
[35,     1] loss: 1107.288
[36,     1] loss: 999.291
[37,     1] loss: 1098.130
[38,     1] loss: 1006.062
[39,     1] loss: 1029.060
[40,     1] loss: 994.334
[41,     1] loss: 993.619
[42,     1] loss: 1022.954
[43,     1] loss: 909.229
[44,     1] loss: 1042.939
[45,     1] loss: 932.347
[46,     1] loss: 998.603
[47,     1] loss: 1009.999
[48,     1] loss: 905.035
[49,     1] loss: 938.959
[50,     1] loss: 902.818
[51,     1] loss: 895.097
[52,     1] loss: 920.916
[53,     1] loss: 865.488
[54,     1] loss: 941.920
[55,     1] loss: 815.924
[56,     1] loss: 879.819
[57,     1] loss: 814.327
[58,     1] loss: 800.693
[59,     1] loss: 747.123
[60,     1] loss: 819.773
[61,     1] loss: 866.429
[62,     1] loss: 723.531
[63,     1] loss: 769.544
[64,     1] loss: 722.051
[65,     1] loss: 824.942
[66,     1] loss: 730.937
[67,     1] loss: 757.472
[68,     1] loss: 678.009
[69,     1] loss: 763.556
[70,     1] loss: 676.905
[71,     1] loss: 663.112
[72,     1] loss: 664.269
[73,     1] loss: 704.494
[74,     1] loss: 788.457
[75,     1] loss: 909.820
[76,     1] loss: 1064.286
[77,     1] loss: 741.765
[78,     1] loss: 895.017
Early stopping applied (best metric=0.37916532158851624)
Finished Training
Total time taken: 12.610018968582153
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.453
[2,     1] loss: 1408.179
[3,     1] loss: 1401.457
[4,     1] loss: 1406.223
[5,     1] loss: 1401.076
[6,     1] loss: 1401.958
[7,     1] loss: 1399.179
[8,     1] loss: 1398.397
[9,     1] loss: 1392.211
[10,     1] loss: 1386.957
[11,     1] loss: 1373.389
[12,     1] loss: 1351.145
[13,     1] loss: 1331.616
[14,     1] loss: 1299.454
[15,     1] loss: 1244.969
[16,     1] loss: 1251.881
[17,     1] loss: 1196.213
[18,     1] loss: 1187.779
[19,     1] loss: 1191.686
[20,     1] loss: 1227.112
[21,     1] loss: 1191.028
[22,     1] loss: 1188.088
[23,     1] loss: 1197.517
[24,     1] loss: 1139.250
[25,     1] loss: 1153.056
[26,     1] loss: 1116.616
[27,     1] loss: 1142.454
[28,     1] loss: 1106.327
[29,     1] loss: 1082.546
[30,     1] loss: 1113.759
[31,     1] loss: 1074.480
[32,     1] loss: 1074.519
[33,     1] loss: 1050.880
[34,     1] loss: 1066.769
[35,     1] loss: 980.488
[36,     1] loss: 1031.703
[37,     1] loss: 960.636
[38,     1] loss: 1007.347
[39,     1] loss: 950.771
[40,     1] loss: 946.982
[41,     1] loss: 963.749
[42,     1] loss: 897.663
[43,     1] loss: 948.061
[44,     1] loss: 1062.698
[45,     1] loss: 980.287
[46,     1] loss: 866.990
[47,     1] loss: 992.191
[48,     1] loss: 884.525
[49,     1] loss: 917.727
[50,     1] loss: 810.265
[51,     1] loss: 860.076
[52,     1] loss: 814.142
[53,     1] loss: 797.372
[54,     1] loss: 817.587
[55,     1] loss: 855.929
[56,     1] loss: 813.972
[57,     1] loss: 898.084
[58,     1] loss: 979.271
[59,     1] loss: 802.951
[60,     1] loss: 922.943
[61,     1] loss: 843.551
[62,     1] loss: 894.436
[63,     1] loss: 788.372
[64,     1] loss: 861.331
[65,     1] loss: 755.368
[66,     1] loss: 760.063
[67,     1] loss: 722.121
[68,     1] loss: 827.430
[69,     1] loss: 740.086
[70,     1] loss: 682.809
[71,     1] loss: 733.622
[72,     1] loss: 834.342
[73,     1] loss: 1124.815
[74,     1] loss: 677.791
[75,     1] loss: 825.217
[76,     1] loss: 696.782
[77,     1] loss: 864.325
[78,     1] loss: 796.828
[79,     1] loss: 665.599
[80,     1] loss: 839.965
[81,     1] loss: 683.887
[82,     1] loss: 802.114
[83,     1] loss: 616.980
[84,     1] loss: 707.540
[85,     1] loss: 614.025
[86,     1] loss: 625.777
[87,     1] loss: 644.469
[88,     1] loss: 590.583
[89,     1] loss: 624.490
[90,     1] loss: 653.159
[91,     1] loss: 542.522
[92,     1] loss: 539.524
[93,     1] loss: 577.065
[94,     1] loss: 543.173
[95,     1] loss: 530.468
[96,     1] loss: 513.485
[97,     1] loss: 560.406
[98,     1] loss: 1051.417
[99,     1] loss: 774.393
[100,     1] loss: 538.917
[101,     1] loss: 707.646
[102,     1] loss: 679.037
[103,     1] loss: 627.786
[104,     1] loss: 745.967
[105,     1] loss: 547.398
[106,     1] loss: 685.333
[107,     1] loss: 471.152
[108,     1] loss: 717.294
[109,     1] loss: 562.531
[110,     1] loss: 624.624
[111,     1] loss: 562.282
[112,     1] loss: 499.777
[113,     1] loss: 522.371
[114,     1] loss: 500.145
[115,     1] loss: 545.657
[116,     1] loss: 502.606
[117,     1] loss: 474.324
[118,     1] loss: 457.479
[119,     1] loss: 436.907
[120,     1] loss: 421.755
[121,     1] loss: 434.061
[122,     1] loss: 476.416
[123,     1] loss: 552.384
[124,     1] loss: 557.423
[125,     1] loss: 542.400
[126,     1] loss: 428.733
[127,     1] loss: 622.333
[128,     1] loss: 575.820
[129,     1] loss: 479.968
[130,     1] loss: 585.869
[131,     1] loss: 451.050
[132,     1] loss: 538.490
[133,     1] loss: 737.269
[134,     1] loss: 570.714
[135,     1] loss: 601.079
[136,     1] loss: 745.431
[137,     1] loss: 504.377
[138,     1] loss: 583.792
[139,     1] loss: 485.289
[140,     1] loss: 573.444
[141,     1] loss: 468.764
[142,     1] loss: 573.348
[143,     1] loss: 472.658
[144,     1] loss: 478.026
[145,     1] loss: 528.710
[146,     1] loss: 396.576
[147,     1] loss: 476.888
[148,     1] loss: 433.057
[149,     1] loss: 406.528
[150,     1] loss: 403.507
Early stopping applied (best metric=0.36426934599876404)
Finished Training
Total time taken: 23.716078758239746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.711
[2,     1] loss: 1405.456
[3,     1] loss: 1404.034
[4,     1] loss: 1402.347
[5,     1] loss: 1402.397
[6,     1] loss: 1403.509
[7,     1] loss: 1401.733
[8,     1] loss: 1396.677
[9,     1] loss: 1394.145
[10,     1] loss: 1386.486
[11,     1] loss: 1380.128
[12,     1] loss: 1367.993
[13,     1] loss: 1345.655
[14,     1] loss: 1307.610
[15,     1] loss: 1299.535
[16,     1] loss: 1256.218
[17,     1] loss: 1264.187
[18,     1] loss: 1219.935
[19,     1] loss: 1178.610
[20,     1] loss: 1150.604
[21,     1] loss: 1145.482
[22,     1] loss: 1140.592
[23,     1] loss: 1154.778
[24,     1] loss: 1141.149
[25,     1] loss: 1208.001
[26,     1] loss: 1114.208
[27,     1] loss: 1085.244
[28,     1] loss: 1100.838
[29,     1] loss: 1098.587
[30,     1] loss: 1090.186
[31,     1] loss: 1050.473
[32,     1] loss: 1038.834
[33,     1] loss: 1041.813
[34,     1] loss: 1076.772
[35,     1] loss: 1025.523
[36,     1] loss: 990.115
[37,     1] loss: 1010.403
[38,     1] loss: 1029.790
[39,     1] loss: 1027.928
[40,     1] loss: 1065.456
[41,     1] loss: 993.112
[42,     1] loss: 966.508
[43,     1] loss: 965.352
[44,     1] loss: 972.059
[45,     1] loss: 933.925
[46,     1] loss: 939.826
[47,     1] loss: 925.258
[48,     1] loss: 888.282
[49,     1] loss: 843.917
[50,     1] loss: 854.227
[51,     1] loss: 853.505
[52,     1] loss: 828.247
[53,     1] loss: 900.447
[54,     1] loss: 1152.751
[55,     1] loss: 1000.648
[56,     1] loss: 862.239
[57,     1] loss: 914.311
[58,     1] loss: 913.090
[59,     1] loss: 832.865
[60,     1] loss: 861.693
[61,     1] loss: 854.557
[62,     1] loss: 830.330
[63,     1] loss: 826.017
[64,     1] loss: 779.622
[65,     1] loss: 783.428
[66,     1] loss: 783.763
[67,     1] loss: 725.448
[68,     1] loss: 713.921
[69,     1] loss: 789.186
[70,     1] loss: 724.316
[71,     1] loss: 837.401
[72,     1] loss: 675.170
[73,     1] loss: 788.975
[74,     1] loss: 659.467
[75,     1] loss: 713.144
[76,     1] loss: 680.240
[77,     1] loss: 687.086
[78,     1] loss: 665.189
[79,     1] loss: 734.031
[80,     1] loss: 603.553
[81,     1] loss: 690.399
[82,     1] loss: 605.386
[83,     1] loss: 771.413
[84,     1] loss: 638.432
[85,     1] loss: 564.604
[86,     1] loss: 666.781
[87,     1] loss: 591.820
[88,     1] loss: 637.921
Early stopping applied (best metric=0.38853371143341064)
Finished Training
Total time taken: 14.234050989151001
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1405.265
[2,     1] loss: 1410.915
[3,     1] loss: 1397.096
[4,     1] loss: 1403.469
[5,     1] loss: 1400.373
[6,     1] loss: 1396.328
[7,     1] loss: 1392.021
[8,     1] loss: 1382.844
[9,     1] loss: 1369.962
[10,     1] loss: 1348.833
[11,     1] loss: 1324.000
[12,     1] loss: 1287.996
[13,     1] loss: 1253.565
[14,     1] loss: 1249.605
[15,     1] loss: 1245.294
[16,     1] loss: 1237.510
[17,     1] loss: 1202.827
[18,     1] loss: 1202.000
[19,     1] loss: 1190.054
[20,     1] loss: 1215.380
[21,     1] loss: 1169.917
[22,     1] loss: 1204.806
[23,     1] loss: 1158.836
[24,     1] loss: 1153.714
[25,     1] loss: 1153.535
[26,     1] loss: 1134.813
[27,     1] loss: 1134.040
[28,     1] loss: 1144.028
[29,     1] loss: 1154.998
[30,     1] loss: 1127.619
[31,     1] loss: 1108.256
[32,     1] loss: 1039.195
[33,     1] loss: 1063.646
[34,     1] loss: 1015.743
[35,     1] loss: 1116.922
[36,     1] loss: 1051.084
[37,     1] loss: 1121.624
[38,     1] loss: 1021.257
[39,     1] loss: 1066.602
[40,     1] loss: 1053.537
[41,     1] loss: 1025.516
[42,     1] loss: 1040.480
[43,     1] loss: 1016.368
[44,     1] loss: 999.878
[45,     1] loss: 964.664
[46,     1] loss: 944.383
[47,     1] loss: 983.878
[48,     1] loss: 991.372
[49,     1] loss: 989.863
[50,     1] loss: 940.018
[51,     1] loss: 1043.042
[52,     1] loss: 977.306
[53,     1] loss: 1022.155
[54,     1] loss: 876.163
[55,     1] loss: 982.162
[56,     1] loss: 898.345
[57,     1] loss: 892.122
[58,     1] loss: 844.515
[59,     1] loss: 947.531
[60,     1] loss: 892.618
[61,     1] loss: 854.270
[62,     1] loss: 914.382
[63,     1] loss: 883.945
[64,     1] loss: 912.112
[65,     1] loss: 843.517
[66,     1] loss: 818.092
[67,     1] loss: 882.124
[68,     1] loss: 772.124
[69,     1] loss: 718.132
[70,     1] loss: 791.641
[71,     1] loss: 822.963
[72,     1] loss: 915.108
[73,     1] loss: 714.085
[74,     1] loss: 778.630
[75,     1] loss: 803.881
[76,     1] loss: 695.255
[77,     1] loss: 733.521
[78,     1] loss: 628.909
[79,     1] loss: 724.257
[80,     1] loss: 709.825
[81,     1] loss: 643.486
[82,     1] loss: 804.231
[83,     1] loss: 814.302
[84,     1] loss: 666.219
[85,     1] loss: 682.633
[86,     1] loss: 640.129
[87,     1] loss: 684.867
[88,     1] loss: 708.037
[89,     1] loss: 585.561
[90,     1] loss: 761.347
[91,     1] loss: 718.986
[92,     1] loss: 605.165
[93,     1] loss: 736.827
[94,     1] loss: 640.601
[95,     1] loss: 600.993
[96,     1] loss: 648.815
[97,     1] loss: 526.932
[98,     1] loss: 642.237
[99,     1] loss: 484.100
[100,     1] loss: 572.078
[101,     1] loss: 630.211
[102,     1] loss: 498.332
[103,     1] loss: 643.247
[104,     1] loss: 771.474
[105,     1] loss: 548.415
[106,     1] loss: 659.797
[107,     1] loss: 518.344
[108,     1] loss: 587.553
[109,     1] loss: 491.029
[110,     1] loss: 531.507
[111,     1] loss: 488.186
[112,     1] loss: 566.032
[113,     1] loss: 480.287
[114,     1] loss: 446.463
[115,     1] loss: 473.096
[116,     1] loss: 472.443
[117,     1] loss: 425.320
[118,     1] loss: 566.261
[119,     1] loss: 796.142
[120,     1] loss: 623.335
[121,     1] loss: 446.435
[122,     1] loss: 561.575
[123,     1] loss: 448.178
[124,     1] loss: 473.951
[125,     1] loss: 450.992
[126,     1] loss: 459.343
[127,     1] loss: 420.680
[128,     1] loss: 442.902
[129,     1] loss: 520.959
[130,     1] loss: 544.360
[131,     1] loss: 424.976
[132,     1] loss: 439.090
[133,     1] loss: 545.105
[134,     1] loss: 454.412
Early stopping applied (best metric=0.3038499057292938)
Finished Training
Total time taken: 20.21454644203186
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1408.733
[2,     1] loss: 1401.790
[3,     1] loss: 1406.656
[4,     1] loss: 1404.192
[5,     1] loss: 1399.502
[6,     1] loss: 1406.371
[7,     1] loss: 1399.047
[8,     1] loss: 1398.720
[9,     1] loss: 1395.979
[10,     1] loss: 1387.102
[11,     1] loss: 1380.146
[12,     1] loss: 1363.097
[13,     1] loss: 1336.232
[14,     1] loss: 1302.199
[15,     1] loss: 1271.334
[16,     1] loss: 1242.651
[17,     1] loss: 1230.686
[18,     1] loss: 1176.173
[19,     1] loss: 1170.451
[20,     1] loss: 1224.768
[21,     1] loss: 1148.631
[22,     1] loss: 1168.606
[23,     1] loss: 1141.386
[24,     1] loss: 1155.259
[25,     1] loss: 1144.890
[26,     1] loss: 1125.226
[27,     1] loss: 1105.401
[28,     1] loss: 1087.369
[29,     1] loss: 1083.945
[30,     1] loss: 1082.381
[31,     1] loss: 1077.672
[32,     1] loss: 1055.222
[33,     1] loss: 1068.389
[34,     1] loss: 1031.577
[35,     1] loss: 1029.946
[36,     1] loss: 1028.523
[37,     1] loss: 956.968
[38,     1] loss: 1032.118
[39,     1] loss: 1049.919
[40,     1] loss: 960.226
[41,     1] loss: 1005.444
[42,     1] loss: 999.855
[43,     1] loss: 932.460
[44,     1] loss: 977.144
[45,     1] loss: 935.301
[46,     1] loss: 859.837
[47,     1] loss: 944.109
[48,     1] loss: 870.391
[49,     1] loss: 831.701
[50,     1] loss: 817.532
[51,     1] loss: 839.314
[52,     1] loss: 804.168
[53,     1] loss: 799.292
[54,     1] loss: 965.946
[55,     1] loss: 1343.646
[56,     1] loss: 791.163
[57,     1] loss: 1084.395
[58,     1] loss: 895.470
[59,     1] loss: 936.885
[60,     1] loss: 976.882
[61,     1] loss: 937.816
[62,     1] loss: 864.420
[63,     1] loss: 871.938
[64,     1] loss: 940.454
[65,     1] loss: 900.870
[66,     1] loss: 807.495
[67,     1] loss: 861.072
[68,     1] loss: 791.795
[69,     1] loss: 809.881
[70,     1] loss: 768.775
[71,     1] loss: 755.392
[72,     1] loss: 805.356
[73,     1] loss: 784.995
[74,     1] loss: 695.865
[75,     1] loss: 695.899
[76,     1] loss: 637.833
[77,     1] loss: 697.231
[78,     1] loss: 612.632
[79,     1] loss: 674.182
[80,     1] loss: 766.274
[81,     1] loss: 644.160
[82,     1] loss: 617.254
Early stopping applied (best metric=0.35800832509994507)
Finished Training
Total time taken: 11.854151487350464
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.142
[2,     1] loss: 1400.881
[3,     1] loss: 1401.597
[4,     1] loss: 1400.935
[5,     1] loss: 1400.529
[6,     1] loss: 1401.657
[7,     1] loss: 1399.834
[8,     1] loss: 1395.701
[9,     1] loss: 1396.464
[10,     1] loss: 1388.606
[11,     1] loss: 1383.828
[12,     1] loss: 1368.100
[13,     1] loss: 1344.140
[14,     1] loss: 1311.105
[15,     1] loss: 1271.322
[16,     1] loss: 1223.214
[17,     1] loss: 1190.804
[18,     1] loss: 1200.785
[19,     1] loss: 1165.364
[20,     1] loss: 1155.231
[21,     1] loss: 1135.053
[22,     1] loss: 1152.666
[23,     1] loss: 1147.659
[24,     1] loss: 1154.855
[25,     1] loss: 1091.911
[26,     1] loss: 1081.186
[27,     1] loss: 1041.887
[28,     1] loss: 1127.205
[29,     1] loss: 1034.371
[30,     1] loss: 1045.236
[31,     1] loss: 1042.062
[32,     1] loss: 1025.131
[33,     1] loss: 1006.757
[34,     1] loss: 973.407
[35,     1] loss: 979.629
[36,     1] loss: 964.543
[37,     1] loss: 906.331
[38,     1] loss: 995.855
[39,     1] loss: 961.051
[40,     1] loss: 943.467
[41,     1] loss: 1041.229
[42,     1] loss: 970.350
[43,     1] loss: 967.699
[44,     1] loss: 934.963
[45,     1] loss: 1018.492
[46,     1] loss: 926.819
[47,     1] loss: 897.025
[48,     1] loss: 887.902
[49,     1] loss: 826.626
[50,     1] loss: 834.545
[51,     1] loss: 866.056
[52,     1] loss: 801.771
[53,     1] loss: 793.475
[54,     1] loss: 940.935
[55,     1] loss: 890.137
[56,     1] loss: 902.432
[57,     1] loss: 740.592
[58,     1] loss: 824.182
[59,     1] loss: 821.509
[60,     1] loss: 798.026
[61,     1] loss: 804.057
[62,     1] loss: 747.170
[63,     1] loss: 732.458
[64,     1] loss: 720.159
[65,     1] loss: 722.989
[66,     1] loss: 803.239
[67,     1] loss: 669.132
[68,     1] loss: 767.160
[69,     1] loss: 692.938
[70,     1] loss: 650.486
[71,     1] loss: 664.664
[72,     1] loss: 655.819
[73,     1] loss: 641.221
[74,     1] loss: 659.548
[75,     1] loss: 560.862
[76,     1] loss: 690.176
[77,     1] loss: 964.494
[78,     1] loss: 1578.051
[79,     1] loss: 671.710
[80,     1] loss: 943.948
Early stopping applied (best metric=0.3538646697998047)
Finished Training
Total time taken: 11.649158000946045
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.653
[2,     1] loss: 1403.009
[3,     1] loss: 1401.791
[4,     1] loss: 1397.682
[5,     1] loss: 1394.610
[6,     1] loss: 1397.778
[7,     1] loss: 1398.776
[8,     1] loss: 1394.524
[9,     1] loss: 1392.340
[10,     1] loss: 1383.705
[11,     1] loss: 1371.408
[12,     1] loss: 1348.966
[13,     1] loss: 1318.204
[14,     1] loss: 1267.191
[15,     1] loss: 1247.034
[16,     1] loss: 1207.665
[17,     1] loss: 1184.708
[18,     1] loss: 1184.630
[19,     1] loss: 1229.774
[20,     1] loss: 1145.615
[21,     1] loss: 1162.539
[22,     1] loss: 1137.118
[23,     1] loss: 1163.620
[24,     1] loss: 1071.172
[25,     1] loss: 1070.885
[26,     1] loss: 1129.787
[27,     1] loss: 1094.699
[28,     1] loss: 1076.978
[29,     1] loss: 1053.894
[30,     1] loss: 1004.233
[31,     1] loss: 999.814
[32,     1] loss: 1024.099
[33,     1] loss: 967.794
[34,     1] loss: 966.255
[35,     1] loss: 1043.266
[36,     1] loss: 1044.830
[37,     1] loss: 956.291
[38,     1] loss: 991.313
[39,     1] loss: 940.881
[40,     1] loss: 961.685
[41,     1] loss: 959.119
[42,     1] loss: 956.250
[43,     1] loss: 862.967
[44,     1] loss: 837.197
[45,     1] loss: 879.236
[46,     1] loss: 811.801
[47,     1] loss: 940.558
[48,     1] loss: 1091.124
[49,     1] loss: 1018.973
[50,     1] loss: 866.194
[51,     1] loss: 938.832
[52,     1] loss: 915.309
[53,     1] loss: 851.936
[54,     1] loss: 876.555
[55,     1] loss: 826.524
[56,     1] loss: 804.441
[57,     1] loss: 850.160
[58,     1] loss: 754.135
[59,     1] loss: 771.732
[60,     1] loss: 757.205
[61,     1] loss: 721.373
[62,     1] loss: 741.540
[63,     1] loss: 648.981
[64,     1] loss: 651.338
[65,     1] loss: 730.173
[66,     1] loss: 976.231
[67,     1] loss: 770.163
[68,     1] loss: 731.722
[69,     1] loss: 731.360
[70,     1] loss: 708.135
[71,     1] loss: 800.628
[72,     1] loss: 629.191
[73,     1] loss: 735.451
[74,     1] loss: 712.262
[75,     1] loss: 650.693
[76,     1] loss: 629.564
[77,     1] loss: 603.489
[78,     1] loss: 645.293
[79,     1] loss: 628.058
[80,     1] loss: 733.904
[81,     1] loss: 662.265
[82,     1] loss: 590.131
[83,     1] loss: 549.039
[84,     1] loss: 648.834
[85,     1] loss: 538.757
[86,     1] loss: 582.725
[87,     1] loss: 580.773
[88,     1] loss: 528.035
[89,     1] loss: 615.050
[90,     1] loss: 890.307
Early stopping applied (best metric=0.3498813509941101)
Finished Training
Total time taken: 13.388021469116211
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1403.452
[2,     1] loss: 1398.874
[3,     1] loss: 1398.262
[4,     1] loss: 1407.583
[5,     1] loss: 1399.167
[6,     1] loss: 1391.914
[7,     1] loss: 1384.577
[8,     1] loss: 1370.690
[9,     1] loss: 1346.784
[10,     1] loss: 1296.137
[11,     1] loss: 1261.417
[12,     1] loss: 1207.276
[13,     1] loss: 1176.256
[14,     1] loss: 1158.560
[15,     1] loss: 1150.058
[16,     1] loss: 1127.957
[17,     1] loss: 1134.812
[18,     1] loss: 1101.798
[19,     1] loss: 1097.200
[20,     1] loss: 1114.984
[21,     1] loss: 1169.489
[22,     1] loss: 1094.887
[23,     1] loss: 1076.241
[24,     1] loss: 1015.213
[25,     1] loss: 1087.750
[26,     1] loss: 1015.295
[27,     1] loss: 1059.245
[28,     1] loss: 1001.048
[29,     1] loss: 1002.764
[30,     1] loss: 992.405
[31,     1] loss: 961.533
[32,     1] loss: 998.266
[33,     1] loss: 957.473
[34,     1] loss: 1044.350
[35,     1] loss: 991.996
[36,     1] loss: 996.195
[37,     1] loss: 1001.979
[38,     1] loss: 985.596
[39,     1] loss: 936.664
[40,     1] loss: 978.922
[41,     1] loss: 896.551
[42,     1] loss: 1019.215
[43,     1] loss: 915.888
[44,     1] loss: 862.116
[45,     1] loss: 1006.934
[46,     1] loss: 924.323
[47,     1] loss: 942.990
[48,     1] loss: 877.399
[49,     1] loss: 924.118
[50,     1] loss: 863.825
[51,     1] loss: 949.418
[52,     1] loss: 796.236
[53,     1] loss: 878.038
[54,     1] loss: 826.507
[55,     1] loss: 809.284
[56,     1] loss: 796.708
[57,     1] loss: 760.595
[58,     1] loss: 809.422
[59,     1] loss: 736.945
[60,     1] loss: 749.907
[61,     1] loss: 771.540
[62,     1] loss: 697.565
[63,     1] loss: 884.527
[64,     1] loss: 808.377
[65,     1] loss: 758.492
[66,     1] loss: 771.948
[67,     1] loss: 792.406
[68,     1] loss: 721.281
[69,     1] loss: 834.970
[70,     1] loss: 647.845
[71,     1] loss: 771.821
[72,     1] loss: 708.717
[73,     1] loss: 832.551
[74,     1] loss: 697.552
[75,     1] loss: 716.768
[76,     1] loss: 576.603
[77,     1] loss: 651.752
[78,     1] loss: 655.420
[79,     1] loss: 639.087
[80,     1] loss: 652.527
[81,     1] loss: 674.962
[82,     1] loss: 602.506
[83,     1] loss: 603.584
[84,     1] loss: 536.882
[85,     1] loss: 633.949
[86,     1] loss: 604.803
[87,     1] loss: 587.647
[88,     1] loss: 539.352
[89,     1] loss: 538.766
[90,     1] loss: 594.136
[91,     1] loss: 722.404
[92,     1] loss: 583.067
[93,     1] loss: 526.154
[94,     1] loss: 548.753
[95,     1] loss: 500.690
[96,     1] loss: 597.163
[97,     1] loss: 555.144
[98,     1] loss: 507.199
[99,     1] loss: 576.099
[100,     1] loss: 563.313
[101,     1] loss: 509.524
[102,     1] loss: 593.837
[103,     1] loss: 579.106
[104,     1] loss: 483.459
[105,     1] loss: 495.360
[106,     1] loss: 438.730
[107,     1] loss: 458.546
[108,     1] loss: 448.326
[109,     1] loss: 520.353
[110,     1] loss: 431.813
[111,     1] loss: 471.409
[112,     1] loss: 459.159
[113,     1] loss: 510.651
Early stopping applied (best metric=0.36013326048851013)
Finished Training
Total time taken: 16.87558078765869
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1413.236
[2,     1] loss: 1404.444
[3,     1] loss: 1400.759
[4,     1] loss: 1402.392
[5,     1] loss: 1403.042
[6,     1] loss: 1398.777
[7,     1] loss: 1398.405
[8,     1] loss: 1398.091
[9,     1] loss: 1396.490
[10,     1] loss: 1389.407
[11,     1] loss: 1380.964
[12,     1] loss: 1363.928
[13,     1] loss: 1353.848
[14,     1] loss: 1326.409
[15,     1] loss: 1283.592
[16,     1] loss: 1259.300
[17,     1] loss: 1222.266
[18,     1] loss: 1206.357
[19,     1] loss: 1231.230
[20,     1] loss: 1178.766
[21,     1] loss: 1251.771
[22,     1] loss: 1115.525
[23,     1] loss: 1159.345
[24,     1] loss: 1144.521
[25,     1] loss: 1138.518
[26,     1] loss: 1087.635
[27,     1] loss: 1114.101
[28,     1] loss: 1101.016
[29,     1] loss: 1048.814
[30,     1] loss: 1097.355
[31,     1] loss: 1016.550
[32,     1] loss: 1006.478
[33,     1] loss: 1013.119
[34,     1] loss: 975.357
[35,     1] loss: 1069.732
[36,     1] loss: 1121.508
[37,     1] loss: 923.974
[38,     1] loss: 1039.605
[39,     1] loss: 999.684
[40,     1] loss: 979.218
[41,     1] loss: 1018.340
[42,     1] loss: 947.209
[43,     1] loss: 904.272
[44,     1] loss: 974.399
[45,     1] loss: 939.671
[46,     1] loss: 906.929
[47,     1] loss: 890.719
[48,     1] loss: 895.525
[49,     1] loss: 834.090
[50,     1] loss: 906.391
[51,     1] loss: 883.432
[52,     1] loss: 817.974
[53,     1] loss: 822.760
[54,     1] loss: 865.147
[55,     1] loss: 805.948
[56,     1] loss: 829.372
[57,     1] loss: 894.925
[58,     1] loss: 792.495
[59,     1] loss: 760.853
[60,     1] loss: 761.523
[61,     1] loss: 730.498
[62,     1] loss: 682.809
[63,     1] loss: 671.791
[64,     1] loss: 715.475
[65,     1] loss: 753.204
[66,     1] loss: 850.984
[67,     1] loss: 858.471
[68,     1] loss: 671.599
[69,     1] loss: 689.143
[70,     1] loss: 683.680
[71,     1] loss: 729.717
[72,     1] loss: 658.188
[73,     1] loss: 663.803
[74,     1] loss: 648.483
[75,     1] loss: 593.549
[76,     1] loss: 585.182
[77,     1] loss: 636.366
[78,     1] loss: 705.871
[79,     1] loss: 648.356
[80,     1] loss: 630.003
[81,     1] loss: 583.622
[82,     1] loss: 523.884
[83,     1] loss: 542.473
[84,     1] loss: 610.964
[85,     1] loss: 574.434
[86,     1] loss: 530.341
[87,     1] loss: 564.361
[88,     1] loss: 715.281
[89,     1] loss: 925.162
[90,     1] loss: 524.586
[91,     1] loss: 790.659
[92,     1] loss: 640.809
Early stopping applied (best metric=0.4184782803058624)
Finished Training
Total time taken: 14.064018726348877
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1411.042
[2,     1] loss: 1401.809
[3,     1] loss: 1408.628
[4,     1] loss: 1410.152
[5,     1] loss: 1405.156
[6,     1] loss: 1404.469
[7,     1] loss: 1405.946
[8,     1] loss: 1404.534
[9,     1] loss: 1402.338
[10,     1] loss: 1404.587
[11,     1] loss: 1403.378
[12,     1] loss: 1401.625
[13,     1] loss: 1400.152
[14,     1] loss: 1398.819
[15,     1] loss: 1398.017
[16,     1] loss: 1397.321
[17,     1] loss: 1387.272
[18,     1] loss: 1380.734
[19,     1] loss: 1366.353
[20,     1] loss: 1340.670
[21,     1] loss: 1314.574
[22,     1] loss: 1287.487
[23,     1] loss: 1247.555
[24,     1] loss: 1216.938
[25,     1] loss: 1185.664
[26,     1] loss: 1187.549
[27,     1] loss: 1218.070
[28,     1] loss: 1157.037
[29,     1] loss: 1134.359
[30,     1] loss: 1140.598
[31,     1] loss: 1184.490
[32,     1] loss: 1153.537
[33,     1] loss: 1167.439
[34,     1] loss: 1129.364
[35,     1] loss: 1178.202
[36,     1] loss: 1101.042
[37,     1] loss: 1142.261
[38,     1] loss: 1121.807
[39,     1] loss: 1066.567
[40,     1] loss: 1094.307
[41,     1] loss: 1091.887
[42,     1] loss: 1045.571
[43,     1] loss: 1039.897
[44,     1] loss: 1020.831
[45,     1] loss: 1019.098
[46,     1] loss: 993.055
[47,     1] loss: 1017.240
[48,     1] loss: 975.189
[49,     1] loss: 998.822
[50,     1] loss: 956.148
[51,     1] loss: 930.810
[52,     1] loss: 913.488
[53,     1] loss: 904.192
[54,     1] loss: 865.866
[55,     1] loss: 856.529
[56,     1] loss: 941.530
[57,     1] loss: 937.157
[58,     1] loss: 906.323
[59,     1] loss: 825.036
[60,     1] loss: 886.967
[61,     1] loss: 849.149
[62,     1] loss: 886.540
[63,     1] loss: 851.108
[64,     1] loss: 807.809
[65,     1] loss: 822.507
[66,     1] loss: 793.396
[67,     1] loss: 895.303
[68,     1] loss: 832.323
[69,     1] loss: 717.451
[70,     1] loss: 783.452
[71,     1] loss: 706.507
[72,     1] loss: 730.520
[73,     1] loss: 766.963
[74,     1] loss: 669.164
[75,     1] loss: 663.881
[76,     1] loss: 668.247
[77,     1] loss: 819.606
[78,     1] loss: 919.655
[79,     1] loss: 687.682
[80,     1] loss: 662.037
[81,     1] loss: 737.182
[82,     1] loss: 699.098
[83,     1] loss: 626.124
[84,     1] loss: 616.288
[85,     1] loss: 700.848
[86,     1] loss: 566.865
[87,     1] loss: 724.038
[88,     1] loss: 925.948
[89,     1] loss: 569.604
[90,     1] loss: 864.414
[91,     1] loss: 582.688
[92,     1] loss: 723.460
[93,     1] loss: 654.821
[94,     1] loss: 632.368
[95,     1] loss: 732.779
Early stopping applied (best metric=0.38968682289123535)
Finished Training
Total time taken: 14.236021995544434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1413.527
[2,     1] loss: 1404.369
[3,     1] loss: 1405.703
[4,     1] loss: 1404.095
[5,     1] loss: 1403.712
[6,     1] loss: 1405.022
[7,     1] loss: 1400.576
[8,     1] loss: 1400.790
[9,     1] loss: 1394.669
[10,     1] loss: 1396.944
[11,     1] loss: 1383.986
[12,     1] loss: 1376.536
[13,     1] loss: 1355.875
[14,     1] loss: 1339.228
[15,     1] loss: 1302.534
[16,     1] loss: 1285.658
[17,     1] loss: 1257.537
[18,     1] loss: 1232.695
[19,     1] loss: 1212.780
[20,     1] loss: 1180.711
[21,     1] loss: 1205.411
[22,     1] loss: 1166.797
[23,     1] loss: 1169.760
[24,     1] loss: 1186.135
[25,     1] loss: 1167.578
[26,     1] loss: 1152.546
[27,     1] loss: 1189.590
[28,     1] loss: 1161.172
[29,     1] loss: 1156.022
[30,     1] loss: 1137.387
[31,     1] loss: 1173.350
[32,     1] loss: 1112.069
[33,     1] loss: 1097.704
[34,     1] loss: 1088.028
[35,     1] loss: 1118.557
[36,     1] loss: 1051.726
[37,     1] loss: 1120.144
[38,     1] loss: 1086.220
[39,     1] loss: 1101.893
[40,     1] loss: 1098.781
[41,     1] loss: 1046.201
[42,     1] loss: 991.782
[43,     1] loss: 1053.665
[44,     1] loss: 1046.781
[45,     1] loss: 987.273
[46,     1] loss: 1015.698
[47,     1] loss: 1019.538
[48,     1] loss: 966.661
[49,     1] loss: 961.104
[50,     1] loss: 975.613
[51,     1] loss: 951.543
[52,     1] loss: 926.501
[53,     1] loss: 913.354
[54,     1] loss: 894.837
[55,     1] loss: 879.756
[56,     1] loss: 894.168
[57,     1] loss: 954.681
[58,     1] loss: 900.265
[59,     1] loss: 824.739
[60,     1] loss: 911.067
[61,     1] loss: 910.931
[62,     1] loss: 903.822
[63,     1] loss: 839.198
[64,     1] loss: 804.555
[65,     1] loss: 821.738
[66,     1] loss: 779.836
[67,     1] loss: 781.711
[68,     1] loss: 840.355
[69,     1] loss: 793.651
[70,     1] loss: 883.672
[71,     1] loss: 1023.451
[72,     1] loss: 825.969
[73,     1] loss: 847.392
[74,     1] loss: 843.203
[75,     1] loss: 839.428
[76,     1] loss: 778.680
[77,     1] loss: 854.718
[78,     1] loss: 722.177
[79,     1] loss: 850.044
[80,     1] loss: 788.451
[81,     1] loss: 717.466
[82,     1] loss: 919.467
[83,     1] loss: 664.779
[84,     1] loss: 850.879
[85,     1] loss: 688.033
[86,     1] loss: 781.055
[87,     1] loss: 716.389
[88,     1] loss: 760.599
[89,     1] loss: 637.026
[90,     1] loss: 704.424
[91,     1] loss: 611.259
[92,     1] loss: 721.398
[93,     1] loss: 595.287
[94,     1] loss: 599.577
[95,     1] loss: 557.511
[96,     1] loss: 531.864
[97,     1] loss: 527.130
[98,     1] loss: 533.468
[99,     1] loss: 673.378
[100,     1] loss: 912.476
[101,     1] loss: 584.290
[102,     1] loss: 802.053
[103,     1] loss: 643.519
[104,     1] loss: 801.856
[105,     1] loss: 602.400
[106,     1] loss: 708.591
[107,     1] loss: 551.725
[108,     1] loss: 586.609
[109,     1] loss: 603.213
[110,     1] loss: 632.032
[111,     1] loss: 516.124
[112,     1] loss: 460.713
[113,     1] loss: 529.876
[114,     1] loss: 465.617
[115,     1] loss: 461.500
[116,     1] loss: 525.308
[117,     1] loss: 450.103
[118,     1] loss: 428.753
[119,     1] loss: 464.046
[120,     1] loss: 471.164
[121,     1] loss: 446.993
[122,     1] loss: 639.567
[123,     1] loss: 1055.053
[124,     1] loss: 596.549
[125,     1] loss: 657.742
[126,     1] loss: 559.910
[127,     1] loss: 721.569
[128,     1] loss: 515.292
[129,     1] loss: 660.929
[130,     1] loss: 522.301
[131,     1] loss: 559.859
[132,     1] loss: 477.812
[133,     1] loss: 550.639
[134,     1] loss: 537.615
[135,     1] loss: 505.873
[136,     1] loss: 499.289
[137,     1] loss: 468.648
[138,     1] loss: 475.769
[139,     1] loss: 398.179
[140,     1] loss: 491.170
[141,     1] loss: 440.263
[142,     1] loss: 377.942
[143,     1] loss: 433.852
[144,     1] loss: 529.734
[145,     1] loss: 542.554
[146,     1] loss: 498.934
[147,     1] loss: 382.291
[148,     1] loss: 550.373
[149,     1] loss: 479.740
[150,     1] loss: 457.742
[151,     1] loss: 580.243
[152,     1] loss: 418.155
[153,     1] loss: 523.751
[154,     1] loss: 682.015
[155,     1] loss: 402.085
[156,     1] loss: 562.639
[157,     1] loss: 544.289
[158,     1] loss: 460.390
[159,     1] loss: 452.770
[160,     1] loss: 355.292
[161,     1] loss: 433.988
[162,     1] loss: 458.111
[163,     1] loss: 352.550
[164,     1] loss: 517.081
[165,     1] loss: 648.489
[166,     1] loss: 518.497
[167,     1] loss: 465.774
[168,     1] loss: 508.612
[169,     1] loss: 428.848
[170,     1] loss: 427.188
[171,     1] loss: 415.450
[172,     1] loss: 353.722
[173,     1] loss: 405.354
[174,     1] loss: 701.697
[175,     1] loss: 818.593
[176,     1] loss: 814.587
[177,     1] loss: 689.771
[178,     1] loss: 663.723
[179,     1] loss: 799.347
[180,     1] loss: 595.805
[181,     1] loss: 692.731
[182,     1] loss: 534.807
[183,     1] loss: 615.915
[184,     1] loss: 459.985
[185,     1] loss: 518.682
[186,     1] loss: 497.971
[187,     1] loss: 495.698
[188,     1] loss: 442.385
[189,     1] loss: 452.804
[190,     1] loss: 395.934
Early stopping applied (best metric=0.27075886726379395)
Finished Training
Total time taken: 28.2145893573761
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1406.697
[2,     1] loss: 1402.104
[3,     1] loss: 1402.172
[4,     1] loss: 1403.238
[5,     1] loss: 1402.842
[6,     1] loss: 1405.096
[7,     1] loss: 1400.982
[8,     1] loss: 1398.677
[9,     1] loss: 1395.932
[10,     1] loss: 1389.606
[11,     1] loss: 1381.440
[12,     1] loss: 1363.718
[13,     1] loss: 1342.649
[14,     1] loss: 1323.682
[15,     1] loss: 1301.463
[16,     1] loss: 1266.416
[17,     1] loss: 1238.601
[18,     1] loss: 1187.982
[19,     1] loss: 1204.437
[20,     1] loss: 1198.278
[21,     1] loss: 1198.313
[22,     1] loss: 1162.613
[23,     1] loss: 1172.586
[24,     1] loss: 1159.572
[25,     1] loss: 1172.703
[26,     1] loss: 1134.410
[27,     1] loss: 1140.947
[28,     1] loss: 1098.755
[29,     1] loss: 1121.613
[30,     1] loss: 1153.138
[31,     1] loss: 1090.397
[32,     1] loss: 1039.800
[33,     1] loss: 1022.663
[34,     1] loss: 1074.480
[35,     1] loss: 983.272
[36,     1] loss: 1043.452
[37,     1] loss: 1039.857
[38,     1] loss: 980.159
[39,     1] loss: 997.426
[40,     1] loss: 1014.865
[41,     1] loss: 1001.563
[42,     1] loss: 975.412
[43,     1] loss: 915.897
[44,     1] loss: 912.396
[45,     1] loss: 926.821
[46,     1] loss: 976.248
[47,     1] loss: 912.050
[48,     1] loss: 889.132
[49,     1] loss: 851.097
[50,     1] loss: 866.073
[51,     1] loss: 850.678
[52,     1] loss: 841.500
[53,     1] loss: 841.478
[54,     1] loss: 799.813
[55,     1] loss: 919.827
[56,     1] loss: 1191.454
[57,     1] loss: 840.104
[58,     1] loss: 974.779
[59,     1] loss: 867.419
[60,     1] loss: 933.004
[61,     1] loss: 865.381
[62,     1] loss: 842.910
[63,     1] loss: 848.292
[64,     1] loss: 797.153
[65,     1] loss: 793.343
[66,     1] loss: 763.888
[67,     1] loss: 808.604
[68,     1] loss: 787.861
[69,     1] loss: 750.623
[70,     1] loss: 787.927
[71,     1] loss: 777.829
[72,     1] loss: 711.027
[73,     1] loss: 742.804
[74,     1] loss: 692.805
[75,     1] loss: 834.002
[76,     1] loss: 719.995
[77,     1] loss: 682.805
[78,     1] loss: 738.797
[79,     1] loss: 748.795
[80,     1] loss: 731.088
[81,     1] loss: 626.486
[82,     1] loss: 608.787
[83,     1] loss: 625.927
[84,     1] loss: 771.376
[85,     1] loss: 909.447
[86,     1] loss: 633.476
[87,     1] loss: 633.145
[88,     1] loss: 684.340
[89,     1] loss: 699.381
[90,     1] loss: 630.404
[91,     1] loss: 645.661
[92,     1] loss: 654.061
[93,     1] loss: 627.602
[94,     1] loss: 599.840
[95,     1] loss: 605.630
[96,     1] loss: 637.576
[97,     1] loss: 611.834
[98,     1] loss: 533.551
[99,     1] loss: 575.968
[100,     1] loss: 572.613
[101,     1] loss: 449.640
[102,     1] loss: 545.949
[103,     1] loss: 504.928
[104,     1] loss: 726.412
[105,     1] loss: 1155.507
[106,     1] loss: 917.946
[107,     1] loss: 824.004
[108,     1] loss: 750.282
[109,     1] loss: 920.139
[110,     1] loss: 905.014
[111,     1] loss: 765.099
[112,     1] loss: 739.473
[113,     1] loss: 873.446
[114,     1] loss: 717.712
[115,     1] loss: 736.619
[116,     1] loss: 652.697
[117,     1] loss: 605.805
[118,     1] loss: 650.372
[119,     1] loss: 680.300
[120,     1] loss: 566.827
[121,     1] loss: 566.388
[122,     1] loss: 539.865
[123,     1] loss: 529.754
[124,     1] loss: 484.160
[125,     1] loss: 493.635
[126,     1] loss: 441.899
[127,     1] loss: 477.208
Early stopping applied (best metric=0.3124295771121979)
Finished Training
Total time taken: 18.98462176322937
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1406.532
[2,     1] loss: 1404.394
[3,     1] loss: 1398.364
[4,     1] loss: 1399.918
[5,     1] loss: 1402.946
[6,     1] loss: 1398.825
[7,     1] loss: 1400.289
[8,     1] loss: 1400.832
[9,     1] loss: 1396.966
[10,     1] loss: 1391.836
[11,     1] loss: 1381.790
[12,     1] loss: 1363.915
[13,     1] loss: 1348.234
[14,     1] loss: 1315.335
[15,     1] loss: 1294.723
[16,     1] loss: 1276.789
[17,     1] loss: 1224.578
[18,     1] loss: 1233.560
[19,     1] loss: 1201.598
[20,     1] loss: 1155.489
[21,     1] loss: 1176.276
[22,     1] loss: 1080.649
[23,     1] loss: 1141.383
[24,     1] loss: 1153.864
[25,     1] loss: 1091.592
[26,     1] loss: 1126.933
[27,     1] loss: 1185.854
[28,     1] loss: 1081.306
[29,     1] loss: 1108.727
[30,     1] loss: 1102.126
[31,     1] loss: 1108.676
[32,     1] loss: 1001.502
[33,     1] loss: 1106.451
[34,     1] loss: 1076.762
[35,     1] loss: 1007.368
[36,     1] loss: 1028.362
[37,     1] loss: 1022.040
[38,     1] loss: 1018.960
[39,     1] loss: 1042.340
[40,     1] loss: 989.270
[41,     1] loss: 969.491
[42,     1] loss: 952.856
[43,     1] loss: 897.318
[44,     1] loss: 851.744
[45,     1] loss: 915.126
[46,     1] loss: 999.856
[47,     1] loss: 914.922
[48,     1] loss: 841.746
[49,     1] loss: 931.185
[50,     1] loss: 839.289
[51,     1] loss: 895.773
[52,     1] loss: 863.718
[53,     1] loss: 810.726
[54,     1] loss: 790.836
[55,     1] loss: 799.616
[56,     1] loss: 796.169
[57,     1] loss: 751.108
[58,     1] loss: 690.611
[59,     1] loss: 752.589
[60,     1] loss: 824.932
[61,     1] loss: 1152.046
[62,     1] loss: 1057.042
[63,     1] loss: 982.823
[64,     1] loss: 888.049
[65,     1] loss: 916.831
[66,     1] loss: 891.059
[67,     1] loss: 857.681
[68,     1] loss: 958.998
[69,     1] loss: 947.045
[70,     1] loss: 860.958
[71,     1] loss: 844.843
[72,     1] loss: 826.827
Early stopping applied (best metric=0.38789689540863037)
Finished Training
Total time taken: 10.666051149368286
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.225
[2,     1] loss: 1405.721
[3,     1] loss: 1401.616
[4,     1] loss: 1398.636
[5,     1] loss: 1399.248
[6,     1] loss: 1398.649
[7,     1] loss: 1390.542
[8,     1] loss: 1385.156
[9,     1] loss: 1370.794
[10,     1] loss: 1350.953
[11,     1] loss: 1309.932
[12,     1] loss: 1312.531
[13,     1] loss: 1240.096
[14,     1] loss: 1226.659
[15,     1] loss: 1203.596
[16,     1] loss: 1178.656
[17,     1] loss: 1104.986
[18,     1] loss: 1186.734
[19,     1] loss: 1131.543
[20,     1] loss: 1100.317
[21,     1] loss: 1122.815
[22,     1] loss: 1153.665
[23,     1] loss: 1115.254
[24,     1] loss: 1065.892
[25,     1] loss: 1135.248
[26,     1] loss: 1087.464
[27,     1] loss: 1121.544
[28,     1] loss: 1078.263
[29,     1] loss: 1088.950
[30,     1] loss: 1002.952
[31,     1] loss: 1017.754
[32,     1] loss: 1014.363
[33,     1] loss: 1033.497
[34,     1] loss: 974.423
[35,     1] loss: 1044.278
[36,     1] loss: 1003.290
[37,     1] loss: 974.970
[38,     1] loss: 1031.649
[39,     1] loss: 995.811
[40,     1] loss: 1046.013
[41,     1] loss: 896.878
[42,     1] loss: 1009.681
[43,     1] loss: 912.572
[44,     1] loss: 930.963
[45,     1] loss: 939.025
[46,     1] loss: 965.347
[47,     1] loss: 850.652
[48,     1] loss: 962.952
[49,     1] loss: 849.403
[50,     1] loss: 949.503
[51,     1] loss: 847.803
[52,     1] loss: 830.716
[53,     1] loss: 860.253
[54,     1] loss: 795.588
[55,     1] loss: 830.444
[56,     1] loss: 876.274
[57,     1] loss: 789.833
[58,     1] loss: 764.311
[59,     1] loss: 795.471
[60,     1] loss: 740.718
[61,     1] loss: 751.680
[62,     1] loss: 729.444
[63,     1] loss: 740.956
[64,     1] loss: 787.853
[65,     1] loss: 915.717
[66,     1] loss: 762.231
[67,     1] loss: 707.108
[68,     1] loss: 833.091
[69,     1] loss: 654.852
[70,     1] loss: 761.322
[71,     1] loss: 727.232
[72,     1] loss: 676.370
[73,     1] loss: 669.719
[74,     1] loss: 689.377
[75,     1] loss: 694.138
[76,     1] loss: 681.185
[77,     1] loss: 880.700
[78,     1] loss: 1033.763
[79,     1] loss: 629.348
[80,     1] loss: 934.732
[81,     1] loss: 702.302
[82,     1] loss: 838.017
[83,     1] loss: 799.579
[84,     1] loss: 719.105
[85,     1] loss: 819.379
[86,     1] loss: 721.075
[87,     1] loss: 705.855
[88,     1] loss: 678.187
[89,     1] loss: 619.744
[90,     1] loss: 645.151
[91,     1] loss: 661.955
[92,     1] loss: 598.098
[93,     1] loss: 634.400
[94,     1] loss: 643.277
[95,     1] loss: 574.413
[96,     1] loss: 570.498
[97,     1] loss: 546.676
[98,     1] loss: 565.469
[99,     1] loss: 640.329
[100,     1] loss: 617.730
[101,     1] loss: 569.607
[102,     1] loss: 634.756
[103,     1] loss: 642.390
[104,     1] loss: 588.164
[105,     1] loss: 557.654
[106,     1] loss: 542.015
[107,     1] loss: 629.606
[108,     1] loss: 785.979
[109,     1] loss: 698.575
[110,     1] loss: 558.137
[111,     1] loss: 673.021
[112,     1] loss: 609.566
[113,     1] loss: 529.168
[114,     1] loss: 580.823
[115,     1] loss: 494.579
[116,     1] loss: 541.532
[117,     1] loss: 542.745
[118,     1] loss: 494.876
[119,     1] loss: 608.732
[120,     1] loss: 695.429
[121,     1] loss: 467.867
[122,     1] loss: 537.437
[123,     1] loss: 550.226
[124,     1] loss: 501.465
[125,     1] loss: 512.073
[126,     1] loss: 475.133
[127,     1] loss: 520.309
[128,     1] loss: 515.115
[129,     1] loss: 527.798
[130,     1] loss: 441.036
[131,     1] loss: 452.567
[132,     1] loss: 414.124
[133,     1] loss: 409.156
[134,     1] loss: 465.145
[135,     1] loss: 523.569
[136,     1] loss: 831.109
Early stopping applied (best metric=0.339946985244751)
Finished Training
Total time taken: 20.541553020477295
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1414.878
[2,     1] loss: 1406.212
[3,     1] loss: 1400.796
[4,     1] loss: 1403.142
[5,     1] loss: 1403.543
[6,     1] loss: 1399.843
[7,     1] loss: 1404.217
[8,     1] loss: 1398.947
[9,     1] loss: 1399.315
[10,     1] loss: 1397.757
[11,     1] loss: 1389.291
[12,     1] loss: 1379.350
[13,     1] loss: 1370.212
[14,     1] loss: 1344.974
[15,     1] loss: 1328.196
[16,     1] loss: 1277.038
[17,     1] loss: 1245.067
[18,     1] loss: 1229.360
[19,     1] loss: 1223.304
[20,     1] loss: 1184.531
[21,     1] loss: 1202.375
[22,     1] loss: 1178.510
[23,     1] loss: 1176.719
[24,     1] loss: 1144.114
[25,     1] loss: 1150.471
[26,     1] loss: 1160.836
[27,     1] loss: 1126.006
[28,     1] loss: 1135.195
[29,     1] loss: 1074.930
[30,     1] loss: 1038.395
[31,     1] loss: 1082.508
[32,     1] loss: 1028.665
[33,     1] loss: 1060.240
[34,     1] loss: 995.230
[35,     1] loss: 1033.242
[36,     1] loss: 1000.308
[37,     1] loss: 1004.447
[38,     1] loss: 1029.388
[39,     1] loss: 1002.653
[40,     1] loss: 947.440
[41,     1] loss: 1011.567
[42,     1] loss: 885.335
[43,     1] loss: 902.902
[44,     1] loss: 910.438
[45,     1] loss: 880.308
[46,     1] loss: 887.513
[47,     1] loss: 817.500
[48,     1] loss: 892.430
[49,     1] loss: 925.105
[50,     1] loss: 949.041
[51,     1] loss: 813.200
[52,     1] loss: 797.680
[53,     1] loss: 813.224
[54,     1] loss: 844.857
[55,     1] loss: 793.334
[56,     1] loss: 840.444
[57,     1] loss: 813.103
[58,     1] loss: 804.672
[59,     1] loss: 821.275
[60,     1] loss: 896.897
[61,     1] loss: 806.414
[62,     1] loss: 760.444
[63,     1] loss: 862.647
[64,     1] loss: 758.962
[65,     1] loss: 895.164
[66,     1] loss: 724.639
[67,     1] loss: 845.017
[68,     1] loss: 712.721
[69,     1] loss: 784.063
[70,     1] loss: 703.923
[71,     1] loss: 771.804
[72,     1] loss: 658.920
[73,     1] loss: 748.127
[74,     1] loss: 648.603
Early stopping applied (best metric=0.38721951842308044)
Finished Training
Total time taken: 10.570631265640259
{'Hydroxylation-K Validation Accuracy': 0.7618498817966903, 'Hydroxylation-K Validation Sensitivity': 0.6518518518518518, 'Hydroxylation-K Validation Specificity': 0.7894736842105263, 'Hydroxylation-K Validation Precision': 0.44830532212885155, 'Hydroxylation-K AUC ROC': 0.7748148148148148, 'Hydroxylation-K AUC PR': 0.5547960325380779, 'Hydroxylation-K MCC': 0.3916647063345677, 'Hydroxylation-K F1': 0.5265065151533497, 'Validation Loss (Hydroxylation-K)': 0.4853008031845093, 'Hydroxylation-P Validation Accuracy': 0.7876027443615383, 'Hydroxylation-P Validation Sensitivity': 0.8123809523809524, 'Hydroxylation-P Validation Specificity': 0.7822883934360816, 'Hydroxylation-P Validation Precision': 0.4467958254493497, 'Hydroxylation-P AUC ROC': 0.8552750918978661, 'Hydroxylation-P AUC PR': 0.580065566497213, 'Hydroxylation-P MCC': 0.4864167032799997, 'Hydroxylation-P F1': 0.5755047022792603, 'Validation Loss (Hydroxylation-P)': 0.3576081891854604, 'Validation Loss (total)': 0.8429089903831481, 'TimeToTrain': 16.12127294540405}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007525329786302685,
 'learning_rate_Hydroxylation-K': 0.00024843554913678187,
 'learning_rate_Hydroxylation-P': 0.008656814864189916,
 'log_base': 1.9387451446855668,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2984229370,
 'sample_weights': [2.3430956850053803, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.208533876770839,
 'weight_decay_Hydroxylation-K': 9.218558554815743,
 'weight_decay_Hydroxylation-P': 2.6794971781472907}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1442.298
[2,     1] loss: 1440.266
[3,     1] loss: 1440.480
[4,     1] loss: 1437.920
[5,     1] loss: 1442.062
[6,     1] loss: 1438.655
[7,     1] loss: 1435.051
[8,     1] loss: 1429.802
[9,     1] loss: 1417.560
[10,     1] loss: 1405.531
[11,     1] loss: 1382.714
[12,     1] loss: 1345.828
[13,     1] loss: 1312.525
[14,     1] loss: 1252.792
[15,     1] loss: 1220.609
[16,     1] loss: 1159.815
[17,     1] loss: 1247.064
[18,     1] loss: 1252.542
[19,     1] loss: 1169.354
[20,     1] loss: 1202.625
[21,     1] loss: 1242.552
[22,     1] loss: 1155.164
[23,     1] loss: 1186.556
[24,     1] loss: 1196.818
[25,     1] loss: 1146.700
[26,     1] loss: 1171.444
[27,     1] loss: 1133.013
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018310839469241683,
 'learning_rate_Hydroxylation-K': 0.009954467301993435,
 'learning_rate_Hydroxylation-P': 0.007447112367191571,
 'log_base': 2.6656283765445257,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1870840789,
 'sample_weights': [2.5216615300772864, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4332053457626963,
 'weight_decay_Hydroxylation-K': 9.15852724170912,
 'weight_decay_Hydroxylation-P': 2.6708240365714815}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.135
[2,     1] loss: 1266.764
[3,     1] loss: 1268.389
[4,     1] loss: 1264.958
[5,     1] loss: 1264.313
[6,     1] loss: 1263.217
[7,     1] loss: 1263.254
[8,     1] loss: 1256.437
[9,     1] loss: 1247.406
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035826568645693385,
 'learning_rate_Hydroxylation-K': 0.004970933560219649,
 'learning_rate_Hydroxylation-P': 0.0020250439250728896,
 'log_base': 1.4408910392771002,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 856655447,
 'sample_weights': [1.7027492316200514, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.10894732688640385,
 'weight_decay_Hydroxylation-K': 2.94620107035128,
 'weight_decay_Hydroxylation-P': 7.122652098355248}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1866.875
[2,     1] loss: 1868.257
[3,     1] loss: 1878.779
[4,     1] loss: 1884.241
[5,     1] loss: 1871.540
[6,     1] loss: 1869.695
[7,     1] loss: 1873.797
[8,     1] loss: 1871.303
[9,     1] loss: 1866.907
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0047230065928701125,
 'learning_rate_Hydroxylation-K': 0.001779025868404639,
 'learning_rate_Hydroxylation-P': 0.009680318335288462,
 'log_base': 1.0542424876737364,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 282222578,
 'sample_weights': [4.570539834067364, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.139862594155854,
 'weight_decay_Hydroxylation-K': 4.921436586289865,
 'weight_decay_Hydroxylation-P': 0.44574916190669234}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10290.338
[2,     1] loss: 10320.438
[3,     1] loss: 10300.098
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0051540263462390755,
 'learning_rate_Hydroxylation-K': 0.0004573202953268799,
 'learning_rate_Hydroxylation-P': 0.0020193779997398306,
 'log_base': 2.991506539347422,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 599472298,
 'sample_weights': [31.604780745826304, 3.9507497861117136],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1608297687580373,
 'weight_decay_Hydroxylation-K': 9.64077025497124,
 'weight_decay_Hydroxylation-P': 5.1833515453226555}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1227.461
[2,     1] loss: 1228.626
[3,     1] loss: 1230.372
[4,     1] loss: 1223.814
[5,     1] loss: 1223.822
[6,     1] loss: 1224.290
[7,     1] loss: 1209.338
[8,     1] loss: 1195.649
[9,     1] loss: 1168.788
[10,     1] loss: 1139.872
[11,     1] loss: 1133.359
[12,     1] loss: 1092.230
[13,     1] loss: 1079.633
[14,     1] loss: 1037.857
[15,     1] loss: 1041.952
[16,     1] loss: 1026.329
[17,     1] loss: 1021.694
[18,     1] loss: 979.911
[19,     1] loss: 994.329
[20,     1] loss: 993.984
[21,     1] loss: 964.688
[22,     1] loss: 1042.399
[23,     1] loss: 971.715
[24,     1] loss: 966.819
[25,     1] loss: 929.119
[26,     1] loss: 987.594
[27,     1] loss: 925.476
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007345282441886275,
 'learning_rate_Hydroxylation-K': 0.004141093450089101,
 'learning_rate_Hydroxylation-P': 0.0027216471301542066,
 'log_base': 1.0823890174900028,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1239083745,
 'sample_weights': [1.523524371174513, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.033536934652563,
 'weight_decay_Hydroxylation-K': 4.753342325497932,
 'weight_decay_Hydroxylation-P': 9.308036414967296}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6944.614
[2,     1] loss: 6923.300
[3,     1] loss: 6870.879
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006123134800853892,
 'learning_rate_Hydroxylation-K': 0.0036793559520168643,
 'learning_rate_Hydroxylation-P': 0.0057562246023605095,
 'log_base': 1.5209815161357239,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1792376410,
 'sample_weights': [21.08664155626688, 2.6359317372969695],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.49178260479844,
 'weight_decay_Hydroxylation-K': 7.236310144559879,
 'weight_decay_Hydroxylation-P': 2.6901996510812114}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1754.284
[2,     1] loss: 1750.792
[3,     1] loss: 1745.961
[4,     1] loss: 1753.272
[5,     1] loss: 1750.339
[6,     1] loss: 1753.497
[7,     1] loss: 1744.982
[8,     1] loss: 1744.453
[9,     1] loss: 1748.449
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00289345067965019,
 'learning_rate_Hydroxylation-K': 0.003971467972604019,
 'learning_rate_Hydroxylation-P': 0.008153966648854993,
 'log_base': 1.0821030805736418,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1945846571,
 'sample_weights': [3.980970110666601, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.817783381054793,
 'weight_decay_Hydroxylation-K': 0.9401596663182457,
 'weight_decay_Hydroxylation-P': 0.9955583652642989}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6870.236
[2,     1] loss: 6892.239
[3,     1] loss: 6883.901
[4,     1] loss: 6851.688
[5,     1] loss: 6859.650
[6,     1] loss: 6876.957
[7,     1] loss: 6862.512
[8,     1] loss: 6855.204
[9,     1] loss: 6834.713
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030152474304149827,
 'learning_rate_Hydroxylation-K': 0.005000278090094073,
 'learning_rate_Hydroxylation-P': 0.006225250103508215,
 'log_base': 2.056229838764702,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1954889897,
 'sample_weights': [21.157247158903896, 2.644757777628014],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4186858202837191,
 'weight_decay_Hydroxylation-K': 7.641503855321113,
 'weight_decay_Hydroxylation-P': 2.762430831934313}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.565
[2,     1] loss: 1396.385
[3,     1] loss: 1397.121
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00782662097664403,
 'learning_rate_Hydroxylation-K': 0.007273113510941404,
 'learning_rate_Hydroxylation-P': 0.008075763905039024,
 'log_base': 2.898064398573238,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 957476142,
 'sample_weights': [2.3158594219086357, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1406727448994007,
 'weight_decay_Hydroxylation-K': 8.453644466116291,
 'weight_decay_Hydroxylation-P': 6.62104218434739}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.114
[2,     1] loss: 1242.919
[3,     1] loss: 1239.243
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004903813817978303,
 'learning_rate_Hydroxylation-K': 0.0042553354358937446,
 'learning_rate_Hydroxylation-P': 0.003800225842664381,
 'log_base': 1.7754261991665896,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2813633537,
 'sample_weights': [1.5689620107858446, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5910843988555855,
 'weight_decay_Hydroxylation-K': 2.3911784584726394,
 'weight_decay_Hydroxylation-P': 9.712791516132116}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1528.104
[2,     1] loss: 1520.936
[3,     1] loss: 1522.791
[4,     1] loss: 1520.492
[5,     1] loss: 1517.491
[6,     1] loss: 1529.536
[7,     1] loss: 1520.433
[8,     1] loss: 1518.463
[9,     1] loss: 1519.601
[10,     1] loss: 1519.426
[11,     1] loss: 1519.457
[12,     1] loss: 1517.611
[13,     1] loss: 1513.909
[14,     1] loss: 1509.791
[15,     1] loss: 1503.138
[16,     1] loss: 1497.853
[17,     1] loss: 1487.297
[18,     1] loss: 1479.829
[19,     1] loss: 1467.152
[20,     1] loss: 1432.028
[21,     1] loss: 1405.134
[22,     1] loss: 1377.806
[23,     1] loss: 1349.610
[24,     1] loss: 1343.300
[25,     1] loss: 1301.400
[26,     1] loss: 1315.219
[27,     1] loss: 1253.486
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003967234962572427,
 'learning_rate_Hydroxylation-K': 0.0034814332073515794,
 'learning_rate_Hydroxylation-P': 0.008583028077544205,
 'log_base': 1.033001870103829,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 523886244,
 'sample_weights': [2.9082323095114604, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.546305715491071,
 'weight_decay_Hydroxylation-K': 1.0215103234268217,
 'weight_decay_Hydroxylation-P': 0.3935128068658673}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16735.211
Exploding loss, terminate run (best metric=0.5391513109207153)
Finished Training
Total time taken: 0.20999979972839355
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16680.773
Exploding loss, terminate run (best metric=0.5317420363426208)
Finished Training
Total time taken: 0.24700069427490234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16781.344
Exploding loss, terminate run (best metric=0.5265746116638184)
Finished Training
Total time taken: 0.21899890899658203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16760.000
Exploding loss, terminate run (best metric=0.5275479555130005)
Finished Training
Total time taken: 0.22100114822387695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16699.172
Exploding loss, terminate run (best metric=0.5295027494430542)
Finished Training
Total time taken: 0.20999693870544434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16649.254
Exploding loss, terminate run (best metric=0.5314527153968811)
Finished Training
Total time taken: 0.19999980926513672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16690.297
Exploding loss, terminate run (best metric=0.5294294357299805)
Finished Training
Total time taken: 0.21200323104858398
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16709.879
Exploding loss, terminate run (best metric=0.5298510193824768)
Finished Training
Total time taken: 0.20599913597106934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16753.061
Exploding loss, terminate run (best metric=0.5266910195350647)
Finished Training
Total time taken: 0.20999884605407715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16697.975
Exploding loss, terminate run (best metric=0.5320978760719299)
Finished Training
Total time taken: 0.2160022258758545
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16651.404
Exploding loss, terminate run (best metric=0.5322794914245605)
Finished Training
Total time taken: 0.21200060844421387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16660.770
Exploding loss, terminate run (best metric=0.5262684226036072)
Finished Training
Total time taken: 0.19099879264831543
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16877.660
Exploding loss, terminate run (best metric=0.5274354219436646)
Finished Training
Total time taken: 0.22299981117248535
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16700.725
Exploding loss, terminate run (best metric=0.5274574756622314)
Finished Training
Total time taken: 0.2160027027130127
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16723.943
Exploding loss, terminate run (best metric=0.5278022289276123)
Finished Training
Total time taken: 0.22699999809265137
{'Hydroxylation-K Validation Accuracy': 0.39828605200945627, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6849122807017544, 'Hydroxylation-K AUC PR': 0.40483555306061486, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22208538587848933, 'Validation Loss (Hydroxylation-K)': 0.5570010383923848, 'Hydroxylation-P Validation Accuracy': 0.39229372451483, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6076194602984593, 'Hydroxylation-P AUC PR': 0.298441792757349, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.20039331036639543, 'Validation Loss (Hydroxylation-P)': 0.5296855847040812, 'Validation Loss (total)': 1.086686627070109, 'TimeToTrain': 0.21466684341430664}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036942157875725556,
 'learning_rate_Hydroxylation-K': 0.004722496901195711,
 'learning_rate_Hydroxylation-P': 0.0072217688684274505,
 'log_base': 1.0106936705989067,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 238312214,
 'sample_weights': [51.45466482069917, 6.41846888977844],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.472202819596516,
 'weight_decay_Hydroxylation-K': 4.393610754124079,
 'weight_decay_Hydroxylation-P': 3.943279367861113}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51028.523
Exploding loss, terminate run (best metric=0.5323114991188049)
Finished Training
Total time taken: 0.18999934196472168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51073.250
Exploding loss, terminate run (best metric=0.5296533703804016)
Finished Training
Total time taken: 0.1940004825592041
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 50876.156
Exploding loss, terminate run (best metric=0.5293753743171692)
Finished Training
Total time taken: 0.20999979972839355
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51369.473
Exploding loss, terminate run (best metric=0.5279313325881958)
Finished Training
Total time taken: 0.21900057792663574
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 51057.016
Exploding loss, terminate run (best metric=0.5274271368980408)
Finished Training
Total time taken: 0.21300029754638672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 50902.559
Exploding loss, terminate run (best metric=0.5389193892478943)
Finished Training
Total time taken: 0.20700311660766602
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 50930.559
Exploding loss, terminate run (best metric=0.5267525911331177)
Finished Training
Total time taken: 0.20600104331970215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 50857.637
Exploding loss, terminate run (best metric=0.5268295407295227)
Finished Training
Total time taken: 0.21300029754638672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51026.520
Exploding loss, terminate run (best metric=0.527350127696991)
Finished Training
Total time taken: 0.2030022144317627
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 51194.938
Exploding loss, terminate run (best metric=0.5269978642463684)
Finished Training
Total time taken: 0.20800185203552246
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 50897.219
Exploding loss, terminate run (best metric=0.5345377326011658)
Finished Training
Total time taken: 0.21300196647644043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 50848.766
Exploding loss, terminate run (best metric=0.5298886895179749)
Finished Training
Total time taken: 0.21799755096435547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 50883.855
Exploding loss, terminate run (best metric=0.5286086797714233)
Finished Training
Total time taken: 0.21200037002563477
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 51138.344
Exploding loss, terminate run (best metric=0.532247006893158)
Finished Training
Total time taken: 0.2169969081878662
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 50936.359
Exploding loss, terminate run (best metric=0.5374135375022888)
Finished Training
Total time taken: 0.1939990520477295
{'Hydroxylation-K Validation Accuracy': 0.4991430260047281, 'Hydroxylation-K Validation Sensitivity': 0.5466666666666666, 'Hydroxylation-K Validation Specificity': 0.48947368421052634, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6179922027290449, 'Hydroxylation-K AUC PR': 0.3351268081220973, 'Hydroxylation-K MCC': 0.034364999424781176, 'Hydroxylation-K F1': 0.20820921472037093, 'Validation Loss (Hydroxylation-K)': 0.5565159281094869, 'Hydroxylation-P Validation Accuracy': 0.4878917144645788, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.47845528455284553, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6169785315659175, 'Hydroxylation-P AUC PR': 0.2651357165890277, 'Hydroxylation-P MCC': 0.015365576274315418, 'Hydroxylation-P F1': 0.17321672047794112, 'Validation Loss (Hydroxylation-P)': 0.5304162581761678, 'Validation Loss (total)': 1.0869321902592977, 'TimeToTrain': 0.20780032475789387}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037862753731162675,
 'learning_rate_Hydroxylation-K': 0.00466351381632061,
 'learning_rate_Hydroxylation-P': 0.007172327214060479,
 'log_base': 1.1580145256488397,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1381872614,
 'sample_weights': [157.06473702100772, 19.59229803096542],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.094405566861312,
 'weight_decay_Hydroxylation-K': 5.483741436923414,
 'weight_decay_Hydroxylation-P': 3.301778447318622}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3679.886
[2,     1] loss: 3758.094
[3,     1] loss: 3696.305
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003135072731085898,
 'learning_rate_Hydroxylation-K': 0.006795430811277279,
 'learning_rate_Hydroxylation-P': 0.006539615072938699,
 'log_base': 2.5233483996977935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 461272469,
 'sample_weights': [11.379443556197787, 1.4224852422573993],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.785062917879756,
 'weight_decay_Hydroxylation-K': 9.48347163261657,
 'weight_decay_Hydroxylation-P': 1.6980947507330444}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.711
[2,     1] loss: 1289.035
[3,     1] loss: 1288.801
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004612574676302833,
 'learning_rate_Hydroxylation-K': 0.0018882611645210945,
 'learning_rate_Hydroxylation-P': 0.009732609042901432,
 'log_base': 1.338123984474921,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4171349763,
 'sample_weights': [1.8036592984702695, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.913796871315568,
 'weight_decay_Hydroxylation-K': 1.1335184360037052,
 'weight_decay_Hydroxylation-P': 1.5741404253715616}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2121.245
[2,     1] loss: 2122.650
[3,     1] loss: 2114.635
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013169727969144906,
 'learning_rate_Hydroxylation-K': 0.009021964174519836,
 'learning_rate_Hydroxylation-P': 0.004754102247830197,
 'log_base': 1.5124260978157125,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1061576020,
 'sample_weights': [5.731627180233406, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.524252298208921,
 'weight_decay_Hydroxylation-K': 8.05252851572865,
 'weight_decay_Hydroxylation-P': 0.19414741256393309}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1762.837
[2,     1] loss: 1763.619
[3,     1] loss: 1767.165
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007219049078217392,
 'learning_rate_Hydroxylation-K': 0.00960871406371135,
 'learning_rate_Hydroxylation-P': 0.003838472636105316,
 'log_base': 2.971426623310046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4113800366,
 'sample_weights': [4.035248783778365, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.441082505208383,
 'weight_decay_Hydroxylation-K': 8.563868385371148,
 'weight_decay_Hydroxylation-P': 1.1935689445953788}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.396
[2,     1] loss: 1234.386
[3,     1] loss: 1232.922
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021794005931336496,
 'learning_rate_Hydroxylation-K': 0.004945191599686249,
 'learning_rate_Hydroxylation-P': 0.005908748805303441,
 'log_base': 1.1721215023889766,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3599351172,
 'sample_weights': [1.532946266878506, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.3233846922329295,
 'weight_decay_Hydroxylation-K': 4.005468463263823,
 'weight_decay_Hydroxylation-P': 0.2890811210574299}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3387.136
[2,     1] loss: 3465.770
[3,     1] loss: 3443.310
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008092556882136004,
 'learning_rate_Hydroxylation-K': 0.0017605678483580767,
 'learning_rate_Hydroxylation-P': 0.008135851866905174,
 'log_base': 1.0114232286506082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1674901240,
 'sample_weights': [10.511849618727513, 1.3140318221733145],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.498365728387674,
 'weight_decay_Hydroxylation-K': 9.074422348705324,
 'weight_decay_Hydroxylation-P': 0.8555843985939288}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47864.602
Exploding loss, terminate run (best metric=0.5379401445388794)
Finished Training
Total time taken: 0.1960005760192871
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47587.582
Exploding loss, terminate run (best metric=0.533724308013916)
Finished Training
Total time taken: 0.20099806785583496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47734.141
Exploding loss, terminate run (best metric=0.5274747014045715)
Finished Training
Total time taken: 0.24000096321105957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47856.750
Exploding loss, terminate run (best metric=0.5266339182853699)
Finished Training
Total time taken: 0.2590017318725586
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 47871.238
Exploding loss, terminate run (best metric=0.5559955835342407)
Finished Training
Total time taken: 0.21900033950805664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47988.391
Exploding loss, terminate run (best metric=0.5380395650863647)
Finished Training
Total time taken: 0.21900248527526855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47825.953
Exploding loss, terminate run (best metric=0.5271612405776978)
Finished Training
Total time taken: 0.24400043487548828
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47785.539
Exploding loss, terminate run (best metric=0.5293787121772766)
Finished Training
Total time taken: 0.24700069427490234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47578.711
Exploding loss, terminate run (best metric=0.5276173949241638)
Finished Training
Total time taken: 0.21600055694580078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 47689.766
Exploding loss, terminate run (best metric=0.5375967025756836)
Finished Training
Total time taken: 0.2330007553100586
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47950.391
Exploding loss, terminate run (best metric=0.5388565063476562)
Finished Training
Total time taken: 0.24000310897827148
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47929.207
Exploding loss, terminate run (best metric=0.5282077193260193)
Finished Training
Total time taken: 0.21900177001953125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47935.867
Exploding loss, terminate run (best metric=0.5268937945365906)
Finished Training
Total time taken: 0.23399853706359863
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 47673.988
Exploding loss, terminate run (best metric=0.5246232151985168)
Finished Training
Total time taken: 0.23299622535705566
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 47628.828
Exploding loss, terminate run (best metric=0.5402835011482239)
Finished Training
Total time taken: 0.21700096130371094
{'Hydroxylation-K Validation Accuracy': 0.44166666666666665, 'Hydroxylation-K Validation Sensitivity': 0.6, 'Hydroxylation-K Validation Specificity': 0.4, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6399415204678363, 'Hydroxylation-K AUC PR': 0.3334261633250362, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.20221674876847293, 'Validation Loss (Hydroxylation-K)': 0.5617084344228108, 'Hydroxylation-P Validation Accuracy': 0.4347235165727628, 'Hydroxylation-P Validation Sensitivity': 0.6, 'Hydroxylation-P Validation Specificity': 0.4, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6271084691711656, 'Hydroxylation-P AUC PR': 0.30844453388045123, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.17988314259500704, 'Validation Loss (Hydroxylation-P)': 0.533361800511678, 'Validation Loss (total)': 1.0950702587763468, 'TimeToTrain': 0.2278004805246989}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0066310894265822675,
 'learning_rate_Hydroxylation-K': 0.009969969640931088,
 'learning_rate_Hydroxylation-P': 0.009857250729670003,
 'log_base': 2.279230930655347,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1491844612,
 'sample_weights': [147.08676621981982, 18.34764323836868],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.099496730395191,
 'weight_decay_Hydroxylation-K': 6.763319711038118,
 'weight_decay_Hydroxylation-P': 0.5008465392525681}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1336.286
[2,     1] loss: 1345.652
[3,     1] loss: 1342.700
[4,     1] loss: 1335.077
[5,     1] loss: 1334.542
[6,     1] loss: 1333.497
[7,     1] loss: 1333.767
[8,     1] loss: 1333.189
[9,     1] loss: 1330.244
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007664736689576021,
 'learning_rate_Hydroxylation-K': 0.0057802279774515405,
 'learning_rate_Hydroxylation-P': 0.005890810501189222,
 'log_base': 2.2099307384717015,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2613895537,
 'sample_weights': [2.026421451240741, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.608600335006123,
 'weight_decay_Hydroxylation-K': 1.0130787366154284,
 'weight_decay_Hydroxylation-P': 6.376006247159679}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1354.792
[2,     1] loss: 1358.458
[3,     1] loss: 1351.471
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009821329159191742,
 'learning_rate_Hydroxylation-K': 0.0033750486269322035,
 'learning_rate_Hydroxylation-P': 0.009768997507921217,
 'log_base': 1.078295902496692,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 509743753,
 'sample_weights': [2.1053277261206773, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.423378961395588,
 'weight_decay_Hydroxylation-K': 8.392275338961863,
 'weight_decay_Hydroxylation-P': 1.3565365563238645}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7209.456
[2,     1] loss: 7190.667
[3,     1] loss: 7168.255
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008066728881015173,
 'learning_rate_Hydroxylation-K': 0.0010295849644747705,
 'learning_rate_Hydroxylation-P': 0.007536767220877068,
 'log_base': 1.348908754704667,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2556914998,
 'sample_weights': [22.146464217002396, 2.7684146734670025],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.61398435208642,
 'weight_decay_Hydroxylation-K': 9.484035684328028,
 'weight_decay_Hydroxylation-P': 2.836688357475068}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2088.091
[2,     1] loss: 2092.939
[3,     1] loss: 2075.471
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004702473507751626,
 'learning_rate_Hydroxylation-K': 0.0037293493803228018,
 'learning_rate_Hydroxylation-P': 0.009742285228553868,
 'log_base': 2.4758259569009944,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3229852543,
 'sample_weights': [5.577901161161313, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.489711914280652,
 'weight_decay_Hydroxylation-K': 7.155188593231043,
 'weight_decay_Hydroxylation-P': 7.7528387498022155}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1296.582
[2,     1] loss: 1300.330
[3,     1] loss: 1303.972
[4,     1] loss: 1296.515
[5,     1] loss: 1296.806
[6,     1] loss: 1295.048
[7,     1] loss: 1294.531
[8,     1] loss: 1291.026
[9,     1] loss: 1290.118
[10,     1] loss: 1279.905
[11,     1] loss: 1272.833
[12,     1] loss: 1246.226
[13,     1] loss: 1219.383
[14,     1] loss: 1197.376
[15,     1] loss: 1172.954
[16,     1] loss: 1159.725
[17,     1] loss: 1147.096
[18,     1] loss: 1104.705
[19,     1] loss: 1094.935
[20,     1] loss: 1134.008
[21,     1] loss: 1062.844
[22,     1] loss: 1042.684
[23,     1] loss: 1071.113
[24,     1] loss: 1041.143
[25,     1] loss: 1102.969
[26,     1] loss: 1010.219
[27,     1] loss: 1067.458
[28,     1] loss: 1035.799
[29,     1] loss: 1029.177
[30,     1] loss: 1027.678
[31,     1] loss: 1005.619
[32,     1] loss: 984.012
[33,     1] loss: 1003.457
[34,     1] loss: 949.868
[35,     1] loss: 912.459
[36,     1] loss: 968.677
[37,     1] loss: 888.316
[38,     1] loss: 979.913
[39,     1] loss: 887.831
[40,     1] loss: 893.712
[41,     1] loss: 868.599
[42,     1] loss: 855.875
[43,     1] loss: 820.323
[44,     1] loss: 836.297
[45,     1] loss: 834.136
[46,     1] loss: 791.928
[47,     1] loss: 808.051
[48,     1] loss: 973.345
[49,     1] loss: 836.698
[50,     1] loss: 799.581
[51,     1] loss: 842.415
[52,     1] loss: 743.897
[53,     1] loss: 906.127
[54,     1] loss: 756.230
[55,     1] loss: 852.174
[56,     1] loss: 749.397
[57,     1] loss: 768.946
[58,     1] loss: 713.866
[59,     1] loss: 689.861
[60,     1] loss: 693.045
[61,     1] loss: 763.360
[62,     1] loss: 707.220
[63,     1] loss: 656.799
[64,     1] loss: 832.777
[65,     1] loss: 845.612
[66,     1] loss: 642.211
[67,     1] loss: 785.374
[68,     1] loss: 668.967
[69,     1] loss: 700.919
[70,     1] loss: 617.773
[71,     1] loss: 588.058
[72,     1] loss: 574.892
[73,     1] loss: 615.291
[74,     1] loss: 604.880
[75,     1] loss: 590.368
[76,     1] loss: 648.288
[77,     1] loss: 607.829
[78,     1] loss: 541.380
[79,     1] loss: 521.933
[80,     1] loss: 587.567
[81,     1] loss: 578.103
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006631189950862429,
 'learning_rate_Hydroxylation-K': 0.007873790112146003,
 'learning_rate_Hydroxylation-P': 0.00021782512952468053,
 'log_base': 1.1449124823346088,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 113147020,
 'sample_weights': [1.8414856772490455, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.189891061700536,
 'weight_decay_Hydroxylation-K': 9.047117755355885,
 'weight_decay_Hydroxylation-P': 8.051018277074437}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4001.899
[2,     1] loss: 4002.868
[3,     1] loss: 4025.710
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001211266040593698,
 'learning_rate_Hydroxylation-K': 0.003297470047233397,
 'learning_rate_Hydroxylation-P': 0.007900557300706143,
 'log_base': 1.1691108564219062,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1714137979,
 'sample_weights': [12.336254781332444, 1.5420912529255555],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.888330313733759,
 'weight_decay_Hydroxylation-K': 5.825478072657392,
 'weight_decay_Hydroxylation-P': 3.741675721802185}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3475.286
[2,     1] loss: 3467.237
[3,     1] loss: 3470.362
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001714223724247058,
 'learning_rate_Hydroxylation-K': 0.005428463664523989,
 'learning_rate_Hydroxylation-P': 0.009243788985286272,
 'log_base': 1.2539147484529762,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2514172663,
 'sample_weights': [10.684880078992165, 1.3356614629349153],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.810319430691977,
 'weight_decay_Hydroxylation-K': 0.363185433499172,
 'weight_decay_Hydroxylation-P': 0.37491221231475824}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2468.295
[2,     1] loss: 2471.457
[3,     1] loss: 2464.407
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002727171776432891,
 'learning_rate_Hydroxylation-K': 0.002756914058307203,
 'learning_rate_Hydroxylation-P': 0.004391990539157987,
 'log_base': 2.418681239487802,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1863415121,
 'sample_weights': [7.378087159000788, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.155026104640733,
 'weight_decay_Hydroxylation-K': 5.2348418018663345,
 'weight_decay_Hydroxylation-P': 1.5792331082010862}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1307.815
[2,     1] loss: 1306.632
[3,     1] loss: 1303.734
[4,     1] loss: 1303.450
[5,     1] loss: 1301.057
[6,     1] loss: 1288.606
[7,     1] loss: 1285.595
[8,     1] loss: 1259.939
[9,     1] loss: 1236.767
[10,     1] loss: 1207.610
[11,     1] loss: 1177.275
[12,     1] loss: 1133.918
[13,     1] loss: 1145.242
[14,     1] loss: 1109.831
[15,     1] loss: 1117.130
[16,     1] loss: 1094.733
[17,     1] loss: 1067.674
[18,     1] loss: 1056.727
[19,     1] loss: 1058.220
[20,     1] loss: 1041.137
[21,     1] loss: 1044.560
[22,     1] loss: 1040.503
[23,     1] loss: 1016.567
[24,     1] loss: 1013.432
[25,     1] loss: 1028.325
[26,     1] loss: 986.263
[27,     1] loss: 932.508
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009245170002787514,
 'learning_rate_Hydroxylation-K': 0.0029944136856470717,
 'learning_rate_Hydroxylation-P': 0.006246499605935535,
 'log_base': 2.2968649637479186,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2087735251,
 'sample_weights': [1.8901729103439144, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.153777846127235,
 'weight_decay_Hydroxylation-K': 6.540678798998873,
 'weight_decay_Hydroxylation-P': 9.720950614162913}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1335.903
[2,     1] loss: 1341.181
[3,     1] loss: 1331.892
[4,     1] loss: 1335.149
[5,     1] loss: 1331.412
[6,     1] loss: 1327.826
[7,     1] loss: 1329.980
[8,     1] loss: 1336.290
[9,     1] loss: 1331.935
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023431507198176348,
 'learning_rate_Hydroxylation-K': 0.004004199560296924,
 'learning_rate_Hydroxylation-P': 0.009588529732355454,
 'log_base': 1.0592954930320053,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1851093906,
 'sample_weights': [2.0076398494744216, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.105449339512217,
 'weight_decay_Hydroxylation-K': 5.887141339925083,
 'weight_decay_Hydroxylation-P': 4.565946078868325}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9413.665
[2,     1] loss: 9410.589
[3,     1] loss: 9430.592
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00944826335789161,
 'learning_rate_Hydroxylation-K': 0.0012885612065878396,
 'learning_rate_Hydroxylation-P': 0.0037800517773801025,
 'log_base': 2.751872382934772,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1746371273,
 'sample_weights': [28.981346227695937, 3.6228078381913704],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.4324715244833826,
 'weight_decay_Hydroxylation-K': 4.599887753614512,
 'weight_decay_Hydroxylation-P': 7.083025820289096}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.875
[2,     1] loss: 1266.398
[3,     1] loss: 1254.393
[4,     1] loss: 1255.861
[5,     1] loss: 1254.307
[6,     1] loss: 1256.452
[7,     1] loss: 1255.450
[8,     1] loss: 1256.383
[9,     1] loss: 1254.685
[10,     1] loss: 1253.783
[11,     1] loss: 1254.265
[12,     1] loss: 1253.044
[13,     1] loss: 1250.573
[14,     1] loss: 1247.309
[15,     1] loss: 1237.457
[16,     1] loss: 1218.373
[17,     1] loss: 1193.006
[18,     1] loss: 1170.356
[19,     1] loss: 1146.541
[20,     1] loss: 1114.386
[21,     1] loss: 1107.260
[22,     1] loss: 1070.535
[23,     1] loss: 1036.000
[24,     1] loss: 1127.556
[25,     1] loss: 1219.033
[26,     1] loss: 1116.987
[27,     1] loss: 1044.591
[28,     1] loss: 1121.448
[29,     1] loss: 1088.940
[30,     1] loss: 1052.615
[31,     1] loss: 1060.769
[32,     1] loss: 1069.465
[33,     1] loss: 1023.720
[34,     1] loss: 999.890
[35,     1] loss: 996.551
[36,     1] loss: 963.423
[37,     1] loss: 1010.697
[38,     1] loss: 932.665
[39,     1] loss: 1021.001
[40,     1] loss: 944.319
[41,     1] loss: 994.818
[42,     1] loss: 958.081
[43,     1] loss: 959.448
[44,     1] loss: 897.214
[45,     1] loss: 929.765
[46,     1] loss: 887.390
[47,     1] loss: 862.328
[48,     1] loss: 1006.884
[49,     1] loss: 1003.292
[50,     1] loss: 864.830
[51,     1] loss: 916.261
[52,     1] loss: 875.046
[53,     1] loss: 858.391
[54,     1] loss: 859.181
[55,     1] loss: 865.622
[56,     1] loss: 842.382
[57,     1] loss: 949.078
[58,     1] loss: 1258.558
[59,     1] loss: 915.599
[60,     1] loss: 1032.849
[61,     1] loss: 961.724
[62,     1] loss: 958.197
[63,     1] loss: 974.755
[64,     1] loss: 918.303
[65,     1] loss: 938.043
[66,     1] loss: 909.180
[67,     1] loss: 937.899
[68,     1] loss: 895.677
[69,     1] loss: 908.776
[70,     1] loss: 883.287
[71,     1] loss: 866.344
[72,     1] loss: 871.498
[73,     1] loss: 801.719
[74,     1] loss: 832.154
[75,     1] loss: 931.044
[76,     1] loss: 1378.180
[77,     1] loss: 956.257
[78,     1] loss: 1036.771
[79,     1] loss: 1045.091
[80,     1] loss: 1045.698
[81,     1] loss: 1021.719
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005263399667216971,
 'learning_rate_Hydroxylation-K': 0.0016471807456116933,
 'learning_rate_Hydroxylation-P': 0.006512892899246046,
 'log_base': 1.1401885256809432,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3833326453,
 'sample_weights': [1.6491885613936519, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.500557154651943,
 'weight_decay_Hydroxylation-K': 1.4034322726386166,
 'weight_decay_Hydroxylation-P': 1.0336077744122687}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4195.597
[2,     1] loss: 4125.773
[3,     1] loss: 4121.401
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027700270940163997,
 'learning_rate_Hydroxylation-K': 0.0027067494758125357,
 'learning_rate_Hydroxylation-P': 0.009968750327369416,
 'log_base': 1.1302586465222946,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 458966106,
 'sample_weights': [12.725032819558681, 1.5906903798652376],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.746733680479905,
 'weight_decay_Hydroxylation-K': 0.37700238035304734,
 'weight_decay_Hydroxylation-P': 1.4643476487989702}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4468.494
[2,     1] loss: 4417.553
[3,     1] loss: 4441.010
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006058857930365945,
 'learning_rate_Hydroxylation-K': 0.0012528297210104167,
 'learning_rate_Hydroxylation-P': 0.009335293798358112,
 'log_base': 1.3183188913911617,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1231244948,
 'sample_weights': [13.634062098751414, 1.7043234171965993],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.79971701391813,
 'weight_decay_Hydroxylation-K': 9.09791943826593,
 'weight_decay_Hydroxylation-P': 0.6802667266064042}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2185.106
[2,     1] loss: 2186.441
[3,     1] loss: 2182.519
[4,     1] loss: 2193.602
[5,     1] loss: 2183.540
[6,     1] loss: 2183.116
[7,     1] loss: 2179.787
[8,     1] loss: 2181.155
[9,     1] loss: 2180.165
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015041851196679723,
 'learning_rate_Hydroxylation-K': 0.005911248175096986,
 'learning_rate_Hydroxylation-P': 0.003756799412200958,
 'log_base': 1.3828017820300358,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3819830294,
 'sample_weights': [6.040885470705885, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.179894769162424,
 'weight_decay_Hydroxylation-K': 1.630661983130634,
 'weight_decay_Hydroxylation-P': 3.5099919258481975}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1995.579
[2,     1] loss: 1999.667
[3,     1] loss: 1986.901
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008054031458013238,
 'learning_rate_Hydroxylation-K': 0.009247033408803864,
 'learning_rate_Hydroxylation-P': 0.002226581977772864,
 'log_base': 2.7308260203933408,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 756269716,
 'sample_weights': [5.150826261018709, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.439045888171767,
 'weight_decay_Hydroxylation-K': 6.168397995959021,
 'weight_decay_Hydroxylation-P': 9.555917766463615}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.126
[2,     1] loss: 1262.562
[3,     1] loss: 1269.542
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005806109423642047,
 'learning_rate_Hydroxylation-K': 0.0008194209757452505,
 'learning_rate_Hydroxylation-P': 0.0008166096376801897,
 'log_base': 2.0618313756483864,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 155223457,
 'sample_weights': [1.661792032557251, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.655161691594371,
 'weight_decay_Hydroxylation-K': 9.744969251403596,
 'weight_decay_Hydroxylation-P': 0.33493529583497167}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.856
[2,     1] loss: 1398.377
[3,     1] loss: 1394.858
[4,     1] loss: 1391.465
[5,     1] loss: 1389.565
[6,     1] loss: 1398.180
[7,     1] loss: 1395.750
[8,     1] loss: 1387.081
[9,     1] loss: 1389.047
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019204132406290362,
 'learning_rate_Hydroxylation-K': 0.003692870420083451,
 'learning_rate_Hydroxylation-P': 0.0099090849921149,
 'log_base': 2.9424639773673316,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 498359465,
 'sample_weights': [2.3071525623711313, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.221702519841007,
 'weight_decay_Hydroxylation-K': 6.1369258727598766,
 'weight_decay_Hydroxylation-P': 4.846651023448343}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.009
[2,     1] loss: 1234.998
[3,     1] loss: 1234.005
[4,     1] loss: 1234.173
[5,     1] loss: 1231.753
[6,     1] loss: 1226.262
[7,     1] loss: 1220.875
[8,     1] loss: 1214.767
[9,     1] loss: 1196.475
[10,     1] loss: 1176.318
[11,     1] loss: 1159.047
[12,     1] loss: 1120.528
[13,     1] loss: 1095.307
[14,     1] loss: 1069.540
[15,     1] loss: 1045.915
[16,     1] loss: 1064.102
[17,     1] loss: 1069.807
[18,     1] loss: 1023.202
[19,     1] loss: 1012.829
[20,     1] loss: 995.732
[21,     1] loss: 1013.902
[22,     1] loss: 1029.761
[23,     1] loss: 990.205
[24,     1] loss: 999.712
[25,     1] loss: 976.643
[26,     1] loss: 987.954
[27,     1] loss: 985.769
[28,     1] loss: 950.210
[29,     1] loss: 951.552
[30,     1] loss: 960.742
[31,     1] loss: 920.464
[32,     1] loss: 935.903
[33,     1] loss: 951.733
[34,     1] loss: 954.223
[35,     1] loss: 912.824
[36,     1] loss: 937.211
[37,     1] loss: 913.565
[38,     1] loss: 882.606
[39,     1] loss: 889.276
[40,     1] loss: 876.893
[41,     1] loss: 865.121
[42,     1] loss: 850.114
[43,     1] loss: 885.456
[44,     1] loss: 845.457
[45,     1] loss: 838.064
[46,     1] loss: 829.913
[47,     1] loss: 818.522
[48,     1] loss: 828.448
[49,     1] loss: 818.319
[50,     1] loss: 834.015
[51,     1] loss: 765.616
[52,     1] loss: 807.069
[53,     1] loss: 734.060
[54,     1] loss: 780.077
[55,     1] loss: 787.671
[56,     1] loss: 762.081
[57,     1] loss: 751.287
[58,     1] loss: 742.689
[59,     1] loss: 703.303
[60,     1] loss: 708.248
[61,     1] loss: 748.219
[62,     1] loss: 701.126
[63,     1] loss: 698.893
[64,     1] loss: 661.162
[65,     1] loss: 630.740
[66,     1] loss: 679.580
[67,     1] loss: 676.331
[68,     1] loss: 626.267
[69,     1] loss: 649.894
[70,     1] loss: 652.408
[71,     1] loss: 696.438
[72,     1] loss: 609.525
[73,     1] loss: 717.230
[74,     1] loss: 692.008
[75,     1] loss: 662.266
[76,     1] loss: 701.222
[77,     1] loss: 629.330
[78,     1] loss: 743.154
[79,     1] loss: 618.222
[80,     1] loss: 682.701
[81,     1] loss: 582.958
[82,     1] loss: 591.897
[83,     1] loss: 596.323
[84,     1] loss: 559.862
[85,     1] loss: 605.514
[86,     1] loss: 582.920
[87,     1] loss: 600.063
[88,     1] loss: 499.456
[89,     1] loss: 573.312
[90,     1] loss: 509.008
[91,     1] loss: 551.037
[92,     1] loss: 494.049
[93,     1] loss: 530.194
[94,     1] loss: 507.461
[95,     1] loss: 560.519
[96,     1] loss: 450.578
[97,     1] loss: 521.633
[98,     1] loss: 484.596
[99,     1] loss: 538.207
[100,     1] loss: 481.702
Early stopping applied (best metric=0.3324543833732605)
Finished Training
Total time taken: 14.69653606414795
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.461
[2,     1] loss: 1234.800
[3,     1] loss: 1232.552
[4,     1] loss: 1234.096
[5,     1] loss: 1231.081
[6,     1] loss: 1230.665
[7,     1] loss: 1226.833
[8,     1] loss: 1224.078
[9,     1] loss: 1217.250
[10,     1] loss: 1195.789
[11,     1] loss: 1186.051
[12,     1] loss: 1165.717
[13,     1] loss: 1137.818
[14,     1] loss: 1135.970
[15,     1] loss: 1097.512
[16,     1] loss: 1078.756
[17,     1] loss: 1083.953
[18,     1] loss: 1050.335
[19,     1] loss: 1064.258
[20,     1] loss: 1001.632
[21,     1] loss: 1047.771
[22,     1] loss: 984.297
[23,     1] loss: 1027.135
[24,     1] loss: 966.172
[25,     1] loss: 1012.674
[26,     1] loss: 997.575
[27,     1] loss: 977.526
[28,     1] loss: 985.221
[29,     1] loss: 943.970
[30,     1] loss: 944.577
[31,     1] loss: 968.017
[32,     1] loss: 949.502
[33,     1] loss: 964.376
[34,     1] loss: 925.640
[35,     1] loss: 876.206
[36,     1] loss: 892.224
[37,     1] loss: 950.816
[38,     1] loss: 894.364
[39,     1] loss: 906.400
[40,     1] loss: 874.377
[41,     1] loss: 824.301
[42,     1] loss: 905.994
[43,     1] loss: 869.460
[44,     1] loss: 847.224
[45,     1] loss: 834.021
[46,     1] loss: 832.083
[47,     1] loss: 851.673
[48,     1] loss: 823.105
[49,     1] loss: 865.853
[50,     1] loss: 815.112
[51,     1] loss: 790.579
[52,     1] loss: 761.387
[53,     1] loss: 818.845
[54,     1] loss: 799.588
[55,     1] loss: 786.930
[56,     1] loss: 737.047
[57,     1] loss: 765.418
[58,     1] loss: 729.734
[59,     1] loss: 713.858
[60,     1] loss: 705.082
[61,     1] loss: 687.153
[62,     1] loss: 701.355
[63,     1] loss: 700.788
[64,     1] loss: 743.252
[65,     1] loss: 652.705
[66,     1] loss: 741.579
[67,     1] loss: 806.973
[68,     1] loss: 669.934
[69,     1] loss: 852.171
[70,     1] loss: 658.190
[71,     1] loss: 762.788
[72,     1] loss: 704.035
[73,     1] loss: 703.064
[74,     1] loss: 776.652
[75,     1] loss: 634.694
[76,     1] loss: 686.403
[77,     1] loss: 667.539
[78,     1] loss: 622.885
[79,     1] loss: 689.834
[80,     1] loss: 628.382
[81,     1] loss: 591.087
[82,     1] loss: 576.317
[83,     1] loss: 601.872
[84,     1] loss: 594.420
[85,     1] loss: 551.523
[86,     1] loss: 608.788
[87,     1] loss: 552.416
[88,     1] loss: 574.056
[89,     1] loss: 550.712
[90,     1] loss: 518.056
[91,     1] loss: 530.892
[92,     1] loss: 514.703
[93,     1] loss: 584.925
[94,     1] loss: 536.781
[95,     1] loss: 515.347
[96,     1] loss: 552.225
[97,     1] loss: 504.808
[98,     1] loss: 532.420
[99,     1] loss: 573.571
[100,     1] loss: 492.287
[101,     1] loss: 551.144
[102,     1] loss: 461.653
[103,     1] loss: 607.618
Early stopping applied (best metric=0.39607083797454834)
Finished Training
Total time taken: 15.552080154418945
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.755
[2,     1] loss: 1233.589
[3,     1] loss: 1232.344
[4,     1] loss: 1235.758
[5,     1] loss: 1231.423
[6,     1] loss: 1225.396
[7,     1] loss: 1224.593
[8,     1] loss: 1217.542
[9,     1] loss: 1212.708
[10,     1] loss: 1198.146
[11,     1] loss: 1175.969
[12,     1] loss: 1148.545
[13,     1] loss: 1128.990
[14,     1] loss: 1109.200
[15,     1] loss: 1084.777
[16,     1] loss: 1055.919
[17,     1] loss: 1039.705
[18,     1] loss: 1015.626
[19,     1] loss: 1024.525
[20,     1] loss: 976.391
[21,     1] loss: 1034.965
[22,     1] loss: 990.447
[23,     1] loss: 994.706
[24,     1] loss: 968.272
[25,     1] loss: 958.463
[26,     1] loss: 1003.663
[27,     1] loss: 975.382
[28,     1] loss: 943.299
[29,     1] loss: 920.729
[30,     1] loss: 941.055
[31,     1] loss: 922.625
[32,     1] loss: 903.852
[33,     1] loss: 929.911
[34,     1] loss: 886.573
[35,     1] loss: 895.493
[36,     1] loss: 907.842
[37,     1] loss: 873.242
[38,     1] loss: 848.987
[39,     1] loss: 872.139
[40,     1] loss: 836.399
[41,     1] loss: 819.424
[42,     1] loss: 842.480
[43,     1] loss: 834.970
[44,     1] loss: 813.818
[45,     1] loss: 768.326
[46,     1] loss: 804.586
[47,     1] loss: 807.960
[48,     1] loss: 794.622
[49,     1] loss: 792.571
[50,     1] loss: 792.331
[51,     1] loss: 800.907
[52,     1] loss: 769.919
[53,     1] loss: 749.798
[54,     1] loss: 787.377
[55,     1] loss: 742.815
[56,     1] loss: 708.190
[57,     1] loss: 769.661
[58,     1] loss: 724.131
[59,     1] loss: 678.668
[60,     1] loss: 685.272
[61,     1] loss: 671.816
[62,     1] loss: 709.166
[63,     1] loss: 683.950
[64,     1] loss: 641.775
[65,     1] loss: 681.813
[66,     1] loss: 683.174
[67,     1] loss: 709.329
[68,     1] loss: 619.591
[69,     1] loss: 642.948
[70,     1] loss: 608.637
[71,     1] loss: 636.871
[72,     1] loss: 591.674
[73,     1] loss: 572.470
[74,     1] loss: 643.964
[75,     1] loss: 599.455
[76,     1] loss: 605.049
[77,     1] loss: 553.098
[78,     1] loss: 608.053
[79,     1] loss: 597.445
[80,     1] loss: 564.089
[81,     1] loss: 579.754
[82,     1] loss: 507.784
[83,     1] loss: 623.921
[84,     1] loss: 534.943
[85,     1] loss: 662.722
[86,     1] loss: 481.032
[87,     1] loss: 609.748
[88,     1] loss: 511.720
[89,     1] loss: 554.271
[90,     1] loss: 499.206
[91,     1] loss: 487.625
[92,     1] loss: 480.820
Early stopping applied (best metric=0.364174485206604)
Finished Training
Total time taken: 14.057563304901123
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.744
[2,     1] loss: 1233.486
[3,     1] loss: 1235.508
[4,     1] loss: 1233.356
[5,     1] loss: 1235.050
[6,     1] loss: 1227.466
[7,     1] loss: 1226.801
[8,     1] loss: 1225.344
[9,     1] loss: 1212.013
[10,     1] loss: 1200.487
[11,     1] loss: 1181.602
[12,     1] loss: 1162.998
[13,     1] loss: 1149.988
[14,     1] loss: 1113.979
[15,     1] loss: 1089.184
[16,     1] loss: 1088.817
[17,     1] loss: 1045.746
[18,     1] loss: 991.210
[19,     1] loss: 1023.723
[20,     1] loss: 997.462
[21,     1] loss: 990.015
[22,     1] loss: 1010.048
[23,     1] loss: 986.312
[24,     1] loss: 988.654
[25,     1] loss: 1064.965
[26,     1] loss: 996.996
[27,     1] loss: 995.243
[28,     1] loss: 980.373
[29,     1] loss: 993.553
[30,     1] loss: 977.666
[31,     1] loss: 960.759
[32,     1] loss: 939.611
[33,     1] loss: 930.844
[34,     1] loss: 945.818
[35,     1] loss: 918.494
[36,     1] loss: 880.271
[37,     1] loss: 965.138
[38,     1] loss: 928.821
[39,     1] loss: 906.714
[40,     1] loss: 850.066
[41,     1] loss: 903.629
[42,     1] loss: 889.393
[43,     1] loss: 858.579
[44,     1] loss: 927.816
[45,     1] loss: 907.439
[46,     1] loss: 853.847
[47,     1] loss: 860.973
[48,     1] loss: 848.473
[49,     1] loss: 853.948
[50,     1] loss: 824.996
[51,     1] loss: 805.403
[52,     1] loss: 829.873
[53,     1] loss: 787.120
[54,     1] loss: 814.324
[55,     1] loss: 768.297
[56,     1] loss: 773.434
[57,     1] loss: 738.449
[58,     1] loss: 774.707
[59,     1] loss: 727.439
[60,     1] loss: 703.251
[61,     1] loss: 725.803
[62,     1] loss: 698.619
[63,     1] loss: 791.531
[64,     1] loss: 711.241
[65,     1] loss: 716.649
[66,     1] loss: 741.097
[67,     1] loss: 662.663
[68,     1] loss: 637.916
[69,     1] loss: 660.724
[70,     1] loss: 678.963
[71,     1] loss: 663.798
[72,     1] loss: 662.033
[73,     1] loss: 658.110
[74,     1] loss: 688.264
[75,     1] loss: 684.064
[76,     1] loss: 614.706
[77,     1] loss: 614.952
[78,     1] loss: 595.655
[79,     1] loss: 606.049
[80,     1] loss: 624.821
[81,     1] loss: 600.854
[82,     1] loss: 555.259
[83,     1] loss: 590.542
[84,     1] loss: 534.732
[85,     1] loss: 534.754
[86,     1] loss: 522.373
[87,     1] loss: 506.863
[88,     1] loss: 461.583
[89,     1] loss: 533.741
[90,     1] loss: 575.218
[91,     1] loss: 498.644
[92,     1] loss: 555.558
[93,     1] loss: 525.526
[94,     1] loss: 569.375
[95,     1] loss: 585.397
[96,     1] loss: 565.135
Early stopping applied (best metric=0.39152488112449646)
Finished Training
Total time taken: 14.380017280578613
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1237.434
[2,     1] loss: 1235.780
[3,     1] loss: 1233.807
[4,     1] loss: 1235.765
[5,     1] loss: 1233.465
[6,     1] loss: 1231.645
[7,     1] loss: 1231.959
[8,     1] loss: 1226.812
[9,     1] loss: 1218.311
[10,     1] loss: 1208.678
[11,     1] loss: 1195.013
[12,     1] loss: 1176.879
[13,     1] loss: 1147.393
[14,     1] loss: 1122.576
[15,     1] loss: 1104.420
[16,     1] loss: 1086.026
[17,     1] loss: 1045.058
[18,     1] loss: 1015.738
[19,     1] loss: 1014.802
[20,     1] loss: 1013.738
[21,     1] loss: 1058.451
[22,     1] loss: 1020.973
[23,     1] loss: 1028.222
[24,     1] loss: 972.513
[25,     1] loss: 1016.918
[26,     1] loss: 953.913
[27,     1] loss: 979.302
[28,     1] loss: 937.293
[29,     1] loss: 976.738
[30,     1] loss: 941.563
[31,     1] loss: 930.899
[32,     1] loss: 957.615
[33,     1] loss: 960.140
[34,     1] loss: 939.031
[35,     1] loss: 950.172
[36,     1] loss: 868.247
[37,     1] loss: 900.513
[38,     1] loss: 933.410
[39,     1] loss: 927.973
[40,     1] loss: 902.047
[41,     1] loss: 915.063
[42,     1] loss: 919.068
[43,     1] loss: 889.731
[44,     1] loss: 889.627
[45,     1] loss: 900.811
[46,     1] loss: 860.181
[47,     1] loss: 828.126
[48,     1] loss: 837.666
[49,     1] loss: 820.933
[50,     1] loss: 843.520
[51,     1] loss: 757.917
[52,     1] loss: 762.238
[53,     1] loss: 801.575
[54,     1] loss: 825.808
[55,     1] loss: 801.804
[56,     1] loss: 752.185
[57,     1] loss: 769.592
[58,     1] loss: 755.947
[59,     1] loss: 707.804
[60,     1] loss: 706.012
[61,     1] loss: 739.327
[62,     1] loss: 736.328
[63,     1] loss: 718.660
[64,     1] loss: 732.301
[65,     1] loss: 760.982
[66,     1] loss: 805.109
[67,     1] loss: 710.950
[68,     1] loss: 756.606
[69,     1] loss: 695.937
[70,     1] loss: 744.123
[71,     1] loss: 702.659
[72,     1] loss: 712.221
[73,     1] loss: 660.236
[74,     1] loss: 737.319
[75,     1] loss: 637.291
[76,     1] loss: 667.391
[77,     1] loss: 672.098
[78,     1] loss: 605.700
[79,     1] loss: 606.157
[80,     1] loss: 618.215
[81,     1] loss: 631.290
[82,     1] loss: 610.066
[83,     1] loss: 557.833
[84,     1] loss: 604.509
[85,     1] loss: 537.145
[86,     1] loss: 535.021
[87,     1] loss: 623.223
[88,     1] loss: 572.431
[89,     1] loss: 568.421
[90,     1] loss: 586.709
[91,     1] loss: 558.019
[92,     1] loss: 535.903
[93,     1] loss: 523.848
[94,     1] loss: 545.409
[95,     1] loss: 482.486
[96,     1] loss: 553.366
[97,     1] loss: 474.667
[98,     1] loss: 485.093
[99,     1] loss: 554.404
[100,     1] loss: 515.165
[101,     1] loss: 459.308
[102,     1] loss: 499.523
[103,     1] loss: 433.100
[104,     1] loss: 506.798
[105,     1] loss: 481.934
[106,     1] loss: 411.244
[107,     1] loss: 461.786
[108,     1] loss: 412.787
[109,     1] loss: 419.290
[110,     1] loss: 432.175
[111,     1] loss: 459.836
[112,     1] loss: 472.985
[113,     1] loss: 432.269
[114,     1] loss: 436.468
[115,     1] loss: 418.738
[116,     1] loss: 462.539
[117,     1] loss: 465.567
[118,     1] loss: 418.242
[119,     1] loss: 426.385
Early stopping applied (best metric=0.363632470369339)
Finished Training
Total time taken: 18.0270733833313
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.815
[2,     1] loss: 1231.857
[3,     1] loss: 1233.993
[4,     1] loss: 1234.761
[5,     1] loss: 1232.322
[6,     1] loss: 1228.544
[7,     1] loss: 1227.265
[8,     1] loss: 1224.281
[9,     1] loss: 1218.344
[10,     1] loss: 1202.726
[11,     1] loss: 1185.096
[12,     1] loss: 1156.585
[13,     1] loss: 1109.440
[14,     1] loss: 1099.704
[15,     1] loss: 1074.915
[16,     1] loss: 1056.277
[17,     1] loss: 1026.798
[18,     1] loss: 999.236
[19,     1] loss: 1066.956
[20,     1] loss: 1024.142
[21,     1] loss: 1035.742
[22,     1] loss: 1011.863
[23,     1] loss: 1019.522
[24,     1] loss: 1004.182
[25,     1] loss: 983.678
[26,     1] loss: 1009.269
[27,     1] loss: 958.806
[28,     1] loss: 994.446
[29,     1] loss: 956.142
[30,     1] loss: 981.207
[31,     1] loss: 1003.290
[32,     1] loss: 940.407
[33,     1] loss: 955.062
[34,     1] loss: 956.715
[35,     1] loss: 939.490
[36,     1] loss: 928.501
[37,     1] loss: 904.019
[38,     1] loss: 902.747
[39,     1] loss: 884.377
[40,     1] loss: 902.893
[41,     1] loss: 899.646
[42,     1] loss: 853.481
[43,     1] loss: 847.755
[44,     1] loss: 888.319
[45,     1] loss: 847.405
[46,     1] loss: 888.449
[47,     1] loss: 820.725
[48,     1] loss: 841.118
[49,     1] loss: 824.396
[50,     1] loss: 846.374
[51,     1] loss: 842.736
[52,     1] loss: 825.863
[53,     1] loss: 803.019
[54,     1] loss: 791.297
[55,     1] loss: 774.013
[56,     1] loss: 751.987
[57,     1] loss: 789.249
[58,     1] loss: 746.499
[59,     1] loss: 750.779
[60,     1] loss: 758.836
[61,     1] loss: 811.345
[62,     1] loss: 729.225
[63,     1] loss: 734.372
[64,     1] loss: 664.514
[65,     1] loss: 687.008
[66,     1] loss: 664.829
[67,     1] loss: 713.173
[68,     1] loss: 704.974
[69,     1] loss: 681.661
[70,     1] loss: 635.102
[71,     1] loss: 646.625
[72,     1] loss: 657.306
[73,     1] loss: 603.199
[74,     1] loss: 663.898
[75,     1] loss: 671.810
[76,     1] loss: 630.053
[77,     1] loss: 629.006
[78,     1] loss: 616.660
[79,     1] loss: 596.116
[80,     1] loss: 591.645
[81,     1] loss: 583.008
[82,     1] loss: 590.955
[83,     1] loss: 586.956
[84,     1] loss: 538.681
[85,     1] loss: 571.662
[86,     1] loss: 512.204
[87,     1] loss: 566.974
[88,     1] loss: 496.613
[89,     1] loss: 551.961
Early stopping applied (best metric=0.3935394287109375)
Finished Training
Total time taken: 13.256019592285156
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.731
[2,     1] loss: 1232.986
[3,     1] loss: 1232.335
[4,     1] loss: 1233.316
[5,     1] loss: 1227.775
[6,     1] loss: 1223.063
[7,     1] loss: 1216.332
[8,     1] loss: 1207.840
[9,     1] loss: 1182.170
[10,     1] loss: 1157.869
[11,     1] loss: 1137.737
[12,     1] loss: 1097.683
[13,     1] loss: 1088.311
[14,     1] loss: 1085.775
[15,     1] loss: 1055.767
[16,     1] loss: 1010.160
[17,     1] loss: 1017.100
[18,     1] loss: 1036.707
[19,     1] loss: 976.516
[20,     1] loss: 997.949
[21,     1] loss: 981.017
[22,     1] loss: 991.878
[23,     1] loss: 982.990
[24,     1] loss: 950.586
[25,     1] loss: 958.860
[26,     1] loss: 1006.840
[27,     1] loss: 963.052
[28,     1] loss: 982.108
[29,     1] loss: 938.740
[30,     1] loss: 928.384
[31,     1] loss: 916.029
[32,     1] loss: 931.507
[33,     1] loss: 910.790
[34,     1] loss: 923.524
[35,     1] loss: 901.773
[36,     1] loss: 928.842
[37,     1] loss: 913.004
[38,     1] loss: 918.069
[39,     1] loss: 889.577
[40,     1] loss: 914.011
[41,     1] loss: 887.283
[42,     1] loss: 847.750
[43,     1] loss: 857.676
[44,     1] loss: 866.826
[45,     1] loss: 902.082
[46,     1] loss: 860.534
[47,     1] loss: 860.384
[48,     1] loss: 826.453
[49,     1] loss: 794.246
[50,     1] loss: 772.201
[51,     1] loss: 842.705
[52,     1] loss: 780.271
[53,     1] loss: 762.548
[54,     1] loss: 777.839
[55,     1] loss: 744.584
[56,     1] loss: 744.636
[57,     1] loss: 708.755
[58,     1] loss: 757.534
[59,     1] loss: 756.233
[60,     1] loss: 697.497
[61,     1] loss: 670.308
[62,     1] loss: 669.883
[63,     1] loss: 711.196
[64,     1] loss: 684.841
[65,     1] loss: 662.058
[66,     1] loss: 717.357
[67,     1] loss: 663.640
[68,     1] loss: 637.296
[69,     1] loss: 625.615
[70,     1] loss: 652.135
[71,     1] loss: 696.791
[72,     1] loss: 682.329
[73,     1] loss: 617.112
[74,     1] loss: 598.956
[75,     1] loss: 550.531
[76,     1] loss: 562.365
[77,     1] loss: 557.850
[78,     1] loss: 570.376
[79,     1] loss: 563.706
[80,     1] loss: 600.860
[81,     1] loss: 622.630
[82,     1] loss: 549.091
[83,     1] loss: 512.159
[84,     1] loss: 511.326
[85,     1] loss: 464.324
[86,     1] loss: 574.064
[87,     1] loss: 571.959
[88,     1] loss: 482.050
[89,     1] loss: 565.961
[90,     1] loss: 559.102
[91,     1] loss: 503.739
[92,     1] loss: 602.135
[93,     1] loss: 535.274
[94,     1] loss: 516.915
[95,     1] loss: 606.404
[96,     1] loss: 518.639
[97,     1] loss: 522.963
[98,     1] loss: 503.125
[99,     1] loss: 535.594
[100,     1] loss: 466.496
[101,     1] loss: 476.778
[102,     1] loss: 468.566
[103,     1] loss: 505.150
[104,     1] loss: 453.733
[105,     1] loss: 490.732
[106,     1] loss: 394.826
[107,     1] loss: 492.255
[108,     1] loss: 396.525
[109,     1] loss: 512.305
[110,     1] loss: 401.751
[111,     1] loss: 476.078
[112,     1] loss: 404.143
[113,     1] loss: 452.582
[114,     1] loss: 382.702
[115,     1] loss: 386.518
[116,     1] loss: 392.931
[117,     1] loss: 390.862
[118,     1] loss: 436.242
[119,     1] loss: 393.488
[120,     1] loss: 384.365
[121,     1] loss: 380.364
[122,     1] loss: 362.942
[123,     1] loss: 331.689
[124,     1] loss: 391.334
Early stopping applied (best metric=0.3564505875110626)
Finished Training
Total time taken: 18.82402777671814
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.600
[2,     1] loss: 1235.257
[3,     1] loss: 1234.682
[4,     1] loss: 1234.798
[5,     1] loss: 1233.478
[6,     1] loss: 1233.849
[7,     1] loss: 1236.528
[8,     1] loss: 1229.484
[9,     1] loss: 1226.137
[10,     1] loss: 1224.572
[11,     1] loss: 1222.336
[12,     1] loss: 1217.110
[13,     1] loss: 1200.024
[14,     1] loss: 1183.178
[15,     1] loss: 1163.236
[16,     1] loss: 1133.189
[17,     1] loss: 1096.139
[18,     1] loss: 1066.723
[19,     1] loss: 1067.297
[20,     1] loss: 1056.730
[21,     1] loss: 1021.581
[22,     1] loss: 1006.727
[23,     1] loss: 1010.902
[24,     1] loss: 959.284
[25,     1] loss: 988.601
[26,     1] loss: 1015.612
[27,     1] loss: 1021.193
[28,     1] loss: 976.882
[29,     1] loss: 900.041
[30,     1] loss: 1020.834
[31,     1] loss: 953.479
[32,     1] loss: 934.019
[33,     1] loss: 994.046
[34,     1] loss: 968.134
[35,     1] loss: 907.971
[36,     1] loss: 908.255
[37,     1] loss: 945.974
[38,     1] loss: 923.514
[39,     1] loss: 946.849
[40,     1] loss: 889.057
[41,     1] loss: 921.957
[42,     1] loss: 873.240
[43,     1] loss: 865.917
[44,     1] loss: 883.742
[45,     1] loss: 890.076
[46,     1] loss: 869.953
[47,     1] loss: 834.369
[48,     1] loss: 849.385
[49,     1] loss: 833.739
[50,     1] loss: 858.521
[51,     1] loss: 822.265
[52,     1] loss: 816.427
[53,     1] loss: 788.894
[54,     1] loss: 797.208
[55,     1] loss: 819.907
[56,     1] loss: 797.875
[57,     1] loss: 775.466
[58,     1] loss: 739.962
[59,     1] loss: 752.635
[60,     1] loss: 700.888
[61,     1] loss: 719.694
[62,     1] loss: 723.011
[63,     1] loss: 700.989
[64,     1] loss: 755.922
[65,     1] loss: 776.797
[66,     1] loss: 654.091
[67,     1] loss: 628.892
[68,     1] loss: 628.946
[69,     1] loss: 618.143
[70,     1] loss: 646.966
[71,     1] loss: 643.942
[72,     1] loss: 709.064
[73,     1] loss: 748.896
[74,     1] loss: 675.451
[75,     1] loss: 650.681
[76,     1] loss: 592.036
[77,     1] loss: 644.513
[78,     1] loss: 588.013
[79,     1] loss: 603.132
[80,     1] loss: 526.628
[81,     1] loss: 592.727
[82,     1] loss: 540.228
[83,     1] loss: 580.221
[84,     1] loss: 565.728
[85,     1] loss: 461.260
[86,     1] loss: 528.122
[87,     1] loss: 518.768
[88,     1] loss: 553.395
[89,     1] loss: 524.326
[90,     1] loss: 489.780
[91,     1] loss: 471.019
[92,     1] loss: 508.080
[93,     1] loss: 470.249
[94,     1] loss: 504.481
[95,     1] loss: 485.838
[96,     1] loss: 519.071
[97,     1] loss: 529.818
[98,     1] loss: 435.771
[99,     1] loss: 508.347
Early stopping applied (best metric=0.36533018946647644)
Finished Training
Total time taken: 14.184017419815063
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.177
[2,     1] loss: 1234.952
[3,     1] loss: 1236.709
[4,     1] loss: 1232.235
[5,     1] loss: 1235.382
[6,     1] loss: 1231.251
[7,     1] loss: 1232.284
[8,     1] loss: 1231.199
[9,     1] loss: 1229.935
[10,     1] loss: 1227.520
[11,     1] loss: 1222.139
[12,     1] loss: 1216.037
[13,     1] loss: 1204.800
[14,     1] loss: 1180.403
[15,     1] loss: 1180.843
[16,     1] loss: 1156.854
[17,     1] loss: 1118.871
[18,     1] loss: 1133.115
[19,     1] loss: 1103.130
[20,     1] loss: 1091.681
[21,     1] loss: 1063.970
[22,     1] loss: 1070.026
[23,     1] loss: 1028.511
[24,     1] loss: 1085.684
[25,     1] loss: 1036.035
[26,     1] loss: 1039.451
[27,     1] loss: 1028.512
[28,     1] loss: 1039.667
[29,     1] loss: 1018.246
[30,     1] loss: 980.019
[31,     1] loss: 980.681
[32,     1] loss: 1031.137
[33,     1] loss: 992.455
[34,     1] loss: 974.890
[35,     1] loss: 954.377
[36,     1] loss: 946.041
[37,     1] loss: 946.381
[38,     1] loss: 927.774
[39,     1] loss: 952.597
[40,     1] loss: 925.166
[41,     1] loss: 898.196
[42,     1] loss: 906.359
[43,     1] loss: 936.378
[44,     1] loss: 917.878
[45,     1] loss: 875.824
[46,     1] loss: 874.166
[47,     1] loss: 888.756
[48,     1] loss: 901.338
[49,     1] loss: 873.851
[50,     1] loss: 814.937
[51,     1] loss: 836.842
[52,     1] loss: 849.505
[53,     1] loss: 838.668
[54,     1] loss: 835.983
[55,     1] loss: 817.217
[56,     1] loss: 774.772
[57,     1] loss: 848.900
[58,     1] loss: 815.764
[59,     1] loss: 816.954
[60,     1] loss: 818.912
[61,     1] loss: 776.987
[62,     1] loss: 852.590
[63,     1] loss: 754.203
[64,     1] loss: 827.179
[65,     1] loss: 797.288
[66,     1] loss: 750.226
[67,     1] loss: 792.242
[68,     1] loss: 753.825
[69,     1] loss: 796.406
[70,     1] loss: 721.307
[71,     1] loss: 747.464
[72,     1] loss: 713.261
[73,     1] loss: 728.633
[74,     1] loss: 697.872
[75,     1] loss: 691.368
[76,     1] loss: 693.395
[77,     1] loss: 666.230
[78,     1] loss: 632.607
[79,     1] loss: 657.132
[80,     1] loss: 610.765
[81,     1] loss: 639.945
[82,     1] loss: 660.284
[83,     1] loss: 585.212
[84,     1] loss: 671.349
[85,     1] loss: 711.757
[86,     1] loss: 601.497
[87,     1] loss: 643.614
[88,     1] loss: 563.176
[89,     1] loss: 643.280
[90,     1] loss: 594.601
[91,     1] loss: 585.390
[92,     1] loss: 584.558
[93,     1] loss: 583.909
[94,     1] loss: 555.598
[95,     1] loss: 586.064
[96,     1] loss: 545.472
[97,     1] loss: 569.284
[98,     1] loss: 570.168
[99,     1] loss: 556.046
[100,     1] loss: 513.579
[101,     1] loss: 533.915
[102,     1] loss: 518.857
[103,     1] loss: 517.521
[104,     1] loss: 478.722
[105,     1] loss: 483.520
[106,     1] loss: 542.601
[107,     1] loss: 434.277
[108,     1] loss: 527.565
[109,     1] loss: 498.533
[110,     1] loss: 485.147
[111,     1] loss: 514.542
[112,     1] loss: 453.519
[113,     1] loss: 495.151
[114,     1] loss: 458.039
[115,     1] loss: 465.353
[116,     1] loss: 456.838
[117,     1] loss: 486.127
[118,     1] loss: 477.348
[119,     1] loss: 448.991
[120,     1] loss: 414.946
[121,     1] loss: 468.087
[122,     1] loss: 422.915
[123,     1] loss: 443.023
[124,     1] loss: 433.427
[125,     1] loss: 392.395
[126,     1] loss: 389.084
[127,     1] loss: 426.170
[128,     1] loss: 467.060
[129,     1] loss: 401.467
[130,     1] loss: 405.160
[131,     1] loss: 452.340
[132,     1] loss: 369.928
[133,     1] loss: 443.489
[134,     1] loss: 387.330
[135,     1] loss: 414.473
[136,     1] loss: 383.568
[137,     1] loss: 393.932
[138,     1] loss: 413.935
Early stopping applied (best metric=0.3287574350833893)
Finished Training
Total time taken: 20.36302876472473
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1237.813
[2,     1] loss: 1233.430
[3,     1] loss: 1233.055
[4,     1] loss: 1234.714
[5,     1] loss: 1233.479
[6,     1] loss: 1226.566
[7,     1] loss: 1222.151
[8,     1] loss: 1208.043
[9,     1] loss: 1191.686
[10,     1] loss: 1167.966
[11,     1] loss: 1142.511
[12,     1] loss: 1108.387
[13,     1] loss: 1112.450
[14,     1] loss: 1069.797
[15,     1] loss: 1014.976
[16,     1] loss: 1022.337
[17,     1] loss: 1041.909
[18,     1] loss: 1017.204
[19,     1] loss: 1041.406
[20,     1] loss: 1029.670
[21,     1] loss: 1025.986
[22,     1] loss: 1012.818
[23,     1] loss: 974.875
[24,     1] loss: 961.546
[25,     1] loss: 962.701
[26,     1] loss: 948.611
[27,     1] loss: 1027.567
[28,     1] loss: 991.352
[29,     1] loss: 944.871
[30,     1] loss: 943.888
[31,     1] loss: 961.754
[32,     1] loss: 952.605
[33,     1] loss: 974.715
[34,     1] loss: 925.321
[35,     1] loss: 921.505
[36,     1] loss: 898.765
[37,     1] loss: 922.432
[38,     1] loss: 890.857
[39,     1] loss: 903.367
[40,     1] loss: 912.988
[41,     1] loss: 859.657
[42,     1] loss: 856.883
[43,     1] loss: 871.391
[44,     1] loss: 873.440
[45,     1] loss: 862.984
[46,     1] loss: 830.102
[47,     1] loss: 797.182
[48,     1] loss: 845.288
[49,     1] loss: 809.785
[50,     1] loss: 856.044
[51,     1] loss: 821.311
[52,     1] loss: 838.197
[53,     1] loss: 797.133
[54,     1] loss: 803.324
[55,     1] loss: 764.692
[56,     1] loss: 771.668
[57,     1] loss: 802.516
[58,     1] loss: 762.731
[59,     1] loss: 716.824
[60,     1] loss: 743.625
[61,     1] loss: 744.152
[62,     1] loss: 696.346
[63,     1] loss: 726.292
[64,     1] loss: 642.747
[65,     1] loss: 664.212
[66,     1] loss: 703.792
[67,     1] loss: 689.955
[68,     1] loss: 772.125
[69,     1] loss: 700.095
[70,     1] loss: 632.445
[71,     1] loss: 670.332
[72,     1] loss: 648.760
[73,     1] loss: 666.349
[74,     1] loss: 664.037
[75,     1] loss: 617.392
[76,     1] loss: 644.153
[77,     1] loss: 671.022
[78,     1] loss: 657.244
[79,     1] loss: 627.394
[80,     1] loss: 626.726
[81,     1] loss: 593.282
[82,     1] loss: 536.993
[83,     1] loss: 569.612
[84,     1] loss: 628.238
[85,     1] loss: 557.008
[86,     1] loss: 670.514
[87,     1] loss: 577.343
[88,     1] loss: 603.489
[89,     1] loss: 650.965
[90,     1] loss: 563.942
[91,     1] loss: 632.344
[92,     1] loss: 549.886
[93,     1] loss: 600.017
[94,     1] loss: 547.474
[95,     1] loss: 530.508
[96,     1] loss: 569.737
Early stopping applied (best metric=0.39860641956329346)
Finished Training
Total time taken: 14.155577421188354
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1238.584
[2,     1] loss: 1235.912
[3,     1] loss: 1236.266
[4,     1] loss: 1233.573
[5,     1] loss: 1230.064
[6,     1] loss: 1227.510
[7,     1] loss: 1225.744
[8,     1] loss: 1218.006
[9,     1] loss: 1207.033
[10,     1] loss: 1193.563
[11,     1] loss: 1173.708
[12,     1] loss: 1152.415
[13,     1] loss: 1127.124
[14,     1] loss: 1102.710
[15,     1] loss: 1093.268
[16,     1] loss: 1084.975
[17,     1] loss: 1117.995
[18,     1] loss: 1097.114
[19,     1] loss: 1070.891
[20,     1] loss: 1014.172
[21,     1] loss: 1031.905
[22,     1] loss: 1011.980
[23,     1] loss: 1061.435
[24,     1] loss: 1028.966
[25,     1] loss: 1006.776
[26,     1] loss: 1018.024
[27,     1] loss: 1026.770
[28,     1] loss: 992.921
[29,     1] loss: 1009.544
[30,     1] loss: 958.320
[31,     1] loss: 952.220
[32,     1] loss: 938.423
[33,     1] loss: 962.671
[34,     1] loss: 954.537
[35,     1] loss: 929.414
[36,     1] loss: 965.855
[37,     1] loss: 937.426
[38,     1] loss: 954.981
[39,     1] loss: 934.546
[40,     1] loss: 899.349
[41,     1] loss: 919.277
[42,     1] loss: 917.097
[43,     1] loss: 875.281
[44,     1] loss: 908.188
[45,     1] loss: 931.278
[46,     1] loss: 898.848
[47,     1] loss: 898.853
[48,     1] loss: 889.978
[49,     1] loss: 858.206
[50,     1] loss: 856.166
[51,     1] loss: 826.110
[52,     1] loss: 838.125
[53,     1] loss: 805.700
[54,     1] loss: 834.146
[55,     1] loss: 788.670
[56,     1] loss: 860.446
[57,     1] loss: 827.401
[58,     1] loss: 773.428
[59,     1] loss: 781.925
[60,     1] loss: 797.503
[61,     1] loss: 801.997
[62,     1] loss: 762.677
[63,     1] loss: 802.738
[64,     1] loss: 762.975
[65,     1] loss: 793.483
[66,     1] loss: 693.555
[67,     1] loss: 796.263
[68,     1] loss: 713.787
[69,     1] loss: 720.732
[70,     1] loss: 697.437
[71,     1] loss: 748.670
[72,     1] loss: 733.169
[73,     1] loss: 693.772
[74,     1] loss: 726.979
[75,     1] loss: 657.620
[76,     1] loss: 691.695
[77,     1] loss: 659.446
[78,     1] loss: 664.006
[79,     1] loss: 718.130
[80,     1] loss: 622.360
[81,     1] loss: 693.637
[82,     1] loss: 631.358
[83,     1] loss: 691.250
[84,     1] loss: 666.545
[85,     1] loss: 626.827
[86,     1] loss: 674.905
[87,     1] loss: 594.642
[88,     1] loss: 634.475
[89,     1] loss: 567.094
[90,     1] loss: 622.341
[91,     1] loss: 604.403
[92,     1] loss: 643.632
[93,     1] loss: 593.777
[94,     1] loss: 606.558
[95,     1] loss: 568.672
[96,     1] loss: 585.030
[97,     1] loss: 538.843
[98,     1] loss: 540.247
[99,     1] loss: 527.781
[100,     1] loss: 570.877
[101,     1] loss: 529.094
[102,     1] loss: 600.419
[103,     1] loss: 544.730
[104,     1] loss: 551.285
[105,     1] loss: 522.056
[106,     1] loss: 534.005
[107,     1] loss: 562.243
[108,     1] loss: 570.512
[109,     1] loss: 507.608
[110,     1] loss: 570.367
[111,     1] loss: 510.564
[112,     1] loss: 537.767
[113,     1] loss: 492.404
[114,     1] loss: 538.484
[115,     1] loss: 487.657
[116,     1] loss: 549.822
[117,     1] loss: 458.480
Early stopping applied (best metric=0.34755176305770874)
Finished Training
Total time taken: 17.57609486579895
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.350
[2,     1] loss: 1234.558
[3,     1] loss: 1235.382
[4,     1] loss: 1233.000
[5,     1] loss: 1237.583
[6,     1] loss: 1226.412
[7,     1] loss: 1225.277
[8,     1] loss: 1222.863
[9,     1] loss: 1213.357
[10,     1] loss: 1197.093
[11,     1] loss: 1183.005
[12,     1] loss: 1162.260
[13,     1] loss: 1158.893
[14,     1] loss: 1129.034
[15,     1] loss: 1096.113
[16,     1] loss: 1077.519
[17,     1] loss: 1097.359
[18,     1] loss: 1035.926
[19,     1] loss: 1032.328
[20,     1] loss: 1024.567
[21,     1] loss: 1056.628
[22,     1] loss: 980.825
[23,     1] loss: 990.669
[24,     1] loss: 995.938
[25,     1] loss: 998.884
[26,     1] loss: 989.391
[27,     1] loss: 988.319
[28,     1] loss: 980.688
[29,     1] loss: 927.119
[30,     1] loss: 973.930
[31,     1] loss: 930.013
[32,     1] loss: 968.128
[33,     1] loss: 923.802
[34,     1] loss: 875.693
[35,     1] loss: 934.583
[36,     1] loss: 908.005
[37,     1] loss: 894.036
[38,     1] loss: 905.120
[39,     1] loss: 876.500
[40,     1] loss: 909.270
[41,     1] loss: 869.874
[42,     1] loss: 873.288
[43,     1] loss: 846.777
[44,     1] loss: 871.515
[45,     1] loss: 824.192
[46,     1] loss: 838.384
[47,     1] loss: 823.175
[48,     1] loss: 784.316
[49,     1] loss: 776.111
[50,     1] loss: 839.577
[51,     1] loss: 897.227
[52,     1] loss: 779.934
[53,     1] loss: 889.102
[54,     1] loss: 798.083
[55,     1] loss: 775.872
[56,     1] loss: 741.935
[57,     1] loss: 741.974
[58,     1] loss: 741.595
[59,     1] loss: 745.254
[60,     1] loss: 706.495
[61,     1] loss: 723.254
[62,     1] loss: 711.179
[63,     1] loss: 687.407
[64,     1] loss: 685.301
[65,     1] loss: 728.342
[66,     1] loss: 709.844
[67,     1] loss: 690.422
[68,     1] loss: 685.045
[69,     1] loss: 715.407
[70,     1] loss: 645.342
[71,     1] loss: 734.959
[72,     1] loss: 691.772
[73,     1] loss: 696.620
[74,     1] loss: 697.724
[75,     1] loss: 681.895
[76,     1] loss: 722.782
[77,     1] loss: 672.223
[78,     1] loss: 613.086
[79,     1] loss: 652.845
[80,     1] loss: 628.783
[81,     1] loss: 620.770
[82,     1] loss: 589.025
[83,     1] loss: 583.292
[84,     1] loss: 579.668
[85,     1] loss: 604.059
[86,     1] loss: 583.340
[87,     1] loss: 554.697
[88,     1] loss: 590.462
[89,     1] loss: 572.664
[90,     1] loss: 501.350
[91,     1] loss: 579.393
[92,     1] loss: 489.742
[93,     1] loss: 538.668
Early stopping applied (best metric=0.3828011453151703)
Finished Training
Total time taken: 13.983061075210571
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.699
[2,     1] loss: 1235.224
[3,     1] loss: 1235.469
[4,     1] loss: 1231.876
[5,     1] loss: 1232.784
[6,     1] loss: 1229.686
[7,     1] loss: 1227.069
[8,     1] loss: 1223.054
[9,     1] loss: 1214.342
[10,     1] loss: 1199.504
[11,     1] loss: 1181.749
[12,     1] loss: 1156.638
[13,     1] loss: 1125.567
[14,     1] loss: 1101.285
[15,     1] loss: 1077.889
[16,     1] loss: 1082.460
[17,     1] loss: 1067.830
[18,     1] loss: 1059.466
[19,     1] loss: 1042.013
[20,     1] loss: 1003.881
[21,     1] loss: 1044.912
[22,     1] loss: 1008.656
[23,     1] loss: 1007.436
[24,     1] loss: 1001.313
[25,     1] loss: 1008.548
[26,     1] loss: 1017.587
[27,     1] loss: 971.851
[28,     1] loss: 1024.300
[29,     1] loss: 988.521
[30,     1] loss: 951.309
[31,     1] loss: 1040.008
[32,     1] loss: 1001.844
[33,     1] loss: 935.947
[34,     1] loss: 961.253
[35,     1] loss: 972.449
[36,     1] loss: 938.763
[37,     1] loss: 959.390
[38,     1] loss: 973.714
[39,     1] loss: 921.662
[40,     1] loss: 924.536
[41,     1] loss: 924.126
[42,     1] loss: 969.451
[43,     1] loss: 934.488
[44,     1] loss: 905.637
[45,     1] loss: 876.439
[46,     1] loss: 842.953
[47,     1] loss: 882.515
[48,     1] loss: 900.137
[49,     1] loss: 862.163
[50,     1] loss: 839.542
[51,     1] loss: 881.284
[52,     1] loss: 869.785
[53,     1] loss: 865.199
[54,     1] loss: 884.777
[55,     1] loss: 860.395
[56,     1] loss: 821.518
[57,     1] loss: 832.472
[58,     1] loss: 790.753
[59,     1] loss: 792.385
[60,     1] loss: 864.921
[61,     1] loss: 821.907
[62,     1] loss: 864.925
[63,     1] loss: 806.486
[64,     1] loss: 847.597
[65,     1] loss: 796.202
[66,     1] loss: 747.532
[67,     1] loss: 780.397
[68,     1] loss: 730.475
[69,     1] loss: 741.587
[70,     1] loss: 735.973
[71,     1] loss: 760.801
[72,     1] loss: 726.388
[73,     1] loss: 733.557
[74,     1] loss: 711.811
[75,     1] loss: 784.565
[76,     1] loss: 682.719
[77,     1] loss: 671.830
[78,     1] loss: 665.074
[79,     1] loss: 673.078
[80,     1] loss: 682.458
[81,     1] loss: 664.982
[82,     1] loss: 635.137
[83,     1] loss: 624.057
[84,     1] loss: 633.424
[85,     1] loss: 653.727
[86,     1] loss: 613.522
[87,     1] loss: 575.487
[88,     1] loss: 547.895
[89,     1] loss: 612.207
[90,     1] loss: 557.447
[91,     1] loss: 597.278
[92,     1] loss: 567.818
[93,     1] loss: 634.723
[94,     1] loss: 576.772
[95,     1] loss: 557.670
[96,     1] loss: 575.958
[97,     1] loss: 540.589
[98,     1] loss: 525.411
[99,     1] loss: 574.669
[100,     1] loss: 494.953
[101,     1] loss: 580.058
[102,     1] loss: 526.685
[103,     1] loss: 537.383
[104,     1] loss: 520.643
[105,     1] loss: 552.799
[106,     1] loss: 502.803
[107,     1] loss: 696.400
[108,     1] loss: 679.445
[109,     1] loss: 532.986
[110,     1] loss: 661.928
[111,     1] loss: 495.526
[112,     1] loss: 539.182
[113,     1] loss: 589.490
[114,     1] loss: 494.128
[115,     1] loss: 497.637
[116,     1] loss: 481.045
[117,     1] loss: 503.618
[118,     1] loss: 530.232
[119,     1] loss: 446.182
[120,     1] loss: 549.980
[121,     1] loss: 468.514
[122,     1] loss: 492.484
[123,     1] loss: 423.610
[124,     1] loss: 453.039
Early stopping applied (best metric=0.3382294476032257)
Finished Training
Total time taken: 18.18167996406555
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.109
[2,     1] loss: 1233.900
[3,     1] loss: 1231.466
[4,     1] loss: 1232.477
[5,     1] loss: 1228.232
[6,     1] loss: 1229.571
[7,     1] loss: 1222.994
[8,     1] loss: 1210.177
[9,     1] loss: 1200.333
[10,     1] loss: 1172.224
[11,     1] loss: 1160.205
[12,     1] loss: 1130.154
[13,     1] loss: 1100.376
[14,     1] loss: 1086.762
[15,     1] loss: 1096.862
[16,     1] loss: 1055.576
[17,     1] loss: 1062.656
[18,     1] loss: 1023.967
[19,     1] loss: 991.835
[20,     1] loss: 989.434
[21,     1] loss: 1009.526
[22,     1] loss: 1010.865
[23,     1] loss: 972.596
[24,     1] loss: 958.824
[25,     1] loss: 966.133
[26,     1] loss: 952.307
[27,     1] loss: 955.003
[28,     1] loss: 904.389
[29,     1] loss: 936.334
[30,     1] loss: 939.094
[31,     1] loss: 932.861
[32,     1] loss: 914.040
[33,     1] loss: 954.416
[34,     1] loss: 864.425
[35,     1] loss: 894.854
[36,     1] loss: 884.376
[37,     1] loss: 891.918
[38,     1] loss: 919.147
[39,     1] loss: 844.195
[40,     1] loss: 920.359
[41,     1] loss: 808.645
[42,     1] loss: 873.182
[43,     1] loss: 868.437
[44,     1] loss: 861.370
[45,     1] loss: 789.523
[46,     1] loss: 825.032
[47,     1] loss: 810.353
[48,     1] loss: 763.080
[49,     1] loss: 802.325
[50,     1] loss: 826.683
[51,     1] loss: 835.350
[52,     1] loss: 767.799
[53,     1] loss: 753.539
[54,     1] loss: 751.776
[55,     1] loss: 749.424
[56,     1] loss: 678.126
[57,     1] loss: 711.473
[58,     1] loss: 741.497
[59,     1] loss: 745.683
[60,     1] loss: 764.676
[61,     1] loss: 700.633
[62,     1] loss: 709.029
[63,     1] loss: 702.338
[64,     1] loss: 671.427
[65,     1] loss: 686.615
[66,     1] loss: 671.862
[67,     1] loss: 707.092
[68,     1] loss: 667.792
[69,     1] loss: 659.963
[70,     1] loss: 643.926
[71,     1] loss: 623.840
Early stopping applied (best metric=0.4163246154785156)
Finished Training
Total time taken: 10.157119512557983
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1236.511
[2,     1] loss: 1236.898
[3,     1] loss: 1237.191
[4,     1] loss: 1235.375
[5,     1] loss: 1235.906
[6,     1] loss: 1235.133
[7,     1] loss: 1235.310
[8,     1] loss: 1230.022
[9,     1] loss: 1234.179
[10,     1] loss: 1224.814
[11,     1] loss: 1220.560
[12,     1] loss: 1213.860
[13,     1] loss: 1191.819
[14,     1] loss: 1180.590
[15,     1] loss: 1145.030
[16,     1] loss: 1125.722
[17,     1] loss: 1105.306
[18,     1] loss: 1065.244
[19,     1] loss: 1073.770
[20,     1] loss: 1046.505
[21,     1] loss: 1055.083
[22,     1] loss: 1035.677
[23,     1] loss: 1017.779
[24,     1] loss: 1073.578
[25,     1] loss: 1025.717
[26,     1] loss: 1023.459
[27,     1] loss: 979.203
[28,     1] loss: 1000.082
[29,     1] loss: 993.057
[30,     1] loss: 983.627
[31,     1] loss: 1035.830
[32,     1] loss: 984.607
[33,     1] loss: 1003.327
[34,     1] loss: 948.599
[35,     1] loss: 957.669
[36,     1] loss: 973.000
[37,     1] loss: 963.032
[38,     1] loss: 928.054
[39,     1] loss: 919.542
[40,     1] loss: 897.662
[41,     1] loss: 907.085
[42,     1] loss: 887.686
[43,     1] loss: 904.450
[44,     1] loss: 900.796
[45,     1] loss: 904.114
[46,     1] loss: 881.031
[47,     1] loss: 890.347
[48,     1] loss: 872.741
[49,     1] loss: 850.640
[50,     1] loss: 867.370
[51,     1] loss: 872.525
[52,     1] loss: 861.216
[53,     1] loss: 838.305
[54,     1] loss: 784.137
[55,     1] loss: 824.698
[56,     1] loss: 863.315
[57,     1] loss: 774.483
[58,     1] loss: 820.509
[59,     1] loss: 788.557
[60,     1] loss: 788.294
[61,     1] loss: 823.520
[62,     1] loss: 757.868
[63,     1] loss: 814.445
[64,     1] loss: 779.447
[65,     1] loss: 757.891
[66,     1] loss: 719.497
[67,     1] loss: 744.999
[68,     1] loss: 717.105
[69,     1] loss: 714.939
[70,     1] loss: 742.990
[71,     1] loss: 731.397
[72,     1] loss: 682.681
[73,     1] loss: 791.503
[74,     1] loss: 742.953
[75,     1] loss: 692.863
[76,     1] loss: 753.005
[77,     1] loss: 663.232
[78,     1] loss: 807.732
[79,     1] loss: 649.250
[80,     1] loss: 769.482
[81,     1] loss: 641.931
[82,     1] loss: 662.173
[83,     1] loss: 657.889
[84,     1] loss: 627.953
[85,     1] loss: 663.208
[86,     1] loss: 607.719
[87,     1] loss: 617.017
[88,     1] loss: 594.828
[89,     1] loss: 606.915
[90,     1] loss: 555.900
[91,     1] loss: 611.426
[92,     1] loss: 586.497
[93,     1] loss: 559.404
[94,     1] loss: 557.297
[95,     1] loss: 546.233
[96,     1] loss: 494.653
[97,     1] loss: 573.971
[98,     1] loss: 480.249
[99,     1] loss: 495.414
[100,     1] loss: 522.687
[101,     1] loss: 512.774
[102,     1] loss: 534.206
[103,     1] loss: 464.392
[104,     1] loss: 468.818
[105,     1] loss: 487.022
[106,     1] loss: 510.061
[107,     1] loss: 451.133
[108,     1] loss: 485.530
[109,     1] loss: 434.385
[110,     1] loss: 509.712
[111,     1] loss: 436.631
[112,     1] loss: 493.454
[113,     1] loss: 502.393
Early stopping applied (best metric=0.3637621998786926)
Finished Training
Total time taken: 16.03454613685608
{'Hydroxylation-K Validation Accuracy': 0.7645981087470449, 'Hydroxylation-K Validation Sensitivity': 0.6333333333333333, 'Hydroxylation-K Validation Specificity': 0.7982456140350878, 'Hydroxylation-K Validation Precision': 0.44914864633130885, 'Hydroxylation-K AUC ROC': 0.7872904483430799, 'Hydroxylation-K AUC PR': 0.5277276261317705, 'Hydroxylation-K MCC': 0.3857535389696926, 'Hydroxylation-K F1': 0.5180310832484746, 'Validation Loss (Hydroxylation-K)': 0.4694162944952647, 'Hydroxylation-P Validation Accuracy': 0.7902373483579513, 'Hydroxylation-P Validation Sensitivity': 0.7744973544973546, 'Hydroxylation-P Validation Specificity': 0.7936555439174023, 'Hydroxylation-P Validation Precision': 0.45185511836332803, 'Hydroxylation-P AUC ROC': 0.8467362659895953, 'Hydroxylation-P AUC PR': 0.5905227533397992, 'Hydroxylation-P MCC': 0.4730825021265085, 'Hydroxylation-P F1': 0.56872186884337, 'Validation Loss (Hydroxylation-P)': 0.3692806859811147, 'Validation Loss (total)': 0.8386969804763794, 'TimeToTrain': 15.561896181106567}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006189368675579526,
 'learning_rate_Hydroxylation-K': 0.0076071996383019055,
 'learning_rate_Hydroxylation-P': 0.005694844280566073,
 'log_base': 1.2125406593224264,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1936963380,
 'sample_weights': [1.5480061983746232, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.670427381138703,
 'weight_decay_Hydroxylation-K': 3.865928806877208,
 'weight_decay_Hydroxylation-P': 4.59811824484725}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2799.377
[2,     1] loss: 2818.969
[3,     1] loss: 2809.776
[4,     1] loss: 2810.635
[5,     1] loss: 2795.092
[6,     1] loss: 2792.451
[7,     1] loss: 2762.710
[8,     1] loss: 2734.056
[9,     1] loss: 2653.129
[10,     1] loss: 2543.190
[11,     1] loss: 2487.880
[12,     1] loss: 2368.268
[13,     1] loss: 2265.922
[14,     1] loss: 2238.727
[15,     1] loss: 2664.509
[16,     1] loss: 2078.563
[17,     1] loss: 2441.572
[18,     1] loss: 2117.352
[19,     1] loss: 2375.998
[20,     1] loss: 2213.603
[21,     1] loss: 2105.329
[22,     1] loss: 2330.489
[23,     1] loss: 2113.915
[24,     1] loss: 2143.747
[25,     1] loss: 1926.587
[26,     1] loss: 2019.570
[27,     1] loss: 1927.215
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009985320987680341,
 'learning_rate_Hydroxylation-K': 0.0028549299335235115,
 'learning_rate_Hydroxylation-P': 0.006582549483271761,
 'log_base': 2.033238446290372,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3439676881,
 'sample_weights': [8.66262733773632, 1.0828701321159084],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.3248615143372313,
 'weight_decay_Hydroxylation-K': 8.891149909211983,
 'weight_decay_Hydroxylation-P': 1.6055516931646827}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1405.276
[2,     1] loss: 1408.182
[3,     1] loss: 1408.556
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005034340294405586,
 'learning_rate_Hydroxylation-K': 0.004881819614415941,
 'learning_rate_Hydroxylation-P': 0.0012940136177716772,
 'log_base': 2.5670081822914037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1844155322,
 'sample_weights': [2.352554966793339, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0920217500454177,
 'weight_decay_Hydroxylation-K': 1.0684543948723038,
 'weight_decay_Hydroxylation-P': 3.313538779422952}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1285.491
[2,     1] loss: 1281.994
[3,     1] loss: 1287.156
[4,     1] loss: 1282.834
[5,     1] loss: 1283.151
[6,     1] loss: 1281.451
[7,     1] loss: 1274.720
[8,     1] loss: 1275.740
[9,     1] loss: 1272.680
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004363523249434722,
 'learning_rate_Hydroxylation-K': 0.004030286290274149,
 'learning_rate_Hydroxylation-P': 0.009819423126503794,
 'log_base': 1.0241829741037263,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4122419635,
 'sample_weights': [1.7708394873604147, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.745922286710149,
 'weight_decay_Hydroxylation-K': 3.9867646815938564,
 'weight_decay_Hydroxylation-P': 2.11965434972268}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22682.887
Exploding loss, terminate run (best metric=0.5339956283569336)
Finished Training
Total time taken: 0.21999764442443848
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22560.844
Exploding loss, terminate run (best metric=0.5299833416938782)
Finished Training
Total time taken: 0.21199703216552734
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22555.217
Exploding loss, terminate run (best metric=0.5425975918769836)
Finished Training
Total time taken: 0.19099926948547363
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22603.023
Exploding loss, terminate run (best metric=0.5260845422744751)
Finished Training
Total time taken: 0.21400094032287598
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22740.139
Exploding loss, terminate run (best metric=0.5269157290458679)
Finished Training
Total time taken: 0.19499945640563965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22847.857
Exploding loss, terminate run (best metric=0.5325498580932617)
Finished Training
Total time taken: 0.2279970645904541
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22674.771
Exploding loss, terminate run (best metric=0.5279541611671448)
Finished Training
Total time taken: 0.19699978828430176
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22594.176
Exploding loss, terminate run (best metric=0.5267583727836609)
Finished Training
Total time taken: 0.2220008373260498
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22654.967
Exploding loss, terminate run (best metric=0.5269132256507874)
Finished Training
Total time taken: 0.20400047302246094
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22682.891
Exploding loss, terminate run (best metric=0.5287896990776062)
Finished Training
Total time taken: 0.22699952125549316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22718.186
Exploding loss, terminate run (best metric=0.5334221124649048)
Finished Training
Total time taken: 0.20499968528747559
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22658.703
Exploding loss, terminate run (best metric=0.5270155072212219)
Finished Training
Total time taken: 0.20800018310546875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22793.844
Exploding loss, terminate run (best metric=0.5347930788993835)
Finished Training
Total time taken: 0.20851659774780273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22710.527
Exploding loss, terminate run (best metric=0.5268746018409729)
Finished Training
Total time taken: 0.21899843215942383
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22692.477
Exploding loss, terminate run (best metric=0.5334618091583252)
Finished Training
Total time taken: 0.20300006866455078
{'Hydroxylation-K Validation Accuracy': 0.3552304964539007, 'Hydroxylation-K Validation Sensitivity': 0.74, 'Hydroxylation-K Validation Specificity': 0.2596491228070176, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5948538011695906, 'Hydroxylation-K AUC PR': 0.31792133711915377, 'Hydroxylation-K MCC': -0.004734788601457521, 'Hydroxylation-K F1': 0.2592002318168647, 'Validation Loss (Hydroxylation-K)': 0.5584547956784566, 'Hydroxylation-P Validation Accuracy': 0.34474431077948664, 'Hydroxylation-P Validation Sensitivity': 0.7518518518518519, 'Hydroxylation-P Validation Specificity': 0.2573170731707317, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5938113803720567, 'Hydroxylation-P AUC PR': 0.2978506126231867, 'Hydroxylation-P MCC': 0.00195344530131058, 'Hydroxylation-P F1': 0.24246838972961038, 'Validation Loss (Hydroxylation-P)': 0.5305406173070272, 'Validation Loss (total)': 1.0889954010645548, 'TimeToTrain': 0.21023379961649577}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001965546321944462,
 'learning_rate_Hydroxylation-K': 0.00920727530183117,
 'learning_rate_Hydroxylation-P': 0.009814276833964772,
 'log_base': 1.0449816299332637,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 15915322,
 'sample_weights': [69.91704594021552, 8.721471333964574],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.545742511237078,
 'weight_decay_Hydroxylation-K': 5.112704617916687,
 'weight_decay_Hydroxylation-P': 2.5981773632567537}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12257.660
[2,     1] loss: 12336.580
[3,     1] loss: 12239.023
[4,     1] loss: 12326.891
[5,     1] loss: 12348.844
[6,     1] loss: 12318.371
[7,     1] loss: 12307.005
[8,     1] loss: 12287.822
[9,     1] loss: 12310.609
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034056989537053495,
 'learning_rate_Hydroxylation-K': 0.003314280912885488,
 'learning_rate_Hydroxylation-P': 0.008547931049181234,
 'log_base': 2.7980314797062884,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3433215462,
 'sample_weights': [37.94248795421771, 4.742993706402084],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2968504258174596,
 'weight_decay_Hydroxylation-K': 4.757619987009183,
 'weight_decay_Hydroxylation-P': 3.6687486948993078}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.807
[2,     1] loss: 1251.608
[3,     1] loss: 1249.093
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005830550418959003,
 'learning_rate_Hydroxylation-K': 0.00663422873499653,
 'learning_rate_Hydroxylation-P': 0.0078099905359495314,
 'log_base': 1.1920332502260909,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1446898739,
 'sample_weights': [1.6225259801713112, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.937379148447006,
 'weight_decay_Hydroxylation-K': 1.5566632947374375,
 'weight_decay_Hydroxylation-P': 3.2431503921064144}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3099.649
[2,     1] loss: 3083.414
[3,     1] loss: 3080.564
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007357724557764398,
 'learning_rate_Hydroxylation-K': 0.006531561466601484,
 'learning_rate_Hydroxylation-P': 0.008329861896794752,
 'log_base': 2.7449190595757518,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 142165789,
 'sample_weights': [9.503807068431295, 1.1880216491554167],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.424531213492843,
 'weight_decay_Hydroxylation-K': 7.716326435828146,
 'weight_decay_Hydroxylation-P': 9.570388666192116}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.032
[2,     1] loss: 1257.043
[3,     1] loss: 1258.828
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021380739941302494,
 'learning_rate_Hydroxylation-K': 0.004869478928630818,
 'learning_rate_Hydroxylation-P': 0.008057646653955796,
 'log_base': 1.032555621137576,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4073575634,
 'sample_weights': [1.653320645600647, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.66027597989175,
 'weight_decay_Hydroxylation-K': 0.23270289213921913,
 'weight_decay_Hydroxylation-P': 0.8232897021560196}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16868.422
Exploding loss, terminate run (best metric=0.5376055240631104)
Finished Training
Total time taken: 0.19699883460998535
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16912.262
Exploding loss, terminate run (best metric=0.533865213394165)
Finished Training
Total time taken: 0.2129974365234375
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16923.844
Exploding loss, terminate run (best metric=0.5312334299087524)
Finished Training
Total time taken: 0.21200180053710938
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16930.475
Exploding loss, terminate run (best metric=0.5324939489364624)
Finished Training
Total time taken: 0.21000003814697266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16713.379
Exploding loss, terminate run (best metric=0.5293989181518555)
Finished Training
Total time taken: 0.2050001621246338
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17001.113
Exploding loss, terminate run (best metric=0.5319852232933044)
Finished Training
Total time taken: 0.20099973678588867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16909.891
Exploding loss, terminate run (best metric=0.5294168591499329)
Finished Training
Total time taken: 0.20700335502624512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16895.109
Exploding loss, terminate run (best metric=0.5339604616165161)
Finished Training
Total time taken: 0.2070002555847168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16956.582
Exploding loss, terminate run (best metric=0.5313056111335754)
Finished Training
Total time taken: 0.20299983024597168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17012.672
Exploding loss, terminate run (best metric=0.5294063091278076)
Finished Training
Total time taken: 0.21199774742126465
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17013.650
Exploding loss, terminate run (best metric=0.5328578948974609)
Finished Training
Total time taken: 0.2109980583190918
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16936.039
Exploding loss, terminate run (best metric=0.5311410427093506)
Finished Training
Total time taken: 0.20699810981750488
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16973.207
Exploding loss, terminate run (best metric=0.5288965106010437)
Finished Training
Total time taken: 0.21500039100646973
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17105.742
Exploding loss, terminate run (best metric=0.532382071018219)
Finished Training
Total time taken: 0.22200441360473633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16900.498
Exploding loss, terminate run (best metric=0.5277988314628601)
Finished Training
Total time taken: 0.21199798583984375
{'Hydroxylation-K Validation Accuracy': 0.5569444444444445, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.5982456140350877, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6045614035087719, 'Hydroxylation-K AUC PR': 0.3082758254418161, 'Hydroxylation-K MCC': 0.001013768674308847, 'Hydroxylation-K F1': 0.14115067958720204, 'Validation Loss (Hydroxylation-K)': 0.5565988183021545, 'Hydroxylation-P Validation Accuracy': 0.5555612405461652, 'Hydroxylation-P Validation Sensitivity': 0.4247619047619048, 'Hydroxylation-P Validation Specificity': 0.5829268292682926, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5842184940546455, 'Hydroxylation-P AUC PR': 0.2588429704924619, 'Hydroxylation-P MCC': 0.009464941310510004, 'Hydroxylation-P F1': 0.13949337439608667, 'Validation Loss (Hydroxylation-P)': 0.5315831899642944, 'Validation Loss (total)': 1.088182020187378, 'TimeToTrain': 0.2089332103729248}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023269494626041273,
 'learning_rate_Hydroxylation-K': 0.005071135639337736,
 'learning_rate_Hydroxylation-P': 0.007044494467469176,
 'log_base': 1.2603395946389484,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2530602511,
 'sample_weights': [52.14864010913802, 6.505035556078595],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.901610445231855,
 'weight_decay_Hydroxylation-K': 0.6102222748342765,
 'weight_decay_Hydroxylation-P': 1.9105233617396897}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2437.801
[2,     1] loss: 2429.684
[3,     1] loss: 2418.841
[4,     1] loss: 2431.587
[5,     1] loss: 2428.146
[6,     1] loss: 2428.662
[7,     1] loss: 2435.160
[8,     1] loss: 2426.810
[9,     1] loss: 2433.616
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004009274758320838,
 'learning_rate_Hydroxylation-K': 0.005577933665089758,
 'learning_rate_Hydroxylation-P': 0.009395108053754108,
 'log_base': 1.056482227675265,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2525636439,
 'sample_weights': [7.215119971240521, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.818554828664662,
 'weight_decay_Hydroxylation-K': 1.338592999827343,
 'weight_decay_Hydroxylation-P': 0.06727520556578281}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9841.813
[2,     1] loss: 9864.965
[3,     1] loss: 9856.777
[4,     1] loss: 9900.442
[5,     1] loss: 9812.579
[6,     1] loss: 9862.326
[7,     1] loss: 9848.102
[8,     1] loss: 9835.798
[9,     1] loss: 9819.434
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002841183110351021,
 'learning_rate_Hydroxylation-K': 0.004768653873285377,
 'learning_rate_Hydroxylation-P': 0.009609874977392553,
 'log_base': 1.2412630440261114,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 816179962,
 'sample_weights': [30.38404163182905, 3.798151518390468],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.081722941003921,
 'weight_decay_Hydroxylation-K': 3.513233205403353,
 'weight_decay_Hydroxylation-P': 0.81128388669948}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2541.570
[2,     1] loss: 2533.343
[3,     1] loss: 2539.676
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037392069310939902,
 'learning_rate_Hydroxylation-K': 0.004609568731098253,
 'learning_rate_Hydroxylation-P': 0.009888688304620532,
 'log_base': 1.1371391332130252,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1616456376,
 'sample_weights': [7.724274434782838, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.0877652550338714,
 'weight_decay_Hydroxylation-K': 3.647837911197509,
 'weight_decay_Hydroxylation-P': 2.008366562144383}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4196.497
[2,     1] loss: 4191.881
[3,     1] loss: 4202.765
[4,     1] loss: 4206.150
[5,     1] loss: 4232.300
[6,     1] loss: 4189.456
[7,     1] loss: 4224.064
[8,     1] loss: 4198.562
[9,     1] loss: 4230.289
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009157260070771372,
 'learning_rate_Hydroxylation-K': 0.002931579725600321,
 'learning_rate_Hydroxylation-P': 0.00924693410163244,
 'log_base': 2.792123074914735,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3812299600,
 'sample_weights': [12.9902008737459, 1.623837663555923],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4467104468002274,
 'weight_decay_Hydroxylation-K': 3.710903951417038,
 'weight_decay_Hydroxylation-P': 5.588429491983634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.836
[2,     1] loss: 1251.257
[3,     1] loss: 1257.135
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008610830308860391,
 'learning_rate_Hydroxylation-K': 0.0067833397277653595,
 'learning_rate_Hydroxylation-P': 0.007687971956888868,
 'log_base': 1.0130713515696015,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1019070720,
 'sample_weights': [1.6258662490344582, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.110688411668615,
 'weight_decay_Hydroxylation-K': 1.362408300992733,
 'weight_decay_Hydroxylation-P': 0.813098101022258}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41744.109
Exploding loss, terminate run (best metric=0.5319222807884216)
Finished Training
Total time taken: 0.20799922943115234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41682.098
Exploding loss, terminate run (best metric=0.5270885825157166)
Finished Training
Total time taken: 0.20600080490112305
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41663.621
Exploding loss, terminate run (best metric=0.5271774530410767)
Finished Training
Total time taken: 0.2129979133605957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41923.914
Exploding loss, terminate run (best metric=0.5305618643760681)
Finished Training
Total time taken: 0.20600032806396484
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 41800.793
Exploding loss, terminate run (best metric=0.5307568907737732)
Finished Training
Total time taken: 0.22700047492980957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41708.695
Exploding loss, terminate run (best metric=0.5331723690032959)
Finished Training
Total time taken: 0.20203018188476562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41721.730
Exploding loss, terminate run (best metric=0.5268957614898682)
Finished Training
Total time taken: 0.2239999771118164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41709.914
Exploding loss, terminate run (best metric=0.5272383093833923)
Finished Training
Total time taken: 0.1960010528564453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41668.391
Exploding loss, terminate run (best metric=0.5315721035003662)
Finished Training
Total time taken: 0.22500014305114746
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 42138.121
Exploding loss, terminate run (best metric=0.527900755405426)
Finished Training
Total time taken: 0.21700310707092285
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41631.098
Exploding loss, terminate run (best metric=0.531704306602478)
Finished Training
Total time taken: 0.222001314163208
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41819.055
Exploding loss, terminate run (best metric=0.5285934805870056)
Finished Training
Total time taken: 0.2050008773803711
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41615.105
Exploding loss, terminate run (best metric=0.5267050862312317)
Finished Training
Total time taken: 0.2220001220703125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 41724.309
Exploding loss, terminate run (best metric=0.5302841663360596)
Finished Training
Total time taken: 0.2070021629333496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 41810.820
Exploding loss, terminate run (best metric=0.5280446410179138)
Finished Training
Total time taken: 0.225999116897583
{'Hydroxylation-K Validation Accuracy': 0.36613475177304966, 'Hydroxylation-K Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-K Validation Specificity': 0.26666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6131189083820663, 'Hydroxylation-K AUC PR': 0.3046091477648499, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.2497536945812808, 'Validation Loss (Hydroxylation-K)': 0.5574755628903707, 'Hydroxylation-P Validation Accuracy': 0.3496270578481634, 'Hydroxylation-P Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-P Validation Specificity': 0.26666666666666666, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6046091282706575, 'Hydroxylation-P AUC PR': 0.27100728948148617, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.22073229341724288, 'Validation Loss (Hydroxylation-P)': 0.5293078700701396, 'Validation Loss (total)': 1.0867834568023682, 'TimeToTrain': 0.21373578707377117}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006410091518760673,
 'learning_rate_Hydroxylation-K': 0.005628192834760119,
 'learning_rate_Hydroxylation-P': 0.008981588016580208,
 'log_base': 1.1974735735300408,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1382964924,
 'sample_weights': [128.64598798614495, 16.04733554404023],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3420619613855767,
 'weight_decay_Hydroxylation-K': 2.1557123840153065,
 'weight_decay_Hydroxylation-P': 1.646382841143741}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3028.853
[2,     1] loss: 3006.015
[3,     1] loss: 3005.763
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004862473853515783,
 'learning_rate_Hydroxylation-K': 0.008756463224593832,
 'learning_rate_Hydroxylation-P': 0.008540653706062084,
 'log_base': 1.784034464931325,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3694964185,
 'sample_weights': [9.263671601142716, 1.1580035593715328],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.169176147281498,
 'weight_decay_Hydroxylation-K': 8.165432892712333,
 'weight_decay_Hydroxylation-P': 1.6413975609866704}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1521.026
[2,     1] loss: 1515.662
[3,     1] loss: 1511.463
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00534227092375379,
 'learning_rate_Hydroxylation-K': 0.00046211038819471103,
 'learning_rate_Hydroxylation-P': 0.007935385787847372,
 'log_base': 2.2086345020421714,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2477133876,
 'sample_weights': [2.8839323893413864, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.648415682856746,
 'weight_decay_Hydroxylation-K': 7.849605967978036,
 'weight_decay_Hydroxylation-P': 1.7379307250049028}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1359.227
[2,     1] loss: 1365.895
[3,     1] loss: 1352.282
[4,     1] loss: 1358.913
[5,     1] loss: 1350.778
[6,     1] loss: 1353.235
[7,     1] loss: 1351.238
[8,     1] loss: 1351.951
[9,     1] loss: 1350.868
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002070564026098595,
 'learning_rate_Hydroxylation-K': 0.0027090335324212135,
 'learning_rate_Hydroxylation-P': 0.008707986162666492,
 'log_base': 1.43830641327248,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3059331036,
 'sample_weights': [2.106886640290108, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.801179081704833,
 'weight_decay_Hydroxylation-K': 6.030036490539551,
 'weight_decay_Hydroxylation-P': 9.075689766778368}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1885.078
[2,     1] loss: 1876.411
[3,     1] loss: 1874.789
[4,     1] loss: 1878.504
[5,     1] loss: 1876.335
[6,     1] loss: 1880.794
[7,     1] loss: 1872.502
[8,     1] loss: 1867.430
[9,     1] loss: 1868.269
[10,     1] loss: 1861.580
[11,     1] loss: 1855.026
[12,     1] loss: 1840.954
[13,     1] loss: 1805.309
[14,     1] loss: 1790.065
[15,     1] loss: 1746.774
[16,     1] loss: 1723.944
[17,     1] loss: 1672.057
[18,     1] loss: 1666.914
[19,     1] loss: 1662.583
[20,     1] loss: 1634.919
[21,     1] loss: 1544.940
[22,     1] loss: 1573.465
[23,     1] loss: 1534.753
[24,     1] loss: 1599.366
[25,     1] loss: 1485.062
[26,     1] loss: 1605.275
[27,     1] loss: 1506.719
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032143416768424006,
 'learning_rate_Hydroxylation-K': 0.0008082229851798721,
 'learning_rate_Hydroxylation-P': 0.005562975008941332,
 'log_base': 1.9602936377511138,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3924342538,
 'sample_weights': [4.593116494319803, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.407412064112483,
 'weight_decay_Hydroxylation-K': 6.237772474747297,
 'weight_decay_Hydroxylation-P': 0.5924317090442375}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1447.746
[2,     1] loss: 1429.431
[3,     1] loss: 1434.059
[4,     1] loss: 1430.842
[5,     1] loss: 1432.599
[6,     1] loss: 1426.303
[7,     1] loss: 1430.249
[8,     1] loss: 1426.727
[9,     1] loss: 1426.463
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00743724882852695,
 'learning_rate_Hydroxylation-K': 0.0016835390237017285,
 'learning_rate_Hydroxylation-P': 0.005645170731803319,
 'log_base': 1.113934092140679,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3733345828,
 'sample_weights': [2.480251584321002, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.448625740734518,
 'weight_decay_Hydroxylation-K': 9.306474282585096,
 'weight_decay_Hydroxylation-P': 1.2674078115052692}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5022.527
[2,     1] loss: 5108.487
[3,     1] loss: 5036.908
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005339112756752656,
 'learning_rate_Hydroxylation-K': 0.004452986458065196,
 'learning_rate_Hydroxylation-P': 0.0069926205088200535,
 'log_base': 1.03335019924159,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 965413114,
 'sample_weights': [15.472423130491405, 1.9341273987952379],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.66445768309623,
 'weight_decay_Hydroxylation-K': 3.2589816551857025,
 'weight_decay_Hydroxylation-P': 3.5310418833284034}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16543.189
Exploding loss, terminate run (best metric=0.5326215624809265)
Finished Training
Total time taken: 0.18900012969970703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16649.762
Exploding loss, terminate run (best metric=0.5267451405525208)
Finished Training
Total time taken: 0.2330024242401123
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16435.012
Exploding loss, terminate run (best metric=0.5262007117271423)
Finished Training
Total time taken: 0.19299793243408203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16522.900
Exploding loss, terminate run (best metric=0.5327843427658081)
Finished Training
Total time taken: 0.2349996566772461
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16635.762
Exploding loss, terminate run (best metric=0.5294471979141235)
Finished Training
Total time taken: 0.1940007209777832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16514.484
Exploding loss, terminate run (best metric=0.539628267288208)
Finished Training
Total time taken: 0.2109992504119873
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16498.322
Exploding loss, terminate run (best metric=0.5330610871315002)
Finished Training
Total time taken: 0.20100164413452148
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16543.092
Exploding loss, terminate run (best metric=0.5272902846336365)
Finished Training
Total time taken: 0.1959993839263916
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16559.963
Exploding loss, terminate run (best metric=0.5472754240036011)
Finished Training
Total time taken: 0.2049999237060547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16485.730
Exploding loss, terminate run (best metric=0.5310384035110474)
Finished Training
Total time taken: 0.22200250625610352
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16712.684
Exploding loss, terminate run (best metric=0.5322798490524292)
Finished Training
Total time taken: 0.2010023593902588
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16583.330
Exploding loss, terminate run (best metric=0.5283951759338379)
Finished Training
Total time taken: 0.21500229835510254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16485.566
Exploding loss, terminate run (best metric=0.5252991914749146)
Finished Training
Total time taken: 0.20599746704101562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16514.383
Exploding loss, terminate run (best metric=0.5286158323287964)
Finished Training
Total time taken: 0.2090015411376953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16558.201
Exploding loss, terminate run (best metric=0.5292158722877502)
Finished Training
Total time taken: 0.20100164413452148
{'Hydroxylation-K Validation Accuracy': 0.4097222222222222, 'Hydroxylation-K Validation Sensitivity': 0.66, 'Hydroxylation-K Validation Specificity': 0.34385964912280703, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6905653021442495, 'Hydroxylation-K AUC PR': 0.40757799754358287, 'Hydroxylation-K MCC': 0.004441183064849528, 'Hydroxylation-K F1': 0.2257461605331788, 'Validation Loss (Hydroxylation-K)': 0.5572728753089905, 'Hydroxylation-P Validation Accuracy': 0.3949737915165051, 'Hydroxylation-P Validation Sensitivity': 0.6628571428571428, 'Hydroxylation-P Validation Specificity': 0.33739837398373984, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6156578090028636, 'Hydroxylation-P AUC PR': 0.3049764879432788, 'Hydroxylation-P MCC': 0.00040866180536619745, 'Hydroxylation-P F1': 0.2002701102431953, 'Validation Loss (Hydroxylation-P)': 0.5313265562057495, 'Validation Loss (total)': 1.088599443435669, 'TimeToTrain': 0.2074005921681722}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008208564754979031,
 'learning_rate_Hydroxylation-K': 0.0038737133914609743,
 'learning_rate_Hydroxylation-P': 0.00908734245658151,
 'log_base': 1.892319213005226,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3849490109,
 'sample_weights': [50.92587261561561, 6.35250720623987],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.30101872852105,
 'weight_decay_Hydroxylation-K': 2.6036782344172544,
 'weight_decay_Hydroxylation-P': 2.375578789519896}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1461.879
[2,     1] loss: 1460.593
[3,     1] loss: 1460.622
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0071941870749186575,
 'learning_rate_Hydroxylation-K': 0.005020120490564473,
 'learning_rate_Hydroxylation-P': 0.007221982296072626,
 'log_base': 1.3755710487215058,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3482550545,
 'sample_weights': [2.617489558601015, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.536054738788714,
 'weight_decay_Hydroxylation-K': 3.1513891504404237,
 'weight_decay_Hydroxylation-P': 2.166322908707286}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2035.548
[2,     1] loss: 2009.085
[3,     1] loss: 2021.510
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004398175951513064,
 'learning_rate_Hydroxylation-K': 0.0020154795384870476,
 'learning_rate_Hydroxylation-P': 0.00788612232076356,
 'log_base': 2.032071370808691,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 171619125,
 'sample_weights': [5.235514876525127, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.668855821047304,
 'weight_decay_Hydroxylation-K': 9.539810017267477,
 'weight_decay_Hydroxylation-P': 9.408430612412737}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1413.389
[2,     1] loss: 1401.746
[3,     1] loss: 1404.747
[4,     1] loss: 1406.801
[5,     1] loss: 1400.762
[6,     1] loss: 1405.995
[7,     1] loss: 1394.139
[8,     1] loss: 1392.762
[9,     1] loss: 1389.405
[10,     1] loss: 1361.025
[11,     1] loss: 1335.950
[12,     1] loss: 1306.592
[13,     1] loss: 1281.318
[14,     1] loss: 1256.166
[15,     1] loss: 1247.578
[16,     1] loss: 1216.363
[17,     1] loss: 1235.125
[18,     1] loss: 1171.117
[19,     1] loss: 1145.938
[20,     1] loss: 1135.521
[21,     1] loss: 1102.443
[22,     1] loss: 1144.982
[23,     1] loss: 1125.962
[24,     1] loss: 1104.217
[25,     1] loss: 1128.069
[26,     1] loss: 1063.718
[27,     1] loss: 1069.360
[28,     1] loss: 1051.225
[29,     1] loss: 1017.941
[30,     1] loss: 1014.879
[31,     1] loss: 1027.520
[32,     1] loss: 977.967
[33,     1] loss: 999.258
[34,     1] loss: 994.004
[35,     1] loss: 989.419
[36,     1] loss: 954.344
[37,     1] loss: 972.590
[38,     1] loss: 1017.737
[39,     1] loss: 939.252
[40,     1] loss: 986.945
[41,     1] loss: 945.480
[42,     1] loss: 947.677
[43,     1] loss: 946.943
[44,     1] loss: 891.997
[45,     1] loss: 890.608
[46,     1] loss: 877.761
[47,     1] loss: 871.969
[48,     1] loss: 822.038
[49,     1] loss: 884.437
[50,     1] loss: 957.616
[51,     1] loss: 869.638
[52,     1] loss: 795.607
[53,     1] loss: 837.029
[54,     1] loss: 783.978
[55,     1] loss: 864.630
[56,     1] loss: 752.181
[57,     1] loss: 799.201
[58,     1] loss: 781.147
[59,     1] loss: 713.442
[60,     1] loss: 658.938
[61,     1] loss: 906.463
[62,     1] loss: 987.583
[63,     1] loss: 638.238
[64,     1] loss: 824.531
[65,     1] loss: 692.591
[66,     1] loss: 800.943
[67,     1] loss: 664.920
[68,     1] loss: 795.980
[69,     1] loss: 631.114
[70,     1] loss: 656.781
[71,     1] loss: 643.165
[72,     1] loss: 605.029
[73,     1] loss: 567.745
[74,     1] loss: 629.874
[75,     1] loss: 698.913
[76,     1] loss: 716.889
[77,     1] loss: 616.325
[78,     1] loss: 618.453
[79,     1] loss: 669.820
[80,     1] loss: 550.494
[81,     1] loss: 585.083
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0056380921084618054,
 'learning_rate_Hydroxylation-K': 0.004260298494046761,
 'learning_rate_Hydroxylation-P': 0.007918234865692382,
 'log_base': 1.0255519625752607,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3549142112,
 'sample_weights': [2.3544599658284753, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.33978866405753,
 'weight_decay_Hydroxylation-K': 2.550809907608761,
 'weight_decay_Hydroxylation-P': 5.105705274797014}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21360.262
Exploding loss, terminate run (best metric=0.5323551893234253)
Finished Training
Total time taken: 0.1979973316192627
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21495.277
Exploding loss, terminate run (best metric=0.5355026125907898)
Finished Training
Total time taken: 0.21700215339660645
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21544.523
Exploding loss, terminate run (best metric=0.5273377895355225)
Finished Training
Total time taken: 0.192000150680542
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21430.738
Exploding loss, terminate run (best metric=0.5280406475067139)
Finished Training
Total time taken: 0.21400022506713867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21501.887
Exploding loss, terminate run (best metric=0.5279998183250427)
Finished Training
Total time taken: 0.20400142669677734
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21542.793
Exploding loss, terminate run (best metric=0.5328307747840881)
Finished Training
Total time taken: 0.2180008888244629
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21526.977
Exploding loss, terminate run (best metric=0.5267894268035889)
Finished Training
Total time taken: 0.22999906539916992
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21496.594
Exploding loss, terminate run (best metric=0.5274869203567505)
Finished Training
Total time taken: 0.24900007247924805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21605.574
Exploding loss, terminate run (best metric=0.5265053510665894)
Finished Training
Total time taken: 0.20599913597106934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21485.186
Exploding loss, terminate run (best metric=0.541134774684906)
Finished Training
Total time taken: 0.2050018310546875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21435.770
Exploding loss, terminate run (best metric=0.5313095450401306)
Finished Training
Total time taken: 0.21999907493591309
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21520.742
Exploding loss, terminate run (best metric=0.5265733003616333)
Finished Training
Total time taken: 0.22600030899047852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21506.205
Exploding loss, terminate run (best metric=0.5278342962265015)
Finished Training
Total time taken: 0.21200060844421387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 21457.186
Exploding loss, terminate run (best metric=0.5277444124221802)
Finished Training
Total time taken: 0.2330012321472168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 21541.393
Exploding loss, terminate run (best metric=0.5341047644615173)
Finished Training
Total time taken: 0.2650010585784912
{'Hydroxylation-K Validation Accuracy': 0.3283096926713948, 'Hydroxylation-K Validation Sensitivity': 0.7866666666666666, 'Hydroxylation-K Validation Specificity': 0.21052631578947367, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6948148148148149, 'Hydroxylation-K AUC PR': 0.41593155338595644, 'Hydroxylation-K MCC': -0.00305887645160749, 'Hydroxylation-K F1': 0.26796715927750414, 'Validation Loss (Hydroxylation-K)': 0.5584325551986694, 'Hydroxylation-P Validation Accuracy': 0.3146258734751197, 'Hydroxylation-P Validation Sensitivity': 0.7925925925925926, 'Hydroxylation-P Validation Specificity': 0.21178861788617886, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6431179849448216, 'Hydroxylation-P AUC PR': 0.35187955671598814, 'Hydroxylation-P MCC': 0.004534736813716613, 'Hydroxylation-P F1': 0.24126880034651765, 'Validation Loss (Hydroxylation-P)': 0.5302366415659586, 'Validation Loss (total)': 1.088669204711914, 'TimeToTrain': 0.2192669709523519}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033271284437601436,
 'learning_rate_Hydroxylation-K': 0.0002842107787465181,
 'learning_rate_Hydroxylation-P': 0.004805057948923018,
 'log_base': 1.624800655366836,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4099790380,
 'sample_weights': [66.21551562771627, 8.259741435652403],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.544417685950618,
 'weight_decay_Hydroxylation-K': 6.312718559895426,
 'weight_decay_Hydroxylation-P': 2.4958978005218375}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1630.139
[2,     1] loss: 1629.960
[3,     1] loss: 1646.723
[4,     1] loss: 1631.621
[5,     1] loss: 1637.403
[6,     1] loss: 1632.521
[7,     1] loss: 1627.948
[8,     1] loss: 1631.833
[9,     1] loss: 1628.370
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013124895135883264,
 'learning_rate_Hydroxylation-K': 0.006780457645281022,
 'learning_rate_Hydroxylation-P': 0.008317212389675416,
 'log_base': 1.0471396069756287,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 160748281,
 'sample_weights': [3.439419603722926, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.211666838288494,
 'weight_decay_Hydroxylation-K': 3.365940876082008,
 'weight_decay_Hydroxylation-P': 0.15280417979852667}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11777.209
[2,     1] loss: 11814.580
[3,     1] loss: 11761.510
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002367166762251969,
 'learning_rate_Hydroxylation-K': 0.004970925235382235,
 'learning_rate_Hydroxylation-P': 0.009131837114536518,
 'log_base': 2.8733925346280547,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2101138526,
 'sample_weights': [36.243185611533576, 4.530572730572815],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.188719004161328,
 'weight_decay_Hydroxylation-K': 5.324396866617663,
 'weight_decay_Hydroxylation-P': 5.375018035289016}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.406
[2,     1] loss: 1239.788
[3,     1] loss: 1244.305
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002860601124950576,
 'learning_rate_Hydroxylation-K': 0.0033100151415137863,
 'learning_rate_Hydroxylation-P': 0.004504523488490456,
 'log_base': 2.5698317224969376,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4235921631,
 'sample_weights': [1.5816708548095548, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.275491994147403,
 'weight_decay_Hydroxylation-K': 2.9903429013351115,
 'weight_decay_Hydroxylation-P': 8.605169743400301}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1286.486
[2,     1] loss: 1282.523
[3,     1] loss: 1281.770
[4,     1] loss: 1280.636
[5,     1] loss: 1278.157
[6,     1] loss: 1282.439
[7,     1] loss: 1278.524
[8,     1] loss: 1277.278
[9,     1] loss: 1275.597
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004419799780769987,
 'learning_rate_Hydroxylation-K': 0.0063869555794583074,
 'learning_rate_Hydroxylation-P': 0.007951426105553829,
 'log_base': 1.1090353230547196,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2619222352,
 'sample_weights': [1.7687769177679247, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.3233142372953,
 'weight_decay_Hydroxylation-K': 0.6688043691365343,
 'weight_decay_Hydroxylation-P': 0.2807498949377274}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5240.009
[2,     1] loss: 5226.451
[3,     1] loss: 5239.791
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 1.361058323867746e-05,
 'learning_rate_Hydroxylation-K': 0.006981459222065293,
 'learning_rate_Hydroxylation-P': 0.004214245681505997,
 'log_base': 1.1212905284977497,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2855875454,
 'sample_weights': [16.13135692275669, 2.0164972959253626],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.55769945165729,
 'weight_decay_Hydroxylation-K': 1.7547454608347288,
 'weight_decay_Hydroxylation-P': 0.44995506177421923}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4740.731
[2,     1] loss: 4740.220
[3,     1] loss: 4732.846
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00536469412316907,
 'learning_rate_Hydroxylation-K': 0.00680623764500673,
 'learning_rate_Hydroxylation-P': 0.0014034496697906994,
 'log_base': 2.4030640058425616,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 855643088,
 'sample_weights': [14.582801106992335, 1.822920361881262],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7407705404371705,
 'weight_decay_Hydroxylation-K': 5.052444353262198,
 'weight_decay_Hydroxylation-P': 8.534506472656226}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1309.725
[2,     1] loss: 1312.935
[3,     1] loss: 1311.841
[4,     1] loss: 1306.665
[5,     1] loss: 1305.482
[6,     1] loss: 1312.554
[7,     1] loss: 1302.337
[8,     1] loss: 1305.091
[9,     1] loss: 1297.616
[10,     1] loss: 1291.092
[11,     1] loss: 1280.838
[12,     1] loss: 1248.093
[13,     1] loss: 1235.999
[14,     1] loss: 1205.374
[15,     1] loss: 1171.351
[16,     1] loss: 1156.535
[17,     1] loss: 1079.009
[18,     1] loss: 1128.471
[19,     1] loss: 1064.432
[20,     1] loss: 1181.399
[21,     1] loss: 1103.818
[22,     1] loss: 1035.120
[23,     1] loss: 1051.586
[24,     1] loss: 1077.107
[25,     1] loss: 1077.762
[26,     1] loss: 1012.662
[27,     1] loss: 1022.923
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00992838406280061,
 'learning_rate_Hydroxylation-K': 0.004438423141492382,
 'learning_rate_Hydroxylation-P': 0.004575893387304267,
 'log_base': 2.233499737087315,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1757007184,
 'sample_weights': [1.9041385168094458, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8932501398186425,
 'weight_decay_Hydroxylation-K': 8.908973434701334,
 'weight_decay_Hydroxylation-P': 6.640938570704581}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1352.523
[2,     1] loss: 1353.484
[3,     1] loss: 1353.999
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006946082846011105,
 'learning_rate_Hydroxylation-K': 0.004071084420138988,
 'learning_rate_Hydroxylation-P': 0.0092849992428633,
 'log_base': 1.204978450778802,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2503315567,
 'sample_weights': [2.0775336051263045, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.511942929459423,
 'weight_decay_Hydroxylation-K': 0.556346763344942,
 'weight_decay_Hydroxylation-P': 0.36552068999366155}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2923.123
[2,     1] loss: 2917.383
[3,     1] loss: 2896.068
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006340848102731506,
 'learning_rate_Hydroxylation-K': 0.008291468683790425,
 'learning_rate_Hydroxylation-P': 0.0004947931239729832,
 'log_base': 2.9455365054339757,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1902271663,
 'sample_weights': [8.953277236846047, 1.1192027691297814],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.326725115948566,
 'weight_decay_Hydroxylation-K': 0.27107321792259853,
 'weight_decay_Hydroxylation-P': 9.443295335357213}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.204
[2,     1] loss: 1234.580
[3,     1] loss: 1229.848
[4,     1] loss: 1232.429
[5,     1] loss: 1226.219
[6,     1] loss: 1219.883
[7,     1] loss: 1205.345
[8,     1] loss: 1164.008
[9,     1] loss: 1128.295
[10,     1] loss: 1096.430
[11,     1] loss: 1068.788
[12,     1] loss: 1046.668
[13,     1] loss: 1039.823
[14,     1] loss: 1099.271
[15,     1] loss: 997.654
[16,     1] loss: 1116.962
[17,     1] loss: 1047.874
[18,     1] loss: 1047.334
[19,     1] loss: 1052.654
[20,     1] loss: 1006.198
[21,     1] loss: 983.508
[22,     1] loss: 1006.543
[23,     1] loss: 959.758
[24,     1] loss: 1013.792
[25,     1] loss: 931.107
[26,     1] loss: 962.462
[27,     1] loss: 872.799
[28,     1] loss: 921.908
[29,     1] loss: 936.011
[30,     1] loss: 895.950
[31,     1] loss: 886.532
[32,     1] loss: 852.228
[33,     1] loss: 874.341
[34,     1] loss: 963.067
[35,     1] loss: 892.524
[36,     1] loss: 913.434
[37,     1] loss: 877.029
[38,     1] loss: 874.321
[39,     1] loss: 849.790
[40,     1] loss: 846.174
[41,     1] loss: 872.651
[42,     1] loss: 794.737
[43,     1] loss: 832.738
[44,     1] loss: 756.198
[45,     1] loss: 772.674
[46,     1] loss: 779.526
[47,     1] loss: 752.344
[48,     1] loss: 763.012
[49,     1] loss: 741.860
[50,     1] loss: 713.874
[51,     1] loss: 663.798
[52,     1] loss: 683.601
[53,     1] loss: 886.223
[54,     1] loss: 1166.512
[55,     1] loss: 733.633
[56,     1] loss: 898.541
[57,     1] loss: 859.556
[58,     1] loss: 840.369
[59,     1] loss: 854.101
[60,     1] loss: 855.664
[61,     1] loss: 794.477
[62,     1] loss: 807.457
[63,     1] loss: 760.138
[64,     1] loss: 791.125
[65,     1] loss: 807.118
[66,     1] loss: 737.477
[67,     1] loss: 747.168
[68,     1] loss: 713.753
[69,     1] loss: 738.492
[70,     1] loss: 729.877
[71,     1] loss: 717.853
[72,     1] loss: 667.225
[73,     1] loss: 649.182
[74,     1] loss: 618.685
[75,     1] loss: 609.387
[76,     1] loss: 553.119
[77,     1] loss: 654.914
[78,     1] loss: 1326.430
[79,     1] loss: 1687.709
[80,     1] loss: 1174.263
[81,     1] loss: 1094.142
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004296056461535731,
 'learning_rate_Hydroxylation-K': 0.009444732690040931,
 'learning_rate_Hydroxylation-P': 0.005652701074482357,
 'log_base': 1.6654491919658976,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 260118539,
 'sample_weights': [1.545364337235131, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7786893473791033,
 'weight_decay_Hydroxylation-K': 8.060337101105995,
 'weight_decay_Hydroxylation-P': 4.4898258405735465}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1602.354
[2,     1] loss: 1595.317
[3,     1] loss: 1595.451
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004853330265703046,
 'learning_rate_Hydroxylation-K': 0.003988759280436262,
 'learning_rate_Hydroxylation-P': 0.00708812629478004,
 'log_base': 1.2222563894282645,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 738583815,
 'sample_weights': [3.2728091165481574, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.10298973780236,
 'weight_decay_Hydroxylation-K': 3.07004743094345,
 'weight_decay_Hydroxylation-P': 4.719149260806399}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2723.788
[2,     1] loss: 2729.527
[3,     1] loss: 2698.364
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002702522684702778,
 'learning_rate_Hydroxylation-K': 0.004854591846560511,
 'learning_rate_Hydroxylation-P': 0.009444587335860538,
 'log_base': 1.0397874412467305,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3557176572,
 'sample_weights': [8.31815832811892, 1.0398098471225448],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.906296183928005,
 'weight_decay_Hydroxylation-K': 3.6802917096690124,
 'weight_decay_Hydroxylation-P': 1.1407378725295294}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13841.809
[2,     1] loss: 13923.139
[3,     1] loss: 13886.232
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00549704364505436,
 'learning_rate_Hydroxylation-K': 3.643525628996714e-05,
 'learning_rate_Hydroxylation-P': 0.008657779575764073,
 'log_base': 2.7234421366149384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 220669290,
 'sample_weights': [42.788341509885434, 5.3487487360778125],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7471949191566747,
 'weight_decay_Hydroxylation-K': 3.9551447909599906,
 'weight_decay_Hydroxylation-P': 9.027512662417921}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.499
[2,     1] loss: 1263.118
[3,     1] loss: 1262.227
[4,     1] loss: 1256.765
[5,     1] loss: 1259.871
[6,     1] loss: 1254.630
[7,     1] loss: 1251.631
[8,     1] loss: 1239.603
[9,     1] loss: 1226.814
[10,     1] loss: 1200.664
[11,     1] loss: 1170.713
[12,     1] loss: 1132.834
[13,     1] loss: 1114.719
[14,     1] loss: 1102.158
[15,     1] loss: 1088.573
[16,     1] loss: 1066.094
[17,     1] loss: 1037.683
[18,     1] loss: 1061.058
[19,     1] loss: 1018.563
[20,     1] loss: 1026.900
[21,     1] loss: 1049.478
[22,     1] loss: 1033.522
[23,     1] loss: 1008.728
[24,     1] loss: 986.624
[25,     1] loss: 993.620
[26,     1] loss: 1021.951
[27,     1] loss: 1006.669
[28,     1] loss: 965.009
[29,     1] loss: 943.279
[30,     1] loss: 990.653
[31,     1] loss: 926.111
[32,     1] loss: 947.491
[33,     1] loss: 974.854
[34,     1] loss: 915.619
[35,     1] loss: 953.585
[36,     1] loss: 926.529
[37,     1] loss: 967.422
[38,     1] loss: 908.349
[39,     1] loss: 925.629
[40,     1] loss: 911.746
[41,     1] loss: 866.398
[42,     1] loss: 870.859
[43,     1] loss: 866.460
[44,     1] loss: 894.265
[45,     1] loss: 831.960
[46,     1] loss: 857.111
[47,     1] loss: 817.453
[48,     1] loss: 857.724
[49,     1] loss: 823.913
[50,     1] loss: 837.375
[51,     1] loss: 811.518
[52,     1] loss: 828.532
[53,     1] loss: 823.251
[54,     1] loss: 780.787
[55,     1] loss: 805.955
[56,     1] loss: 723.118
[57,     1] loss: 809.197
[58,     1] loss: 803.243
[59,     1] loss: 762.714
[60,     1] loss: 728.656
[61,     1] loss: 706.539
[62,     1] loss: 720.187
[63,     1] loss: 737.714
[64,     1] loss: 716.616
[65,     1] loss: 675.160
[66,     1] loss: 687.115
[67,     1] loss: 701.082
[68,     1] loss: 666.485
[69,     1] loss: 692.246
[70,     1] loss: 632.447
[71,     1] loss: 732.774
[72,     1] loss: 665.755
[73,     1] loss: 630.762
[74,     1] loss: 636.619
[75,     1] loss: 576.726
[76,     1] loss: 608.395
[77,     1] loss: 627.731
[78,     1] loss: 593.466
[79,     1] loss: 594.559
[80,     1] loss: 572.741
[81,     1] loss: 528.854
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003477536741126723,
 'learning_rate_Hydroxylation-K': 0.002073031972586889,
 'learning_rate_Hydroxylation-P': 0.0002555608560982565,
 'log_base': 1.6823586541071018,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1833984671,
 'sample_weights': [1.666282922470171, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.7044667704281324,
 'weight_decay_Hydroxylation-K': 2.0050577310540056,
 'weight_decay_Hydroxylation-P': 3.184984072455571}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1584.621
[2,     1] loss: 1583.404
[3,     1] loss: 1584.006
[4,     1] loss: 1583.427
[5,     1] loss: 1590.166
[6,     1] loss: 1582.714
[7,     1] loss: 1591.076
[8,     1] loss: 1583.061
[9,     1] loss: 1586.794
[10,     1] loss: 1582.445
[11,     1] loss: 1582.340
[12,     1] loss: 1587.215
[13,     1] loss: 1583.894
[14,     1] loss: 1581.728
[15,     1] loss: 1579.322
[16,     1] loss: 1582.191
[17,     1] loss: 1579.480
[18,     1] loss: 1580.553
[19,     1] loss: 1575.722
[20,     1] loss: 1579.386
[21,     1] loss: 1577.650
[22,     1] loss: 1576.097
[23,     1] loss: 1572.754
[24,     1] loss: 1569.368
[25,     1] loss: 1569.791
[26,     1] loss: 1568.152
[27,     1] loss: 1571.928
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019046829391163346,
 'learning_rate_Hydroxylation-K': 0.002758893488870213,
 'learning_rate_Hydroxylation-P': 0.002439622804285776,
 'log_base': 1.615517048118445,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2932195004,
 'sample_weights': [3.209253200149856, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.19502663012655397,
 'weight_decay_Hydroxylation-K': 2.1740399849279433,
 'weight_decay_Hydroxylation-P': 4.131400104025506}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1649.235
[2,     1] loss: 1648.366
[3,     1] loss: 1639.376
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007412591192095759,
 'learning_rate_Hydroxylation-K': 0.006245641652959666,
 'learning_rate_Hydroxylation-P': 0.007591873057991123,
 'log_base': 1.056231971548593,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3208124955,
 'sample_weights': [3.480507743752813, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.8831375980394975,
 'weight_decay_Hydroxylation-K': 7.3345500397582315,
 'weight_decay_Hydroxylation-P': 8.708173793408836}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9950.148
[2,     1] loss: 9903.182
[3,     1] loss: 9930.684
[4,     1] loss: 9868.477
[5,     1] loss: 9950.051
[6,     1] loss: 9903.888
[7,     1] loss: 9886.371
[8,     1] loss: 9908.093
[9,     1] loss: 9901.151
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004964329544418074,
 'learning_rate_Hydroxylation-K': 0.004512106547491496,
 'learning_rate_Hydroxylation-P': 0.004473265054819256,
 'log_base': 1.095201472331276,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2132356339,
 'sample_weights': [30.515615611055257, 3.814598899388498],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.171769784598323,
 'weight_decay_Hydroxylation-K': 5.401263897935598,
 'weight_decay_Hydroxylation-P': 4.193266261282298}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5966.805
[2,     1] loss: 5940.392
[3,     1] loss: 5957.023
[4,     1] loss: 5965.285
[5,     1] loss: 5967.542
[6,     1] loss: 5944.435
[7,     1] loss: 5922.968
[8,     1] loss: 5951.097
[9,     1] loss: 5881.204
[10,     1] loss: 5920.741
[11,     1] loss: 5905.638
[12,     1] loss: 5852.800
[13,     1] loss: 5746.519
[14,     1] loss: 5659.409
[15,     1] loss: 5503.186
[16,     1] loss: 5387.671
[17,     1] loss: 5284.347
[18,     1] loss: 4961.718
[19,     1] loss: 5179.038
[20,     1] loss: 4876.029
[21,     1] loss: 5096.868
[22,     1] loss: 4985.763
[23,     1] loss: 4774.898
[24,     1] loss: 4875.301
[25,     1] loss: 4829.882
[26,     1] loss: 4424.365
[27,     1] loss: 4322.501
[28,     1] loss: 4414.482
[29,     1] loss: 4220.763
[30,     1] loss: 4353.934
[31,     1] loss: 4878.431
[32,     1] loss: 4153.893
[33,     1] loss: 5447.451
[34,     1] loss: 4250.752
[35,     1] loss: 4571.480
[36,     1] loss: 4570.453
[37,     1] loss: 4261.471
[38,     1] loss: 4645.194
[39,     1] loss: 4252.604
[40,     1] loss: 4155.747
[41,     1] loss: 3951.382
[42,     1] loss: 3630.236
[43,     1] loss: 4183.685
[44,     1] loss: 3352.385
[45,     1] loss: 3874.715
[46,     1] loss: 3585.413
[47,     1] loss: 3914.112
[48,     1] loss: 4142.255
[49,     1] loss: 3882.025
[50,     1] loss: 4168.703
[51,     1] loss: 4048.140
[52,     1] loss: 3593.239
[53,     1] loss: 4052.966
[54,     1] loss: 4126.754
[55,     1] loss: 3373.778
[56,     1] loss: 3737.282
[57,     1] loss: 3693.187
[58,     1] loss: 3249.902
[59,     1] loss: 3536.947
[60,     1] loss: 3115.372
[61,     1] loss: 3405.368
[62,     1] loss: 3052.351
[63,     1] loss: 3300.543
[64,     1] loss: 3218.835
[65,     1] loss: 2862.020
[66,     1] loss: 2903.043
[67,     1] loss: 2953.113
[68,     1] loss: 3030.971
[69,     1] loss: 3454.243
[70,     1] loss: 3788.304
[71,     1] loss: 3000.875
[72,     1] loss: 4202.072
[73,     1] loss: 3817.500
[74,     1] loss: 4880.379
[75,     1] loss: 3578.186
[76,     1] loss: 4325.935
[77,     1] loss: 4272.007
[78,     1] loss: 3720.101
[79,     1] loss: 3923.426
[80,     1] loss: 3941.341
[81,     1] loss: 3569.305
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025376362233597827,
 'learning_rate_Hydroxylation-K': 0.003998777341416183,
 'learning_rate_Hydroxylation-P': 0.00284347524437395,
 'log_base': 1.803469282932613,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3247564965,
 'sample_weights': [18.3579682621442, 2.294834435600056],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.699529325667688,
 'weight_decay_Hydroxylation-K': 8.950539236050687,
 'weight_decay_Hydroxylation-P': 6.062113098449274}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1506.040
[2,     1] loss: 1507.472
[3,     1] loss: 1505.314
[4,     1] loss: 1504.370
[5,     1] loss: 1503.723
[6,     1] loss: 1502.851
[7,     1] loss: 1496.871
[8,     1] loss: 1499.796
[9,     1] loss: 1488.819
[10,     1] loss: 1482.443
[11,     1] loss: 1462.329
[12,     1] loss: 1446.472
[13,     1] loss: 1401.896
[14,     1] loss: 1354.020
[15,     1] loss: 1359.027
[16,     1] loss: 1307.286
[17,     1] loss: 1292.102
[18,     1] loss: 1263.986
[19,     1] loss: 1284.236
[20,     1] loss: 1235.816
[21,     1] loss: 1257.284
[22,     1] loss: 1241.014
[23,     1] loss: 1293.313
[24,     1] loss: 1217.520
[25,     1] loss: 1235.201
[26,     1] loss: 1214.558
[27,     1] loss: 1188.763
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029022887668244315,
 'learning_rate_Hydroxylation-K': 0.008468121142893924,
 'learning_rate_Hydroxylation-P': 0.009037578493120129,
 'log_base': 1.0339157524426594,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 578486966,
 'sample_weights': [2.83094563369119, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.95233271727991,
 'weight_decay_Hydroxylation-K': 1.7158430471400457,
 'weight_decay_Hydroxylation-P': 2.4752433200900645}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16336.967
Exploding loss, terminate run (best metric=0.5327757000923157)
Finished Training
Total time taken: 0.22699904441833496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16316.941
Exploding loss, terminate run (best metric=0.528688371181488)
Finished Training
Total time taken: 0.21799826622009277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16334.462
Exploding loss, terminate run (best metric=0.5298943519592285)
Finished Training
Total time taken: 0.237044095993042
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16231.809
Exploding loss, terminate run (best metric=0.5292730927467346)
Finished Training
Total time taken: 0.2069993019104004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16452.238
Exploding loss, terminate run (best metric=0.5288857221603394)
Finished Training
Total time taken: 0.2200014591217041
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16490.941
Exploding loss, terminate run (best metric=0.5334478616714478)
Finished Training
Total time taken: 0.21300029754638672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16199.967
Exploding loss, terminate run (best metric=0.5324978232383728)
Finished Training
Total time taken: 0.22400331497192383
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16170.949
Exploding loss, terminate run (best metric=0.5267942547798157)
Finished Training
Total time taken: 0.20100021362304688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16330.404
Exploding loss, terminate run (best metric=0.5344534516334534)
Finished Training
Total time taken: 0.24499964714050293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16233.380
Exploding loss, terminate run (best metric=0.5313589572906494)
Finished Training
Total time taken: 0.2313392162322998
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16236.413
Exploding loss, terminate run (best metric=0.5350472927093506)
Finished Training
Total time taken: 0.23400330543518066
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16301.537
Exploding loss, terminate run (best metric=0.5352500677108765)
Finished Training
Total time taken: 0.23000216484069824
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16230.441
Exploding loss, terminate run (best metric=0.526461124420166)
Finished Training
Total time taken: 0.22699999809265137
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16168.486
Exploding loss, terminate run (best metric=0.5411481857299805)
Finished Training
Total time taken: 0.22394108772277832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16421.264
Exploding loss, terminate run (best metric=0.5277823209762573)
Finished Training
Total time taken: 0.23400354385375977
{'Hydroxylation-K Validation Accuracy': 0.5407210401891253, 'Hydroxylation-K Validation Sensitivity': 0.45185185185185184, 'Hydroxylation-K Validation Specificity': 0.5631578947368421, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6273489278752437, 'Hydroxylation-K AUC PR': 0.3616332002696683, 'Hydroxylation-K MCC': 0.012034570542255367, 'Hydroxylation-K F1': 0.16003639107087383, 'Validation Loss (Hydroxylation-K)': 0.55682878891627, 'Hydroxylation-P Validation Accuracy': 0.5416957345650812, 'Hydroxylation-P Validation Sensitivity': 0.44391534391534393, 'Hydroxylation-P Validation Specificity': 0.5621951219512196, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6070277888617331, 'Hydroxylation-P AUC PR': 0.2992999745087619, 'Hydroxylation-P MCC': 0.016312491436898814, 'Hydroxylation-P F1': 0.1468042695654902, 'Validation Loss (Hydroxylation-P)': 0.5315839052200317, 'Validation Loss (total)': 1.0884126822153728, 'TimeToTrain': 0.2248223304748535}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006180528526638332,
 'learning_rate_Hydroxylation-K': 0.008389673303051714,
 'learning_rate_Hydroxylation-P': 0.009629949858953563,
 'log_base': 1.3525320368767306,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2099404702,
 'sample_weights': [50.090448805891235, 6.2482962129085315],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.381469310989637,
 'weight_decay_Hydroxylation-K': 8.213234592201434,
 'weight_decay_Hydroxylation-P': 1.5794263832754023}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2078.269
[2,     1] loss: 2073.754
[3,     1] loss: 2077.677
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033562936722848465,
 'learning_rate_Hydroxylation-K': 0.006154153458899387,
 'learning_rate_Hydroxylation-P': 0.005828573569940293,
 'log_base': 1.3202850079203818,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3303486374,
 'sample_weights': [5.528352506425337, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8613574262765393,
 'weight_decay_Hydroxylation-K': 0.7798343441197968,
 'weight_decay_Hydroxylation-P': 9.62329093271435}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2174.256
[2,     1] loss: 2174.353
[3,     1] loss: 2171.048
[4,     1] loss: 2161.983
[5,     1] loss: 2157.901
[6,     1] loss: 2179.858
[7,     1] loss: 2171.427
[8,     1] loss: 2170.074
[9,     1] loss: 2157.705
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005192075626742548,
 'learning_rate_Hydroxylation-K': 0.0027146339124933727,
 'learning_rate_Hydroxylation-P': 0.006167689278691475,
 'log_base': 1.670895819588369,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1989460661,
 'sample_weights': [6.00848442413698, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.935249547971052,
 'weight_decay_Hydroxylation-K': 7.130424501359716,
 'weight_decay_Hydroxylation-P': 6.916150624063472}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1598.807
[2,     1] loss: 1597.443
[3,     1] loss: 1594.891
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007469438471956402,
 'learning_rate_Hydroxylation-K': 0.001178760407284505,
 'learning_rate_Hydroxylation-P': 0.0028872691729827918,
 'log_base': 1.6530147829090187,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2458104843,
 'sample_weights': [3.251993664623974, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.028813287787557,
 'weight_decay_Hydroxylation-K': 1.53393854168278,
 'weight_decay_Hydroxylation-P': 5.051693829666669}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1615.062
[2,     1] loss: 1621.065
[3,     1] loss: 1605.912
[4,     1] loss: 1608.822
[5,     1] loss: 1606.203
[6,     1] loss: 1610.532
[7,     1] loss: 1614.067
[8,     1] loss: 1605.188
[9,     1] loss: 1602.704
[10,     1] loss: 1608.225
[11,     1] loss: 1604.074
[12,     1] loss: 1605.150
[13,     1] loss: 1606.662
[14,     1] loss: 1601.756
[15,     1] loss: 1601.261
[16,     1] loss: 1595.320
[17,     1] loss: 1580.173
[18,     1] loss: 1557.814
[19,     1] loss: 1521.726
[20,     1] loss: 1487.778
[21,     1] loss: 1418.480
[22,     1] loss: 1359.258
[23,     1] loss: 1401.354
[24,     1] loss: 1360.456
[25,     1] loss: 1315.561
[26,     1] loss: 1345.776
[27,     1] loss: 1353.486
[28,     1] loss: 1300.842
[29,     1] loss: 1327.503
[30,     1] loss: 1293.422
[31,     1] loss: 1309.566
[32,     1] loss: 1267.391
[33,     1] loss: 1260.740
[34,     1] loss: 1225.347
[35,     1] loss: 1250.500
[36,     1] loss: 1263.461
[37,     1] loss: 1251.115
[38,     1] loss: 1105.211
[39,     1] loss: 1244.540
[40,     1] loss: 1189.763
[41,     1] loss: 1117.910
[42,     1] loss: 1139.414
[43,     1] loss: 1085.024
[44,     1] loss: 1028.617
[45,     1] loss: 1043.665
[46,     1] loss: 1214.586
[47,     1] loss: 1253.786
[48,     1] loss: 1042.340
[49,     1] loss: 1084.088
[50,     1] loss: 1015.130
[51,     1] loss: 1101.972
[52,     1] loss: 1016.372
[53,     1] loss: 997.110
[54,     1] loss: 1007.969
[55,     1] loss: 1217.156
[56,     1] loss: 947.295
[57,     1] loss: 905.725
[58,     1] loss: 940.560
[59,     1] loss: 909.873
[60,     1] loss: 1003.525
[61,     1] loss: 833.584
[62,     1] loss: 888.483
[63,     1] loss: 858.187
[64,     1] loss: 923.782
[65,     1] loss: 900.962
[66,     1] loss: 916.347
[67,     1] loss: 820.245
[68,     1] loss: 829.006
[69,     1] loss: 937.916
[70,     1] loss: 800.272
[71,     1] loss: 739.592
[72,     1] loss: 782.184
[73,     1] loss: 769.206
[74,     1] loss: 734.182
[75,     1] loss: 737.942
[76,     1] loss: 737.871
[77,     1] loss: 970.460
[78,     1] loss: 2425.960
[79,     1] loss: 1078.594
[80,     1] loss: 1372.109
[81,     1] loss: 1348.976
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011344947797928924,
 'learning_rate_Hydroxylation-K': 0.0024305538980869696,
 'learning_rate_Hydroxylation-P': 0.00819908826921486,
 'log_base': 2.827677332632262,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 506261760,
 'sample_weights': [3.32160886741829, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.2621556094715745,
 'weight_decay_Hydroxylation-K': 7.61787018010093,
 'weight_decay_Hydroxylation-P': 7.439200220906318}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.902
[2,     1] loss: 1245.487
[3,     1] loss: 1245.609
[4,     1] loss: 1246.080
[5,     1] loss: 1244.478
[6,     1] loss: 1246.248
[7,     1] loss: 1239.351
[8,     1] loss: 1241.261
[9,     1] loss: 1237.619
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001837386736963727,
 'learning_rate_Hydroxylation-K': 0.0008163482369275475,
 'learning_rate_Hydroxylation-P': 0.0055206550665120365,
 'log_base': 1.3182652981647374,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1649203159,
 'sample_weights': [1.6060744456145595, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.076887320593915,
 'weight_decay_Hydroxylation-K': 2.8323767609199417,
 'weight_decay_Hydroxylation-P': 0.45539639171089696}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2191.261
[2,     1] loss: 2182.563
[3,     1] loss: 2179.407
[4,     1] loss: 2174.266
[5,     1] loss: 2185.157
[6,     1] loss: 2181.777
[7,     1] loss: 2179.456
[8,     1] loss: 2177.468
[9,     1] loss: 2169.410
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009231911145018579,
 'learning_rate_Hydroxylation-K': 0.0019313447997229382,
 'learning_rate_Hydroxylation-P': 0.0008748248554897448,
 'log_base': 2.052971861473227,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3732280351,
 'sample_weights': [6.0417742454751595, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.870690965466811,
 'weight_decay_Hydroxylation-K': 8.584395462007944,
 'weight_decay_Hydroxylation-P': 9.169537994602122}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.460
[2,     1] loss: 1397.821
[3,     1] loss: 1398.695
[4,     1] loss: 1396.472
[5,     1] loss: 1394.321
[6,     1] loss: 1394.418
[7,     1] loss: 1396.119
[8,     1] loss: 1394.621
[9,     1] loss: 1394.556
[10,     1] loss: 1393.179
[11,     1] loss: 1392.676
[12,     1] loss: 1388.079
[13,     1] loss: 1385.211
[14,     1] loss: 1380.240
[15,     1] loss: 1368.593
[16,     1] loss: 1360.847
[17,     1] loss: 1352.242
[18,     1] loss: 1346.531
[19,     1] loss: 1324.250
[20,     1] loss: 1303.361
[21,     1] loss: 1285.241
[22,     1] loss: 1285.073
[23,     1] loss: 1245.319
[24,     1] loss: 1241.166
[25,     1] loss: 1235.740
[26,     1] loss: 1201.532
[27,     1] loss: 1189.870
[28,     1] loss: 1159.775
[29,     1] loss: 1149.062
[30,     1] loss: 1145.089
[31,     1] loss: 1190.523
[32,     1] loss: 1150.194
[33,     1] loss: 1151.692
[34,     1] loss: 1108.063
[35,     1] loss: 1104.915
[36,     1] loss: 1124.822
[37,     1] loss: 1100.448
[38,     1] loss: 1086.388
[39,     1] loss: 1078.638
[40,     1] loss: 1076.947
[41,     1] loss: 1154.699
[42,     1] loss: 1115.516
[43,     1] loss: 1087.809
[44,     1] loss: 1086.112
[45,     1] loss: 1071.777
[46,     1] loss: 1078.999
[47,     1] loss: 1085.042
[48,     1] loss: 1059.977
[49,     1] loss: 1070.252
[50,     1] loss: 1108.249
[51,     1] loss: 1064.207
[52,     1] loss: 1050.972
[53,     1] loss: 1040.986
[54,     1] loss: 1018.075
[55,     1] loss: 998.889
[56,     1] loss: 1003.322
[57,     1] loss: 1033.423
[58,     1] loss: 997.032
[59,     1] loss: 1025.458
[60,     1] loss: 1025.021
[61,     1] loss: 960.876
[62,     1] loss: 961.249
[63,     1] loss: 906.452
[64,     1] loss: 950.512
[65,     1] loss: 942.969
[66,     1] loss: 979.040
[67,     1] loss: 954.379
[68,     1] loss: 913.821
[69,     1] loss: 928.983
[70,     1] loss: 925.856
[71,     1] loss: 908.929
[72,     1] loss: 914.721
[73,     1] loss: 882.089
[74,     1] loss: 861.859
[75,     1] loss: 887.670
[76,     1] loss: 891.282
[77,     1] loss: 904.000
[78,     1] loss: 933.707
[79,     1] loss: 895.896
[80,     1] loss: 865.990
[81,     1] loss: 844.116
[82,     1] loss: 862.272
[83,     1] loss: 831.246
[84,     1] loss: 850.933
[85,     1] loss: 773.030
[86,     1] loss: 871.689
[87,     1] loss: 825.705
[88,     1] loss: 839.345
[89,     1] loss: 845.135
[90,     1] loss: 789.734
[91,     1] loss: 772.009
[92,     1] loss: 796.289
[93,     1] loss: 875.595
[94,     1] loss: 779.687
[95,     1] loss: 856.084
[96,     1] loss: 751.348
[97,     1] loss: 771.085
[98,     1] loss: 767.811
[99,     1] loss: 865.755
[100,     1] loss: 740.374
[101,     1] loss: 776.605
[102,     1] loss: 751.437
[103,     1] loss: 777.984
[104,     1] loss: 760.821
[105,     1] loss: 695.638
[106,     1] loss: 731.090
[107,     1] loss: 736.587
[108,     1] loss: 743.915
[109,     1] loss: 675.407
[110,     1] loss: 665.717
[111,     1] loss: 723.117
[112,     1] loss: 673.417
[113,     1] loss: 611.950
[114,     1] loss: 653.922
[115,     1] loss: 621.876
[116,     1] loss: 683.249
[117,     1] loss: 635.066
[118,     1] loss: 635.223
[119,     1] loss: 593.599
[120,     1] loss: 639.146
[121,     1] loss: 599.890
[122,     1] loss: 633.975
[123,     1] loss: 632.982
[124,     1] loss: 589.632
[125,     1] loss: 602.013
[126,     1] loss: 588.459
[127,     1] loss: 533.768
[128,     1] loss: 629.431
[129,     1] loss: 563.755
[130,     1] loss: 557.286
[131,     1] loss: 551.705
[132,     1] loss: 612.855
[133,     1] loss: 613.427
[134,     1] loss: 522.730
[135,     1] loss: 548.480
[136,     1] loss: 503.355
[137,     1] loss: 545.326
[138,     1] loss: 565.320
[139,     1] loss: 511.401
[140,     1] loss: 492.407
[141,     1] loss: 496.489
[142,     1] loss: 584.065
[143,     1] loss: 516.856
[144,     1] loss: 511.894
[145,     1] loss: 454.893
Early stopping applied (best metric=0.3804982006549835)
Finished Training
Total time taken: 21.38612389564514
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.480
[2,     1] loss: 1398.466
[3,     1] loss: 1396.719
[4,     1] loss: 1398.815
[5,     1] loss: 1395.713
[6,     1] loss: 1397.277
[7,     1] loss: 1399.221
[8,     1] loss: 1393.407
[9,     1] loss: 1395.760
[10,     1] loss: 1395.434
[11,     1] loss: 1396.893
[12,     1] loss: 1390.531
[13,     1] loss: 1390.433
[14,     1] loss: 1388.815
[15,     1] loss: 1388.548
[16,     1] loss: 1380.379
[17,     1] loss: 1375.495
[18,     1] loss: 1370.188
[19,     1] loss: 1358.569
[20,     1] loss: 1341.929
[21,     1] loss: 1333.034
[22,     1] loss: 1316.269
[23,     1] loss: 1297.641
[24,     1] loss: 1286.402
[25,     1] loss: 1265.662
[26,     1] loss: 1247.393
[27,     1] loss: 1242.589
[28,     1] loss: 1200.135
[29,     1] loss: 1209.733
[30,     1] loss: 1187.200
[31,     1] loss: 1185.381
[32,     1] loss: 1130.510
[33,     1] loss: 1159.261
[34,     1] loss: 1167.411
[35,     1] loss: 1159.040
[36,     1] loss: 1143.652
[37,     1] loss: 1138.684
[38,     1] loss: 1127.595
[39,     1] loss: 1132.786
[40,     1] loss: 1155.150
[41,     1] loss: 1151.607
[42,     1] loss: 1140.960
[43,     1] loss: 1146.241
[44,     1] loss: 1099.008
[45,     1] loss: 1102.560
[46,     1] loss: 1117.667
[47,     1] loss: 1087.482
[48,     1] loss: 1110.773
[49,     1] loss: 1105.316
[50,     1] loss: 1118.883
[51,     1] loss: 1061.633
[52,     1] loss: 1076.309
[53,     1] loss: 1117.967
[54,     1] loss: 1017.972
[55,     1] loss: 1110.470
[56,     1] loss: 1036.607
[57,     1] loss: 1028.750
[58,     1] loss: 999.350
[59,     1] loss: 1092.465
[60,     1] loss: 1018.816
[61,     1] loss: 981.030
[62,     1] loss: 977.590
[63,     1] loss: 1008.177
[64,     1] loss: 1068.333
[65,     1] loss: 1031.682
[66,     1] loss: 1041.154
[67,     1] loss: 995.419
[68,     1] loss: 983.022
[69,     1] loss: 1018.494
[70,     1] loss: 1006.854
[71,     1] loss: 971.091
[72,     1] loss: 972.920
[73,     1] loss: 993.454
[74,     1] loss: 965.573
[75,     1] loss: 973.879
[76,     1] loss: 922.511
[77,     1] loss: 889.035
[78,     1] loss: 891.521
[79,     1] loss: 907.446
[80,     1] loss: 902.880
[81,     1] loss: 925.239
[82,     1] loss: 938.206
[83,     1] loss: 946.401
[84,     1] loss: 904.740
[85,     1] loss: 906.230
[86,     1] loss: 878.933
[87,     1] loss: 900.479
[88,     1] loss: 875.300
[89,     1] loss: 914.400
[90,     1] loss: 845.542
[91,     1] loss: 844.942
[92,     1] loss: 857.821
[93,     1] loss: 899.477
[94,     1] loss: 843.478
[95,     1] loss: 890.215
[96,     1] loss: 851.608
[97,     1] loss: 892.491
[98,     1] loss: 762.354
[99,     1] loss: 818.553
[100,     1] loss: 815.463
[101,     1] loss: 762.645
[102,     1] loss: 832.533
[103,     1] loss: 775.789
[104,     1] loss: 773.891
[105,     1] loss: 734.995
[106,     1] loss: 754.385
[107,     1] loss: 729.185
[108,     1] loss: 787.313
[109,     1] loss: 736.084
[110,     1] loss: 742.959
[111,     1] loss: 734.948
[112,     1] loss: 788.155
[113,     1] loss: 718.617
[114,     1] loss: 739.757
[115,     1] loss: 707.201
[116,     1] loss: 685.362
Early stopping applied (best metric=0.39305099844932556)
Finished Training
Total time taken: 17.110651969909668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.836
[2,     1] loss: 1398.656
[3,     1] loss: 1399.177
[4,     1] loss: 1395.783
[5,     1] loss: 1397.856
[6,     1] loss: 1396.621
[7,     1] loss: 1395.181
[8,     1] loss: 1394.585
[9,     1] loss: 1395.212
[10,     1] loss: 1393.880
[11,     1] loss: 1394.272
[12,     1] loss: 1391.045
[13,     1] loss: 1391.321
[14,     1] loss: 1389.845
[15,     1] loss: 1386.861
[16,     1] loss: 1381.953
[17,     1] loss: 1379.310
[18,     1] loss: 1373.644
[19,     1] loss: 1357.426
[20,     1] loss: 1355.270
[21,     1] loss: 1336.017
[22,     1] loss: 1326.101
[23,     1] loss: 1308.934
[24,     1] loss: 1292.831
[25,     1] loss: 1273.192
[26,     1] loss: 1261.247
[27,     1] loss: 1253.349
[28,     1] loss: 1227.266
[29,     1] loss: 1190.842
[30,     1] loss: 1201.201
[31,     1] loss: 1189.421
[32,     1] loss: 1159.146
[33,     1] loss: 1156.584
[34,     1] loss: 1149.396
[35,     1] loss: 1125.389
[36,     1] loss: 1160.034
[37,     1] loss: 1148.486
[38,     1] loss: 1127.961
[39,     1] loss: 1140.454
[40,     1] loss: 1152.983
[41,     1] loss: 1106.927
[42,     1] loss: 1128.500
[43,     1] loss: 1118.976
[44,     1] loss: 1081.866
[45,     1] loss: 1066.732
[46,     1] loss: 1080.535
[47,     1] loss: 1102.894
[48,     1] loss: 1079.301
[49,     1] loss: 1107.083
[50,     1] loss: 1061.841
[51,     1] loss: 1032.416
[52,     1] loss: 1066.941
[53,     1] loss: 989.159
[54,     1] loss: 1032.661
[55,     1] loss: 1047.047
[56,     1] loss: 1039.092
[57,     1] loss: 1034.184
[58,     1] loss: 960.036
[59,     1] loss: 951.263
[60,     1] loss: 958.493
[61,     1] loss: 963.279
[62,     1] loss: 1011.610
[63,     1] loss: 996.924
[64,     1] loss: 984.428
[65,     1] loss: 964.328
[66,     1] loss: 908.974
[67,     1] loss: 879.026
[68,     1] loss: 917.523
[69,     1] loss: 941.117
[70,     1] loss: 880.294
[71,     1] loss: 926.147
[72,     1] loss: 870.892
[73,     1] loss: 943.096
[74,     1] loss: 872.411
[75,     1] loss: 851.307
[76,     1] loss: 856.249
[77,     1] loss: 879.262
[78,     1] loss: 910.557
[79,     1] loss: 794.454
[80,     1] loss: 881.473
[81,     1] loss: 797.702
[82,     1] loss: 848.718
[83,     1] loss: 803.905
[84,     1] loss: 793.129
[85,     1] loss: 815.163
[86,     1] loss: 769.756
[87,     1] loss: 798.104
[88,     1] loss: 769.168
[89,     1] loss: 733.648
[90,     1] loss: 753.254
[91,     1] loss: 801.658
[92,     1] loss: 739.137
[93,     1] loss: 748.102
[94,     1] loss: 756.352
[95,     1] loss: 737.968
[96,     1] loss: 687.976
[97,     1] loss: 737.192
[98,     1] loss: 718.186
[99,     1] loss: 663.861
[100,     1] loss: 684.637
[101,     1] loss: 654.839
[102,     1] loss: 661.151
Early stopping applied (best metric=0.37644901871681213)
Finished Training
Total time taken: 14.621067762374878
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.209
[2,     1] loss: 1400.031
[3,     1] loss: 1396.055
[4,     1] loss: 1402.094
[5,     1] loss: 1400.998
[6,     1] loss: 1398.172
[7,     1] loss: 1398.668
[8,     1] loss: 1395.840
[9,     1] loss: 1393.510
[10,     1] loss: 1395.576
[11,     1] loss: 1397.497
[12,     1] loss: 1388.602
[13,     1] loss: 1392.073
[14,     1] loss: 1388.134
[15,     1] loss: 1385.364
[16,     1] loss: 1387.786
[17,     1] loss: 1380.543
[18,     1] loss: 1373.678
[19,     1] loss: 1363.411
[20,     1] loss: 1349.401
[21,     1] loss: 1342.044
[22,     1] loss: 1317.286
[23,     1] loss: 1311.953
[24,     1] loss: 1291.901
[25,     1] loss: 1277.190
[26,     1] loss: 1270.817
[27,     1] loss: 1243.719
[28,     1] loss: 1219.779
[29,     1] loss: 1219.619
[30,     1] loss: 1195.563
[31,     1] loss: 1198.233
[32,     1] loss: 1141.278
[33,     1] loss: 1132.250
[34,     1] loss: 1131.838
[35,     1] loss: 1145.916
[36,     1] loss: 1132.127
[37,     1] loss: 1157.816
[38,     1] loss: 1104.798
[39,     1] loss: 1153.123
[40,     1] loss: 1134.724
[41,     1] loss: 1137.645
[42,     1] loss: 1113.070
[43,     1] loss: 1131.427
[44,     1] loss: 1101.787
[45,     1] loss: 1086.552
[46,     1] loss: 1099.571
[47,     1] loss: 1033.763
[48,     1] loss: 1084.781
[49,     1] loss: 1095.589
[50,     1] loss: 1051.237
[51,     1] loss: 1058.896
[52,     1] loss: 1077.139
[53,     1] loss: 1050.643
[54,     1] loss: 1031.513
[55,     1] loss: 1007.975
[56,     1] loss: 1061.662
[57,     1] loss: 1051.076
[58,     1] loss: 994.244
[59,     1] loss: 998.022
[60,     1] loss: 996.817
[61,     1] loss: 980.223
[62,     1] loss: 969.885
[63,     1] loss: 1020.896
[64,     1] loss: 1000.774
[65,     1] loss: 971.389
[66,     1] loss: 969.688
[67,     1] loss: 993.824
[68,     1] loss: 995.880
[69,     1] loss: 971.063
[70,     1] loss: 1000.594
[71,     1] loss: 941.158
[72,     1] loss: 905.900
[73,     1] loss: 972.013
[74,     1] loss: 925.028
[75,     1] loss: 924.461
[76,     1] loss: 947.906
[77,     1] loss: 954.342
[78,     1] loss: 924.927
[79,     1] loss: 889.373
[80,     1] loss: 943.851
[81,     1] loss: 845.328
[82,     1] loss: 834.194
[83,     1] loss: 896.140
[84,     1] loss: 863.273
[85,     1] loss: 824.427
[86,     1] loss: 806.658
[87,     1] loss: 842.298
[88,     1] loss: 821.921
[89,     1] loss: 892.972
[90,     1] loss: 808.782
[91,     1] loss: 751.236
[92,     1] loss: 746.398
[93,     1] loss: 783.723
[94,     1] loss: 797.184
[95,     1] loss: 766.033
[96,     1] loss: 669.553
[97,     1] loss: 781.838
[98,     1] loss: 783.105
[99,     1] loss: 681.149
[100,     1] loss: 728.556
[101,     1] loss: 690.173
[102,     1] loss: 670.484
[103,     1] loss: 719.350
[104,     1] loss: 692.841
[105,     1] loss: 694.140
[106,     1] loss: 647.614
[107,     1] loss: 644.156
[108,     1] loss: 622.924
[109,     1] loss: 649.450
[110,     1] loss: 726.383
[111,     1] loss: 577.131
[112,     1] loss: 681.992
[113,     1] loss: 709.014
[114,     1] loss: 634.493
[115,     1] loss: 622.995
[116,     1] loss: 630.613
[117,     1] loss: 615.492
[118,     1] loss: 626.551
[119,     1] loss: 670.995
[120,     1] loss: 625.977
[121,     1] loss: 578.649
[122,     1] loss: 581.249
[123,     1] loss: 592.073
[124,     1] loss: 623.140
[125,     1] loss: 546.588
Early stopping applied (best metric=0.39376458525657654)
Finished Training
Total time taken: 19.4591281414032
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1407.945
[2,     1] loss: 1400.733
[3,     1] loss: 1399.799
[4,     1] loss: 1400.617
[5,     1] loss: 1398.992
[6,     1] loss: 1397.619
[7,     1] loss: 1401.180
[8,     1] loss: 1392.792
[9,     1] loss: 1393.354
[10,     1] loss: 1387.171
[11,     1] loss: 1387.860
[12,     1] loss: 1380.902
[13,     1] loss: 1373.708
[14,     1] loss: 1361.478
[15,     1] loss: 1348.500
[16,     1] loss: 1338.136
[17,     1] loss: 1331.671
[18,     1] loss: 1305.478
[19,     1] loss: 1284.317
[20,     1] loss: 1264.077
[21,     1] loss: 1268.949
[22,     1] loss: 1243.115
[23,     1] loss: 1231.515
[24,     1] loss: 1201.753
[25,     1] loss: 1210.292
[26,     1] loss: 1175.763
[27,     1] loss: 1185.367
[28,     1] loss: 1164.095
[29,     1] loss: 1183.759
[30,     1] loss: 1103.922
[31,     1] loss: 1159.109
[32,     1] loss: 1115.660
[33,     1] loss: 1201.102
[34,     1] loss: 1151.466
[35,     1] loss: 1106.719
[36,     1] loss: 1092.812
[37,     1] loss: 1173.315
[38,     1] loss: 1102.409
[39,     1] loss: 1088.962
[40,     1] loss: 1126.163
[41,     1] loss: 1069.048
[42,     1] loss: 1090.039
[43,     1] loss: 1052.614
[44,     1] loss: 1101.863
[45,     1] loss: 1041.498
[46,     1] loss: 1113.505
[47,     1] loss: 1034.697
[48,     1] loss: 1008.422
[49,     1] loss: 1037.114
[50,     1] loss: 1029.917
[51,     1] loss: 1036.534
[52,     1] loss: 1007.058
[53,     1] loss: 1005.023
[54,     1] loss: 1013.646
[55,     1] loss: 985.704
[56,     1] loss: 979.770
[57,     1] loss: 969.604
[58,     1] loss: 949.137
[59,     1] loss: 1002.859
[60,     1] loss: 949.463
[61,     1] loss: 1007.235
[62,     1] loss: 905.261
[63,     1] loss: 886.953
[64,     1] loss: 940.651
[65,     1] loss: 868.233
[66,     1] loss: 892.460
[67,     1] loss: 883.908
[68,     1] loss: 847.418
[69,     1] loss: 870.974
[70,     1] loss: 856.268
[71,     1] loss: 916.848
[72,     1] loss: 838.993
[73,     1] loss: 880.071
[74,     1] loss: 902.052
[75,     1] loss: 815.821
[76,     1] loss: 896.198
[77,     1] loss: 867.481
[78,     1] loss: 818.185
[79,     1] loss: 831.608
[80,     1] loss: 803.813
[81,     1] loss: 836.546
[82,     1] loss: 785.863
[83,     1] loss: 807.378
[84,     1] loss: 769.107
[85,     1] loss: 809.177
[86,     1] loss: 780.741
[87,     1] loss: 729.357
Early stopping applied (best metric=0.43427351117134094)
Finished Training
Total time taken: 14.894195318222046
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.546
[2,     1] loss: 1397.367
[3,     1] loss: 1398.940
[4,     1] loss: 1395.465
[5,     1] loss: 1394.140
[6,     1] loss: 1394.230
[7,     1] loss: 1394.051
[8,     1] loss: 1396.132
[9,     1] loss: 1391.906
[10,     1] loss: 1391.214
[11,     1] loss: 1389.867
[12,     1] loss: 1387.105
[13,     1] loss: 1382.607
[14,     1] loss: 1377.019
[15,     1] loss: 1369.042
[16,     1] loss: 1360.188
[17,     1] loss: 1343.291
[18,     1] loss: 1329.581
[19,     1] loss: 1312.290
[20,     1] loss: 1297.057
[21,     1] loss: 1277.610
[22,     1] loss: 1263.087
[23,     1] loss: 1253.112
[24,     1] loss: 1246.722
[25,     1] loss: 1211.216
[26,     1] loss: 1196.172
[27,     1] loss: 1186.745
[28,     1] loss: 1191.648
[29,     1] loss: 1171.390
[30,     1] loss: 1164.481
[31,     1] loss: 1201.064
[32,     1] loss: 1174.671
[33,     1] loss: 1152.059
[34,     1] loss: 1145.409
[35,     1] loss: 1155.903
[36,     1] loss: 1132.278
[37,     1] loss: 1138.250
[38,     1] loss: 1134.254
[39,     1] loss: 1208.825
[40,     1] loss: 1116.364
[41,     1] loss: 1103.238
[42,     1] loss: 1109.668
[43,     1] loss: 1131.860
[44,     1] loss: 1108.504
[45,     1] loss: 1090.482
[46,     1] loss: 1103.336
[47,     1] loss: 1077.707
[48,     1] loss: 1132.231
[49,     1] loss: 1090.452
[50,     1] loss: 1054.387
[51,     1] loss: 1036.221
[52,     1] loss: 1075.218
[53,     1] loss: 1042.757
[54,     1] loss: 1034.095
[55,     1] loss: 1059.337
[56,     1] loss: 1096.036
[57,     1] loss: 987.394
[58,     1] loss: 981.255
[59,     1] loss: 1058.962
[60,     1] loss: 1059.427
[61,     1] loss: 995.270
[62,     1] loss: 1022.428
[63,     1] loss: 973.940
[64,     1] loss: 1001.565
[65,     1] loss: 1017.312
[66,     1] loss: 962.581
[67,     1] loss: 964.631
[68,     1] loss: 929.169
[69,     1] loss: 995.026
[70,     1] loss: 985.119
[71,     1] loss: 927.889
[72,     1] loss: 969.956
[73,     1] loss: 964.805
[74,     1] loss: 945.998
[75,     1] loss: 991.120
[76,     1] loss: 925.430
[77,     1] loss: 929.331
[78,     1] loss: 947.375
[79,     1] loss: 951.215
[80,     1] loss: 874.576
[81,     1] loss: 880.932
[82,     1] loss: 859.104
[83,     1] loss: 843.238
[84,     1] loss: 835.139
[85,     1] loss: 871.084
[86,     1] loss: 840.243
[87,     1] loss: 857.045
[88,     1] loss: 821.178
[89,     1] loss: 815.060
[90,     1] loss: 820.307
[91,     1] loss: 793.478
[92,     1] loss: 840.630
[93,     1] loss: 786.959
[94,     1] loss: 821.190
[95,     1] loss: 763.537
[96,     1] loss: 858.874
[97,     1] loss: 752.704
[98,     1] loss: 755.619
[99,     1] loss: 842.199
[100,     1] loss: 701.644
[101,     1] loss: 754.594
[102,     1] loss: 758.876
[103,     1] loss: 757.712
[104,     1] loss: 720.703
[105,     1] loss: 823.379
[106,     1] loss: 711.389
[107,     1] loss: 829.601
[108,     1] loss: 680.189
[109,     1] loss: 767.399
[110,     1] loss: 714.247
[111,     1] loss: 681.631
[112,     1] loss: 693.998
[113,     1] loss: 668.500
[114,     1] loss: 708.460
[115,     1] loss: 681.635
[116,     1] loss: 706.838
[117,     1] loss: 683.351
[118,     1] loss: 707.974
[119,     1] loss: 616.897
[120,     1] loss: 658.796
[121,     1] loss: 654.237
Early stopping applied (best metric=0.3549497127532959)
Finished Training
Total time taken: 19.844898462295532
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1409.083
[2,     1] loss: 1398.830
[3,     1] loss: 1395.392
[4,     1] loss: 1403.309
[5,     1] loss: 1401.896
[6,     1] loss: 1396.474
[7,     1] loss: 1395.474
[8,     1] loss: 1394.148
[9,     1] loss: 1394.557
[10,     1] loss: 1395.948
[11,     1] loss: 1390.079
[12,     1] loss: 1386.221
[13,     1] loss: 1383.929
[14,     1] loss: 1381.923
[15,     1] loss: 1371.455
[16,     1] loss: 1356.208
[17,     1] loss: 1342.893
[18,     1] loss: 1340.065
[19,     1] loss: 1325.557
[20,     1] loss: 1290.788
[21,     1] loss: 1285.446
[22,     1] loss: 1279.829
[23,     1] loss: 1257.444
[24,     1] loss: 1239.197
[25,     1] loss: 1234.865
[26,     1] loss: 1224.541
[27,     1] loss: 1180.953
[28,     1] loss: 1179.264
[29,     1] loss: 1212.169
[30,     1] loss: 1184.893
[31,     1] loss: 1208.339
[32,     1] loss: 1168.530
[33,     1] loss: 1186.699
[34,     1] loss: 1144.078
[35,     1] loss: 1149.307
[36,     1] loss: 1130.482
[37,     1] loss: 1171.290
[38,     1] loss: 1163.256
[39,     1] loss: 1131.875
[40,     1] loss: 1125.646
[41,     1] loss: 1142.858
[42,     1] loss: 1117.968
[43,     1] loss: 1178.885
[44,     1] loss: 1094.310
[45,     1] loss: 1118.593
[46,     1] loss: 1101.973
[47,     1] loss: 1085.492
[48,     1] loss: 1121.658
[49,     1] loss: 1103.864
[50,     1] loss: 1128.231
[51,     1] loss: 1067.958
[52,     1] loss: 1120.688
[53,     1] loss: 1038.970
[54,     1] loss: 1069.667
[55,     1] loss: 1070.153
[56,     1] loss: 1032.792
[57,     1] loss: 1079.305
[58,     1] loss: 1032.765
[59,     1] loss: 1023.974
[60,     1] loss: 1021.302
[61,     1] loss: 1022.884
[62,     1] loss: 1021.232
[63,     1] loss: 1037.854
[64,     1] loss: 973.403
[65,     1] loss: 955.822
[66,     1] loss: 1044.369
[67,     1] loss: 988.148
[68,     1] loss: 985.806
[69,     1] loss: 1007.077
[70,     1] loss: 977.274
[71,     1] loss: 903.619
[72,     1] loss: 1013.312
[73,     1] loss: 936.458
[74,     1] loss: 987.570
[75,     1] loss: 919.034
[76,     1] loss: 961.067
[77,     1] loss: 907.111
[78,     1] loss: 896.579
[79,     1] loss: 918.929
[80,     1] loss: 897.473
[81,     1] loss: 999.231
[82,     1] loss: 845.591
[83,     1] loss: 1043.705
[84,     1] loss: 862.917
[85,     1] loss: 899.967
[86,     1] loss: 879.378
[87,     1] loss: 858.370
[88,     1] loss: 894.011
[89,     1] loss: 831.281
[90,     1] loss: 878.348
[91,     1] loss: 791.393
[92,     1] loss: 873.431
[93,     1] loss: 876.695
[94,     1] loss: 848.880
[95,     1] loss: 839.077
[96,     1] loss: 848.119
[97,     1] loss: 827.539
[98,     1] loss: 838.008
[99,     1] loss: 803.213
[100,     1] loss: 807.787
[101,     1] loss: 800.225
[102,     1] loss: 823.169
[103,     1] loss: 806.597
[104,     1] loss: 748.649
[105,     1] loss: 766.812
[106,     1] loss: 796.018
[107,     1] loss: 710.323
[108,     1] loss: 781.710
[109,     1] loss: 723.353
[110,     1] loss: 799.633
[111,     1] loss: 749.806
[112,     1] loss: 756.327
[113,     1] loss: 729.188
Early stopping applied (best metric=0.3274068832397461)
Finished Training
Total time taken: 19.674614667892456
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1397.406
[2,     1] loss: 1397.093
[3,     1] loss: 1393.881
[4,     1] loss: 1398.870
[5,     1] loss: 1394.488
[6,     1] loss: 1395.239
[7,     1] loss: 1397.235
[8,     1] loss: 1392.655
[9,     1] loss: 1390.171
[10,     1] loss: 1393.168
[11,     1] loss: 1387.942
[12,     1] loss: 1383.247
[13,     1] loss: 1377.682
[14,     1] loss: 1367.704
[15,     1] loss: 1356.754
[16,     1] loss: 1343.987
[17,     1] loss: 1346.058
[18,     1] loss: 1322.992
[19,     1] loss: 1313.881
[20,     1] loss: 1293.499
[21,     1] loss: 1269.547
[22,     1] loss: 1285.255
[23,     1] loss: 1234.463
[24,     1] loss: 1246.508
[25,     1] loss: 1257.009
[26,     1] loss: 1233.193
[27,     1] loss: 1210.193
[28,     1] loss: 1226.495
[29,     1] loss: 1185.324
[30,     1] loss: 1180.587
[31,     1] loss: 1189.561
[32,     1] loss: 1173.492
[33,     1] loss: 1123.691
[34,     1] loss: 1162.546
[35,     1] loss: 1138.466
[36,     1] loss: 1130.336
[37,     1] loss: 1116.986
[38,     1] loss: 1126.703
[39,     1] loss: 1124.008
[40,     1] loss: 1109.751
[41,     1] loss: 1097.851
[42,     1] loss: 1087.965
[43,     1] loss: 1139.518
[44,     1] loss: 1131.375
[45,     1] loss: 1073.371
[46,     1] loss: 1103.549
[47,     1] loss: 1054.932
[48,     1] loss: 1095.279
[49,     1] loss: 1073.365
[50,     1] loss: 1081.537
[51,     1] loss: 1039.590
[52,     1] loss: 1049.776
[53,     1] loss: 1046.842
[54,     1] loss: 1084.678
[55,     1] loss: 1049.661
[56,     1] loss: 1046.680
[57,     1] loss: 1043.625
[58,     1] loss: 1004.481
[59,     1] loss: 1021.584
[60,     1] loss: 1080.048
[61,     1] loss: 1024.865
[62,     1] loss: 994.872
[63,     1] loss: 934.879
[64,     1] loss: 991.348
[65,     1] loss: 977.561
[66,     1] loss: 999.342
[67,     1] loss: 1015.547
[68,     1] loss: 992.871
[69,     1] loss: 973.002
[70,     1] loss: 940.895
[71,     1] loss: 919.091
[72,     1] loss: 1018.960
[73,     1] loss: 910.523
[74,     1] loss: 974.554
[75,     1] loss: 877.287
[76,     1] loss: 962.689
[77,     1] loss: 895.270
[78,     1] loss: 914.209
[79,     1] loss: 917.943
[80,     1] loss: 929.356
[81,     1] loss: 946.377
[82,     1] loss: 826.623
[83,     1] loss: 927.984
[84,     1] loss: 897.485
[85,     1] loss: 881.543
[86,     1] loss: 874.492
[87,     1] loss: 844.968
[88,     1] loss: 838.725
[89,     1] loss: 863.761
[90,     1] loss: 868.659
[91,     1] loss: 817.334
[92,     1] loss: 809.059
[93,     1] loss: 768.599
[94,     1] loss: 780.021
[95,     1] loss: 817.344
[96,     1] loss: 796.589
[97,     1] loss: 761.567
[98,     1] loss: 728.733
[99,     1] loss: 738.009
[100,     1] loss: 728.015
[101,     1] loss: 710.919
[102,     1] loss: 698.553
[103,     1] loss: 747.593
[104,     1] loss: 666.228
[105,     1] loss: 708.037
[106,     1] loss: 695.586
[107,     1] loss: 675.650
[108,     1] loss: 688.041
[109,     1] loss: 775.148
[110,     1] loss: 680.418
[111,     1] loss: 655.597
[112,     1] loss: 647.463
[113,     1] loss: 671.620
[114,     1] loss: 627.839
[115,     1] loss: 690.955
[116,     1] loss: 667.227
[117,     1] loss: 668.383
[118,     1] loss: 650.155
[119,     1] loss: 653.156
[120,     1] loss: 667.377
[121,     1] loss: 565.508
[122,     1] loss: 743.348
[123,     1] loss: 607.709
[124,     1] loss: 759.393
[125,     1] loss: 668.316
[126,     1] loss: 634.885
[127,     1] loss: 620.286
[128,     1] loss: 688.219
[129,     1] loss: 599.288
[130,     1] loss: 645.709
[131,     1] loss: 566.424
[132,     1] loss: 554.999
[133,     1] loss: 595.436
[134,     1] loss: 599.243
Early stopping applied (best metric=0.36464840173721313)
Finished Training
Total time taken: 23.174631595611572
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1410.046
[2,     1] loss: 1398.658
[3,     1] loss: 1393.981
[4,     1] loss: 1396.489
[5,     1] loss: 1403.447
[6,     1] loss: 1400.460
[7,     1] loss: 1393.977
[8,     1] loss: 1397.388
[9,     1] loss: 1396.945
[10,     1] loss: 1400.656
[11,     1] loss: 1396.364
[12,     1] loss: 1392.706
[13,     1] loss: 1395.583
[14,     1] loss: 1394.714
[15,     1] loss: 1386.823
[16,     1] loss: 1389.100
[17,     1] loss: 1385.939
[18,     1] loss: 1378.033
[19,     1] loss: 1372.569
[20,     1] loss: 1360.810
[21,     1] loss: 1354.549
[22,     1] loss: 1338.435
[23,     1] loss: 1322.933
[24,     1] loss: 1317.405
[25,     1] loss: 1291.940
[26,     1] loss: 1287.611
[27,     1] loss: 1261.210
[28,     1] loss: 1255.684
[29,     1] loss: 1234.261
[30,     1] loss: 1238.154
[31,     1] loss: 1232.913
[32,     1] loss: 1189.964
[33,     1] loss: 1183.093
[34,     1] loss: 1158.041
[35,     1] loss: 1217.793
[36,     1] loss: 1160.844
[37,     1] loss: 1183.959
[38,     1] loss: 1144.365
[39,     1] loss: 1153.271
[40,     1] loss: 1113.454
[41,     1] loss: 1147.208
[42,     1] loss: 1147.809
[43,     1] loss: 1114.286
[44,     1] loss: 1108.725
[45,     1] loss: 1118.611
[46,     1] loss: 1092.109
[47,     1] loss: 1108.547
[48,     1] loss: 1089.567
[49,     1] loss: 1075.859
[50,     1] loss: 1082.498
[51,     1] loss: 1079.980
[52,     1] loss: 1133.599
[53,     1] loss: 1061.069
[54,     1] loss: 993.575
[55,     1] loss: 1006.873
[56,     1] loss: 1017.017
[57,     1] loss: 1017.530
[58,     1] loss: 993.396
[59,     1] loss: 1013.899
[60,     1] loss: 1019.873
[61,     1] loss: 1020.728
[62,     1] loss: 954.588
[63,     1] loss: 975.195
[64,     1] loss: 924.775
[65,     1] loss: 917.987
[66,     1] loss: 928.165
[67,     1] loss: 999.454
[68,     1] loss: 917.375
[69,     1] loss: 909.865
[70,     1] loss: 914.511
[71,     1] loss: 932.283
[72,     1] loss: 920.753
[73,     1] loss: 919.304
[74,     1] loss: 902.749
[75,     1] loss: 872.805
[76,     1] loss: 910.700
[77,     1] loss: 874.570
[78,     1] loss: 864.105
[79,     1] loss: 845.073
[80,     1] loss: 876.919
[81,     1] loss: 878.418
[82,     1] loss: 843.010
[83,     1] loss: 796.016
[84,     1] loss: 820.126
[85,     1] loss: 855.093
[86,     1] loss: 806.450
[87,     1] loss: 761.220
[88,     1] loss: 738.210
[89,     1] loss: 732.022
[90,     1] loss: 747.870
[91,     1] loss: 742.109
[92,     1] loss: 730.619
[93,     1] loss: 744.467
[94,     1] loss: 716.517
[95,     1] loss: 801.921
[96,     1] loss: 749.738
[97,     1] loss: 795.504
[98,     1] loss: 734.686
[99,     1] loss: 732.430
[100,     1] loss: 749.000
[101,     1] loss: 692.054
[102,     1] loss: 763.992
[103,     1] loss: 659.252
[104,     1] loss: 750.759
[105,     1] loss: 702.354
[106,     1] loss: 685.442
[107,     1] loss: 663.918
[108,     1] loss: 676.846
Early stopping applied (best metric=0.3931337296962738)
Finished Training
Total time taken: 20.122125148773193
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1406.262
[2,     1] loss: 1398.041
[3,     1] loss: 1398.853
[4,     1] loss: 1398.020
[5,     1] loss: 1399.151
[6,     1] loss: 1395.917
[7,     1] loss: 1395.641
[8,     1] loss: 1396.237
[9,     1] loss: 1396.935
[10,     1] loss: 1390.801
[11,     1] loss: 1391.770
[12,     1] loss: 1392.801
[13,     1] loss: 1389.106
[14,     1] loss: 1380.673
[15,     1] loss: 1376.896
[16,     1] loss: 1368.679
[17,     1] loss: 1348.880
[18,     1] loss: 1335.567
[19,     1] loss: 1324.872
[20,     1] loss: 1307.845
[21,     1] loss: 1296.095
[22,     1] loss: 1274.910
[23,     1] loss: 1272.678
[24,     1] loss: 1255.798
[25,     1] loss: 1219.467
[26,     1] loss: 1203.166
[27,     1] loss: 1203.690
[28,     1] loss: 1174.793
[29,     1] loss: 1175.750
[30,     1] loss: 1181.807
[31,     1] loss: 1177.955
[32,     1] loss: 1123.514
[33,     1] loss: 1163.626
[34,     1] loss: 1162.614
[35,     1] loss: 1166.482
[36,     1] loss: 1127.236
[37,     1] loss: 1116.590
[38,     1] loss: 1134.807
[39,     1] loss: 1118.873
[40,     1] loss: 1132.942
[41,     1] loss: 1095.542
[42,     1] loss: 1131.166
[43,     1] loss: 1123.582
[44,     1] loss: 1086.923
[45,     1] loss: 1105.458
[46,     1] loss: 1062.584
[47,     1] loss: 1119.720
[48,     1] loss: 1065.493
[49,     1] loss: 1060.328
[50,     1] loss: 1048.481
[51,     1] loss: 1054.618
[52,     1] loss: 1016.773
[53,     1] loss: 1063.206
[54,     1] loss: 1012.390
[55,     1] loss: 1045.634
[56,     1] loss: 1023.716
[57,     1] loss: 976.109
[58,     1] loss: 1037.791
[59,     1] loss: 1008.544
[60,     1] loss: 980.365
[61,     1] loss: 1063.401
[62,     1] loss: 1016.357
[63,     1] loss: 1005.428
[64,     1] loss: 948.483
[65,     1] loss: 989.242
[66,     1] loss: 931.794
[67,     1] loss: 974.114
[68,     1] loss: 950.930
[69,     1] loss: 922.007
[70,     1] loss: 938.032
[71,     1] loss: 920.032
[72,     1] loss: 897.464
[73,     1] loss: 904.361
[74,     1] loss: 919.942
[75,     1] loss: 904.082
[76,     1] loss: 897.626
[77,     1] loss: 847.054
[78,     1] loss: 936.387
[79,     1] loss: 859.388
[80,     1] loss: 916.281
[81,     1] loss: 839.611
[82,     1] loss: 905.653
[83,     1] loss: 849.043
[84,     1] loss: 817.038
[85,     1] loss: 855.002
[86,     1] loss: 789.941
[87,     1] loss: 829.225
[88,     1] loss: 816.434
[89,     1] loss: 830.416
[90,     1] loss: 792.770
[91,     1] loss: 769.998
[92,     1] loss: 774.394
[93,     1] loss: 832.796
[94,     1] loss: 804.857
[95,     1] loss: 731.064
[96,     1] loss: 680.474
[97,     1] loss: 770.552
[98,     1] loss: 811.303
[99,     1] loss: 716.249
[100,     1] loss: 803.119
[101,     1] loss: 731.958
[102,     1] loss: 697.833
[103,     1] loss: 753.992
[104,     1] loss: 758.090
[105,     1] loss: 686.601
[106,     1] loss: 717.571
[107,     1] loss: 658.973
[108,     1] loss: 724.809
[109,     1] loss: 661.043
[110,     1] loss: 706.891
[111,     1] loss: 657.254
[112,     1] loss: 655.608
[113,     1] loss: 714.203
[114,     1] loss: 688.457
[115,     1] loss: 632.967
Early stopping applied (best metric=0.4007183015346527)
Finished Training
Total time taken: 19.716688871383667
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.256
[2,     1] loss: 1396.960
[3,     1] loss: 1399.622
[4,     1] loss: 1397.125
[5,     1] loss: 1397.473
[6,     1] loss: 1396.020
[7,     1] loss: 1397.016
[8,     1] loss: 1392.155
[9,     1] loss: 1394.234
[10,     1] loss: 1390.692
[11,     1] loss: 1391.163
[12,     1] loss: 1390.069
[13,     1] loss: 1387.551
[14,     1] loss: 1382.035
[15,     1] loss: 1375.434
[16,     1] loss: 1365.169
[17,     1] loss: 1353.784
[18,     1] loss: 1348.104
[19,     1] loss: 1327.527
[20,     1] loss: 1306.689
[21,     1] loss: 1309.023
[22,     1] loss: 1290.003
[23,     1] loss: 1246.336
[24,     1] loss: 1234.826
[25,     1] loss: 1213.134
[26,     1] loss: 1205.233
[27,     1] loss: 1216.285
[28,     1] loss: 1179.033
[29,     1] loss: 1212.793
[30,     1] loss: 1153.601
[31,     1] loss: 1181.991
[32,     1] loss: 1167.925
[33,     1] loss: 1181.089
[34,     1] loss: 1119.478
[35,     1] loss: 1141.878
[36,     1] loss: 1210.825
[37,     1] loss: 1100.562
[38,     1] loss: 1123.323
[39,     1] loss: 1123.940
[40,     1] loss: 1119.776
[41,     1] loss: 1098.642
[42,     1] loss: 1092.758
[43,     1] loss: 1115.744
[44,     1] loss: 1130.735
[45,     1] loss: 1102.413
[46,     1] loss: 1102.213
[47,     1] loss: 1069.837
[48,     1] loss: 1080.271
[49,     1] loss: 1072.755
[50,     1] loss: 1044.713
[51,     1] loss: 1032.399
[52,     1] loss: 1076.276
[53,     1] loss: 1077.622
[54,     1] loss: 1030.514
[55,     1] loss: 985.919
[56,     1] loss: 1019.419
[57,     1] loss: 1021.567
[58,     1] loss: 1045.185
[59,     1] loss: 993.907
[60,     1] loss: 963.384
[61,     1] loss: 998.976
[62,     1] loss: 1009.951
[63,     1] loss: 961.835
[64,     1] loss: 938.870
[65,     1] loss: 958.847
[66,     1] loss: 952.552
[67,     1] loss: 936.989
[68,     1] loss: 943.249
[69,     1] loss: 917.265
[70,     1] loss: 916.907
[71,     1] loss: 900.383
[72,     1] loss: 950.294
[73,     1] loss: 947.750
[74,     1] loss: 877.696
[75,     1] loss: 885.533
[76,     1] loss: 859.029
[77,     1] loss: 869.423
[78,     1] loss: 889.735
[79,     1] loss: 835.724
[80,     1] loss: 834.982
[81,     1] loss: 812.308
[82,     1] loss: 868.426
[83,     1] loss: 760.582
[84,     1] loss: 785.779
[85,     1] loss: 820.941
[86,     1] loss: 802.299
[87,     1] loss: 761.899
[88,     1] loss: 762.419
[89,     1] loss: 801.960
[90,     1] loss: 786.416
[91,     1] loss: 701.343
[92,     1] loss: 815.759
[93,     1] loss: 697.778
[94,     1] loss: 779.575
[95,     1] loss: 694.236
[96,     1] loss: 784.206
[97,     1] loss: 850.106
[98,     1] loss: 721.898
[99,     1] loss: 712.591
[100,     1] loss: 735.488
[101,     1] loss: 722.846
[102,     1] loss: 766.963
[103,     1] loss: 642.496
Early stopping applied (best metric=0.36923113465309143)
Finished Training
Total time taken: 15.920642137527466
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.601
[2,     1] loss: 1397.662
[3,     1] loss: 1400.860
[4,     1] loss: 1398.164
[5,     1] loss: 1391.889
[6,     1] loss: 1397.654
[7,     1] loss: 1390.058
[8,     1] loss: 1395.038
[9,     1] loss: 1389.362
[10,     1] loss: 1390.776
[11,     1] loss: 1385.878
[12,     1] loss: 1381.261
[13,     1] loss: 1371.864
[14,     1] loss: 1369.783
[15,     1] loss: 1361.983
[16,     1] loss: 1349.763
[17,     1] loss: 1322.308
[18,     1] loss: 1312.211
[19,     1] loss: 1290.477
[20,     1] loss: 1285.907
[21,     1] loss: 1263.857
[22,     1] loss: 1247.596
[23,     1] loss: 1232.624
[24,     1] loss: 1199.859
[25,     1] loss: 1220.221
[26,     1] loss: 1154.506
[27,     1] loss: 1173.576
[28,     1] loss: 1156.771
[29,     1] loss: 1167.784
[30,     1] loss: 1122.358
[31,     1] loss: 1186.651
[32,     1] loss: 1155.776
[33,     1] loss: 1182.134
[34,     1] loss: 1123.478
[35,     1] loss: 1111.153
[36,     1] loss: 1101.666
[37,     1] loss: 1145.950
[38,     1] loss: 1112.206
[39,     1] loss: 1094.996
[40,     1] loss: 1094.667
[41,     1] loss: 1080.477
[42,     1] loss: 1113.043
[43,     1] loss: 1091.869
[44,     1] loss: 1076.997
[45,     1] loss: 1066.489
[46,     1] loss: 1055.055
[47,     1] loss: 1038.303
[48,     1] loss: 1026.819
[49,     1] loss: 1067.443
[50,     1] loss: 1052.241
[51,     1] loss: 1084.539
[52,     1] loss: 1027.323
[53,     1] loss: 1082.786
[54,     1] loss: 1013.874
[55,     1] loss: 978.564
[56,     1] loss: 1013.020
[57,     1] loss: 966.979
[58,     1] loss: 963.551
[59,     1] loss: 1009.084
[60,     1] loss: 961.513
[61,     1] loss: 993.589
[62,     1] loss: 916.066
[63,     1] loss: 912.341
[64,     1] loss: 981.588
[65,     1] loss: 921.946
[66,     1] loss: 927.789
[67,     1] loss: 896.340
[68,     1] loss: 863.773
[69,     1] loss: 917.148
[70,     1] loss: 839.189
[71,     1] loss: 860.891
[72,     1] loss: 848.376
[73,     1] loss: 905.709
[74,     1] loss: 839.593
[75,     1] loss: 912.176
[76,     1] loss: 833.173
[77,     1] loss: 766.021
[78,     1] loss: 824.768
[79,     1] loss: 873.619
[80,     1] loss: 819.562
[81,     1] loss: 817.888
[82,     1] loss: 803.724
[83,     1] loss: 829.149
[84,     1] loss: 787.192
[85,     1] loss: 754.173
Early stopping applied (best metric=0.43244636058807373)
Finished Training
Total time taken: 12.75471043586731
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.555
[2,     1] loss: 1399.109
[3,     1] loss: 1396.673
[4,     1] loss: 1396.379
[5,     1] loss: 1394.531
[6,     1] loss: 1395.565
[7,     1] loss: 1395.579
[8,     1] loss: 1393.646
[9,     1] loss: 1392.042
[10,     1] loss: 1390.310
[11,     1] loss: 1388.303
[12,     1] loss: 1390.959
[13,     1] loss: 1384.518
[14,     1] loss: 1380.509
[15,     1] loss: 1375.435
[16,     1] loss: 1365.973
[17,     1] loss: 1351.942
[18,     1] loss: 1344.809
[19,     1] loss: 1330.111
[20,     1] loss: 1309.950
[21,     1] loss: 1290.608
[22,     1] loss: 1267.823
[23,     1] loss: 1259.323
[24,     1] loss: 1253.321
[25,     1] loss: 1218.665
[26,     1] loss: 1179.633
[27,     1] loss: 1201.848
[28,     1] loss: 1180.495
[29,     1] loss: 1165.366
[30,     1] loss: 1166.565
[31,     1] loss: 1144.259
[32,     1] loss: 1120.370
[33,     1] loss: 1191.216
[34,     1] loss: 1143.656
[35,     1] loss: 1121.146
[36,     1] loss: 1178.549
[37,     1] loss: 1148.679
[38,     1] loss: 1149.618
[39,     1] loss: 1119.455
[40,     1] loss: 1112.756
[41,     1] loss: 1125.222
[42,     1] loss: 1131.216
[43,     1] loss: 1083.536
[44,     1] loss: 1100.166
[45,     1] loss: 1072.808
[46,     1] loss: 1098.911
[47,     1] loss: 1065.451
[48,     1] loss: 1067.010
[49,     1] loss: 1065.404
[50,     1] loss: 1069.475
[51,     1] loss: 1090.552
[52,     1] loss: 1055.307
[53,     1] loss: 1045.095
[54,     1] loss: 1051.759
[55,     1] loss: 1017.239
[56,     1] loss: 1001.905
[57,     1] loss: 1034.421
[58,     1] loss: 1020.444
[59,     1] loss: 1040.961
[60,     1] loss: 1011.090
[61,     1] loss: 987.548
[62,     1] loss: 1014.877
[63,     1] loss: 1006.887
[64,     1] loss: 996.712
[65,     1] loss: 1026.189
[66,     1] loss: 953.100
[67,     1] loss: 1014.654
[68,     1] loss: 979.085
[69,     1] loss: 939.849
[70,     1] loss: 929.416
[71,     1] loss: 941.347
[72,     1] loss: 941.067
[73,     1] loss: 902.787
[74,     1] loss: 897.487
[75,     1] loss: 873.762
[76,     1] loss: 911.911
[77,     1] loss: 888.104
[78,     1] loss: 876.937
[79,     1] loss: 905.591
[80,     1] loss: 860.211
[81,     1] loss: 841.277
[82,     1] loss: 811.423
[83,     1] loss: 828.805
[84,     1] loss: 807.973
[85,     1] loss: 835.613
[86,     1] loss: 914.069
[87,     1] loss: 832.279
[88,     1] loss: 817.967
[89,     1] loss: 839.977
[90,     1] loss: 800.668
[91,     1] loss: 749.541
[92,     1] loss: 760.929
[93,     1] loss: 728.198
[94,     1] loss: 810.533
[95,     1] loss: 770.509
[96,     1] loss: 736.148
[97,     1] loss: 699.024
[98,     1] loss: 735.921
[99,     1] loss: 745.703
[100,     1] loss: 714.586
[101,     1] loss: 690.446
[102,     1] loss: 704.951
[103,     1] loss: 712.305
[104,     1] loss: 738.966
[105,     1] loss: 750.744
[106,     1] loss: 664.204
[107,     1] loss: 682.119
[108,     1] loss: 674.680
[109,     1] loss: 704.575
[110,     1] loss: 686.598
[111,     1] loss: 704.617
[112,     1] loss: 664.769
[113,     1] loss: 711.069
[114,     1] loss: 756.188
[115,     1] loss: 700.071
[116,     1] loss: 682.788
[117,     1] loss: 682.152
Early stopping applied (best metric=0.3471640646457672)
Finished Training
Total time taken: 17.80418372154236
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1399.328
[2,     1] loss: 1401.656
[3,     1] loss: 1401.652
[4,     1] loss: 1397.077
[5,     1] loss: 1398.566
[6,     1] loss: 1399.244
[7,     1] loss: 1393.976
[8,     1] loss: 1396.070
[9,     1] loss: 1396.623
[10,     1] loss: 1396.079
[11,     1] loss: 1395.707
[12,     1] loss: 1395.654
[13,     1] loss: 1395.522
[14,     1] loss: 1390.527
[15,     1] loss: 1390.736
[16,     1] loss: 1389.071
[17,     1] loss: 1386.795
[18,     1] loss: 1380.067
[19,     1] loss: 1380.149
[20,     1] loss: 1371.934
[21,     1] loss: 1366.177
[22,     1] loss: 1355.591
[23,     1] loss: 1345.030
[24,     1] loss: 1330.196
[25,     1] loss: 1313.007
[26,     1] loss: 1287.732
[27,     1] loss: 1275.823
[28,     1] loss: 1273.017
[29,     1] loss: 1233.108
[30,     1] loss: 1238.104
[31,     1] loss: 1228.360
[32,     1] loss: 1177.406
[33,     1] loss: 1166.948
[34,     1] loss: 1174.968
[35,     1] loss: 1214.574
[36,     1] loss: 1174.113
[37,     1] loss: 1171.215
[38,     1] loss: 1138.474
[39,     1] loss: 1129.095
[40,     1] loss: 1129.418
[41,     1] loss: 1168.459
[42,     1] loss: 1137.758
[43,     1] loss: 1106.559
[44,     1] loss: 1155.971
[45,     1] loss: 1139.611
[46,     1] loss: 1102.207
[47,     1] loss: 1086.008
[48,     1] loss: 1091.892
[49,     1] loss: 1096.468
[50,     1] loss: 1063.405
[51,     1] loss: 1040.352
[52,     1] loss: 1096.076
[53,     1] loss: 1068.863
[54,     1] loss: 1079.665
[55,     1] loss: 1085.990
[56,     1] loss: 1046.855
[57,     1] loss: 1034.095
[58,     1] loss: 1028.042
[59,     1] loss: 1048.430
[60,     1] loss: 1051.422
[61,     1] loss: 1039.818
[62,     1] loss: 987.499
[63,     1] loss: 993.868
[64,     1] loss: 966.136
[65,     1] loss: 967.899
[66,     1] loss: 962.040
[67,     1] loss: 953.940
[68,     1] loss: 963.620
[69,     1] loss: 954.802
[70,     1] loss: 917.532
[71,     1] loss: 898.434
[72,     1] loss: 918.117
[73,     1] loss: 967.108
[74,     1] loss: 875.211
[75,     1] loss: 882.555
[76,     1] loss: 926.701
[77,     1] loss: 896.258
[78,     1] loss: 896.370
[79,     1] loss: 895.872
[80,     1] loss: 908.556
[81,     1] loss: 881.748
[82,     1] loss: 822.367
[83,     1] loss: 905.055
[84,     1] loss: 808.656
[85,     1] loss: 846.391
[86,     1] loss: 847.214
[87,     1] loss: 853.544
[88,     1] loss: 750.779
[89,     1] loss: 823.049
[90,     1] loss: 800.350
[91,     1] loss: 908.202
[92,     1] loss: 789.289
[93,     1] loss: 812.613
[94,     1] loss: 828.577
[95,     1] loss: 789.052
[96,     1] loss: 746.430
[97,     1] loss: 828.365
[98,     1] loss: 804.817
[99,     1] loss: 752.405
[100,     1] loss: 766.845
[101,     1] loss: 741.275
[102,     1] loss: 807.612
[103,     1] loss: 683.868
[104,     1] loss: 739.884
[105,     1] loss: 703.859
[106,     1] loss: 707.734
[107,     1] loss: 690.552
[108,     1] loss: 707.851
[109,     1] loss: 700.499
[110,     1] loss: 737.937
[111,     1] loss: 709.048
[112,     1] loss: 745.450
[113,     1] loss: 707.994
[114,     1] loss: 687.606
[115,     1] loss: 622.512
[116,     1] loss: 672.677
[117,     1] loss: 651.801
[118,     1] loss: 685.518
[119,     1] loss: 619.667
[120,     1] loss: 693.786
[121,     1] loss: 597.886
[122,     1] loss: 666.554
[123,     1] loss: 591.682
[124,     1] loss: 615.935
[125,     1] loss: 621.593
[126,     1] loss: 556.123
[127,     1] loss: 582.968
[128,     1] loss: 651.025
[129,     1] loss: 561.181
[130,     1] loss: 587.179
Early stopping applied (best metric=0.4028204083442688)
Finished Training
Total time taken: 22.761333465576172
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1410.244
[2,     1] loss: 1404.250
[3,     1] loss: 1402.209
[4,     1] loss: 1398.852
[5,     1] loss: 1402.944
[6,     1] loss: 1397.340
[7,     1] loss: 1397.651
[8,     1] loss: 1398.985
[9,     1] loss: 1393.164
[10,     1] loss: 1391.836
[11,     1] loss: 1391.707
[12,     1] loss: 1390.020
[13,     1] loss: 1381.717
[14,     1] loss: 1379.294
[15,     1] loss: 1366.771
[16,     1] loss: 1362.763
[17,     1] loss: 1354.167
[18,     1] loss: 1335.658
[19,     1] loss: 1317.175
[20,     1] loss: 1316.451
[21,     1] loss: 1301.589
[22,     1] loss: 1283.711
[23,     1] loss: 1260.648
[24,     1] loss: 1261.770
[25,     1] loss: 1242.586
[26,     1] loss: 1197.994
[27,     1] loss: 1212.125
[28,     1] loss: 1210.089
[29,     1] loss: 1183.962
[30,     1] loss: 1177.264
[31,     1] loss: 1152.931
[32,     1] loss: 1140.419
[33,     1] loss: 1235.750
[34,     1] loss: 1091.798
[35,     1] loss: 1151.148
[36,     1] loss: 1139.254
[37,     1] loss: 1158.431
[38,     1] loss: 1104.341
[39,     1] loss: 1131.622
[40,     1] loss: 1139.551
[41,     1] loss: 1133.122
[42,     1] loss: 1074.018
[43,     1] loss: 1117.873
[44,     1] loss: 1110.233
[45,     1] loss: 1076.411
[46,     1] loss: 1091.424
[47,     1] loss: 1080.621
[48,     1] loss: 1087.481
[49,     1] loss: 1064.126
[50,     1] loss: 1093.751
[51,     1] loss: 986.263
[52,     1] loss: 1040.111
[53,     1] loss: 1036.389
[54,     1] loss: 1046.955
[55,     1] loss: 1095.990
[56,     1] loss: 1045.763
[57,     1] loss: 1006.695
[58,     1] loss: 1026.979
[59,     1] loss: 1031.387
[60,     1] loss: 1043.873
[61,     1] loss: 961.076
[62,     1] loss: 940.096
[63,     1] loss: 953.568
[64,     1] loss: 976.382
[65,     1] loss: 928.764
[66,     1] loss: 940.321
[67,     1] loss: 903.073
[68,     1] loss: 937.700
[69,     1] loss: 925.297
[70,     1] loss: 905.791
[71,     1] loss: 895.629
[72,     1] loss: 921.359
[73,     1] loss: 905.034
[74,     1] loss: 903.745
[75,     1] loss: 936.054
[76,     1] loss: 848.107
[77,     1] loss: 899.054
[78,     1] loss: 871.665
[79,     1] loss: 873.786
[80,     1] loss: 820.200
[81,     1] loss: 864.432
[82,     1] loss: 858.717
[83,     1] loss: 770.825
[84,     1] loss: 821.249
[85,     1] loss: 813.116
[86,     1] loss: 782.731
[87,     1] loss: 777.650
[88,     1] loss: 766.274
[89,     1] loss: 782.324
[90,     1] loss: 818.773
[91,     1] loss: 784.271
[92,     1] loss: 708.661
[93,     1] loss: 763.509
[94,     1] loss: 663.437
[95,     1] loss: 764.132
[96,     1] loss: 740.196
[97,     1] loss: 729.019
[98,     1] loss: 714.759
[99,     1] loss: 694.041
[100,     1] loss: 728.217
[101,     1] loss: 757.250
[102,     1] loss: 654.771
[103,     1] loss: 706.394
[104,     1] loss: 701.695
[105,     1] loss: 642.463
[106,     1] loss: 672.254
[107,     1] loss: 636.938
[108,     1] loss: 630.251
[109,     1] loss: 687.986
[110,     1] loss: 671.744
[111,     1] loss: 627.037
[112,     1] loss: 688.877
[113,     1] loss: 650.051
[114,     1] loss: 646.599
[115,     1] loss: 730.049
[116,     1] loss: 569.231
[117,     1] loss: 728.290
[118,     1] loss: 610.947
[119,     1] loss: 730.868
[120,     1] loss: 624.627
[121,     1] loss: 720.167
[122,     1] loss: 696.901
[123,     1] loss: 616.740
[124,     1] loss: 696.724
Early stopping applied (best metric=0.36030617356300354)
Finished Training
Total time taken: 20.126752853393555
{'Hydroxylation-K Validation Accuracy': 0.7506501182033097, 'Hydroxylation-K Validation Sensitivity': 0.64, 'Hydroxylation-K Validation Specificity': 0.7789473684210526, 'Hydroxylation-K Validation Precision': 0.42260096198950686, 'Hydroxylation-K AUC ROC': 0.811130604288499, 'Hydroxylation-K AUC PR': 0.563046051365816, 'Hydroxylation-K MCC': 0.3652506402847229, 'Hydroxylation-K F1': 0.5046303751591108, 'Validation Loss (Hydroxylation-K)': 0.44644382198651633, 'Hydroxylation-P Validation Accuracy': 0.7828466744496896, 'Hydroxylation-P Validation Sensitivity': 0.7783597883597884, 'Hydroxylation-P Validation Specificity': 0.7838296174372786, 'Hydroxylation-P Validation Precision': 0.4418669442403803, 'Hydroxylation-P AUC ROC': 0.8310763352009799, 'Hydroxylation-P AUC PR': 0.5579087736508199, 'Hydroxylation-P MCC': 0.46463565215614966, 'Hydroxylation-P F1': 0.5621919763930975, 'Validation Loss (Hydroxylation-P)': 0.3820574323336283, 'Validation Loss (total)': 0.8285012642542521, 'TimeToTrain': 18.62478322982788}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008954231309570961,
 'learning_rate_Hydroxylation-K': 0.005711640296611211,
 'learning_rate_Hydroxylation-P': 0.008942320268207045,
 'log_base': 2.040061869224556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1652359282,
 'sample_weights': [2.3226865101739618, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9124817168341894,
 'weight_decay_Hydroxylation-K': 7.829046851906223,
 'weight_decay_Hydroxylation-P': 8.979519224196038}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.620
[2,     1] loss: 1401.184
[3,     1] loss: 1398.720
[4,     1] loss: 1401.661
[5,     1] loss: 1395.063
[6,     1] loss: 1397.690
[7,     1] loss: 1375.931
[8,     1] loss: 1344.231
[9,     1] loss: 1308.508
[10,     1] loss: 1266.029
[11,     1] loss: 1270.109
[12,     1] loss: 1188.659
[13,     1] loss: 1220.021
[14,     1] loss: 1160.767
[15,     1] loss: 1210.742
[16,     1] loss: 1152.342
[17,     1] loss: 1188.860
[18,     1] loss: 1141.973
[19,     1] loss: 1118.504
[20,     1] loss: 1067.879
[21,     1] loss: 1123.504
[22,     1] loss: 1062.736
[23,     1] loss: 1146.556
[24,     1] loss: 1111.237
[25,     1] loss: 1105.265
[26,     1] loss: 1091.906
[27,     1] loss: 1055.230
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004335531769689938,
 'learning_rate_Hydroxylation-K': 0.003549644485150097,
 'learning_rate_Hydroxylation-P': 0.007841847577694034,
 'log_base': 1.0412746059136762,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 492696598,
 'sample_weights': [2.341500224793598, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.809431774838945,
 'weight_decay_Hydroxylation-K': 2.792917220742649,
 'weight_decay_Hydroxylation-P': 0.9313754641983087}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13392.631
[2,     1] loss: 13390.640
[3,     1] loss: 13400.962
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006046570218045237,
 'learning_rate_Hydroxylation-K': 0.0050520199336946256,
 'learning_rate_Hydroxylation-P': 0.008588336314497985,
 'log_base': 1.105572039161562,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3247649274,
 'sample_weights': [41.27631690972268, 5.159738379900053],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.588072944185722,
 'weight_decay_Hydroxylation-K': 4.045706974093498,
 'weight_decay_Hydroxylation-P': 3.9895487570128974}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5405.625
[2,     1] loss: 5384.885
[3,     1] loss: 5386.912
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0042864920919102245,
 'learning_rate_Hydroxylation-K': 0.0035011888393358444,
 'learning_rate_Hydroxylation-P': 0.00614129004726981,
 'log_base': 2.246542325832837,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2650358921,
 'sample_weights': [16.63406917889533, 2.079338748754544],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.194098996592547,
 'weight_decay_Hydroxylation-K': 3.8412247660349133,
 'weight_decay_Hydroxylation-P': 8.796728063025638}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1347.647
[2,     1] loss: 1338.019
[3,     1] loss: 1353.496
[4,     1] loss: 1340.108
[5,     1] loss: 1339.756
[6,     1] loss: 1336.217
[7,     1] loss: 1332.069
[8,     1] loss: 1327.028
[9,     1] loss: 1309.889
[10,     1] loss: 1275.037
[11,     1] loss: 1255.720
[12,     1] loss: 1227.115
[13,     1] loss: 1208.741
[14,     1] loss: 1162.567
[15,     1] loss: 1191.856
[16,     1] loss: 1168.110
[17,     1] loss: 1130.309
[18,     1] loss: 1081.210
[19,     1] loss: 1124.836
[20,     1] loss: 1080.010
[21,     1] loss: 1074.011
[22,     1] loss: 1030.914
[23,     1] loss: 1061.821
[24,     1] loss: 1071.039
[25,     1] loss: 1022.492
[26,     1] loss: 1056.097
[27,     1] loss: 1011.932
[28,     1] loss: 1036.381
[29,     1] loss: 961.126
[30,     1] loss: 1022.222
[31,     1] loss: 1077.024
[32,     1] loss: 986.850
[33,     1] loss: 970.151
[34,     1] loss: 1013.687
[35,     1] loss: 941.554
[36,     1] loss: 916.932
[37,     1] loss: 913.799
[38,     1] loss: 912.672
[39,     1] loss: 882.849
[40,     1] loss: 941.018
[41,     1] loss: 842.919
[42,     1] loss: 872.130
[43,     1] loss: 824.366
[44,     1] loss: 847.528
[45,     1] loss: 882.962
[46,     1] loss: 1007.926
[47,     1] loss: 1010.043
[48,     1] loss: 860.696
[49,     1] loss: 922.073
[50,     1] loss: 856.825
[51,     1] loss: 909.151
[52,     1] loss: 810.732
[53,     1] loss: 827.358
[54,     1] loss: 803.233
[55,     1] loss: 818.654
[56,     1] loss: 793.175
[57,     1] loss: 824.848
[58,     1] loss: 754.210
[59,     1] loss: 909.516
[60,     1] loss: 807.793
[61,     1] loss: 783.457
[62,     1] loss: 877.437
[63,     1] loss: 778.746
[64,     1] loss: 807.420
[65,     1] loss: 739.310
[66,     1] loss: 781.065
[67,     1] loss: 719.979
[68,     1] loss: 705.608
[69,     1] loss: 738.911
[70,     1] loss: 681.740
[71,     1] loss: 732.458
[72,     1] loss: 624.391
[73,     1] loss: 634.391
[74,     1] loss: 724.539
[75,     1] loss: 818.907
[76,     1] loss: 715.799
[77,     1] loss: 669.154
[78,     1] loss: 727.156
[79,     1] loss: 632.870
[80,     1] loss: 621.748
[81,     1] loss: 606.614
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003495513781305276,
 'learning_rate_Hydroxylation-K': 0.006312826693907059,
 'learning_rate_Hydroxylation-P': 0.008284326112415736,
 'log_base': 1.1273669384850658,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1956462871,
 'sample_weights': [2.062588398433387, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1556129238651511,
 'weight_decay_Hydroxylation-K': 1.2638121929779134,
 'weight_decay_Hydroxylation-P': 0.9290507053430614}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4541.416
[2,     1] loss: 4520.610
[3,     1] loss: 4519.723
[4,     1] loss: 4495.729
[5,     1] loss: 4502.783
[6,     1] loss: 4529.596
[7,     1] loss: 4512.959
[8,     1] loss: 4543.997
[9,     1] loss: 4506.271
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005955345358888905,
 'learning_rate_Hydroxylation-K': 0.004368566735014019,
 'learning_rate_Hydroxylation-P': 0.008848136283575957,
 'log_base': 2.8904496881494417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3252717307,
 'sample_weights': [13.925398001094669, 1.740741807918113],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.851986985893985,
 'weight_decay_Hydroxylation-K': 7.262050996509182,
 'weight_decay_Hydroxylation-P': 3.7590836645057113}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.672
[2,     1] loss: 1239.285
[3,     1] loss: 1237.407
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035600608430634115,
 'learning_rate_Hydroxylation-K': 0.0020981614516423722,
 'learning_rate_Hydroxylation-P': 0.00693880840082686,
 'log_base': 2.093200886422182,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3844387740,
 'sample_weights': [1.5728510734548062, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.932416343444305,
 'weight_decay_Hydroxylation-K': 4.095530232412207,
 'weight_decay_Hydroxylation-P': 9.47388016985456}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1385.665
[2,     1] loss: 1384.341
[3,     1] loss: 1384.731
[4,     1] loss: 1383.550
[5,     1] loss: 1382.550
[6,     1] loss: 1383.461
[7,     1] loss: 1377.851
[8,     1] loss: 1369.517
[9,     1] loss: 1371.490
[10,     1] loss: 1352.167
[11,     1] loss: 1336.468
[12,     1] loss: 1309.259
[13,     1] loss: 1252.386
[14,     1] loss: 1252.501
[15,     1] loss: 1203.412
[16,     1] loss: 1203.927
[17,     1] loss: 1166.580
[18,     1] loss: 1130.735
[19,     1] loss: 1141.908
[20,     1] loss: 1148.017
[21,     1] loss: 1157.006
[22,     1] loss: 1119.638
[23,     1] loss: 1146.323
[24,     1] loss: 1146.922
[25,     1] loss: 1112.214
[26,     1] loss: 1094.992
[27,     1] loss: 1062.943
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007133538735238212,
 'learning_rate_Hydroxylation-K': 0.00944992526220602,
 'learning_rate_Hydroxylation-P': 0.009984948048053393,
 'log_base': 1.9826534183196969,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 813864876,
 'sample_weights': [2.2599915533404755, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.187202811838388,
 'weight_decay_Hydroxylation-K': 6.447520951853264,
 'weight_decay_Hydroxylation-P': 4.776365658669796}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1435.327
[2,     1] loss: 1430.530
[3,     1] loss: 1423.834
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008218383418428184,
 'learning_rate_Hydroxylation-K': 0.00037851887688290895,
 'learning_rate_Hydroxylation-P': 0.007427054034470146,
 'log_base': 1.040141965547055,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 152167595,
 'sample_weights': [2.4391513689894038, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.76314856321101,
 'weight_decay_Hydroxylation-K': 8.594856264976526,
 'weight_decay_Hydroxylation-P': 0.9824240447992083}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13770.040
[2,     1] loss: 13819.375
[3,     1] loss: 13736.354
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006153290373808237,
 'learning_rate_Hydroxylation-K': 0.00013993790201505873,
 'learning_rate_Hydroxylation-P': 0.007462815915262118,
 'log_base': 1.7164715853865908,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 783185652,
 'sample_weights': [42.41772174879005, 5.302419481220706],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.868059884856943,
 'weight_decay_Hydroxylation-K': 6.920456833783937,
 'weight_decay_Hydroxylation-P': 0.270035177391466}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1555.599
[2,     1] loss: 1552.925
[3,     1] loss: 1566.835
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011115320321535287,
 'learning_rate_Hydroxylation-K': 0.004979229270324134,
 'learning_rate_Hydroxylation-P': 0.009477329255700565,
 'log_base': 1.0245747596154566,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 158581315,
 'sample_weights': [3.0900119149847356, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.78627157807407,
 'weight_decay_Hydroxylation-K': 0.3076830628527298,
 'weight_decay_Hydroxylation-P': 0.5978487712120307}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22387.180
Exploding loss, terminate run (best metric=0.5316290259361267)
Finished Training
Total time taken: 0.24499940872192383
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22479.176
Exploding loss, terminate run (best metric=0.533979594707489)
Finished Training
Total time taken: 0.22399687767028809
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22268.453
Exploding loss, terminate run (best metric=0.5275548100471497)
Finished Training
Total time taken: 0.22800111770629883
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22352.072
Exploding loss, terminate run (best metric=0.5336157083511353)
Finished Training
Total time taken: 0.22300124168395996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22240.387
Exploding loss, terminate run (best metric=0.532492995262146)
Finished Training
Total time taken: 0.2500007152557373
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22292.910
Exploding loss, terminate run (best metric=0.5389419794082642)
Finished Training
Total time taken: 0.2669994831085205
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22308.535
Exploding loss, terminate run (best metric=0.5270625948905945)
Finished Training
Total time taken: 0.283998966217041
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22246.918
Exploding loss, terminate run (best metric=0.5279883742332458)
Finished Training
Total time taken: 0.2609992027282715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22308.029
Exploding loss, terminate run (best metric=0.5290628671646118)
Finished Training
Total time taken: 0.28800153732299805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22478.809
Exploding loss, terminate run (best metric=0.5282996296882629)
Finished Training
Total time taken: 0.265000581741333
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22403.959
Exploding loss, terminate run (best metric=0.5332453846931458)
Finished Training
Total time taken: 0.2870063781738281
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22457.053
Exploding loss, terminate run (best metric=0.5316635966300964)
Finished Training
Total time taken: 0.2760016918182373
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22473.129
Exploding loss, terminate run (best metric=0.5272215008735657)
Finished Training
Total time taken: 0.26900196075439453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22406.238
Exploding loss, terminate run (best metric=0.5305675864219666)
Finished Training
Total time taken: 0.2680022716522217
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 22352.496
Exploding loss, terminate run (best metric=0.5333759188652039)
Finished Training
Total time taken: 0.28499889373779297
{'Hydroxylation-K Validation Accuracy': 0.4828014184397163, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6160428849902534, 'Hydroxylation-K AUC PR': 0.29975896264948754, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1807881773399015, 'Validation Loss (Hydroxylation-K)': 0.5567820747693379, 'Hydroxylation-P Validation Accuracy': 0.47894078134781654, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.46707317073170734, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.568801633418759, 'Hydroxylation-P AUC PR': 0.25378237743139276, 'Hydroxylation-P MCC': 0.0022141728903656, 'Hydroxylation-P F1': 0.16050822698413122, 'Validation Loss (Hydroxylation-P)': 0.5311134378115336, 'Validation Loss (total)': 1.0878954887390138, 'TimeToTrain': 0.26133402188618976}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011442320448866276,
 'learning_rate_Hydroxylation-K': 0.0031060626227716616,
 'learning_rate_Hydroxylation-P': 0.009757792848027823,
 'log_base': 1.0887798080164053,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1373440055,
 'sample_weights': [68.81559780751938, 8.584076394204502],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.61909051837031,
 'weight_decay_Hydroxylation-K': 1.6745246634803634,
 'weight_decay_Hydroxylation-P': 1.2676742297208659}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6412.115
[2,     1] loss: 6371.248
[3,     1] loss: 6358.938
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004839463856569115,
 'learning_rate_Hydroxylation-K': 0.009952149642033139,
 'learning_rate_Hydroxylation-P': 0.006592259117535098,
 'log_base': 2.7384225477819037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3457623256,
 'sample_weights': [19.627201064644222, 2.453494647905599],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.08414622448184,
 'weight_decay_Hydroxylation-K': 6.227464018437063,
 'weight_decay_Hydroxylation-P': 2.875383087608192}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.588
[2,     1] loss: 1259.908
[3,     1] loss: 1258.008
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00115543295277396,
 'learning_rate_Hydroxylation-K': 0.009540772373131546,
 'learning_rate_Hydroxylation-P': 0.008600423596448003,
 'log_base': 1.1651630021903507,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1619420192,
 'sample_weights': [1.6572095566654956, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3808539032990246,
 'weight_decay_Hydroxylation-K': 0.9080875691790906,
 'weight_decay_Hydroxylation-P': 4.706570177342755}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3532.036
[2,     1] loss: 3540.448
[3,     1] loss: 3552.432
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005539440899864082,
 'learning_rate_Hydroxylation-K': 0.008583472724739681,
 'learning_rate_Hydroxylation-P': 0.007065310402177671,
 'log_base': 1.812129688936654,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 979672160,
 'sample_weights': [10.921315580072031, 1.365217039125516],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.250910973379964,
 'weight_decay_Hydroxylation-K': 6.61036872729148,
 'weight_decay_Hydroxylation-P': 2.8301407589514396}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1496.300
[2,     1] loss: 1504.951
[3,     1] loss: 1499.317
[4,     1] loss: 1505.136
[5,     1] loss: 1498.055
[6,     1] loss: 1498.728
[7,     1] loss: 1498.688
[8,     1] loss: 1497.229
[9,     1] loss: 1494.996
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029524463629488685,
 'learning_rate_Hydroxylation-K': 0.006187888146420341,
 'learning_rate_Hydroxylation-P': 0.006637793525547395,
 'log_base': 1.1444412921266216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4060992756,
 'sample_weights': [2.8081334707100485, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.678906321969576,
 'weight_decay_Hydroxylation-K': 2.90017573059487,
 'weight_decay_Hydroxylation-P': 5.626907152416017}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4043.456
[2,     1] loss: 4007.874
[3,     1] loss: 4010.849
[4,     1] loss: 4001.248
[5,     1] loss: 4017.174
[6,     1] loss: 4002.177
[7,     1] loss: 4022.411
[8,     1] loss: 4003.748
[9,     1] loss: 3988.606
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007012080484267348,
 'learning_rate_Hydroxylation-K': 0.0007040075294966011,
 'learning_rate_Hydroxylation-P': 0.006861154799640714,
 'log_base': 1.0134518646482602,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3502703294,
 'sample_weights': [12.373893203465826, 1.5467962369401236],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.19842806199665,
 'weight_decay_Hydroxylation-K': 2.2007086630413104,
 'weight_decay_Hydroxylation-P': 8.507401491704936}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40525.672
Exploding loss, terminate run (best metric=0.534812331199646)
Finished Training
Total time taken: 0.2200026512145996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40555.242
Exploding loss, terminate run (best metric=0.528015673160553)
Finished Training
Total time taken: 0.22299909591674805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40804.676
Exploding loss, terminate run (best metric=0.5452277064323425)
Finished Training
Total time taken: 0.2149975299835205
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40636.750
Exploding loss, terminate run (best metric=0.5292674899101257)
Finished Training
Total time taken: 0.2220015525817871
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40547.637
Exploding loss, terminate run (best metric=0.5303276181221008)
Finished Training
Total time taken: 0.21200180053710938
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40665.535
Exploding loss, terminate run (best metric=0.5325143337249756)
Finished Training
Total time taken: 0.22452402114868164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40810.742
Exploding loss, terminate run (best metric=0.5293340682983398)
Finished Training
Total time taken: 0.2200005054473877
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40578.703
Exploding loss, terminate run (best metric=0.5379770398139954)
Finished Training
Total time taken: 0.22099661827087402
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40428.203
Exploding loss, terminate run (best metric=0.526789665222168)
Finished Training
Total time taken: 0.21199917793273926
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40652.398
Exploding loss, terminate run (best metric=0.5383324027061462)
Finished Training
Total time taken: 0.222001314163208
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40545.141
Exploding loss, terminate run (best metric=0.5317350625991821)
Finished Training
Total time taken: 0.21699905395507812
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40529.000
Exploding loss, terminate run (best metric=0.5351883172988892)
Finished Training
Total time taken: 0.2180018424987793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40448.988
Exploding loss, terminate run (best metric=0.5265546441078186)
Finished Training
Total time taken: 0.2219998836517334
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40687.805
Exploding loss, terminate run (best metric=0.5320142507553101)
Finished Training
Total time taken: 0.2220017910003662
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40510.727
Exploding loss, terminate run (best metric=0.5267765522003174)
Finished Training
Total time taken: 0.22800087928771973
{'Hydroxylation-K Validation Accuracy': 0.48339243498817963, 'Hydroxylation-K Validation Sensitivity': 0.5259259259259259, 'Hydroxylation-K Validation Specificity': 0.47192982456140353, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6376608187134503, 'Hydroxylation-K AUC PR': 0.3409485582077261, 'Hydroxylation-K MCC': -0.005230966339876347, 'Hydroxylation-K F1': 0.1791330049261084, 'Validation Loss (Hydroxylation-K)': 0.5575300375620524, 'Hydroxylation-P Validation Accuracy': 0.4779306634181006, 'Hydroxylation-P Validation Sensitivity': 0.5371428571428571, 'Hydroxylation-P Validation Specificity': 0.4650256870666866, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5876530318180775, 'Hydroxylation-P AUC PR': 0.2679036695035762, 'Hydroxylation-P MCC': 0.005729285064321058, 'Hydroxylation-P F1': 0.16679515109339765, 'Validation Loss (Hydroxylation-P)': 0.532324477036794, 'Validation Loss (total)': 1.0898545265197754, 'TimeToTrain': 0.21990184783935546}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035054440154495053,
 'learning_rate_Hydroxylation-K': 0.0027727208914212777,
 'learning_rate_Hydroxylation-P': 0.008998685469061015,
 'log_base': 1.037297386731547,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 91925865,
 'sample_weights': [125.03050303954649, 15.596338968082181],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.552454981970598,
 'weight_decay_Hydroxylation-K': 5.149001141131866,
 'weight_decay_Hydroxylation-P': 4.617240521201697}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14786.020
[2,     1] loss: 14812.836
[3,     1] loss: 14763.984
[4,     1] loss: 14766.498
[5,     1] loss: 14794.320
[6,     1] loss: 14732.419
[7,     1] loss: 14795.625
[8,     1] loss: 14730.143
[9,     1] loss: 14737.193
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002628363005088884,
 'learning_rate_Hydroxylation-K': 0.003975859176448418,
 'learning_rate_Hydroxylation-P': 0.009063846805077148,
 'log_base': 1.0747929458857086,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1462895316,
 'sample_weights': [45.589952173277915, 5.698963560163496],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.45869187025663,
 'weight_decay_Hydroxylation-K': 0.24883193836258233,
 'weight_decay_Hydroxylation-P': 0.3739137792752797}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7548.614
[2,     1] loss: 7508.473
[3,     1] loss: 7489.026
[4,     1] loss: 7531.609
[5,     1] loss: 7505.778
[6,     1] loss: 7504.166
[7,     1] loss: 7470.431
[8,     1] loss: 7477.774
[9,     1] loss: 7456.338
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025119767835815997,
 'learning_rate_Hydroxylation-K': 0.004046827850608083,
 'learning_rate_Hydroxylation-P': 0.009795642416905944,
 'log_base': 1.1244340602983762,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2140642591,
 'sample_weights': [23.145551626955143, 2.89330541081847],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.112981393804574,
 'weight_decay_Hydroxylation-K': 0.5321815646631931,
 'weight_decay_Hydroxylation-P': 1.7002060821063951}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4618.704
[2,     1] loss: 4611.602
[3,     1] loss: 4606.111
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001989872904487589,
 'learning_rate_Hydroxylation-K': 0.0003363625166828087,
 'learning_rate_Hydroxylation-P': 0.0011692228542480895,
 'log_base': 1.7504169199373212,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2914407301,
 'sample_weights': [14.234696967163798, 1.7794056681065296],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9648602496478964,
 'weight_decay_Hydroxylation-K': 5.100327810620707,
 'weight_decay_Hydroxylation-P': 0.7935549499213033}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1542.570
[2,     1] loss: 1544.712
[3,     1] loss: 1538.043
[4,     1] loss: 1534.899
[5,     1] loss: 1538.565
[6,     1] loss: 1533.857
[7,     1] loss: 1535.781
[8,     1] loss: 1533.560
[9,     1] loss: 1526.315
[10,     1] loss: 1526.603
[11,     1] loss: 1527.052
[12,     1] loss: 1511.353
[13,     1] loss: 1507.060
[14,     1] loss: 1490.046
[15,     1] loss: 1475.238
[16,     1] loss: 1450.479
[17,     1] loss: 1421.280
[18,     1] loss: 1406.206
[19,     1] loss: 1380.876
[20,     1] loss: 1355.801
[21,     1] loss: 1363.902
[22,     1] loss: 1316.949
[23,     1] loss: 1264.977
[24,     1] loss: 1292.497
[25,     1] loss: 1328.428
[26,     1] loss: 1251.424
[27,     1] loss: 1279.418
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005013267241149422,
 'learning_rate_Hydroxylation-K': 0.0015361392484844706,
 'learning_rate_Hydroxylation-P': 0.007522978389174319,
 'log_base': 2.0825024999358717,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2171453591,
 'sample_weights': [2.9819259107556064, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.790599825505156,
 'weight_decay_Hydroxylation-K': 1.3876742577487797,
 'weight_decay_Hydroxylation-P': 9.089904541548396}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1392.072
[2,     1] loss: 1386.530
[3,     1] loss: 1387.608
[4,     1] loss: 1387.327
[5,     1] loss: 1379.156
[6,     1] loss: 1375.605
[7,     1] loss: 1356.189
[8,     1] loss: 1328.399
[9,     1] loss: 1295.689
[10,     1] loss: 1231.503
[11,     1] loss: 1242.953
[12,     1] loss: 1179.769
[13,     1] loss: 1166.702
[14,     1] loss: 1133.510
[15,     1] loss: 1115.865
[16,     1] loss: 1119.136
[17,     1] loss: 1082.838
[18,     1] loss: 1087.572
[19,     1] loss: 1061.124
[20,     1] loss: 1079.447
[21,     1] loss: 1088.212
[22,     1] loss: 1048.443
[23,     1] loss: 1059.439
[24,     1] loss: 1062.276
[25,     1] loss: 1004.663
[26,     1] loss: 1036.643
[27,     1] loss: 968.655
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010722552198455084,
 'learning_rate_Hydroxylation-K': 0.0007368158111634101,
 'learning_rate_Hydroxylation-P': 0.00021646712059767044,
 'log_base': 2.3085925292668668,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1150069894,
 'sample_weights': [2.2757780101614227, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.590177470917427,
 'weight_decay_Hydroxylation-K': 8.414525569505471,
 'weight_decay_Hydroxylation-P': 9.11996373282681}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.104
[2,     1] loss: 1334.241
[3,     1] loss: 1327.616
[4,     1] loss: 1329.210
[5,     1] loss: 1329.326
[6,     1] loss: 1331.417
[7,     1] loss: 1329.275
[8,     1] loss: 1327.888
[9,     1] loss: 1327.791
[10,     1] loss: 1327.559
[11,     1] loss: 1322.449
[12,     1] loss: 1322.668
[13,     1] loss: 1318.851
[14,     1] loss: 1309.558
[15,     1] loss: 1303.807
[16,     1] loss: 1297.698
[17,     1] loss: 1283.981
[18,     1] loss: 1269.274
[19,     1] loss: 1256.196
[20,     1] loss: 1254.877
[21,     1] loss: 1240.068
[22,     1] loss: 1224.968
[23,     1] loss: 1197.262
[24,     1] loss: 1175.884
[25,     1] loss: 1163.456
[26,     1] loss: 1147.417
[27,     1] loss: 1127.373
[28,     1] loss: 1143.224
[29,     1] loss: 1155.930
[30,     1] loss: 1113.861
[31,     1] loss: 1112.004
[32,     1] loss: 1093.310
[33,     1] loss: 1064.493
[34,     1] loss: 1096.884
[35,     1] loss: 1081.262
[36,     1] loss: 1115.204
[37,     1] loss: 1055.941
[38,     1] loss: 1029.843
[39,     1] loss: 1060.775
[40,     1] loss: 1081.269
[41,     1] loss: 1033.478
[42,     1] loss: 1024.524
[43,     1] loss: 1013.455
[44,     1] loss: 973.880
[45,     1] loss: 1031.691
[46,     1] loss: 988.073
[47,     1] loss: 1027.706
[48,     1] loss: 1001.129
[49,     1] loss: 965.143
[50,     1] loss: 967.626
[51,     1] loss: 955.739
[52,     1] loss: 1013.487
[53,     1] loss: 969.744
[54,     1] loss: 1001.335
[55,     1] loss: 937.994
[56,     1] loss: 986.800
[57,     1] loss: 902.015
[58,     1] loss: 985.690
[59,     1] loss: 928.737
[60,     1] loss: 926.472
[61,     1] loss: 937.823
[62,     1] loss: 902.875
[63,     1] loss: 864.232
[64,     1] loss: 884.861
[65,     1] loss: 866.655
[66,     1] loss: 842.883
[67,     1] loss: 780.368
[68,     1] loss: 784.662
[69,     1] loss: 858.451
[70,     1] loss: 818.984
[71,     1] loss: 822.482
[72,     1] loss: 850.376
[73,     1] loss: 742.998
[74,     1] loss: 768.797
[75,     1] loss: 777.703
[76,     1] loss: 798.833
[77,     1] loss: 741.755
[78,     1] loss: 758.130
[79,     1] loss: 759.405
[80,     1] loss: 793.982
[81,     1] loss: 680.035
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004237867264317496,
 'learning_rate_Hydroxylation-K': 0.006982984447033458,
 'learning_rate_Hydroxylation-P': 0.007439896921992043,
 'log_base': 1.4348695565802534,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 894611544,
 'sample_weights': [1.9954186390395237, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.393336829627369,
 'weight_decay_Hydroxylation-K': 4.5529157073208815,
 'weight_decay_Hydroxylation-P': 7.656481931988635}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1882.741
[2,     1] loss: 1879.657
[3,     1] loss: 1883.313
[4,     1] loss: 1883.737
[5,     1] loss: 1869.521
[6,     1] loss: 1885.703
[7,     1] loss: 1876.406
[8,     1] loss: 1868.104
[9,     1] loss: 1855.858
[10,     1] loss: 1852.129
[11,     1] loss: 1825.120
[12,     1] loss: 1819.226
[13,     1] loss: 1766.081
[14,     1] loss: 1749.534
[15,     1] loss: 1725.333
[16,     1] loss: 1721.463
[17,     1] loss: 1629.421
[18,     1] loss: 1642.825
[19,     1] loss: 1597.777
[20,     1] loss: 1571.059
[21,     1] loss: 1521.006
[22,     1] loss: 1574.069
[23,     1] loss: 1474.381
[24,     1] loss: 1497.009
[25,     1] loss: 1459.354
[26,     1] loss: 1407.762
[27,     1] loss: 1458.105
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004602269896335998,
 'learning_rate_Hydroxylation-K': 0.0022741514310997066,
 'learning_rate_Hydroxylation-P': 0.005094979791522439,
 'log_base': 2.0881959154993384,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 29529167,
 'sample_weights': [4.623549210231922, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.272185557065661,
 'weight_decay_Hydroxylation-K': 3.6894175708774077,
 'weight_decay_Hydroxylation-P': 4.639211229863434}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.641
[2,     1] loss: 1384.453
[3,     1] loss: 1385.703
[4,     1] loss: 1390.323
[5,     1] loss: 1387.269
[6,     1] loss: 1385.012
[7,     1] loss: 1387.492
[8,     1] loss: 1380.969
[9,     1] loss: 1385.689
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003863158274441187,
 'learning_rate_Hydroxylation-K': 0.0039996934893204205,
 'learning_rate_Hydroxylation-P': 0.009783803200602705,
 'log_base': 1.275992907982169,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3563357105,
 'sample_weights': [2.2673394341448243, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.632511844788006,
 'weight_decay_Hydroxylation-K': 6.725510080674169,
 'weight_decay_Hydroxylation-P': 2.140666619631061}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2359.082
[2,     1] loss: 2369.090
[3,     1] loss: 2354.423
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006834042471142266,
 'learning_rate_Hydroxylation-K': 0.003946855836849392,
 'learning_rate_Hydroxylation-P': 0.009384742866975073,
 'log_base': 1.071619879384648,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3090776835,
 'sample_weights': [6.849710546946188, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.338982631878135,
 'weight_decay_Hydroxylation-K': 6.992721236299097,
 'weight_decay_Hydroxylation-P': 0.039869477236403716}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7828.431
[2,     1] loss: 7841.971
[3,     1] loss: 7832.074
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004363031310450415,
 'learning_rate_Hydroxylation-K': 0.009774955442456221,
 'learning_rate_Hydroxylation-P': 0.00028192729177587816,
 'log_base': 2.091707306574227,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 461322530,
 'sample_weights': [24.134872429265965, 3.016975275183579],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4033583069607163,
 'weight_decay_Hydroxylation-K': 2.724676106148715,
 'weight_decay_Hydroxylation-P': 7.063569231163875}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1384.872
[2,     1] loss: 1386.800
[3,     1] loss: 1384.515
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0073683003813232615,
 'learning_rate_Hydroxylation-K': 0.004253368999984559,
 'learning_rate_Hydroxylation-P': 0.005278799209729092,
 'log_base': 1.2729957828436027,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4159403322,
 'sample_weights': [2.2621774739261356, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.360868277532955,
 'weight_decay_Hydroxylation-K': 7.029524900918349,
 'weight_decay_Hydroxylation-P': 6.526106595261295}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2359.014
[2,     1] loss: 2364.733
[3,     1] loss: 2383.536
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00021112613876573226,
 'learning_rate_Hydroxylation-K': 0.004801499862832015,
 'learning_rate_Hydroxylation-P': 0.006126099472100899,
 'log_base': 1.1534860572022114,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3504796716,
 'sample_weights': [6.9164450886982785, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.673534896839875,
 'weight_decay_Hydroxylation-K': 1.2135659047786937,
 'weight_decay_Hydroxylation-P': 1.726821717007789}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3789.857
[2,     1] loss: 3802.435
[3,     1] loss: 3800.101
[4,     1] loss: 3798.608
[5,     1] loss: 3800.897
[6,     1] loss: 3798.582
[7,     1] loss: 3813.245
[8,     1] loss: 3797.989
[9,     1] loss: 3804.194
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006368171738845879,
 'learning_rate_Hydroxylation-K': 0.00534291287657461,
 'learning_rate_Hydroxylation-P': 0.006790363615145454,
 'log_base': 2.4786247536033494,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3307959217,
 'sample_weights': [11.691702612159443, 1.4615191279367006],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.979730575398726,
 'weight_decay_Hydroxylation-K': 4.6313405763094435,
 'weight_decay_Hydroxylation-P': 9.745087738873668}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1292.686
[2,     1] loss: 1297.809
[3,     1] loss: 1298.509
[4,     1] loss: 1292.329
[5,     1] loss: 1293.295
[6,     1] loss: 1291.665
[7,     1] loss: 1285.115
[8,     1] loss: 1277.123
[9,     1] loss: 1243.290
[10,     1] loss: 1204.902
[11,     1] loss: 1156.186
[12,     1] loss: 1100.729
[13,     1] loss: 1123.809
[14,     1] loss: 1115.232
[15,     1] loss: 1085.733
[16,     1] loss: 1152.426
[17,     1] loss: 1027.619
[18,     1] loss: 1090.487
[19,     1] loss: 1045.540
[20,     1] loss: 1009.214
[21,     1] loss: 1074.386
[22,     1] loss: 998.385
[23,     1] loss: 1031.771
[24,     1] loss: 1000.094
[25,     1] loss: 991.687
[26,     1] loss: 958.134
[27,     1] loss: 1009.081
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005318173666952466,
 'learning_rate_Hydroxylation-K': 0.0031278838102291114,
 'learning_rate_Hydroxylation-P': 0.003625675801958831,
 'log_base': 2.828391744698659,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2473719573,
 'sample_weights': [1.839193595828751, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.055351026645983,
 'weight_decay_Hydroxylation-K': 7.433735284893331,
 'weight_decay_Hydroxylation-P': 0.9040201358016073}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1247.987
[2,     1] loss: 1245.636
[3,     1] loss: 1247.997
[4,     1] loss: 1244.131
[5,     1] loss: 1242.893
[6,     1] loss: 1238.529
[7,     1] loss: 1231.999
[8,     1] loss: 1217.591
[9,     1] loss: 1178.483
[10,     1] loss: 1138.409
[11,     1] loss: 1105.335
[12,     1] loss: 1094.517
[13,     1] loss: 1031.471
[14,     1] loss: 1060.716
[15,     1] loss: 1054.166
[16,     1] loss: 1057.493
[17,     1] loss: 980.821
[18,     1] loss: 998.728
[19,     1] loss: 994.996
[20,     1] loss: 1026.851
[21,     1] loss: 996.265
[22,     1] loss: 978.154
[23,     1] loss: 994.570
[24,     1] loss: 975.861
[25,     1] loss: 989.852
[26,     1] loss: 970.586
[27,     1] loss: 939.646
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036573158800572185,
 'learning_rate_Hydroxylation-K': 0.008199407723197296,
 'learning_rate_Hydroxylation-P': 0.007861380217752486,
 'log_base': 1.3257523129017894,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 861203312,
 'sample_weights': [1.6056842177734416, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0350014811872077,
 'weight_decay_Hydroxylation-K': 3.5654759551339934,
 'weight_decay_Hydroxylation-P': 2.1380952183989095}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2174.742
[2,     1] loss: 2148.534
[3,     1] loss: 2158.550
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004915783587477616,
 'learning_rate_Hydroxylation-K': 0.0055931559436154365,
 'learning_rate_Hydroxylation-P': 0.009356486096213189,
 'log_base': 1.0439301895086108,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3821328074,
 'sample_weights': [5.920429335858016, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.920290350276614,
 'weight_decay_Hydroxylation-K': 0.1951338015490076,
 'weight_decay_Hydroxylation-P': 0.21596725003129308}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12614.635
[2,     1] loss: 12595.408
[3,     1] loss: 12707.990
[4,     1] loss: 12619.802
[5,     1] loss: 12637.199
[6,     1] loss: 12559.693
[7,     1] loss: 12597.928
[8,     1] loss: 12585.738
[9,     1] loss: 12615.797
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015353767530955423,
 'learning_rate_Hydroxylation-K': 0.009647133274997138,
 'learning_rate_Hydroxylation-P': 0.007597260502583672,
 'log_base': 1.1468127546916587,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4147379500,
 'sample_weights': [38.83092467186929, 4.854052574377279],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.341594332887873,
 'weight_decay_Hydroxylation-K': 4.0364164442383,
 'weight_decay_Hydroxylation-P': 1.8533113947960103}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3965.226
[2,     1] loss: 3963.697
[3,     1] loss: 3937.894
[4,     1] loss: 3930.694
[5,     1] loss: 3942.917
[6,     1] loss: 3959.275
[7,     1] loss: 3932.630
[8,     1] loss: 3930.026
[9,     1] loss: 3933.309
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0001450932936147895,
 'learning_rate_Hydroxylation-K': 0.0044556628948664535,
 'learning_rate_Hydroxylation-P': 0.008797182751053246,
 'log_base': 1.2653389154516004,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4212815966,
 'sample_weights': [12.186910459395486, 1.5234224935155616],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.646927086482413,
 'weight_decay_Hydroxylation-K': 1.6029296483756283,
 'weight_decay_Hydroxylation-P': 0.33397402423447753}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2403.943
[2,     1] loss: 2395.912
[3,     1] loss: 2390.424
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036575686625345254,
 'learning_rate_Hydroxylation-K': 0.004530954554616585,
 'learning_rate_Hydroxylation-P': 0.0010188340611859198,
 'log_base': 2.476288856397484,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2518891282,
 'sample_weights': [7.093749985289345, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.763346157060647,
 'weight_decay_Hydroxylation-K': 8.08218488485338,
 'weight_decay_Hydroxylation-P': 9.424737710311872}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1301.568
[2,     1] loss: 1293.943
[3,     1] loss: 1298.051
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004954052596990249,
 'learning_rate_Hydroxylation-K': 0.002119997506474632,
 'learning_rate_Hydroxylation-P': 0.00994355830225411,
 'log_base': 1.1281549519783087,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1452693002,
 'sample_weights': [1.8411060114230389, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.792193065251702,
 'weight_decay_Hydroxylation-K': 6.8943177631932295,
 'weight_decay_Hydroxylation-P': 0.10704816149951574}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4484.453
[2,     1] loss: 4504.903
[3,     1] loss: 4476.989
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004622434669505123,
 'learning_rate_Hydroxylation-K': 0.006641334847418327,
 'learning_rate_Hydroxylation-P': 0.006359095236116598,
 'log_base': 1.117754627794249,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4281466215,
 'sample_weights': [13.844704918314761, 1.7306547839929267],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.290346878937848,
 'weight_decay_Hydroxylation-K': 6.544516528835367,
 'weight_decay_Hydroxylation-P': 0.32015918583001735}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4872.434
[2,     1] loss: 4856.762
[3,     1] loss: 4932.863
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006572084309684114,
 'learning_rate_Hydroxylation-K': 0.009615758340991983,
 'learning_rate_Hydroxylation-P': 0.006972971206008146,
 'log_base': 2.800375423641306,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2520774359,
 'sample_weights': [14.996541573053092, 1.8746399125068793],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.928127280138588,
 'weight_decay_Hydroxylation-K': 5.9167531429418805,
 'weight_decay_Hydroxylation-P': 1.7015322209628971}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.709
[2,     1] loss: 1263.291
[3,     1] loss: 1249.011
[4,     1] loss: 1251.466
[5,     1] loss: 1251.512
[6,     1] loss: 1251.722
[7,     1] loss: 1251.497
[8,     1] loss: 1248.734
[9,     1] loss: 1250.226
[10,     1] loss: 1250.743
[11,     1] loss: 1245.839
[12,     1] loss: 1247.877
[13,     1] loss: 1241.753
[14,     1] loss: 1235.643
[15,     1] loss: 1222.675
[16,     1] loss: 1212.719
[17,     1] loss: 1188.671
[18,     1] loss: 1157.152
[19,     1] loss: 1131.444
[20,     1] loss: 1094.924
[21,     1] loss: 1065.869
[22,     1] loss: 1063.279
[23,     1] loss: 1067.237
[24,     1] loss: 982.961
[25,     1] loss: 1050.229
[26,     1] loss: 962.547
[27,     1] loss: 1083.781
[28,     1] loss: 948.064
[29,     1] loss: 945.609
[30,     1] loss: 992.958
[31,     1] loss: 959.474
[32,     1] loss: 988.318
[33,     1] loss: 948.539
[34,     1] loss: 921.379
[35,     1] loss: 994.645
[36,     1] loss: 896.743
[37,     1] loss: 939.399
[38,     1] loss: 888.550
[39,     1] loss: 898.441
[40,     1] loss: 896.599
[41,     1] loss: 871.633
[42,     1] loss: 877.215
[43,     1] loss: 911.227
[44,     1] loss: 853.418
[45,     1] loss: 803.752
[46,     1] loss: 807.713
[47,     1] loss: 801.121
[48,     1] loss: 809.686
[49,     1] loss: 857.877
[50,     1] loss: 890.511
[51,     1] loss: 806.915
[52,     1] loss: 807.919
[53,     1] loss: 758.191
[54,     1] loss: 713.520
[55,     1] loss: 771.244
[56,     1] loss: 768.670
[57,     1] loss: 785.576
[58,     1] loss: 738.927
[59,     1] loss: 703.897
[60,     1] loss: 731.547
[61,     1] loss: 795.106
[62,     1] loss: 832.136
[63,     1] loss: 697.036
[64,     1] loss: 799.630
[65,     1] loss: 818.852
[66,     1] loss: 719.745
[67,     1] loss: 760.267
[68,     1] loss: 674.196
[69,     1] loss: 748.452
[70,     1] loss: 657.836
[71,     1] loss: 676.115
[72,     1] loss: 689.494
[73,     1] loss: 803.920
[74,     1] loss: 972.883
[75,     1] loss: 664.388
[76,     1] loss: 870.288
[77,     1] loss: 686.095
[78,     1] loss: 811.604
[79,     1] loss: 738.293
[80,     1] loss: 685.526
[81,     1] loss: 691.699
[82,     1] loss: 628.162
[83,     1] loss: 652.137
[84,     1] loss: 699.424
[85,     1] loss: 643.766
[86,     1] loss: 543.468
[87,     1] loss: 572.338
[88,     1] loss: 637.714
[89,     1] loss: 597.384
[90,     1] loss: 520.559
[91,     1] loss: 522.499
[92,     1] loss: 509.300
[93,     1] loss: 530.770
[94,     1] loss: 1796.957
[95,     1] loss: 1705.521
[96,     1] loss: 1029.917
[97,     1] loss: 1175.087
[98,     1] loss: 1215.035
[99,     1] loss: 1228.556
[100,     1] loss: 1229.484
[101,     1] loss: 1230.526
[102,     1] loss: 1229.134
[103,     1] loss: 1226.485
[104,     1] loss: 1219.366
[105,     1] loss: 1196.426
[106,     1] loss: 1187.388
[107,     1] loss: 1167.794
[108,     1] loss: 1145.118
[109,     1] loss: 1100.290
[110,     1] loss: 1078.760
[111,     1] loss: 1021.857
[112,     1] loss: 1024.641
[113,     1] loss: 985.544
[114,     1] loss: 1002.152
[115,     1] loss: 1008.840
Early stopping applied (best metric=0.4081972539424896)
Finished Training
Total time taken: 19.396546363830566
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.568
[2,     1] loss: 1253.439
[3,     1] loss: 1248.995
[4,     1] loss: 1248.813
[5,     1] loss: 1249.848
[6,     1] loss: 1247.859
[7,     1] loss: 1247.817
[8,     1] loss: 1242.217
[9,     1] loss: 1238.940
[10,     1] loss: 1223.920
[11,     1] loss: 1196.409
[12,     1] loss: 1164.693
[13,     1] loss: 1139.447
[14,     1] loss: 1108.709
[15,     1] loss: 1091.688
[16,     1] loss: 1033.511
[17,     1] loss: 1071.003
[18,     1] loss: 1062.480
[19,     1] loss: 1055.898
[20,     1] loss: 1052.542
[21,     1] loss: 1031.031
[22,     1] loss: 1018.498
[23,     1] loss: 1029.805
[24,     1] loss: 968.841
[25,     1] loss: 1055.716
[26,     1] loss: 957.867
[27,     1] loss: 1016.684
[28,     1] loss: 946.909
[29,     1] loss: 1023.599
[30,     1] loss: 914.754
[31,     1] loss: 993.557
[32,     1] loss: 897.924
[33,     1] loss: 921.507
[34,     1] loss: 907.292
[35,     1] loss: 914.865
[36,     1] loss: 935.388
[37,     1] loss: 940.940
[38,     1] loss: 902.422
[39,     1] loss: 855.208
[40,     1] loss: 889.571
[41,     1] loss: 867.777
[42,     1] loss: 909.566
[43,     1] loss: 859.207
[44,     1] loss: 803.878
[45,     1] loss: 815.644
[46,     1] loss: 952.135
[47,     1] loss: 1316.834
[48,     1] loss: 864.269
[49,     1] loss: 1098.285
[50,     1] loss: 913.811
[51,     1] loss: 944.766
[52,     1] loss: 1025.940
[53,     1] loss: 1011.992
[54,     1] loss: 974.487
[55,     1] loss: 954.111
[56,     1] loss: 930.566
[57,     1] loss: 899.244
[58,     1] loss: 901.702
[59,     1] loss: 894.698
[60,     1] loss: 925.672
[61,     1] loss: 840.477
[62,     1] loss: 828.764
[63,     1] loss: 875.052
[64,     1] loss: 808.320
[65,     1] loss: 878.547
[66,     1] loss: 780.922
[67,     1] loss: 872.952
[68,     1] loss: 761.759
[69,     1] loss: 804.244
[70,     1] loss: 755.315
[71,     1] loss: 749.866
[72,     1] loss: 804.843
[73,     1] loss: 676.607
[74,     1] loss: 793.777
[75,     1] loss: 841.744
[76,     1] loss: 668.707
[77,     1] loss: 789.313
[78,     1] loss: 653.564
[79,     1] loss: 714.665
[80,     1] loss: 732.529
[81,     1] loss: 632.141
[82,     1] loss: 662.337
[83,     1] loss: 821.330
[84,     1] loss: 890.724
[85,     1] loss: 669.403
[86,     1] loss: 828.771
[87,     1] loss: 645.890
[88,     1] loss: 665.134
[89,     1] loss: 629.479
[90,     1] loss: 682.372
[91,     1] loss: 829.153
[92,     1] loss: 530.569
[93,     1] loss: 687.654
[94,     1] loss: 764.928
[95,     1] loss: 581.186
[96,     1] loss: 892.951
[97,     1] loss: 699.853
[98,     1] loss: 784.106
[99,     1] loss: 601.883
[100,     1] loss: 871.911
[101,     1] loss: 618.669
[102,     1] loss: 747.714
[103,     1] loss: 578.359
[104,     1] loss: 715.495
[105,     1] loss: 553.525
[106,     1] loss: 675.208
[107,     1] loss: 536.511
[108,     1] loss: 517.775
[109,     1] loss: 665.445
[110,     1] loss: 559.621
[111,     1] loss: 540.909
[112,     1] loss: 597.514
[113,     1] loss: 586.175
[114,     1] loss: 545.278
[115,     1] loss: 471.540
[116,     1] loss: 487.841
[117,     1] loss: 727.574
[118,     1] loss: 1501.778
[119,     1] loss: 1333.610
[120,     1] loss: 1150.037
[121,     1] loss: 1198.994
[122,     1] loss: 1242.255
[123,     1] loss: 1247.173
[124,     1] loss: 1248.246
[125,     1] loss: 1249.295
[126,     1] loss: 1248.097
[127,     1] loss: 1249.653
[128,     1] loss: 1250.312
[129,     1] loss: 1248.971
[130,     1] loss: 1248.905
Early stopping applied (best metric=0.3141772747039795)
Finished Training
Total time taken: 24.34939694404602
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.338
[2,     1] loss: 1252.622
[3,     1] loss: 1248.833
[4,     1] loss: 1252.491
[5,     1] loss: 1246.676
[6,     1] loss: 1246.517
[7,     1] loss: 1244.392
[8,     1] loss: 1238.123
[9,     1] loss: 1224.387
[10,     1] loss: 1198.590
[11,     1] loss: 1163.826
[12,     1] loss: 1135.741
[13,     1] loss: 1077.846
[14,     1] loss: 1069.047
[15,     1] loss: 1043.186
[16,     1] loss: 1013.974
[17,     1] loss: 956.851
[18,     1] loss: 1023.339
[19,     1] loss: 1081.046
[20,     1] loss: 1000.120
[21,     1] loss: 1005.232
[22,     1] loss: 999.693
[23,     1] loss: 982.432
[24,     1] loss: 939.706
[25,     1] loss: 1042.969
[26,     1] loss: 937.724
[27,     1] loss: 985.816
[28,     1] loss: 908.643
[29,     1] loss: 963.215
[30,     1] loss: 945.016
[31,     1] loss: 889.896
[32,     1] loss: 929.540
[33,     1] loss: 914.347
[34,     1] loss: 894.524
[35,     1] loss: 907.371
[36,     1] loss: 854.827
[37,     1] loss: 873.564
[38,     1] loss: 853.840
[39,     1] loss: 828.816
[40,     1] loss: 806.572
[41,     1] loss: 803.240
[42,     1] loss: 928.088
[43,     1] loss: 1036.234
[44,     1] loss: 816.731
[45,     1] loss: 956.411
[46,     1] loss: 876.418
[47,     1] loss: 849.412
[48,     1] loss: 902.922
[49,     1] loss: 834.322
[50,     1] loss: 873.473
[51,     1] loss: 864.558
[52,     1] loss: 789.436
[53,     1] loss: 825.212
[54,     1] loss: 747.229
[55,     1] loss: 785.974
[56,     1] loss: 780.229
[57,     1] loss: 863.636
[58,     1] loss: 839.874
[59,     1] loss: 703.642
[60,     1] loss: 799.529
[61,     1] loss: 697.786
[62,     1] loss: 821.479
[63,     1] loss: 761.709
[64,     1] loss: 690.947
[65,     1] loss: 744.806
[66,     1] loss: 640.824
[67,     1] loss: 694.931
[68,     1] loss: 666.970
[69,     1] loss: 614.944
[70,     1] loss: 612.310
[71,     1] loss: 620.453
[72,     1] loss: 810.581
[73,     1] loss: 1035.270
[74,     1] loss: 950.580
[75,     1] loss: 1013.192
[76,     1] loss: 979.992
[77,     1] loss: 957.309
[78,     1] loss: 880.098
[79,     1] loss: 895.530
[80,     1] loss: 830.976
[81,     1] loss: 858.665
[82,     1] loss: 858.050
[83,     1] loss: 806.965
[84,     1] loss: 831.734
[85,     1] loss: 751.084
[86,     1] loss: 791.980
[87,     1] loss: 776.037
[88,     1] loss: 690.827
[89,     1] loss: 696.677
[90,     1] loss: 635.459
[91,     1] loss: 681.940
[92,     1] loss: 679.132
[93,     1] loss: 753.775
[94,     1] loss: 587.563
[95,     1] loss: 679.618
[96,     1] loss: 692.456
[97,     1] loss: 508.572
[98,     1] loss: 817.541
[99,     1] loss: 1229.818
[100,     1] loss: 869.214
[101,     1] loss: 769.457
[102,     1] loss: 1072.028
[103,     1] loss: 867.122
[104,     1] loss: 873.261
[105,     1] loss: 816.438
[106,     1] loss: 775.836
[107,     1] loss: 776.793
[108,     1] loss: 774.768
[109,     1] loss: 836.163
[110,     1] loss: 719.506
[111,     1] loss: 729.861
Early stopping applied (best metric=0.3795124590396881)
Finished Training
Total time taken: 20.08134150505066
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.695
[2,     1] loss: 1255.455
[3,     1] loss: 1247.271
[4,     1] loss: 1250.796
[5,     1] loss: 1247.079
[6,     1] loss: 1246.534
[7,     1] loss: 1245.914
[8,     1] loss: 1234.760
[9,     1] loss: 1216.963
[10,     1] loss: 1196.812
[11,     1] loss: 1156.986
[12,     1] loss: 1105.733
[13,     1] loss: 1096.524
[14,     1] loss: 1114.576
[15,     1] loss: 1023.165
[16,     1] loss: 1009.387
[17,     1] loss: 1019.035
[18,     1] loss: 966.794
[19,     1] loss: 1002.480
[20,     1] loss: 991.098
[21,     1] loss: 924.399
[22,     1] loss: 943.978
[23,     1] loss: 946.179
[24,     1] loss: 945.816
[25,     1] loss: 899.504
[26,     1] loss: 881.540
[27,     1] loss: 870.532
[28,     1] loss: 907.474
[29,     1] loss: 855.017
[30,     1] loss: 877.266
[31,     1] loss: 894.477
[32,     1] loss: 950.742
[33,     1] loss: 908.091
[34,     1] loss: 868.047
[35,     1] loss: 884.802
[36,     1] loss: 846.024
[37,     1] loss: 820.713
[38,     1] loss: 780.893
[39,     1] loss: 875.954
[40,     1] loss: 822.243
[41,     1] loss: 817.614
[42,     1] loss: 793.662
[43,     1] loss: 802.795
[44,     1] loss: 833.156
[45,     1] loss: 774.424
[46,     1] loss: 725.445
[47,     1] loss: 808.107
[48,     1] loss: 775.553
[49,     1] loss: 729.826
[50,     1] loss: 702.172
[51,     1] loss: 723.281
[52,     1] loss: 680.377
[53,     1] loss: 698.787
[54,     1] loss: 606.842
[55,     1] loss: 686.505
[56,     1] loss: 1760.768
[57,     1] loss: 1544.128
[58,     1] loss: 1056.833
[59,     1] loss: 905.041
[60,     1] loss: 1010.919
[61,     1] loss: 1120.764
[62,     1] loss: 1128.998
[63,     1] loss: 1095.300
[64,     1] loss: 1123.714
[65,     1] loss: 1111.751
[66,     1] loss: 1107.641
[67,     1] loss: 1092.153
[68,     1] loss: 1081.276
[69,     1] loss: 1058.587
[70,     1] loss: 1059.145
[71,     1] loss: 1056.841
[72,     1] loss: 1006.914
[73,     1] loss: 997.847
[74,     1] loss: 949.973
[75,     1] loss: 935.891
[76,     1] loss: 936.112
[77,     1] loss: 922.648
[78,     1] loss: 931.152
[79,     1] loss: 854.521
[80,     1] loss: 905.810
[81,     1] loss: 884.098
[82,     1] loss: 845.410
[83,     1] loss: 844.110
[84,     1] loss: 843.102
[85,     1] loss: 839.857
[86,     1] loss: 830.810
[87,     1] loss: 881.146
[88,     1] loss: 857.691
[89,     1] loss: 805.726
[90,     1] loss: 791.223
[91,     1] loss: 847.706
[92,     1] loss: 834.062
[93,     1] loss: 792.958
[94,     1] loss: 785.325
[95,     1] loss: 748.612
[96,     1] loss: 727.823
[97,     1] loss: 730.908
[98,     1] loss: 808.113
[99,     1] loss: 1098.461
[100,     1] loss: 897.867
[101,     1] loss: 882.826
Early stopping applied (best metric=0.40527603030204773)
Finished Training
Total time taken: 16.719609022140503
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1251.010
[2,     1] loss: 1250.888
[3,     1] loss: 1250.096
[4,     1] loss: 1248.679
[5,     1] loss: 1245.573
[6,     1] loss: 1249.524
[7,     1] loss: 1235.208
[8,     1] loss: 1222.874
[9,     1] loss: 1189.973
[10,     1] loss: 1165.019
[11,     1] loss: 1152.815
[12,     1] loss: 1123.413
[13,     1] loss: 1077.849
[14,     1] loss: 1062.387
[15,     1] loss: 1056.350
[16,     1] loss: 999.033
[17,     1] loss: 1013.762
[18,     1] loss: 1000.921
[19,     1] loss: 1006.446
[20,     1] loss: 1022.178
[21,     1] loss: 979.702
[22,     1] loss: 986.372
[23,     1] loss: 994.694
[24,     1] loss: 985.958
[25,     1] loss: 937.886
[26,     1] loss: 929.723
[27,     1] loss: 964.859
[28,     1] loss: 961.829
[29,     1] loss: 1039.139
[30,     1] loss: 958.986
[31,     1] loss: 954.337
[32,     1] loss: 905.460
[33,     1] loss: 934.693
[34,     1] loss: 883.108
[35,     1] loss: 924.898
[36,     1] loss: 926.579
[37,     1] loss: 864.910
[38,     1] loss: 852.353
[39,     1] loss: 852.842
[40,     1] loss: 883.040
[41,     1] loss: 833.270
[42,     1] loss: 854.352
[43,     1] loss: 823.177
[44,     1] loss: 826.227
[45,     1] loss: 843.664
[46,     1] loss: 797.718
[47,     1] loss: 781.752
[48,     1] loss: 767.674
[49,     1] loss: 844.628
[50,     1] loss: 921.175
[51,     1] loss: 800.905
[52,     1] loss: 764.742
[53,     1] loss: 827.558
[54,     1] loss: 721.423
[55,     1] loss: 743.731
[56,     1] loss: 698.424
[57,     1] loss: 685.030
[58,     1] loss: 671.298
[59,     1] loss: 675.330
[60,     1] loss: 712.820
[61,     1] loss: 1324.265
[62,     1] loss: 850.088
[63,     1] loss: 888.541
[64,     1] loss: 875.065
[65,     1] loss: 946.371
[66,     1] loss: 939.990
[67,     1] loss: 843.671
[68,     1] loss: 816.021
[69,     1] loss: 923.690
[70,     1] loss: 760.889
[71,     1] loss: 804.769
[72,     1] loss: 826.168
[73,     1] loss: 744.693
[74,     1] loss: 846.786
[75,     1] loss: 719.702
[76,     1] loss: 815.378
[77,     1] loss: 665.567
[78,     1] loss: 714.645
[79,     1] loss: 658.394
[80,     1] loss: 674.518
[81,     1] loss: 638.483
[82,     1] loss: 574.525
[83,     1] loss: 666.233
[84,     1] loss: 583.172
[85,     1] loss: 681.220
[86,     1] loss: 748.685
[87,     1] loss: 845.053
[88,     1] loss: 615.183
[89,     1] loss: 887.582
[90,     1] loss: 669.792
[91,     1] loss: 780.971
[92,     1] loss: 591.721
[93,     1] loss: 757.040
[94,     1] loss: 638.031
[95,     1] loss: 558.493
[96,     1] loss: 635.543
[97,     1] loss: 619.445
[98,     1] loss: 496.068
[99,     1] loss: 564.088
[100,     1] loss: 573.582
[101,     1] loss: 485.985
[102,     1] loss: 557.838
[103,     1] loss: 961.041
[104,     1] loss: 630.949
[105,     1] loss: 615.182
[106,     1] loss: 609.382
[107,     1] loss: 565.293
[108,     1] loss: 620.222
[109,     1] loss: 685.040
[110,     1] loss: 501.776
[111,     1] loss: 586.644
[112,     1] loss: 750.986
[113,     1] loss: 540.947
[114,     1] loss: 870.413
[115,     1] loss: 680.117
[116,     1] loss: 742.540
[117,     1] loss: 596.272
[118,     1] loss: 752.553
[119,     1] loss: 541.116
[120,     1] loss: 655.533
[121,     1] loss: 674.427
[122,     1] loss: 521.819
[123,     1] loss: 629.453
[124,     1] loss: 547.047
[125,     1] loss: 612.931
[126,     1] loss: 486.186
[127,     1] loss: 605.938
[128,     1] loss: 634.837
[129,     1] loss: 648.261
[130,     1] loss: 469.717
[131,     1] loss: 621.165
[132,     1] loss: 582.467
[133,     1] loss: 499.259
[134,     1] loss: 738.279
[135,     1] loss: 536.120
[136,     1] loss: 533.657
[137,     1] loss: 668.263
[138,     1] loss: 497.943
[139,     1] loss: 587.408
[140,     1] loss: 614.175
[141,     1] loss: 420.111
[142,     1] loss: 616.653
[143,     1] loss: 557.277
[144,     1] loss: 451.833
[145,     1] loss: 486.762
[146,     1] loss: 589.567
[147,     1] loss: 659.004
Early stopping applied (best metric=0.25668543577194214)
Finished Training
Total time taken: 22.965553045272827
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.123
[2,     1] loss: 1252.248
[3,     1] loss: 1254.702
[4,     1] loss: 1249.600
[5,     1] loss: 1249.870
[6,     1] loss: 1251.733
[7,     1] loss: 1250.359
[8,     1] loss: 1240.186
[9,     1] loss: 1239.070
[10,     1] loss: 1230.779
[11,     1] loss: 1200.861
[12,     1] loss: 1185.758
[13,     1] loss: 1151.740
[14,     1] loss: 1130.101
[15,     1] loss: 1080.337
[16,     1] loss: 1038.017
[17,     1] loss: 1042.625
[18,     1] loss: 1059.185
[19,     1] loss: 1027.748
[20,     1] loss: 1000.685
[21,     1] loss: 975.915
[22,     1] loss: 1024.139
[23,     1] loss: 1000.371
[24,     1] loss: 996.186
[25,     1] loss: 952.882
[26,     1] loss: 964.203
[27,     1] loss: 929.438
[28,     1] loss: 934.059
[29,     1] loss: 988.692
[30,     1] loss: 973.921
[31,     1] loss: 915.093
[32,     1] loss: 935.470
[33,     1] loss: 883.065
[34,     1] loss: 880.750
[35,     1] loss: 863.371
[36,     1] loss: 920.906
[37,     1] loss: 835.812
[38,     1] loss: 814.810
[39,     1] loss: 890.245
[40,     1] loss: 871.389
[41,     1] loss: 861.863
[42,     1] loss: 802.921
[43,     1] loss: 959.100
[44,     1] loss: 1008.659
[45,     1] loss: 894.744
[46,     1] loss: 881.618
[47,     1] loss: 841.345
[48,     1] loss: 950.554
[49,     1] loss: 808.495
[50,     1] loss: 846.339
[51,     1] loss: 809.427
[52,     1] loss: 778.430
[53,     1] loss: 765.406
[54,     1] loss: 746.199
[55,     1] loss: 822.686
[56,     1] loss: 822.984
[57,     1] loss: 767.265
[58,     1] loss: 720.822
[59,     1] loss: 728.821
[60,     1] loss: 672.092
[61,     1] loss: 697.245
[62,     1] loss: 679.385
[63,     1] loss: 670.141
[64,     1] loss: 684.066
[65,     1] loss: 1313.044
[66,     1] loss: 1658.601
[67,     1] loss: 1194.042
[68,     1] loss: 1073.727
[69,     1] loss: 1149.988
[70,     1] loss: 1141.684
[71,     1] loss: 1135.788
[72,     1] loss: 1170.001
[73,     1] loss: 1208.493
[74,     1] loss: 1172.247
[75,     1] loss: 1169.832
[76,     1] loss: 1158.145
[77,     1] loss: 1161.781
[78,     1] loss: 1147.408
[79,     1] loss: 1132.885
[80,     1] loss: 1117.161
[81,     1] loss: 1123.785
[82,     1] loss: 1106.919
[83,     1] loss: 1099.760
[84,     1] loss: 1066.612
[85,     1] loss: 1084.536
[86,     1] loss: 1055.792
[87,     1] loss: 1023.328
[88,     1] loss: 980.352
[89,     1] loss: 946.695
[90,     1] loss: 996.902
[91,     1] loss: 965.887
[92,     1] loss: 977.069
[93,     1] loss: 971.670
[94,     1] loss: 952.321
[95,     1] loss: 962.842
[96,     1] loss: 946.849
[97,     1] loss: 942.581
[98,     1] loss: 926.917
[99,     1] loss: 935.724
[100,     1] loss: 922.708
[101,     1] loss: 880.257
[102,     1] loss: 888.048
[103,     1] loss: 990.049
[104,     1] loss: 850.312
[105,     1] loss: 898.374
[106,     1] loss: 877.408
[107,     1] loss: 846.318
[108,     1] loss: 774.156
[109,     1] loss: 780.045
[110,     1] loss: 779.780
[111,     1] loss: 946.292
[112,     1] loss: 1579.125
[113,     1] loss: 876.116
Early stopping applied (best metric=0.2949085235595703)
Finished Training
Total time taken: 18.438035249710083
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.801
[2,     1] loss: 1254.255
[3,     1] loss: 1249.398
[4,     1] loss: 1247.552
[5,     1] loss: 1245.494
[6,     1] loss: 1252.854
[7,     1] loss: 1243.809
[8,     1] loss: 1246.640
[9,     1] loss: 1239.761
[10,     1] loss: 1227.969
[11,     1] loss: 1209.834
[12,     1] loss: 1177.327
[13,     1] loss: 1151.467
[14,     1] loss: 1115.803
[15,     1] loss: 1070.270
[16,     1] loss: 1076.865
[17,     1] loss: 1046.679
[18,     1] loss: 1036.896
[19,     1] loss: 1096.890
[20,     1] loss: 1011.592
[21,     1] loss: 1082.179
[22,     1] loss: 1019.697
[23,     1] loss: 1045.203
[24,     1] loss: 1011.830
[25,     1] loss: 991.750
[26,     1] loss: 1006.322
[27,     1] loss: 944.225
[28,     1] loss: 944.998
[29,     1] loss: 957.599
[30,     1] loss: 921.868
[31,     1] loss: 912.699
[32,     1] loss: 895.488
[33,     1] loss: 892.045
[34,     1] loss: 918.160
[35,     1] loss: 898.080
[36,     1] loss: 899.615
[37,     1] loss: 961.157
[38,     1] loss: 852.322
[39,     1] loss: 874.736
[40,     1] loss: 834.992
[41,     1] loss: 889.150
[42,     1] loss: 898.908
[43,     1] loss: 815.041
[44,     1] loss: 943.701
[45,     1] loss: 843.023
[46,     1] loss: 819.734
[47,     1] loss: 857.763
[48,     1] loss: 775.421
[49,     1] loss: 800.303
[50,     1] loss: 822.818
[51,     1] loss: 702.147
[52,     1] loss: 791.453
[53,     1] loss: 787.929
[54,     1] loss: 681.786
[55,     1] loss: 754.049
[56,     1] loss: 745.826
[57,     1] loss: 662.486
[58,     1] loss: 654.724
[59,     1] loss: 752.612
[60,     1] loss: 759.602
[61,     1] loss: 790.956
[62,     1] loss: 636.544
[63,     1] loss: 792.157
[64,     1] loss: 658.545
[65,     1] loss: 726.978
[66,     1] loss: 624.470
[67,     1] loss: 659.013
[68,     1] loss: 681.286
[69,     1] loss: 531.504
[70,     1] loss: 731.129
[71,     1] loss: 762.076
[72,     1] loss: 569.888
[73,     1] loss: 745.615
[74,     1] loss: 592.864
[75,     1] loss: 658.342
[76,     1] loss: 562.417
[77,     1] loss: 514.694
[78,     1] loss: 635.213
[79,     1] loss: 467.871
[80,     1] loss: 510.532
[81,     1] loss: 510.591
[82,     1] loss: 682.599
[83,     1] loss: 528.230
[84,     1] loss: 472.516
[85,     1] loss: 530.172
[86,     1] loss: 577.458
[87,     1] loss: 836.076
[88,     1] loss: 472.003
[89,     1] loss: 550.504
[90,     1] loss: 795.972
[91,     1] loss: 476.292
[92,     1] loss: 876.465
[93,     1] loss: 885.938
[94,     1] loss: 890.752
[95,     1] loss: 786.307
[96,     1] loss: 829.087
[97,     1] loss: 788.381
[98,     1] loss: 636.946
[99,     1] loss: 754.477
[100,     1] loss: 578.467
Early stopping applied (best metric=0.3270513117313385)
Finished Training
Total time taken: 16.9862277507782
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.484
[2,     1] loss: 1259.522
[3,     1] loss: 1253.010
[4,     1] loss: 1251.276
[5,     1] loss: 1252.755
[6,     1] loss: 1247.792
[7,     1] loss: 1252.693
[8,     1] loss: 1249.405
[9,     1] loss: 1251.574
[10,     1] loss: 1250.920
[11,     1] loss: 1247.255
[12,     1] loss: 1248.538
[13,     1] loss: 1247.190
[14,     1] loss: 1246.482
[15,     1] loss: 1245.362
[16,     1] loss: 1242.895
[17,     1] loss: 1235.395
[18,     1] loss: 1227.552
[19,     1] loss: 1213.007
[20,     1] loss: 1198.423
[21,     1] loss: 1175.699
[22,     1] loss: 1153.199
[23,     1] loss: 1134.210
[24,     1] loss: 1123.851
[25,     1] loss: 1102.719
[26,     1] loss: 1033.440
[27,     1] loss: 1069.005
[28,     1] loss: 1063.676
[29,     1] loss: 1093.610
[30,     1] loss: 1024.676
[31,     1] loss: 1182.434
[32,     1] loss: 999.196
[33,     1] loss: 1017.343
[34,     1] loss: 1012.072
[35,     1] loss: 992.401
[36,     1] loss: 1022.076
[37,     1] loss: 987.526
[38,     1] loss: 987.847
[39,     1] loss: 969.622
[40,     1] loss: 965.626
[41,     1] loss: 942.203
[42,     1] loss: 950.573
[43,     1] loss: 894.788
[44,     1] loss: 929.532
[45,     1] loss: 917.348
[46,     1] loss: 948.727
[47,     1] loss: 823.296
[48,     1] loss: 860.217
[49,     1] loss: 841.813
[50,     1] loss: 822.685
[51,     1] loss: 918.800
[52,     1] loss: 930.602
[53,     1] loss: 875.651
[54,     1] loss: 821.214
[55,     1] loss: 820.913
[56,     1] loss: 825.006
[57,     1] loss: 804.100
[58,     1] loss: 760.746
[59,     1] loss: 754.115
[60,     1] loss: 711.791
[61,     1] loss: 706.780
[62,     1] loss: 688.826
[63,     1] loss: 829.627
[64,     1] loss: 2049.509
[65,     1] loss: 1353.338
[66,     1] loss: 923.636
[67,     1] loss: 988.954
[68,     1] loss: 1210.039
[69,     1] loss: 1073.959
[70,     1] loss: 1110.131
[71,     1] loss: 1145.754
[72,     1] loss: 1138.565
[73,     1] loss: 1102.845
[74,     1] loss: 1066.648
[75,     1] loss: 1069.614
[76,     1] loss: 1097.530
[77,     1] loss: 1069.697
[78,     1] loss: 1056.225
[79,     1] loss: 1050.135
[80,     1] loss: 1052.361
[81,     1] loss: 1030.733
[82,     1] loss: 1027.482
[83,     1] loss: 1019.325
[84,     1] loss: 996.197
[85,     1] loss: 956.142
[86,     1] loss: 941.558
[87,     1] loss: 900.800
[88,     1] loss: 894.364
[89,     1] loss: 874.796
[90,     1] loss: 812.931
[91,     1] loss: 836.094
[92,     1] loss: 781.380
[93,     1] loss: 779.668
[94,     1] loss: 838.850
[95,     1] loss: 784.690
[96,     1] loss: 712.325
[97,     1] loss: 810.128
[98,     1] loss: 790.673
[99,     1] loss: 685.886
[100,     1] loss: 853.276
[101,     1] loss: 824.776
[102,     1] loss: 756.846
[103,     1] loss: 697.736
[104,     1] loss: 730.874
[105,     1] loss: 706.121
[106,     1] loss: 684.658
[107,     1] loss: 676.868
[108,     1] loss: 636.436
[109,     1] loss: 652.125
[110,     1] loss: 650.978
[111,     1] loss: 865.552
[112,     1] loss: 721.299
[113,     1] loss: 579.629
[114,     1] loss: 674.706
[115,     1] loss: 604.178
[116,     1] loss: 665.058
[117,     1] loss: 713.468
[118,     1] loss: 573.418
[119,     1] loss: 894.474
[120,     1] loss: 861.189
[121,     1] loss: 765.817
[122,     1] loss: 708.647
[123,     1] loss: 734.197
[124,     1] loss: 614.964
[125,     1] loss: 670.331
[126,     1] loss: 575.248
[127,     1] loss: 787.755
[128,     1] loss: 628.776
[129,     1] loss: 618.775
[130,     1] loss: 605.761
[131,     1] loss: 573.533
[132,     1] loss: 697.002
[133,     1] loss: 461.795
[134,     1] loss: 694.312
[135,     1] loss: 895.470
[136,     1] loss: 585.592
Early stopping applied (best metric=0.4113800823688507)
Finished Training
Total time taken: 22.0610933303833
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.956
[2,     1] loss: 1255.088
[3,     1] loss: 1250.174
[4,     1] loss: 1251.730
[5,     1] loss: 1248.876
[6,     1] loss: 1245.916
[7,     1] loss: 1241.211
[8,     1] loss: 1230.169
[9,     1] loss: 1224.708
[10,     1] loss: 1201.190
[11,     1] loss: 1157.740
[12,     1] loss: 1129.689
[13,     1] loss: 1107.696
[14,     1] loss: 1064.245
[15,     1] loss: 1061.627
[16,     1] loss: 1042.297
[17,     1] loss: 1030.438
[18,     1] loss: 1063.541
[19,     1] loss: 1022.682
[20,     1] loss: 1092.305
[21,     1] loss: 1026.091
[22,     1] loss: 1043.251
[23,     1] loss: 1050.580
[24,     1] loss: 1018.005
[25,     1] loss: 1030.164
[26,     1] loss: 1004.998
[27,     1] loss: 961.990
[28,     1] loss: 949.444
[29,     1] loss: 943.974
[30,     1] loss: 949.839
[31,     1] loss: 978.997
[32,     1] loss: 899.336
[33,     1] loss: 925.895
[34,     1] loss: 897.961
[35,     1] loss: 896.754
[36,     1] loss: 891.404
[37,     1] loss: 931.072
[38,     1] loss: 900.191
[39,     1] loss: 887.112
[40,     1] loss: 840.310
[41,     1] loss: 873.049
[42,     1] loss: 846.936
[43,     1] loss: 838.133
[44,     1] loss: 815.444
[45,     1] loss: 858.073
[46,     1] loss: 843.184
[47,     1] loss: 791.636
[48,     1] loss: 751.935
[49,     1] loss: 740.934
[50,     1] loss: 764.918
[51,     1] loss: 1000.270
[52,     1] loss: 1437.974
[53,     1] loss: 859.368
[54,     1] loss: 977.696
[55,     1] loss: 961.966
[56,     1] loss: 979.775
[57,     1] loss: 967.910
[58,     1] loss: 970.954
[59,     1] loss: 934.990
[60,     1] loss: 928.110
[61,     1] loss: 879.228
[62,     1] loss: 918.629
[63,     1] loss: 919.121
[64,     1] loss: 850.557
[65,     1] loss: 862.020
[66,     1] loss: 880.798
[67,     1] loss: 815.664
[68,     1] loss: 801.856
[69,     1] loss: 798.494
[70,     1] loss: 791.172
[71,     1] loss: 756.428
[72,     1] loss: 731.807
[73,     1] loss: 728.748
[74,     1] loss: 711.872
[75,     1] loss: 710.993
[76,     1] loss: 701.486
[77,     1] loss: 821.313
[78,     1] loss: 995.511
[79,     1] loss: 818.284
[80,     1] loss: 895.412
[81,     1] loss: 809.328
[82,     1] loss: 851.240
[83,     1] loss: 741.071
[84,     1] loss: 785.031
[85,     1] loss: 729.617
[86,     1] loss: 694.959
Early stopping applied (best metric=0.3656909763813019)
Finished Training
Total time taken: 14.67110538482666
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1254.399
[2,     1] loss: 1253.863
[3,     1] loss: 1251.062
[4,     1] loss: 1245.682
[5,     1] loss: 1234.271
[6,     1] loss: 1214.019
[7,     1] loss: 1181.278
[8,     1] loss: 1132.323
[9,     1] loss: 1106.397
[10,     1] loss: 1106.106
[11,     1] loss: 1048.403
[12,     1] loss: 1047.195
[13,     1] loss: 998.184
[14,     1] loss: 988.304
[15,     1] loss: 998.970
[16,     1] loss: 976.399
[17,     1] loss: 1000.742
[18,     1] loss: 921.220
[19,     1] loss: 1005.529
[20,     1] loss: 926.661
[21,     1] loss: 963.816
[22,     1] loss: 962.039
[23,     1] loss: 969.922
[24,     1] loss: 877.994
[25,     1] loss: 962.288
[26,     1] loss: 882.423
[27,     1] loss: 985.178
[28,     1] loss: 836.088
[29,     1] loss: 872.558
[30,     1] loss: 847.633
[31,     1] loss: 863.908
[32,     1] loss: 801.013
[33,     1] loss: 803.892
[34,     1] loss: 861.085
[35,     1] loss: 826.123
[36,     1] loss: 808.395
[37,     1] loss: 779.201
[38,     1] loss: 863.170
[39,     1] loss: 798.523
[40,     1] loss: 803.020
[41,     1] loss: 852.561
[42,     1] loss: 861.579
[43,     1] loss: 769.114
[44,     1] loss: 788.859
[45,     1] loss: 742.576
[46,     1] loss: 720.331
[47,     1] loss: 712.757
[48,     1] loss: 675.647
[49,     1] loss: 655.258
[50,     1] loss: 638.930
[51,     1] loss: 791.505
[52,     1] loss: 1381.171
[53,     1] loss: 705.654
[54,     1] loss: 987.215
[55,     1] loss: 849.721
[56,     1] loss: 876.143
[57,     1] loss: 930.506
[58,     1] loss: 939.402
[59,     1] loss: 849.691
[60,     1] loss: 802.779
[61,     1] loss: 910.127
[62,     1] loss: 868.581
[63,     1] loss: 814.252
[64,     1] loss: 818.735
[65,     1] loss: 779.902
[66,     1] loss: 737.053
[67,     1] loss: 768.787
[68,     1] loss: 718.991
[69,     1] loss: 694.590
[70,     1] loss: 681.762
[71,     1] loss: 665.355
[72,     1] loss: 670.596
[73,     1] loss: 617.519
[74,     1] loss: 580.303
[75,     1] loss: 608.829
[76,     1] loss: 589.141
[77,     1] loss: 968.393
[78,     1] loss: 1148.348
[79,     1] loss: 691.923
[80,     1] loss: 900.709
[81,     1] loss: 813.740
[82,     1] loss: 790.115
[83,     1] loss: 806.990
[84,     1] loss: 761.586
[85,     1] loss: 805.921
[86,     1] loss: 783.357
[87,     1] loss: 722.195
[88,     1] loss: 711.795
[89,     1] loss: 712.496
[90,     1] loss: 701.974
[91,     1] loss: 684.714
[92,     1] loss: 685.519
[93,     1] loss: 705.620
Early stopping applied (best metric=0.37287405133247375)
Finished Training
Total time taken: 18.486583471298218
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.582
[2,     1] loss: 1249.990
[3,     1] loss: 1248.225
[4,     1] loss: 1253.216
[5,     1] loss: 1244.936
[6,     1] loss: 1241.905
[7,     1] loss: 1230.392
[8,     1] loss: 1204.064
[9,     1] loss: 1175.990
[10,     1] loss: 1115.259
[11,     1] loss: 1086.990
[12,     1] loss: 1103.893
[13,     1] loss: 1058.867
[14,     1] loss: 1032.013
[15,     1] loss: 1041.104
[16,     1] loss: 1026.921
[17,     1] loss: 996.444
[18,     1] loss: 1019.921
[19,     1] loss: 1036.014
[20,     1] loss: 988.283
[21,     1] loss: 997.772
[22,     1] loss: 978.518
[23,     1] loss: 963.774
[24,     1] loss: 923.900
[25,     1] loss: 941.542
[26,     1] loss: 918.596
[27,     1] loss: 886.863
[28,     1] loss: 913.913
[29,     1] loss: 899.252
[30,     1] loss: 891.018
[31,     1] loss: 862.713
[32,     1] loss: 846.746
[33,     1] loss: 869.694
[34,     1] loss: 1088.443
[35,     1] loss: 1017.779
[36,     1] loss: 885.226
[37,     1] loss: 939.066
[38,     1] loss: 963.050
[39,     1] loss: 956.977
[40,     1] loss: 908.399
[41,     1] loss: 916.774
[42,     1] loss: 919.122
[43,     1] loss: 838.396
[44,     1] loss: 859.753
[45,     1] loss: 861.186
[46,     1] loss: 842.056
[47,     1] loss: 840.420
[48,     1] loss: 778.590
[49,     1] loss: 829.573
[50,     1] loss: 772.055
[51,     1] loss: 808.307
[52,     1] loss: 712.101
[53,     1] loss: 742.075
[54,     1] loss: 728.011
[55,     1] loss: 676.855
[56,     1] loss: 728.213
[57,     1] loss: 841.444
[58,     1] loss: 743.908
[59,     1] loss: 702.555
[60,     1] loss: 737.349
[61,     1] loss: 700.576
[62,     1] loss: 664.289
[63,     1] loss: 674.202
[64,     1] loss: 977.198
[65,     1] loss: 1132.893
[66,     1] loss: 740.360
[67,     1] loss: 841.496
[68,     1] loss: 903.865
[69,     1] loss: 821.209
[70,     1] loss: 839.753
[71,     1] loss: 793.349
[72,     1] loss: 761.689
[73,     1] loss: 729.759
[74,     1] loss: 739.228
[75,     1] loss: 689.508
[76,     1] loss: 674.653
[77,     1] loss: 695.514
[78,     1] loss: 605.675
[79,     1] loss: 645.585
[80,     1] loss: 644.003
[81,     1] loss: 606.049
[82,     1] loss: 595.501
[83,     1] loss: 627.792
[84,     1] loss: 688.914
[85,     1] loss: 616.368
[86,     1] loss: 488.408
[87,     1] loss: 511.015
[88,     1] loss: 597.140
[89,     1] loss: 803.470
[90,     1] loss: 656.422
[91,     1] loss: 567.070
[92,     1] loss: 515.065
[93,     1] loss: 463.493
[94,     1] loss: 532.628
[95,     1] loss: 557.296
[96,     1] loss: 1195.751
[97,     1] loss: 1717.978
[98,     1] loss: 1187.838
[99,     1] loss: 1190.623
[100,     1] loss: 1242.260
[101,     1] loss: 1246.922
[102,     1] loss: 1250.929
[103,     1] loss: 1251.142
[104,     1] loss: 1249.816
[105,     1] loss: 1251.153
[106,     1] loss: 1250.192
[107,     1] loss: 1249.838
[108,     1] loss: 1249.517
[109,     1] loss: 1249.053
[110,     1] loss: 1250.369
[111,     1] loss: 1250.028
[112,     1] loss: 1249.154
[113,     1] loss: 1248.956
[114,     1] loss: 1250.422
[115,     1] loss: 1249.144
[116,     1] loss: 1249.765
[117,     1] loss: 1249.462
[118,     1] loss: 1249.568
[119,     1] loss: 1249.298
[120,     1] loss: 1250.542
[121,     1] loss: 1249.281
Early stopping applied (best metric=0.36910560727119446)
Finished Training
Total time taken: 23.60568618774414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.817
[2,     1] loss: 1253.634
[3,     1] loss: 1248.008
[4,     1] loss: 1247.358
[5,     1] loss: 1251.385
[6,     1] loss: 1246.692
[7,     1] loss: 1242.105
[8,     1] loss: 1239.740
[9,     1] loss: 1226.933
[10,     1] loss: 1207.664
[11,     1] loss: 1170.100
[12,     1] loss: 1129.677
[13,     1] loss: 1114.522
[14,     1] loss: 1079.445
[15,     1] loss: 1084.179
[16,     1] loss: 993.270
[17,     1] loss: 1035.608
[18,     1] loss: 998.144
[19,     1] loss: 1011.579
[20,     1] loss: 1007.724
[21,     1] loss: 1067.976
[22,     1] loss: 1031.266
[23,     1] loss: 959.249
[24,     1] loss: 1037.718
[25,     1] loss: 979.924
[26,     1] loss: 985.468
[27,     1] loss: 988.054
[28,     1] loss: 964.772
[29,     1] loss: 977.418
[30,     1] loss: 962.834
[31,     1] loss: 924.908
[32,     1] loss: 930.225
[33,     1] loss: 907.229
[34,     1] loss: 904.159
[35,     1] loss: 926.693
[36,     1] loss: 875.288
[37,     1] loss: 862.995
[38,     1] loss: 863.014
[39,     1] loss: 902.187
[40,     1] loss: 934.799
[41,     1] loss: 903.983
[42,     1] loss: 843.323
[43,     1] loss: 859.369
[44,     1] loss: 777.020
[45,     1] loss: 798.901
[46,     1] loss: 789.466
[47,     1] loss: 776.740
[48,     1] loss: 736.007
[49,     1] loss: 816.856
[50,     1] loss: 880.010
[51,     1] loss: 1207.739
[52,     1] loss: 754.679
[53,     1] loss: 1006.881
[54,     1] loss: 915.492
[55,     1] loss: 861.506
[56,     1] loss: 929.391
[57,     1] loss: 918.409
[58,     1] loss: 854.610
[59,     1] loss: 872.471
[60,     1] loss: 910.860
[61,     1] loss: 822.413
[62,     1] loss: 787.571
[63,     1] loss: 787.719
[64,     1] loss: 804.102
[65,     1] loss: 731.808
[66,     1] loss: 701.248
[67,     1] loss: 741.768
[68,     1] loss: 675.849
[69,     1] loss: 670.367
[70,     1] loss: 690.638
[71,     1] loss: 841.984
[72,     1] loss: 822.576
[73,     1] loss: 580.742
[74,     1] loss: 867.898
[75,     1] loss: 806.832
[76,     1] loss: 703.916
[77,     1] loss: 692.452
[78,     1] loss: 710.682
[79,     1] loss: 674.244
[80,     1] loss: 628.730
[81,     1] loss: 774.491
[82,     1] loss: 808.843
[83,     1] loss: 668.798
[84,     1] loss: 798.151
[85,     1] loss: 581.555
[86,     1] loss: 692.000
[87,     1] loss: 603.069
[88,     1] loss: 780.041
[89,     1] loss: 620.624
[90,     1] loss: 663.359
[91,     1] loss: 666.456
[92,     1] loss: 539.004
[93,     1] loss: 563.618
[94,     1] loss: 560.532
[95,     1] loss: 486.199
[96,     1] loss: 515.163
Early stopping applied (best metric=0.37767112255096436)
Finished Training
Total time taken: 17.882163286209106
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.060
[2,     1] loss: 1251.216
[3,     1] loss: 1247.853
[4,     1] loss: 1248.902
[5,     1] loss: 1247.414
[6,     1] loss: 1248.196
[7,     1] loss: 1250.126
[8,     1] loss: 1241.047
[9,     1] loss: 1227.400
[10,     1] loss: 1208.898
[11,     1] loss: 1172.040
[12,     1] loss: 1130.194
[13,     1] loss: 1095.367
[14,     1] loss: 1091.644
[15,     1] loss: 1057.213
[16,     1] loss: 1008.813
[17,     1] loss: 1068.492
[18,     1] loss: 1002.624
[19,     1] loss: 1097.466
[20,     1] loss: 1013.333
[21,     1] loss: 1022.758
[22,     1] loss: 1063.783
[23,     1] loss: 1017.227
[24,     1] loss: 1001.374
[25,     1] loss: 961.765
[26,     1] loss: 990.646
[27,     1] loss: 967.209
[28,     1] loss: 924.276
[29,     1] loss: 1020.954
[30,     1] loss: 914.767
[31,     1] loss: 879.570
[32,     1] loss: 870.412
[33,     1] loss: 877.322
[34,     1] loss: 883.099
[35,     1] loss: 834.531
[36,     1] loss: 816.604
[37,     1] loss: 792.215
[38,     1] loss: 809.622
[39,     1] loss: 897.168
[40,     1] loss: 1303.873
[41,     1] loss: 874.185
[42,     1] loss: 966.136
[43,     1] loss: 866.912
[44,     1] loss: 932.270
[45,     1] loss: 926.190
[46,     1] loss: 904.614
[47,     1] loss: 876.769
[48,     1] loss: 878.726
[49,     1] loss: 846.214
[50,     1] loss: 813.768
[51,     1] loss: 792.803
[52,     1] loss: 788.242
[53,     1] loss: 789.046
[54,     1] loss: 804.052
[55,     1] loss: 758.730
[56,     1] loss: 714.872
[57,     1] loss: 721.958
[58,     1] loss: 712.205
[59,     1] loss: 714.848
[60,     1] loss: 831.682
[61,     1] loss: 823.778
[62,     1] loss: 764.366
[63,     1] loss: 759.226
[64,     1] loss: 733.858
[65,     1] loss: 780.404
[66,     1] loss: 727.546
[67,     1] loss: 743.702
[68,     1] loss: 682.095
[69,     1] loss: 669.892
[70,     1] loss: 709.208
[71,     1] loss: 623.573
[72,     1] loss: 724.966
[73,     1] loss: 800.263
[74,     1] loss: 600.880
[75,     1] loss: 829.349
[76,     1] loss: 646.191
[77,     1] loss: 741.211
[78,     1] loss: 610.887
[79,     1] loss: 697.649
[80,     1] loss: 612.305
[81,     1] loss: 671.518
[82,     1] loss: 672.086
[83,     1] loss: 477.754
[84,     1] loss: 568.852
[85,     1] loss: 622.184
[86,     1] loss: 672.949
[87,     1] loss: 543.575
[88,     1] loss: 511.612
[89,     1] loss: 502.971
[90,     1] loss: 457.547
[91,     1] loss: 514.387
[92,     1] loss: 582.488
[93,     1] loss: 1181.906
[94,     1] loss: 605.440
[95,     1] loss: 666.019
[96,     1] loss: 695.867
[97,     1] loss: 685.785
[98,     1] loss: 714.920
[99,     1] loss: 584.538
[100,     1] loss: 532.482
[101,     1] loss: 550.032
[102,     1] loss: 614.430
[103,     1] loss: 653.199
[104,     1] loss: 494.122
[105,     1] loss: 622.798
[106,     1] loss: 793.439
[107,     1] loss: 492.694
[108,     1] loss: 830.076
[109,     1] loss: 589.704
[110,     1] loss: 744.170
[111,     1] loss: 543.013
[112,     1] loss: 768.337
[113,     1] loss: 499.113
[114,     1] loss: 658.072
[115,     1] loss: 497.071
Early stopping applied (best metric=0.3648494482040405)
Finished Training
Total time taken: 23.247708320617676
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.779
[2,     1] loss: 1244.492
[3,     1] loss: 1254.651
[4,     1] loss: 1247.274
[5,     1] loss: 1253.879
[6,     1] loss: 1244.061
[7,     1] loss: 1244.400
[8,     1] loss: 1245.973
[9,     1] loss: 1234.840
[10,     1] loss: 1222.177
[11,     1] loss: 1202.366
[12,     1] loss: 1166.451
[13,     1] loss: 1137.464
[14,     1] loss: 1123.642
[15,     1] loss: 1062.758
[16,     1] loss: 1086.800
[17,     1] loss: 1044.073
[18,     1] loss: 1006.975
[19,     1] loss: 1018.404
[20,     1] loss: 997.661
[21,     1] loss: 996.743
[22,     1] loss: 1006.013
[23,     1] loss: 975.872
[24,     1] loss: 961.003
[25,     1] loss: 950.418
[26,     1] loss: 964.170
[27,     1] loss: 951.388
[28,     1] loss: 882.964
[29,     1] loss: 881.174
[30,     1] loss: 907.660
[31,     1] loss: 901.631
[32,     1] loss: 845.046
[33,     1] loss: 852.377
[34,     1] loss: 844.477
[35,     1] loss: 825.711
[36,     1] loss: 847.512
[37,     1] loss: 887.648
[38,     1] loss: 817.188
[39,     1] loss: 808.729
[40,     1] loss: 940.784
[41,     1] loss: 815.276
[42,     1] loss: 813.412
[43,     1] loss: 847.269
[44,     1] loss: 761.013
[45,     1] loss: 815.461
[46,     1] loss: 730.703
[47,     1] loss: 732.655
[48,     1] loss: 697.308
[49,     1] loss: 828.711
[50,     1] loss: 872.529
[51,     1] loss: 838.684
[52,     1] loss: 792.296
[53,     1] loss: 768.176
[54,     1] loss: 802.434
[55,     1] loss: 678.027
[56,     1] loss: 836.377
[57,     1] loss: 628.179
[58,     1] loss: 770.720
[59,     1] loss: 687.155
[60,     1] loss: 688.207
[61,     1] loss: 641.555
[62,     1] loss: 653.130
[63,     1] loss: 713.834
[64,     1] loss: 636.876
[65,     1] loss: 597.146
[66,     1] loss: 525.734
[67,     1] loss: 526.599
[68,     1] loss: 526.971
[69,     1] loss: 509.535
[70,     1] loss: 871.901
[71,     1] loss: 2226.652
[72,     1] loss: 1364.886
Early stopping applied (best metric=0.3889477550983429)
Finished Training
Total time taken: 14.754642248153687
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1252.922
[2,     1] loss: 1252.647
[3,     1] loss: 1251.896
[4,     1] loss: 1250.169
[5,     1] loss: 1247.227
[6,     1] loss: 1242.496
[7,     1] loss: 1233.577
[8,     1] loss: 1209.967
[9,     1] loss: 1167.973
[10,     1] loss: 1111.020
[11,     1] loss: 1063.015
[12,     1] loss: 1045.632
[13,     1] loss: 1078.410
[14,     1] loss: 1022.349
[15,     1] loss: 972.367
[16,     1] loss: 988.700
[17,     1] loss: 1012.700
[18,     1] loss: 983.769
[19,     1] loss: 1004.454
[20,     1] loss: 982.692
[21,     1] loss: 971.689
[22,     1] loss: 954.329
[23,     1] loss: 988.324
[24,     1] loss: 973.450
[25,     1] loss: 922.659
[26,     1] loss: 941.527
[27,     1] loss: 934.320
[28,     1] loss: 900.290
[29,     1] loss: 940.509
[30,     1] loss: 899.642
[31,     1] loss: 941.145
[32,     1] loss: 883.303
[33,     1] loss: 911.955
[34,     1] loss: 908.469
[35,     1] loss: 865.533
[36,     1] loss: 867.970
[37,     1] loss: 836.206
[38,     1] loss: 866.147
[39,     1] loss: 938.901
[40,     1] loss: 874.621
[41,     1] loss: 843.218
[42,     1] loss: 801.239
[43,     1] loss: 829.758
[44,     1] loss: 790.966
[45,     1] loss: 796.669
[46,     1] loss: 745.801
[47,     1] loss: 881.054
[48,     1] loss: 1158.391
[49,     1] loss: 949.582
[50,     1] loss: 843.814
[51,     1] loss: 828.728
[52,     1] loss: 871.381
[53,     1] loss: 869.877
[54,     1] loss: 829.513
[55,     1] loss: 878.138
[56,     1] loss: 817.024
[57,     1] loss: 767.115
[58,     1] loss: 779.912
[59,     1] loss: 758.234
[60,     1] loss: 745.870
[61,     1] loss: 733.440
[62,     1] loss: 706.062
[63,     1] loss: 686.959
[64,     1] loss: 721.172
[65,     1] loss: 661.133
[66,     1] loss: 667.046
[67,     1] loss: 623.852
[68,     1] loss: 683.790
[69,     1] loss: 945.594
[70,     1] loss: 1102.790
[71,     1] loss: 700.356
[72,     1] loss: 963.105
[73,     1] loss: 814.929
[74,     1] loss: 829.906
[75,     1] loss: 847.445
[76,     1] loss: 720.908
[77,     1] loss: 807.080
[78,     1] loss: 791.188
[79,     1] loss: 713.689
[80,     1] loss: 783.161
[81,     1] loss: 705.818
[82,     1] loss: 711.575
[83,     1] loss: 760.402
[84,     1] loss: 658.722
[85,     1] loss: 619.639
[86,     1] loss: 576.461
[87,     1] loss: 615.746
[88,     1] loss: 588.642
[89,     1] loss: 613.267
[90,     1] loss: 1078.542
[91,     1] loss: 816.573
[92,     1] loss: 619.222
[93,     1] loss: 693.726
[94,     1] loss: 655.069
[95,     1] loss: 693.287
[96,     1] loss: 589.995
[97,     1] loss: 620.809
[98,     1] loss: 572.015
[99,     1] loss: 573.307
[100,     1] loss: 544.143
[101,     1] loss: 534.251
[102,     1] loss: 862.633
[103,     1] loss: 971.030
[104,     1] loss: 651.570
[105,     1] loss: 817.662
[106,     1] loss: 712.286
[107,     1] loss: 723.127
[108,     1] loss: 638.436
[109,     1] loss: 748.005
[110,     1] loss: 644.455
[111,     1] loss: 547.560
[112,     1] loss: 584.488
[113,     1] loss: 718.811
[114,     1] loss: 686.363
[115,     1] loss: 542.761
[116,     1] loss: 650.780
[117,     1] loss: 653.014
[118,     1] loss: 600.113
[119,     1] loss: 702.991
[120,     1] loss: 575.125
[121,     1] loss: 537.488
[122,     1] loss: 610.441
[123,     1] loss: 620.907
[124,     1] loss: 468.222
[125,     1] loss: 580.349
[126,     1] loss: 743.333
[127,     1] loss: 495.042
[128,     1] loss: 518.004
[129,     1] loss: 790.981
[130,     1] loss: 612.378
[131,     1] loss: 469.434
[132,     1] loss: 618.534
[133,     1] loss: 522.285
[134,     1] loss: 428.995
[135,     1] loss: 517.273
[136,     1] loss: 667.150
[137,     1] loss: 1201.944
[138,     1] loss: 576.558
[139,     1] loss: 906.937
[140,     1] loss: 731.767
[141,     1] loss: 788.601
[142,     1] loss: 740.184
[143,     1] loss: 605.999
[144,     1] loss: 715.458
[145,     1] loss: 618.441
[146,     1] loss: 562.666
[147,     1] loss: 579.794
[148,     1] loss: 672.013
[149,     1] loss: 567.807
[150,     1] loss: 544.538
[151,     1] loss: 678.791
[152,     1] loss: 464.795
[153,     1] loss: 620.499
[154,     1] loss: 958.542
[155,     1] loss: 532.431
[156,     1] loss: 911.047
[157,     1] loss: 602.096
[158,     1] loss: 779.595
[159,     1] loss: 582.364
[160,     1] loss: 673.788
[161,     1] loss: 524.526
[162,     1] loss: 597.587
[163,     1] loss: 498.677
[164,     1] loss: 525.153
[165,     1] loss: 476.846
[166,     1] loss: 494.423
[167,     1] loss: 577.128
[168,     1] loss: 753.844
[169,     1] loss: 487.966
Early stopping applied (best metric=0.29273200035095215)
Finished Training
Total time taken: 33.81971764564514
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.694
[2,     1] loss: 1253.843
[3,     1] loss: 1252.383
[4,     1] loss: 1247.144
[5,     1] loss: 1248.861
[6,     1] loss: 1246.104
[7,     1] loss: 1242.576
[8,     1] loss: 1240.803
[9,     1] loss: 1229.217
[10,     1] loss: 1207.011
[11,     1] loss: 1167.882
[12,     1] loss: 1126.294
[13,     1] loss: 1106.712
[14,     1] loss: 1042.682
[15,     1] loss: 1036.376
[16,     1] loss: 1081.990
[17,     1] loss: 1123.413
[18,     1] loss: 981.808
[19,     1] loss: 1082.783
[20,     1] loss: 1033.443
[21,     1] loss: 1034.949
[22,     1] loss: 1043.770
[23,     1] loss: 1023.645
[24,     1] loss: 1013.962
[25,     1] loss: 974.791
[26,     1] loss: 942.013
[27,     1] loss: 969.342
[28,     1] loss: 964.739
[29,     1] loss: 956.838
[30,     1] loss: 967.795
[31,     1] loss: 935.188
[32,     1] loss: 935.129
[33,     1] loss: 935.322
[34,     1] loss: 897.131
[35,     1] loss: 892.666
[36,     1] loss: 900.865
[37,     1] loss: 867.847
[38,     1] loss: 884.520
[39,     1] loss: 852.259
[40,     1] loss: 891.380
[41,     1] loss: 858.601
[42,     1] loss: 809.160
[43,     1] loss: 910.441
[44,     1] loss: 1379.474
[45,     1] loss: 860.047
[46,     1] loss: 1077.177
[47,     1] loss: 1004.759
[48,     1] loss: 980.625
[49,     1] loss: 1010.287
[50,     1] loss: 999.859
[51,     1] loss: 993.492
[52,     1] loss: 966.499
[53,     1] loss: 949.060
[54,     1] loss: 955.931
[55,     1] loss: 905.255
[56,     1] loss: 897.935
[57,     1] loss: 881.185
[58,     1] loss: 918.077
[59,     1] loss: 819.476
[60,     1] loss: 858.776
[61,     1] loss: 866.862
[62,     1] loss: 829.617
[63,     1] loss: 830.889
[64,     1] loss: 809.393
[65,     1] loss: 798.449
[66,     1] loss: 748.854
[67,     1] loss: 802.453
[68,     1] loss: 743.976
[69,     1] loss: 822.709
[70,     1] loss: 969.726
[71,     1] loss: 972.800
[72,     1] loss: 771.464
[73,     1] loss: 914.115
[74,     1] loss: 871.143
[75,     1] loss: 812.863
[76,     1] loss: 856.670
[77,     1] loss: 833.633
[78,     1] loss: 780.377
[79,     1] loss: 845.645
[80,     1] loss: 730.831
[81,     1] loss: 808.196
[82,     1] loss: 692.136
[83,     1] loss: 796.971
[84,     1] loss: 699.002
[85,     1] loss: 779.044
[86,     1] loss: 672.272
[87,     1] loss: 658.775
[88,     1] loss: 687.737
[89,     1] loss: 747.269
[90,     1] loss: 869.926
[91,     1] loss: 856.484
[92,     1] loss: 681.212
[93,     1] loss: 759.374
[94,     1] loss: 660.096
[95,     1] loss: 727.108
[96,     1] loss: 621.765
[97,     1] loss: 724.446
[98,     1] loss: 679.442
[99,     1] loss: 648.740
[100,     1] loss: 650.213
[101,     1] loss: 754.049
[102,     1] loss: 601.469
[103,     1] loss: 570.078
[104,     1] loss: 620.870
[105,     1] loss: 677.538
[106,     1] loss: 871.166
[107,     1] loss: 656.364
[108,     1] loss: 612.411
[109,     1] loss: 671.927
[110,     1] loss: 579.117
[111,     1] loss: 604.775
[112,     1] loss: 676.888
[113,     1] loss: 909.271
[114,     1] loss: 1160.419
[115,     1] loss: 828.821
[116,     1] loss: 858.018
[117,     1] loss: 952.897
[118,     1] loss: 775.361
[119,     1] loss: 898.614
[120,     1] loss: 782.917
[121,     1] loss: 765.510
[122,     1] loss: 724.570
[123,     1] loss: 729.567
[124,     1] loss: 783.306
[125,     1] loss: 695.110
[126,     1] loss: 723.427
[127,     1] loss: 650.005
[128,     1] loss: 594.388
[129,     1] loss: 860.332
[130,     1] loss: 741.821
[131,     1] loss: 627.760
[132,     1] loss: 765.065
[133,     1] loss: 615.643
[134,     1] loss: 671.721
[135,     1] loss: 611.291
[136,     1] loss: 762.392
[137,     1] loss: 542.825
[138,     1] loss: 575.655
[139,     1] loss: 550.108
[140,     1] loss: 464.434
[141,     1] loss: 605.060
[142,     1] loss: 811.940
[143,     1] loss: 674.799
[144,     1] loss: 569.605
[145,     1] loss: 760.195
[146,     1] loss: 575.892
[147,     1] loss: 649.439
[148,     1] loss: 489.679
[149,     1] loss: 624.123
[150,     1] loss: 681.970
[151,     1] loss: 501.957
[152,     1] loss: 498.615
[153,     1] loss: 611.091
Early stopping applied (best metric=0.32875025272369385)
Finished Training
Total time taken: 30.794328212738037
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.333
[2,     1] loss: 1249.793
[3,     1] loss: 1246.816
[4,     1] loss: 1248.001
[5,     1] loss: 1238.915
[6,     1] loss: 1227.996
[7,     1] loss: 1207.415
[8,     1] loss: 1168.042
[9,     1] loss: 1141.239
[10,     1] loss: 1117.733
[11,     1] loss: 1089.983
[12,     1] loss: 1041.542
[13,     1] loss: 1020.907
[14,     1] loss: 964.228
[15,     1] loss: 995.985
[16,     1] loss: 1019.475
[17,     1] loss: 1001.907
[18,     1] loss: 972.497
[19,     1] loss: 964.485
[20,     1] loss: 944.827
[21,     1] loss: 959.360
[22,     1] loss: 923.230
[23,     1] loss: 909.109
[24,     1] loss: 949.598
[25,     1] loss: 860.551
[26,     1] loss: 914.299
[27,     1] loss: 926.875
[28,     1] loss: 866.921
[29,     1] loss: 921.268
[30,     1] loss: 865.493
[31,     1] loss: 884.462
[32,     1] loss: 817.228
[33,     1] loss: 837.763
[34,     1] loss: 886.165
[35,     1] loss: 818.266
[36,     1] loss: 835.223
[37,     1] loss: 838.091
[38,     1] loss: 758.462
[39,     1] loss: 768.097
[40,     1] loss: 769.254
[41,     1] loss: 772.191
[42,     1] loss: 747.383
[43,     1] loss: 833.654
[44,     1] loss: 907.262
[45,     1] loss: 751.119
[46,     1] loss: 784.615
[47,     1] loss: 739.028
[48,     1] loss: 728.568
[49,     1] loss: 703.848
[50,     1] loss: 716.073
[51,     1] loss: 719.341
[52,     1] loss: 705.228
[53,     1] loss: 649.242
[54,     1] loss: 690.526
[55,     1] loss: 737.792
[56,     1] loss: 741.868
[57,     1] loss: 737.582
[58,     1] loss: 635.328
[59,     1] loss: 732.111
[60,     1] loss: 671.944
[61,     1] loss: 659.085
[62,     1] loss: 755.557
[63,     1] loss: 712.444
[64,     1] loss: 580.758
[65,     1] loss: 690.193
Early stopping applied (best metric=0.3892180323600769)
Finished Training
Total time taken: 13.40015697479248
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.859
[2,     1] loss: 1252.591
[3,     1] loss: 1251.516
[4,     1] loss: 1250.526
[5,     1] loss: 1253.044
[6,     1] loss: 1250.487
[7,     1] loss: 1248.693
[8,     1] loss: 1247.450
[9,     1] loss: 1247.044
[10,     1] loss: 1239.275
[11,     1] loss: 1237.438
[12,     1] loss: 1220.485
[13,     1] loss: 1205.405
[14,     1] loss: 1173.551
[15,     1] loss: 1118.223
[16,     1] loss: 1089.072
[17,     1] loss: 1089.544
[18,     1] loss: 1035.248
[19,     1] loss: 1051.305
[20,     1] loss: 1074.841
[21,     1] loss: 1117.694
[22,     1] loss: 1020.984
[23,     1] loss: 1027.449
[24,     1] loss: 1009.829
[25,     1] loss: 1003.396
[26,     1] loss: 1034.063
[27,     1] loss: 1015.175
[28,     1] loss: 1039.672
[29,     1] loss: 962.950
[30,     1] loss: 987.539
[31,     1] loss: 946.879
[32,     1] loss: 925.876
[33,     1] loss: 952.976
[34,     1] loss: 966.030
[35,     1] loss: 891.121
[36,     1] loss: 922.599
[37,     1] loss: 950.960
[38,     1] loss: 909.752
[39,     1] loss: 888.844
[40,     1] loss: 865.530
[41,     1] loss: 875.532
[42,     1] loss: 851.006
[43,     1] loss: 787.633
[44,     1] loss: 843.742
[45,     1] loss: 952.454
[46,     1] loss: 942.293
[47,     1] loss: 781.384
[48,     1] loss: 928.015
[49,     1] loss: 829.037
[50,     1] loss: 901.405
[51,     1] loss: 810.983
[52,     1] loss: 835.863
[53,     1] loss: 859.500
[54,     1] loss: 799.453
[55,     1] loss: 844.462
[56,     1] loss: 808.953
[57,     1] loss: 807.636
[58,     1] loss: 740.105
[59,     1] loss: 748.695
[60,     1] loss: 770.300
[61,     1] loss: 703.648
[62,     1] loss: 690.177
[63,     1] loss: 849.433
[64,     1] loss: 782.517
[65,     1] loss: 656.206
[66,     1] loss: 814.853
[67,     1] loss: 653.512
[68,     1] loss: 773.960
[69,     1] loss: 681.188
[70,     1] loss: 727.595
[71,     1] loss: 755.051
[72,     1] loss: 606.242
[73,     1] loss: 599.762
[74,     1] loss: 619.448
[75,     1] loss: 624.628
[76,     1] loss: 611.645
[77,     1] loss: 696.728
[78,     1] loss: 820.939
[79,     1] loss: 569.456
[80,     1] loss: 629.618
[81,     1] loss: 672.951
[82,     1] loss: 564.436
[83,     1] loss: 778.766
[84,     1] loss: 893.138
[85,     1] loss: 683.837
[86,     1] loss: 761.126
[87,     1] loss: 710.608
[88,     1] loss: 668.729
[89,     1] loss: 684.002
[90,     1] loss: 591.490
[91,     1] loss: 763.444
[92,     1] loss: 660.276
[93,     1] loss: 582.172
[94,     1] loss: 745.682
[95,     1] loss: 509.104
[96,     1] loss: 639.928
[97,     1] loss: 518.572
[98,     1] loss: 603.619
[99,     1] loss: 569.577
[100,     1] loss: 432.360
[101,     1] loss: 666.786
[102,     1] loss: 1011.914
[103,     1] loss: 503.406
[104,     1] loss: 805.370
[105,     1] loss: 624.092
[106,     1] loss: 738.797
[107,     1] loss: 544.905
[108,     1] loss: 932.384
[109,     1] loss: 746.282
[110,     1] loss: 807.349
[111,     1] loss: 645.111
[112,     1] loss: 841.101
[113,     1] loss: 641.064
[114,     1] loss: 667.703
[115,     1] loss: 733.416
[116,     1] loss: 630.288
[117,     1] loss: 569.285
[118,     1] loss: 566.247
[119,     1] loss: 564.169
[120,     1] loss: 662.185
[121,     1] loss: 505.639
[122,     1] loss: 508.102
[123,     1] loss: 555.154
[124,     1] loss: 441.215
[125,     1] loss: 489.263
[126,     1] loss: 655.518
[127,     1] loss: 1144.676
[128,     1] loss: 455.998
[129,     1] loss: 827.123
[130,     1] loss: 658.999
Early stopping applied (best metric=0.30502650141716003)
Finished Training
Total time taken: 26.6005539894104
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.807
[2,     1] loss: 1261.098
[3,     1] loss: 1250.589
[4,     1] loss: 1252.463
[5,     1] loss: 1250.754
[6,     1] loss: 1251.765
[7,     1] loss: 1248.808
[8,     1] loss: 1249.671
[9,     1] loss: 1252.786
[10,     1] loss: 1249.563
[11,     1] loss: 1249.659
[12,     1] loss: 1249.028
[13,     1] loss: 1249.603
[14,     1] loss: 1248.317
[15,     1] loss: 1251.483
[16,     1] loss: 1248.368
[17,     1] loss: 1248.686
[18,     1] loss: 1250.708
[19,     1] loss: 1247.003
[20,     1] loss: 1246.577
[21,     1] loss: 1243.467
[22,     1] loss: 1241.713
[23,     1] loss: 1236.818
[24,     1] loss: 1229.722
[25,     1] loss: 1216.996
[26,     1] loss: 1174.098
[27,     1] loss: 1162.202
[28,     1] loss: 1135.583
[29,     1] loss: 1085.720
[30,     1] loss: 1050.167
[31,     1] loss: 1057.087
[32,     1] loss: 1066.416
[33,     1] loss: 1043.896
[34,     1] loss: 991.490
[35,     1] loss: 1048.646
[36,     1] loss: 1017.797
[37,     1] loss: 1007.918
[38,     1] loss: 970.699
[39,     1] loss: 1014.079
[40,     1] loss: 999.305
[41,     1] loss: 963.928
[42,     1] loss: 952.320
[43,     1] loss: 978.683
[44,     1] loss: 903.972
[45,     1] loss: 936.253
[46,     1] loss: 899.337
[47,     1] loss: 912.363
[48,     1] loss: 918.924
[49,     1] loss: 911.281
[50,     1] loss: 860.065
[51,     1] loss: 890.207
[52,     1] loss: 912.996
[53,     1] loss: 900.762
[54,     1] loss: 834.896
[55,     1] loss: 843.232
[56,     1] loss: 821.865
[57,     1] loss: 830.062
[58,     1] loss: 821.366
[59,     1] loss: 778.804
[60,     1] loss: 831.320
[61,     1] loss: 838.649
[62,     1] loss: 838.633
[63,     1] loss: 816.496
[64,     1] loss: 738.282
[65,     1] loss: 755.376
[66,     1] loss: 753.797
[67,     1] loss: 752.238
[68,     1] loss: 888.775
[69,     1] loss: 1293.937
[70,     1] loss: 840.644
[71,     1] loss: 900.696
[72,     1] loss: 1044.282
[73,     1] loss: 922.088
[74,     1] loss: 945.682
[75,     1] loss: 946.688
[76,     1] loss: 923.279
[77,     1] loss: 862.324
[78,     1] loss: 858.147
[79,     1] loss: 855.446
[80,     1] loss: 803.077
[81,     1] loss: 859.724
[82,     1] loss: 762.075
[83,     1] loss: 780.299
[84,     1] loss: 745.768
[85,     1] loss: 708.907
[86,     1] loss: 687.032
[87,     1] loss: 764.549
[88,     1] loss: 864.662
[89,     1] loss: 664.808
[90,     1] loss: 917.155
[91,     1] loss: 866.252
[92,     1] loss: 928.844
[93,     1] loss: 810.730
[94,     1] loss: 854.215
[95,     1] loss: 805.959
[96,     1] loss: 792.844
[97,     1] loss: 790.346
[98,     1] loss: 734.955
[99,     1] loss: 767.295
[100,     1] loss: 713.855
[101,     1] loss: 762.110
[102,     1] loss: 628.513
[103,     1] loss: 754.052
[104,     1] loss: 778.087
[105,     1] loss: 653.935
[106,     1] loss: 755.163
[107,     1] loss: 646.742
[108,     1] loss: 729.475
[109,     1] loss: 678.257
[110,     1] loss: 649.139
[111,     1] loss: 661.359
[112,     1] loss: 600.699
[113,     1] loss: 712.084
[114,     1] loss: 1153.132
[115,     1] loss: 808.572
[116,     1] loss: 979.349
Early stopping applied (best metric=0.3517809510231018)
Finished Training
Total time taken: 24.33971619606018
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1256.539
[2,     1] loss: 1251.920
[3,     1] loss: 1250.306
[4,     1] loss: 1251.449
[5,     1] loss: 1253.475
[6,     1] loss: 1250.747
[7,     1] loss: 1244.430
[8,     1] loss: 1241.941
[9,     1] loss: 1218.606
[10,     1] loss: 1193.466
[11,     1] loss: 1156.660
[12,     1] loss: 1115.340
[13,     1] loss: 1087.741
[14,     1] loss: 1064.622
[15,     1] loss: 1073.376
[16,     1] loss: 978.573
[17,     1] loss: 1025.850
[18,     1] loss: 1032.870
[19,     1] loss: 1013.972
[20,     1] loss: 977.456
[21,     1] loss: 1015.040
[22,     1] loss: 948.764
[23,     1] loss: 978.104
[24,     1] loss: 957.516
[25,     1] loss: 912.181
[26,     1] loss: 918.980
[27,     1] loss: 924.224
[28,     1] loss: 866.324
[29,     1] loss: 933.125
[30,     1] loss: 865.829
[31,     1] loss: 895.799
[32,     1] loss: 929.943
[33,     1] loss: 862.125
[34,     1] loss: 885.294
[35,     1] loss: 882.941
[36,     1] loss: 838.325
[37,     1] loss: 899.117
[38,     1] loss: 808.248
[39,     1] loss: 879.325
[40,     1] loss: 816.015
[41,     1] loss: 796.254
[42,     1] loss: 804.320
[43,     1] loss: 822.204
[44,     1] loss: 718.890
[45,     1] loss: 759.689
[46,     1] loss: 804.692
[47,     1] loss: 757.591
[48,     1] loss: 674.809
[49,     1] loss: 643.308
[50,     1] loss: 759.857
[51,     1] loss: 707.833
[52,     1] loss: 637.027
[53,     1] loss: 714.098
[54,     1] loss: 688.889
[55,     1] loss: 1006.635
[56,     1] loss: 1022.656
[57,     1] loss: 767.443
[58,     1] loss: 826.104
[59,     1] loss: 908.350
[60,     1] loss: 820.152
[61,     1] loss: 794.195
[62,     1] loss: 840.886
[63,     1] loss: 772.944
[64,     1] loss: 774.266
[65,     1] loss: 743.928
[66,     1] loss: 668.596
[67,     1] loss: 711.718
[68,     1] loss: 689.465
[69,     1] loss: 624.155
[70,     1] loss: 649.715
[71,     1] loss: 624.951
[72,     1] loss: 549.527
[73,     1] loss: 657.953
[74,     1] loss: 595.488
[75,     1] loss: 618.269
[76,     1] loss: 530.088
[77,     1] loss: 548.908
[78,     1] loss: 739.374
[79,     1] loss: 814.996
[80,     1] loss: 626.110
[81,     1] loss: 673.062
[82,     1] loss: 620.076
[83,     1] loss: 644.390
[84,     1] loss: 653.713
[85,     1] loss: 577.574
[86,     1] loss: 679.001
[87,     1] loss: 533.737
[88,     1] loss: 511.539
[89,     1] loss: 465.156
[90,     1] loss: 440.846
[91,     1] loss: 519.279
[92,     1] loss: 777.301
[93,     1] loss: 1308.944
[94,     1] loss: 549.605
Early stopping applied (best metric=0.4147120714187622)
Finished Training
Total time taken: 19.865949392318726
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.977
[2,     1] loss: 1249.378
[3,     1] loss: 1252.194
[4,     1] loss: 1247.278
[5,     1] loss: 1245.367
[6,     1] loss: 1235.583
[7,     1] loss: 1223.681
[8,     1] loss: 1205.589
[9,     1] loss: 1153.325
[10,     1] loss: 1155.279
[11,     1] loss: 1075.952
[12,     1] loss: 1060.714
[13,     1] loss: 1122.302
[14,     1] loss: 1049.232
[15,     1] loss: 1076.323
[16,     1] loss: 1068.323
[17,     1] loss: 1032.282
[18,     1] loss: 1055.854
[19,     1] loss: 1011.200
[20,     1] loss: 1021.819
[21,     1] loss: 1006.499
[22,     1] loss: 992.338
[23,     1] loss: 985.820
[24,     1] loss: 961.739
[25,     1] loss: 983.339
[26,     1] loss: 997.726
[27,     1] loss: 915.440
[28,     1] loss: 913.552
[29,     1] loss: 902.735
[30,     1] loss: 913.477
[31,     1] loss: 922.787
[32,     1] loss: 850.958
[33,     1] loss: 967.396
[34,     1] loss: 989.184
[35,     1] loss: 823.960
[36,     1] loss: 873.278
[37,     1] loss: 924.488
[38,     1] loss: 864.536
[39,     1] loss: 886.127
[40,     1] loss: 815.066
[41,     1] loss: 834.546
[42,     1] loss: 818.332
[43,     1] loss: 755.991
[44,     1] loss: 766.276
[45,     1] loss: 791.039
[46,     1] loss: 1239.247
[47,     1] loss: 960.290
[48,     1] loss: 905.480
[49,     1] loss: 848.640
[50,     1] loss: 930.062
[51,     1] loss: 980.255
[52,     1] loss: 889.819
[53,     1] loss: 885.713
[54,     1] loss: 878.497
[55,     1] loss: 822.337
[56,     1] loss: 842.605
[57,     1] loss: 894.093
[58,     1] loss: 779.032
[59,     1] loss: 843.155
[60,     1] loss: 737.476
[61,     1] loss: 810.762
[62,     1] loss: 738.328
[63,     1] loss: 769.824
[64,     1] loss: 700.749
[65,     1] loss: 816.108
[66,     1] loss: 653.219
[67,     1] loss: 793.994
[68,     1] loss: 776.414
[69,     1] loss: 658.977
[70,     1] loss: 748.432
[71,     1] loss: 644.218
[72,     1] loss: 685.237
[73,     1] loss: 735.348
[74,     1] loss: 592.347
[75,     1] loss: 590.952
[76,     1] loss: 716.816
[77,     1] loss: 728.598
[78,     1] loss: 723.239
[79,     1] loss: 636.371
[80,     1] loss: 750.478
[81,     1] loss: 622.588
[82,     1] loss: 677.336
[83,     1] loss: 603.668
[84,     1] loss: 506.190
[85,     1] loss: 523.073
[86,     1] loss: 527.170
[87,     1] loss: 859.234
[88,     1] loss: 1649.513
[89,     1] loss: 722.669
[90,     1] loss: 1026.433
[91,     1] loss: 1007.177
[92,     1] loss: 1000.055
[93,     1] loss: 1006.627
[94,     1] loss: 955.794
[95,     1] loss: 914.271
[96,     1] loss: 878.620
[97,     1] loss: 914.898
[98,     1] loss: 873.571
[99,     1] loss: 835.681
[100,     1] loss: 841.351
[101,     1] loss: 819.634
[102,     1] loss: 841.656
[103,     1] loss: 800.308
[104,     1] loss: 788.488
[105,     1] loss: 835.164
[106,     1] loss: 859.155
[107,     1] loss: 752.194
[108,     1] loss: 767.622
[109,     1] loss: 758.840
[110,     1] loss: 710.171
[111,     1] loss: 705.419
[112,     1] loss: 684.537
[113,     1] loss: 610.617
[114,     1] loss: 809.443
[115,     1] loss: 1347.995
[116,     1] loss: 806.718
[117,     1] loss: 876.360
[118,     1] loss: 862.884
[119,     1] loss: 930.971
[120,     1] loss: 854.672
[121,     1] loss: 737.618
[122,     1] loss: 824.394
[123,     1] loss: 716.238
[124,     1] loss: 785.980
[125,     1] loss: 764.673
[126,     1] loss: 684.673
[127,     1] loss: 667.928
[128,     1] loss: 663.893
[129,     1] loss: 632.792
[130,     1] loss: 627.395
[131,     1] loss: 599.814
[132,     1] loss: 572.353
[133,     1] loss: 698.097
Early stopping applied (best metric=0.3120771050453186)
Finished Training
Total time taken: 28.088775634765625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.684
[2,     1] loss: 1262.164
[3,     1] loss: 1253.600
[4,     1] loss: 1246.667
[5,     1] loss: 1250.119
[6,     1] loss: 1246.561
[7,     1] loss: 1245.995
[8,     1] loss: 1243.958
[9,     1] loss: 1238.790
[10,     1] loss: 1227.703
[11,     1] loss: 1204.710
[12,     1] loss: 1185.050
[13,     1] loss: 1144.677
[14,     1] loss: 1127.302
[15,     1] loss: 1056.706
[16,     1] loss: 1049.841
[17,     1] loss: 1059.340
[18,     1] loss: 1011.430
[19,     1] loss: 1026.116
[20,     1] loss: 992.475
[21,     1] loss: 1004.666
[22,     1] loss: 1056.904
[23,     1] loss: 969.951
[24,     1] loss: 1010.720
[25,     1] loss: 968.071
[26,     1] loss: 972.100
[27,     1] loss: 953.779
[28,     1] loss: 941.795
[29,     1] loss: 940.255
[30,     1] loss: 930.236
[31,     1] loss: 952.074
[32,     1] loss: 922.108
[33,     1] loss: 955.602
[34,     1] loss: 895.408
[35,     1] loss: 951.382
[36,     1] loss: 921.560
[37,     1] loss: 941.828
[38,     1] loss: 919.331
[39,     1] loss: 914.846
[40,     1] loss: 894.487
[41,     1] loss: 889.738
[42,     1] loss: 839.222
[43,     1] loss: 817.879
[44,     1] loss: 820.379
[45,     1] loss: 1043.675
[46,     1] loss: 1064.839
[47,     1] loss: 819.938
[48,     1] loss: 917.740
[49,     1] loss: 930.421
[50,     1] loss: 887.792
[51,     1] loss: 941.904
[52,     1] loss: 895.068
[53,     1] loss: 856.506
[54,     1] loss: 838.819
[55,     1] loss: 849.404
[56,     1] loss: 876.310
[57,     1] loss: 801.552
[58,     1] loss: 833.932
[59,     1] loss: 868.738
[60,     1] loss: 803.850
[61,     1] loss: 804.849
[62,     1] loss: 837.740
[63,     1] loss: 814.016
[64,     1] loss: 812.243
[65,     1] loss: 768.977
[66,     1] loss: 817.342
[67,     1] loss: 768.110
[68,     1] loss: 747.229
[69,     1] loss: 699.508
[70,     1] loss: 685.070
[71,     1] loss: 747.387
[72,     1] loss: 925.875
[73,     1] loss: 995.891
[74,     1] loss: 739.886
[75,     1] loss: 866.120
[76,     1] loss: 796.521
[77,     1] loss: 820.955
[78,     1] loss: 807.940
[79,     1] loss: 769.219
[80,     1] loss: 792.486
[81,     1] loss: 824.077
[82,     1] loss: 782.125
[83,     1] loss: 687.464
[84,     1] loss: 720.476
[85,     1] loss: 645.647
[86,     1] loss: 714.710
[87,     1] loss: 645.836
[88,     1] loss: 650.250
[89,     1] loss: 767.358
[90,     1] loss: 800.881
[91,     1] loss: 597.951
[92,     1] loss: 723.250
[93,     1] loss: 766.999
[94,     1] loss: 648.552
[95,     1] loss: 667.274
[96,     1] loss: 590.675
[97,     1] loss: 600.320
[98,     1] loss: 838.529
[99,     1] loss: 1473.112
[100,     1] loss: 773.319
[101,     1] loss: 934.438
[102,     1] loss: 1063.686
[103,     1] loss: 976.269
[104,     1] loss: 967.106
[105,     1] loss: 933.098
[106,     1] loss: 893.698
[107,     1] loss: 835.939
[108,     1] loss: 893.316
[109,     1] loss: 812.527
[110,     1] loss: 838.879
[111,     1] loss: 789.875
[112,     1] loss: 782.669
[113,     1] loss: 754.436
[114,     1] loss: 776.093
[115,     1] loss: 743.964
[116,     1] loss: 719.872
[117,     1] loss: 693.892
[118,     1] loss: 679.862
[119,     1] loss: 646.661
[120,     1] loss: 655.936
[121,     1] loss: 631.982
[122,     1] loss: 722.898
[123,     1] loss: 946.236
[124,     1] loss: 1094.121
[125,     1] loss: 927.264
[126,     1] loss: 844.622
[127,     1] loss: 911.628
[128,     1] loss: 896.567
[129,     1] loss: 793.002
[130,     1] loss: 846.460
[131,     1] loss: 855.461
[132,     1] loss: 764.534
[133,     1] loss: 836.512
[134,     1] loss: 726.527
[135,     1] loss: 824.358
[136,     1] loss: 685.612
[137,     1] loss: 764.581
[138,     1] loss: 689.596
[139,     1] loss: 706.377
[140,     1] loss: 619.052
[141,     1] loss: 678.999
[142,     1] loss: 695.818
[143,     1] loss: 578.267
[144,     1] loss: 665.080
[145,     1] loss: 710.737
[146,     1] loss: 550.442
[147,     1] loss: 765.217
[148,     1] loss: 1182.172
[149,     1] loss: 748.716
[150,     1] loss: 785.089
[151,     1] loss: 944.487
[152,     1] loss: 793.413
[153,     1] loss: 866.306
[154,     1] loss: 841.798
[155,     1] loss: 705.428
[156,     1] loss: 855.575
[157,     1] loss: 740.659
[158,     1] loss: 811.844
[159,     1] loss: 681.724
[160,     1] loss: 758.347
[161,     1] loss: 662.960
[162,     1] loss: 662.247
[163,     1] loss: 638.109
[164,     1] loss: 589.100
[165,     1] loss: 566.842
[166,     1] loss: 570.467
[167,     1] loss: 524.252
[168,     1] loss: 593.087
[169,     1] loss: 669.279
[170,     1] loss: 980.552
[171,     1] loss: 646.982
[172,     1] loss: 820.082
[173,     1] loss: 682.872
[174,     1] loss: 661.248
[175,     1] loss: 601.389
[176,     1] loss: 755.775
[177,     1] loss: 798.490
[178,     1] loss: 621.687
[179,     1] loss: 768.905
[180,     1] loss: 618.687
[181,     1] loss: 718.868
[182,     1] loss: 610.895
[183,     1] loss: 635.206
[184,     1] loss: 689.628
[185,     1] loss: 596.667
[186,     1] loss: 674.705
[187,     1] loss: 568.101
[188,     1] loss: 571.433
[189,     1] loss: 576.525
[190,     1] loss: 474.908
[191,     1] loss: 458.018
[192,     1] loss: 495.984
[193,     1] loss: 562.977
[194,     1] loss: 656.045
[195,     1] loss: 1251.327
[196,     1] loss: 577.022
[197,     1] loss: 1006.620
[198,     1] loss: 859.584
[199,     1] loss: 987.121
[200,     1] loss: 931.566
Finished Training
Total time taken: 40.32515525817871
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.469
[2,     1] loss: 1256.520
[3,     1] loss: 1250.825
[4,     1] loss: 1250.406
[5,     1] loss: 1250.276
[6,     1] loss: 1246.382
[7,     1] loss: 1251.647
[8,     1] loss: 1246.413
[9,     1] loss: 1245.587
[10,     1] loss: 1243.666
[11,     1] loss: 1239.393
[12,     1] loss: 1229.844
[13,     1] loss: 1211.033
[14,     1] loss: 1187.645
[15,     1] loss: 1153.002
[16,     1] loss: 1092.701
[17,     1] loss: 1057.133
[18,     1] loss: 1029.655
[19,     1] loss: 1108.609
[20,     1] loss: 1103.670
[21,     1] loss: 1049.408
[22,     1] loss: 1010.906
[23,     1] loss: 1002.166
[24,     1] loss: 1008.122
[25,     1] loss: 984.237
[26,     1] loss: 1018.771
[27,     1] loss: 965.611
[28,     1] loss: 942.990
[29,     1] loss: 928.160
[30,     1] loss: 909.137
[31,     1] loss: 905.636
[32,     1] loss: 922.908
[33,     1] loss: 893.950
[34,     1] loss: 880.726
[35,     1] loss: 845.117
[36,     1] loss: 852.083
[37,     1] loss: 926.532
[38,     1] loss: 969.096
[39,     1] loss: 817.403
[40,     1] loss: 810.333
[41,     1] loss: 804.158
[42,     1] loss: 830.261
[43,     1] loss: 775.681
[44,     1] loss: 779.208
[45,     1] loss: 786.507
[46,     1] loss: 723.382
[47,     1] loss: 781.540
[48,     1] loss: 918.500
[49,     1] loss: 726.949
[50,     1] loss: 924.140
[51,     1] loss: 770.932
[52,     1] loss: 837.997
[53,     1] loss: 731.576
[54,     1] loss: 816.661
[55,     1] loss: 704.969
[56,     1] loss: 736.744
[57,     1] loss: 649.193
[58,     1] loss: 811.301
[59,     1] loss: 789.065
[60,     1] loss: 645.975
[61,     1] loss: 806.540
[62,     1] loss: 686.321
[63,     1] loss: 762.280
[64,     1] loss: 675.950
[65,     1] loss: 708.129
[66,     1] loss: 626.493
[67,     1] loss: 700.203
[68,     1] loss: 576.545
[69,     1] loss: 522.687
[70,     1] loss: 576.498
[71,     1] loss: 786.912
[72,     1] loss: 1104.697
[73,     1] loss: 584.462
[74,     1] loss: 916.558
[75,     1] loss: 714.430
[76,     1] loss: 748.846
[77,     1] loss: 790.056
[78,     1] loss: 624.548
[79,     1] loss: 752.914
[80,     1] loss: 676.226
[81,     1] loss: 673.673
[82,     1] loss: 646.549
[83,     1] loss: 641.025
[84,     1] loss: 554.606
[85,     1] loss: 608.829
[86,     1] loss: 560.075
[87,     1] loss: 619.304
[88,     1] loss: 592.076
[89,     1] loss: 480.338
[90,     1] loss: 673.924
[91,     1] loss: 1038.868
Early stopping applied (best metric=0.3400684595108032)
Finished Training
Total time taken: 16.172898530960083
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.650
[2,     1] loss: 1251.539
[3,     1] loss: 1251.200
[4,     1] loss: 1246.121
[5,     1] loss: 1245.922
[6,     1] loss: 1243.593
[7,     1] loss: 1236.254
[8,     1] loss: 1215.579
[9,     1] loss: 1181.407
[10,     1] loss: 1139.677
[11,     1] loss: 1069.930
[12,     1] loss: 1025.885
[13,     1] loss: 1014.350
[14,     1] loss: 1088.418
[15,     1] loss: 1155.807
[16,     1] loss: 1026.692
[17,     1] loss: 1084.417
[18,     1] loss: 1008.372
[19,     1] loss: 1036.461
[20,     1] loss: 1022.615
[21,     1] loss: 1001.513
[22,     1] loss: 1017.345
[23,     1] loss: 1001.537
[24,     1] loss: 970.403
[25,     1] loss: 984.944
[26,     1] loss: 928.735
[27,     1] loss: 984.699
[28,     1] loss: 934.402
[29,     1] loss: 860.156
[30,     1] loss: 881.076
[31,     1] loss: 872.773
[32,     1] loss: 870.206
[33,     1] loss: 862.476
[34,     1] loss: 811.330
[35,     1] loss: 834.341
[36,     1] loss: 829.506
[37,     1] loss: 830.873
[38,     1] loss: 828.338
[39,     1] loss: 812.125
[40,     1] loss: 808.539
[41,     1] loss: 895.889
[42,     1] loss: 846.414
[43,     1] loss: 807.846
[44,     1] loss: 786.702
[45,     1] loss: 804.746
[46,     1] loss: 738.870
[47,     1] loss: 772.538
[48,     1] loss: 781.903
[49,     1] loss: 720.719
[50,     1] loss: 763.461
[51,     1] loss: 805.816
[52,     1] loss: 923.029
[53,     1] loss: 723.304
[54,     1] loss: 818.914
[55,     1] loss: 757.072
[56,     1] loss: 826.854
[57,     1] loss: 713.366
[58,     1] loss: 808.321
[59,     1] loss: 718.183
[60,     1] loss: 733.393
[61,     1] loss: 844.129
[62,     1] loss: 732.705
[63,     1] loss: 734.149
[64,     1] loss: 744.781
[65,     1] loss: 650.467
[66,     1] loss: 730.195
[67,     1] loss: 624.704
[68,     1] loss: 685.051
[69,     1] loss: 591.469
[70,     1] loss: 607.912
[71,     1] loss: 700.760
[72,     1] loss: 973.017
[73,     1] loss: 803.193
[74,     1] loss: 610.980
[75,     1] loss: 752.821
[76,     1] loss: 706.393
[77,     1] loss: 674.275
[78,     1] loss: 731.110
[79,     1] loss: 612.719
[80,     1] loss: 719.577
Early stopping applied (best metric=0.4000401794910431)
Finished Training
Total time taken: 15.542856931686401
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1265.054
[2,     1] loss: 1253.865
[3,     1] loss: 1253.069
[4,     1] loss: 1255.266
[5,     1] loss: 1251.807
[6,     1] loss: 1251.821
[7,     1] loss: 1252.443
[8,     1] loss: 1248.710
[9,     1] loss: 1248.808
[10,     1] loss: 1247.078
[11,     1] loss: 1245.555
[12,     1] loss: 1246.606
[13,     1] loss: 1246.184
[14,     1] loss: 1238.762
[15,     1] loss: 1226.987
[16,     1] loss: 1213.438
[17,     1] loss: 1180.802
[18,     1] loss: 1151.438
[19,     1] loss: 1082.379
[20,     1] loss: 1076.393
[21,     1] loss: 1061.871
[22,     1] loss: 1008.278
[23,     1] loss: 1031.456
[24,     1] loss: 1063.896
[25,     1] loss: 1044.064
[26,     1] loss: 960.657
[27,     1] loss: 998.888
[28,     1] loss: 1003.464
[29,     1] loss: 932.521
[30,     1] loss: 984.587
[31,     1] loss: 943.662
[32,     1] loss: 950.783
[33,     1] loss: 933.908
[34,     1] loss: 938.624
[35,     1] loss: 925.066
[36,     1] loss: 894.882
[37,     1] loss: 905.492
[38,     1] loss: 876.363
[39,     1] loss: 878.562
[40,     1] loss: 875.404
[41,     1] loss: 865.526
[42,     1] loss: 897.824
[43,     1] loss: 882.644
[44,     1] loss: 896.947
[45,     1] loss: 896.616
[46,     1] loss: 814.205
[47,     1] loss: 851.371
[48,     1] loss: 828.301
[49,     1] loss: 805.268
[50,     1] loss: 753.432
[51,     1] loss: 827.475
[52,     1] loss: 911.833
[53,     1] loss: 919.421
[54,     1] loss: 747.559
[55,     1] loss: 812.294
[56,     1] loss: 753.851
[57,     1] loss: 814.384
[58,     1] loss: 771.979
[59,     1] loss: 759.687
[60,     1] loss: 691.284
[61,     1] loss: 638.885
[62,     1] loss: 671.504
[63,     1] loss: 797.305
[64,     1] loss: 959.355
[65,     1] loss: 732.961
[66,     1] loss: 755.211
[67,     1] loss: 805.952
[68,     1] loss: 776.801
[69,     1] loss: 735.556
[70,     1] loss: 761.223
[71,     1] loss: 685.555
[72,     1] loss: 751.168
[73,     1] loss: 698.705
[74,     1] loss: 607.769
[75,     1] loss: 623.278
[76,     1] loss: 680.532
[77,     1] loss: 577.756
[78,     1] loss: 540.092
[79,     1] loss: 580.282
[80,     1] loss: 829.767
[81,     1] loss: 1186.981
[82,     1] loss: 882.563
[83,     1] loss: 921.790
[84,     1] loss: 850.651
[85,     1] loss: 884.448
[86,     1] loss: 877.437
[87,     1] loss: 850.973
[88,     1] loss: 784.730
[89,     1] loss: 864.607
[90,     1] loss: 804.702
[91,     1] loss: 783.470
[92,     1] loss: 827.229
[93,     1] loss: 739.049
[94,     1] loss: 771.600
[95,     1] loss: 804.298
[96,     1] loss: 731.072
[97,     1] loss: 745.734
[98,     1] loss: 641.439
[99,     1] loss: 666.814
[100,     1] loss: 628.763
[101,     1] loss: 597.957
[102,     1] loss: 677.912
[103,     1] loss: 734.480
[104,     1] loss: 616.718
[105,     1] loss: 558.684
[106,     1] loss: 574.629
[107,     1] loss: 523.997
[108,     1] loss: 489.829
[109,     1] loss: 502.870
[110,     1] loss: 511.048
[111,     1] loss: 542.729
Early stopping applied (best metric=0.3542868196964264)
Finished Training
Total time taken: 23.008970022201538
results!
{'gpu_mode': True, 'epochs': 200, 'batch_size': 2048, 'learning_rate': 0.006572084309684114, 'test_data_ratio': 0.2, 'data_sample_mode': ['oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample'], 'crossValidation': True, 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>, 'optimizer': <class 'torch.optim.adamw.AdamW'>, 'folds': 5, 'earlyStopping': True, 'ValidationMetric': 'Validation Loss (Hydroxylation-P)', 'earlyStoppingPatience': 50, 'CV_Repeats': 5, 'Experiment Name': 'Model architecture - added max, ranges, bceloss', 'CreateFigures': False, 'weight_decay': 5.928127280138588, 'embeddingType': 'adaptiveEmbedding', 'LSTM_layers': 1, 'LSTM_hidden_size': 64, 'LSTM_dropout': 0, 'MultiTask': True, 'MultiTask_sample_method': 'balanced', 'UseUncertaintyBasedLoss': False, 'useLrWeight': True, 'CNNType': 'Musite', 'FCType': 'Adapt', 'layerToSplitOn': 'FC', 'dontAverageLoss': False, 'useWeightDecayWeight': False, 'SeperateTuningLRandWD': True, 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'], 'n_trials': 500, 'FloatsToTune': {'learning_rate': [1e-05, 0.01], 'weight_decay': [0, 10], 'log_base': [1.01, 3], 'learning_rate_Hydroxylation-K': [1e-05, 0.01], 'learning_rate_Hydroxylation-P': [1e-05, 0.01], 'weight_decay_Hydroxylation-P': [0, 10], 'weight_decay_Hydroxylation-K': [0, 10]}, 'IntsToTune': {}, 'log_base': 2.800375423641306, 'learning_rate_Hydroxylation-K': 0.009615758340991983, 'learning_rate_Hydroxylation-P': 0.006972971206008146, 'weight_decay_Hydroxylation-P': 1.7015322209628971, 'weight_decay_Hydroxylation-K': 5.9167531429418805, 'random_state': 2520774384, 'current_CV_Repeat': 5, 'sample_weights': [1.6224092047640803, 1], 'WeightDecayWeights': [], 'currentFold': 4}
{'Hydroxylation-K Validation Accuracy': 0.7211702127659575, 'Hydroxylation-K Validation Sensitivity': 0.6413333333333333, 'Hydroxylation-K Validation Specificity': 0.7410526315789474, 'Hydroxylation-K Validation Precision': 0.3933991983996022, 'Hydroxylation-K AUC ROC': 0.7656842105263159, 'Hydroxylation-K AUC PR': 0.5418653930508972, 'Hydroxylation-K MCC': 0.3298894573624849, 'Hydroxylation-K F1': 0.48236246654350345, 'Validation Loss (Hydroxylation-K)': 0.49309839606285094, 'Hydroxylation-P Validation Accuracy': 0.7835244505355058, 'Hydroxylation-P Validation Sensitivity': 0.840920634920635, 'Hydroxylation-P Validation Specificity': 0.7711746221756696, 'Hydroxylation-P Validation Precision': 0.44703206922163863, 'Hydroxylation-P AUC ROC': 0.8598138999926371, 'Hydroxylation-P AUC PR': 0.6000772645529404, 'Hydroxylation-P MCC': 0.4975253371642582, 'Hydroxylation-P F1': 0.5813229540809273, 'Validation Loss (Hydroxylation-P)': 0.35436455249786375, 'Validation Loss (total)': 0.8474629497528077, 'TimeToTrain': 21.82419083595276}
{'Hydroxylation-K Validation Accuracy': 0.07110914190875776, 'Hydroxylation-K Validation Sensitivity': 0.15300555410989944, 'Hydroxylation-K Validation Specificity': 0.08205857872059193, 'Hydroxylation-K Validation Precision': 0.0942705858867769, 'Hydroxylation-K AUC ROC': 0.07528430093295808, 'Hydroxylation-K AUC PR': 0.13832773993417502, 'Hydroxylation-K MCC': 0.14668134672088778, 'Hydroxylation-K F1': 0.10435399725733972, 'Validation Loss (Hydroxylation-K)': 0.08889066222538597, 'Hydroxylation-P Validation Accuracy': 0.03845170761395644, 'Hydroxylation-P Validation Sensitivity': 0.05883377817436677, 'Hydroxylation-P Validation Specificity': 0.04905739868608598, 'Hydroxylation-P Validation Precision': 0.05294359289214459, 'Hydroxylation-P AUC ROC': 0.040799662394349706, 'Hydroxylation-P AUC PR': 0.07556152288642876, 'Hydroxylation-P MCC': 0.0589525675869363, 'Hydroxylation-P F1': 0.046607098215998594, 'Validation Loss (Hydroxylation-P)': 0.04261730755422453, 'Validation Loss (total)': 0.10028151617779253, 'TimeToTrain': 6.4331268852455}
