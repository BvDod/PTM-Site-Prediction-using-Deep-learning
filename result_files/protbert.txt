{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'learning_rate': 0.006229820704773117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4002499113,
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.132526326819445}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.694
[3,     1] loss: 0.679
[4,     1] loss: 0.644
[5,     1] loss: 0.632
[6,     1] loss: 0.616
[7,     1] loss: 0.588
[8,     1] loss: 0.529
[9,     1] loss: 0.550
[10,     1] loss: 0.486
[11,     1] loss: 0.431
[12,     1] loss: 0.407
[13,     1] loss: 0.470
[14,     1] loss: 0.507
[15,     1] loss: 0.493
[16,     1] loss: 0.417
[17,     1] loss: 0.449
[18,     1] loss: 0.464
[19,     1] loss: 0.422
[20,     1] loss: 0.404
[21,     1] loss: 0.420
[22,     1] loss: 0.379
[23,     1] loss: 0.367
[24,     1] loss: 0.391
[25,     1] loss: 0.416
[26,     1] loss: 0.451
[27,     1] loss: 0.416
[28,     1] loss: 0.397
[29,     1] loss: 0.351
[30,     1] loss: 0.330
[31,     1] loss: 0.349
[32,     1] loss: 0.339
[33,     1] loss: 0.346
[34,     1] loss: 0.308
[35,     1] loss: 0.360
[36,     1] loss: 0.315
[37,     1] loss: 0.300
[38,     1] loss: 0.268
[39,     1] loss: 0.318
[40,     1] loss: 0.289
[41,     1] loss: 0.296
[42,     1] loss: 0.366
[43,     1] loss: 0.404
[44,     1] loss: 0.487
[45,     1] loss: 0.351
[46,     1] loss: 0.564
[47,     1] loss: 0.390
[48,     1] loss: 0.450
[49,     1] loss: 0.428
[50,     1] loss: 0.458
[51,     1] loss: 0.432
[52,     1] loss: 0.409
[53,     1] loss: 0.386
[54,     1] loss: 0.384
[55,     1] loss: 0.364
[56,     1] loss: 0.340
[57,     1] loss: 0.353
[58,     1] loss: 0.318
[59,     1] loss: 0.332
[60,     1] loss: 0.349
[61,     1] loss: 0.469
[62,     1] loss: 0.360
[63,     1] loss: 0.331
[64,     1] loss: 0.349
[65,     1] loss: 0.380
[66,     1] loss: 0.341
[67,     1] loss: 0.323
[68,     1] loss: 0.307
[69,     1] loss: 0.337
[70,     1] loss: 0.306
[71,     1] loss: 0.297
[72,     1] loss: 0.370
[73,     1] loss: 0.272
[74,     1] loss: 0.356
[75,     1] loss: 0.336
[76,     1] loss: 0.337
[77,     1] loss: 0.343
[78,     1] loss: 0.303
[79,     1] loss: 0.347
[80,     1] loss: 0.286
[81,     1] loss: 0.324
[82,     1] loss: 0.302
[83,     1] loss: 0.316
[84,     1] loss: 0.264
[85,     1] loss: 0.306
[86,     1] loss: 0.305
Early stopping applied (best metric=0.3065936267375946)
Finished Training
Total time taken: 12.225379467010498
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.712
[3,     1] loss: 0.691
[4,     1] loss: 0.686
[5,     1] loss: 0.680
[6,     1] loss: 0.663
[7,     1] loss: 0.639
[8,     1] loss: 0.625
[9,     1] loss: 0.595
[10,     1] loss: 0.566
[11,     1] loss: 0.536
[12,     1] loss: 0.518
[13,     1] loss: 0.485
[14,     1] loss: 0.467
[15,     1] loss: 0.401
[16,     1] loss: 0.439
[17,     1] loss: 0.413
[18,     1] loss: 0.403
[19,     1] loss: 0.376
[20,     1] loss: 0.377
[21,     1] loss: 0.408
[22,     1] loss: 0.327
[23,     1] loss: 0.323
[24,     1] loss: 0.534
[25,     1] loss: 0.343
[26,     1] loss: 0.330
[27,     1] loss: 0.377
[28,     1] loss: 0.369
[29,     1] loss: 0.376
[30,     1] loss: 0.379
[31,     1] loss: 0.331
[32,     1] loss: 0.371
[33,     1] loss: 0.396
[34,     1] loss: 0.351
[35,     1] loss: 0.390
[36,     1] loss: 0.392
[37,     1] loss: 0.392
[38,     1] loss: 0.321
[39,     1] loss: 0.339
[40,     1] loss: 0.373
[41,     1] loss: 0.349
[42,     1] loss: 0.382
[43,     1] loss: 0.366
[44,     1] loss: 0.417
[45,     1] loss: 0.342
[46,     1] loss: 0.338
[47,     1] loss: 0.331
[48,     1] loss: 0.316
[49,     1] loss: 0.377
[50,     1] loss: 0.325
[51,     1] loss: 0.342
[52,     1] loss: 0.316
[53,     1] loss: 0.517
[54,     1] loss: 0.382
[55,     1] loss: 0.338
[56,     1] loss: 0.386
[57,     1] loss: 0.351
[58,     1] loss: 0.343
[59,     1] loss: 0.306
[60,     1] loss: 0.301
[61,     1] loss: 0.291
[62,     1] loss: 0.258
[63,     1] loss: 0.301
[64,     1] loss: 0.278
[65,     1] loss: 0.339
Early stopping applied (best metric=0.4240521192550659)
Finished Training
Total time taken: 7.269998788833618
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.678
[4,     1] loss: 0.650
[5,     1] loss: 0.616
[6,     1] loss: 0.576
[7,     1] loss: 0.539
[8,     1] loss: 0.520
[9,     1] loss: 0.447
[10,     1] loss: 0.440
[11,     1] loss: 0.411
[12,     1] loss: 0.433
[13,     1] loss: 0.404
[14,     1] loss: 0.356
[15,     1] loss: 0.334
[16,     1] loss: 0.307
[17,     1] loss: 0.336
[18,     1] loss: 0.392
[19,     1] loss: 0.453
[20,     1] loss: 0.445
[21,     1] loss: 0.358
[22,     1] loss: 0.388
[23,     1] loss: 0.414
[24,     1] loss: 0.386
[25,     1] loss: 0.401
[26,     1] loss: 0.331
[27,     1] loss: 0.345
[28,     1] loss: 0.385
[29,     1] loss: 0.324
[30,     1] loss: 0.347
[31,     1] loss: 0.320
[32,     1] loss: 0.355
[33,     1] loss: 0.305
[34,     1] loss: 0.341
[35,     1] loss: 0.321
[36,     1] loss: 0.318
[37,     1] loss: 0.298
[38,     1] loss: 0.343
[39,     1] loss: 0.341
[40,     1] loss: 0.307
[41,     1] loss: 0.291
[42,     1] loss: 0.308
[43,     1] loss: 0.328
[44,     1] loss: 0.317
[45,     1] loss: 0.324
[46,     1] loss: 0.302
[47,     1] loss: 0.295
[48,     1] loss: 0.374
[49,     1] loss: 0.342
[50,     1] loss: 0.356
[51,     1] loss: 0.338
[52,     1] loss: 0.383
[53,     1] loss: 0.315
[54,     1] loss: 0.339
[55,     1] loss: 0.322
[56,     1] loss: 0.304
[57,     1] loss: 0.340
[58,     1] loss: 0.329
[59,     1] loss: 0.324
[60,     1] loss: 0.392
[61,     1] loss: 0.295
[62,     1] loss: 0.344
[63,     1] loss: 0.358
[64,     1] loss: 0.406
[65,     1] loss: 0.417
[66,     1] loss: 0.350
[67,     1] loss: 0.351
[68,     1] loss: 0.361
[69,     1] loss: 0.363
[70,     1] loss: 0.317
[71,     1] loss: 0.363
[72,     1] loss: 0.354
[73,     1] loss: 0.321
[74,     1] loss: 0.310
[75,     1] loss: 0.337
[76,     1] loss: 0.332
[77,     1] loss: 0.299
[78,     1] loss: 0.371
[79,     1] loss: 0.331
[80,     1] loss: 0.320
[81,     1] loss: 0.286
[82,     1] loss: 0.403
[83,     1] loss: 0.350
[84,     1] loss: 0.480
[85,     1] loss: 0.376
[86,     1] loss: 0.408
[87,     1] loss: 0.373
[88,     1] loss: 0.346
[89,     1] loss: 0.385
[90,     1] loss: 0.357
[91,     1] loss: 0.333
[92,     1] loss: 0.339
[93,     1] loss: 0.336
[94,     1] loss: 0.376
[95,     1] loss: 0.341
[96,     1] loss: 0.326
[97,     1] loss: 0.303
[98,     1] loss: 0.326
[99,     1] loss: 0.332
[100,     1] loss: 0.293
[101,     1] loss: 0.337
[102,     1] loss: 0.328
[103,     1] loss: 0.332
[104,     1] loss: 0.327
[105,     1] loss: 0.293
[106,     1] loss: 0.380
[107,     1] loss: 0.364
[108,     1] loss: 0.330
[109,     1] loss: 0.323
[110,     1] loss: 0.362
[111,     1] loss: 0.376
[112,     1] loss: 0.357
[113,     1] loss: 0.370
[114,     1] loss: 0.343
[115,     1] loss: 0.346
[116,     1] loss: 0.303
[117,     1] loss: 0.318
[118,     1] loss: 0.290
[119,     1] loss: 0.324
[120,     1] loss: 0.301
[121,     1] loss: 0.320
[122,     1] loss: 0.314
[123,     1] loss: 0.325
[124,     1] loss: 0.306
[125,     1] loss: 0.333
[126,     1] loss: 0.300
[127,     1] loss: 0.359
[128,     1] loss: 0.302
[129,     1] loss: 0.305
[130,     1] loss: 0.357
[131,     1] loss: 0.326
Early stopping applied (best metric=0.3610825538635254)
Finished Training
Total time taken: 14.857343435287476
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.677
[4,     1] loss: 0.657
[5,     1] loss: 0.623
[6,     1] loss: 0.598
[7,     1] loss: 0.558
[8,     1] loss: 0.538
[9,     1] loss: 0.480
[10,     1] loss: 0.463
[11,     1] loss: 0.435
[12,     1] loss: 0.430
[13,     1] loss: 0.465
[14,     1] loss: 0.451
[15,     1] loss: 0.452
[16,     1] loss: 0.373
[17,     1] loss: 0.399
[18,     1] loss: 0.373
[19,     1] loss: 0.398
[20,     1] loss: 0.411
[21,     1] loss: 0.349
[22,     1] loss: 0.360
[23,     1] loss: 0.325
[24,     1] loss: 0.345
[25,     1] loss: 0.363
[26,     1] loss: 0.376
[27,     1] loss: 0.374
[28,     1] loss: 0.389
[29,     1] loss: 0.347
[30,     1] loss: 0.329
[31,     1] loss: 0.313
[32,     1] loss: 0.341
[33,     1] loss: 0.355
[34,     1] loss: 0.309
[35,     1] loss: 0.313
[36,     1] loss: 0.260
[37,     1] loss: 0.283
[38,     1] loss: 0.302
[39,     1] loss: 0.332
[40,     1] loss: 0.328
[41,     1] loss: 0.384
[42,     1] loss: 0.297
[43,     1] loss: 0.363
[44,     1] loss: 0.317
[45,     1] loss: 0.290
[46,     1] loss: 0.299
[47,     1] loss: 0.301
[48,     1] loss: 0.307
[49,     1] loss: 0.298
[50,     1] loss: 0.315
[51,     1] loss: 0.316
[52,     1] loss: 0.288
[53,     1] loss: 0.324
[54,     1] loss: 0.317
[55,     1] loss: 0.325
[56,     1] loss: 0.321
[57,     1] loss: 0.336
[58,     1] loss: 0.301
[59,     1] loss: 0.308
[60,     1] loss: 0.325
[61,     1] loss: 0.312
[62,     1] loss: 0.338
[63,     1] loss: 0.289
[64,     1] loss: 0.303
[65,     1] loss: 0.263
[66,     1] loss: 0.329
[67,     1] loss: 0.286
[68,     1] loss: 0.269
[69,     1] loss: 0.279
[70,     1] loss: 0.272
[71,     1] loss: 0.282
Early stopping applied (best metric=0.42691928148269653)
Finished Training
Total time taken: 8.202736377716064
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.680
[4,     1] loss: 0.656
[5,     1] loss: 0.612
[6,     1] loss: 0.605
[7,     1] loss: 0.558
[8,     1] loss: 0.536
[9,     1] loss: 0.513
[10,     1] loss: 0.480
[11,     1] loss: 0.435
[12,     1] loss: 0.429
[13,     1] loss: 0.431
[14,     1] loss: 0.356
[15,     1] loss: 0.376
[16,     1] loss: 0.364
[17,     1] loss: 0.378
[18,     1] loss: 0.334
[19,     1] loss: 0.329
[20,     1] loss: 0.358
[21,     1] loss: 0.330
[22,     1] loss: 0.351
[23,     1] loss: 0.343
[24,     1] loss: 0.315
[25,     1] loss: 0.337
[26,     1] loss: 0.338
[27,     1] loss: 0.395
[28,     1] loss: 0.328
[29,     1] loss: 0.372
[30,     1] loss: 0.339
[31,     1] loss: 0.353
[32,     1] loss: 0.369
[33,     1] loss: 0.360
[34,     1] loss: 0.348
[35,     1] loss: 0.321
[36,     1] loss: 0.324
[37,     1] loss: 0.338
[38,     1] loss: 0.415
[39,     1] loss: 0.360
[40,     1] loss: 0.380
[41,     1] loss: 0.360
[42,     1] loss: 0.349
[43,     1] loss: 0.366
[44,     1] loss: 0.362
[45,     1] loss: 0.322
[46,     1] loss: 0.321
[47,     1] loss: 0.358
[48,     1] loss: 0.353
[49,     1] loss: 0.342
[50,     1] loss: 0.304
[51,     1] loss: 0.297
[52,     1] loss: 0.311
[53,     1] loss: 0.336
[54,     1] loss: 0.337
[55,     1] loss: 0.673
[56,     1] loss: 0.302
[57,     1] loss: 0.350
[58,     1] loss: 0.303
[59,     1] loss: 0.389
[60,     1] loss: 0.372
[61,     1] loss: 0.295
[62,     1] loss: 0.373
[63,     1] loss: 0.344
[64,     1] loss: 0.451
[65,     1] loss: 0.415
[66,     1] loss: 0.407
[67,     1] loss: 0.387
[68,     1] loss: 0.374
[69,     1] loss: 0.351
[70,     1] loss: 0.353
[71,     1] loss: 0.381
[72,     1] loss: 0.363
[73,     1] loss: 0.362
[74,     1] loss: 0.338
[75,     1] loss: 0.357
[76,     1] loss: 0.344
[77,     1] loss: 0.365
[78,     1] loss: 0.333
[79,     1] loss: 0.320
[80,     1] loss: 0.331
[81,     1] loss: 0.304
[82,     1] loss: 0.356
[83,     1] loss: 0.315
[84,     1] loss: 0.330
[85,     1] loss: 0.404
[86,     1] loss: 0.571
[87,     1] loss: 0.370
[88,     1] loss: 0.404
[89,     1] loss: 0.378
[90,     1] loss: 0.373
[91,     1] loss: 0.374
[92,     1] loss: 0.398
[93,     1] loss: 0.352
[94,     1] loss: 0.348
[95,     1] loss: 0.331
[96,     1] loss: 0.369
[97,     1] loss: 0.329
[98,     1] loss: 0.333
[99,     1] loss: 0.370
[100,     1] loss: 0.285
[101,     1] loss: 0.371
[102,     1] loss: 0.355
[103,     1] loss: 0.350
[104,     1] loss: 0.377
[105,     1] loss: 0.385
[106,     1] loss: 0.343
[107,     1] loss: 0.373
[108,     1] loss: 0.362
[109,     1] loss: 0.341
[110,     1] loss: 0.317
Early stopping applied (best metric=0.3692377805709839)
Finished Training
Total time taken: 13.065540552139282
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.694
[3,     1] loss: 0.687
[4,     1] loss: 0.673
[5,     1] loss: 0.652
[6,     1] loss: 0.618
[7,     1] loss: 0.592
[8,     1] loss: 0.573
[9,     1] loss: 0.557
[10,     1] loss: 0.536
[11,     1] loss: 0.538
[12,     1] loss: 0.507
[13,     1] loss: 0.488
[14,     1] loss: 0.485
[15,     1] loss: 0.452
[16,     1] loss: 0.462
[17,     1] loss: 0.478
[18,     1] loss: 0.423
[19,     1] loss: 0.420
[20,     1] loss: 0.429
[21,     1] loss: 0.400
[22,     1] loss: 0.393
[23,     1] loss: 0.396
[24,     1] loss: 0.386
[25,     1] loss: 0.366
[26,     1] loss: 0.339
[27,     1] loss: 0.355
[28,     1] loss: 0.388
[29,     1] loss: 0.370
[30,     1] loss: 0.370
[31,     1] loss: 0.340
[32,     1] loss: 0.357
[33,     1] loss: 0.434
[34,     1] loss: 0.584
[35,     1] loss: 0.394
[36,     1] loss: 0.479
[37,     1] loss: 0.395
[38,     1] loss: 0.394
[39,     1] loss: 0.422
[40,     1] loss: 0.408
[41,     1] loss: 0.390
[42,     1] loss: 0.376
[43,     1] loss: 0.376
[44,     1] loss: 0.352
[45,     1] loss: 0.346
[46,     1] loss: 0.390
[47,     1] loss: 0.330
[48,     1] loss: 0.342
[49,     1] loss: 0.371
[50,     1] loss: 0.381
[51,     1] loss: 0.412
[52,     1] loss: 0.366
[53,     1] loss: 0.389
[54,     1] loss: 0.427
[55,     1] loss: 0.388
[56,     1] loss: 0.364
[57,     1] loss: 0.377
[58,     1] loss: 0.351
[59,     1] loss: 0.340
[60,     1] loss: 0.357
[61,     1] loss: 0.367
[62,     1] loss: 0.388
[63,     1] loss: 0.331
Early stopping applied (best metric=0.4314090609550476)
Finished Training
Total time taken: 7.236511468887329
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.670
[4,     1] loss: 0.636
[5,     1] loss: 0.607
[6,     1] loss: 0.546
[7,     1] loss: 0.517
[8,     1] loss: 0.492
[9,     1] loss: 0.450
[10,     1] loss: 0.431
[11,     1] loss: 0.460
[12,     1] loss: 0.470
[13,     1] loss: 0.429
[14,     1] loss: 0.467
[15,     1] loss: 0.397
[16,     1] loss: 0.457
[17,     1] loss: 0.369
[18,     1] loss: 0.422
[19,     1] loss: 0.399
[20,     1] loss: 0.417
[21,     1] loss: 0.362
[22,     1] loss: 0.333
[23,     1] loss: 0.339
[24,     1] loss: 0.328
[25,     1] loss: 0.342
[26,     1] loss: 0.341
[27,     1] loss: 0.382
[28,     1] loss: 0.476
[29,     1] loss: 0.521
[30,     1] loss: 0.440
[31,     1] loss: 0.418
[32,     1] loss: 0.384
[33,     1] loss: 0.345
[34,     1] loss: 0.384
[35,     1] loss: 0.380
[36,     1] loss: 0.372
[37,     1] loss: 0.352
[38,     1] loss: 0.357
[39,     1] loss: 0.340
[40,     1] loss: 0.331
[41,     1] loss: 0.388
[42,     1] loss: 0.363
[43,     1] loss: 0.326
[44,     1] loss: 0.305
[45,     1] loss: 0.360
[46,     1] loss: 0.316
[47,     1] loss: 0.382
[48,     1] loss: 0.327
[49,     1] loss: 0.343
[50,     1] loss: 0.321
[51,     1] loss: 0.303
[52,     1] loss: 0.308
[53,     1] loss: 0.332
[54,     1] loss: 0.330
[55,     1] loss: 0.360
[56,     1] loss: 0.365
[57,     1] loss: 0.343
[58,     1] loss: 0.316
[59,     1] loss: 0.359
[60,     1] loss: 0.340
[61,     1] loss: 0.345
[62,     1] loss: 0.362
[63,     1] loss: 0.392
[64,     1] loss: 0.337
[65,     1] loss: 0.376
[66,     1] loss: 0.364
[67,     1] loss: 0.325
[68,     1] loss: 0.313
[69,     1] loss: 0.365
[70,     1] loss: 0.296
[71,     1] loss: 0.299
[72,     1] loss: 0.300
[73,     1] loss: 0.306
[74,     1] loss: 0.285
[75,     1] loss: 0.261
[76,     1] loss: 0.313
[77,     1] loss: 0.294
[78,     1] loss: 0.325
[79,     1] loss: 0.308
[80,     1] loss: 0.386
[81,     1] loss: 0.310
[82,     1] loss: 0.330
[83,     1] loss: 0.293
[84,     1] loss: 0.243
[85,     1] loss: 0.279
[86,     1] loss: 0.265
[87,     1] loss: 0.292
[88,     1] loss: 0.266
[89,     1] loss: 0.261
[90,     1] loss: 0.258
[91,     1] loss: 0.287
[92,     1] loss: 0.316
[93,     1] loss: 0.313
[94,     1] loss: 0.398
[95,     1] loss: 0.777
[96,     1] loss: 0.431
[97,     1] loss: 0.397
[98,     1] loss: 0.425
[99,     1] loss: 0.423
[100,     1] loss: 0.452
[101,     1] loss: 0.407
[102,     1] loss: 0.366
Early stopping applied (best metric=0.33545148372650146)
Finished Training
Total time taken: 12.233107089996338
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.639
[5,     1] loss: 0.601
[6,     1] loss: 0.576
[7,     1] loss: 0.517
[8,     1] loss: 0.499
[9,     1] loss: 0.462
[10,     1] loss: 0.446
[11,     1] loss: 0.472
[12,     1] loss: 0.448
[13,     1] loss: 0.455
[14,     1] loss: 0.397
[15,     1] loss: 0.424
[16,     1] loss: 0.387
[17,     1] loss: 0.390
[18,     1] loss: 0.405
[19,     1] loss: 0.400
[20,     1] loss: 0.401
[21,     1] loss: 0.445
[22,     1] loss: 0.389
[23,     1] loss: 0.393
[24,     1] loss: 0.372
[25,     1] loss: 0.430
[26,     1] loss: 0.420
[27,     1] loss: 0.417
[28,     1] loss: 0.443
[29,     1] loss: 0.463
[30,     1] loss: 0.361
[31,     1] loss: 0.376
[32,     1] loss: 0.368
[33,     1] loss: 0.372
[34,     1] loss: 0.375
[35,     1] loss: 0.356
[36,     1] loss: 0.329
[37,     1] loss: 0.318
[38,     1] loss: 0.374
[39,     1] loss: 0.297
[40,     1] loss: 0.328
[41,     1] loss: 0.256
[42,     1] loss: 0.331
[43,     1] loss: 0.394
[44,     1] loss: 0.388
[45,     1] loss: 0.295
[46,     1] loss: 0.350
[47,     1] loss: 0.313
[48,     1] loss: 0.349
[49,     1] loss: 0.378
[50,     1] loss: 0.259
[51,     1] loss: 0.319
[52,     1] loss: 0.312
[53,     1] loss: 0.280
[54,     1] loss: 0.417
[55,     1] loss: 0.379
[56,     1] loss: 0.386
[57,     1] loss: 0.371
[58,     1] loss: 0.321
[59,     1] loss: 0.356
[60,     1] loss: 0.348
[61,     1] loss: 0.356
[62,     1] loss: 0.317
[63,     1] loss: 0.313
[64,     1] loss: 0.325
[65,     1] loss: 0.331
[66,     1] loss: 0.331
[67,     1] loss: 0.320
[68,     1] loss: 0.295
[69,     1] loss: 0.309
[70,     1] loss: 0.315
[71,     1] loss: 0.356
[72,     1] loss: 0.360
[73,     1] loss: 0.391
[74,     1] loss: 0.284
[75,     1] loss: 0.354
[76,     1] loss: 0.324
[77,     1] loss: 0.297
[78,     1] loss: 0.302
[79,     1] loss: 0.285
[80,     1] loss: 0.277
[81,     1] loss: 0.281
[82,     1] loss: 0.312
[83,     1] loss: 0.287
Early stopping applied (best metric=0.3715611696243286)
Finished Training
Total time taken: 9.972391605377197
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.684
[4,     1] loss: 0.664
[5,     1] loss: 0.642
[6,     1] loss: 0.626
[7,     1] loss: 0.581
[8,     1] loss: 0.574
[9,     1] loss: 0.523
[10,     1] loss: 0.551
[11,     1] loss: 0.477
[12,     1] loss: 0.499
[13,     1] loss: 0.482
[14,     1] loss: 0.478
[15,     1] loss: 0.443
[16,     1] loss: 0.441
[17,     1] loss: 0.411
[18,     1] loss: 0.449
[19,     1] loss: 0.413
[20,     1] loss: 0.426
[21,     1] loss: 0.511
[22,     1] loss: 0.414
[23,     1] loss: 0.507
[24,     1] loss: 0.405
[25,     1] loss: 0.385
[26,     1] loss: 0.395
[27,     1] loss: 0.377
[28,     1] loss: 0.413
[29,     1] loss: 0.409
[30,     1] loss: 0.386
[31,     1] loss: 0.399
[32,     1] loss: 0.357
[33,     1] loss: 0.374
[34,     1] loss: 0.417
[35,     1] loss: 0.374
[36,     1] loss: 0.328
[37,     1] loss: 0.332
[38,     1] loss: 0.334
[39,     1] loss: 0.354
[40,     1] loss: 0.299
[41,     1] loss: 0.351
[42,     1] loss: 0.375
[43,     1] loss: 0.383
[44,     1] loss: 0.400
[45,     1] loss: 0.334
[46,     1] loss: 0.378
[47,     1] loss: 0.318
[48,     1] loss: 0.344
[49,     1] loss: 0.347
[50,     1] loss: 0.311
[51,     1] loss: 0.314
[52,     1] loss: 0.373
[53,     1] loss: 0.365
[54,     1] loss: 0.315
[55,     1] loss: 0.290
[56,     1] loss: 0.349
[57,     1] loss: 0.328
[58,     1] loss: 0.333
[59,     1] loss: 0.332
[60,     1] loss: 0.284
[61,     1] loss: 0.309
[62,     1] loss: 0.314
[63,     1] loss: 0.331
[64,     1] loss: 0.377
[65,     1] loss: 0.372
[66,     1] loss: 0.293
[67,     1] loss: 0.283
[68,     1] loss: 0.354
[69,     1] loss: 0.355
[70,     1] loss: 0.318
[71,     1] loss: 0.303
[72,     1] loss: 0.328
[73,     1] loss: 0.306
[74,     1] loss: 0.311
[75,     1] loss: 0.299
[76,     1] loss: 0.309
[77,     1] loss: 0.284
[78,     1] loss: 0.346
[79,     1] loss: 0.304
[80,     1] loss: 0.283
[81,     1] loss: 0.294
[82,     1] loss: 0.302
[83,     1] loss: 0.292
[84,     1] loss: 0.298
[85,     1] loss: 0.285
[86,     1] loss: 0.258
[87,     1] loss: 0.275
[88,     1] loss: 0.310
[89,     1] loss: 0.283
[90,     1] loss: 0.284
[91,     1] loss: 0.254
[92,     1] loss: 0.258
[93,     1] loss: 0.320
[94,     1] loss: 0.385
[95,     1] loss: 0.329
[96,     1] loss: 0.397
[97,     1] loss: 0.368
[98,     1] loss: 0.293
[99,     1] loss: 0.309
[100,     1] loss: 0.337
[101,     1] loss: 0.302
[102,     1] loss: 0.303
[103,     1] loss: 0.287
[104,     1] loss: 0.323
[105,     1] loss: 0.279
[106,     1] loss: 0.308
[107,     1] loss: 0.268
[108,     1] loss: 0.259
[109,     1] loss: 0.267
[110,     1] loss: 0.269
[111,     1] loss: 0.285
[112,     1] loss: 0.250
[113,     1] loss: 0.263
[114,     1] loss: 0.276
[115,     1] loss: 0.271
[116,     1] loss: 0.216
[117,     1] loss: 0.262
[118,     1] loss: 0.281
[119,     1] loss: 0.305
[120,     1] loss: 0.262
[121,     1] loss: 0.305
[122,     1] loss: 0.283
[123,     1] loss: 0.257
[124,     1] loss: 0.307
[125,     1] loss: 0.351
[126,     1] loss: 0.362
[127,     1] loss: 0.344
[128,     1] loss: 0.289
[129,     1] loss: 0.288
[130,     1] loss: 0.332
[131,     1] loss: 0.318
[132,     1] loss: 0.274
[133,     1] loss: 0.285
[134,     1] loss: 0.303
[135,     1] loss: 0.271
[136,     1] loss: 0.271
[137,     1] loss: 0.300
[138,     1] loss: 0.273
[139,     1] loss: 0.310
[140,     1] loss: 0.414
[141,     1] loss: 0.387
[142,     1] loss: 0.441
[143,     1] loss: 0.335
[144,     1] loss: 0.395
[145,     1] loss: 0.333
[146,     1] loss: 0.314
[147,     1] loss: 0.295
[148,     1] loss: 0.302
[149,     1] loss: 0.286
[150,     1] loss: 0.310
[151,     1] loss: 0.271
[152,     1] loss: 0.324
[153,     1] loss: 0.280
[154,     1] loss: 0.275
[155,     1] loss: 0.234
[156,     1] loss: 0.262
[157,     1] loss: 0.256
[158,     1] loss: 0.293
[159,     1] loss: 0.353
[160,     1] loss: 0.401
[161,     1] loss: 0.373
[162,     1] loss: 0.343
[163,     1] loss: 0.340
Early stopping applied (best metric=0.20449180901050568)
Finished Training
Total time taken: 18.27372908592224
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.694
[3,     1] loss: 0.684
[4,     1] loss: 0.665
[5,     1] loss: 0.634
[6,     1] loss: 0.598
[7,     1] loss: 0.554
[8,     1] loss: 0.552
[9,     1] loss: 0.507
[10,     1] loss: 0.489
[11,     1] loss: 0.471
[12,     1] loss: 0.465
[13,     1] loss: 0.447
[14,     1] loss: 0.404
[15,     1] loss: 0.470
[16,     1] loss: 0.380
[17,     1] loss: 0.452
[18,     1] loss: 0.407
[19,     1] loss: 0.439
[20,     1] loss: 0.396
[21,     1] loss: 0.419
[22,     1] loss: 0.386
[23,     1] loss: 0.333
[24,     1] loss: 0.332
[25,     1] loss: 0.315
[26,     1] loss: 0.331
[27,     1] loss: 0.291
[28,     1] loss: 0.313
[29,     1] loss: 0.253
[30,     1] loss: 0.325
[31,     1] loss: 0.559
[32,     1] loss: 0.395
[33,     1] loss: 0.465
[34,     1] loss: 0.380
[35,     1] loss: 0.400
[36,     1] loss: 0.444
[37,     1] loss: 0.397
[38,     1] loss: 0.396
[39,     1] loss: 0.397
[40,     1] loss: 0.369
[41,     1] loss: 0.336
[42,     1] loss: 0.317
[43,     1] loss: 0.340
[44,     1] loss: 0.326
[45,     1] loss: 0.333
[46,     1] loss: 0.508
[47,     1] loss: 0.351
[48,     1] loss: 0.348
[49,     1] loss: 0.366
[50,     1] loss: 0.359
[51,     1] loss: 0.401
[52,     1] loss: 0.356
[53,     1] loss: 0.318
[54,     1] loss: 0.305
[55,     1] loss: 0.314
[56,     1] loss: 0.318
[57,     1] loss: 0.313
[58,     1] loss: 0.323
[59,     1] loss: 0.312
[60,     1] loss: 0.357
[61,     1] loss: 0.368
[62,     1] loss: 0.309
[63,     1] loss: 0.324
[64,     1] loss: 0.310
[65,     1] loss: 0.316
Early stopping applied (best metric=0.36787307262420654)
Finished Training
Total time taken: 7.22824764251709
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.689
[3,     1] loss: 0.673
[4,     1] loss: 0.649
[5,     1] loss: 0.615
[6,     1] loss: 0.573
[7,     1] loss: 0.556
[8,     1] loss: 0.518
[9,     1] loss: 0.492
[10,     1] loss: 0.461
[11,     1] loss: 0.436
[12,     1] loss: 0.453
[13,     1] loss: 0.434
[14,     1] loss: 0.396
[15,     1] loss: 0.429
[16,     1] loss: 0.423
[17,     1] loss: 0.408
[18,     1] loss: 0.418
[19,     1] loss: 0.385
[20,     1] loss: 0.365
[21,     1] loss: 0.358
[22,     1] loss: 0.350
[23,     1] loss: 0.372
[24,     1] loss: 0.363
[25,     1] loss: 0.377
[26,     1] loss: 0.351
[27,     1] loss: 0.326
[28,     1] loss: 0.351
[29,     1] loss: 0.330
[30,     1] loss: 0.341
[31,     1] loss: 0.319
[32,     1] loss: 0.379
[33,     1] loss: 0.272
[34,     1] loss: 0.365
[35,     1] loss: 0.330
[36,     1] loss: 0.420
[37,     1] loss: 0.317
[38,     1] loss: 0.350
[39,     1] loss: 0.363
[40,     1] loss: 0.318
[41,     1] loss: 0.349
[42,     1] loss: 0.330
[43,     1] loss: 0.321
[44,     1] loss: 0.325
[45,     1] loss: 0.334
[46,     1] loss: 0.312
[47,     1] loss: 0.295
[48,     1] loss: 0.297
[49,     1] loss: 0.286
[50,     1] loss: 0.307
[51,     1] loss: 0.275
[52,     1] loss: 0.261
[53,     1] loss: 0.279
[54,     1] loss: 0.395
[55,     1] loss: 0.545
[56,     1] loss: 0.303
[57,     1] loss: 0.422
[58,     1] loss: 0.386
[59,     1] loss: 0.349
[60,     1] loss: 0.360
[61,     1] loss: 0.360
[62,     1] loss: 0.336
[63,     1] loss: 0.337
[64,     1] loss: 0.345
[65,     1] loss: 0.299
[66,     1] loss: 0.317
[67,     1] loss: 0.288
[68,     1] loss: 0.275
[69,     1] loss: 0.276
[70,     1] loss: 0.485
[71,     1] loss: 0.280
[72,     1] loss: 0.278
[73,     1] loss: 0.309
[74,     1] loss: 0.357
[75,     1] loss: 0.329
[76,     1] loss: 0.338
[77,     1] loss: 0.380
[78,     1] loss: 0.338
[79,     1] loss: 0.297
[80,     1] loss: 0.289
[81,     1] loss: 0.302
[82,     1] loss: 0.349
[83,     1] loss: 0.314
[84,     1] loss: 0.302
[85,     1] loss: 0.305
[86,     1] loss: 0.311
[87,     1] loss: 0.281
[88,     1] loss: 0.350
[89,     1] loss: 0.350
[90,     1] loss: 0.361
[91,     1] loss: 0.320
[92,     1] loss: 0.317
[93,     1] loss: 0.344
[94,     1] loss: 0.311
[95,     1] loss: 0.328
[96,     1] loss: 0.353
[97,     1] loss: 0.349
Early stopping applied (best metric=0.35456520318984985)
Finished Training
Total time taken: 10.82495903968811
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.677
[4,     1] loss: 0.651
[5,     1] loss: 0.627
[6,     1] loss: 0.593
[7,     1] loss: 0.571
[8,     1] loss: 0.531
[9,     1] loss: 0.507
[10,     1] loss: 0.474
[11,     1] loss: 0.463
[12,     1] loss: 0.451
[13,     1] loss: 0.454
[14,     1] loss: 0.420
[15,     1] loss: 0.402
[16,     1] loss: 0.391
[17,     1] loss: 0.487
[18,     1] loss: 0.441
[19,     1] loss: 0.432
[20,     1] loss: 0.368
[21,     1] loss: 0.385
[22,     1] loss: 0.383
[23,     1] loss: 0.371
[24,     1] loss: 0.325
[25,     1] loss: 0.373
[26,     1] loss: 0.346
[27,     1] loss: 0.363
[28,     1] loss: 0.334
[29,     1] loss: 0.321
[30,     1] loss: 0.309
[31,     1] loss: 0.295
[32,     1] loss: 0.312
[33,     1] loss: 0.293
[34,     1] loss: 0.325
[35,     1] loss: 0.296
[36,     1] loss: 0.285
[37,     1] loss: 0.289
[38,     1] loss: 0.328
[39,     1] loss: 0.292
[40,     1] loss: 0.308
[41,     1] loss: 0.378
[42,     1] loss: 0.533
[43,     1] loss: 0.328
[44,     1] loss: 0.428
[45,     1] loss: 0.332
[46,     1] loss: 0.351
[47,     1] loss: 0.344
[48,     1] loss: 0.369
[49,     1] loss: 0.333
[50,     1] loss: 0.325
[51,     1] loss: 0.315
[52,     1] loss: 0.299
[53,     1] loss: 0.320
[54,     1] loss: 0.291
[55,     1] loss: 0.362
[56,     1] loss: 0.322
[57,     1] loss: 0.318
[58,     1] loss: 0.258
[59,     1] loss: 0.325
[60,     1] loss: 0.295
[61,     1] loss: 0.280
[62,     1] loss: 0.298
[63,     1] loss: 0.249
[64,     1] loss: 0.314
[65,     1] loss: 0.341
[66,     1] loss: 0.282
[67,     1] loss: 0.313
[68,     1] loss: 0.301
[69,     1] loss: 0.308
[70,     1] loss: 0.279
[71,     1] loss: 0.283
[72,     1] loss: 0.321
[73,     1] loss: 0.360
[74,     1] loss: 0.330
[75,     1] loss: 0.355
[76,     1] loss: 0.325
[77,     1] loss: 0.309
[78,     1] loss: 0.285
[79,     1] loss: 0.316
[80,     1] loss: 0.283
[81,     1] loss: 0.395
[82,     1] loss: 0.391
[83,     1] loss: 0.342
[84,     1] loss: 0.329
[85,     1] loss: 0.337
[86,     1] loss: 0.334
[87,     1] loss: 0.371
Early stopping applied (best metric=0.45303940773010254)
Finished Training
Total time taken: 9.741047859191895
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.688
[4,     1] loss: 0.664
[5,     1] loss: 0.630
[6,     1] loss: 0.602
[7,     1] loss: 0.575
[8,     1] loss: 0.551
[9,     1] loss: 0.491
[10,     1] loss: 0.467
[11,     1] loss: 0.430
[12,     1] loss: 0.431
[13,     1] loss: 0.460
[14,     1] loss: 0.410
[15,     1] loss: 0.380
[16,     1] loss: 0.374
[17,     1] loss: 0.388
[18,     1] loss: 0.350
[19,     1] loss: 0.377
[20,     1] loss: 0.323
[21,     1] loss: 0.435
[22,     1] loss: 0.360
[23,     1] loss: 0.418
[24,     1] loss: 0.336
[25,     1] loss: 0.424
[26,     1] loss: 0.315
[27,     1] loss: 0.338
[28,     1] loss: 0.315
[29,     1] loss: 0.335
[30,     1] loss: 0.348
[31,     1] loss: 0.345
[32,     1] loss: 0.330
[33,     1] loss: 0.290
[34,     1] loss: 0.330
[35,     1] loss: 0.335
[36,     1] loss: 0.335
[37,     1] loss: 0.290
[38,     1] loss: 0.367
[39,     1] loss: 0.283
[40,     1] loss: 0.311
[41,     1] loss: 0.315
[42,     1] loss: 0.308
[43,     1] loss: 0.316
[44,     1] loss: 0.293
[45,     1] loss: 0.296
[46,     1] loss: 0.273
[47,     1] loss: 0.265
[48,     1] loss: 0.311
[49,     1] loss: 0.531
[50,     1] loss: 0.810
[51,     1] loss: 0.454
[52,     1] loss: 0.463
[53,     1] loss: 0.427
[54,     1] loss: 0.475
[55,     1] loss: 0.469
[56,     1] loss: 0.456
[57,     1] loss: 0.481
[58,     1] loss: 0.468
[59,     1] loss: 0.447
[60,     1] loss: 0.406
[61,     1] loss: 0.412
[62,     1] loss: 0.405
[63,     1] loss: 0.357
[64,     1] loss: 0.362
[65,     1] loss: 0.310
[66,     1] loss: 0.320
[67,     1] loss: 0.304
[68,     1] loss: 0.296
Early stopping applied (best metric=0.4053305387496948)
Finished Training
Total time taken: 7.6212849617004395
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.658
[4,     1] loss: 0.648
[5,     1] loss: 0.611
[6,     1] loss: 0.587
[7,     1] loss: 0.517
[8,     1] loss: 0.490
[9,     1] loss: 0.467
[10,     1] loss: 0.475
[11,     1] loss: 0.425
[12,     1] loss: 0.445
[13,     1] loss: 0.419
[14,     1] loss: 0.390
[15,     1] loss: 0.370
[16,     1] loss: 0.361
[17,     1] loss: 0.352
[18,     1] loss: 0.390
[19,     1] loss: 0.488
[20,     1] loss: 0.369
[21,     1] loss: 0.421
[22,     1] loss: 0.380
[23,     1] loss: 0.418
[24,     1] loss: 0.442
[25,     1] loss: 0.394
[26,     1] loss: 0.398
[27,     1] loss: 0.423
[28,     1] loss: 0.356
[29,     1] loss: 0.360
[30,     1] loss: 0.538
[31,     1] loss: 0.344
[32,     1] loss: 0.450
[33,     1] loss: 0.432
[34,     1] loss: 0.406
[35,     1] loss: 0.353
[36,     1] loss: 0.367
[37,     1] loss: 0.377
[38,     1] loss: 0.355
[39,     1] loss: 0.373
[40,     1] loss: 0.341
[41,     1] loss: 0.360
[42,     1] loss: 0.378
[43,     1] loss: 0.331
[44,     1] loss: 0.358
[45,     1] loss: 0.332
[46,     1] loss: 0.307
[47,     1] loss: 0.320
[48,     1] loss: 0.302
[49,     1] loss: 0.297
[50,     1] loss: 0.303
[51,     1] loss: 0.523
[52,     1] loss: 0.303
[53,     1] loss: 0.318
[54,     1] loss: 0.287
[55,     1] loss: 0.415
[56,     1] loss: 0.331
[57,     1] loss: 0.358
[58,     1] loss: 0.335
[59,     1] loss: 0.278
[60,     1] loss: 0.348
[61,     1] loss: 0.567
[62,     1] loss: 0.444
[63,     1] loss: 0.319
[64,     1] loss: 0.341
[65,     1] loss: 0.365
[66,     1] loss: 0.343
[67,     1] loss: 0.338
[68,     1] loss: 0.323
[69,     1] loss: 0.315
[70,     1] loss: 0.384
[71,     1] loss: 0.449
[72,     1] loss: 0.368
[73,     1] loss: 0.353
[74,     1] loss: 0.411
[75,     1] loss: 0.342
[76,     1] loss: 0.317
[77,     1] loss: 0.352
[78,     1] loss: 0.364
[79,     1] loss: 0.365
[80,     1] loss: 0.324
[81,     1] loss: 0.325
[82,     1] loss: 0.308
[83,     1] loss: 0.305
[84,     1] loss: 0.315
[85,     1] loss: 0.335
[86,     1] loss: 0.323
[87,     1] loss: 0.319
[88,     1] loss: 0.354
[89,     1] loss: 0.346
[90,     1] loss: 0.325
[91,     1] loss: 0.299
[92,     1] loss: 0.340
[93,     1] loss: 0.319
[94,     1] loss: 0.285
[95,     1] loss: 0.360
[96,     1] loss: 0.342
[97,     1] loss: 0.348
[98,     1] loss: 0.325
[99,     1] loss: 0.308
[100,     1] loss: 0.325
[101,     1] loss: 0.311
[102,     1] loss: 0.340
[103,     1] loss: 0.328
[104,     1] loss: 0.364
[105,     1] loss: 0.446
[106,     1] loss: 0.369
[107,     1] loss: 0.395
[108,     1] loss: 0.429
[109,     1] loss: 0.374
[110,     1] loss: 0.383
[111,     1] loss: 0.393
[112,     1] loss: 0.367
[113,     1] loss: 0.360
[114,     1] loss: 0.337
[115,     1] loss: 0.337
[116,     1] loss: 0.366
[117,     1] loss: 0.352
[118,     1] loss: 0.325
[119,     1] loss: 0.382
[120,     1] loss: 0.332
[121,     1] loss: 0.329
[122,     1] loss: 0.318
[123,     1] loss: 0.314
[124,     1] loss: 0.323
[125,     1] loss: 0.320
[126,     1] loss: 0.326
[127,     1] loss: 0.746
[128,     1] loss: 0.443
[129,     1] loss: 0.385
[130,     1] loss: 0.384
[131,     1] loss: 0.338
[132,     1] loss: 0.374
[133,     1] loss: 0.391
[134,     1] loss: 0.352
[135,     1] loss: 0.363
[136,     1] loss: 0.386
[137,     1] loss: 0.384
[138,     1] loss: 0.329
[139,     1] loss: 0.343
[140,     1] loss: 0.442
[141,     1] loss: 0.359
Early stopping applied (best metric=0.28887927532196045)
Finished Training
Total time taken: 15.74275827407837
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.689
[3,     1] loss: 0.684
[4,     1] loss: 0.666
[5,     1] loss: 0.633
[6,     1] loss: 0.591
[7,     1] loss: 0.566
[8,     1] loss: 0.517
[9,     1] loss: 0.501
[10,     1] loss: 0.452
[11,     1] loss: 0.453
[12,     1] loss: 0.391
[13,     1] loss: 0.426
[14,     1] loss: 0.398
[15,     1] loss: 0.392
[16,     1] loss: 0.370
[17,     1] loss: 0.312
[18,     1] loss: 0.318
[19,     1] loss: 0.339
[20,     1] loss: 0.343
[21,     1] loss: 0.327
[22,     1] loss: 0.366
[23,     1] loss: 0.360
[24,     1] loss: 0.313
[25,     1] loss: 0.288
[26,     1] loss: 0.299
[27,     1] loss: 0.342
[28,     1] loss: 0.348
[29,     1] loss: 0.310
[30,     1] loss: 0.361
[31,     1] loss: 0.308
[32,     1] loss: 0.399
[33,     1] loss: 0.404
[34,     1] loss: 0.399
[35,     1] loss: 0.331
[36,     1] loss: 0.402
[37,     1] loss: 0.378
[38,     1] loss: 0.370
[39,     1] loss: 0.379
[40,     1] loss: 0.363
[41,     1] loss: 0.365
[42,     1] loss: 0.365
[43,     1] loss: 0.343
[44,     1] loss: 0.341
[45,     1] loss: 0.336
[46,     1] loss: 0.294
[47,     1] loss: 0.344
[48,     1] loss: 0.300
[49,     1] loss: 0.306
[50,     1] loss: 0.305
[51,     1] loss: 0.354
[52,     1] loss: 0.271
[53,     1] loss: 0.344
[54,     1] loss: 0.324
[55,     1] loss: 0.336
[56,     1] loss: 0.304
[57,     1] loss: 0.272
[58,     1] loss: 0.281
[59,     1] loss: 0.255
[60,     1] loss: 0.305
[61,     1] loss: 0.263
[62,     1] loss: 0.224
Early stopping applied (best metric=0.4721866250038147)
Finished Training
Total time taken: 6.9275147914886475
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.689
[3,     1] loss: 0.662
[4,     1] loss: 0.627
[5,     1] loss: 0.590
[6,     1] loss: 0.558
[7,     1] loss: 0.515
[8,     1] loss: 0.491
[9,     1] loss: 0.437
[10,     1] loss: 0.443
[11,     1] loss: 0.451
[12,     1] loss: 0.451
[13,     1] loss: 0.399
[14,     1] loss: 0.496
[15,     1] loss: 0.439
[16,     1] loss: 0.385
[17,     1] loss: 0.403
[18,     1] loss: 0.405
[19,     1] loss: 0.398
[20,     1] loss: 0.395
[21,     1] loss: 0.629
[22,     1] loss: 0.373
[23,     1] loss: 0.445
[24,     1] loss: 0.415
[25,     1] loss: 0.417
[26,     1] loss: 0.417
[27,     1] loss: 0.360
[28,     1] loss: 0.431
[29,     1] loss: 0.357
[30,     1] loss: 0.399
[31,     1] loss: 0.372
[32,     1] loss: 0.383
[33,     1] loss: 0.375
[34,     1] loss: 0.378
[35,     1] loss: 0.377
[36,     1] loss: 0.394
[37,     1] loss: 0.377
[38,     1] loss: 0.385
[39,     1] loss: 0.391
[40,     1] loss: 0.387
[41,     1] loss: 0.323
[42,     1] loss: 0.335
[43,     1] loss: 0.312
[44,     1] loss: 0.301
[45,     1] loss: 0.403
[46,     1] loss: 0.296
[47,     1] loss: 0.351
[48,     1] loss: 0.286
[49,     1] loss: 0.293
[50,     1] loss: 0.276
[51,     1] loss: 0.273
[52,     1] loss: 0.282
[53,     1] loss: 0.345
[54,     1] loss: 0.330
[55,     1] loss: 0.301
[56,     1] loss: 0.277
[57,     1] loss: 0.287
[58,     1] loss: 0.298
[59,     1] loss: 0.303
[60,     1] loss: 0.267
[61,     1] loss: 0.315
[62,     1] loss: 0.328
[63,     1] loss: 0.441
[64,     1] loss: 0.317
[65,     1] loss: 0.369
[66,     1] loss: 0.315
[67,     1] loss: 0.339
[68,     1] loss: 0.328
[69,     1] loss: 0.338
[70,     1] loss: 0.295
[71,     1] loss: 0.257
[72,     1] loss: 0.283
[73,     1] loss: 0.284
[74,     1] loss: 0.339
[75,     1] loss: 0.341
[76,     1] loss: 0.318
[77,     1] loss: 0.308
[78,     1] loss: 0.293
[79,     1] loss: 0.251
[80,     1] loss: 0.293
[81,     1] loss: 0.266
[82,     1] loss: 0.296
[83,     1] loss: 0.278
[84,     1] loss: 0.310
[85,     1] loss: 0.289
[86,     1] loss: 0.262
[87,     1] loss: 0.306
[88,     1] loss: 0.303
[89,     1] loss: 0.280
[90,     1] loss: 0.265
[91,     1] loss: 0.238
[92,     1] loss: 0.328
[93,     1] loss: 0.978
[94,     1] loss: 0.372
[95,     1] loss: 0.373
[96,     1] loss: 0.406
[97,     1] loss: 0.412
[98,     1] loss: 0.406
[99,     1] loss: 0.394
[100,     1] loss: 0.402
[101,     1] loss: 0.380
[102,     1] loss: 0.357
[103,     1] loss: 0.353
[104,     1] loss: 0.330
[105,     1] loss: 0.405
[106,     1] loss: 0.426
[107,     1] loss: 0.330
[108,     1] loss: 0.386
[109,     1] loss: 0.403
[110,     1] loss: 0.337
[111,     1] loss: 0.374
[112,     1] loss: 0.362
[113,     1] loss: 0.402
[114,     1] loss: 0.353
[115,     1] loss: 0.357
[116,     1] loss: 0.326
[117,     1] loss: 0.324
[118,     1] loss: 0.307
[119,     1] loss: 0.304
[120,     1] loss: 0.309
[121,     1] loss: 0.302
[122,     1] loss: 0.320
[123,     1] loss: 0.316
[124,     1] loss: 0.440
[125,     1] loss: 0.313
[126,     1] loss: 0.358
[127,     1] loss: 0.351
[128,     1] loss: 0.487
[129,     1] loss: 0.335
[130,     1] loss: 0.323
[131,     1] loss: 0.351
[132,     1] loss: 0.316
[133,     1] loss: 0.323
[134,     1] loss: 0.310
[135,     1] loss: 0.278
[136,     1] loss: 0.286
[137,     1] loss: 0.278
[138,     1] loss: 0.311
[139,     1] loss: 0.279
[140,     1] loss: 0.308
Early stopping applied (best metric=0.2421424835920334)
Finished Training
Total time taken: 15.560615062713623
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.693
[3,     1] loss: 0.679
[4,     1] loss: 0.667
[5,     1] loss: 0.639
[6,     1] loss: 0.609
[7,     1] loss: 0.569
[8,     1] loss: 0.532
[9,     1] loss: 0.535
[10,     1] loss: 0.523
[11,     1] loss: 0.443
[12,     1] loss: 0.430
[13,     1] loss: 0.469
[14,     1] loss: 0.395
[15,     1] loss: 0.457
[16,     1] loss: 0.462
[17,     1] loss: 0.363
[18,     1] loss: 0.421
[19,     1] loss: 0.393
[20,     1] loss: 0.397
[21,     1] loss: 0.360
[22,     1] loss: 0.380
[23,     1] loss: 0.363
[24,     1] loss: 0.395
[25,     1] loss: 0.372
[26,     1] loss: 0.420
[27,     1] loss: 0.326
[28,     1] loss: 0.443
[29,     1] loss: 0.300
[30,     1] loss: 0.379
[31,     1] loss: 0.332
[32,     1] loss: 0.364
[33,     1] loss: 0.392
[34,     1] loss: 0.308
[35,     1] loss: 0.289
[36,     1] loss: 0.279
[37,     1] loss: 0.315
[38,     1] loss: 0.321
[39,     1] loss: 0.305
[40,     1] loss: 0.287
[41,     1] loss: 0.324
[42,     1] loss: 0.271
[43,     1] loss: 0.307
[44,     1] loss: 0.244
[45,     1] loss: 0.263
[46,     1] loss: 0.257
[47,     1] loss: 0.289
[48,     1] loss: 0.523
[49,     1] loss: 0.515
[50,     1] loss: 0.485
[51,     1] loss: 0.344
[52,     1] loss: 0.361
[53,     1] loss: 0.397
[54,     1] loss: 0.396
[55,     1] loss: 0.324
[56,     1] loss: 0.341
[57,     1] loss: 0.318
[58,     1] loss: 0.295
[59,     1] loss: 0.338
[60,     1] loss: 0.300
[61,     1] loss: 0.324
[62,     1] loss: 0.279
[63,     1] loss: 0.266
[64,     1] loss: 0.342
[65,     1] loss: 0.314
[66,     1] loss: 0.275
[67,     1] loss: 0.312
[68,     1] loss: 0.279
[69,     1] loss: 0.287
[70,     1] loss: 0.291
[71,     1] loss: 0.272
[72,     1] loss: 0.249
[73,     1] loss: 0.290
[74,     1] loss: 0.239
[75,     1] loss: 0.240
[76,     1] loss: 0.217
[77,     1] loss: 0.246
[78,     1] loss: 0.226
[79,     1] loss: 0.248
[80,     1] loss: 0.647
[81,     1] loss: 0.385
[82,     1] loss: 0.386
[83,     1] loss: 0.332
[84,     1] loss: 0.334
[85,     1] loss: 0.319
[86,     1] loss: 0.321
[87,     1] loss: 0.316
[88,     1] loss: 0.311
[89,     1] loss: 0.295
[90,     1] loss: 0.290
[91,     1] loss: 0.310
[92,     1] loss: 0.272
[93,     1] loss: 0.216
[94,     1] loss: 0.255
[95,     1] loss: 0.268
[96,     1] loss: 0.246
[97,     1] loss: 0.200
[98,     1] loss: 0.241
[99,     1] loss: 0.292
[100,     1] loss: 0.553
Early stopping applied (best metric=0.3803552985191345)
Finished Training
Total time taken: 11.148959159851074
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.672
[4,     1] loss: 0.649
[5,     1] loss: 0.621
[6,     1] loss: 0.591
[7,     1] loss: 0.556
[8,     1] loss: 0.546
[9,     1] loss: 0.513
[10,     1] loss: 0.471
[11,     1] loss: 0.457
[12,     1] loss: 0.432
[13,     1] loss: 0.438
[14,     1] loss: 0.404
[15,     1] loss: 0.399
[16,     1] loss: 0.384
[17,     1] loss: 0.424
[18,     1] loss: 0.373
[19,     1] loss: 0.475
[20,     1] loss: 0.386
[21,     1] loss: 0.409
[22,     1] loss: 0.502
[23,     1] loss: 0.383
[24,     1] loss: 0.411
[25,     1] loss: 0.385
[26,     1] loss: 0.396
[27,     1] loss: 0.360
[28,     1] loss: 0.347
[29,     1] loss: 0.389
[30,     1] loss: 0.341
[31,     1] loss: 0.349
[32,     1] loss: 0.328
[33,     1] loss: 0.328
[34,     1] loss: 0.347
[35,     1] loss: 0.334
[36,     1] loss: 0.317
[37,     1] loss: 0.315
[38,     1] loss: 0.297
[39,     1] loss: 0.306
[40,     1] loss: 0.323
[41,     1] loss: 0.378
[42,     1] loss: 0.352
[43,     1] loss: 0.474
[44,     1] loss: 0.319
[45,     1] loss: 0.395
[46,     1] loss: 0.374
[47,     1] loss: 0.355
[48,     1] loss: 0.365
[49,     1] loss: 0.363
[50,     1] loss: 0.369
[51,     1] loss: 0.368
[52,     1] loss: 0.314
[53,     1] loss: 0.341
[54,     1] loss: 0.330
[55,     1] loss: 0.482
[56,     1] loss: 0.483
[57,     1] loss: 0.386
[58,     1] loss: 0.357
[59,     1] loss: 0.374
[60,     1] loss: 0.377
[61,     1] loss: 0.371
[62,     1] loss: 0.359
[63,     1] loss: 0.403
[64,     1] loss: 0.328
Early stopping applied (best metric=0.4337480664253235)
Finished Training
Total time taken: 7.127027988433838
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.699
[3,     1] loss: 0.679
[4,     1] loss: 0.652
[5,     1] loss: 0.633
[6,     1] loss: 0.616
[7,     1] loss: 0.571
[8,     1] loss: 0.529
[9,     1] loss: 0.506
[10,     1] loss: 0.467
[11,     1] loss: 0.476
[12,     1] loss: 0.430
[13,     1] loss: 0.503
[14,     1] loss: 0.528
[15,     1] loss: 0.387
[16,     1] loss: 0.427
[17,     1] loss: 0.395
[18,     1] loss: 0.392
[19,     1] loss: 0.405
[20,     1] loss: 0.487
[21,     1] loss: 0.337
[22,     1] loss: 0.405
[23,     1] loss: 0.339
[24,     1] loss: 0.369
[25,     1] loss: 0.426
[26,     1] loss: 0.385
[27,     1] loss: 0.374
[28,     1] loss: 0.312
[29,     1] loss: 0.344
[30,     1] loss: 0.321
[31,     1] loss: 0.326
[32,     1] loss: 0.337
[33,     1] loss: 0.310
[34,     1] loss: 0.307
[35,     1] loss: 0.279
[36,     1] loss: 0.304
[37,     1] loss: 0.290
[38,     1] loss: 0.283
[39,     1] loss: 0.461
[40,     1] loss: 0.386
[41,     1] loss: 0.345
[42,     1] loss: 0.373
[43,     1] loss: 0.408
[44,     1] loss: 0.342
[45,     1] loss: 0.329
[46,     1] loss: 0.353
[47,     1] loss: 0.346
[48,     1] loss: 0.314
[49,     1] loss: 0.321
[50,     1] loss: 0.357
[51,     1] loss: 0.344
[52,     1] loss: 0.344
[53,     1] loss: 0.306
[54,     1] loss: 0.326
[55,     1] loss: 0.282
[56,     1] loss: 0.291
[57,     1] loss: 0.297
[58,     1] loss: 0.271
[59,     1] loss: 0.302
[60,     1] loss: 0.255
[61,     1] loss: 0.265
[62,     1] loss: 0.265
[63,     1] loss: 0.248
[64,     1] loss: 0.237
[65,     1] loss: 0.309
[66,     1] loss: 0.386
[67,     1] loss: 0.272
[68,     1] loss: 0.370
[69,     1] loss: 0.290
[70,     1] loss: 0.425
[71,     1] loss: 0.367
[72,     1] loss: 0.454
[73,     1] loss: 0.319
[74,     1] loss: 0.414
[75,     1] loss: 0.322
[76,     1] loss: 0.327
[77,     1] loss: 0.320
[78,     1] loss: 0.313
[79,     1] loss: 0.334
[80,     1] loss: 0.308
[81,     1] loss: 0.318
[82,     1] loss: 0.369
[83,     1] loss: 0.275
[84,     1] loss: 0.282
[85,     1] loss: 0.306
[86,     1] loss: 0.277
[87,     1] loss: 0.278
[88,     1] loss: 0.274
[89,     1] loss: 0.309
[90,     1] loss: 0.302
[91,     1] loss: 0.261
[92,     1] loss: 0.350
[93,     1] loss: 0.675
[94,     1] loss: 0.349
[95,     1] loss: 0.411
[96,     1] loss: 0.356
[97,     1] loss: 0.405
[98,     1] loss: 0.337
[99,     1] loss: 0.332
[100,     1] loss: 0.308
[101,     1] loss: 0.330
[102,     1] loss: 0.327
[103,     1] loss: 0.324
[104,     1] loss: 0.317
[105,     1] loss: 0.328
[106,     1] loss: 0.328
[107,     1] loss: 0.313
[108,     1] loss: 0.291
[109,     1] loss: 0.316
[110,     1] loss: 0.317
[111,     1] loss: 0.297
[112,     1] loss: 0.284
[113,     1] loss: 0.289
[114,     1] loss: 0.301
[115,     1] loss: 0.274
[116,     1] loss: 0.271
[117,     1] loss: 0.262
Early stopping applied (best metric=0.3755623698234558)
Finished Training
Total time taken: 13.030142307281494
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.689
[3,     1] loss: 0.686
[4,     1] loss: 0.675
[5,     1] loss: 0.650
[6,     1] loss: 0.642
[7,     1] loss: 0.616
[8,     1] loss: 0.608
[9,     1] loss: 0.560
[10,     1] loss: 0.553
[11,     1] loss: 0.541
[12,     1] loss: 0.487
[13,     1] loss: 0.496
[14,     1] loss: 0.478
[15,     1] loss: 0.459
[16,     1] loss: 0.415
[17,     1] loss: 0.418
[18,     1] loss: 0.406
[19,     1] loss: 0.391
[20,     1] loss: 0.403
[21,     1] loss: 0.387
[22,     1] loss: 0.403
[23,     1] loss: 0.347
[24,     1] loss: 0.385
[25,     1] loss: 0.364
[26,     1] loss: 0.354
[27,     1] loss: 0.393
[28,     1] loss: 0.378
[29,     1] loss: 0.475
[30,     1] loss: 0.410
[31,     1] loss: 0.417
[32,     1] loss: 0.385
[33,     1] loss: 0.434
[34,     1] loss: 0.423
[35,     1] loss: 0.393
[36,     1] loss: 0.353
[37,     1] loss: 0.378
[38,     1] loss: 0.335
[39,     1] loss: 0.334
[40,     1] loss: 0.364
[41,     1] loss: 0.361
[42,     1] loss: 0.373
[43,     1] loss: 0.435
[44,     1] loss: 0.366
[45,     1] loss: 0.376
[46,     1] loss: 0.362
[47,     1] loss: 0.365
[48,     1] loss: 0.366
[49,     1] loss: 0.385
[50,     1] loss: 0.389
[51,     1] loss: 0.373
[52,     1] loss: 0.355
[53,     1] loss: 0.379
[54,     1] loss: 0.344
[55,     1] loss: 0.362
[56,     1] loss: 0.324
[57,     1] loss: 0.345
[58,     1] loss: 0.341
[59,     1] loss: 0.331
[60,     1] loss: 0.294
[61,     1] loss: 0.347
[62,     1] loss: 0.598
[63,     1] loss: 0.552
[64,     1] loss: 0.412
[65,     1] loss: 0.383
Early stopping applied (best metric=0.3384113311767578)
Finished Training
Total time taken: 7.24266242980957
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.693
[3,     1] loss: 0.682
[4,     1] loss: 0.653
[5,     1] loss: 0.614
[6,     1] loss: 0.578
[7,     1] loss: 0.541
[8,     1] loss: 0.510
[9,     1] loss: 0.473
[10,     1] loss: 0.426
[11,     1] loss: 0.438
[12,     1] loss: 0.403
[13,     1] loss: 0.469
[14,     1] loss: 0.375
[15,     1] loss: 0.413
[16,     1] loss: 0.471
[17,     1] loss: 0.378
[18,     1] loss: 0.413
[19,     1] loss: 0.387
[20,     1] loss: 0.402
[21,     1] loss: 0.339
[22,     1] loss: 0.388
[23,     1] loss: 0.354
[24,     1] loss: 0.372
[25,     1] loss: 0.354
[26,     1] loss: 0.358
[27,     1] loss: 0.390
[28,     1] loss: 0.478
[29,     1] loss: 0.341
[30,     1] loss: 0.402
[31,     1] loss: 0.349
[32,     1] loss: 0.336
[33,     1] loss: 0.381
[34,     1] loss: 0.377
[35,     1] loss: 0.364
[36,     1] loss: 0.352
[37,     1] loss: 0.344
[38,     1] loss: 0.323
[39,     1] loss: 0.312
[40,     1] loss: 0.306
[41,     1] loss: 0.325
[42,     1] loss: 0.359
[43,     1] loss: 0.324
[44,     1] loss: 0.355
[45,     1] loss: 0.305
[46,     1] loss: 0.315
[47,     1] loss: 0.306
[48,     1] loss: 0.304
[49,     1] loss: 0.416
[50,     1] loss: 0.347
[51,     1] loss: 0.414
[52,     1] loss: 0.363
[53,     1] loss: 0.341
[54,     1] loss: 0.362
[55,     1] loss: 0.339
[56,     1] loss: 0.343
[57,     1] loss: 0.328
[58,     1] loss: 0.341
[59,     1] loss: 0.344
[60,     1] loss: 0.337
[61,     1] loss: 0.314
[62,     1] loss: 0.346
[63,     1] loss: 0.324
[64,     1] loss: 0.311
[65,     1] loss: 0.315
[66,     1] loss: 0.320
[67,     1] loss: 0.337
[68,     1] loss: 0.489
[69,     1] loss: 0.432
[70,     1] loss: 0.407
[71,     1] loss: 0.367
[72,     1] loss: 0.403
[73,     1] loss: 0.391
[74,     1] loss: 0.359
[75,     1] loss: 0.356
[76,     1] loss: 0.345
[77,     1] loss: 0.311
[78,     1] loss: 0.304
[79,     1] loss: 0.329
[80,     1] loss: 0.306
[81,     1] loss: 0.398
[82,     1] loss: 0.348
[83,     1] loss: 0.393
[84,     1] loss: 0.337
[85,     1] loss: 0.313
[86,     1] loss: 0.352
[87,     1] loss: 0.345
[88,     1] loss: 0.306
[89,     1] loss: 0.299
[90,     1] loss: 0.299
[91,     1] loss: 0.295
[92,     1] loss: 0.261
[93,     1] loss: 0.302
[94,     1] loss: 0.288
[95,     1] loss: 0.273
[96,     1] loss: 0.270
[97,     1] loss: 0.256
[98,     1] loss: 0.423
[99,     1] loss: 0.475
[100,     1] loss: 0.335
[101,     1] loss: 0.442
[102,     1] loss: 0.372
[103,     1] loss: 0.348
[104,     1] loss: 0.363
[105,     1] loss: 0.358
[106,     1] loss: 0.345
[107,     1] loss: 0.323
[108,     1] loss: 0.357
[109,     1] loss: 0.299
[110,     1] loss: 0.329
[111,     1] loss: 0.262
[112,     1] loss: 0.281
[113,     1] loss: 0.355
[114,     1] loss: 0.348
[115,     1] loss: 0.412
[116,     1] loss: 0.373
[117,     1] loss: 0.334
[118,     1] loss: 0.307
[119,     1] loss: 0.334
[120,     1] loss: 0.305
[121,     1] loss: 0.337
[122,     1] loss: 0.301
[123,     1] loss: 0.317
[124,     1] loss: 0.300
[125,     1] loss: 0.311
[126,     1] loss: 0.271
[127,     1] loss: 0.256
[128,     1] loss: 0.282
[129,     1] loss: 0.364
[130,     1] loss: 0.494
[131,     1] loss: 0.361
[132,     1] loss: 0.393
[133,     1] loss: 0.378
[134,     1] loss: 0.356
[135,     1] loss: 0.368
[136,     1] loss: 0.333
[137,     1] loss: 0.321
[138,     1] loss: 0.301
[139,     1] loss: 0.287
[140,     1] loss: 0.278
[141,     1] loss: 0.300
[142,     1] loss: 0.262
[143,     1] loss: 0.288
[144,     1] loss: 0.282
[145,     1] loss: 0.260
[146,     1] loss: 0.239
[147,     1] loss: 0.263
[148,     1] loss: 0.336
[149,     1] loss: 0.329
[150,     1] loss: 0.292
[151,     1] loss: 0.270
[152,     1] loss: 0.273
[153,     1] loss: 0.269
Early stopping applied (best metric=0.37967920303344727)
Finished Training
Total time taken: 17.058926820755005
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.691
[3,     1] loss: 0.689
[4,     1] loss: 0.681
[5,     1] loss: 0.668
[6,     1] loss: 0.660
[7,     1] loss: 0.644
[8,     1] loss: 0.617
[9,     1] loss: 0.601
[10,     1] loss: 0.606
[11,     1] loss: 0.576
[12,     1] loss: 0.537
[13,     1] loss: 0.557
[14,     1] loss: 0.555
[15,     1] loss: 0.555
[16,     1] loss: 0.551
[17,     1] loss: 0.504
[18,     1] loss: 0.508
[19,     1] loss: 0.470
[20,     1] loss: 0.430
[21,     1] loss: 0.439
[22,     1] loss: 0.483
[23,     1] loss: 0.456
[24,     1] loss: 0.457
[25,     1] loss: 0.419
[26,     1] loss: 0.419
[27,     1] loss: 0.402
[28,     1] loss: 0.377
[29,     1] loss: 0.384
[30,     1] loss: 0.373
[31,     1] loss: 0.351
[32,     1] loss: 0.363
[33,     1] loss: 0.389
[34,     1] loss: 0.377
[35,     1] loss: 0.506
[36,     1] loss: 0.383
[37,     1] loss: 0.397
[38,     1] loss: 0.384
[39,     1] loss: 0.401
[40,     1] loss: 0.372
[41,     1] loss: 0.404
[42,     1] loss: 0.416
[43,     1] loss: 0.423
[44,     1] loss: 0.369
[45,     1] loss: 0.318
[46,     1] loss: 0.392
[47,     1] loss: 0.347
[48,     1] loss: 0.341
[49,     1] loss: 0.310
[50,     1] loss: 0.375
[51,     1] loss: 0.377
[52,     1] loss: 0.305
[53,     1] loss: 0.359
[54,     1] loss: 0.382
[55,     1] loss: 0.315
[56,     1] loss: 0.341
[57,     1] loss: 0.305
[58,     1] loss: 0.319
[59,     1] loss: 0.344
[60,     1] loss: 0.589
[61,     1] loss: 0.457
[62,     1] loss: 0.425
[63,     1] loss: 0.424
[64,     1] loss: 0.436
[65,     1] loss: 0.446
[66,     1] loss: 0.358
[67,     1] loss: 0.384
[68,     1] loss: 0.380
[69,     1] loss: 0.349
[70,     1] loss: 0.371
[71,     1] loss: 0.345
[72,     1] loss: 0.306
[73,     1] loss: 0.353
[74,     1] loss: 0.369
[75,     1] loss: 0.449
[76,     1] loss: 0.395
[77,     1] loss: 0.428
[78,     1] loss: 0.393
[79,     1] loss: 0.355
[80,     1] loss: 0.396
[81,     1] loss: 0.422
[82,     1] loss: 0.387
[83,     1] loss: 0.375
[84,     1] loss: 0.350
[85,     1] loss: 0.365
[86,     1] loss: 0.330
[87,     1] loss: 0.366
[88,     1] loss: 0.354
[89,     1] loss: 0.318
[90,     1] loss: 0.345
[91,     1] loss: 0.326
[92,     1] loss: 0.293
[93,     1] loss: 0.310
[94,     1] loss: 0.348
[95,     1] loss: 0.430
[96,     1] loss: 0.344
[97,     1] loss: 0.380
[98,     1] loss: 0.357
[99,     1] loss: 0.322
[100,     1] loss: 0.350
[101,     1] loss: 0.501
Early stopping applied (best metric=0.2789362967014313)
Finished Training
Total time taken: 11.217758417129517
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.693
[3,     1] loss: 0.689
[4,     1] loss: 0.673
[5,     1] loss: 0.646
[6,     1] loss: 0.632
[7,     1] loss: 0.589
[8,     1] loss: 0.555
[9,     1] loss: 0.538
[10,     1] loss: 0.494
[11,     1] loss: 0.471
[12,     1] loss: 0.441
[13,     1] loss: 0.449
[14,     1] loss: 0.419
[15,     1] loss: 0.425
[16,     1] loss: 0.424
[17,     1] loss: 0.410
[18,     1] loss: 0.381
[19,     1] loss: 0.354
[20,     1] loss: 0.345
[21,     1] loss: 0.355
[22,     1] loss: 0.334
[23,     1] loss: 0.388
[24,     1] loss: 0.370
[25,     1] loss: 0.336
[26,     1] loss: 0.341
[27,     1] loss: 0.310
[28,     1] loss: 0.318
[29,     1] loss: 0.343
[30,     1] loss: 0.355
[31,     1] loss: 0.347
[32,     1] loss: 0.340
[33,     1] loss: 0.342
[34,     1] loss: 0.311
[35,     1] loss: 0.306
[36,     1] loss: 0.322
[37,     1] loss: 0.332
[38,     1] loss: 0.324
[39,     1] loss: 0.294
[40,     1] loss: 0.322
[41,     1] loss: 0.327
[42,     1] loss: 0.320
[43,     1] loss: 0.301
[44,     1] loss: 0.307
[45,     1] loss: 0.338
[46,     1] loss: 0.276
[47,     1] loss: 0.311
[48,     1] loss: 0.290
[49,     1] loss: 0.312
[50,     1] loss: 0.312
[51,     1] loss: 0.329
[52,     1] loss: 0.390
[53,     1] loss: 0.615
[54,     1] loss: 0.391
[55,     1] loss: 0.359
[56,     1] loss: 0.429
[57,     1] loss: 0.336
[58,     1] loss: 0.408
[59,     1] loss: 0.372
[60,     1] loss: 0.321
[61,     1] loss: 0.344
[62,     1] loss: 0.324
[63,     1] loss: 0.338
[64,     1] loss: 0.293
[65,     1] loss: 0.333
[66,     1] loss: 0.368
[67,     1] loss: 0.334
[68,     1] loss: 0.345
[69,     1] loss: 0.342
[70,     1] loss: 0.428
[71,     1] loss: 0.387
[72,     1] loss: 0.403
[73,     1] loss: 0.355
[74,     1] loss: 0.369
[75,     1] loss: 0.328
[76,     1] loss: 0.333
[77,     1] loss: 0.333
[78,     1] loss: 0.364
[79,     1] loss: 0.322
[80,     1] loss: 0.379
[81,     1] loss: 0.316
[82,     1] loss: 0.318
[83,     1] loss: 0.350
[84,     1] loss: 0.324
[85,     1] loss: 0.310
[86,     1] loss: 0.325
[87,     1] loss: 0.305
[88,     1] loss: 0.301
[89,     1] loss: 0.323
[90,     1] loss: 0.322
[91,     1] loss: 0.308
[92,     1] loss: 0.297
[93,     1] loss: 0.294
[94,     1] loss: 0.417
[95,     1] loss: 0.750
[96,     1] loss: 0.404
[97,     1] loss: 0.428
[98,     1] loss: 0.401
[99,     1] loss: 0.378
[100,     1] loss: 0.418
[101,     1] loss: 0.389
[102,     1] loss: 0.354
[103,     1] loss: 0.382
Early stopping applied (best metric=0.44631141424179077)
Finished Training
Total time taken: 11.428526878356934
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.699
[3,     1] loss: 0.691
[4,     1] loss: 0.683
[5,     1] loss: 0.667
[6,     1] loss: 0.654
[7,     1] loss: 0.630
[8,     1] loss: 0.607
[9,     1] loss: 0.592
[10,     1] loss: 0.576
[11,     1] loss: 0.539
[12,     1] loss: 0.498
[13,     1] loss: 0.502
[14,     1] loss: 0.499
[15,     1] loss: 0.486
[16,     1] loss: 0.464
[17,     1] loss: 0.420
[18,     1] loss: 0.472
[19,     1] loss: 0.586
[20,     1] loss: 0.408
[21,     1] loss: 0.465
[22,     1] loss: 0.433
[23,     1] loss: 0.441
[24,     1] loss: 0.443
[25,     1] loss: 0.426
[26,     1] loss: 0.478
[27,     1] loss: 0.378
[28,     1] loss: 0.404
[29,     1] loss: 0.386
[30,     1] loss: 0.362
[31,     1] loss: 0.393
[32,     1] loss: 0.332
[33,     1] loss: 0.314
[34,     1] loss: 0.332
[35,     1] loss: 0.307
[36,     1] loss: 0.355
[37,     1] loss: 0.333
[38,     1] loss: 0.302
[39,     1] loss: 0.279
[40,     1] loss: 0.271
[41,     1] loss: 0.330
[42,     1] loss: 0.329
[43,     1] loss: 0.328
[44,     1] loss: 0.303
[45,     1] loss: 0.312
[46,     1] loss: 0.317
[47,     1] loss: 0.285
[48,     1] loss: 0.336
[49,     1] loss: 0.302
[50,     1] loss: 0.268
[51,     1] loss: 0.258
[52,     1] loss: 0.272
[53,     1] loss: 0.252
[54,     1] loss: 0.277
[55,     1] loss: 0.265
[56,     1] loss: 0.438
[57,     1] loss: 0.534
[58,     1] loss: 0.339
[59,     1] loss: 0.434
[60,     1] loss: 0.402
[61,     1] loss: 0.385
[62,     1] loss: 0.374
[63,     1] loss: 0.458
[64,     1] loss: 0.364
[65,     1] loss: 0.360
[66,     1] loss: 0.358
[67,     1] loss: 0.340
[68,     1] loss: 0.391
[69,     1] loss: 0.323
[70,     1] loss: 0.329
[71,     1] loss: 0.383
[72,     1] loss: 0.322
[73,     1] loss: 0.343
[74,     1] loss: 0.331
[75,     1] loss: 0.349
[76,     1] loss: 0.360
[77,     1] loss: 0.345
[78,     1] loss: 0.283
[79,     1] loss: 0.339
[80,     1] loss: 0.310
[81,     1] loss: 0.349
[82,     1] loss: 0.310
[83,     1] loss: 0.338
[84,     1] loss: 0.364
[85,     1] loss: 0.357
[86,     1] loss: 0.323
[87,     1] loss: 0.345
[88,     1] loss: 0.370
[89,     1] loss: 0.348
[90,     1] loss: 0.304
[91,     1] loss: 0.281
Early stopping applied (best metric=0.33412691950798035)
Finished Training
Total time taken: 10.084084272384644
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.669
[4,     1] loss: 0.648
[5,     1] loss: 0.611
[6,     1] loss: 0.598
[7,     1] loss: 0.535
[8,     1] loss: 0.531
[9,     1] loss: 0.482
[10,     1] loss: 0.510
[11,     1] loss: 0.469
[12,     1] loss: 0.463
[13,     1] loss: 0.415
[14,     1] loss: 0.391
[15,     1] loss: 0.410
[16,     1] loss: 0.419
[17,     1] loss: 0.368
[18,     1] loss: 0.359
[19,     1] loss: 0.336
[20,     1] loss: 0.355
[21,     1] loss: 0.342
[22,     1] loss: 0.366
[23,     1] loss: 0.341
[24,     1] loss: 0.335
[25,     1] loss: 0.270
[26,     1] loss: 0.370
[27,     1] loss: 0.414
[28,     1] loss: 0.312
[29,     1] loss: 0.303
[30,     1] loss: 0.363
[31,     1] loss: 0.296
[32,     1] loss: 0.385
[33,     1] loss: 0.303
[34,     1] loss: 0.433
[35,     1] loss: 0.307
[36,     1] loss: 0.398
[37,     1] loss: 0.336
[38,     1] loss: 0.347
[39,     1] loss: 0.340
[40,     1] loss: 0.336
[41,     1] loss: 0.341
[42,     1] loss: 0.333
[43,     1] loss: 0.292
[44,     1] loss: 0.311
[45,     1] loss: 0.298
[46,     1] loss: 0.278
[47,     1] loss: 0.258
[48,     1] loss: 0.284
[49,     1] loss: 0.284
[50,     1] loss: 0.370
[51,     1] loss: 0.311
[52,     1] loss: 0.348
[53,     1] loss: 0.285
[54,     1] loss: 0.297
[55,     1] loss: 0.330
[56,     1] loss: 0.323
[57,     1] loss: 0.280
[58,     1] loss: 0.266
[59,     1] loss: 0.319
[60,     1] loss: 0.356
[61,     1] loss: 0.320
[62,     1] loss: 0.330
[63,     1] loss: 0.287
[64,     1] loss: 0.308
[65,     1] loss: 0.254
[66,     1] loss: 0.294
[67,     1] loss: 0.296
[68,     1] loss: 0.342
[69,     1] loss: 0.280
[70,     1] loss: 0.301
[71,     1] loss: 0.265
[72,     1] loss: 0.281
[73,     1] loss: 0.313
[74,     1] loss: 0.274
[75,     1] loss: 0.276
[76,     1] loss: 0.286
[77,     1] loss: 0.288
[78,     1] loss: 0.315
Early stopping applied (best metric=0.3744487166404724)
Finished Training
Total time taken: 8.664344787597656
{'Hydroxylation-K Validation Accuracy': 0.8318085106382979, 'Hydroxylation-K Validation Sensitivity': 0.8257777777777778, 'Hydroxylation-K Validation Specificity': 0.8336842105263158, 'Hydroxylation-K Validation Precision': 0.5787619961737609, 'Hydroxylation-K AUC ROC': 0.8345964912280702, 'Hydroxylation-K AUC PR': 0.6203449846387675, 'Hydroxylation-K MCC': 0.5893057609107953, 'Hydroxylation-K F1': 0.6708855746611707, 'Validation Loss (Hydroxylation-K)': 0.36625580430030824, 'Validation Loss (total)': 0.36625580430030824, 'TimeToTrain': 10.959423942565918}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023320571054139634,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 977761110,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.826988722983483}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.696
[3,     1] loss: 0.685
[4,     1] loss: 0.669
[5,     1] loss: 0.649
[6,     1] loss: 0.634
[7,     1] loss: 0.609
[8,     1] loss: 0.606
[9,     1] loss: 0.573
[10,     1] loss: 0.550
[11,     1] loss: 0.508
[12,     1] loss: 0.535
[13,     1] loss: 0.500
[14,     1] loss: 0.457
[15,     1] loss: 0.459
[16,     1] loss: 0.447
[17,     1] loss: 0.455
[18,     1] loss: 0.431
[19,     1] loss: 0.455
[20,     1] loss: 0.419
[21,     1] loss: 0.419
[22,     1] loss: 0.380
[23,     1] loss: 0.417
[24,     1] loss: 0.366
[25,     1] loss: 0.353
[26,     1] loss: 0.423
[27,     1] loss: 0.428
[28,     1] loss: 0.367
[29,     1] loss: 0.365
[30,     1] loss: 0.390
[31,     1] loss: 0.347
[32,     1] loss: 0.343
[33,     1] loss: 0.399
[34,     1] loss: 0.310
[35,     1] loss: 0.290
[36,     1] loss: 0.321
[37,     1] loss: 0.340
[38,     1] loss: 0.288
[39,     1] loss: 0.362
[40,     1] loss: 0.436
[41,     1] loss: 0.367
[42,     1] loss: 0.343
[43,     1] loss: 0.389
[44,     1] loss: 0.401
[45,     1] loss: 0.375
[46,     1] loss: 0.370
[47,     1] loss: 0.381
[48,     1] loss: 0.408
[49,     1] loss: 0.353
[50,     1] loss: 0.400
[51,     1] loss: 0.340
[52,     1] loss: 0.337
[53,     1] loss: 0.302
[54,     1] loss: 0.332
[55,     1] loss: 0.323
[56,     1] loss: 0.301
[57,     1] loss: 0.327
[58,     1] loss: 0.295
[59,     1] loss: 0.293
[60,     1] loss: 0.291
[61,     1] loss: 0.316
[62,     1] loss: 0.253
[63,     1] loss: 0.272
[64,     1] loss: 0.321
[65,     1] loss: 0.260
[66,     1] loss: 0.265
[67,     1] loss: 0.265
[68,     1] loss: 0.254
[69,     1] loss: 0.276
[70,     1] loss: 0.220
[71,     1] loss: 0.257
[72,     1] loss: 0.194
[73,     1] loss: 0.215
[74,     1] loss: 0.218
[75,     1] loss: 0.239
[76,     1] loss: 0.204
[77,     1] loss: 0.270
[78,     1] loss: 0.240
[79,     1] loss: 0.210
[80,     1] loss: 0.182
[81,     1] loss: 0.206
[82,     1] loss: 0.206
[83,     1] loss: 0.201
[84,     1] loss: 0.205
[85,     1] loss: 0.215
[86,     1] loss: 0.186
[87,     1] loss: 0.170
[88,     1] loss: 0.188
[89,     1] loss: 0.209
[90,     1] loss: 0.212
[91,     1] loss: 0.195
[92,     1] loss: 0.240
[93,     1] loss: 0.279
[94,     1] loss: 0.517
[95,     1] loss: 0.292
[96,     1] loss: 0.396
[97,     1] loss: 0.417
[98,     1] loss: 0.358
[99,     1] loss: 0.311
[100,     1] loss: 0.400
[101,     1] loss: 0.363
[102,     1] loss: 0.328
[103,     1] loss: 0.315
[104,     1] loss: 0.319
[105,     1] loss: 0.293
[106,     1] loss: 0.301
[107,     1] loss: 0.293
[108,     1] loss: 0.304
[109,     1] loss: 0.306
[110,     1] loss: 0.307
[111,     1] loss: 0.283
[112,     1] loss: 0.305
[113,     1] loss: 0.311
[114,     1] loss: 0.312
[115,     1] loss: 0.267
[116,     1] loss: 0.306
[117,     1] loss: 0.262
[118,     1] loss: 0.286
[119,     1] loss: 0.290
[120,     1] loss: 0.243
[121,     1] loss: 0.268
[122,     1] loss: 0.328
[123,     1] loss: 0.269
[124,     1] loss: 0.335
[125,     1] loss: 0.283
[126,     1] loss: 0.292
[127,     1] loss: 0.288
[128,     1] loss: 0.247
[129,     1] loss: 0.283
[130,     1] loss: 0.305
[131,     1] loss: 0.336
Early stopping applied (best metric=0.3213669955730438)
Finished Training
Total time taken: 14.586530447006226
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.688
[4,     1] loss: 0.672
[5,     1] loss: 0.647
[6,     1] loss: 0.630
[7,     1] loss: 0.598
[8,     1] loss: 0.590
[9,     1] loss: 0.558
[10,     1] loss: 0.545
[11,     1] loss: 0.543
[12,     1] loss: 0.508
[13,     1] loss: 0.478
[14,     1] loss: 0.473
[15,     1] loss: 0.469
[16,     1] loss: 0.448
[17,     1] loss: 0.411
[18,     1] loss: 0.405
[19,     1] loss: 0.381
[20,     1] loss: 0.424
[21,     1] loss: 0.356
[22,     1] loss: 0.349
[23,     1] loss: 0.326
[24,     1] loss: 0.348
[25,     1] loss: 0.296
[26,     1] loss: 0.324
[27,     1] loss: 0.313
[28,     1] loss: 0.322
[29,     1] loss: 0.288
[30,     1] loss: 0.329
[31,     1] loss: 0.288
[32,     1] loss: 0.289
[33,     1] loss: 0.275
[34,     1] loss: 0.298
[35,     1] loss: 0.298
[36,     1] loss: 0.321
[37,     1] loss: 0.348
[38,     1] loss: 0.342
[39,     1] loss: 0.351
[40,     1] loss: 0.274
[41,     1] loss: 0.341
[42,     1] loss: 0.319
[43,     1] loss: 0.319
[44,     1] loss: 0.316
[45,     1] loss: 0.365
[46,     1] loss: 0.317
[47,     1] loss: 0.301
[48,     1] loss: 0.273
[49,     1] loss: 0.298
[50,     1] loss: 0.328
[51,     1] loss: 0.325
[52,     1] loss: 0.326
[53,     1] loss: 0.307
[54,     1] loss: 0.259
[55,     1] loss: 0.273
[56,     1] loss: 0.338
[57,     1] loss: 0.327
[58,     1] loss: 0.343
[59,     1] loss: 0.315
[60,     1] loss: 0.298
[61,     1] loss: 0.298
[62,     1] loss: 0.270
[63,     1] loss: 0.306
[64,     1] loss: 0.256
[65,     1] loss: 0.263
[66,     1] loss: 0.297
[67,     1] loss: 0.264
[68,     1] loss: 0.325
[69,     1] loss: 0.279
[70,     1] loss: 0.293
[71,     1] loss: 0.306
[72,     1] loss: 0.250
[73,     1] loss: 0.280
[74,     1] loss: 0.253
[75,     1] loss: 0.290
[76,     1] loss: 0.298
[77,     1] loss: 0.244
[78,     1] loss: 0.254
[79,     1] loss: 0.255
[80,     1] loss: 0.279
[81,     1] loss: 0.272
[82,     1] loss: 0.229
[83,     1] loss: 0.224
[84,     1] loss: 0.211
[85,     1] loss: 0.206
[86,     1] loss: 0.198
[87,     1] loss: 0.194
[88,     1] loss: 0.186
[89,     1] loss: 0.206
[90,     1] loss: 0.282
[91,     1] loss: 0.262
[92,     1] loss: 0.269
[93,     1] loss: 0.268
[94,     1] loss: 0.232
[95,     1] loss: 0.289
[96,     1] loss: 0.230
[97,     1] loss: 0.241
[98,     1] loss: 0.231
[99,     1] loss: 0.225
[100,     1] loss: 0.258
[101,     1] loss: 0.244
[102,     1] loss: 0.273
[103,     1] loss: 0.240
[104,     1] loss: 0.216
[105,     1] loss: 0.247
[106,     1] loss: 0.211
[107,     1] loss: 0.298
[108,     1] loss: 0.291
[109,     1] loss: 0.289
[110,     1] loss: 0.290
[111,     1] loss: 0.328
[112,     1] loss: 0.340
[113,     1] loss: 0.283
[114,     1] loss: 0.258
[115,     1] loss: 0.242
[116,     1] loss: 0.276
[117,     1] loss: 0.274
[118,     1] loss: 0.272
[119,     1] loss: 0.275
[120,     1] loss: 0.268
[121,     1] loss: 0.271
[122,     1] loss: 0.260
[123,     1] loss: 0.234
[124,     1] loss: 0.259
[125,     1] loss: 0.220
[126,     1] loss: 0.195
[127,     1] loss: 0.198
[128,     1] loss: 0.221
[129,     1] loss: 0.181
[130,     1] loss: 0.218
[131,     1] loss: 0.189
[132,     1] loss: 0.207
[133,     1] loss: 0.175
[134,     1] loss: 0.212
[135,     1] loss: 0.196
[136,     1] loss: 0.202
[137,     1] loss: 0.191
[138,     1] loss: 0.188
[139,     1] loss: 0.216
[140,     1] loss: 0.267
[141,     1] loss: 0.213
[142,     1] loss: 0.176
[143,     1] loss: 0.361
[144,     1] loss: 0.360
[145,     1] loss: 0.300
[146,     1] loss: 0.308
[147,     1] loss: 0.307
[148,     1] loss: 0.287
[149,     1] loss: 0.271
[150,     1] loss: 0.295
[151,     1] loss: 0.268
[152,     1] loss: 0.290
[153,     1] loss: 0.250
[154,     1] loss: 0.276
[155,     1] loss: 0.270
[156,     1] loss: 0.264
[157,     1] loss: 0.256
[158,     1] loss: 0.243
[159,     1] loss: 0.248
[160,     1] loss: 0.236
[161,     1] loss: 0.232
[162,     1] loss: 0.214
[163,     1] loss: 0.219
[164,     1] loss: 0.218
[165,     1] loss: 0.219
[166,     1] loss: 0.246
[167,     1] loss: 0.252
[168,     1] loss: 0.204
[169,     1] loss: 0.223
[170,     1] loss: 0.264
[171,     1] loss: 0.243
[172,     1] loss: 0.235
[173,     1] loss: 0.200
[174,     1] loss: 0.202
[175,     1] loss: 0.212
[176,     1] loss: 0.215
[177,     1] loss: 0.224
[178,     1] loss: 0.229
[179,     1] loss: 0.193
[180,     1] loss: 0.217
[181,     1] loss: 0.189
[182,     1] loss: 0.198
[183,     1] loss: 0.231
[184,     1] loss: 0.187
[185,     1] loss: 0.179
[186,     1] loss: 0.194
[187,     1] loss: 0.197
[188,     1] loss: 0.300
[189,     1] loss: 0.382
[190,     1] loss: 0.262
[191,     1] loss: 0.310
Early stopping applied (best metric=0.31634312868118286)
Finished Training
Total time taken: 21.229663848876953
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.687
[3,     1] loss: 0.670
[4,     1] loss: 0.651
[5,     1] loss: 0.629
[6,     1] loss: 0.611
[7,     1] loss: 0.596
[8,     1] loss: 0.598
[9,     1] loss: 0.564
[10,     1] loss: 0.545
[11,     1] loss: 0.554
[12,     1] loss: 0.532
[13,     1] loss: 0.502
[14,     1] loss: 0.468
[15,     1] loss: 0.491
[16,     1] loss: 0.459
[17,     1] loss: 0.407
[18,     1] loss: 0.435
[19,     1] loss: 0.426
[20,     1] loss: 0.395
[21,     1] loss: 0.372
[22,     1] loss: 0.389
[23,     1] loss: 0.370
[24,     1] loss: 0.367
[25,     1] loss: 0.343
[26,     1] loss: 0.328
[27,     1] loss: 0.267
[28,     1] loss: 0.263
[29,     1] loss: 0.260
[30,     1] loss: 0.326
[31,     1] loss: 0.345
[32,     1] loss: 0.291
[33,     1] loss: 0.279
[34,     1] loss: 0.352
[35,     1] loss: 0.247
[36,     1] loss: 0.327
[37,     1] loss: 0.320
[38,     1] loss: 0.248
[39,     1] loss: 0.234
[40,     1] loss: 0.307
[41,     1] loss: 0.253
[42,     1] loss: 0.340
[43,     1] loss: 0.274
[44,     1] loss: 0.361
[45,     1] loss: 0.287
[46,     1] loss: 0.285
[47,     1] loss: 0.272
[48,     1] loss: 0.252
[49,     1] loss: 0.284
[50,     1] loss: 0.228
[51,     1] loss: 0.213
[52,     1] loss: 0.233
[53,     1] loss: 0.197
[54,     1] loss: 0.231
[55,     1] loss: 0.217
[56,     1] loss: 0.215
[57,     1] loss: 0.229
[58,     1] loss: 0.250
[59,     1] loss: 0.207
[60,     1] loss: 0.311
[61,     1] loss: 0.171
[62,     1] loss: 0.296
[63,     1] loss: 0.210
[64,     1] loss: 0.310
[65,     1] loss: 0.236
[66,     1] loss: 0.233
[67,     1] loss: 0.252
[68,     1] loss: 0.178
[69,     1] loss: 0.226
[70,     1] loss: 0.175
[71,     1] loss: 0.200
[72,     1] loss: 0.209
[73,     1] loss: 0.166
[74,     1] loss: 0.184
[75,     1] loss: 0.206
[76,     1] loss: 0.277
[77,     1] loss: 0.186
[78,     1] loss: 0.166
[79,     1] loss: 0.192
[80,     1] loss: 0.196
[81,     1] loss: 0.194
[82,     1] loss: 0.189
[83,     1] loss: 0.177
[84,     1] loss: 0.165
[85,     1] loss: 0.183
[86,     1] loss: 0.208
[87,     1] loss: 0.177
[88,     1] loss: 0.181
[89,     1] loss: 0.197
[90,     1] loss: 0.249
[91,     1] loss: 0.269
[92,     1] loss: 0.172
[93,     1] loss: 0.257
Early stopping applied (best metric=0.3409818410873413)
Finished Training
Total time taken: 10.931727170944214
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.683
[4,     1] loss: 0.669
[5,     1] loss: 0.651
[6,     1] loss: 0.629
[7,     1] loss: 0.615
[8,     1] loss: 0.588
[9,     1] loss: 0.583
[10,     1] loss: 0.561
[11,     1] loss: 0.530
[12,     1] loss: 0.518
[13,     1] loss: 0.488
[14,     1] loss: 0.482
[15,     1] loss: 0.464
[16,     1] loss: 0.423
[17,     1] loss: 0.407
[18,     1] loss: 0.430
[19,     1] loss: 0.413
[20,     1] loss: 0.404
[21,     1] loss: 0.386
[22,     1] loss: 0.355
[23,     1] loss: 0.359
[24,     1] loss: 0.341
[25,     1] loss: 0.347
[26,     1] loss: 0.350
[27,     1] loss: 0.318
[28,     1] loss: 0.314
[29,     1] loss: 0.308
[30,     1] loss: 0.352
[31,     1] loss: 0.325
[32,     1] loss: 0.293
[33,     1] loss: 0.293
[34,     1] loss: 0.364
[35,     1] loss: 0.275
[36,     1] loss: 0.307
[37,     1] loss: 0.317
[38,     1] loss: 0.271
[39,     1] loss: 0.324
[40,     1] loss: 0.279
[41,     1] loss: 0.312
[42,     1] loss: 0.267
[43,     1] loss: 0.291
[44,     1] loss: 0.294
[45,     1] loss: 0.256
[46,     1] loss: 0.272
[47,     1] loss: 0.292
[48,     1] loss: 0.305
[49,     1] loss: 0.289
[50,     1] loss: 0.251
[51,     1] loss: 0.283
[52,     1] loss: 0.290
[53,     1] loss: 0.287
[54,     1] loss: 0.291
[55,     1] loss: 0.239
[56,     1] loss: 0.288
[57,     1] loss: 0.280
[58,     1] loss: 0.257
[59,     1] loss: 0.225
[60,     1] loss: 0.317
[61,     1] loss: 0.290
[62,     1] loss: 0.284
[63,     1] loss: 0.316
[64,     1] loss: 0.269
[65,     1] loss: 0.282
[66,     1] loss: 0.250
[67,     1] loss: 0.267
[68,     1] loss: 0.286
[69,     1] loss: 0.254
[70,     1] loss: 0.246
[71,     1] loss: 0.253
[72,     1] loss: 0.250
[73,     1] loss: 0.287
[74,     1] loss: 0.273
[75,     1] loss: 0.261
[76,     1] loss: 0.299
[77,     1] loss: 0.216
[78,     1] loss: 0.236
[79,     1] loss: 0.283
[80,     1] loss: 0.218
[81,     1] loss: 0.239
[82,     1] loss: 0.252
[83,     1] loss: 0.252
[84,     1] loss: 0.210
[85,     1] loss: 0.308
[86,     1] loss: 0.310
[87,     1] loss: 0.334
[88,     1] loss: 0.313
[89,     1] loss: 0.258
[90,     1] loss: 0.282
[91,     1] loss: 0.301
[92,     1] loss: 0.289
[93,     1] loss: 0.291
[94,     1] loss: 0.319
[95,     1] loss: 0.308
[96,     1] loss: 0.303
[97,     1] loss: 0.273
[98,     1] loss: 0.280
[99,     1] loss: 0.279
[100,     1] loss: 0.310
[101,     1] loss: 0.268
[102,     1] loss: 0.313
[103,     1] loss: 0.289
[104,     1] loss: 0.300
[105,     1] loss: 0.293
[106,     1] loss: 0.327
[107,     1] loss: 0.266
[108,     1] loss: 0.294
[109,     1] loss: 0.283
[110,     1] loss: 0.299
[111,     1] loss: 0.303
[112,     1] loss: 0.263
[113,     1] loss: 0.256
[114,     1] loss: 0.245
[115,     1] loss: 0.232
[116,     1] loss: 0.253
[117,     1] loss: 0.244
[118,     1] loss: 0.225
[119,     1] loss: 0.248
[120,     1] loss: 0.267
[121,     1] loss: 0.229
[122,     1] loss: 0.180
[123,     1] loss: 0.217
[124,     1] loss: 0.220
[125,     1] loss: 0.181
[126,     1] loss: 0.222
[127,     1] loss: 0.235
[128,     1] loss: 0.181
[129,     1] loss: 0.288
[130,     1] loss: 0.265
[131,     1] loss: 0.282
[132,     1] loss: 0.224
[133,     1] loss: 0.339
[134,     1] loss: 0.234
[135,     1] loss: 0.253
[136,     1] loss: 0.228
[137,     1] loss: 0.284
[138,     1] loss: 0.237
[139,     1] loss: 0.271
[140,     1] loss: 0.231
[141,     1] loss: 0.234
[142,     1] loss: 0.222
[143,     1] loss: 0.244
[144,     1] loss: 0.224
[145,     1] loss: 0.241
[146,     1] loss: 0.234
[147,     1] loss: 0.229
[148,     1] loss: 0.247
[149,     1] loss: 0.204
[150,     1] loss: 0.240
[151,     1] loss: 0.217
[152,     1] loss: 0.219
[153,     1] loss: 0.222
[154,     1] loss: 0.207
[155,     1] loss: 0.233
[156,     1] loss: 0.298
[157,     1] loss: 0.310
[158,     1] loss: 0.440
[159,     1] loss: 0.278
[160,     1] loss: 0.312
[161,     1] loss: 0.257
[162,     1] loss: 0.261
[163,     1] loss: 0.302
[164,     1] loss: 0.272
[165,     1] loss: 0.259
[166,     1] loss: 0.310
[167,     1] loss: 0.265
[168,     1] loss: 0.256
[169,     1] loss: 0.233
[170,     1] loss: 0.262
[171,     1] loss: 0.231
[172,     1] loss: 0.245
[173,     1] loss: 0.234
[174,     1] loss: 0.259
[175,     1] loss: 0.258
[176,     1] loss: 0.218
[177,     1] loss: 0.230
[178,     1] loss: 0.208
[179,     1] loss: 0.250
[180,     1] loss: 0.217
[181,     1] loss: 0.233
[182,     1] loss: 0.207
[183,     1] loss: 0.245
[184,     1] loss: 0.238
[185,     1] loss: 0.269
[186,     1] loss: 0.269
[187,     1] loss: 0.240
[188,     1] loss: 0.250
[189,     1] loss: 0.302
[190,     1] loss: 0.247
[191,     1] loss: 0.261
[192,     1] loss: 0.250
[193,     1] loss: 0.234
[194,     1] loss: 0.263
[195,     1] loss: 0.228
[196,     1] loss: 0.220
[197,     1] loss: 0.219
[198,     1] loss: 0.226
[199,     1] loss: 0.223
[200,     1] loss: 0.211
Finished Training
Total time taken: 23.117849349975586
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.690
[3,     1] loss: 0.678
[4,     1] loss: 0.655
[5,     1] loss: 0.623
[6,     1] loss: 0.608
[7,     1] loss: 0.567
[8,     1] loss: 0.549
[9,     1] loss: 0.534
[10,     1] loss: 0.518
[11,     1] loss: 0.491
[12,     1] loss: 0.466
[13,     1] loss: 0.474
[14,     1] loss: 0.464
[15,     1] loss: 0.410
[16,     1] loss: 0.411
[17,     1] loss: 0.369
[18,     1] loss: 0.378
[19,     1] loss: 0.474
[20,     1] loss: 0.328
[21,     1] loss: 0.402
[22,     1] loss: 0.378
[23,     1] loss: 0.331
[24,     1] loss: 0.394
[25,     1] loss: 0.362
[26,     1] loss: 0.338
[27,     1] loss: 0.312
[28,     1] loss: 0.336
[29,     1] loss: 0.347
[30,     1] loss: 0.293
[31,     1] loss: 0.300
[32,     1] loss: 0.301
[33,     1] loss: 0.282
[34,     1] loss: 0.293
[35,     1] loss: 0.277
[36,     1] loss: 0.276
[37,     1] loss: 0.259
[38,     1] loss: 0.269
[39,     1] loss: 0.252
[40,     1] loss: 0.226
[41,     1] loss: 0.242
[42,     1] loss: 0.231
[43,     1] loss: 0.214
[44,     1] loss: 0.225
[45,     1] loss: 0.224
[46,     1] loss: 0.192
[47,     1] loss: 0.237
[48,     1] loss: 0.256
[49,     1] loss: 0.294
[50,     1] loss: 0.284
[51,     1] loss: 0.340
[52,     1] loss: 0.366
[53,     1] loss: 0.361
[54,     1] loss: 0.349
[55,     1] loss: 0.342
[56,     1] loss: 0.313
[57,     1] loss: 0.316
[58,     1] loss: 0.315
[59,     1] loss: 0.312
[60,     1] loss: 0.301
[61,     1] loss: 0.311
[62,     1] loss: 0.335
[63,     1] loss: 0.342
[64,     1] loss: 0.288
[65,     1] loss: 0.314
[66,     1] loss: 0.387
[67,     1] loss: 0.317
[68,     1] loss: 0.353
[69,     1] loss: 0.343
[70,     1] loss: 0.324
[71,     1] loss: 0.312
Early stopping applied (best metric=0.41542571783065796)
Finished Training
Total time taken: 7.994953870773315
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.689
[3,     1] loss: 0.684
[4,     1] loss: 0.653
[5,     1] loss: 0.630
[6,     1] loss: 0.605
[7,     1] loss: 0.588
[8,     1] loss: 0.548
[9,     1] loss: 0.538
[10,     1] loss: 0.512
[11,     1] loss: 0.476
[12,     1] loss: 0.481
[13,     1] loss: 0.456
[14,     1] loss: 0.433
[15,     1] loss: 0.397
[16,     1] loss: 0.395
[17,     1] loss: 0.404
[18,     1] loss: 0.375
[19,     1] loss: 0.391
[20,     1] loss: 0.351
[21,     1] loss: 0.386
[22,     1] loss: 0.347
[23,     1] loss: 0.309
[24,     1] loss: 0.327
[25,     1] loss: 0.349
[26,     1] loss: 0.271
[27,     1] loss: 0.306
[28,     1] loss: 0.334
[29,     1] loss: 0.293
[30,     1] loss: 0.302
[31,     1] loss: 0.331
[32,     1] loss: 0.303
[33,     1] loss: 0.381
[34,     1] loss: 0.351
[35,     1] loss: 0.356
[36,     1] loss: 0.417
[37,     1] loss: 0.305
[38,     1] loss: 0.305
[39,     1] loss: 0.352
[40,     1] loss: 0.300
[41,     1] loss: 0.313
[42,     1] loss: 0.282
[43,     1] loss: 0.296
[44,     1] loss: 0.300
[45,     1] loss: 0.268
[46,     1] loss: 0.286
[47,     1] loss: 0.247
[48,     1] loss: 0.296
[49,     1] loss: 0.289
[50,     1] loss: 0.275
[51,     1] loss: 0.287
[52,     1] loss: 0.234
[53,     1] loss: 0.260
[54,     1] loss: 0.264
[55,     1] loss: 0.255
[56,     1] loss: 0.241
[57,     1] loss: 0.228
[58,     1] loss: 0.222
[59,     1] loss: 0.230
[60,     1] loss: 0.278
[61,     1] loss: 0.252
[62,     1] loss: 0.238
[63,     1] loss: 0.234
[64,     1] loss: 0.204
[65,     1] loss: 0.193
[66,     1] loss: 0.232
[67,     1] loss: 0.197
[68,     1] loss: 0.218
[69,     1] loss: 0.183
[70,     1] loss: 0.230
Early stopping applied (best metric=0.40303438901901245)
Finished Training
Total time taken: 7.945918321609497
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.637
[4,     1] loss: 0.613
[5,     1] loss: 0.582
[6,     1] loss: 0.565
[7,     1] loss: 0.523
[8,     1] loss: 0.504
[9,     1] loss: 0.483
[10,     1] loss: 0.477
[11,     1] loss: 0.432
[12,     1] loss: 0.412
[13,     1] loss: 0.408
[14,     1] loss: 0.351
[15,     1] loss: 0.388
[16,     1] loss: 0.377
[17,     1] loss: 0.353
[18,     1] loss: 0.374
[19,     1] loss: 0.394
[20,     1] loss: 0.361
[21,     1] loss: 0.330
[22,     1] loss: 0.375
[23,     1] loss: 0.341
[24,     1] loss: 0.365
[25,     1] loss: 0.405
[26,     1] loss: 0.363
[27,     1] loss: 0.366
[28,     1] loss: 0.340
[29,     1] loss: 0.319
[30,     1] loss: 0.328
[31,     1] loss: 0.342
[32,     1] loss: 0.301
[33,     1] loss: 0.312
[34,     1] loss: 0.356
[35,     1] loss: 0.347
[36,     1] loss: 0.317
[37,     1] loss: 0.301
[38,     1] loss: 0.337
[39,     1] loss: 0.299
[40,     1] loss: 0.315
[41,     1] loss: 0.299
[42,     1] loss: 0.310
[43,     1] loss: 0.330
[44,     1] loss: 0.301
[45,     1] loss: 0.334
[46,     1] loss: 0.321
[47,     1] loss: 0.306
[48,     1] loss: 0.374
[49,     1] loss: 0.329
[50,     1] loss: 0.370
[51,     1] loss: 0.359
[52,     1] loss: 0.317
[53,     1] loss: 0.300
[54,     1] loss: 0.315
[55,     1] loss: 0.333
[56,     1] loss: 0.300
[57,     1] loss: 0.252
[58,     1] loss: 0.279
[59,     1] loss: 0.230
[60,     1] loss: 0.279
[61,     1] loss: 0.248
[62,     1] loss: 0.268
[63,     1] loss: 0.260
[64,     1] loss: 0.271
[65,     1] loss: 0.277
[66,     1] loss: 0.287
[67,     1] loss: 0.266
[68,     1] loss: 0.237
[69,     1] loss: 0.255
[70,     1] loss: 0.250
[71,     1] loss: 0.324
[72,     1] loss: 0.250
[73,     1] loss: 0.282
[74,     1] loss: 0.229
[75,     1] loss: 0.222
[76,     1] loss: 0.263
[77,     1] loss: 0.228
[78,     1] loss: 0.282
[79,     1] loss: 0.252
[80,     1] loss: 0.256
[81,     1] loss: 0.218
[82,     1] loss: 0.253
[83,     1] loss: 0.229
[84,     1] loss: 0.264
[85,     1] loss: 0.256
[86,     1] loss: 0.224
[87,     1] loss: 0.228
[88,     1] loss: 0.283
[89,     1] loss: 0.219
[90,     1] loss: 0.260
[91,     1] loss: 0.341
[92,     1] loss: 0.294
[93,     1] loss: 0.230
[94,     1] loss: 0.266
[95,     1] loss: 0.257
[96,     1] loss: 0.299
[97,     1] loss: 0.266
[98,     1] loss: 0.260
[99,     1] loss: 0.255
[100,     1] loss: 0.287
[101,     1] loss: 0.256
[102,     1] loss: 0.253
[103,     1] loss: 0.208
[104,     1] loss: 0.240
[105,     1] loss: 0.257
[106,     1] loss: 0.317
[107,     1] loss: 0.253
[108,     1] loss: 0.210
[109,     1] loss: 0.220
[110,     1] loss: 0.214
[111,     1] loss: 0.201
[112,     1] loss: 0.206
[113,     1] loss: 0.210
[114,     1] loss: 0.234
[115,     1] loss: 0.209
[116,     1] loss: 0.209
[117,     1] loss: 0.223
[118,     1] loss: 0.232
[119,     1] loss: 0.207
[120,     1] loss: 0.210
[121,     1] loss: 0.281
[122,     1] loss: 0.301
[123,     1] loss: 0.290
[124,     1] loss: 0.269
[125,     1] loss: 0.360
[126,     1] loss: 0.327
[127,     1] loss: 0.298
[128,     1] loss: 0.273
[129,     1] loss: 0.315
[130,     1] loss: 0.302
[131,     1] loss: 0.303
[132,     1] loss: 0.289
[133,     1] loss: 0.315
[134,     1] loss: 0.255
[135,     1] loss: 0.281
[136,     1] loss: 0.292
[137,     1] loss: 0.312
[138,     1] loss: 0.310
[139,     1] loss: 0.278
[140,     1] loss: 0.322
[141,     1] loss: 0.264
[142,     1] loss: 0.252
[143,     1] loss: 0.237
[144,     1] loss: 0.254
[145,     1] loss: 0.264
[146,     1] loss: 0.265
[147,     1] loss: 0.269
[148,     1] loss: 0.276
Early stopping applied (best metric=0.4882506728172302)
Finished Training
Total time taken: 17.426885843276978
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.679
[4,     1] loss: 0.660
[5,     1] loss: 0.659
[6,     1] loss: 0.632
[7,     1] loss: 0.614
[8,     1] loss: 0.589
[9,     1] loss: 0.582
[10,     1] loss: 0.560
[11,     1] loss: 0.519
[12,     1] loss: 0.549
[13,     1] loss: 0.483
[14,     1] loss: 0.502
[15,     1] loss: 0.497
[16,     1] loss: 0.468
[17,     1] loss: 0.443
[18,     1] loss: 0.410
[19,     1] loss: 0.398
[20,     1] loss: 0.381
[21,     1] loss: 0.362
[22,     1] loss: 0.363
[23,     1] loss: 0.361
[24,     1] loss: 0.355
[25,     1] loss: 0.324
[26,     1] loss: 0.338
[27,     1] loss: 0.327
[28,     1] loss: 0.287
[29,     1] loss: 0.347
[30,     1] loss: 0.295
[31,     1] loss: 0.353
[32,     1] loss: 0.252
[33,     1] loss: 0.298
[34,     1] loss: 0.220
[35,     1] loss: 0.253
[36,     1] loss: 0.257
[37,     1] loss: 0.265
[38,     1] loss: 0.333
[39,     1] loss: 0.294
[40,     1] loss: 0.357
[41,     1] loss: 0.301
[42,     1] loss: 0.322
[43,     1] loss: 0.310
[44,     1] loss: 0.297
[45,     1] loss: 0.240
[46,     1] loss: 0.216
[47,     1] loss: 0.262
[48,     1] loss: 0.310
[49,     1] loss: 0.248
[50,     1] loss: 0.262
[51,     1] loss: 0.242
[52,     1] loss: 0.291
[53,     1] loss: 0.241
[54,     1] loss: 0.263
[55,     1] loss: 0.223
[56,     1] loss: 0.251
[57,     1] loss: 0.217
[58,     1] loss: 0.293
[59,     1] loss: 0.248
[60,     1] loss: 0.205
[61,     1] loss: 0.214
[62,     1] loss: 0.194
[63,     1] loss: 0.237
[64,     1] loss: 0.199
[65,     1] loss: 0.222
[66,     1] loss: 0.183
[67,     1] loss: 0.199
[68,     1] loss: 0.176
[69,     1] loss: 0.176
[70,     1] loss: 0.192
[71,     1] loss: 0.148
[72,     1] loss: 0.162
[73,     1] loss: 0.164
[74,     1] loss: 0.150
[75,     1] loss: 0.163
[76,     1] loss: 0.156
[77,     1] loss: 0.176
[78,     1] loss: 0.254
[79,     1] loss: 0.221
[80,     1] loss: 0.180
[81,     1] loss: 0.226
[82,     1] loss: 0.327
[83,     1] loss: 0.204
[84,     1] loss: 0.252
[85,     1] loss: 0.216
[86,     1] loss: 0.197
[87,     1] loss: 0.192
[88,     1] loss: 0.220
[89,     1] loss: 0.182
[90,     1] loss: 0.228
[91,     1] loss: 0.195
[92,     1] loss: 0.248
[93,     1] loss: 0.243
[94,     1] loss: 0.282
[95,     1] loss: 0.317
[96,     1] loss: 0.237
[97,     1] loss: 0.252
[98,     1] loss: 0.259
[99,     1] loss: 0.225
[100,     1] loss: 0.208
[101,     1] loss: 0.222
[102,     1] loss: 0.194
[103,     1] loss: 0.257
[104,     1] loss: 0.208
Early stopping applied (best metric=0.41915881633758545)
Finished Training
Total time taken: 11.386051177978516
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.689
[3,     1] loss: 0.672
[4,     1] loss: 0.649
[5,     1] loss: 0.643
[6,     1] loss: 0.619
[7,     1] loss: 0.595
[8,     1] loss: 0.596
[9,     1] loss: 0.557
[10,     1] loss: 0.551
[11,     1] loss: 0.527
[12,     1] loss: 0.515
[13,     1] loss: 0.483
[14,     1] loss: 0.448
[15,     1] loss: 0.429
[16,     1] loss: 0.437
[17,     1] loss: 0.412
[18,     1] loss: 0.426
[19,     1] loss: 0.373
[20,     1] loss: 0.356
[21,     1] loss: 0.379
[22,     1] loss: 0.353
[23,     1] loss: 0.312
[24,     1] loss: 0.348
[25,     1] loss: 0.329
[26,     1] loss: 0.348
[27,     1] loss: 0.305
[28,     1] loss: 0.279
[29,     1] loss: 0.314
[30,     1] loss: 0.268
[31,     1] loss: 0.299
[32,     1] loss: 0.261
[33,     1] loss: 0.258
[34,     1] loss: 0.288
[35,     1] loss: 0.238
[36,     1] loss: 0.226
[37,     1] loss: 0.223
[38,     1] loss: 0.217
[39,     1] loss: 0.230
[40,     1] loss: 0.254
[41,     1] loss: 0.218
[42,     1] loss: 0.203
[43,     1] loss: 0.180
[44,     1] loss: 0.189
[45,     1] loss: 0.209
[46,     1] loss: 0.216
[47,     1] loss: 0.213
[48,     1] loss: 0.190
[49,     1] loss: 0.172
[50,     1] loss: 0.208
[51,     1] loss: 0.178
[52,     1] loss: 0.229
[53,     1] loss: 0.180
[54,     1] loss: 0.217
[55,     1] loss: 0.236
[56,     1] loss: 0.182
[57,     1] loss: 0.235
[58,     1] loss: 0.223
[59,     1] loss: 0.237
[60,     1] loss: 0.260
[61,     1] loss: 0.212
[62,     1] loss: 0.215
[63,     1] loss: 0.210
[64,     1] loss: 0.185
[65,     1] loss: 0.260
[66,     1] loss: 0.199
[67,     1] loss: 0.207
[68,     1] loss: 0.206
[69,     1] loss: 0.176
[70,     1] loss: 0.180
[71,     1] loss: 0.190
[72,     1] loss: 0.192
[73,     1] loss: 0.234
[74,     1] loss: 0.171
[75,     1] loss: 0.172
[76,     1] loss: 0.194
[77,     1] loss: 0.169
[78,     1] loss: 0.178
[79,     1] loss: 0.173
[80,     1] loss: 0.166
[81,     1] loss: 0.161
Early stopping applied (best metric=0.31115251779556274)
Finished Training
Total time taken: 8.948075294494629
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.696
[3,     1] loss: 0.678
[4,     1] loss: 0.659
[5,     1] loss: 0.639
[6,     1] loss: 0.612
[7,     1] loss: 0.613
[8,     1] loss: 0.561
[9,     1] loss: 0.553
[10,     1] loss: 0.549
[11,     1] loss: 0.513
[12,     1] loss: 0.482
[13,     1] loss: 0.479
[14,     1] loss: 0.454
[15,     1] loss: 0.418
[16,     1] loss: 0.413
[17,     1] loss: 0.393
[18,     1] loss: 0.396
[19,     1] loss: 0.371
[20,     1] loss: 0.377
[21,     1] loss: 0.384
[22,     1] loss: 0.310
[23,     1] loss: 0.373
[24,     1] loss: 0.320
[25,     1] loss: 0.293
[26,     1] loss: 0.351
[27,     1] loss: 0.316
[28,     1] loss: 0.366
[29,     1] loss: 0.274
[30,     1] loss: 0.304
[31,     1] loss: 0.280
[32,     1] loss: 0.309
[33,     1] loss: 0.298
[34,     1] loss: 0.239
[35,     1] loss: 0.286
[36,     1] loss: 0.298
[37,     1] loss: 0.285
[38,     1] loss: 0.295
[39,     1] loss: 0.256
[40,     1] loss: 0.272
[41,     1] loss: 0.255
[42,     1] loss: 0.286
[43,     1] loss: 0.265
[44,     1] loss: 0.251
[45,     1] loss: 0.247
[46,     1] loss: 0.296
[47,     1] loss: 0.292
[48,     1] loss: 0.245
[49,     1] loss: 0.248
[50,     1] loss: 0.263
[51,     1] loss: 0.260
[52,     1] loss: 0.281
[53,     1] loss: 0.272
[54,     1] loss: 0.328
[55,     1] loss: 0.218
[56,     1] loss: 0.348
[57,     1] loss: 0.253
[58,     1] loss: 0.234
[59,     1] loss: 0.268
[60,     1] loss: 0.287
[61,     1] loss: 0.272
[62,     1] loss: 0.238
[63,     1] loss: 0.226
[64,     1] loss: 0.237
[65,     1] loss: 0.239
[66,     1] loss: 0.301
[67,     1] loss: 0.225
[68,     1] loss: 0.268
[69,     1] loss: 0.211
[70,     1] loss: 0.272
[71,     1] loss: 0.199
[72,     1] loss: 0.228
[73,     1] loss: 0.190
[74,     1] loss: 0.234
[75,     1] loss: 0.203
[76,     1] loss: 0.199
[77,     1] loss: 0.209
[78,     1] loss: 0.218
[79,     1] loss: 0.216
[80,     1] loss: 0.198
[81,     1] loss: 0.201
[82,     1] loss: 0.200
[83,     1] loss: 0.225
[84,     1] loss: 0.175
[85,     1] loss: 0.221
[86,     1] loss: 0.166
[87,     1] loss: 0.298
[88,     1] loss: 0.357
[89,     1] loss: 0.371
[90,     1] loss: 0.393
[91,     1] loss: 0.301
[92,     1] loss: 0.362
[93,     1] loss: 0.303
[94,     1] loss: 0.349
[95,     1] loss: 0.349
[96,     1] loss: 0.261
[97,     1] loss: 0.333
[98,     1] loss: 0.282
[99,     1] loss: 0.287
[100,     1] loss: 0.284
[101,     1] loss: 0.269
[102,     1] loss: 0.271
[103,     1] loss: 0.339
[104,     1] loss: 0.255
Early stopping applied (best metric=0.2815294563770294)
Finished Training
Total time taken: 11.415574073791504
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.679
[4,     1] loss: 0.657
[5,     1] loss: 0.648
[6,     1] loss: 0.634
[7,     1] loss: 0.598
[8,     1] loss: 0.598
[9,     1] loss: 0.583
[10,     1] loss: 0.536
[11,     1] loss: 0.528
[12,     1] loss: 0.514
[13,     1] loss: 0.466
[14,     1] loss: 0.475
[15,     1] loss: 0.460
[16,     1] loss: 0.441
[17,     1] loss: 0.435
[18,     1] loss: 0.405
[19,     1] loss: 0.387
[20,     1] loss: 0.416
[21,     1] loss: 0.423
[22,     1] loss: 0.412
[23,     1] loss: 0.328
[24,     1] loss: 0.353
[25,     1] loss: 0.374
[26,     1] loss: 0.362
[27,     1] loss: 0.382
[28,     1] loss: 0.336
[29,     1] loss: 0.350
[30,     1] loss: 0.334
[31,     1] loss: 0.326
[32,     1] loss: 0.340
[33,     1] loss: 0.347
[34,     1] loss: 0.332
[35,     1] loss: 0.369
[36,     1] loss: 0.343
[37,     1] loss: 0.374
[38,     1] loss: 0.417
[39,     1] loss: 0.388
[40,     1] loss: 0.312
[41,     1] loss: 0.307
[42,     1] loss: 0.354
[43,     1] loss: 0.334
[44,     1] loss: 0.294
[45,     1] loss: 0.307
[46,     1] loss: 0.274
[47,     1] loss: 0.297
[48,     1] loss: 0.287
[49,     1] loss: 0.279
[50,     1] loss: 0.334
[51,     1] loss: 0.337
[52,     1] loss: 0.282
[53,     1] loss: 0.322
[54,     1] loss: 0.310
[55,     1] loss: 0.265
[56,     1] loss: 0.280
[57,     1] loss: 0.310
[58,     1] loss: 0.245
[59,     1] loss: 0.258
[60,     1] loss: 0.231
[61,     1] loss: 0.254
[62,     1] loss: 0.246
[63,     1] loss: 0.207
[64,     1] loss: 0.267
[65,     1] loss: 0.194
[66,     1] loss: 0.205
[67,     1] loss: 0.246
[68,     1] loss: 0.267
[69,     1] loss: 0.250
[70,     1] loss: 0.224
[71,     1] loss: 0.219
[72,     1] loss: 0.219
[73,     1] loss: 0.230
[74,     1] loss: 0.227
[75,     1] loss: 0.519
[76,     1] loss: 0.211
[77,     1] loss: 0.261
[78,     1] loss: 0.232
[79,     1] loss: 0.234
[80,     1] loss: 0.266
[81,     1] loss: 0.231
[82,     1] loss: 0.293
[83,     1] loss: 0.248
[84,     1] loss: 0.313
[85,     1] loss: 0.196
[86,     1] loss: 0.241
[87,     1] loss: 0.246
[88,     1] loss: 0.255
[89,     1] loss: 0.225
[90,     1] loss: 0.207
[91,     1] loss: 0.220
[92,     1] loss: 0.262
[93,     1] loss: 0.253
[94,     1] loss: 0.306
[95,     1] loss: 0.249
[96,     1] loss: 0.378
[97,     1] loss: 0.310
[98,     1] loss: 0.311
[99,     1] loss: 0.367
[100,     1] loss: 0.339
[101,     1] loss: 0.281
[102,     1] loss: 0.289
[103,     1] loss: 0.307
[104,     1] loss: 0.256
[105,     1] loss: 0.249
[106,     1] loss: 0.368
[107,     1] loss: 0.285
[108,     1] loss: 0.336
[109,     1] loss: 0.298
[110,     1] loss: 0.295
[111,     1] loss: 0.301
[112,     1] loss: 0.299
[113,     1] loss: 0.274
[114,     1] loss: 0.276
[115,     1] loss: 0.298
[116,     1] loss: 0.258
[117,     1] loss: 0.254
[118,     1] loss: 0.292
[119,     1] loss: 0.256
[120,     1] loss: 0.245
[121,     1] loss: 0.254
[122,     1] loss: 0.272
[123,     1] loss: 0.265
[124,     1] loss: 0.280
[125,     1] loss: 0.245
[126,     1] loss: 0.264
[127,     1] loss: 0.243
[128,     1] loss: 0.208
Early stopping applied (best metric=0.27941903471946716)
Finished Training
Total time taken: 13.87775444984436
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.678
[4,     1] loss: 0.672
[5,     1] loss: 0.658
[6,     1] loss: 0.646
[7,     1] loss: 0.629
[8,     1] loss: 0.609
[9,     1] loss: 0.580
[10,     1] loss: 0.592
[11,     1] loss: 0.580
[12,     1] loss: 0.546
[13,     1] loss: 0.514
[14,     1] loss: 0.526
[15,     1] loss: 0.496
[16,     1] loss: 0.466
[17,     1] loss: 0.449
[18,     1] loss: 0.466
[19,     1] loss: 0.416
[20,     1] loss: 0.414
[21,     1] loss: 0.368
[22,     1] loss: 0.377
[23,     1] loss: 0.339
[24,     1] loss: 0.355
[25,     1] loss: 0.336
[26,     1] loss: 0.318
[27,     1] loss: 0.402
[28,     1] loss: 0.361
[29,     1] loss: 0.351
[30,     1] loss: 0.372
[31,     1] loss: 0.378
[32,     1] loss: 0.343
[33,     1] loss: 0.344
[34,     1] loss: 0.332
[35,     1] loss: 0.310
[36,     1] loss: 0.317
[37,     1] loss: 0.313
[38,     1] loss: 0.288
[39,     1] loss: 0.283
[40,     1] loss: 0.281
[41,     1] loss: 0.324
[42,     1] loss: 0.304
[43,     1] loss: 0.283
[44,     1] loss: 0.300
[45,     1] loss: 0.317
[46,     1] loss: 0.264
[47,     1] loss: 0.325
[48,     1] loss: 0.286
[49,     1] loss: 0.319
[50,     1] loss: 0.277
[51,     1] loss: 0.325
[52,     1] loss: 0.297
[53,     1] loss: 0.309
[54,     1] loss: 0.316
[55,     1] loss: 0.332
[56,     1] loss: 0.339
[57,     1] loss: 0.301
[58,     1] loss: 0.261
[59,     1] loss: 0.261
[60,     1] loss: 0.360
[61,     1] loss: 0.327
[62,     1] loss: 0.291
[63,     1] loss: 0.342
[64,     1] loss: 0.291
[65,     1] loss: 0.303
[66,     1] loss: 0.288
[67,     1] loss: 0.284
[68,     1] loss: 0.284
[69,     1] loss: 0.272
[70,     1] loss: 0.267
[71,     1] loss: 0.273
[72,     1] loss: 0.288
[73,     1] loss: 0.267
[74,     1] loss: 0.305
[75,     1] loss: 0.280
[76,     1] loss: 0.301
[77,     1] loss: 0.323
[78,     1] loss: 0.319
[79,     1] loss: 0.336
[80,     1] loss: 0.288
[81,     1] loss: 0.294
[82,     1] loss: 0.293
[83,     1] loss: 0.255
[84,     1] loss: 0.276
[85,     1] loss: 0.249
[86,     1] loss: 0.253
[87,     1] loss: 0.228
[88,     1] loss: 0.243
[89,     1] loss: 0.237
[90,     1] loss: 0.209
[91,     1] loss: 0.290
[92,     1] loss: 0.223
[93,     1] loss: 0.291
[94,     1] loss: 0.239
[95,     1] loss: 0.282
[96,     1] loss: 0.281
[97,     1] loss: 0.232
[98,     1] loss: 0.237
[99,     1] loss: 0.274
[100,     1] loss: 0.232
[101,     1] loss: 0.223
[102,     1] loss: 0.269
[103,     1] loss: 0.224
[104,     1] loss: 0.223
[105,     1] loss: 0.212
[106,     1] loss: 0.214
[107,     1] loss: 0.208
[108,     1] loss: 0.236
[109,     1] loss: 0.259
[110,     1] loss: 0.285
[111,     1] loss: 0.315
[112,     1] loss: 0.240
[113,     1] loss: 0.241
[114,     1] loss: 0.243
[115,     1] loss: 0.203
[116,     1] loss: 0.253
[117,     1] loss: 0.245
[118,     1] loss: 0.267
[119,     1] loss: 0.209
[120,     1] loss: 0.222
[121,     1] loss: 0.226
[122,     1] loss: 0.265
[123,     1] loss: 0.270
[124,     1] loss: 0.230
[125,     1] loss: 0.236
[126,     1] loss: 0.239
[127,     1] loss: 0.217
[128,     1] loss: 0.238
[129,     1] loss: 0.272
[130,     1] loss: 0.232
[131,     1] loss: 0.257
[132,     1] loss: 0.237
[133,     1] loss: 0.262
[134,     1] loss: 0.235
[135,     1] loss: 0.240
[136,     1] loss: 0.221
[137,     1] loss: 0.269
[138,     1] loss: 0.241
[139,     1] loss: 0.257
[140,     1] loss: 0.269
[141,     1] loss: 0.272
[142,     1] loss: 0.245
[143,     1] loss: 0.224
[144,     1] loss: 0.264
[145,     1] loss: 0.223
[146,     1] loss: 0.244
[147,     1] loss: 0.224
[148,     1] loss: 0.249
[149,     1] loss: 0.213
[150,     1] loss: 0.253
[151,     1] loss: 0.222
[152,     1] loss: 0.243
[153,     1] loss: 0.204
[154,     1] loss: 0.235
[155,     1] loss: 0.232
[156,     1] loss: 0.249
[157,     1] loss: 0.249
[158,     1] loss: 0.321
[159,     1] loss: 0.217
[160,     1] loss: 0.298
[161,     1] loss: 0.194
[162,     1] loss: 0.312
[163,     1] loss: 0.269
[164,     1] loss: 0.275
[165,     1] loss: 0.320
[166,     1] loss: 0.313
[167,     1] loss: 0.318
[168,     1] loss: 0.298
[169,     1] loss: 0.288
[170,     1] loss: 0.305
[171,     1] loss: 0.295
[172,     1] loss: 0.267
[173,     1] loss: 0.302
[174,     1] loss: 0.260
[175,     1] loss: 0.310
[176,     1] loss: 0.256
[177,     1] loss: 0.266
[178,     1] loss: 0.275
[179,     1] loss: 0.266
[180,     1] loss: 0.264
[181,     1] loss: 0.291
[182,     1] loss: 0.284
[183,     1] loss: 0.253
[184,     1] loss: 0.265
[185,     1] loss: 0.239
[186,     1] loss: 0.229
[187,     1] loss: 0.244
[188,     1] loss: 0.212
[189,     1] loss: 0.205
[190,     1] loss: 0.233
[191,     1] loss: 0.227
[192,     1] loss: 0.207
[193,     1] loss: 0.221
[194,     1] loss: 0.197
Early stopping applied (best metric=0.22067013382911682)
Finished Training
Total time taken: 21.180364847183228
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.685
[4,     1] loss: 0.659
[5,     1] loss: 0.647
[6,     1] loss: 0.637
[7,     1] loss: 0.611
[8,     1] loss: 0.607
[9,     1] loss: 0.571
[10,     1] loss: 0.553
[11,     1] loss: 0.555
[12,     1] loss: 0.507
[13,     1] loss: 0.509
[14,     1] loss: 0.474
[15,     1] loss: 0.446
[16,     1] loss: 0.466
[17,     1] loss: 0.420
[18,     1] loss: 0.384
[19,     1] loss: 0.373
[20,     1] loss: 0.410
[21,     1] loss: 0.361
[22,     1] loss: 0.464
[23,     1] loss: 0.333
[24,     1] loss: 0.385
[25,     1] loss: 0.352
[26,     1] loss: 0.336
[27,     1] loss: 0.291
[28,     1] loss: 0.332
[29,     1] loss: 0.322
[30,     1] loss: 0.285
[31,     1] loss: 0.301
[32,     1] loss: 0.322
[33,     1] loss: 0.283
[34,     1] loss: 0.274
[35,     1] loss: 0.259
[36,     1] loss: 0.284
[37,     1] loss: 0.301
[38,     1] loss: 0.360
[39,     1] loss: 0.262
[40,     1] loss: 0.348
[41,     1] loss: 0.278
[42,     1] loss: 0.248
[43,     1] loss: 0.280
[44,     1] loss: 0.268
[45,     1] loss: 0.271
[46,     1] loss: 0.257
[47,     1] loss: 0.278
[48,     1] loss: 0.317
[49,     1] loss: 0.266
[50,     1] loss: 0.243
[51,     1] loss: 0.248
[52,     1] loss: 0.259
[53,     1] loss: 0.258
[54,     1] loss: 0.285
[55,     1] loss: 0.274
[56,     1] loss: 0.263
[57,     1] loss: 0.233
[58,     1] loss: 0.323
[59,     1] loss: 0.318
[60,     1] loss: 0.327
[61,     1] loss: 0.272
[62,     1] loss: 0.267
[63,     1] loss: 0.307
[64,     1] loss: 0.292
[65,     1] loss: 0.303
[66,     1] loss: 0.261
[67,     1] loss: 0.317
[68,     1] loss: 0.244
[69,     1] loss: 0.276
[70,     1] loss: 0.269
[71,     1] loss: 0.291
[72,     1] loss: 0.286
[73,     1] loss: 0.270
[74,     1] loss: 0.273
[75,     1] loss: 0.241
[76,     1] loss: 0.304
[77,     1] loss: 0.282
[78,     1] loss: 0.247
[79,     1] loss: 0.318
[80,     1] loss: 0.268
[81,     1] loss: 0.267
[82,     1] loss: 0.300
[83,     1] loss: 0.293
[84,     1] loss: 0.255
[85,     1] loss: 0.239
[86,     1] loss: 0.303
[87,     1] loss: 0.253
[88,     1] loss: 0.216
[89,     1] loss: 0.267
[90,     1] loss: 0.281
[91,     1] loss: 0.251
[92,     1] loss: 0.240
[93,     1] loss: 0.222
[94,     1] loss: 0.230
[95,     1] loss: 0.242
[96,     1] loss: 0.229
[97,     1] loss: 0.254
[98,     1] loss: 0.210
[99,     1] loss: 0.238
[100,     1] loss: 0.236
[101,     1] loss: 0.217
[102,     1] loss: 0.220
[103,     1] loss: 0.227
[104,     1] loss: 0.225
[105,     1] loss: 0.223
[106,     1] loss: 0.280
[107,     1] loss: 0.241
[108,     1] loss: 0.255
[109,     1] loss: 0.272
[110,     1] loss: 0.279
[111,     1] loss: 0.268
[112,     1] loss: 0.290
[113,     1] loss: 0.280
[114,     1] loss: 0.249
[115,     1] loss: 0.224
[116,     1] loss: 0.230
[117,     1] loss: 0.226
[118,     1] loss: 0.231
[119,     1] loss: 0.254
[120,     1] loss: 0.220
[121,     1] loss: 0.208
[122,     1] loss: 0.230
[123,     1] loss: 0.243
[124,     1] loss: 0.191
[125,     1] loss: 0.244
[126,     1] loss: 0.280
[127,     1] loss: 0.227
[128,     1] loss: 0.264
[129,     1] loss: 0.284
[130,     1] loss: 0.339
[131,     1] loss: 0.280
[132,     1] loss: 0.320
[133,     1] loss: 0.415
[134,     1] loss: 0.333
[135,     1] loss: 0.295
[136,     1] loss: 0.325
[137,     1] loss: 0.296
[138,     1] loss: 0.309
[139,     1] loss: 0.297
[140,     1] loss: 0.300
[141,     1] loss: 0.278
[142,     1] loss: 0.271
[143,     1] loss: 0.264
[144,     1] loss: 0.261
[145,     1] loss: 0.255
[146,     1] loss: 0.255
[147,     1] loss: 0.323
[148,     1] loss: 0.297
[149,     1] loss: 0.317
[150,     1] loss: 0.290
[151,     1] loss: 0.260
[152,     1] loss: 0.272
Early stopping applied (best metric=0.31364667415618896)
Finished Training
Total time taken: 16.567493438720703
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.679
[4,     1] loss: 0.655
[5,     1] loss: 0.632
[6,     1] loss: 0.604
[7,     1] loss: 0.595
[8,     1] loss: 0.571
[9,     1] loss: 0.550
[10,     1] loss: 0.543
[11,     1] loss: 0.507
[12,     1] loss: 0.494
[13,     1] loss: 0.506
[14,     1] loss: 0.468
[15,     1] loss: 0.433
[16,     1] loss: 0.432
[17,     1] loss: 0.409
[18,     1] loss: 0.370
[19,     1] loss: 0.385
[20,     1] loss: 0.331
[21,     1] loss: 0.322
[22,     1] loss: 0.367
[23,     1] loss: 0.307
[24,     1] loss: 0.335
[25,     1] loss: 0.310
[26,     1] loss: 0.338
[27,     1] loss: 0.356
[28,     1] loss: 0.329
[29,     1] loss: 0.293
[30,     1] loss: 0.322
[31,     1] loss: 0.318
[32,     1] loss: 0.306
[33,     1] loss: 0.296
[34,     1] loss: 0.275
[35,     1] loss: 0.277
[36,     1] loss: 0.270
[37,     1] loss: 0.271
[38,     1] loss: 0.262
[39,     1] loss: 0.262
[40,     1] loss: 0.248
[41,     1] loss: 0.278
[42,     1] loss: 0.317
[43,     1] loss: 0.346
[44,     1] loss: 0.297
[45,     1] loss: 0.310
[46,     1] loss: 0.333
[47,     1] loss: 0.298
[48,     1] loss: 0.305
[49,     1] loss: 0.310
[50,     1] loss: 0.296
[51,     1] loss: 0.327
[52,     1] loss: 0.263
[53,     1] loss: 0.279
[54,     1] loss: 0.282
[55,     1] loss: 0.260
[56,     1] loss: 0.271
[57,     1] loss: 0.271
[58,     1] loss: 0.250
[59,     1] loss: 0.288
[60,     1] loss: 0.242
[61,     1] loss: 0.276
[62,     1] loss: 0.204
[63,     1] loss: 0.211
[64,     1] loss: 0.243
[65,     1] loss: 0.235
[66,     1] loss: 0.259
[67,     1] loss: 0.256
[68,     1] loss: 0.237
[69,     1] loss: 0.223
[70,     1] loss: 0.310
[71,     1] loss: 0.272
[72,     1] loss: 0.418
[73,     1] loss: 0.328
[74,     1] loss: 0.270
[75,     1] loss: 0.358
[76,     1] loss: 0.312
[77,     1] loss: 0.278
[78,     1] loss: 0.313
[79,     1] loss: 0.314
[80,     1] loss: 0.318
[81,     1] loss: 0.266
[82,     1] loss: 0.272
[83,     1] loss: 0.300
[84,     1] loss: 0.296
[85,     1] loss: 0.342
[86,     1] loss: 0.306
[87,     1] loss: 0.284
[88,     1] loss: 0.273
[89,     1] loss: 0.313
[90,     1] loss: 0.306
[91,     1] loss: 0.298
[92,     1] loss: 0.293
[93,     1] loss: 0.272
[94,     1] loss: 0.310
[95,     1] loss: 0.300
[96,     1] loss: 0.325
Early stopping applied (best metric=0.23950152099132538)
Finished Training
Total time taken: 10.431342601776123
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.687
[3,     1] loss: 0.676
[4,     1] loss: 0.656
[5,     1] loss: 0.623
[6,     1] loss: 0.605
[7,     1] loss: 0.595
[8,     1] loss: 0.573
[9,     1] loss: 0.551
[10,     1] loss: 0.495
[11,     1] loss: 0.466
[12,     1] loss: 0.453
[13,     1] loss: 0.408
[14,     1] loss: 0.440
[15,     1] loss: 0.445
[16,     1] loss: 0.369
[17,     1] loss: 0.398
[18,     1] loss: 0.364
[19,     1] loss: 0.329
[20,     1] loss: 0.373
[21,     1] loss: 0.399
[22,     1] loss: 0.327
[23,     1] loss: 0.353
[24,     1] loss: 0.321
[25,     1] loss: 0.356
[26,     1] loss: 0.362
[27,     1] loss: 0.276
[28,     1] loss: 0.313
[29,     1] loss: 0.363
[30,     1] loss: 0.328
[31,     1] loss: 0.339
[32,     1] loss: 0.305
[33,     1] loss: 0.342
[34,     1] loss: 0.278
[35,     1] loss: 0.350
[36,     1] loss: 0.277
[37,     1] loss: 0.332
[38,     1] loss: 0.324
[39,     1] loss: 0.284
[40,     1] loss: 0.294
[41,     1] loss: 0.262
[42,     1] loss: 0.297
[43,     1] loss: 0.289
[44,     1] loss: 0.301
[45,     1] loss: 0.273
[46,     1] loss: 0.259
[47,     1] loss: 0.234
[48,     1] loss: 0.257
[49,     1] loss: 0.243
[50,     1] loss: 0.214
[51,     1] loss: 0.248
[52,     1] loss: 0.298
[53,     1] loss: 0.306
[54,     1] loss: 0.302
[55,     1] loss: 0.258
[56,     1] loss: 0.353
[57,     1] loss: 0.324
[58,     1] loss: 0.315
[59,     1] loss: 0.339
[60,     1] loss: 0.319
Early stopping applied (best metric=0.43322816491127014)
Finished Training
Total time taken: 6.5470311641693115
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.683
[4,     1] loss: 0.671
[5,     1] loss: 0.666
[6,     1] loss: 0.649
[7,     1] loss: 0.629
[8,     1] loss: 0.616
[9,     1] loss: 0.603
[10,     1] loss: 0.577
[11,     1] loss: 0.552
[12,     1] loss: 0.524
[13,     1] loss: 0.517
[14,     1] loss: 0.490
[15,     1] loss: 0.450
[16,     1] loss: 0.424
[17,     1] loss: 0.433
[18,     1] loss: 0.393
[19,     1] loss: 0.391
[20,     1] loss: 0.344
[21,     1] loss: 0.351
[22,     1] loss: 0.341
[23,     1] loss: 0.332
[24,     1] loss: 0.340
[25,     1] loss: 0.298
[26,     1] loss: 0.275
[27,     1] loss: 0.269
[28,     1] loss: 0.275
[29,     1] loss: 0.307
[30,     1] loss: 0.304
[31,     1] loss: 0.249
[32,     1] loss: 0.301
[33,     1] loss: 0.296
[34,     1] loss: 0.236
[35,     1] loss: 0.256
[36,     1] loss: 0.272
[37,     1] loss: 0.279
[38,     1] loss: 0.258
[39,     1] loss: 0.258
[40,     1] loss: 0.273
[41,     1] loss: 0.277
[42,     1] loss: 0.245
[43,     1] loss: 0.321
[44,     1] loss: 0.304
[45,     1] loss: 0.276
[46,     1] loss: 0.294
[47,     1] loss: 0.260
[48,     1] loss: 0.285
[49,     1] loss: 0.227
[50,     1] loss: 0.235
[51,     1] loss: 0.275
[52,     1] loss: 0.273
[53,     1] loss: 0.318
[54,     1] loss: 0.277
[55,     1] loss: 0.272
[56,     1] loss: 0.244
[57,     1] loss: 0.282
[58,     1] loss: 0.277
[59,     1] loss: 0.270
[60,     1] loss: 0.242
[61,     1] loss: 0.274
[62,     1] loss: 0.270
[63,     1] loss: 0.237
[64,     1] loss: 0.248
[65,     1] loss: 0.232
[66,     1] loss: 0.234
[67,     1] loss: 0.266
[68,     1] loss: 0.241
[69,     1] loss: 0.227
[70,     1] loss: 0.227
[71,     1] loss: 0.258
[72,     1] loss: 0.283
[73,     1] loss: 0.247
[74,     1] loss: 0.259
[75,     1] loss: 0.224
[76,     1] loss: 0.267
[77,     1] loss: 0.243
[78,     1] loss: 0.285
Early stopping applied (best metric=0.43741554021835327)
Finished Training
Total time taken: 8.542490720748901
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.688
[3,     1] loss: 0.674
[4,     1] loss: 0.662
[5,     1] loss: 0.649
[6,     1] loss: 0.634
[7,     1] loss: 0.616
[8,     1] loss: 0.602
[9,     1] loss: 0.568
[10,     1] loss: 0.581
[11,     1] loss: 0.549
[12,     1] loss: 0.542
[13,     1] loss: 0.535
[14,     1] loss: 0.509
[15,     1] loss: 0.489
[16,     1] loss: 0.490
[17,     1] loss: 0.474
[18,     1] loss: 0.424
[19,     1] loss: 0.407
[20,     1] loss: 0.414
[21,     1] loss: 0.405
[22,     1] loss: 0.382
[23,     1] loss: 0.405
[24,     1] loss: 0.367
[25,     1] loss: 0.390
[26,     1] loss: 0.329
[27,     1] loss: 0.360
[28,     1] loss: 0.348
[29,     1] loss: 0.351
[30,     1] loss: 0.334
[31,     1] loss: 0.331
[32,     1] loss: 0.312
[33,     1] loss: 0.321
[34,     1] loss: 0.272
[35,     1] loss: 0.312
[36,     1] loss: 0.289
[37,     1] loss: 0.325
[38,     1] loss: 0.361
[39,     1] loss: 0.408
[40,     1] loss: 0.363
[41,     1] loss: 0.357
[42,     1] loss: 0.334
[43,     1] loss: 0.311
[44,     1] loss: 0.317
[45,     1] loss: 0.438
[46,     1] loss: 0.400
[47,     1] loss: 0.340
[48,     1] loss: 0.348
[49,     1] loss: 0.306
[50,     1] loss: 0.376
[51,     1] loss: 0.294
[52,     1] loss: 0.311
[53,     1] loss: 0.298
[54,     1] loss: 0.323
[55,     1] loss: 0.295
[56,     1] loss: 0.328
[57,     1] loss: 0.298
[58,     1] loss: 0.284
[59,     1] loss: 0.274
[60,     1] loss: 0.262
[61,     1] loss: 0.286
[62,     1] loss: 0.285
[63,     1] loss: 0.252
[64,     1] loss: 0.287
[65,     1] loss: 0.282
[66,     1] loss: 0.262
[67,     1] loss: 0.242
[68,     1] loss: 0.255
[69,     1] loss: 0.214
[70,     1] loss: 0.311
[71,     1] loss: 0.287
[72,     1] loss: 0.352
[73,     1] loss: 0.308
[74,     1] loss: 0.280
[75,     1] loss: 0.335
[76,     1] loss: 0.287
Early stopping applied (best metric=0.3516203761100769)
Finished Training
Total time taken: 8.357164859771729
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.679
[4,     1] loss: 0.663
[5,     1] loss: 0.640
[6,     1] loss: 0.619
[7,     1] loss: 0.599
[8,     1] loss: 0.591
[9,     1] loss: 0.564
[10,     1] loss: 0.541
[11,     1] loss: 0.532
[12,     1] loss: 0.517
[13,     1] loss: 0.476
[14,     1] loss: 0.482
[15,     1] loss: 0.447
[16,     1] loss: 0.412
[17,     1] loss: 0.413
[18,     1] loss: 0.403
[19,     1] loss: 0.405
[20,     1] loss: 0.419
[21,     1] loss: 0.350
[22,     1] loss: 0.329
[23,     1] loss: 0.339
[24,     1] loss: 0.325
[25,     1] loss: 0.328
[26,     1] loss: 0.331
[27,     1] loss: 0.342
[28,     1] loss: 0.299
[29,     1] loss: 0.298
[30,     1] loss: 0.254
[31,     1] loss: 0.290
[32,     1] loss: 0.269
[33,     1] loss: 0.334
[34,     1] loss: 0.321
[35,     1] loss: 0.282
[36,     1] loss: 0.397
[37,     1] loss: 0.298
[38,     1] loss: 0.371
[39,     1] loss: 0.302
[40,     1] loss: 0.410
[41,     1] loss: 0.389
[42,     1] loss: 0.320
[43,     1] loss: 0.347
[44,     1] loss: 0.394
[45,     1] loss: 0.332
[46,     1] loss: 0.329
[47,     1] loss: 0.352
[48,     1] loss: 0.343
[49,     1] loss: 0.317
[50,     1] loss: 0.342
[51,     1] loss: 0.308
[52,     1] loss: 0.336
[53,     1] loss: 0.314
[54,     1] loss: 0.341
[55,     1] loss: 0.311
[56,     1] loss: 0.312
[57,     1] loss: 0.287
[58,     1] loss: 0.309
[59,     1] loss: 0.275
[60,     1] loss: 0.272
[61,     1] loss: 0.269
[62,     1] loss: 0.274
[63,     1] loss: 0.321
[64,     1] loss: 0.278
[65,     1] loss: 0.261
[66,     1] loss: 0.264
[67,     1] loss: 0.251
[68,     1] loss: 0.254
[69,     1] loss: 0.243
[70,     1] loss: 0.205
[71,     1] loss: 0.264
[72,     1] loss: 0.242
Early stopping applied (best metric=0.35451123118400574)
Finished Training
Total time taken: 7.880803108215332
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.685
[3,     1] loss: 0.675
[4,     1] loss: 0.655
[5,     1] loss: 0.639
[6,     1] loss: 0.614
[7,     1] loss: 0.607
[8,     1] loss: 0.559
[9,     1] loss: 0.586
[10,     1] loss: 0.559
[11,     1] loss: 0.550
[12,     1] loss: 0.514
[13,     1] loss: 0.495
[14,     1] loss: 0.481
[15,     1] loss: 0.494
[16,     1] loss: 0.475
[17,     1] loss: 0.393
[18,     1] loss: 0.376
[19,     1] loss: 0.388
[20,     1] loss: 0.366
[21,     1] loss: 0.408
[22,     1] loss: 0.345
[23,     1] loss: 0.317
[24,     1] loss: 0.343
[25,     1] loss: 0.314
[26,     1] loss: 0.321
[27,     1] loss: 0.346
[28,     1] loss: 0.303
[29,     1] loss: 0.315
[30,     1] loss: 0.350
[31,     1] loss: 0.259
[32,     1] loss: 0.375
[33,     1] loss: 0.349
[34,     1] loss: 0.297
[35,     1] loss: 0.262
[36,     1] loss: 0.411
[37,     1] loss: 0.280
[38,     1] loss: 0.345
[39,     1] loss: 0.334
[40,     1] loss: 0.296
[41,     1] loss: 0.266
[42,     1] loss: 0.292
[43,     1] loss: 0.345
[44,     1] loss: 0.301
[45,     1] loss: 0.255
[46,     1] loss: 0.244
[47,     1] loss: 0.273
[48,     1] loss: 0.242
[49,     1] loss: 0.257
[50,     1] loss: 0.260
[51,     1] loss: 0.208
[52,     1] loss: 0.241
[53,     1] loss: 0.232
[54,     1] loss: 0.210
[55,     1] loss: 0.233
[56,     1] loss: 0.245
[57,     1] loss: 0.235
[58,     1] loss: 0.252
[59,     1] loss: 0.314
[60,     1] loss: 0.298
[61,     1] loss: 0.243
[62,     1] loss: 0.297
[63,     1] loss: 0.292
[64,     1] loss: 0.243
[65,     1] loss: 0.353
[66,     1] loss: 0.286
[67,     1] loss: 0.345
[68,     1] loss: 0.286
[69,     1] loss: 0.314
[70,     1] loss: 0.313
[71,     1] loss: 0.312
[72,     1] loss: 0.290
[73,     1] loss: 0.274
[74,     1] loss: 0.253
[75,     1] loss: 0.250
[76,     1] loss: 0.287
[77,     1] loss: 0.257
[78,     1] loss: 0.249
[79,     1] loss: 0.276
[80,     1] loss: 0.256
[81,     1] loss: 0.214
[82,     1] loss: 0.218
[83,     1] loss: 0.227
[84,     1] loss: 0.211
[85,     1] loss: 0.239
[86,     1] loss: 0.205
[87,     1] loss: 0.191
[88,     1] loss: 0.215
[89,     1] loss: 0.190
[90,     1] loss: 0.198
[91,     1] loss: 0.182
[92,     1] loss: 0.207
[93,     1] loss: 0.184
[94,     1] loss: 0.292
[95,     1] loss: 0.294
[96,     1] loss: 0.394
[97,     1] loss: 0.292
[98,     1] loss: 0.278
[99,     1] loss: 0.326
[100,     1] loss: 0.259
[101,     1] loss: 0.277
[102,     1] loss: 0.269
[103,     1] loss: 0.292
[104,     1] loss: 0.250
[105,     1] loss: 0.259
[106,     1] loss: 0.280
[107,     1] loss: 0.256
[108,     1] loss: 0.264
[109,     1] loss: 0.252
Early stopping applied (best metric=0.37682342529296875)
Finished Training
Total time taken: 11.879252433776855
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.661
[5,     1] loss: 0.640
[6,     1] loss: 0.636
[7,     1] loss: 0.606
[8,     1] loss: 0.580
[9,     1] loss: 0.580
[10,     1] loss: 0.535
[11,     1] loss: 0.508
[12,     1] loss: 0.471
[13,     1] loss: 0.509
[14,     1] loss: 0.474
[15,     1] loss: 0.437
[16,     1] loss: 0.427
[17,     1] loss: 0.449
[18,     1] loss: 0.393
[19,     1] loss: 0.395
[20,     1] loss: 0.390
[21,     1] loss: 0.386
[22,     1] loss: 0.405
[23,     1] loss: 0.402
[24,     1] loss: 0.389
[25,     1] loss: 0.353
[26,     1] loss: 0.383
[27,     1] loss: 0.362
[28,     1] loss: 0.318
[29,     1] loss: 0.330
[30,     1] loss: 0.348
[31,     1] loss: 0.354
[32,     1] loss: 0.422
[33,     1] loss: 0.321
[34,     1] loss: 0.360
[35,     1] loss: 0.354
[36,     1] loss: 0.393
[37,     1] loss: 0.335
[38,     1] loss: 0.402
[39,     1] loss: 0.373
[40,     1] loss: 0.339
[41,     1] loss: 0.419
[42,     1] loss: 0.383
[43,     1] loss: 0.339
[44,     1] loss: 0.353
[45,     1] loss: 0.350
[46,     1] loss: 0.371
[47,     1] loss: 0.370
[48,     1] loss: 0.376
[49,     1] loss: 0.311
[50,     1] loss: 0.366
[51,     1] loss: 0.376
[52,     1] loss: 0.390
[53,     1] loss: 0.335
[54,     1] loss: 0.336
[55,     1] loss: 0.335
[56,     1] loss: 0.339
[57,     1] loss: 0.317
[58,     1] loss: 0.334
[59,     1] loss: 0.324
[60,     1] loss: 0.298
[61,     1] loss: 0.349
[62,     1] loss: 0.303
[63,     1] loss: 0.320
[64,     1] loss: 0.297
[65,     1] loss: 0.288
[66,     1] loss: 0.294
[67,     1] loss: 0.261
[68,     1] loss: 0.259
[69,     1] loss: 0.284
[70,     1] loss: 0.311
[71,     1] loss: 0.247
[72,     1] loss: 0.337
[73,     1] loss: 0.285
[74,     1] loss: 0.259
[75,     1] loss: 0.328
[76,     1] loss: 0.273
[77,     1] loss: 0.267
[78,     1] loss: 0.291
[79,     1] loss: 0.294
[80,     1] loss: 0.242
[81,     1] loss: 0.283
[82,     1] loss: 0.255
[83,     1] loss: 0.276
[84,     1] loss: 0.230
[85,     1] loss: 0.217
[86,     1] loss: 0.256
[87,     1] loss: 0.252
[88,     1] loss: 0.255
[89,     1] loss: 0.235
[90,     1] loss: 0.219
[91,     1] loss: 0.221
[92,     1] loss: 0.228
[93,     1] loss: 0.305
[94,     1] loss: 0.258
[95,     1] loss: 0.338
[96,     1] loss: 0.328
[97,     1] loss: 0.281
[98,     1] loss: 0.322
[99,     1] loss: 0.289
[100,     1] loss: 0.337
[101,     1] loss: 0.280
[102,     1] loss: 0.303
[103,     1] loss: 0.273
[104,     1] loss: 0.292
[105,     1] loss: 0.287
[106,     1] loss: 0.290
[107,     1] loss: 0.260
[108,     1] loss: 0.257
[109,     1] loss: 0.292
[110,     1] loss: 0.243
[111,     1] loss: 0.233
[112,     1] loss: 0.231
[113,     1] loss: 0.237
[114,     1] loss: 0.225
[115,     1] loss: 0.263
[116,     1] loss: 0.228
[117,     1] loss: 0.202
[118,     1] loss: 0.226
[119,     1] loss: 0.223
[120,     1] loss: 0.222
[121,     1] loss: 0.209
[122,     1] loss: 0.179
[123,     1] loss: 0.203
[124,     1] loss: 0.185
[125,     1] loss: 0.181
[126,     1] loss: 0.175
[127,     1] loss: 0.219
[128,     1] loss: 0.359
[129,     1] loss: 0.264
[130,     1] loss: 0.439
[131,     1] loss: 0.274
[132,     1] loss: 0.391
[133,     1] loss: 0.334
[134,     1] loss: 0.266
[135,     1] loss: 0.241
[136,     1] loss: 0.332
[137,     1] loss: 0.261
[138,     1] loss: 0.236
[139,     1] loss: 0.285
[140,     1] loss: 0.265
[141,     1] loss: 0.261
[142,     1] loss: 0.279
[143,     1] loss: 0.239
[144,     1] loss: 0.237
[145,     1] loss: 0.235
[146,     1] loss: 0.255
[147,     1] loss: 0.254
[148,     1] loss: 0.222
[149,     1] loss: 0.246
[150,     1] loss: 0.228
[151,     1] loss: 0.209
[152,     1] loss: 0.207
[153,     1] loss: 0.214
[154,     1] loss: 0.213
[155,     1] loss: 0.231
[156,     1] loss: 0.287
[157,     1] loss: 0.235
[158,     1] loss: 0.333
[159,     1] loss: 0.289
[160,     1] loss: 0.304
[161,     1] loss: 0.281
[162,     1] loss: 0.236
[163,     1] loss: 0.259
[164,     1] loss: 0.287
[165,     1] loss: 0.276
[166,     1] loss: 0.307
[167,     1] loss: 0.251
[168,     1] loss: 0.253
[169,     1] loss: 0.266
[170,     1] loss: 0.255
[171,     1] loss: 0.241
[172,     1] loss: 0.240
[173,     1] loss: 0.227
[174,     1] loss: 0.230
[175,     1] loss: 0.231
[176,     1] loss: 0.232
[177,     1] loss: 0.234
[178,     1] loss: 0.237
[179,     1] loss: 0.205
[180,     1] loss: 0.220
[181,     1] loss: 0.217
[182,     1] loss: 0.197
[183,     1] loss: 0.208
Early stopping applied (best metric=0.24716004729270935)
Finished Training
Total time taken: 20.087608575820923
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.682
[4,     1] loss: 0.673
[5,     1] loss: 0.654
[6,     1] loss: 0.633
[7,     1] loss: 0.611
[8,     1] loss: 0.610
[9,     1] loss: 0.578
[10,     1] loss: 0.576
[11,     1] loss: 0.558
[12,     1] loss: 0.516
[13,     1] loss: 0.509
[14,     1] loss: 0.491
[15,     1] loss: 0.487
[16,     1] loss: 0.471
[17,     1] loss: 0.453
[18,     1] loss: 0.445
[19,     1] loss: 0.379
[20,     1] loss: 0.381
[21,     1] loss: 0.400
[22,     1] loss: 0.370
[23,     1] loss: 0.402
[24,     1] loss: 0.348
[25,     1] loss: 0.387
[26,     1] loss: 0.363
[27,     1] loss: 0.339
[28,     1] loss: 0.350
[29,     1] loss: 0.392
[30,     1] loss: 0.275
[31,     1] loss: 0.311
[32,     1] loss: 0.308
[33,     1] loss: 0.263
[34,     1] loss: 0.269
[35,     1] loss: 0.287
[36,     1] loss: 0.292
[37,     1] loss: 0.216
[38,     1] loss: 0.242
[39,     1] loss: 0.260
[40,     1] loss: 0.224
[41,     1] loss: 0.228
[42,     1] loss: 0.218
[43,     1] loss: 0.258
[44,     1] loss: 0.232
[45,     1] loss: 0.247
[46,     1] loss: 0.200
[47,     1] loss: 0.189
[48,     1] loss: 0.226
[49,     1] loss: 0.245
[50,     1] loss: 0.182
[51,     1] loss: 0.217
[52,     1] loss: 0.244
[53,     1] loss: 0.177
[54,     1] loss: 0.238
[55,     1] loss: 0.191
[56,     1] loss: 0.173
[57,     1] loss: 0.292
[58,     1] loss: 0.368
[59,     1] loss: 0.301
[60,     1] loss: 0.333
[61,     1] loss: 0.364
[62,     1] loss: 0.333
[63,     1] loss: 0.288
[64,     1] loss: 0.275
[65,     1] loss: 0.346
[66,     1] loss: 0.308
[67,     1] loss: 0.324
[68,     1] loss: 0.312
[69,     1] loss: 0.304
[70,     1] loss: 0.312
[71,     1] loss: 0.319
[72,     1] loss: 0.319
[73,     1] loss: 0.345
[74,     1] loss: 0.312
[75,     1] loss: 0.298
[76,     1] loss: 0.292
[77,     1] loss: 0.291
[78,     1] loss: 0.287
[79,     1] loss: 0.277
[80,     1] loss: 0.297
[81,     1] loss: 0.272
[82,     1] loss: 0.339
[83,     1] loss: 0.310
[84,     1] loss: 0.317
[85,     1] loss: 0.328
[86,     1] loss: 0.321
[87,     1] loss: 0.344
[88,     1] loss: 0.258
[89,     1] loss: 0.244
[90,     1] loss: 0.289
[91,     1] loss: 0.282
[92,     1] loss: 0.258
[93,     1] loss: 0.213
[94,     1] loss: 0.263
[95,     1] loss: 0.247
[96,     1] loss: 0.279
[97,     1] loss: 0.249
[98,     1] loss: 0.201
[99,     1] loss: 0.244
[100,     1] loss: 0.246
[101,     1] loss: 0.193
Early stopping applied (best metric=0.2870115041732788)
Finished Training
Total time taken: 11.02112627029419
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.677
[4,     1] loss: 0.657
[5,     1] loss: 0.635
[6,     1] loss: 0.618
[7,     1] loss: 0.578
[8,     1] loss: 0.577
[9,     1] loss: 0.568
[10,     1] loss: 0.533
[11,     1] loss: 0.518
[12,     1] loss: 0.514
[13,     1] loss: 0.487
[14,     1] loss: 0.443
[15,     1] loss: 0.446
[16,     1] loss: 0.430
[17,     1] loss: 0.396
[18,     1] loss: 0.439
[19,     1] loss: 0.406
[20,     1] loss: 0.421
[21,     1] loss: 0.419
[22,     1] loss: 0.373
[23,     1] loss: 0.420
[24,     1] loss: 0.373
[25,     1] loss: 0.395
[26,     1] loss: 0.396
[27,     1] loss: 0.306
[28,     1] loss: 0.399
[29,     1] loss: 0.339
[30,     1] loss: 0.345
[31,     1] loss: 0.334
[32,     1] loss: 0.296
[33,     1] loss: 0.349
[34,     1] loss: 0.282
[35,     1] loss: 0.324
[36,     1] loss: 0.331
[37,     1] loss: 0.280
[38,     1] loss: 0.302
[39,     1] loss: 0.304
[40,     1] loss: 0.267
[41,     1] loss: 0.296
[42,     1] loss: 0.519
[43,     1] loss: 0.408
[44,     1] loss: 0.498
[45,     1] loss: 0.398
[46,     1] loss: 0.392
[47,     1] loss: 0.360
[48,     1] loss: 0.402
[49,     1] loss: 0.337
[50,     1] loss: 0.345
[51,     1] loss: 0.292
[52,     1] loss: 0.322
[53,     1] loss: 0.322
[54,     1] loss: 0.305
[55,     1] loss: 0.303
[56,     1] loss: 0.288
[57,     1] loss: 0.291
[58,     1] loss: 0.296
[59,     1] loss: 0.254
[60,     1] loss: 0.253
[61,     1] loss: 0.228
[62,     1] loss: 0.259
[63,     1] loss: 0.241
[64,     1] loss: 0.222
[65,     1] loss: 0.285
[66,     1] loss: 0.240
[67,     1] loss: 0.242
[68,     1] loss: 0.229
[69,     1] loss: 0.188
Early stopping applied (best metric=0.32483428716659546)
Finished Training
Total time taken: 7.665841102600098
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.689
[3,     1] loss: 0.671
[4,     1] loss: 0.641
[5,     1] loss: 0.619
[6,     1] loss: 0.591
[7,     1] loss: 0.577
[8,     1] loss: 0.550
[9,     1] loss: 0.543
[10,     1] loss: 0.537
[11,     1] loss: 0.500
[12,     1] loss: 0.467
[13,     1] loss: 0.426
[14,     1] loss: 0.416
[15,     1] loss: 0.401
[16,     1] loss: 0.402
[17,     1] loss: 0.391
[18,     1] loss: 0.401
[19,     1] loss: 0.367
[20,     1] loss: 0.367
[21,     1] loss: 0.339
[22,     1] loss: 0.348
[23,     1] loss: 0.355
[24,     1] loss: 0.306
[25,     1] loss: 0.328
[26,     1] loss: 0.326
[27,     1] loss: 0.288
[28,     1] loss: 0.336
[29,     1] loss: 0.309
[30,     1] loss: 0.309
[31,     1] loss: 0.308
[32,     1] loss: 0.368
[33,     1] loss: 0.354
[34,     1] loss: 0.313
[35,     1] loss: 0.335
[36,     1] loss: 0.330
[37,     1] loss: 0.338
[38,     1] loss: 0.324
[39,     1] loss: 0.333
[40,     1] loss: 0.280
[41,     1] loss: 0.341
[42,     1] loss: 0.308
[43,     1] loss: 0.327
[44,     1] loss: 0.336
[45,     1] loss: 0.330
[46,     1] loss: 0.360
[47,     1] loss: 0.371
[48,     1] loss: 0.320
[49,     1] loss: 0.296
[50,     1] loss: 0.304
[51,     1] loss: 0.307
[52,     1] loss: 0.279
[53,     1] loss: 0.283
[54,     1] loss: 0.300
[55,     1] loss: 0.311
[56,     1] loss: 0.295
[57,     1] loss: 0.264
[58,     1] loss: 0.282
[59,     1] loss: 0.277
[60,     1] loss: 0.272
[61,     1] loss: 0.303
[62,     1] loss: 0.263
[63,     1] loss: 0.314
[64,     1] loss: 0.290
[65,     1] loss: 0.276
[66,     1] loss: 0.304
[67,     1] loss: 0.270
[68,     1] loss: 0.297
[69,     1] loss: 0.276
[70,     1] loss: 0.265
[71,     1] loss: 0.270
[72,     1] loss: 0.259
[73,     1] loss: 0.293
[74,     1] loss: 0.276
[75,     1] loss: 0.292
[76,     1] loss: 0.276
[77,     1] loss: 0.305
[78,     1] loss: 0.294
[79,     1] loss: 0.287
[80,     1] loss: 0.308
[81,     1] loss: 0.290
[82,     1] loss: 0.307
[83,     1] loss: 0.286
[84,     1] loss: 0.301
[85,     1] loss: 0.271
[86,     1] loss: 0.281
[87,     1] loss: 0.281
[88,     1] loss: 0.326
[89,     1] loss: 0.354
[90,     1] loss: 0.309
[91,     1] loss: 0.335
[92,     1] loss: 0.320
[93,     1] loss: 0.303
[94,     1] loss: 0.287
[95,     1] loss: 0.318
[96,     1] loss: 0.336
[97,     1] loss: 0.278
[98,     1] loss: 0.303
[99,     1] loss: 0.315
[100,     1] loss: 0.260
[101,     1] loss: 0.302
[102,     1] loss: 0.254
[103,     1] loss: 0.265
[104,     1] loss: 0.315
[105,     1] loss: 0.277
[106,     1] loss: 0.301
[107,     1] loss: 0.295
[108,     1] loss: 0.266
[109,     1] loss: 0.292
[110,     1] loss: 0.264
[111,     1] loss: 0.292
[112,     1] loss: 0.248
[113,     1] loss: 0.275
[114,     1] loss: 0.259
[115,     1] loss: 0.305
[116,     1] loss: 0.321
[117,     1] loss: 0.309
[118,     1] loss: 0.269
Early stopping applied (best metric=0.3532460927963257)
Finished Training
Total time taken: 12.9215669631958
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.685
[4,     1] loss: 0.674
[5,     1] loss: 0.649
[6,     1] loss: 0.630
[7,     1] loss: 0.607
[8,     1] loss: 0.580
[9,     1] loss: 0.554
[10,     1] loss: 0.542
[11,     1] loss: 0.519
[12,     1] loss: 0.486
[13,     1] loss: 0.465
[14,     1] loss: 0.447
[15,     1] loss: 0.428
[16,     1] loss: 0.405
[17,     1] loss: 0.402
[18,     1] loss: 0.400
[19,     1] loss: 0.389
[20,     1] loss: 0.373
[21,     1] loss: 0.396
[22,     1] loss: 0.349
[23,     1] loss: 0.347
[24,     1] loss: 0.337
[25,     1] loss: 0.276
[26,     1] loss: 0.322
[27,     1] loss: 0.321
[28,     1] loss: 0.276
[29,     1] loss: 0.289
[30,     1] loss: 0.316
[31,     1] loss: 0.289
[32,     1] loss: 0.316
[33,     1] loss: 0.261
[34,     1] loss: 0.243
[35,     1] loss: 0.260
[36,     1] loss: 0.243
[37,     1] loss: 0.275
[38,     1] loss: 0.249
[39,     1] loss: 0.229
[40,     1] loss: 0.241
[41,     1] loss: 0.230
[42,     1] loss: 0.218
[43,     1] loss: 0.216
[44,     1] loss: 0.244
[45,     1] loss: 0.219
[46,     1] loss: 0.234
[47,     1] loss: 0.240
[48,     1] loss: 0.204
[49,     1] loss: 0.217
[50,     1] loss: 0.221
[51,     1] loss: 0.195
[52,     1] loss: 0.193
[53,     1] loss: 0.211
[54,     1] loss: 0.219
[55,     1] loss: 0.178
[56,     1] loss: 0.200
[57,     1] loss: 0.198
[58,     1] loss: 0.237
[59,     1] loss: 0.169
[60,     1] loss: 0.253
[61,     1] loss: 0.243
[62,     1] loss: 0.273
[63,     1] loss: 0.266
[64,     1] loss: 0.201
[65,     1] loss: 0.240
[66,     1] loss: 0.223
[67,     1] loss: 0.208
[68,     1] loss: 0.244
[69,     1] loss: 0.225
[70,     1] loss: 0.240
[71,     1] loss: 0.215
[72,     1] loss: 0.226
[73,     1] loss: 0.202
[74,     1] loss: 0.206
[75,     1] loss: 0.188
[76,     1] loss: 0.181
[77,     1] loss: 0.217
[78,     1] loss: 0.195
[79,     1] loss: 0.208
[80,     1] loss: 0.232
[81,     1] loss: 0.179
[82,     1] loss: 0.265
[83,     1] loss: 0.239
[84,     1] loss: 0.299
[85,     1] loss: 0.212
[86,     1] loss: 0.306
[87,     1] loss: 0.201
[88,     1] loss: 0.234
[89,     1] loss: 0.238
[90,     1] loss: 0.255
[91,     1] loss: 0.207
[92,     1] loss: 0.193
[93,     1] loss: 0.185
[94,     1] loss: 0.216
[95,     1] loss: 0.220
[96,     1] loss: 0.199
[97,     1] loss: 0.197
[98,     1] loss: 0.240
[99,     1] loss: 0.166
[100,     1] loss: 0.194
[101,     1] loss: 0.197
[102,     1] loss: 0.265
[103,     1] loss: 0.280
[104,     1] loss: 0.235
[105,     1] loss: 0.188
[106,     1] loss: 0.264
[107,     1] loss: 0.264
[108,     1] loss: 0.268
[109,     1] loss: 0.261
[110,     1] loss: 0.228
[111,     1] loss: 0.292
[112,     1] loss: 0.271
Early stopping applied (best metric=0.38511019945144653)
Finished Training
Total time taken: 12.247321605682373
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.681
[4,     1] loss: 0.667
[5,     1] loss: 0.644
[6,     1] loss: 0.616
[7,     1] loss: 0.604
[8,     1] loss: 0.580
[9,     1] loss: 0.543
[10,     1] loss: 0.517
[11,     1] loss: 0.501
[12,     1] loss: 0.459
[13,     1] loss: 0.446
[14,     1] loss: 0.416
[15,     1] loss: 0.438
[16,     1] loss: 0.427
[17,     1] loss: 0.363
[18,     1] loss: 0.349
[19,     1] loss: 0.447
[20,     1] loss: 0.346
[21,     1] loss: 0.358
[22,     1] loss: 0.396
[23,     1] loss: 0.374
[24,     1] loss: 0.338
[25,     1] loss: 0.318
[26,     1] loss: 0.359
[27,     1] loss: 0.378
[28,     1] loss: 0.337
[29,     1] loss: 0.327
[30,     1] loss: 0.338
[31,     1] loss: 0.319
[32,     1] loss: 0.335
[33,     1] loss: 0.308
[34,     1] loss: 0.300
[35,     1] loss: 0.349
[36,     1] loss: 0.313
[37,     1] loss: 0.325
[38,     1] loss: 0.326
[39,     1] loss: 0.324
[40,     1] loss: 0.338
[41,     1] loss: 0.329
[42,     1] loss: 0.287
[43,     1] loss: 0.274
[44,     1] loss: 0.263
[45,     1] loss: 0.294
[46,     1] loss: 0.258
[47,     1] loss: 0.254
[48,     1] loss: 0.243
[49,     1] loss: 0.196
[50,     1] loss: 0.235
[51,     1] loss: 0.230
[52,     1] loss: 0.227
[53,     1] loss: 0.254
[54,     1] loss: 0.229
[55,     1] loss: 0.230
[56,     1] loss: 0.230
[57,     1] loss: 0.216
[58,     1] loss: 0.331
[59,     1] loss: 0.251
[60,     1] loss: 0.219
[61,     1] loss: 0.246
[62,     1] loss: 0.266
[63,     1] loss: 0.298
[64,     1] loss: 0.357
[65,     1] loss: 0.289
[66,     1] loss: 0.241
[67,     1] loss: 0.232
[68,     1] loss: 0.215
[69,     1] loss: 0.212
[70,     1] loss: 0.208
[71,     1] loss: 0.252
[72,     1] loss: 0.217
[73,     1] loss: 0.201
[74,     1] loss: 0.186
[75,     1] loss: 0.208
[76,     1] loss: 0.187
[77,     1] loss: 0.228
[78,     1] loss: 0.197
[79,     1] loss: 0.223
[80,     1] loss: 0.185
[81,     1] loss: 0.231
[82,     1] loss: 0.265
[83,     1] loss: 0.275
[84,     1] loss: 0.212
[85,     1] loss: 0.220
[86,     1] loss: 0.171
[87,     1] loss: 0.216
[88,     1] loss: 0.202
[89,     1] loss: 0.183
[90,     1] loss: 0.174
[91,     1] loss: 0.206
[92,     1] loss: 0.182
[93,     1] loss: 0.176
[94,     1] loss: 0.179
[95,     1] loss: 0.203
[96,     1] loss: 0.161
[97,     1] loss: 0.190
[98,     1] loss: 0.276
[99,     1] loss: 0.238
[100,     1] loss: 0.264
[101,     1] loss: 0.269
[102,     1] loss: 0.209
[103,     1] loss: 0.208
[104,     1] loss: 0.479
[105,     1] loss: 0.277
[106,     1] loss: 0.267
[107,     1] loss: 0.213
[108,     1] loss: 0.244
[109,     1] loss: 0.225
[110,     1] loss: 0.230
[111,     1] loss: 0.209
[112,     1] loss: 0.210
[113,     1] loss: 0.236
[114,     1] loss: 0.208
[115,     1] loss: 0.220
[116,     1] loss: 0.193
[117,     1] loss: 0.195
[118,     1] loss: 0.189
[119,     1] loss: 0.189
[120,     1] loss: 0.184
[121,     1] loss: 0.177
[122,     1] loss: 0.174
[123,     1] loss: 0.187
[124,     1] loss: 0.181
[125,     1] loss: 0.158
[126,     1] loss: 0.172
[127,     1] loss: 0.165
[128,     1] loss: 0.216
[129,     1] loss: 0.253
[130,     1] loss: 0.217
[131,     1] loss: 0.185
[132,     1] loss: 0.158
[133,     1] loss: 0.207
Early stopping applied (best metric=0.3278961777687073)
Finished Training
Total time taken: 14.66664433479309
{'Hydroxylation-K Validation Accuracy': 0.8563652482269504, 'Hydroxylation-K Validation Sensitivity': 0.8342222222222222, 'Hydroxylation-K Validation Specificity': 0.8621052631578947, 'Hydroxylation-K Validation Precision': 0.6074424105306458, 'Hydroxylation-K AUC ROC': 0.8617309941520468, 'Hydroxylation-K AUC PR': 0.6555176168593763, 'Hydroxylation-K MCC': 0.6249709998325182, 'Hydroxylation-K F1': 0.6992782603446219, 'Validation Loss (Hydroxylation-K)': 0.3371957790851593, 'Validation Loss (total)': 0.3371957790851593, 'TimeToTrain': 12.754281435012818}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002951054991485929,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1707833531,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.298521887693434}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.692
[3,     1] loss: 0.696
[4,     1] loss: 0.689
[5,     1] loss: 0.694
[6,     1] loss: 0.686
[7,     1] loss: 0.683
[8,     1] loss: 0.682
[9,     1] loss: 0.678
[10,     1] loss: 0.670
[11,     1] loss: 0.667
[12,     1] loss: 0.663
[13,     1] loss: 0.656
[14,     1] loss: 0.658
[15,     1] loss: 0.655
[16,     1] loss: 0.640
[17,     1] loss: 0.643
[18,     1] loss: 0.632
[19,     1] loss: 0.627
[20,     1] loss: 0.620
[21,     1] loss: 0.618
[22,     1] loss: 0.604
[23,     1] loss: 0.608
[24,     1] loss: 0.601
[25,     1] loss: 0.598
[26,     1] loss: 0.597
[27,     1] loss: 0.598
[28,     1] loss: 0.579
[29,     1] loss: 0.584
[30,     1] loss: 0.577
[31,     1] loss: 0.562
[32,     1] loss: 0.545
[33,     1] loss: 0.555
[34,     1] loss: 0.546
[35,     1] loss: 0.537
[36,     1] loss: 0.528
[37,     1] loss: 0.535
[38,     1] loss: 0.511
[39,     1] loss: 0.497
[40,     1] loss: 0.504
[41,     1] loss: 0.507
[42,     1] loss: 0.504
[43,     1] loss: 0.489
[44,     1] loss: 0.494
[45,     1] loss: 0.463
[46,     1] loss: 0.459
[47,     1] loss: 0.452
[48,     1] loss: 0.435
[49,     1] loss: 0.440
[50,     1] loss: 0.430
[51,     1] loss: 0.421
[52,     1] loss: 0.425
[53,     1] loss: 0.403
[54,     1] loss: 0.435
[55,     1] loss: 0.406
[56,     1] loss: 0.437
[57,     1] loss: 0.401
[58,     1] loss: 0.394
[59,     1] loss: 0.370
[60,     1] loss: 0.369
[61,     1] loss: 0.375
[62,     1] loss: 0.363
[63,     1] loss: 0.380
[64,     1] loss: 0.367
[65,     1] loss: 0.384
[66,     1] loss: 0.370
[67,     1] loss: 0.360
[68,     1] loss: 0.323
[69,     1] loss: 0.341
[70,     1] loss: 0.361
[71,     1] loss: 0.365
[72,     1] loss: 0.352
[73,     1] loss: 0.332
[74,     1] loss: 0.319
[75,     1] loss: 0.335
[76,     1] loss: 0.300
[77,     1] loss: 0.304
[78,     1] loss: 0.321
[79,     1] loss: 0.276
[80,     1] loss: 0.314
[81,     1] loss: 0.316
[82,     1] loss: 0.295
[83,     1] loss: 0.281
[84,     1] loss: 0.252
[85,     1] loss: 0.250
[86,     1] loss: 0.263
[87,     1] loss: 0.311
[88,     1] loss: 0.298
[89,     1] loss: 0.294
[90,     1] loss: 0.280
[91,     1] loss: 0.241
[92,     1] loss: 0.230
[93,     1] loss: 0.270
[94,     1] loss: 0.283
[95,     1] loss: 0.225
[96,     1] loss: 0.274
[97,     1] loss: 0.209
[98,     1] loss: 0.198
[99,     1] loss: 0.246
[100,     1] loss: 0.230
[101,     1] loss: 0.257
[102,     1] loss: 0.270
[103,     1] loss: 0.226
[104,     1] loss: 0.281
[105,     1] loss: 0.222
[106,     1] loss: 0.294
[107,     1] loss: 0.215
[108,     1] loss: 0.186
[109,     1] loss: 0.264
[110,     1] loss: 0.216
[111,     1] loss: 0.207
[112,     1] loss: 0.221
[113,     1] loss: 0.260
[114,     1] loss: 0.274
[115,     1] loss: 0.193
[116,     1] loss: 0.227
[117,     1] loss: 0.231
[118,     1] loss: 0.200
[119,     1] loss: 0.197
[120,     1] loss: 0.259
[121,     1] loss: 0.144
[122,     1] loss: 0.162
[123,     1] loss: 0.204
[124,     1] loss: 0.184
[125,     1] loss: 0.148
[126,     1] loss: 0.188
[127,     1] loss: 0.201
[128,     1] loss: 0.223
[129,     1] loss: 0.148
[130,     1] loss: 0.220
[131,     1] loss: 0.182
[132,     1] loss: 0.163
[133,     1] loss: 0.183
[134,     1] loss: 0.189
[135,     1] loss: 0.169
[136,     1] loss: 0.140
[137,     1] loss: 0.147
[138,     1] loss: 0.168
[139,     1] loss: 0.134
[140,     1] loss: 0.142
[141,     1] loss: 0.204
[142,     1] loss: 0.137
[143,     1] loss: 0.133
[144,     1] loss: 0.134
[145,     1] loss: 0.170
[146,     1] loss: 0.141
[147,     1] loss: 0.145
[148,     1] loss: 0.141
[149,     1] loss: 0.135
[150,     1] loss: 0.151
[151,     1] loss: 0.162
[152,     1] loss: 0.114
[153,     1] loss: 0.136
Early stopping applied (best metric=0.30956384539604187)
Finished Training
Total time taken: 16.7767174243927
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.691
[3,     1] loss: 0.693
[4,     1] loss: 0.692
[5,     1] loss: 0.690
[6,     1] loss: 0.693
[7,     1] loss: 0.692
[8,     1] loss: 0.690
[9,     1] loss: 0.684
[10,     1] loss: 0.683
[11,     1] loss: 0.686
[12,     1] loss: 0.673
[13,     1] loss: 0.670
[14,     1] loss: 0.667
[15,     1] loss: 0.662
[16,     1] loss: 0.660
[17,     1] loss: 0.650
[18,     1] loss: 0.655
[19,     1] loss: 0.633
[20,     1] loss: 0.638
[21,     1] loss: 0.634
[22,     1] loss: 0.621
[23,     1] loss: 0.612
[24,     1] loss: 0.620
[25,     1] loss: 0.616
[26,     1] loss: 0.590
[27,     1] loss: 0.593
[28,     1] loss: 0.589
[29,     1] loss: 0.558
[30,     1] loss: 0.572
[31,     1] loss: 0.572
[32,     1] loss: 0.580
[33,     1] loss: 0.551
[34,     1] loss: 0.547
[35,     1] loss: 0.536
[36,     1] loss: 0.539
[37,     1] loss: 0.514
[38,     1] loss: 0.531
[39,     1] loss: 0.495
[40,     1] loss: 0.492
[41,     1] loss: 0.504
[42,     1] loss: 0.496
[43,     1] loss: 0.477
[44,     1] loss: 0.525
[45,     1] loss: 0.478
[46,     1] loss: 0.493
[47,     1] loss: 0.460
[48,     1] loss: 0.466
[49,     1] loss: 0.464
[50,     1] loss: 0.424
[51,     1] loss: 0.463
[52,     1] loss: 0.438
[53,     1] loss: 0.426
[54,     1] loss: 0.439
[55,     1] loss: 0.470
[56,     1] loss: 0.423
[57,     1] loss: 0.407
[58,     1] loss: 0.436
[59,     1] loss: 0.380
[60,     1] loss: 0.413
[61,     1] loss: 0.392
[62,     1] loss: 0.415
[63,     1] loss: 0.347
[64,     1] loss: 0.352
[65,     1] loss: 0.385
[66,     1] loss: 0.408
[67,     1] loss: 0.353
[68,     1] loss: 0.358
[69,     1] loss: 0.370
[70,     1] loss: 0.358
[71,     1] loss: 0.361
[72,     1] loss: 0.353
[73,     1] loss: 0.340
[74,     1] loss: 0.337
[75,     1] loss: 0.301
[76,     1] loss: 0.322
[77,     1] loss: 0.315
[78,     1] loss: 0.292
[79,     1] loss: 0.314
[80,     1] loss: 0.282
[81,     1] loss: 0.309
[82,     1] loss: 0.310
[83,     1] loss: 0.257
[84,     1] loss: 0.298
[85,     1] loss: 0.311
[86,     1] loss: 0.269
[87,     1] loss: 0.284
[88,     1] loss: 0.242
[89,     1] loss: 0.258
[90,     1] loss: 0.251
[91,     1] loss: 0.268
[92,     1] loss: 0.245
[93,     1] loss: 0.187
[94,     1] loss: 0.251
[95,     1] loss: 0.264
[96,     1] loss: 0.252
[97,     1] loss: 0.206
[98,     1] loss: 0.266
[99,     1] loss: 0.238
[100,     1] loss: 0.188
[101,     1] loss: 0.216
[102,     1] loss: 0.195
[103,     1] loss: 0.210
[104,     1] loss: 0.165
[105,     1] loss: 0.206
[106,     1] loss: 0.220
[107,     1] loss: 0.237
Early stopping applied (best metric=0.2833298444747925)
Finished Training
Total time taken: 11.695886135101318
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.692
[4,     1] loss: 0.690
[5,     1] loss: 0.690
[6,     1] loss: 0.684
[7,     1] loss: 0.685
[8,     1] loss: 0.681
[9,     1] loss: 0.679
[10,     1] loss: 0.670
[11,     1] loss: 0.662
[12,     1] loss: 0.664
[13,     1] loss: 0.653
[14,     1] loss: 0.641
[15,     1] loss: 0.645
[16,     1] loss: 0.629
[17,     1] loss: 0.629
[18,     1] loss: 0.625
[19,     1] loss: 0.615
[20,     1] loss: 0.601
[21,     1] loss: 0.612
[22,     1] loss: 0.606
[23,     1] loss: 0.591
[24,     1] loss: 0.581
[25,     1] loss: 0.567
[26,     1] loss: 0.573
[27,     1] loss: 0.564
[28,     1] loss: 0.578
[29,     1] loss: 0.565
[30,     1] loss: 0.539
[31,     1] loss: 0.530
[32,     1] loss: 0.528
[33,     1] loss: 0.519
[34,     1] loss: 0.489
[35,     1] loss: 0.520
[36,     1] loss: 0.512
[37,     1] loss: 0.522
[38,     1] loss: 0.498
[39,     1] loss: 0.470
[40,     1] loss: 0.495
[41,     1] loss: 0.478
[42,     1] loss: 0.474
[43,     1] loss: 0.489
[44,     1] loss: 0.476
[45,     1] loss: 0.453
[46,     1] loss: 0.465
[47,     1] loss: 0.421
[48,     1] loss: 0.460
[49,     1] loss: 0.416
[50,     1] loss: 0.436
[51,     1] loss: 0.429
[52,     1] loss: 0.443
[53,     1] loss: 0.443
[54,     1] loss: 0.428
[55,     1] loss: 0.381
[56,     1] loss: 0.387
[57,     1] loss: 0.428
[58,     1] loss: 0.392
[59,     1] loss: 0.411
[60,     1] loss: 0.365
[61,     1] loss: 0.356
[62,     1] loss: 0.386
[63,     1] loss: 0.385
[64,     1] loss: 0.379
[65,     1] loss: 0.349
[66,     1] loss: 0.378
[67,     1] loss: 0.344
[68,     1] loss: 0.330
[69,     1] loss: 0.326
[70,     1] loss: 0.360
[71,     1] loss: 0.281
[72,     1] loss: 0.341
[73,     1] loss: 0.302
[74,     1] loss: 0.321
[75,     1] loss: 0.312
[76,     1] loss: 0.354
[77,     1] loss: 0.328
[78,     1] loss: 0.295
[79,     1] loss: 0.345
[80,     1] loss: 0.309
[81,     1] loss: 0.325
[82,     1] loss: 0.301
[83,     1] loss: 0.289
[84,     1] loss: 0.284
[85,     1] loss: 0.296
[86,     1] loss: 0.265
[87,     1] loss: 0.306
[88,     1] loss: 0.278
[89,     1] loss: 0.281
[90,     1] loss: 0.247
[91,     1] loss: 0.237
[92,     1] loss: 0.231
[93,     1] loss: 0.272
[94,     1] loss: 0.215
[95,     1] loss: 0.260
[96,     1] loss: 0.245
[97,     1] loss: 0.261
[98,     1] loss: 0.267
[99,     1] loss: 0.267
[100,     1] loss: 0.240
[101,     1] loss: 0.223
[102,     1] loss: 0.273
[103,     1] loss: 0.253
[104,     1] loss: 0.283
[105,     1] loss: 0.234
[106,     1] loss: 0.278
[107,     1] loss: 0.281
[108,     1] loss: 0.261
[109,     1] loss: 0.221
[110,     1] loss: 0.235
[111,     1] loss: 0.260
[112,     1] loss: 0.269
[113,     1] loss: 0.210
[114,     1] loss: 0.238
[115,     1] loss: 0.207
[116,     1] loss: 0.196
[117,     1] loss: 0.169
[118,     1] loss: 0.190
[119,     1] loss: 0.189
[120,     1] loss: 0.151
[121,     1] loss: 0.157
[122,     1] loss: 0.204
[123,     1] loss: 0.200
[124,     1] loss: 0.178
[125,     1] loss: 0.202
[126,     1] loss: 0.213
[127,     1] loss: 0.200
Early stopping applied (best metric=0.45162710547447205)
Finished Training
Total time taken: 13.998576641082764
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.695
[3,     1] loss: 0.693
[4,     1] loss: 0.688
[5,     1] loss: 0.685
[6,     1] loss: 0.684
[7,     1] loss: 0.675
[8,     1] loss: 0.672
[9,     1] loss: 0.665
[10,     1] loss: 0.660
[11,     1] loss: 0.654
[12,     1] loss: 0.636
[13,     1] loss: 0.650
[14,     1] loss: 0.627
[15,     1] loss: 0.627
[16,     1] loss: 0.625
[17,     1] loss: 0.600
[18,     1] loss: 0.591
[19,     1] loss: 0.584
[20,     1] loss: 0.581
[21,     1] loss: 0.581
[22,     1] loss: 0.558
[23,     1] loss: 0.566
[24,     1] loss: 0.541
[25,     1] loss: 0.542
[26,     1] loss: 0.535
[27,     1] loss: 0.535
[28,     1] loss: 0.527
[29,     1] loss: 0.525
[30,     1] loss: 0.533
[31,     1] loss: 0.491
[32,     1] loss: 0.485
[33,     1] loss: 0.480
[34,     1] loss: 0.496
[35,     1] loss: 0.478
[36,     1] loss: 0.476
[37,     1] loss: 0.438
[38,     1] loss: 0.453
[39,     1] loss: 0.444
[40,     1] loss: 0.443
[41,     1] loss: 0.443
[42,     1] loss: 0.408
[43,     1] loss: 0.411
[44,     1] loss: 0.425
[45,     1] loss: 0.464
[46,     1] loss: 0.414
[47,     1] loss: 0.415
[48,     1] loss: 0.404
[49,     1] loss: 0.398
[50,     1] loss: 0.363
[51,     1] loss: 0.382
[52,     1] loss: 0.361
[53,     1] loss: 0.383
[54,     1] loss: 0.365
[55,     1] loss: 0.358
[56,     1] loss: 0.327
[57,     1] loss: 0.356
[58,     1] loss: 0.369
[59,     1] loss: 0.345
[60,     1] loss: 0.324
[61,     1] loss: 0.327
[62,     1] loss: 0.271
[63,     1] loss: 0.347
[64,     1] loss: 0.286
[65,     1] loss: 0.310
[66,     1] loss: 0.289
[67,     1] loss: 0.284
[68,     1] loss: 0.273
[69,     1] loss: 0.281
[70,     1] loss: 0.318
Early stopping applied (best metric=0.5132887363433838)
Finished Training
Total time taken: 7.714042663574219
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.694
[3,     1] loss: 0.694
[4,     1] loss: 0.686
[5,     1] loss: 0.688
[6,     1] loss: 0.687
[7,     1] loss: 0.692
[8,     1] loss: 0.686
[9,     1] loss: 0.681
[10,     1] loss: 0.682
[11,     1] loss: 0.676
[12,     1] loss: 0.673
[13,     1] loss: 0.665
[14,     1] loss: 0.655
[15,     1] loss: 0.654
[16,     1] loss: 0.660
[17,     1] loss: 0.638
[18,     1] loss: 0.633
[19,     1] loss: 0.635
[20,     1] loss: 0.621
[21,     1] loss: 0.621
[22,     1] loss: 0.603
[23,     1] loss: 0.605
[24,     1] loss: 0.604
[25,     1] loss: 0.577
[26,     1] loss: 0.585
[27,     1] loss: 0.575
[28,     1] loss: 0.547
[29,     1] loss: 0.574
[30,     1] loss: 0.575
[31,     1] loss: 0.557
[32,     1] loss: 0.533
[33,     1] loss: 0.523
[34,     1] loss: 0.516
[35,     1] loss: 0.507
[36,     1] loss: 0.502
[37,     1] loss: 0.516
[38,     1] loss: 0.501
[39,     1] loss: 0.486
[40,     1] loss: 0.473
[41,     1] loss: 0.474
[42,     1] loss: 0.463
[43,     1] loss: 0.458
[44,     1] loss: 0.466
[45,     1] loss: 0.474
[46,     1] loss: 0.431
[47,     1] loss: 0.459
[48,     1] loss: 0.394
[49,     1] loss: 0.418
[50,     1] loss: 0.423
[51,     1] loss: 0.409
[52,     1] loss: 0.414
[53,     1] loss: 0.384
[54,     1] loss: 0.395
[55,     1] loss: 0.354
[56,     1] loss: 0.369
[57,     1] loss: 0.364
[58,     1] loss: 0.354
[59,     1] loss: 0.373
[60,     1] loss: 0.361
[61,     1] loss: 0.377
[62,     1] loss: 0.335
[63,     1] loss: 0.345
[64,     1] loss: 0.356
[65,     1] loss: 0.339
[66,     1] loss: 0.321
[67,     1] loss: 0.311
[68,     1] loss: 0.311
[69,     1] loss: 0.279
[70,     1] loss: 0.293
[71,     1] loss: 0.289
[72,     1] loss: 0.299
[73,     1] loss: 0.303
[74,     1] loss: 0.282
[75,     1] loss: 0.308
[76,     1] loss: 0.267
[77,     1] loss: 0.268
[78,     1] loss: 0.289
[79,     1] loss: 0.253
[80,     1] loss: 0.265
[81,     1] loss: 0.273
[82,     1] loss: 0.266
[83,     1] loss: 0.258
[84,     1] loss: 0.269
[85,     1] loss: 0.203
[86,     1] loss: 0.218
[87,     1] loss: 0.254
[88,     1] loss: 0.181
[89,     1] loss: 0.262
[90,     1] loss: 0.259
[91,     1] loss: 0.249
[92,     1] loss: 0.177
[93,     1] loss: 0.246
[94,     1] loss: 0.232
[95,     1] loss: 0.195
[96,     1] loss: 0.217
[97,     1] loss: 0.207
[98,     1] loss: 0.201
[99,     1] loss: 0.178
[100,     1] loss: 0.208
[101,     1] loss: 0.230
[102,     1] loss: 0.164
[103,     1] loss: 0.182
[104,     1] loss: 0.156
[105,     1] loss: 0.177
[106,     1] loss: 0.158
[107,     1] loss: 0.168
[108,     1] loss: 0.185
[109,     1] loss: 0.194
[110,     1] loss: 0.191
[111,     1] loss: 0.126
[112,     1] loss: 0.170
[113,     1] loss: 0.154
[114,     1] loss: 0.140
[115,     1] loss: 0.157
[116,     1] loss: 0.161
[117,     1] loss: 0.107
[118,     1] loss: 0.156
[119,     1] loss: 0.149
[120,     1] loss: 0.133
[121,     1] loss: 0.148
[122,     1] loss: 0.110
[123,     1] loss: 0.120
[124,     1] loss: 0.131
[125,     1] loss: 0.122
[126,     1] loss: 0.119
[127,     1] loss: 0.118
[128,     1] loss: 0.105
[129,     1] loss: 0.104
[130,     1] loss: 0.103
[131,     1] loss: 0.114
[132,     1] loss: 0.080
[133,     1] loss: 0.111
[134,     1] loss: 0.095
[135,     1] loss: 0.091
[136,     1] loss: 0.093
[137,     1] loss: 0.090
[138,     1] loss: 0.106
[139,     1] loss: 0.092
[140,     1] loss: 0.092
[141,     1] loss: 0.085
[142,     1] loss: 0.087
[143,     1] loss: 0.095
[144,     1] loss: 0.091
[145,     1] loss: 0.104
[146,     1] loss: 0.063
[147,     1] loss: 0.095
[148,     1] loss: 0.088
[149,     1] loss: 0.076
[150,     1] loss: 0.086
[151,     1] loss: 0.120
[152,     1] loss: 0.110
[153,     1] loss: 0.083
Early stopping applied (best metric=0.2909733057022095)
Finished Training
Total time taken: 16.912073373794556
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.697
[3,     1] loss: 0.692
[4,     1] loss: 0.694
[5,     1] loss: 0.687
[6,     1] loss: 0.686
[7,     1] loss: 0.685
[8,     1] loss: 0.678
[9,     1] loss: 0.682
[10,     1] loss: 0.668
[11,     1] loss: 0.663
[12,     1] loss: 0.656
[13,     1] loss: 0.648
[14,     1] loss: 0.644
[15,     1] loss: 0.640
[16,     1] loss: 0.624
[17,     1] loss: 0.622
[18,     1] loss: 0.625
[19,     1] loss: 0.615
[20,     1] loss: 0.615
[21,     1] loss: 0.589
[22,     1] loss: 0.589
[23,     1] loss: 0.601
[24,     1] loss: 0.580
[25,     1] loss: 0.579
[26,     1] loss: 0.566
[27,     1] loss: 0.546
[28,     1] loss: 0.550
[29,     1] loss: 0.554
[30,     1] loss: 0.531
[31,     1] loss: 0.545
[32,     1] loss: 0.516
[33,     1] loss: 0.530
[34,     1] loss: 0.521
[35,     1] loss: 0.511
[36,     1] loss: 0.476
[37,     1] loss: 0.480
[38,     1] loss: 0.508
[39,     1] loss: 0.470
[40,     1] loss: 0.447
[41,     1] loss: 0.469
[42,     1] loss: 0.440
[43,     1] loss: 0.458
[44,     1] loss: 0.432
[45,     1] loss: 0.449
[46,     1] loss: 0.419
[47,     1] loss: 0.423
[48,     1] loss: 0.402
[49,     1] loss: 0.412
[50,     1] loss: 0.402
[51,     1] loss: 0.386
[52,     1] loss: 0.411
[53,     1] loss: 0.402
[54,     1] loss: 0.355
[55,     1] loss: 0.382
[56,     1] loss: 0.359
[57,     1] loss: 0.357
[58,     1] loss: 0.327
[59,     1] loss: 0.307
[60,     1] loss: 0.332
[61,     1] loss: 0.334
[62,     1] loss: 0.306
[63,     1] loss: 0.340
[64,     1] loss: 0.274
[65,     1] loss: 0.318
[66,     1] loss: 0.318
[67,     1] loss: 0.320
[68,     1] loss: 0.291
[69,     1] loss: 0.297
[70,     1] loss: 0.314
[71,     1] loss: 0.305
[72,     1] loss: 0.279
[73,     1] loss: 0.271
[74,     1] loss: 0.268
[75,     1] loss: 0.300
[76,     1] loss: 0.295
[77,     1] loss: 0.304
[78,     1] loss: 0.244
[79,     1] loss: 0.240
[80,     1] loss: 0.251
[81,     1] loss: 0.235
[82,     1] loss: 0.271
[83,     1] loss: 0.216
[84,     1] loss: 0.271
[85,     1] loss: 0.253
[86,     1] loss: 0.203
[87,     1] loss: 0.276
[88,     1] loss: 0.225
[89,     1] loss: 0.189
[90,     1] loss: 0.241
[91,     1] loss: 0.248
[92,     1] loss: 0.219
[93,     1] loss: 0.218
[94,     1] loss: 0.248
[95,     1] loss: 0.269
[96,     1] loss: 0.241
[97,     1] loss: 0.186
[98,     1] loss: 0.229
[99,     1] loss: 0.273
[100,     1] loss: 0.242
Early stopping applied (best metric=0.436917781829834)
Finished Training
Total time taken: 10.997594833374023
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.691
[3,     1] loss: 0.690
[4,     1] loss: 0.684
[5,     1] loss: 0.691
[6,     1] loss: 0.681
[7,     1] loss: 0.676
[8,     1] loss: 0.683
[9,     1] loss: 0.671
[10,     1] loss: 0.675
[11,     1] loss: 0.667
[12,     1] loss: 0.660
[13,     1] loss: 0.643
[14,     1] loss: 0.635
[15,     1] loss: 0.636
[16,     1] loss: 0.619
[17,     1] loss: 0.631
[18,     1] loss: 0.604
[19,     1] loss: 0.610
[20,     1] loss: 0.605
[21,     1] loss: 0.596
[22,     1] loss: 0.578
[23,     1] loss: 0.592
[24,     1] loss: 0.571
[25,     1] loss: 0.576
[26,     1] loss: 0.586
[27,     1] loss: 0.563
[28,     1] loss: 0.546
[29,     1] loss: 0.540
[30,     1] loss: 0.561
[31,     1] loss: 0.554
[32,     1] loss: 0.549
[33,     1] loss: 0.514
[34,     1] loss: 0.546
[35,     1] loss: 0.492
[36,     1] loss: 0.526
[37,     1] loss: 0.509
[38,     1] loss: 0.492
[39,     1] loss: 0.482
[40,     1] loss: 0.510
[41,     1] loss: 0.468
[42,     1] loss: 0.475
[43,     1] loss: 0.472
[44,     1] loss: 0.489
[45,     1] loss: 0.470
[46,     1] loss: 0.442
[47,     1] loss: 0.493
[48,     1] loss: 0.479
[49,     1] loss: 0.436
[50,     1] loss: 0.440
[51,     1] loss: 0.448
[52,     1] loss: 0.437
[53,     1] loss: 0.421
[54,     1] loss: 0.430
[55,     1] loss: 0.415
[56,     1] loss: 0.412
[57,     1] loss: 0.403
[58,     1] loss: 0.381
[59,     1] loss: 0.439
[60,     1] loss: 0.373
[61,     1] loss: 0.372
[62,     1] loss: 0.344
[63,     1] loss: 0.375
[64,     1] loss: 0.337
[65,     1] loss: 0.344
[66,     1] loss: 0.358
[67,     1] loss: 0.329
[68,     1] loss: 0.361
[69,     1] loss: 0.314
[70,     1] loss: 0.310
[71,     1] loss: 0.322
[72,     1] loss: 0.332
[73,     1] loss: 0.389
[74,     1] loss: 0.316
[75,     1] loss: 0.296
[76,     1] loss: 0.304
[77,     1] loss: 0.298
[78,     1] loss: 0.274
[79,     1] loss: 0.283
[80,     1] loss: 0.315
[81,     1] loss: 0.267
[82,     1] loss: 0.273
[83,     1] loss: 0.253
[84,     1] loss: 0.258
[85,     1] loss: 0.261
[86,     1] loss: 0.241
[87,     1] loss: 0.277
[88,     1] loss: 0.255
[89,     1] loss: 0.249
[90,     1] loss: 0.250
[91,     1] loss: 0.241
[92,     1] loss: 0.253
[93,     1] loss: 0.218
[94,     1] loss: 0.282
[95,     1] loss: 0.216
Early stopping applied (best metric=0.46893084049224854)
Finished Training
Total time taken: 10.306374549865723
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.692
[3,     1] loss: 0.692
[4,     1] loss: 0.691
[5,     1] loss: 0.685
[6,     1] loss: 0.687
[7,     1] loss: 0.682
[8,     1] loss: 0.671
[9,     1] loss: 0.676
[10,     1] loss: 0.670
[11,     1] loss: 0.660
[12,     1] loss: 0.653
[13,     1] loss: 0.651
[14,     1] loss: 0.646
[15,     1] loss: 0.633
[16,     1] loss: 0.620
[17,     1] loss: 0.632
[18,     1] loss: 0.625
[19,     1] loss: 0.612
[20,     1] loss: 0.607
[21,     1] loss: 0.577
[22,     1] loss: 0.581
[23,     1] loss: 0.594
[24,     1] loss: 0.560
[25,     1] loss: 0.573
[26,     1] loss: 0.577
[27,     1] loss: 0.562
[28,     1] loss: 0.555
[29,     1] loss: 0.526
[30,     1] loss: 0.555
[31,     1] loss: 0.541
[32,     1] loss: 0.552
[33,     1] loss: 0.558
[34,     1] loss: 0.536
[35,     1] loss: 0.496
[36,     1] loss: 0.504
[37,     1] loss: 0.515
[38,     1] loss: 0.516
[39,     1] loss: 0.492
[40,     1] loss: 0.470
[41,     1] loss: 0.466
[42,     1] loss: 0.517
[43,     1] loss: 0.494
[44,     1] loss: 0.498
[45,     1] loss: 0.467
[46,     1] loss: 0.488
[47,     1] loss: 0.455
[48,     1] loss: 0.465
[49,     1] loss: 0.472
[50,     1] loss: 0.437
[51,     1] loss: 0.450
[52,     1] loss: 0.441
[53,     1] loss: 0.479
[54,     1] loss: 0.438
[55,     1] loss: 0.429
[56,     1] loss: 0.437
[57,     1] loss: 0.403
[58,     1] loss: 0.405
[59,     1] loss: 0.403
[60,     1] loss: 0.416
[61,     1] loss: 0.362
[62,     1] loss: 0.395
[63,     1] loss: 0.372
[64,     1] loss: 0.367
[65,     1] loss: 0.380
[66,     1] loss: 0.360
[67,     1] loss: 0.374
[68,     1] loss: 0.396
[69,     1] loss: 0.354
[70,     1] loss: 0.382
[71,     1] loss: 0.357
[72,     1] loss: 0.336
[73,     1] loss: 0.333
[74,     1] loss: 0.348
[75,     1] loss: 0.353
[76,     1] loss: 0.297
[77,     1] loss: 0.314
[78,     1] loss: 0.351
[79,     1] loss: 0.312
[80,     1] loss: 0.320
[81,     1] loss: 0.337
[82,     1] loss: 0.305
[83,     1] loss: 0.349
[84,     1] loss: 0.310
[85,     1] loss: 0.284
[86,     1] loss: 0.297
[87,     1] loss: 0.283
[88,     1] loss: 0.283
[89,     1] loss: 0.287
[90,     1] loss: 0.270
[91,     1] loss: 0.307
[92,     1] loss: 0.240
[93,     1] loss: 0.280
[94,     1] loss: 0.274
[95,     1] loss: 0.276
[96,     1] loss: 0.263
[97,     1] loss: 0.207
[98,     1] loss: 0.222
[99,     1] loss: 0.260
[100,     1] loss: 0.281
[101,     1] loss: 0.269
[102,     1] loss: 0.255
[103,     1] loss: 0.229
[104,     1] loss: 0.219
[105,     1] loss: 0.212
[106,     1] loss: 0.198
[107,     1] loss: 0.227
[108,     1] loss: 0.214
[109,     1] loss: 0.213
[110,     1] loss: 0.205
[111,     1] loss: 0.193
[112,     1] loss: 0.193
[113,     1] loss: 0.199
[114,     1] loss: 0.200
[115,     1] loss: 0.176
[116,     1] loss: 0.177
[117,     1] loss: 0.180
[118,     1] loss: 0.217
[119,     1] loss: 0.201
[120,     1] loss: 0.181
[121,     1] loss: 0.167
[122,     1] loss: 0.182
[123,     1] loss: 0.161
[124,     1] loss: 0.166
[125,     1] loss: 0.168
[126,     1] loss: 0.134
[127,     1] loss: 0.165
[128,     1] loss: 0.155
[129,     1] loss: 0.136
[130,     1] loss: 0.180
[131,     1] loss: 0.122
[132,     1] loss: 0.126
[133,     1] loss: 0.143
Early stopping applied (best metric=0.39115679264068604)
Finished Training
Total time taken: 14.603870153427124
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.693
[3,     1] loss: 0.686
[4,     1] loss: 0.692
[5,     1] loss: 0.687
[6,     1] loss: 0.697
[7,     1] loss: 0.689
[8,     1] loss: 0.684
[9,     1] loss: 0.670
[10,     1] loss: 0.677
[11,     1] loss: 0.658
[12,     1] loss: 0.669
[13,     1] loss: 0.666
[14,     1] loss: 0.651
[15,     1] loss: 0.649
[16,     1] loss: 0.638
[17,     1] loss: 0.639
[18,     1] loss: 0.621
[19,     1] loss: 0.603
[20,     1] loss: 0.628
[21,     1] loss: 0.604
[22,     1] loss: 0.597
[23,     1] loss: 0.593
[24,     1] loss: 0.595
[25,     1] loss: 0.590
[26,     1] loss: 0.552
[27,     1] loss: 0.556
[28,     1] loss: 0.583
[29,     1] loss: 0.576
[30,     1] loss: 0.561
[31,     1] loss: 0.548
[32,     1] loss: 0.560
[33,     1] loss: 0.577
[34,     1] loss: 0.554
[35,     1] loss: 0.535
[36,     1] loss: 0.538
[37,     1] loss: 0.542
[38,     1] loss: 0.501
[39,     1] loss: 0.513
[40,     1] loss: 0.521
[41,     1] loss: 0.520
[42,     1] loss: 0.525
[43,     1] loss: 0.513
[44,     1] loss: 0.508
[45,     1] loss: 0.473
[46,     1] loss: 0.483
[47,     1] loss: 0.447
[48,     1] loss: 0.473
[49,     1] loss: 0.448
[50,     1] loss: 0.463
[51,     1] loss: 0.459
[52,     1] loss: 0.424
[53,     1] loss: 0.429
[54,     1] loss: 0.410
[55,     1] loss: 0.433
[56,     1] loss: 0.423
[57,     1] loss: 0.396
[58,     1] loss: 0.403
[59,     1] loss: 0.432
[60,     1] loss: 0.394
[61,     1] loss: 0.373
[62,     1] loss: 0.398
[63,     1] loss: 0.345
[64,     1] loss: 0.395
[65,     1] loss: 0.358
[66,     1] loss: 0.380
[67,     1] loss: 0.354
[68,     1] loss: 0.354
[69,     1] loss: 0.367
[70,     1] loss: 0.330
[71,     1] loss: 0.327
[72,     1] loss: 0.324
[73,     1] loss: 0.351
[74,     1] loss: 0.323
[75,     1] loss: 0.333
[76,     1] loss: 0.353
[77,     1] loss: 0.327
[78,     1] loss: 0.301
[79,     1] loss: 0.316
[80,     1] loss: 0.303
[81,     1] loss: 0.297
[82,     1] loss: 0.337
[83,     1] loss: 0.337
[84,     1] loss: 0.371
[85,     1] loss: 0.292
[86,     1] loss: 0.300
[87,     1] loss: 0.301
[88,     1] loss: 0.300
[89,     1] loss: 0.302
[90,     1] loss: 0.291
[91,     1] loss: 0.294
[92,     1] loss: 0.280
[93,     1] loss: 0.282
[94,     1] loss: 0.289
[95,     1] loss: 0.252
[96,     1] loss: 0.269
[97,     1] loss: 0.251
[98,     1] loss: 0.276
Early stopping applied (best metric=0.4086347818374634)
Finished Training
Total time taken: 10.780076742172241
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.710
[2,     1] loss: 0.700
[3,     1] loss: 0.697
[4,     1] loss: 0.692
[5,     1] loss: 0.693
[6,     1] loss: 0.693
[7,     1] loss: 0.685
[8,     1] loss: 0.686
[9,     1] loss: 0.684
[10,     1] loss: 0.684
[11,     1] loss: 0.677
[12,     1] loss: 0.674
[13,     1] loss: 0.666
[14,     1] loss: 0.656
[15,     1] loss: 0.660
[16,     1] loss: 0.652
[17,     1] loss: 0.650
[18,     1] loss: 0.634
[19,     1] loss: 0.627
[20,     1] loss: 0.630
[21,     1] loss: 0.625
[22,     1] loss: 0.599
[23,     1] loss: 0.614
[24,     1] loss: 0.617
[25,     1] loss: 0.587
[26,     1] loss: 0.580
[27,     1] loss: 0.559
[28,     1] loss: 0.556
[29,     1] loss: 0.544
[30,     1] loss: 0.541
[31,     1] loss: 0.544
[32,     1] loss: 0.540
[33,     1] loss: 0.524
[34,     1] loss: 0.548
[35,     1] loss: 0.544
[36,     1] loss: 0.506
[37,     1] loss: 0.472
[38,     1] loss: 0.525
[39,     1] loss: 0.511
[40,     1] loss: 0.503
[41,     1] loss: 0.487
[42,     1] loss: 0.492
[43,     1] loss: 0.481
[44,     1] loss: 0.451
[45,     1] loss: 0.462
[46,     1] loss: 0.445
[47,     1] loss: 0.460
[48,     1] loss: 0.446
[49,     1] loss: 0.426
[50,     1] loss: 0.405
[51,     1] loss: 0.444
[52,     1] loss: 0.432
[53,     1] loss: 0.438
[54,     1] loss: 0.425
[55,     1] loss: 0.389
[56,     1] loss: 0.433
[57,     1] loss: 0.402
[58,     1] loss: 0.362
[59,     1] loss: 0.364
[60,     1] loss: 0.414
[61,     1] loss: 0.359
[62,     1] loss: 0.377
[63,     1] loss: 0.346
[64,     1] loss: 0.336
[65,     1] loss: 0.353
[66,     1] loss: 0.384
[67,     1] loss: 0.328
[68,     1] loss: 0.309
[69,     1] loss: 0.295
[70,     1] loss: 0.309
[71,     1] loss: 0.333
[72,     1] loss: 0.287
[73,     1] loss: 0.303
[74,     1] loss: 0.264
[75,     1] loss: 0.272
[76,     1] loss: 0.265
[77,     1] loss: 0.261
[78,     1] loss: 0.281
[79,     1] loss: 0.270
[80,     1] loss: 0.333
[81,     1] loss: 0.238
[82,     1] loss: 0.241
[83,     1] loss: 0.227
[84,     1] loss: 0.266
[85,     1] loss: 0.287
[86,     1] loss: 0.222
[87,     1] loss: 0.232
[88,     1] loss: 0.211
[89,     1] loss: 0.251
[90,     1] loss: 0.234
[91,     1] loss: 0.301
[92,     1] loss: 0.235
[93,     1] loss: 0.233
[94,     1] loss: 0.193
[95,     1] loss: 0.198
[96,     1] loss: 0.184
[97,     1] loss: 0.278
[98,     1] loss: 0.212
[99,     1] loss: 0.187
[100,     1] loss: 0.211
[101,     1] loss: 0.209
[102,     1] loss: 0.230
[103,     1] loss: 0.214
[104,     1] loss: 0.217
[105,     1] loss: 0.177
[106,     1] loss: 0.205
[107,     1] loss: 0.268
[108,     1] loss: 0.195
[109,     1] loss: 0.153
[110,     1] loss: 0.193
[111,     1] loss: 0.155
[112,     1] loss: 0.222
[113,     1] loss: 0.159
Early stopping applied (best metric=0.294478178024292)
Finished Training
Total time taken: 12.301109313964844
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.695
[3,     1] loss: 0.695
[4,     1] loss: 0.690
[5,     1] loss: 0.688
[6,     1] loss: 0.686
[7,     1] loss: 0.679
[8,     1] loss: 0.684
[9,     1] loss: 0.677
[10,     1] loss: 0.676
[11,     1] loss: 0.669
[12,     1] loss: 0.671
[13,     1] loss: 0.659
[14,     1] loss: 0.656
[15,     1] loss: 0.654
[16,     1] loss: 0.651
[17,     1] loss: 0.637
[18,     1] loss: 0.640
[19,     1] loss: 0.633
[20,     1] loss: 0.626
[21,     1] loss: 0.618
[22,     1] loss: 0.622
[23,     1] loss: 0.599
[24,     1] loss: 0.601
[25,     1] loss: 0.601
[26,     1] loss: 0.579
[27,     1] loss: 0.590
[28,     1] loss: 0.588
[29,     1] loss: 0.573
[30,     1] loss: 0.557
[31,     1] loss: 0.563
[32,     1] loss: 0.531
[33,     1] loss: 0.542
[34,     1] loss: 0.544
[35,     1] loss: 0.539
[36,     1] loss: 0.535
[37,     1] loss: 0.507
[38,     1] loss: 0.501
[39,     1] loss: 0.496
[40,     1] loss: 0.487
[41,     1] loss: 0.470
[42,     1] loss: 0.458
[43,     1] loss: 0.451
[44,     1] loss: 0.473
[45,     1] loss: 0.458
[46,     1] loss: 0.437
[47,     1] loss: 0.432
[48,     1] loss: 0.407
[49,     1] loss: 0.417
[50,     1] loss: 0.398
[51,     1] loss: 0.415
[52,     1] loss: 0.382
[53,     1] loss: 0.417
[54,     1] loss: 0.393
[55,     1] loss: 0.366
[56,     1] loss: 0.396
[57,     1] loss: 0.370
[58,     1] loss: 0.360
[59,     1] loss: 0.367
[60,     1] loss: 0.365
[61,     1] loss: 0.334
[62,     1] loss: 0.381
[63,     1] loss: 0.336
[64,     1] loss: 0.357
[65,     1] loss: 0.365
[66,     1] loss: 0.295
[67,     1] loss: 0.308
[68,     1] loss: 0.282
[69,     1] loss: 0.328
[70,     1] loss: 0.279
[71,     1] loss: 0.269
[72,     1] loss: 0.295
[73,     1] loss: 0.269
[74,     1] loss: 0.257
[75,     1] loss: 0.255
[76,     1] loss: 0.250
[77,     1] loss: 0.231
[78,     1] loss: 0.254
[79,     1] loss: 0.238
[80,     1] loss: 0.217
[81,     1] loss: 0.234
[82,     1] loss: 0.237
[83,     1] loss: 0.212
[84,     1] loss: 0.240
[85,     1] loss: 0.228
[86,     1] loss: 0.195
[87,     1] loss: 0.190
[88,     1] loss: 0.210
[89,     1] loss: 0.220
[90,     1] loss: 0.182
[91,     1] loss: 0.197
[92,     1] loss: 0.171
[93,     1] loss: 0.201
[94,     1] loss: 0.190
[95,     1] loss: 0.206
[96,     1] loss: 0.196
[97,     1] loss: 0.180
[98,     1] loss: 0.167
[99,     1] loss: 0.153
[100,     1] loss: 0.148
[101,     1] loss: 0.145
[102,     1] loss: 0.176
[103,     1] loss: 0.134
[104,     1] loss: 0.140
[105,     1] loss: 0.162
[106,     1] loss: 0.127
[107,     1] loss: 0.117
[108,     1] loss: 0.132
[109,     1] loss: 0.125
Early stopping applied (best metric=0.3210133910179138)
Finished Training
Total time taken: 12.262062311172485
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.695
[4,     1] loss: 0.691
[5,     1] loss: 0.689
[6,     1] loss: 0.692
[7,     1] loss: 0.688
[8,     1] loss: 0.685
[9,     1] loss: 0.678
[10,     1] loss: 0.678
[11,     1] loss: 0.672
[12,     1] loss: 0.675
[13,     1] loss: 0.669
[14,     1] loss: 0.664
[15,     1] loss: 0.660
[16,     1] loss: 0.652
[17,     1] loss: 0.657
[18,     1] loss: 0.648
[19,     1] loss: 0.648
[20,     1] loss: 0.632
[21,     1] loss: 0.641
[22,     1] loss: 0.628
[23,     1] loss: 0.626
[24,     1] loss: 0.612
[25,     1] loss: 0.617
[26,     1] loss: 0.605
[27,     1] loss: 0.613
[28,     1] loss: 0.606
[29,     1] loss: 0.597
[30,     1] loss: 0.589
[31,     1] loss: 0.576
[32,     1] loss: 0.577
[33,     1] loss: 0.573
[34,     1] loss: 0.563
[35,     1] loss: 0.562
[36,     1] loss: 0.549
[37,     1] loss: 0.567
[38,     1] loss: 0.536
[39,     1] loss: 0.552
[40,     1] loss: 0.559
[41,     1] loss: 0.510
[42,     1] loss: 0.557
[43,     1] loss: 0.479
[44,     1] loss: 0.520
[45,     1] loss: 0.501
[46,     1] loss: 0.493
[47,     1] loss: 0.498
[48,     1] loss: 0.500
[49,     1] loss: 0.479
[50,     1] loss: 0.457
[51,     1] loss: 0.496
[52,     1] loss: 0.451
[53,     1] loss: 0.465
[54,     1] loss: 0.428
[55,     1] loss: 0.441
[56,     1] loss: 0.439
[57,     1] loss: 0.463
[58,     1] loss: 0.438
[59,     1] loss: 0.429
[60,     1] loss: 0.465
[61,     1] loss: 0.420
[62,     1] loss: 0.455
[63,     1] loss: 0.430
[64,     1] loss: 0.408
[65,     1] loss: 0.405
[66,     1] loss: 0.384
[67,     1] loss: 0.389
[68,     1] loss: 0.401
[69,     1] loss: 0.392
[70,     1] loss: 0.376
[71,     1] loss: 0.355
[72,     1] loss: 0.352
[73,     1] loss: 0.347
[74,     1] loss: 0.344
[75,     1] loss: 0.348
[76,     1] loss: 0.338
[77,     1] loss: 0.310
[78,     1] loss: 0.357
[79,     1] loss: 0.343
[80,     1] loss: 0.312
[81,     1] loss: 0.295
[82,     1] loss: 0.335
[83,     1] loss: 0.352
[84,     1] loss: 0.279
[85,     1] loss: 0.309
[86,     1] loss: 0.289
[87,     1] loss: 0.282
[88,     1] loss: 0.266
[89,     1] loss: 0.304
[90,     1] loss: 0.311
[91,     1] loss: 0.273
[92,     1] loss: 0.283
[93,     1] loss: 0.284
[94,     1] loss: 0.289
[95,     1] loss: 0.246
[96,     1] loss: 0.239
[97,     1] loss: 0.279
[98,     1] loss: 0.284
[99,     1] loss: 0.259
[100,     1] loss: 0.242
[101,     1] loss: 0.262
[102,     1] loss: 0.271
[103,     1] loss: 0.275
[104,     1] loss: 0.230
[105,     1] loss: 0.229
[106,     1] loss: 0.258
Early stopping applied (best metric=0.44661805033683777)
Finished Training
Total time taken: 12.006584644317627
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.693
[3,     1] loss: 0.690
[4,     1] loss: 0.687
[5,     1] loss: 0.684
[6,     1] loss: 0.682
[7,     1] loss: 0.678
[8,     1] loss: 0.676
[9,     1] loss: 0.670
[10,     1] loss: 0.666
[11,     1] loss: 0.661
[12,     1] loss: 0.657
[13,     1] loss: 0.651
[14,     1] loss: 0.641
[15,     1] loss: 0.629
[16,     1] loss: 0.630
[17,     1] loss: 0.623
[18,     1] loss: 0.613
[19,     1] loss: 0.604
[20,     1] loss: 0.595
[21,     1] loss: 0.593
[22,     1] loss: 0.588
[23,     1] loss: 0.588
[24,     1] loss: 0.573
[25,     1] loss: 0.553
[26,     1] loss: 0.565
[27,     1] loss: 0.583
[28,     1] loss: 0.543
[29,     1] loss: 0.555
[30,     1] loss: 0.534
[31,     1] loss: 0.516
[32,     1] loss: 0.526
[33,     1] loss: 0.535
[34,     1] loss: 0.530
[35,     1] loss: 0.508
[36,     1] loss: 0.473
[37,     1] loss: 0.481
[38,     1] loss: 0.480
[39,     1] loss: 0.462
[40,     1] loss: 0.444
[41,     1] loss: 0.480
[42,     1] loss: 0.460
[43,     1] loss: 0.461
[44,     1] loss: 0.421
[45,     1] loss: 0.440
[46,     1] loss: 0.433
[47,     1] loss: 0.418
[48,     1] loss: 0.426
[49,     1] loss: 0.388
[50,     1] loss: 0.438
[51,     1] loss: 0.399
[52,     1] loss: 0.373
[53,     1] loss: 0.400
[54,     1] loss: 0.362
[55,     1] loss: 0.395
[56,     1] loss: 0.374
[57,     1] loss: 0.392
[58,     1] loss: 0.358
[59,     1] loss: 0.320
[60,     1] loss: 0.335
[61,     1] loss: 0.351
[62,     1] loss: 0.369
[63,     1] loss: 0.312
[64,     1] loss: 0.305
[65,     1] loss: 0.357
[66,     1] loss: 0.350
[67,     1] loss: 0.329
[68,     1] loss: 0.331
[69,     1] loss: 0.322
[70,     1] loss: 0.329
[71,     1] loss: 0.351
[72,     1] loss: 0.290
[73,     1] loss: 0.291
[74,     1] loss: 0.330
[75,     1] loss: 0.319
[76,     1] loss: 0.296
[77,     1] loss: 0.317
[78,     1] loss: 0.282
[79,     1] loss: 0.297
[80,     1] loss: 0.243
[81,     1] loss: 0.287
[82,     1] loss: 0.257
[83,     1] loss: 0.229
[84,     1] loss: 0.266
[85,     1] loss: 0.221
[86,     1] loss: 0.275
[87,     1] loss: 0.234
[88,     1] loss: 0.269
[89,     1] loss: 0.230
[90,     1] loss: 0.237
[91,     1] loss: 0.204
[92,     1] loss: 0.243
[93,     1] loss: 0.199
[94,     1] loss: 0.197
[95,     1] loss: 0.203
[96,     1] loss: 0.194
[97,     1] loss: 0.195
[98,     1] loss: 0.166
[99,     1] loss: 0.162
[100,     1] loss: 0.224
[101,     1] loss: 0.160
[102,     1] loss: 0.164
[103,     1] loss: 0.190
[104,     1] loss: 0.148
[105,     1] loss: 0.146
[106,     1] loss: 0.160
[107,     1] loss: 0.170
[108,     1] loss: 0.148
[109,     1] loss: 0.144
Early stopping applied (best metric=0.3824458122253418)
Finished Training
Total time taken: 11.95266842842102
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.697
[3,     1] loss: 0.693
[4,     1] loss: 0.698
[5,     1] loss: 0.696
[6,     1] loss: 0.686
[7,     1] loss: 0.690
[8,     1] loss: 0.690
[9,     1] loss: 0.689
[10,     1] loss: 0.686
[11,     1] loss: 0.676
[12,     1] loss: 0.668
[13,     1] loss: 0.667
[14,     1] loss: 0.674
[15,     1] loss: 0.661
[16,     1] loss: 0.653
[17,     1] loss: 0.643
[18,     1] loss: 0.639
[19,     1] loss: 0.645
[20,     1] loss: 0.606
[21,     1] loss: 0.628
[22,     1] loss: 0.603
[23,     1] loss: 0.604
[24,     1] loss: 0.601
[25,     1] loss: 0.588
[26,     1] loss: 0.589
[27,     1] loss: 0.568
[28,     1] loss: 0.584
[29,     1] loss: 0.551
[30,     1] loss: 0.555
[31,     1] loss: 0.552
[32,     1] loss: 0.527
[33,     1] loss: 0.526
[34,     1] loss: 0.536
[35,     1] loss: 0.548
[36,     1] loss: 0.540
[37,     1] loss: 0.524
[38,     1] loss: 0.514
[39,     1] loss: 0.516
[40,     1] loss: 0.509
[41,     1] loss: 0.480
[42,     1] loss: 0.493
[43,     1] loss: 0.494
[44,     1] loss: 0.463
[45,     1] loss: 0.473
[46,     1] loss: 0.449
[47,     1] loss: 0.459
[48,     1] loss: 0.472
[49,     1] loss: 0.476
[50,     1] loss: 0.403
[51,     1] loss: 0.406
[52,     1] loss: 0.431
[53,     1] loss: 0.415
[54,     1] loss: 0.445
[55,     1] loss: 0.401
[56,     1] loss: 0.394
[57,     1] loss: 0.370
[58,     1] loss: 0.397
[59,     1] loss: 0.388
[60,     1] loss: 0.399
[61,     1] loss: 0.365
[62,     1] loss: 0.360
[63,     1] loss: 0.334
[64,     1] loss: 0.362
[65,     1] loss: 0.369
[66,     1] loss: 0.347
[67,     1] loss: 0.326
[68,     1] loss: 0.326
[69,     1] loss: 0.326
[70,     1] loss: 0.323
[71,     1] loss: 0.312
[72,     1] loss: 0.304
[73,     1] loss: 0.315
[74,     1] loss: 0.282
[75,     1] loss: 0.274
[76,     1] loss: 0.258
[77,     1] loss: 0.249
[78,     1] loss: 0.269
[79,     1] loss: 0.255
[80,     1] loss: 0.271
[81,     1] loss: 0.244
[82,     1] loss: 0.255
[83,     1] loss: 0.265
[84,     1] loss: 0.232
[85,     1] loss: 0.251
[86,     1] loss: 0.259
[87,     1] loss: 0.250
[88,     1] loss: 0.220
[89,     1] loss: 0.231
[90,     1] loss: 0.191
[91,     1] loss: 0.200
[92,     1] loss: 0.223
[93,     1] loss: 0.215
[94,     1] loss: 0.196
[95,     1] loss: 0.190
[96,     1] loss: 0.220
[97,     1] loss: 0.201
[98,     1] loss: 0.203
[99,     1] loss: 0.198
[100,     1] loss: 0.179
[101,     1] loss: 0.174
[102,     1] loss: 0.186
[103,     1] loss: 0.177
[104,     1] loss: 0.172
[105,     1] loss: 0.149
[106,     1] loss: 0.166
Early stopping applied (best metric=0.3605090379714966)
Finished Training
Total time taken: 11.637092113494873
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.697
[4,     1] loss: 0.685
[5,     1] loss: 0.691
[6,     1] loss: 0.681
[7,     1] loss: 0.686
[8,     1] loss: 0.675
[9,     1] loss: 0.675
[10,     1] loss: 0.669
[11,     1] loss: 0.665
[12,     1] loss: 0.656
[13,     1] loss: 0.645
[14,     1] loss: 0.647
[15,     1] loss: 0.634
[16,     1] loss: 0.622
[17,     1] loss: 0.633
[18,     1] loss: 0.607
[19,     1] loss: 0.600
[20,     1] loss: 0.602
[21,     1] loss: 0.588
[22,     1] loss: 0.580
[23,     1] loss: 0.581
[24,     1] loss: 0.594
[25,     1] loss: 0.568
[26,     1] loss: 0.572
[27,     1] loss: 0.584
[28,     1] loss: 0.572
[29,     1] loss: 0.545
[30,     1] loss: 0.528
[31,     1] loss: 0.543
[32,     1] loss: 0.526
[33,     1] loss: 0.509
[34,     1] loss: 0.509
[35,     1] loss: 0.529
[36,     1] loss: 0.500
[37,     1] loss: 0.495
[38,     1] loss: 0.508
[39,     1] loss: 0.475
[40,     1] loss: 0.503
[41,     1] loss: 0.516
[42,     1] loss: 0.476
[43,     1] loss: 0.465
[44,     1] loss: 0.490
[45,     1] loss: 0.461
[46,     1] loss: 0.448
[47,     1] loss: 0.432
[48,     1] loss: 0.482
[49,     1] loss: 0.434
[50,     1] loss: 0.454
[51,     1] loss: 0.406
[52,     1] loss: 0.432
[53,     1] loss: 0.406
[54,     1] loss: 0.431
[55,     1] loss: 0.373
[56,     1] loss: 0.413
[57,     1] loss: 0.395
[58,     1] loss: 0.398
[59,     1] loss: 0.382
[60,     1] loss: 0.368
[61,     1] loss: 0.380
[62,     1] loss: 0.391
[63,     1] loss: 0.369
[64,     1] loss: 0.332
[65,     1] loss: 0.346
[66,     1] loss: 0.386
[67,     1] loss: 0.345
[68,     1] loss: 0.343
[69,     1] loss: 0.338
[70,     1] loss: 0.311
[71,     1] loss: 0.334
[72,     1] loss: 0.329
[73,     1] loss: 0.331
[74,     1] loss: 0.306
[75,     1] loss: 0.303
[76,     1] loss: 0.302
[77,     1] loss: 0.278
[78,     1] loss: 0.313
[79,     1] loss: 0.286
[80,     1] loss: 0.294
[81,     1] loss: 0.284
[82,     1] loss: 0.251
[83,     1] loss: 0.279
[84,     1] loss: 0.257
[85,     1] loss: 0.260
[86,     1] loss: 0.259
[87,     1] loss: 0.259
[88,     1] loss: 0.259
[89,     1] loss: 0.211
[90,     1] loss: 0.206
[91,     1] loss: 0.297
[92,     1] loss: 0.241
[93,     1] loss: 0.230
[94,     1] loss: 0.205
[95,     1] loss: 0.259
[96,     1] loss: 0.175
[97,     1] loss: 0.228
[98,     1] loss: 0.235
[99,     1] loss: 0.213
[100,     1] loss: 0.220
[101,     1] loss: 0.220
[102,     1] loss: 0.218
[103,     1] loss: 0.213
[104,     1] loss: 0.208
[105,     1] loss: 0.200
[106,     1] loss: 0.202
[107,     1] loss: 0.190
[108,     1] loss: 0.223
[109,     1] loss: 0.170
[110,     1] loss: 0.159
[111,     1] loss: 0.191
[112,     1] loss: 0.189
[113,     1] loss: 0.199
[114,     1] loss: 0.221
[115,     1] loss: 0.164
[116,     1] loss: 0.184
[117,     1] loss: 0.168
[118,     1] loss: 0.138
[119,     1] loss: 0.168
[120,     1] loss: 0.146
[121,     1] loss: 0.153
[122,     1] loss: 0.153
[123,     1] loss: 0.144
[124,     1] loss: 0.186
[125,     1] loss: 0.147
[126,     1] loss: 0.181
[127,     1] loss: 0.116
Early stopping applied (best metric=0.37395215034484863)
Finished Training
Total time taken: 13.821654319763184
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.698
[3,     1] loss: 0.687
[4,     1] loss: 0.696
[5,     1] loss: 0.692
[6,     1] loss: 0.679
[7,     1] loss: 0.689
[8,     1] loss: 0.677
[9,     1] loss: 0.670
[10,     1] loss: 0.681
[11,     1] loss: 0.665
[12,     1] loss: 0.668
[13,     1] loss: 0.661
[14,     1] loss: 0.643
[15,     1] loss: 0.646
[16,     1] loss: 0.635
[17,     1] loss: 0.623
[18,     1] loss: 0.609
[19,     1] loss: 0.616
[20,     1] loss: 0.623
[21,     1] loss: 0.609
[22,     1] loss: 0.617
[23,     1] loss: 0.587
[24,     1] loss: 0.595
[25,     1] loss: 0.583
[26,     1] loss: 0.595
[27,     1] loss: 0.591
[28,     1] loss: 0.583
[29,     1] loss: 0.561
[30,     1] loss: 0.555
[31,     1] loss: 0.538
[32,     1] loss: 0.535
[33,     1] loss: 0.550
[34,     1] loss: 0.523
[35,     1] loss: 0.519
[36,     1] loss: 0.516
[37,     1] loss: 0.543
[38,     1] loss: 0.516
[39,     1] loss: 0.509
[40,     1] loss: 0.501
[41,     1] loss: 0.486
[42,     1] loss: 0.497
[43,     1] loss: 0.457
[44,     1] loss: 0.475
[45,     1] loss: 0.470
[46,     1] loss: 0.440
[47,     1] loss: 0.454
[48,     1] loss: 0.445
[49,     1] loss: 0.448
[50,     1] loss: 0.416
[51,     1] loss: 0.413
[52,     1] loss: 0.460
[53,     1] loss: 0.459
[54,     1] loss: 0.436
[55,     1] loss: 0.427
[56,     1] loss: 0.421
[57,     1] loss: 0.408
[58,     1] loss: 0.367
[59,     1] loss: 0.381
[60,     1] loss: 0.405
[61,     1] loss: 0.374
[62,     1] loss: 0.390
[63,     1] loss: 0.395
[64,     1] loss: 0.345
[65,     1] loss: 0.387
[66,     1] loss: 0.369
[67,     1] loss: 0.338
[68,     1] loss: 0.335
[69,     1] loss: 0.351
[70,     1] loss: 0.331
[71,     1] loss: 0.320
[72,     1] loss: 0.298
[73,     1] loss: 0.308
[74,     1] loss: 0.313
[75,     1] loss: 0.287
[76,     1] loss: 0.255
[77,     1] loss: 0.288
[78,     1] loss: 0.277
[79,     1] loss: 0.268
[80,     1] loss: 0.255
[81,     1] loss: 0.261
[82,     1] loss: 0.261
[83,     1] loss: 0.264
[84,     1] loss: 0.259
[85,     1] loss: 0.222
[86,     1] loss: 0.226
[87,     1] loss: 0.287
[88,     1] loss: 0.243
[89,     1] loss: 0.224
[90,     1] loss: 0.212
[91,     1] loss: 0.201
[92,     1] loss: 0.210
[93,     1] loss: 0.226
[94,     1] loss: 0.201
[95,     1] loss: 0.191
[96,     1] loss: 0.183
[97,     1] loss: 0.193
[98,     1] loss: 0.177
[99,     1] loss: 0.175
[100,     1] loss: 0.194
[101,     1] loss: 0.187
[102,     1] loss: 0.152
[103,     1] loss: 0.160
[104,     1] loss: 0.173
[105,     1] loss: 0.178
Early stopping applied (best metric=0.3566324710845947)
Finished Training
Total time taken: 11.505656719207764
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.695
[4,     1] loss: 0.690
[5,     1] loss: 0.687
[6,     1] loss: 0.687
[7,     1] loss: 0.688
[8,     1] loss: 0.686
[9,     1] loss: 0.680
[10,     1] loss: 0.668
[11,     1] loss: 0.663
[12,     1] loss: 0.669
[13,     1] loss: 0.661
[14,     1] loss: 0.644
[15,     1] loss: 0.652
[16,     1] loss: 0.637
[17,     1] loss: 0.642
[18,     1] loss: 0.628
[19,     1] loss: 0.624
[20,     1] loss: 0.625
[21,     1] loss: 0.603
[22,     1] loss: 0.614
[23,     1] loss: 0.611
[24,     1] loss: 0.613
[25,     1] loss: 0.593
[26,     1] loss: 0.606
[27,     1] loss: 0.590
[28,     1] loss: 0.568
[29,     1] loss: 0.565
[30,     1] loss: 0.567
[31,     1] loss: 0.550
[32,     1] loss: 0.560
[33,     1] loss: 0.548
[34,     1] loss: 0.540
[35,     1] loss: 0.525
[36,     1] loss: 0.541
[37,     1] loss: 0.532
[38,     1] loss: 0.497
[39,     1] loss: 0.510
[40,     1] loss: 0.491
[41,     1] loss: 0.488
[42,     1] loss: 0.501
[43,     1] loss: 0.474
[44,     1] loss: 0.456
[45,     1] loss: 0.465
[46,     1] loss: 0.476
[47,     1] loss: 0.444
[48,     1] loss: 0.443
[49,     1] loss: 0.424
[50,     1] loss: 0.445
[51,     1] loss: 0.441
[52,     1] loss: 0.418
[53,     1] loss: 0.431
[54,     1] loss: 0.391
[55,     1] loss: 0.404
[56,     1] loss: 0.399
[57,     1] loss: 0.370
[58,     1] loss: 0.352
[59,     1] loss: 0.369
[60,     1] loss: 0.341
[61,     1] loss: 0.365
[62,     1] loss: 0.349
[63,     1] loss: 0.340
[64,     1] loss: 0.328
[65,     1] loss: 0.320
[66,     1] loss: 0.338
[67,     1] loss: 0.316
[68,     1] loss: 0.337
[69,     1] loss: 0.285
[70,     1] loss: 0.311
[71,     1] loss: 0.318
[72,     1] loss: 0.278
[73,     1] loss: 0.268
[74,     1] loss: 0.266
[75,     1] loss: 0.281
[76,     1] loss: 0.286
[77,     1] loss: 0.263
[78,     1] loss: 0.266
[79,     1] loss: 0.251
[80,     1] loss: 0.221
[81,     1] loss: 0.214
[82,     1] loss: 0.242
[83,     1] loss: 0.241
[84,     1] loss: 0.226
[85,     1] loss: 0.245
[86,     1] loss: 0.238
[87,     1] loss: 0.233
[88,     1] loss: 0.231
[89,     1] loss: 0.209
[90,     1] loss: 0.196
[91,     1] loss: 0.228
[92,     1] loss: 0.199
[93,     1] loss: 0.188
[94,     1] loss: 0.190
[95,     1] loss: 0.208
[96,     1] loss: 0.214
[97,     1] loss: 0.218
[98,     1] loss: 0.222
[99,     1] loss: 0.209
Early stopping applied (best metric=0.4549255073070526)
Finished Training
Total time taken: 10.786162376403809
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.692
[4,     1] loss: 0.685
[5,     1] loss: 0.680
[6,     1] loss: 0.681
[7,     1] loss: 0.685
[8,     1] loss: 0.680
[9,     1] loss: 0.672
[10,     1] loss: 0.659
[11,     1] loss: 0.664
[12,     1] loss: 0.654
[13,     1] loss: 0.654
[14,     1] loss: 0.653
[15,     1] loss: 0.636
[16,     1] loss: 0.631
[17,     1] loss: 0.633
[18,     1] loss: 0.623
[19,     1] loss: 0.613
[20,     1] loss: 0.599
[21,     1] loss: 0.611
[22,     1] loss: 0.590
[23,     1] loss: 0.593
[24,     1] loss: 0.584
[25,     1] loss: 0.580
[26,     1] loss: 0.573
[27,     1] loss: 0.583
[28,     1] loss: 0.558
[29,     1] loss: 0.569
[30,     1] loss: 0.537
[31,     1] loss: 0.562
[32,     1] loss: 0.519
[33,     1] loss: 0.512
[34,     1] loss: 0.505
[35,     1] loss: 0.524
[36,     1] loss: 0.524
[37,     1] loss: 0.522
[38,     1] loss: 0.504
[39,     1] loss: 0.512
[40,     1] loss: 0.495
[41,     1] loss: 0.501
[42,     1] loss: 0.476
[43,     1] loss: 0.467
[44,     1] loss: 0.485
[45,     1] loss: 0.447
[46,     1] loss: 0.454
[47,     1] loss: 0.455
[48,     1] loss: 0.469
[49,     1] loss: 0.466
[50,     1] loss: 0.409
[51,     1] loss: 0.422
[52,     1] loss: 0.446
[53,     1] loss: 0.401
[54,     1] loss: 0.411
[55,     1] loss: 0.373
[56,     1] loss: 0.379
[57,     1] loss: 0.387
[58,     1] loss: 0.346
[59,     1] loss: 0.357
[60,     1] loss: 0.370
[61,     1] loss: 0.376
[62,     1] loss: 0.378
[63,     1] loss: 0.345
[64,     1] loss: 0.352
[65,     1] loss: 0.361
[66,     1] loss: 0.347
[67,     1] loss: 0.352
[68,     1] loss: 0.318
[69,     1] loss: 0.315
[70,     1] loss: 0.349
[71,     1] loss: 0.342
[72,     1] loss: 0.278
[73,     1] loss: 0.276
[74,     1] loss: 0.339
[75,     1] loss: 0.288
[76,     1] loss: 0.300
[77,     1] loss: 0.304
[78,     1] loss: 0.315
[79,     1] loss: 0.305
[80,     1] loss: 0.273
[81,     1] loss: 0.264
[82,     1] loss: 0.279
[83,     1] loss: 0.265
[84,     1] loss: 0.264
[85,     1] loss: 0.193
[86,     1] loss: 0.291
[87,     1] loss: 0.215
[88,     1] loss: 0.293
[89,     1] loss: 0.260
[90,     1] loss: 0.205
[91,     1] loss: 0.257
[92,     1] loss: 0.266
[93,     1] loss: 0.219
[94,     1] loss: 0.197
[95,     1] loss: 0.196
[96,     1] loss: 0.182
[97,     1] loss: 0.253
[98,     1] loss: 0.210
[99,     1] loss: 0.190
[100,     1] loss: 0.197
[101,     1] loss: 0.190
[102,     1] loss: 0.208
[103,     1] loss: 0.186
[104,     1] loss: 0.163
[105,     1] loss: 0.224
[106,     1] loss: 0.186
[107,     1] loss: 0.176
[108,     1] loss: 0.184
[109,     1] loss: 0.175
[110,     1] loss: 0.144
Early stopping applied (best metric=0.4001908302307129)
Finished Training
Total time taken: 12.055143594741821
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.693
[3,     1] loss: 0.701
[4,     1] loss: 0.697
[5,     1] loss: 0.687
[6,     1] loss: 0.687
[7,     1] loss: 0.688
[8,     1] loss: 0.688
[9,     1] loss: 0.679
[10,     1] loss: 0.682
[11,     1] loss: 0.677
[12,     1] loss: 0.679
[13,     1] loss: 0.652
[14,     1] loss: 0.651
[15,     1] loss: 0.652
[16,     1] loss: 0.641
[17,     1] loss: 0.639
[18,     1] loss: 0.621
[19,     1] loss: 0.631
[20,     1] loss: 0.628
[21,     1] loss: 0.615
[22,     1] loss: 0.597
[23,     1] loss: 0.613
[24,     1] loss: 0.587
[25,     1] loss: 0.598
[26,     1] loss: 0.590
[27,     1] loss: 0.569
[28,     1] loss: 0.564
[29,     1] loss: 0.568
[30,     1] loss: 0.555
[31,     1] loss: 0.564
[32,     1] loss: 0.565
[33,     1] loss: 0.530
[34,     1] loss: 0.545
[35,     1] loss: 0.550
[36,     1] loss: 0.528
[37,     1] loss: 0.537
[38,     1] loss: 0.523
[39,     1] loss: 0.541
[40,     1] loss: 0.504
[41,     1] loss: 0.518
[42,     1] loss: 0.476
[43,     1] loss: 0.488
[44,     1] loss: 0.472
[45,     1] loss: 0.475
[46,     1] loss: 0.482
[47,     1] loss: 0.497
[48,     1] loss: 0.487
[49,     1] loss: 0.447
[50,     1] loss: 0.474
[51,     1] loss: 0.449
[52,     1] loss: 0.460
[53,     1] loss: 0.401
[54,     1] loss: 0.420
[55,     1] loss: 0.412
[56,     1] loss: 0.434
[57,     1] loss: 0.406
[58,     1] loss: 0.434
[59,     1] loss: 0.424
[60,     1] loss: 0.414
[61,     1] loss: 0.389
[62,     1] loss: 0.363
[63,     1] loss: 0.376
[64,     1] loss: 0.389
[65,     1] loss: 0.377
[66,     1] loss: 0.372
[67,     1] loss: 0.370
[68,     1] loss: 0.386
[69,     1] loss: 0.368
[70,     1] loss: 0.362
[71,     1] loss: 0.339
[72,     1] loss: 0.337
[73,     1] loss: 0.356
[74,     1] loss: 0.340
[75,     1] loss: 0.366
[76,     1] loss: 0.332
[77,     1] loss: 0.295
[78,     1] loss: 0.350
[79,     1] loss: 0.338
[80,     1] loss: 0.292
[81,     1] loss: 0.315
[82,     1] loss: 0.326
[83,     1] loss: 0.300
[84,     1] loss: 0.351
[85,     1] loss: 0.307
[86,     1] loss: 0.240
[87,     1] loss: 0.290
[88,     1] loss: 0.312
[89,     1] loss: 0.273
[90,     1] loss: 0.315
[91,     1] loss: 0.318
[92,     1] loss: 0.280
[93,     1] loss: 0.319
[94,     1] loss: 0.248
[95,     1] loss: 0.243
[96,     1] loss: 0.281
[97,     1] loss: 0.318
[98,     1] loss: 0.268
[99,     1] loss: 0.306
[100,     1] loss: 0.308
[101,     1] loss: 0.252
[102,     1] loss: 0.224
[103,     1] loss: 0.276
[104,     1] loss: 0.257
[105,     1] loss: 0.209
[106,     1] loss: 0.247
[107,     1] loss: 0.229
[108,     1] loss: 0.258
[109,     1] loss: 0.251
[110,     1] loss: 0.238
[111,     1] loss: 0.239
[112,     1] loss: 0.286
[113,     1] loss: 0.257
[114,     1] loss: 0.208
[115,     1] loss: 0.259
[116,     1] loss: 0.292
[117,     1] loss: 0.270
[118,     1] loss: 0.177
[119,     1] loss: 0.252
[120,     1] loss: 0.224
[121,     1] loss: 0.214
[122,     1] loss: 0.217
[123,     1] loss: 0.214
[124,     1] loss: 0.218
[125,     1] loss: 0.179
[126,     1] loss: 0.186
[127,     1] loss: 0.223
[128,     1] loss: 0.196
[129,     1] loss: 0.202
[130,     1] loss: 0.232
[131,     1] loss: 0.244
[132,     1] loss: 0.211
[133,     1] loss: 0.160
[134,     1] loss: 0.158
[135,     1] loss: 0.178
[136,     1] loss: 0.181
[137,     1] loss: 0.190
[138,     1] loss: 0.191
[139,     1] loss: 0.143
[140,     1] loss: 0.198
[141,     1] loss: 0.167
[142,     1] loss: 0.194
[143,     1] loss: 0.159
[144,     1] loss: 0.196
[145,     1] loss: 0.209
[146,     1] loss: 0.129
[147,     1] loss: 0.191
[148,     1] loss: 0.189
[149,     1] loss: 0.178
[150,     1] loss: 0.160
[151,     1] loss: 0.155
[152,     1] loss: 0.164
[153,     1] loss: 0.127
[154,     1] loss: 0.172
[155,     1] loss: 0.153
[156,     1] loss: 0.143
[157,     1] loss: 0.127
[158,     1] loss: 0.122
[159,     1] loss: 0.115
[160,     1] loss: 0.110
[161,     1] loss: 0.128
[162,     1] loss: 0.115
[163,     1] loss: 0.097
[164,     1] loss: 0.130
[165,     1] loss: 0.107
[166,     1] loss: 0.113
[167,     1] loss: 0.116
[168,     1] loss: 0.109
[169,     1] loss: 0.120
[170,     1] loss: 0.101
[171,     1] loss: 0.106
[172,     1] loss: 0.087
[173,     1] loss: 0.082
[174,     1] loss: 0.108
[175,     1] loss: 0.090
[176,     1] loss: 0.085
[177,     1] loss: 0.101
[178,     1] loss: 0.084
[179,     1] loss: 0.097
[180,     1] loss: 0.080
[181,     1] loss: 0.102
[182,     1] loss: 0.095
[183,     1] loss: 0.099
[184,     1] loss: 0.085
[185,     1] loss: 0.079
[186,     1] loss: 0.087
[187,     1] loss: 0.078
[188,     1] loss: 0.082
[189,     1] loss: 0.089
[190,     1] loss: 0.078
[191,     1] loss: 0.084
[192,     1] loss: 0.070
[193,     1] loss: 0.083
[194,     1] loss: 0.078
[195,     1] loss: 0.074
[196,     1] loss: 0.077
[197,     1] loss: 0.074
[198,     1] loss: 0.080
[199,     1] loss: 0.069
[200,     1] loss: 0.072
Finished Training
Total time taken: 21.751136541366577
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.692
[4,     1] loss: 0.690
[5,     1] loss: 0.688
[6,     1] loss: 0.689
[7,     1] loss: 0.688
[8,     1] loss: 0.681
[9,     1] loss: 0.673
[10,     1] loss: 0.671
[11,     1] loss: 0.668
[12,     1] loss: 0.662
[13,     1] loss: 0.655
[14,     1] loss: 0.659
[15,     1] loss: 0.641
[16,     1] loss: 0.633
[17,     1] loss: 0.608
[18,     1] loss: 0.609
[19,     1] loss: 0.613
[20,     1] loss: 0.594
[21,     1] loss: 0.600
[22,     1] loss: 0.583
[23,     1] loss: 0.591
[24,     1] loss: 0.594
[25,     1] loss: 0.552
[26,     1] loss: 0.565
[27,     1] loss: 0.552
[28,     1] loss: 0.539
[29,     1] loss: 0.542
[30,     1] loss: 0.534
[31,     1] loss: 0.517
[32,     1] loss: 0.526
[33,     1] loss: 0.493
[34,     1] loss: 0.509
[35,     1] loss: 0.497
[36,     1] loss: 0.469
[37,     1] loss: 0.461
[38,     1] loss: 0.459
[39,     1] loss: 0.464
[40,     1] loss: 0.447
[41,     1] loss: 0.451
[42,     1] loss: 0.427
[43,     1] loss: 0.437
[44,     1] loss: 0.400
[45,     1] loss: 0.428
[46,     1] loss: 0.410
[47,     1] loss: 0.370
[48,     1] loss: 0.393
[49,     1] loss: 0.385
[50,     1] loss: 0.394
[51,     1] loss: 0.372
[52,     1] loss: 0.352
[53,     1] loss: 0.346
[54,     1] loss: 0.326
[55,     1] loss: 0.327
[56,     1] loss: 0.364
[57,     1] loss: 0.338
[58,     1] loss: 0.329
[59,     1] loss: 0.343
[60,     1] loss: 0.338
[61,     1] loss: 0.357
[62,     1] loss: 0.316
[63,     1] loss: 0.319
[64,     1] loss: 0.289
[65,     1] loss: 0.329
[66,     1] loss: 0.268
[67,     1] loss: 0.320
[68,     1] loss: 0.256
[69,     1] loss: 0.283
[70,     1] loss: 0.303
[71,     1] loss: 0.282
[72,     1] loss: 0.258
[73,     1] loss: 0.242
[74,     1] loss: 0.272
[75,     1] loss: 0.292
[76,     1] loss: 0.248
[77,     1] loss: 0.214
[78,     1] loss: 0.261
[79,     1] loss: 0.244
[80,     1] loss: 0.229
[81,     1] loss: 0.257
[82,     1] loss: 0.253
[83,     1] loss: 0.242
[84,     1] loss: 0.245
[85,     1] loss: 0.295
[86,     1] loss: 0.233
[87,     1] loss: 0.220
[88,     1] loss: 0.204
[89,     1] loss: 0.235
[90,     1] loss: 0.219
[91,     1] loss: 0.246
[92,     1] loss: 0.222
[93,     1] loss: 0.242
[94,     1] loss: 0.193
[95,     1] loss: 0.236
[96,     1] loss: 0.222
[97,     1] loss: 0.206
[98,     1] loss: 0.266
[99,     1] loss: 0.227
[100,     1] loss: 0.211
[101,     1] loss: 0.230
[102,     1] loss: 0.203
[103,     1] loss: 0.206
[104,     1] loss: 0.170
[105,     1] loss: 0.175
[106,     1] loss: 0.245
Early stopping applied (best metric=0.37641215324401855)
Finished Training
Total time taken: 11.684380769729614
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.695
[3,     1] loss: 0.687
[4,     1] loss: 0.688
[5,     1] loss: 0.683
[6,     1] loss: 0.678
[7,     1] loss: 0.673
[8,     1] loss: 0.675
[9,     1] loss: 0.668
[10,     1] loss: 0.658
[11,     1] loss: 0.653
[12,     1] loss: 0.639
[13,     1] loss: 0.627
[14,     1] loss: 0.620
[15,     1] loss: 0.620
[16,     1] loss: 0.596
[17,     1] loss: 0.612
[18,     1] loss: 0.611
[19,     1] loss: 0.588
[20,     1] loss: 0.583
[21,     1] loss: 0.587
[22,     1] loss: 0.571
[23,     1] loss: 0.578
[24,     1] loss: 0.554
[25,     1] loss: 0.567
[26,     1] loss: 0.542
[27,     1] loss: 0.561
[28,     1] loss: 0.564
[29,     1] loss: 0.541
[30,     1] loss: 0.516
[31,     1] loss: 0.522
[32,     1] loss: 0.535
[33,     1] loss: 0.508
[34,     1] loss: 0.472
[35,     1] loss: 0.494
[36,     1] loss: 0.508
[37,     1] loss: 0.475
[38,     1] loss: 0.478
[39,     1] loss: 0.505
[40,     1] loss: 0.473
[41,     1] loss: 0.485
[42,     1] loss: 0.436
[43,     1] loss: 0.462
[44,     1] loss: 0.440
[45,     1] loss: 0.407
[46,     1] loss: 0.443
[47,     1] loss: 0.441
[48,     1] loss: 0.421
[49,     1] loss: 0.397
[50,     1] loss: 0.416
[51,     1] loss: 0.399
[52,     1] loss: 0.386
[53,     1] loss: 0.364
[54,     1] loss: 0.373
[55,     1] loss: 0.385
[56,     1] loss: 0.342
[57,     1] loss: 0.368
[58,     1] loss: 0.382
[59,     1] loss: 0.366
[60,     1] loss: 0.328
[61,     1] loss: 0.327
[62,     1] loss: 0.321
[63,     1] loss: 0.340
[64,     1] loss: 0.320
[65,     1] loss: 0.307
[66,     1] loss: 0.295
[67,     1] loss: 0.295
[68,     1] loss: 0.282
[69,     1] loss: 0.275
[70,     1] loss: 0.297
[71,     1] loss: 0.284
[72,     1] loss: 0.284
[73,     1] loss: 0.256
[74,     1] loss: 0.249
[75,     1] loss: 0.258
[76,     1] loss: 0.242
[77,     1] loss: 0.217
[78,     1] loss: 0.226
[79,     1] loss: 0.215
[80,     1] loss: 0.199
[81,     1] loss: 0.224
[82,     1] loss: 0.243
[83,     1] loss: 0.201
[84,     1] loss: 0.210
[85,     1] loss: 0.208
[86,     1] loss: 0.228
[87,     1] loss: 0.220
[88,     1] loss: 0.184
[89,     1] loss: 0.179
[90,     1] loss: 0.184
[91,     1] loss: 0.189
[92,     1] loss: 0.161
[93,     1] loss: 0.179
[94,     1] loss: 0.185
Early stopping applied (best metric=0.46794506907463074)
Finished Training
Total time taken: 10.293553352355957
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.695
[3,     1] loss: 0.690
[4,     1] loss: 0.694
[5,     1] loss: 0.688
[6,     1] loss: 0.688
[7,     1] loss: 0.680
[8,     1] loss: 0.678
[9,     1] loss: 0.674
[10,     1] loss: 0.671
[11,     1] loss: 0.665
[12,     1] loss: 0.654
[13,     1] loss: 0.653
[14,     1] loss: 0.645
[15,     1] loss: 0.636
[16,     1] loss: 0.636
[17,     1] loss: 0.639
[18,     1] loss: 0.627
[19,     1] loss: 0.625
[20,     1] loss: 0.613
[21,     1] loss: 0.605
[22,     1] loss: 0.594
[23,     1] loss: 0.601
[24,     1] loss: 0.595
[25,     1] loss: 0.584
[26,     1] loss: 0.579
[27,     1] loss: 0.572
[28,     1] loss: 0.565
[29,     1] loss: 0.564
[30,     1] loss: 0.577
[31,     1] loss: 0.547
[32,     1] loss: 0.556
[33,     1] loss: 0.530
[34,     1] loss: 0.539
[35,     1] loss: 0.545
[36,     1] loss: 0.530
[37,     1] loss: 0.527
[38,     1] loss: 0.512
[39,     1] loss: 0.500
[40,     1] loss: 0.504
[41,     1] loss: 0.507
[42,     1] loss: 0.481
[43,     1] loss: 0.487
[44,     1] loss: 0.439
[45,     1] loss: 0.508
[46,     1] loss: 0.481
[47,     1] loss: 0.455
[48,     1] loss: 0.484
[49,     1] loss: 0.476
[50,     1] loss: 0.464
[51,     1] loss: 0.429
[52,     1] loss: 0.438
[53,     1] loss: 0.416
[54,     1] loss: 0.395
[55,     1] loss: 0.414
[56,     1] loss: 0.395
[57,     1] loss: 0.419
[58,     1] loss: 0.415
[59,     1] loss: 0.397
[60,     1] loss: 0.370
[61,     1] loss: 0.396
[62,     1] loss: 0.343
[63,     1] loss: 0.407
[64,     1] loss: 0.356
[65,     1] loss: 0.346
[66,     1] loss: 0.341
[67,     1] loss: 0.349
[68,     1] loss: 0.311
[69,     1] loss: 0.316
[70,     1] loss: 0.333
[71,     1] loss: 0.301
[72,     1] loss: 0.310
[73,     1] loss: 0.314
[74,     1] loss: 0.308
[75,     1] loss: 0.328
[76,     1] loss: 0.256
[77,     1] loss: 0.279
[78,     1] loss: 0.311
[79,     1] loss: 0.275
[80,     1] loss: 0.257
[81,     1] loss: 0.273
[82,     1] loss: 0.273
[83,     1] loss: 0.285
[84,     1] loss: 0.270
[85,     1] loss: 0.271
[86,     1] loss: 0.273
[87,     1] loss: 0.259
[88,     1] loss: 0.270
[89,     1] loss: 0.235
[90,     1] loss: 0.258
[91,     1] loss: 0.222
[92,     1] loss: 0.181
[93,     1] loss: 0.265
[94,     1] loss: 0.288
[95,     1] loss: 0.242
[96,     1] loss: 0.234
[97,     1] loss: 0.214
[98,     1] loss: 0.243
[99,     1] loss: 0.245
[100,     1] loss: 0.261
[101,     1] loss: 0.202
[102,     1] loss: 0.191
[103,     1] loss: 0.275
[104,     1] loss: 0.232
[105,     1] loss: 0.215
[106,     1] loss: 0.178
[107,     1] loss: 0.194
[108,     1] loss: 0.235
[109,     1] loss: 0.241
[110,     1] loss: 0.186
[111,     1] loss: 0.213
[112,     1] loss: 0.281
[113,     1] loss: 0.243
[114,     1] loss: 0.188
[115,     1] loss: 0.169
[116,     1] loss: 0.190
[117,     1] loss: 0.171
[118,     1] loss: 0.205
[119,     1] loss: 0.224
[120,     1] loss: 0.159
[121,     1] loss: 0.209
[122,     1] loss: 0.155
[123,     1] loss: 0.182
[124,     1] loss: 0.185
[125,     1] loss: 0.178
[126,     1] loss: 0.154
[127,     1] loss: 0.130
[128,     1] loss: 0.141
[129,     1] loss: 0.136
[130,     1] loss: 0.138
[131,     1] loss: 0.184
[132,     1] loss: 0.128
[133,     1] loss: 0.161
[134,     1] loss: 0.147
[135,     1] loss: 0.128
[136,     1] loss: 0.146
[137,     1] loss: 0.155
[138,     1] loss: 0.137
[139,     1] loss: 0.149
[140,     1] loss: 0.122
[141,     1] loss: 0.175
[142,     1] loss: 0.118
[143,     1] loss: 0.136
[144,     1] loss: 0.142
[145,     1] loss: 0.135
[146,     1] loss: 0.120
[147,     1] loss: 0.170
[148,     1] loss: 0.098
[149,     1] loss: 0.120
[150,     1] loss: 0.148
[151,     1] loss: 0.115
[152,     1] loss: 0.123
[153,     1] loss: 0.106
[154,     1] loss: 0.112
[155,     1] loss: 0.102
[156,     1] loss: 0.098
[157,     1] loss: 0.128
Early stopping applied (best metric=0.2942066192626953)
Finished Training
Total time taken: 17.15756607055664
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.692
[3,     1] loss: 0.690
[4,     1] loss: 0.690
[5,     1] loss: 0.691
[6,     1] loss: 0.686
[7,     1] loss: 0.685
[8,     1] loss: 0.681
[9,     1] loss: 0.683
[10,     1] loss: 0.676
[11,     1] loss: 0.674
[12,     1] loss: 0.675
[13,     1] loss: 0.660
[14,     1] loss: 0.654
[15,     1] loss: 0.648
[16,     1] loss: 0.648
[17,     1] loss: 0.642
[18,     1] loss: 0.631
[19,     1] loss: 0.634
[20,     1] loss: 0.622
[21,     1] loss: 0.611
[22,     1] loss: 0.608
[23,     1] loss: 0.615
[24,     1] loss: 0.597
[25,     1] loss: 0.591
[26,     1] loss: 0.584
[27,     1] loss: 0.566
[28,     1] loss: 0.578
[29,     1] loss: 0.572
[30,     1] loss: 0.570
[31,     1] loss: 0.564
[32,     1] loss: 0.538
[33,     1] loss: 0.544
[34,     1] loss: 0.541
[35,     1] loss: 0.513
[36,     1] loss: 0.537
[37,     1] loss: 0.527
[38,     1] loss: 0.510
[39,     1] loss: 0.520
[40,     1] loss: 0.507
[41,     1] loss: 0.501
[42,     1] loss: 0.490
[43,     1] loss: 0.481
[44,     1] loss: 0.492
[45,     1] loss: 0.481
[46,     1] loss: 0.479
[47,     1] loss: 0.491
[48,     1] loss: 0.459
[49,     1] loss: 0.451
[50,     1] loss: 0.468
[51,     1] loss: 0.449
[52,     1] loss: 0.434
[53,     1] loss: 0.423
[54,     1] loss: 0.426
[55,     1] loss: 0.384
[56,     1] loss: 0.416
[57,     1] loss: 0.418
[58,     1] loss: 0.394
[59,     1] loss: 0.400
[60,     1] loss: 0.355
[61,     1] loss: 0.378
[62,     1] loss: 0.384
[63,     1] loss: 0.348
[64,     1] loss: 0.333
[65,     1] loss: 0.398
[66,     1] loss: 0.390
[67,     1] loss: 0.381
[68,     1] loss: 0.393
[69,     1] loss: 0.354
[70,     1] loss: 0.302
[71,     1] loss: 0.320
[72,     1] loss: 0.306
[73,     1] loss: 0.337
[74,     1] loss: 0.290
[75,     1] loss: 0.286
[76,     1] loss: 0.338
[77,     1] loss: 0.277
[78,     1] loss: 0.299
[79,     1] loss: 0.319
[80,     1] loss: 0.286
[81,     1] loss: 0.260
[82,     1] loss: 0.241
[83,     1] loss: 0.256
[84,     1] loss: 0.229
[85,     1] loss: 0.228
[86,     1] loss: 0.290
[87,     1] loss: 0.255
[88,     1] loss: 0.221
[89,     1] loss: 0.234
[90,     1] loss: 0.222
[91,     1] loss: 0.249
[92,     1] loss: 0.198
[93,     1] loss: 0.206
[94,     1] loss: 0.230
[95,     1] loss: 0.216
[96,     1] loss: 0.165
[97,     1] loss: 0.182
[98,     1] loss: 0.163
[99,     1] loss: 0.180
[100,     1] loss: 0.188
[101,     1] loss: 0.193
[102,     1] loss: 0.179
[103,     1] loss: 0.192
[104,     1] loss: 0.180
[105,     1] loss: 0.165
[106,     1] loss: 0.155
[107,     1] loss: 0.196
[108,     1] loss: 0.138
[109,     1] loss: 0.191
[110,     1] loss: 0.142
[111,     1] loss: 0.151
[112,     1] loss: 0.138
Early stopping applied (best metric=0.30370938777923584)
Finished Training
Total time taken: 12.311402559280396
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.697
[3,     1] loss: 0.695
[4,     1] loss: 0.692
[5,     1] loss: 0.692
[6,     1] loss: 0.689
[7,     1] loss: 0.689
[8,     1] loss: 0.685
[9,     1] loss: 0.684
[10,     1] loss: 0.688
[11,     1] loss: 0.677
[12,     1] loss: 0.676
[13,     1] loss: 0.671
[14,     1] loss: 0.668
[15,     1] loss: 0.664
[16,     1] loss: 0.657
[17,     1] loss: 0.656
[18,     1] loss: 0.651
[19,     1] loss: 0.643
[20,     1] loss: 0.640
[21,     1] loss: 0.634
[22,     1] loss: 0.627
[23,     1] loss: 0.621
[24,     1] loss: 0.618
[25,     1] loss: 0.611
[26,     1] loss: 0.600
[27,     1] loss: 0.608
[28,     1] loss: 0.588
[29,     1] loss: 0.571
[30,     1] loss: 0.581
[31,     1] loss: 0.579
[32,     1] loss: 0.582
[33,     1] loss: 0.555
[34,     1] loss: 0.539
[35,     1] loss: 0.562
[36,     1] loss: 0.555
[37,     1] loss: 0.522
[38,     1] loss: 0.536
[39,     1] loss: 0.530
[40,     1] loss: 0.530
[41,     1] loss: 0.508
[42,     1] loss: 0.493
[43,     1] loss: 0.494
[44,     1] loss: 0.514
[45,     1] loss: 0.485
[46,     1] loss: 0.509
[47,     1] loss: 0.483
[48,     1] loss: 0.467
[49,     1] loss: 0.451
[50,     1] loss: 0.467
[51,     1] loss: 0.438
[52,     1] loss: 0.469
[53,     1] loss: 0.471
[54,     1] loss: 0.426
[55,     1] loss: 0.443
[56,     1] loss: 0.423
[57,     1] loss: 0.422
[58,     1] loss: 0.430
[59,     1] loss: 0.400
[60,     1] loss: 0.399
[61,     1] loss: 0.403
[62,     1] loss: 0.398
[63,     1] loss: 0.363
[64,     1] loss: 0.394
[65,     1] loss: 0.379
[66,     1] loss: 0.361
[67,     1] loss: 0.378
[68,     1] loss: 0.327
[69,     1] loss: 0.350
[70,     1] loss: 0.344
[71,     1] loss: 0.300
[72,     1] loss: 0.325
[73,     1] loss: 0.339
[74,     1] loss: 0.313
[75,     1] loss: 0.289
[76,     1] loss: 0.321
[77,     1] loss: 0.311
[78,     1] loss: 0.285
[79,     1] loss: 0.323
[80,     1] loss: 0.295
[81,     1] loss: 0.298
[82,     1] loss: 0.261
[83,     1] loss: 0.309
[84,     1] loss: 0.324
[85,     1] loss: 0.287
[86,     1] loss: 0.234
[87,     1] loss: 0.243
[88,     1] loss: 0.248
[89,     1] loss: 0.203
[90,     1] loss: 0.260
[91,     1] loss: 0.244
[92,     1] loss: 0.254
[93,     1] loss: 0.231
[94,     1] loss: 0.224
[95,     1] loss: 0.255
[96,     1] loss: 0.239
[97,     1] loss: 0.193
[98,     1] loss: 0.225
[99,     1] loss: 0.263
[100,     1] loss: 0.233
[101,     1] loss: 0.192
[102,     1] loss: 0.215
[103,     1] loss: 0.243
[104,     1] loss: 0.211
[105,     1] loss: 0.222
[106,     1] loss: 0.199
[107,     1] loss: 0.216
Early stopping applied (best metric=0.45562320947647095)
Finished Training
Total time taken: 11.676041603088379
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.703
[3,     1] loss: 0.688
[4,     1] loss: 0.695
[5,     1] loss: 0.687
[6,     1] loss: 0.687
[7,     1] loss: 0.687
[8,     1] loss: 0.684
[9,     1] loss: 0.679
[10,     1] loss: 0.680
[11,     1] loss: 0.677
[12,     1] loss: 0.675
[13,     1] loss: 0.668
[14,     1] loss: 0.661
[15,     1] loss: 0.659
[16,     1] loss: 0.649
[17,     1] loss: 0.644
[18,     1] loss: 0.637
[19,     1] loss: 0.637
[20,     1] loss: 0.637
[21,     1] loss: 0.623
[22,     1] loss: 0.621
[23,     1] loss: 0.622
[24,     1] loss: 0.615
[25,     1] loss: 0.602
[26,     1] loss: 0.603
[27,     1] loss: 0.602
[28,     1] loss: 0.587
[29,     1] loss: 0.586
[30,     1] loss: 0.569
[31,     1] loss: 0.562
[32,     1] loss: 0.549
[33,     1] loss: 0.559
[34,     1] loss: 0.554
[35,     1] loss: 0.547
[36,     1] loss: 0.546
[37,     1] loss: 0.543
[38,     1] loss: 0.531
[39,     1] loss: 0.537
[40,     1] loss: 0.505
[41,     1] loss: 0.532
[42,     1] loss: 0.504
[43,     1] loss: 0.507
[44,     1] loss: 0.521
[45,     1] loss: 0.515
[46,     1] loss: 0.501
[47,     1] loss: 0.477
[48,     1] loss: 0.484
[49,     1] loss: 0.464
[50,     1] loss: 0.448
[51,     1] loss: 0.455
[52,     1] loss: 0.426
[53,     1] loss: 0.453
[54,     1] loss: 0.423
[55,     1] loss: 0.401
[56,     1] loss: 0.451
[57,     1] loss: 0.415
[58,     1] loss: 0.420
[59,     1] loss: 0.433
[60,     1] loss: 0.381
[61,     1] loss: 0.363
[62,     1] loss: 0.406
[63,     1] loss: 0.389
[64,     1] loss: 0.375
[65,     1] loss: 0.361
[66,     1] loss: 0.352
[67,     1] loss: 0.329
[68,     1] loss: 0.320
[69,     1] loss: 0.323
[70,     1] loss: 0.337
[71,     1] loss: 0.367
[72,     1] loss: 0.326
[73,     1] loss: 0.318
[74,     1] loss: 0.311
[75,     1] loss: 0.300
[76,     1] loss: 0.310
[77,     1] loss: 0.255
[78,     1] loss: 0.293
[79,     1] loss: 0.336
[80,     1] loss: 0.252
[81,     1] loss: 0.296
[82,     1] loss: 0.282
[83,     1] loss: 0.301
[84,     1] loss: 0.256
[85,     1] loss: 0.258
[86,     1] loss: 0.250
[87,     1] loss: 0.262
[88,     1] loss: 0.266
[89,     1] loss: 0.252
[90,     1] loss: 0.249
[91,     1] loss: 0.265
[92,     1] loss: 0.246
[93,     1] loss: 0.224
[94,     1] loss: 0.243
[95,     1] loss: 0.278
[96,     1] loss: 0.233
[97,     1] loss: 0.224
[98,     1] loss: 0.234
[99,     1] loss: 0.232
[100,     1] loss: 0.212
[101,     1] loss: 0.238
[102,     1] loss: 0.216
[103,     1] loss: 0.207
[104,     1] loss: 0.179
[105,     1] loss: 0.215
[106,     1] loss: 0.250
[107,     1] loss: 0.217
[108,     1] loss: 0.195
[109,     1] loss: 0.166
[110,     1] loss: 0.201
[111,     1] loss: 0.179
[112,     1] loss: 0.209
[113,     1] loss: 0.171
[114,     1] loss: 0.169
[115,     1] loss: 0.156
[116,     1] loss: 0.172
[117,     1] loss: 0.175
[118,     1] loss: 0.169
[119,     1] loss: 0.155
[120,     1] loss: 0.149
[121,     1] loss: 0.199
Early stopping applied (best metric=0.3131491243839264)
Finished Training
Total time taken: 13.28894829750061
{'Hydroxylation-K Validation Accuracy': 0.834468085106383, 'Hydroxylation-K Validation Sensitivity': 0.784, 'Hydroxylation-K Validation Specificity': 0.8473684210526315, 'Hydroxylation-K Validation Precision': 0.5795118966001319, 'Hydroxylation-K AUC ROC': 0.8184561403508772, 'Hydroxylation-K AUC PR': 0.6133374556893997, 'Hydroxylation-K MCC': 0.5714342153555074, 'Hydroxylation-K F1': 0.6614144378208452, 'Validation Loss (Hydroxylation-K)': 0.371088088452816, 'Validation Loss (total)': 0.371088088452816, 'TimeToTrain': 12.81105502128601}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005982222706488786,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 920233898,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.5167347875252868}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.705
[3,     1] loss: 0.686
[4,     1] loss: 0.664
[5,     1] loss: 0.651
[6,     1] loss: 0.589
[7,     1] loss: 0.536
[8,     1] loss: 0.512
[9,     1] loss: 0.428
[10,     1] loss: 0.415
[11,     1] loss: 0.362
[12,     1] loss: 0.272
[13,     1] loss: 0.271
[14,     1] loss: 0.210
[15,     1] loss: 0.294
[16,     1] loss: 0.237
[17,     1] loss: 0.168
[18,     1] loss: 0.174
[19,     1] loss: 0.254
[20,     1] loss: 0.157
[21,     1] loss: 0.201
[22,     1] loss: 0.153
[23,     1] loss: 0.195
[24,     1] loss: 0.190
[25,     1] loss: 0.217
[26,     1] loss: 0.154
[27,     1] loss: 0.179
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029194386854302407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2992366025,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.834547042116697}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.678
[4,     1] loss: 0.656
[5,     1] loss: 0.625
[6,     1] loss: 0.598
[7,     1] loss: 0.568
[8,     1] loss: 0.518
[9,     1] loss: 0.483
[10,     1] loss: 0.456
[11,     1] loss: 0.423
[12,     1] loss: 0.397
[13,     1] loss: 0.396
[14,     1] loss: 0.323
[15,     1] loss: 0.358
[16,     1] loss: 0.305
[17,     1] loss: 0.274
[18,     1] loss: 0.261
[19,     1] loss: 0.347
[20,     1] loss: 0.255
[21,     1] loss: 0.206
[22,     1] loss: 0.255
[23,     1] loss: 0.207
[24,     1] loss: 0.218
[25,     1] loss: 0.232
[26,     1] loss: 0.213
[27,     1] loss: 0.218
[28,     1] loss: 0.236
[29,     1] loss: 0.263
[30,     1] loss: 0.232
[31,     1] loss: 0.135
[32,     1] loss: 0.291
[33,     1] loss: 0.166
[34,     1] loss: 0.206
[35,     1] loss: 0.207
[36,     1] loss: 0.141
[37,     1] loss: 0.181
[38,     1] loss: 0.172
[39,     1] loss: 0.182
[40,     1] loss: 0.120
[41,     1] loss: 0.136
[42,     1] loss: 0.131
[43,     1] loss: 0.131
[44,     1] loss: 0.105
[45,     1] loss: 0.086
[46,     1] loss: 0.101
[47,     1] loss: 0.154
[48,     1] loss: 0.103
[49,     1] loss: 0.094
[50,     1] loss: 0.120
[51,     1] loss: 0.077
[52,     1] loss: 0.097
[53,     1] loss: 0.081
[54,     1] loss: 0.108
[55,     1] loss: 0.096
[56,     1] loss: 0.090
[57,     1] loss: 0.151
[58,     1] loss: 0.069
[59,     1] loss: 0.087
[60,     1] loss: 0.085
[61,     1] loss: 0.112
[62,     1] loss: 0.056
[63,     1] loss: 0.118
[64,     1] loss: 0.085
[65,     1] loss: 0.094
[66,     1] loss: 0.137
[67,     1] loss: 0.101
[68,     1] loss: 0.115
[69,     1] loss: 0.100
[70,     1] loss: 0.115
[71,     1] loss: 0.150
[72,     1] loss: 0.076
[73,     1] loss: 0.132
[74,     1] loss: 0.134
[75,     1] loss: 0.079
Early stopping applied (best metric=0.21851077675819397)
Finished Training
Total time taken: 8.2138090133667
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.668
[4,     1] loss: 0.625
[5,     1] loss: 0.589
[6,     1] loss: 0.558
[7,     1] loss: 0.530
[8,     1] loss: 0.503
[9,     1] loss: 0.479
[10,     1] loss: 0.462
[11,     1] loss: 0.424
[12,     1] loss: 0.374
[13,     1] loss: 0.388
[14,     1] loss: 0.337
[15,     1] loss: 0.353
[16,     1] loss: 0.343
[17,     1] loss: 0.303
[18,     1] loss: 0.319
[19,     1] loss: 0.316
[20,     1] loss: 0.274
[21,     1] loss: 0.301
[22,     1] loss: 0.325
[23,     1] loss: 0.250
[24,     1] loss: 0.241
[25,     1] loss: 0.293
[26,     1] loss: 0.185
[27,     1] loss: 0.228
[28,     1] loss: 0.255
[29,     1] loss: 0.252
[30,     1] loss: 0.181
[31,     1] loss: 0.163
[32,     1] loss: 0.187
[33,     1] loss: 0.157
[34,     1] loss: 0.192
[35,     1] loss: 0.178
[36,     1] loss: 0.146
[37,     1] loss: 0.152
[38,     1] loss: 0.227
[39,     1] loss: 0.175
[40,     1] loss: 0.189
[41,     1] loss: 0.155
[42,     1] loss: 0.210
[43,     1] loss: 0.264
[44,     1] loss: 0.257
[45,     1] loss: 0.224
[46,     1] loss: 0.179
[47,     1] loss: 0.270
[48,     1] loss: 0.194
[49,     1] loss: 0.241
[50,     1] loss: 0.244
[51,     1] loss: 0.168
[52,     1] loss: 0.207
[53,     1] loss: 0.175
[54,     1] loss: 0.147
[55,     1] loss: 0.146
[56,     1] loss: 0.194
[57,     1] loss: 0.108
[58,     1] loss: 0.110
[59,     1] loss: 0.134
[60,     1] loss: 0.097
[61,     1] loss: 0.150
[62,     1] loss: 0.199
[63,     1] loss: 0.117
[64,     1] loss: 0.104
[65,     1] loss: 0.113
[66,     1] loss: 0.122
[67,     1] loss: 0.140
[68,     1] loss: 0.122
Early stopping applied (best metric=0.38071513175964355)
Finished Training
Total time taken: 7.439042091369629
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.665
[4,     1] loss: 0.617
[5,     1] loss: 0.619
[6,     1] loss: 0.600
[7,     1] loss: 0.572
[8,     1] loss: 0.509
[9,     1] loss: 0.519
[10,     1] loss: 0.504
[11,     1] loss: 0.486
[12,     1] loss: 0.439
[13,     1] loss: 0.429
[14,     1] loss: 0.370
[15,     1] loss: 0.348
[16,     1] loss: 0.376
[17,     1] loss: 0.323
[18,     1] loss: 0.347
[19,     1] loss: 0.292
[20,     1] loss: 0.372
[21,     1] loss: 0.273
[22,     1] loss: 0.299
[23,     1] loss: 0.288
[24,     1] loss: 0.254
[25,     1] loss: 0.278
[26,     1] loss: 0.202
[27,     1] loss: 0.325
[28,     1] loss: 0.275
[29,     1] loss: 0.246
[30,     1] loss: 0.201
[31,     1] loss: 0.155
[32,     1] loss: 0.325
[33,     1] loss: 0.239
[34,     1] loss: 0.272
[35,     1] loss: 0.160
[36,     1] loss: 0.231
[37,     1] loss: 0.233
[38,     1] loss: 0.181
[39,     1] loss: 0.250
[40,     1] loss: 0.190
[41,     1] loss: 0.146
[42,     1] loss: 0.189
[43,     1] loss: 0.278
[44,     1] loss: 0.159
[45,     1] loss: 0.250
[46,     1] loss: 0.141
[47,     1] loss: 0.154
[48,     1] loss: 0.171
[49,     1] loss: 0.117
[50,     1] loss: 0.112
[51,     1] loss: 0.091
[52,     1] loss: 0.119
[53,     1] loss: 0.090
[54,     1] loss: 0.085
[55,     1] loss: 0.078
[56,     1] loss: 0.093
[57,     1] loss: 0.077
[58,     1] loss: 0.079
[59,     1] loss: 0.053
[60,     1] loss: 0.121
[61,     1] loss: 0.103
[62,     1] loss: 0.115
[63,     1] loss: 0.215
[64,     1] loss: 0.146
[65,     1] loss: 0.116
[66,     1] loss: 0.137
[67,     1] loss: 0.148
[68,     1] loss: 0.188
[69,     1] loss: 0.120
[70,     1] loss: 0.152
[71,     1] loss: 0.117
[72,     1] loss: 0.153
[73,     1] loss: 0.121
[74,     1] loss: 0.150
[75,     1] loss: 0.163
[76,     1] loss: 0.113
[77,     1] loss: 0.082
Early stopping applied (best metric=0.34472477436065674)
Finished Training
Total time taken: 8.607714176177979
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.679
[4,     1] loss: 0.650
[5,     1] loss: 0.608
[6,     1] loss: 0.591
[7,     1] loss: 0.553
[8,     1] loss: 0.489
[9,     1] loss: 0.466
[10,     1] loss: 0.459
[11,     1] loss: 0.417
[12,     1] loss: 0.397
[13,     1] loss: 0.421
[14,     1] loss: 0.338
[15,     1] loss: 0.374
[16,     1] loss: 0.288
[17,     1] loss: 0.319
[18,     1] loss: 0.285
[19,     1] loss: 0.313
[20,     1] loss: 0.331
[21,     1] loss: 0.308
[22,     1] loss: 0.269
[23,     1] loss: 0.287
[24,     1] loss: 0.223
[25,     1] loss: 0.239
[26,     1] loss: 0.213
[27,     1] loss: 0.206
[28,     1] loss: 0.231
[29,     1] loss: 0.195
[30,     1] loss: 0.204
[31,     1] loss: 0.218
[32,     1] loss: 0.212
[33,     1] loss: 0.173
[34,     1] loss: 0.172
[35,     1] loss: 0.172
[36,     1] loss: 0.167
[37,     1] loss: 0.149
[38,     1] loss: 0.190
[39,     1] loss: 0.122
[40,     1] loss: 0.145
[41,     1] loss: 0.147
[42,     1] loss: 0.171
[43,     1] loss: 0.131
[44,     1] loss: 0.188
[45,     1] loss: 0.111
[46,     1] loss: 0.100
[47,     1] loss: 0.104
[48,     1] loss: 0.151
[49,     1] loss: 0.143
[50,     1] loss: 0.130
[51,     1] loss: 0.112
[52,     1] loss: 0.136
[53,     1] loss: 0.101
[54,     1] loss: 0.112
[55,     1] loss: 0.205
[56,     1] loss: 0.278
[57,     1] loss: 0.268
[58,     1] loss: 0.292
[59,     1] loss: 0.173
[60,     1] loss: 0.216
[61,     1] loss: 0.274
[62,     1] loss: 0.183
[63,     1] loss: 0.160
[64,     1] loss: 0.222
[65,     1] loss: 0.171
[66,     1] loss: 0.182
[67,     1] loss: 0.169
[68,     1] loss: 0.156
Early stopping applied (best metric=0.4232422411441803)
Finished Training
Total time taken: 7.502520322799683
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.652
[5,     1] loss: 0.625
[6,     1] loss: 0.611
[7,     1] loss: 0.575
[8,     1] loss: 0.554
[9,     1] loss: 0.502
[10,     1] loss: 0.497
[11,     1] loss: 0.495
[12,     1] loss: 0.426
[13,     1] loss: 0.373
[14,     1] loss: 0.393
[15,     1] loss: 0.358
[16,     1] loss: 0.314
[17,     1] loss: 0.368
[18,     1] loss: 0.341
[19,     1] loss: 0.272
[20,     1] loss: 0.312
[21,     1] loss: 0.270
[22,     1] loss: 0.259
[23,     1] loss: 0.285
[24,     1] loss: 0.262
[25,     1] loss: 0.325
[26,     1] loss: 0.227
[27,     1] loss: 0.221
[28,     1] loss: 0.223
[29,     1] loss: 0.217
[30,     1] loss: 0.145
[31,     1] loss: 0.196
[32,     1] loss: 0.168
[33,     1] loss: 0.166
[34,     1] loss: 0.198
[35,     1] loss: 0.234
[36,     1] loss: 0.162
[37,     1] loss: 0.141
[38,     1] loss: 0.148
[39,     1] loss: 0.139
[40,     1] loss: 0.132
[41,     1] loss: 0.132
[42,     1] loss: 0.147
[43,     1] loss: 0.181
[44,     1] loss: 0.169
[45,     1] loss: 0.137
[46,     1] loss: 0.190
[47,     1] loss: 0.243
[48,     1] loss: 0.196
[49,     1] loss: 0.126
[50,     1] loss: 0.138
[51,     1] loss: 0.135
[52,     1] loss: 0.102
[53,     1] loss: 0.110
[54,     1] loss: 0.154
[55,     1] loss: 0.125
[56,     1] loss: 0.164
[57,     1] loss: 0.110
[58,     1] loss: 0.101
[59,     1] loss: 0.095
[60,     1] loss: 0.170
[61,     1] loss: 0.178
[62,     1] loss: 0.109
[63,     1] loss: 0.142
[64,     1] loss: 0.103
[65,     1] loss: 0.178
[66,     1] loss: 0.124
[67,     1] loss: 0.118
[68,     1] loss: 0.249
[69,     1] loss: 0.134
[70,     1] loss: 0.241
[71,     1] loss: 0.215
[72,     1] loss: 0.202
[73,     1] loss: 0.189
[74,     1] loss: 0.155
[75,     1] loss: 0.117
[76,     1] loss: 0.136
[77,     1] loss: 0.158
[78,     1] loss: 0.119
[79,     1] loss: 0.143
[80,     1] loss: 0.175
[81,     1] loss: 0.091
[82,     1] loss: 0.092
[83,     1] loss: 0.137
[84,     1] loss: 0.071
[85,     1] loss: 0.155
[86,     1] loss: 0.122
[87,     1] loss: 0.230
[88,     1] loss: 0.096
[89,     1] loss: 0.107
[90,     1] loss: 0.081
[91,     1] loss: 0.106
[92,     1] loss: 0.083
[93,     1] loss: 0.125
[94,     1] loss: 0.078
[95,     1] loss: 0.068
[96,     1] loss: 0.078
[97,     1] loss: 0.086
[98,     1] loss: 0.067
[99,     1] loss: 0.113
[100,     1] loss: 0.103
[101,     1] loss: 0.068
[102,     1] loss: 0.076
[103,     1] loss: 0.088
[104,     1] loss: 0.053
[105,     1] loss: 0.066
[106,     1] loss: 0.073
[107,     1] loss: 0.072
[108,     1] loss: 0.084
[109,     1] loss: 0.097
Early stopping applied (best metric=0.3709157407283783)
Finished Training
Total time taken: 11.90602421760559
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.689
[2,     1] loss: 0.692
[3,     1] loss: 0.671
[4,     1] loss: 0.642
[5,     1] loss: 0.611
[6,     1] loss: 0.580
[7,     1] loss: 0.571
[8,     1] loss: 0.524
[9,     1] loss: 0.505
[10,     1] loss: 0.484
[11,     1] loss: 0.482
[12,     1] loss: 0.418
[13,     1] loss: 0.409
[14,     1] loss: 0.362
[15,     1] loss: 0.422
[16,     1] loss: 0.361
[17,     1] loss: 0.281
[18,     1] loss: 0.315
[19,     1] loss: 0.268
[20,     1] loss: 0.315
[21,     1] loss: 0.284
[22,     1] loss: 0.288
[23,     1] loss: 0.242
[24,     1] loss: 0.210
[25,     1] loss: 0.243
[26,     1] loss: 0.177
[27,     1] loss: 0.233
[28,     1] loss: 0.202
[29,     1] loss: 0.209
[30,     1] loss: 0.176
[31,     1] loss: 0.222
[32,     1] loss: 0.154
[33,     1] loss: 0.194
[34,     1] loss: 0.222
[35,     1] loss: 0.167
[36,     1] loss: 0.187
[37,     1] loss: 0.159
[38,     1] loss: 0.128
[39,     1] loss: 0.224
[40,     1] loss: 0.125
[41,     1] loss: 0.204
[42,     1] loss: 0.144
[43,     1] loss: 0.138
[44,     1] loss: 0.134
[45,     1] loss: 0.138
[46,     1] loss: 0.177
[47,     1] loss: 0.157
[48,     1] loss: 0.180
[49,     1] loss: 0.187
[50,     1] loss: 0.118
[51,     1] loss: 0.151
[52,     1] loss: 0.197
[53,     1] loss: 0.143
[54,     1] loss: 0.265
[55,     1] loss: 0.182
[56,     1] loss: 0.187
[57,     1] loss: 0.142
[58,     1] loss: 0.169
[59,     1] loss: 0.159
[60,     1] loss: 0.181
[61,     1] loss: 0.142
[62,     1] loss: 0.124
[63,     1] loss: 0.092
[64,     1] loss: 0.099
[65,     1] loss: 0.159
Early stopping applied (best metric=0.32315725088119507)
Finished Training
Total time taken: 7.331808567047119
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.683
[4,     1] loss: 0.645
[5,     1] loss: 0.635
[6,     1] loss: 0.624
[7,     1] loss: 0.581
[8,     1] loss: 0.546
[9,     1] loss: 0.524
[10,     1] loss: 0.511
[11,     1] loss: 0.476
[12,     1] loss: 0.480
[13,     1] loss: 0.444
[14,     1] loss: 0.445
[15,     1] loss: 0.400
[16,     1] loss: 0.373
[17,     1] loss: 0.408
[18,     1] loss: 0.324
[19,     1] loss: 0.349
[20,     1] loss: 0.342
[21,     1] loss: 0.297
[22,     1] loss: 0.369
[23,     1] loss: 0.352
[24,     1] loss: 0.305
[25,     1] loss: 0.301
[26,     1] loss: 0.334
[27,     1] loss: 0.322
[28,     1] loss: 0.304
[29,     1] loss: 0.257
[30,     1] loss: 0.224
[31,     1] loss: 0.253
[32,     1] loss: 0.254
[33,     1] loss: 0.261
[34,     1] loss: 0.341
[35,     1] loss: 0.312
[36,     1] loss: 0.294
[37,     1] loss: 0.334
[38,     1] loss: 0.315
[39,     1] loss: 0.249
[40,     1] loss: 0.269
[41,     1] loss: 0.275
[42,     1] loss: 0.262
[43,     1] loss: 0.209
[44,     1] loss: 0.204
[45,     1] loss: 0.239
[46,     1] loss: 0.213
[47,     1] loss: 0.221
[48,     1] loss: 0.165
[49,     1] loss: 0.171
[50,     1] loss: 0.175
[51,     1] loss: 0.125
[52,     1] loss: 0.174
[53,     1] loss: 0.131
[54,     1] loss: 0.130
[55,     1] loss: 0.139
[56,     1] loss: 0.158
[57,     1] loss: 0.105
[58,     1] loss: 0.165
[59,     1] loss: 0.157
[60,     1] loss: 0.209
[61,     1] loss: 0.153
[62,     1] loss: 0.130
[63,     1] loss: 0.156
[64,     1] loss: 0.110
[65,     1] loss: 0.108
[66,     1] loss: 0.148
[67,     1] loss: 0.173
[68,     1] loss: 0.123
[69,     1] loss: 0.135
[70,     1] loss: 0.126
[71,     1] loss: 0.109
[72,     1] loss: 0.092
[73,     1] loss: 0.095
[74,     1] loss: 0.153
[75,     1] loss: 0.146
[76,     1] loss: 0.152
[77,     1] loss: 0.113
Early stopping applied (best metric=0.39493894577026367)
Finished Training
Total time taken: 8.521031618118286
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.643
[5,     1] loss: 0.616
[6,     1] loss: 0.594
[7,     1] loss: 0.536
[8,     1] loss: 0.519
[9,     1] loss: 0.473
[10,     1] loss: 0.468
[11,     1] loss: 0.402
[12,     1] loss: 0.395
[13,     1] loss: 0.380
[14,     1] loss: 0.367
[15,     1] loss: 0.294
[16,     1] loss: 0.364
[17,     1] loss: 0.311
[18,     1] loss: 0.302
[19,     1] loss: 0.285
[20,     1] loss: 0.285
[21,     1] loss: 0.257
[22,     1] loss: 0.248
[23,     1] loss: 0.221
[24,     1] loss: 0.203
[25,     1] loss: 0.198
[26,     1] loss: 0.203
[27,     1] loss: 0.144
[28,     1] loss: 0.230
[29,     1] loss: 0.172
[30,     1] loss: 0.230
[31,     1] loss: 0.224
[32,     1] loss: 0.192
[33,     1] loss: 0.123
[34,     1] loss: 0.121
[35,     1] loss: 0.159
[36,     1] loss: 0.142
[37,     1] loss: 0.133
[38,     1] loss: 0.108
[39,     1] loss: 0.104
[40,     1] loss: 0.135
[41,     1] loss: 0.146
[42,     1] loss: 0.121
[43,     1] loss: 0.132
[44,     1] loss: 0.099
[45,     1] loss: 0.120
[46,     1] loss: 0.080
[47,     1] loss: 0.199
[48,     1] loss: 0.101
[49,     1] loss: 0.110
[50,     1] loss: 0.119
[51,     1] loss: 0.076
[52,     1] loss: 0.163
[53,     1] loss: 0.085
[54,     1] loss: 0.110
[55,     1] loss: 0.099
[56,     1] loss: 0.104
[57,     1] loss: 0.096
[58,     1] loss: 0.080
[59,     1] loss: 0.079
[60,     1] loss: 0.125
[61,     1] loss: 0.112
[62,     1] loss: 0.123
Early stopping applied (best metric=0.3169940710067749)
Finished Training
Total time taken: 6.843326091766357
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.684
[3,     1] loss: 0.683
[4,     1] loss: 0.652
[5,     1] loss: 0.627
[6,     1] loss: 0.611
[7,     1] loss: 0.560
[8,     1] loss: 0.549
[9,     1] loss: 0.516
[10,     1] loss: 0.510
[11,     1] loss: 0.516
[12,     1] loss: 0.460
[13,     1] loss: 0.413
[14,     1] loss: 0.376
[15,     1] loss: 0.425
[16,     1] loss: 0.370
[17,     1] loss: 0.389
[18,     1] loss: 0.350
[19,     1] loss: 0.363
[20,     1] loss: 0.305
[21,     1] loss: 0.322
[22,     1] loss: 0.335
[23,     1] loss: 0.291
[24,     1] loss: 0.303
[25,     1] loss: 0.294
[26,     1] loss: 0.247
[27,     1] loss: 0.215
[28,     1] loss: 0.241
[29,     1] loss: 0.221
[30,     1] loss: 0.191
[31,     1] loss: 0.174
[32,     1] loss: 0.161
[33,     1] loss: 0.204
[34,     1] loss: 0.172
[35,     1] loss: 0.254
[36,     1] loss: 0.352
[37,     1] loss: 0.289
[38,     1] loss: 0.231
[39,     1] loss: 0.227
[40,     1] loss: 0.256
[41,     1] loss: 0.190
[42,     1] loss: 0.205
[43,     1] loss: 0.170
[44,     1] loss: 0.199
[45,     1] loss: 0.167
[46,     1] loss: 0.163
[47,     1] loss: 0.176
[48,     1] loss: 0.147
[49,     1] loss: 0.137
[50,     1] loss: 0.109
[51,     1] loss: 0.125
[52,     1] loss: 0.101
[53,     1] loss: 0.127
[54,     1] loss: 0.080
[55,     1] loss: 0.147
[56,     1] loss: 0.119
[57,     1] loss: 0.176
[58,     1] loss: 0.111
[59,     1] loss: 0.176
[60,     1] loss: 0.146
[61,     1] loss: 0.101
[62,     1] loss: 0.133
[63,     1] loss: 0.079
[64,     1] loss: 0.138
[65,     1] loss: 0.078
[66,     1] loss: 0.103
[67,     1] loss: 0.091
Early stopping applied (best metric=0.29509130120277405)
Finished Training
Total time taken: 7.381913423538208
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.682
[4,     1] loss: 0.681
[5,     1] loss: 0.664
[6,     1] loss: 0.632
[7,     1] loss: 0.612
[8,     1] loss: 0.588
[9,     1] loss: 0.551
[10,     1] loss: 0.521
[11,     1] loss: 0.512
[12,     1] loss: 0.496
[13,     1] loss: 0.464
[14,     1] loss: 0.408
[15,     1] loss: 0.376
[16,     1] loss: 0.376
[17,     1] loss: 0.324
[18,     1] loss: 0.365
[19,     1] loss: 0.382
[20,     1] loss: 0.345
[21,     1] loss: 0.316
[22,     1] loss: 0.309
[23,     1] loss: 0.276
[24,     1] loss: 0.275
[25,     1] loss: 0.227
[26,     1] loss: 0.283
[27,     1] loss: 0.283
[28,     1] loss: 0.270
[29,     1] loss: 0.216
[30,     1] loss: 0.239
[31,     1] loss: 0.199
[32,     1] loss: 0.187
[33,     1] loss: 0.174
[34,     1] loss: 0.159
[35,     1] loss: 0.208
[36,     1] loss: 0.221
[37,     1] loss: 0.213
[38,     1] loss: 0.171
[39,     1] loss: 0.230
[40,     1] loss: 0.182
[41,     1] loss: 0.321
[42,     1] loss: 0.311
[43,     1] loss: 0.236
[44,     1] loss: 0.261
[45,     1] loss: 0.258
[46,     1] loss: 0.188
[47,     1] loss: 0.191
[48,     1] loss: 0.244
[49,     1] loss: 0.231
[50,     1] loss: 0.170
[51,     1] loss: 0.177
[52,     1] loss: 0.208
[53,     1] loss: 0.179
[54,     1] loss: 0.167
[55,     1] loss: 0.173
[56,     1] loss: 0.157
[57,     1] loss: 0.235
[58,     1] loss: 0.263
[59,     1] loss: 0.306
[60,     1] loss: 0.244
[61,     1] loss: 0.245
[62,     1] loss: 0.288
[63,     1] loss: 0.297
[64,     1] loss: 0.237
[65,     1] loss: 0.230
[66,     1] loss: 0.210
[67,     1] loss: 0.198
[68,     1] loss: 0.229
[69,     1] loss: 0.243
[70,     1] loss: 0.155
[71,     1] loss: 0.176
[72,     1] loss: 0.224
[73,     1] loss: 0.166
[74,     1] loss: 0.173
[75,     1] loss: 0.147
[76,     1] loss: 0.192
[77,     1] loss: 0.163
[78,     1] loss: 0.156
[79,     1] loss: 0.147
[80,     1] loss: 0.157
[81,     1] loss: 0.217
[82,     1] loss: 0.084
[83,     1] loss: 0.122
[84,     1] loss: 0.138
[85,     1] loss: 0.130
[86,     1] loss: 0.113
[87,     1] loss: 0.120
[88,     1] loss: 0.134
[89,     1] loss: 0.118
[90,     1] loss: 0.092
[91,     1] loss: 0.116
[92,     1] loss: 0.228
Early stopping applied (best metric=0.13083988428115845)
Finished Training
Total time taken: 10.039054870605469
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.685
[4,     1] loss: 0.673
[5,     1] loss: 0.657
[6,     1] loss: 0.630
[7,     1] loss: 0.593
[8,     1] loss: 0.587
[9,     1] loss: 0.546
[10,     1] loss: 0.526
[11,     1] loss: 0.510
[12,     1] loss: 0.474
[13,     1] loss: 0.450
[14,     1] loss: 0.440
[15,     1] loss: 0.378
[16,     1] loss: 0.400
[17,     1] loss: 0.402
[18,     1] loss: 0.391
[19,     1] loss: 0.274
[20,     1] loss: 0.323
[21,     1] loss: 0.381
[22,     1] loss: 0.315
[23,     1] loss: 0.289
[24,     1] loss: 0.398
[25,     1] loss: 0.356
[26,     1] loss: 0.317
[27,     1] loss: 0.316
[28,     1] loss: 0.376
[29,     1] loss: 0.361
[30,     1] loss: 0.296
[31,     1] loss: 0.303
[32,     1] loss: 0.307
[33,     1] loss: 0.279
[34,     1] loss: 0.302
[35,     1] loss: 0.245
[36,     1] loss: 0.271
[37,     1] loss: 0.306
[38,     1] loss: 0.283
[39,     1] loss: 0.200
[40,     1] loss: 0.218
[41,     1] loss: 0.259
[42,     1] loss: 0.240
[43,     1] loss: 0.180
[44,     1] loss: 0.219
[45,     1] loss: 0.236
[46,     1] loss: 0.162
[47,     1] loss: 0.224
[48,     1] loss: 0.211
[49,     1] loss: 0.242
[50,     1] loss: 0.217
[51,     1] loss: 0.118
[52,     1] loss: 0.231
[53,     1] loss: 0.160
[54,     1] loss: 0.231
[55,     1] loss: 0.139
[56,     1] loss: 0.238
[57,     1] loss: 0.241
[58,     1] loss: 0.279
[59,     1] loss: 0.221
[60,     1] loss: 0.195
[61,     1] loss: 0.210
[62,     1] loss: 0.185
[63,     1] loss: 0.216
[64,     1] loss: 0.263
[65,     1] loss: 0.252
[66,     1] loss: 0.207
[67,     1] loss: 0.169
[68,     1] loss: 0.163
[69,     1] loss: 0.162
[70,     1] loss: 0.173
[71,     1] loss: 0.131
[72,     1] loss: 0.173
[73,     1] loss: 0.172
[74,     1] loss: 0.159
[75,     1] loss: 0.111
[76,     1] loss: 0.130
[77,     1] loss: 0.153
[78,     1] loss: 0.096
[79,     1] loss: 0.157
[80,     1] loss: 0.167
[81,     1] loss: 0.139
[82,     1] loss: 0.182
[83,     1] loss: 0.189
[84,     1] loss: 0.112
[85,     1] loss: 0.139
[86,     1] loss: 0.143
[87,     1] loss: 0.153
[88,     1] loss: 0.142
[89,     1] loss: 0.154
[90,     1] loss: 0.109
[91,     1] loss: 0.135
[92,     1] loss: 0.098
[93,     1] loss: 0.092
[94,     1] loss: 0.100
[95,     1] loss: 0.166
[96,     1] loss: 0.126
[97,     1] loss: 0.106
[98,     1] loss: 0.116
[99,     1] loss: 0.134
[100,     1] loss: 0.131
[101,     1] loss: 0.123
[102,     1] loss: 0.157
[103,     1] loss: 0.131
[104,     1] loss: 0.151
[105,     1] loss: 0.189
[106,     1] loss: 0.107
[107,     1] loss: 0.115
[108,     1] loss: 0.103
[109,     1] loss: 0.089
[110,     1] loss: 0.101
[111,     1] loss: 0.101
[112,     1] loss: 0.072
[113,     1] loss: 0.085
[114,     1] loss: 0.074
[115,     1] loss: 0.106
[116,     1] loss: 0.128
[117,     1] loss: 0.125
[118,     1] loss: 0.202
[119,     1] loss: 0.105
[120,     1] loss: 0.166
[121,     1] loss: 0.075
[122,     1] loss: 0.140
[123,     1] loss: 0.114
[124,     1] loss: 0.106
[125,     1] loss: 0.130
[126,     1] loss: 0.106
[127,     1] loss: 0.176
[128,     1] loss: 0.114
[129,     1] loss: 0.090
Early stopping applied (best metric=0.05990643426775932)
Finished Training
Total time taken: 14.23262643814087
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.695
[3,     1] loss: 0.676
[4,     1] loss: 0.643
[5,     1] loss: 0.604
[6,     1] loss: 0.586
[7,     1] loss: 0.564
[8,     1] loss: 0.523
[9,     1] loss: 0.450
[10,     1] loss: 0.453
[11,     1] loss: 0.425
[12,     1] loss: 0.420
[13,     1] loss: 0.387
[14,     1] loss: 0.404
[15,     1] loss: 0.361
[16,     1] loss: 0.391
[17,     1] loss: 0.320
[18,     1] loss: 0.314
[19,     1] loss: 0.342
[20,     1] loss: 0.304
[21,     1] loss: 0.271
[22,     1] loss: 0.254
[23,     1] loss: 0.230
[24,     1] loss: 0.206
[25,     1] loss: 0.214
[26,     1] loss: 0.223
[27,     1] loss: 0.297
[28,     1] loss: 0.199
[29,     1] loss: 0.226
[30,     1] loss: 0.229
[31,     1] loss: 0.269
[32,     1] loss: 0.229
[33,     1] loss: 0.162
[34,     1] loss: 0.284
[35,     1] loss: 0.148
[36,     1] loss: 0.201
[37,     1] loss: 0.206
[38,     1] loss: 0.248
[39,     1] loss: 0.225
[40,     1] loss: 0.267
[41,     1] loss: 0.224
[42,     1] loss: 0.145
[43,     1] loss: 0.153
[44,     1] loss: 0.211
[45,     1] loss: 0.158
[46,     1] loss: 0.166
[47,     1] loss: 0.183
[48,     1] loss: 0.134
[49,     1] loss: 0.135
[50,     1] loss: 0.093
[51,     1] loss: 0.108
[52,     1] loss: 0.082
[53,     1] loss: 0.113
[54,     1] loss: 0.100
[55,     1] loss: 0.088
[56,     1] loss: 0.145
[57,     1] loss: 0.106
[58,     1] loss: 0.138
[59,     1] loss: 0.123
[60,     1] loss: 0.097
[61,     1] loss: 0.134
[62,     1] loss: 0.134
[63,     1] loss: 0.113
[64,     1] loss: 0.074
[65,     1] loss: 0.072
[66,     1] loss: 0.114
[67,     1] loss: 0.076
Early stopping applied (best metric=0.2771330177783966)
Finished Training
Total time taken: 7.323514938354492
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.661
[4,     1] loss: 0.628
[5,     1] loss: 0.592
[6,     1] loss: 0.557
[7,     1] loss: 0.515
[8,     1] loss: 0.511
[9,     1] loss: 0.456
[10,     1] loss: 0.453
[11,     1] loss: 0.391
[12,     1] loss: 0.369
[13,     1] loss: 0.360
[14,     1] loss: 0.291
[15,     1] loss: 0.321
[16,     1] loss: 0.341
[17,     1] loss: 0.248
[18,     1] loss: 0.257
[19,     1] loss: 0.264
[20,     1] loss: 0.273
[21,     1] loss: 0.279
[22,     1] loss: 0.237
[23,     1] loss: 0.206
[24,     1] loss: 0.212
[25,     1] loss: 0.237
[26,     1] loss: 0.231
[27,     1] loss: 0.211
[28,     1] loss: 0.161
[29,     1] loss: 0.172
[30,     1] loss: 0.193
[31,     1] loss: 0.160
[32,     1] loss: 0.178
[33,     1] loss: 0.199
[34,     1] loss: 0.240
[35,     1] loss: 0.186
[36,     1] loss: 0.164
[37,     1] loss: 0.206
[38,     1] loss: 0.363
[39,     1] loss: 0.227
[40,     1] loss: 0.213
[41,     1] loss: 0.227
[42,     1] loss: 0.196
[43,     1] loss: 0.190
[44,     1] loss: 0.227
[45,     1] loss: 0.156
[46,     1] loss: 0.184
[47,     1] loss: 0.135
[48,     1] loss: 0.123
[49,     1] loss: 0.125
[50,     1] loss: 0.116
[51,     1] loss: 0.156
[52,     1] loss: 0.131
[53,     1] loss: 0.101
[54,     1] loss: 0.111
[55,     1] loss: 0.152
[56,     1] loss: 0.162
[57,     1] loss: 0.143
[58,     1] loss: 0.142
[59,     1] loss: 0.124
[60,     1] loss: 0.113
[61,     1] loss: 0.165
Early stopping applied (best metric=0.41349995136260986)
Finished Training
Total time taken: 6.944338798522949
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.700
[3,     1] loss: 0.675
[4,     1] loss: 0.652
[5,     1] loss: 0.613
[6,     1] loss: 0.612
[7,     1] loss: 0.602
[8,     1] loss: 0.565
[9,     1] loss: 0.514
[10,     1] loss: 0.485
[11,     1] loss: 0.480
[12,     1] loss: 0.474
[13,     1] loss: 0.464
[14,     1] loss: 0.461
[15,     1] loss: 0.370
[16,     1] loss: 0.457
[17,     1] loss: 0.339
[18,     1] loss: 0.356
[19,     1] loss: 0.340
[20,     1] loss: 0.355
[21,     1] loss: 0.334
[22,     1] loss: 0.297
[23,     1] loss: 0.358
[24,     1] loss: 0.326
[25,     1] loss: 0.350
[26,     1] loss: 0.270
[27,     1] loss: 0.247
[28,     1] loss: 0.261
[29,     1] loss: 0.282
[30,     1] loss: 0.256
[31,     1] loss: 0.219
[32,     1] loss: 0.281
[33,     1] loss: 0.214
[34,     1] loss: 0.206
[35,     1] loss: 0.221
[36,     1] loss: 0.236
[37,     1] loss: 0.183
[38,     1] loss: 0.207
[39,     1] loss: 0.162
[40,     1] loss: 0.174
[41,     1] loss: 0.192
[42,     1] loss: 0.170
[43,     1] loss: 0.230
[44,     1] loss: 0.190
[45,     1] loss: 0.235
[46,     1] loss: 0.185
[47,     1] loss: 0.214
[48,     1] loss: 0.218
[49,     1] loss: 0.189
[50,     1] loss: 0.183
[51,     1] loss: 0.151
[52,     1] loss: 0.200
[53,     1] loss: 0.211
[54,     1] loss: 0.155
[55,     1] loss: 0.169
[56,     1] loss: 0.155
[57,     1] loss: 0.154
[58,     1] loss: 0.144
[59,     1] loss: 0.131
[60,     1] loss: 0.151
[61,     1] loss: 0.128
[62,     1] loss: 0.153
[63,     1] loss: 0.139
Early stopping applied (best metric=0.41156724095344543)
Finished Training
Total time taken: 7.141191482543945
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.694
[3,     1] loss: 0.677
[4,     1] loss: 0.664
[5,     1] loss: 0.644
[6,     1] loss: 0.611
[7,     1] loss: 0.594
[8,     1] loss: 0.562
[9,     1] loss: 0.556
[10,     1] loss: 0.490
[11,     1] loss: 0.499
[12,     1] loss: 0.432
[13,     1] loss: 0.456
[14,     1] loss: 0.444
[15,     1] loss: 0.397
[16,     1] loss: 0.438
[17,     1] loss: 0.341
[18,     1] loss: 0.378
[19,     1] loss: 0.422
[20,     1] loss: 0.404
[21,     1] loss: 0.370
[22,     1] loss: 0.342
[23,     1] loss: 0.361
[24,     1] loss: 0.408
[25,     1] loss: 0.300
[26,     1] loss: 0.349
[27,     1] loss: 0.305
[28,     1] loss: 0.326
[29,     1] loss: 0.241
[30,     1] loss: 0.250
[31,     1] loss: 0.219
[32,     1] loss: 0.254
[33,     1] loss: 0.311
[34,     1] loss: 0.264
[35,     1] loss: 0.244
[36,     1] loss: 0.238
[37,     1] loss: 0.239
[38,     1] loss: 0.213
[39,     1] loss: 0.241
[40,     1] loss: 0.186
[41,     1] loss: 0.193
[42,     1] loss: 0.185
[43,     1] loss: 0.141
[44,     1] loss: 0.189
[45,     1] loss: 0.192
[46,     1] loss: 0.209
[47,     1] loss: 0.159
[48,     1] loss: 0.120
[49,     1] loss: 0.237
[50,     1] loss: 0.153
[51,     1] loss: 0.188
[52,     1] loss: 0.181
[53,     1] loss: 0.119
[54,     1] loss: 0.155
[55,     1] loss: 0.120
[56,     1] loss: 0.122
Early stopping applied (best metric=0.5159107446670532)
Finished Training
Total time taken: 6.173999786376953
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.665
[4,     1] loss: 0.627
[5,     1] loss: 0.586
[6,     1] loss: 0.559
[7,     1] loss: 0.503
[8,     1] loss: 0.493
[9,     1] loss: 0.431
[10,     1] loss: 0.444
[11,     1] loss: 0.377
[12,     1] loss: 0.388
[13,     1] loss: 0.339
[14,     1] loss: 0.356
[15,     1] loss: 0.295
[16,     1] loss: 0.301
[17,     1] loss: 0.325
[18,     1] loss: 0.248
[19,     1] loss: 0.305
[20,     1] loss: 0.243
[21,     1] loss: 0.217
[22,     1] loss: 0.304
[23,     1] loss: 0.191
[24,     1] loss: 0.347
[25,     1] loss: 0.193
[26,     1] loss: 0.244
[27,     1] loss: 0.233
[28,     1] loss: 0.204
[29,     1] loss: 0.225
[30,     1] loss: 0.230
[31,     1] loss: 0.244
[32,     1] loss: 0.188
[33,     1] loss: 0.186
[34,     1] loss: 0.246
[35,     1] loss: 0.178
[36,     1] loss: 0.158
[37,     1] loss: 0.207
[38,     1] loss: 0.192
[39,     1] loss: 0.190
[40,     1] loss: 0.159
[41,     1] loss: 0.148
[42,     1] loss: 0.140
[43,     1] loss: 0.133
[44,     1] loss: 0.122
[45,     1] loss: 0.114
[46,     1] loss: 0.142
[47,     1] loss: 0.144
[48,     1] loss: 0.141
[49,     1] loss: 0.154
[50,     1] loss: 0.130
[51,     1] loss: 0.128
[52,     1] loss: 0.143
[53,     1] loss: 0.142
[54,     1] loss: 0.109
[55,     1] loss: 0.125
[56,     1] loss: 0.133
Early stopping applied (best metric=0.5029670000076294)
Finished Training
Total time taken: 6.200953960418701
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.682
[3,     1] loss: 0.671
[4,     1] loss: 0.639
[5,     1] loss: 0.604
[6,     1] loss: 0.574
[7,     1] loss: 0.547
[8,     1] loss: 0.512
[9,     1] loss: 0.482
[10,     1] loss: 0.459
[11,     1] loss: 0.396
[12,     1] loss: 0.420
[13,     1] loss: 0.383
[14,     1] loss: 0.349
[15,     1] loss: 0.322
[16,     1] loss: 0.395
[17,     1] loss: 0.328
[18,     1] loss: 0.245
[19,     1] loss: 0.371
[20,     1] loss: 0.319
[21,     1] loss: 0.340
[22,     1] loss: 0.311
[23,     1] loss: 0.291
[24,     1] loss: 0.293
[25,     1] loss: 0.313
[26,     1] loss: 0.263
[27,     1] loss: 0.328
[28,     1] loss: 0.307
[29,     1] loss: 0.335
[30,     1] loss: 0.337
[31,     1] loss: 0.258
[32,     1] loss: 0.246
[33,     1] loss: 0.319
[34,     1] loss: 0.297
[35,     1] loss: 0.270
[36,     1] loss: 0.240
[37,     1] loss: 0.310
[38,     1] loss: 0.288
[39,     1] loss: 0.260
[40,     1] loss: 0.217
[41,     1] loss: 0.216
[42,     1] loss: 0.220
[43,     1] loss: 0.231
[44,     1] loss: 0.208
[45,     1] loss: 0.193
[46,     1] loss: 0.208
[47,     1] loss: 0.166
[48,     1] loss: 0.148
[49,     1] loss: 0.201
[50,     1] loss: 0.170
[51,     1] loss: 0.164
[52,     1] loss: 0.195
[53,     1] loss: 0.242
[54,     1] loss: 0.181
[55,     1] loss: 0.186
[56,     1] loss: 0.173
[57,     1] loss: 0.274
Early stopping applied (best metric=0.4677034020423889)
Finished Training
Total time taken: 6.258570671081543
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.697
[3,     1] loss: 0.672
[4,     1] loss: 0.651
[5,     1] loss: 0.621
[6,     1] loss: 0.583
[7,     1] loss: 0.572
[8,     1] loss: 0.503
[9,     1] loss: 0.491
[10,     1] loss: 0.484
[11,     1] loss: 0.430
[12,     1] loss: 0.412
[13,     1] loss: 0.384
[14,     1] loss: 0.400
[15,     1] loss: 0.377
[16,     1] loss: 0.375
[17,     1] loss: 0.392
[18,     1] loss: 0.284
[19,     1] loss: 0.315
[20,     1] loss: 0.312
[21,     1] loss: 0.295
[22,     1] loss: 0.375
[23,     1] loss: 0.311
[24,     1] loss: 0.235
[25,     1] loss: 0.333
[26,     1] loss: 0.289
[27,     1] loss: 0.254
[28,     1] loss: 0.273
[29,     1] loss: 0.219
[30,     1] loss: 0.228
[31,     1] loss: 0.218
[32,     1] loss: 0.195
[33,     1] loss: 0.174
[34,     1] loss: 0.183
[35,     1] loss: 0.162
[36,     1] loss: 0.132
[37,     1] loss: 0.138
[38,     1] loss: 0.115
[39,     1] loss: 0.129
[40,     1] loss: 0.137
[41,     1] loss: 0.121
[42,     1] loss: 0.134
[43,     1] loss: 0.112
[44,     1] loss: 0.122
[45,     1] loss: 0.141
[46,     1] loss: 0.093
[47,     1] loss: 0.091
[48,     1] loss: 0.147
[49,     1] loss: 0.107
[50,     1] loss: 0.092
[51,     1] loss: 0.126
[52,     1] loss: 0.107
[53,     1] loss: 0.114
[54,     1] loss: 0.152
[55,     1] loss: 0.126
[56,     1] loss: 0.261
[57,     1] loss: 0.195
[58,     1] loss: 0.161
[59,     1] loss: 0.140
[60,     1] loss: 0.209
[61,     1] loss: 0.135
Early stopping applied (best metric=0.3645218312740326)
Finished Training
Total time taken: 6.73052716255188
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.680
[4,     1] loss: 0.661
[5,     1] loss: 0.638
[6,     1] loss: 0.604
[7,     1] loss: 0.582
[8,     1] loss: 0.550
[9,     1] loss: 0.518
[10,     1] loss: 0.503
[11,     1] loss: 0.487
[12,     1] loss: 0.435
[13,     1] loss: 0.404
[14,     1] loss: 0.380
[15,     1] loss: 0.368
[16,     1] loss: 0.312
[17,     1] loss: 0.305
[18,     1] loss: 0.272
[19,     1] loss: 0.325
[20,     1] loss: 0.280
[21,     1] loss: 0.257
[22,     1] loss: 0.233
[23,     1] loss: 0.210
[24,     1] loss: 0.279
[25,     1] loss: 0.250
[26,     1] loss: 0.294
[27,     1] loss: 0.282
[28,     1] loss: 0.231
[29,     1] loss: 0.343
[30,     1] loss: 0.235
[31,     1] loss: 0.241
[32,     1] loss: 0.220
[33,     1] loss: 0.222
[34,     1] loss: 0.207
[35,     1] loss: 0.183
[36,     1] loss: 0.208
[37,     1] loss: 0.172
[38,     1] loss: 0.200
[39,     1] loss: 0.175
[40,     1] loss: 0.199
[41,     1] loss: 0.187
[42,     1] loss: 0.175
[43,     1] loss: 0.148
[44,     1] loss: 0.135
[45,     1] loss: 0.120
[46,     1] loss: 0.168
[47,     1] loss: 0.165
[48,     1] loss: 0.144
[49,     1] loss: 0.186
[50,     1] loss: 0.251
[51,     1] loss: 0.191
[52,     1] loss: 0.217
[53,     1] loss: 0.178
[54,     1] loss: 0.158
[55,     1] loss: 0.168
[56,     1] loss: 0.161
[57,     1] loss: 0.199
[58,     1] loss: 0.147
[59,     1] loss: 0.171
[60,     1] loss: 0.139
Early stopping applied (best metric=0.4518481194972992)
Finished Training
Total time taken: 6.634539365768433
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.696
[3,     1] loss: 0.677
[4,     1] loss: 0.659
[5,     1] loss: 0.628
[6,     1] loss: 0.597
[7,     1] loss: 0.559
[8,     1] loss: 0.518
[9,     1] loss: 0.498
[10,     1] loss: 0.466
[11,     1] loss: 0.404
[12,     1] loss: 0.448
[13,     1] loss: 0.433
[14,     1] loss: 0.386
[15,     1] loss: 0.451
[16,     1] loss: 0.401
[17,     1] loss: 0.449
[18,     1] loss: 0.393
[19,     1] loss: 0.371
[20,     1] loss: 0.359
[21,     1] loss: 0.361
[22,     1] loss: 0.375
[23,     1] loss: 0.342
[24,     1] loss: 0.322
[25,     1] loss: 0.307
[26,     1] loss: 0.291
[27,     1] loss: 0.271
[28,     1] loss: 0.233
[29,     1] loss: 0.225
[30,     1] loss: 0.287
[31,     1] loss: 0.242
[32,     1] loss: 0.223
[33,     1] loss: 0.236
[34,     1] loss: 0.217
[35,     1] loss: 0.258
[36,     1] loss: 0.219
[37,     1] loss: 0.262
[38,     1] loss: 0.199
[39,     1] loss: 0.242
[40,     1] loss: 0.289
[41,     1] loss: 0.302
[42,     1] loss: 0.191
[43,     1] loss: 0.212
[44,     1] loss: 0.194
[45,     1] loss: 0.157
[46,     1] loss: 0.204
[47,     1] loss: 0.174
[48,     1] loss: 0.175
[49,     1] loss: 0.178
[50,     1] loss: 0.179
[51,     1] loss: 0.134
[52,     1] loss: 0.152
[53,     1] loss: 0.129
[54,     1] loss: 0.120
[55,     1] loss: 0.084
[56,     1] loss: 0.091
[57,     1] loss: 0.123
[58,     1] loss: 0.098
[59,     1] loss: 0.115
[60,     1] loss: 0.088
[61,     1] loss: 0.085
[62,     1] loss: 0.082
[63,     1] loss: 0.070
[64,     1] loss: 0.065
[65,     1] loss: 0.083
[66,     1] loss: 0.067
[67,     1] loss: 0.142
[68,     1] loss: 0.079
[69,     1] loss: 0.210
[70,     1] loss: 0.157
[71,     1] loss: 0.080
[72,     1] loss: 0.169
[73,     1] loss: 0.065
Early stopping applied (best metric=0.43099039793014526)
Finished Training
Total time taken: 8.017098426818848
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.682
[4,     1] loss: 0.660
[5,     1] loss: 0.637
[6,     1] loss: 0.597
[7,     1] loss: 0.592
[8,     1] loss: 0.551
[9,     1] loss: 0.528
[10,     1] loss: 0.503
[11,     1] loss: 0.463
[12,     1] loss: 0.428
[13,     1] loss: 0.401
[14,     1] loss: 0.412
[15,     1] loss: 0.384
[16,     1] loss: 0.331
[17,     1] loss: 0.321
[18,     1] loss: 0.309
[19,     1] loss: 0.336
[20,     1] loss: 0.279
[21,     1] loss: 0.308
[22,     1] loss: 0.256
[23,     1] loss: 0.455
[24,     1] loss: 0.265
[25,     1] loss: 0.245
[26,     1] loss: 0.270
[27,     1] loss: 0.413
[28,     1] loss: 0.306
[29,     1] loss: 0.269
[30,     1] loss: 0.298
[31,     1] loss: 0.271
[32,     1] loss: 0.301
[33,     1] loss: 0.264
[34,     1] loss: 0.234
[35,     1] loss: 0.244
[36,     1] loss: 0.267
[37,     1] loss: 0.226
[38,     1] loss: 0.190
[39,     1] loss: 0.219
[40,     1] loss: 0.246
[41,     1] loss: 0.208
[42,     1] loss: 0.212
[43,     1] loss: 0.199
[44,     1] loss: 0.206
[45,     1] loss: 0.176
[46,     1] loss: 0.193
[47,     1] loss: 0.187
[48,     1] loss: 0.183
[49,     1] loss: 0.155
[50,     1] loss: 0.135
[51,     1] loss: 0.210
[52,     1] loss: 0.192
[53,     1] loss: 0.155
[54,     1] loss: 0.171
[55,     1] loss: 0.132
[56,     1] loss: 0.143
[57,     1] loss: 0.173
[58,     1] loss: 0.126
[59,     1] loss: 0.145
[60,     1] loss: 0.152
[61,     1] loss: 0.162
[62,     1] loss: 0.160
[63,     1] loss: 0.147
[64,     1] loss: 0.102
[65,     1] loss: 0.111
[66,     1] loss: 0.134
Early stopping applied (best metric=0.31140586733818054)
Finished Training
Total time taken: 7.245990514755249
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.653
[4,     1] loss: 0.629
[5,     1] loss: 0.573
[6,     1] loss: 0.535
[7,     1] loss: 0.528
[8,     1] loss: 0.479
[9,     1] loss: 0.431
[10,     1] loss: 0.412
[11,     1] loss: 0.392
[12,     1] loss: 0.343
[13,     1] loss: 0.354
[14,     1] loss: 0.372
[15,     1] loss: 0.325
[16,     1] loss: 0.350
[17,     1] loss: 0.277
[18,     1] loss: 0.296
[19,     1] loss: 0.290
[20,     1] loss: 0.229
[21,     1] loss: 0.331
[22,     1] loss: 0.268
[23,     1] loss: 0.256
[24,     1] loss: 0.281
[25,     1] loss: 0.301
[26,     1] loss: 0.264
[27,     1] loss: 0.274
[28,     1] loss: 0.314
[29,     1] loss: 0.248
[30,     1] loss: 0.265
[31,     1] loss: 0.271
[32,     1] loss: 0.303
[33,     1] loss: 0.373
[34,     1] loss: 0.268
[35,     1] loss: 0.236
[36,     1] loss: 0.255
[37,     1] loss: 0.217
[38,     1] loss: 0.230
[39,     1] loss: 0.187
[40,     1] loss: 0.237
[41,     1] loss: 0.224
[42,     1] loss: 0.196
[43,     1] loss: 0.211
[44,     1] loss: 0.151
[45,     1] loss: 0.193
[46,     1] loss: 0.166
[47,     1] loss: 0.152
[48,     1] loss: 0.193
[49,     1] loss: 0.140
[50,     1] loss: 0.125
[51,     1] loss: 0.166
[52,     1] loss: 0.122
[53,     1] loss: 0.124
[54,     1] loss: 0.086
[55,     1] loss: 0.133
[56,     1] loss: 0.118
[57,     1] loss: 0.101
[58,     1] loss: 0.096
[59,     1] loss: 0.124
[60,     1] loss: 0.244
[61,     1] loss: 0.339
[62,     1] loss: 0.348
Early stopping applied (best metric=0.3090819716453552)
Finished Training
Total time taken: 6.844877004623413
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.666
[4,     1] loss: 0.640
[5,     1] loss: 0.597
[6,     1] loss: 0.587
[7,     1] loss: 0.551
[8,     1] loss: 0.510
[9,     1] loss: 0.483
[10,     1] loss: 0.459
[11,     1] loss: 0.425
[12,     1] loss: 0.369
[13,     1] loss: 0.319
[14,     1] loss: 0.291
[15,     1] loss: 0.300
[16,     1] loss: 0.292
[17,     1] loss: 0.267
[18,     1] loss: 0.291
[19,     1] loss: 0.243
[20,     1] loss: 0.236
[21,     1] loss: 0.223
[22,     1] loss: 0.185
[23,     1] loss: 0.203
[24,     1] loss: 0.193
[25,     1] loss: 0.180
[26,     1] loss: 0.168
[27,     1] loss: 0.173
[28,     1] loss: 0.197
[29,     1] loss: 0.152
[30,     1] loss: 0.174
[31,     1] loss: 0.214
[32,     1] loss: 0.155
[33,     1] loss: 0.177
[34,     1] loss: 0.258
[35,     1] loss: 0.182
[36,     1] loss: 0.193
[37,     1] loss: 0.239
[38,     1] loss: 0.191
[39,     1] loss: 0.248
[40,     1] loss: 0.171
[41,     1] loss: 0.226
[42,     1] loss: 0.171
[43,     1] loss: 0.220
[44,     1] loss: 0.229
[45,     1] loss: 0.183
[46,     1] loss: 0.175
[47,     1] loss: 0.278
[48,     1] loss: 0.171
[49,     1] loss: 0.190
[50,     1] loss: 0.254
[51,     1] loss: 0.200
[52,     1] loss: 0.228
[53,     1] loss: 0.215
[54,     1] loss: 0.165
[55,     1] loss: 0.311
[56,     1] loss: 0.170
Early stopping applied (best metric=0.48373064398765564)
Finished Training
Total time taken: 6.166753768920898
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.682
[3,     1] loss: 0.670
[4,     1] loss: 0.643
[5,     1] loss: 0.607
[6,     1] loss: 0.569
[7,     1] loss: 0.565
[8,     1] loss: 0.504
[9,     1] loss: 0.479
[10,     1] loss: 0.470
[11,     1] loss: 0.465
[12,     1] loss: 0.435
[13,     1] loss: 0.392
[14,     1] loss: 0.401
[15,     1] loss: 0.345
[16,     1] loss: 0.330
[17,     1] loss: 0.303
[18,     1] loss: 0.346
[19,     1] loss: 0.243
[20,     1] loss: 0.300
[21,     1] loss: 0.226
[22,     1] loss: 0.258
[23,     1] loss: 0.234
[24,     1] loss: 0.296
[25,     1] loss: 0.241
[26,     1] loss: 0.310
[27,     1] loss: 0.268
[28,     1] loss: 0.227
[29,     1] loss: 0.138
[30,     1] loss: 0.236
[31,     1] loss: 0.193
[32,     1] loss: 0.184
[33,     1] loss: 0.143
[34,     1] loss: 0.132
[35,     1] loss: 0.140
[36,     1] loss: 0.106
[37,     1] loss: 0.153
[38,     1] loss: 0.142
[39,     1] loss: 0.119
[40,     1] loss: 0.257
[41,     1] loss: 0.091
[42,     1] loss: 0.185
[43,     1] loss: 0.111
[44,     1] loss: 0.112
[45,     1] loss: 0.128
[46,     1] loss: 0.166
[47,     1] loss: 0.085
[48,     1] loss: 0.115
[49,     1] loss: 0.118
[50,     1] loss: 0.094
[51,     1] loss: 0.091
[52,     1] loss: 0.099
[53,     1] loss: 0.096
[54,     1] loss: 0.109
[55,     1] loss: 0.116
[56,     1] loss: 0.108
[57,     1] loss: 0.144
[58,     1] loss: 0.137
[59,     1] loss: 0.161
[60,     1] loss: 0.167
[61,     1] loss: 0.092
Early stopping applied (best metric=0.3909992277622223)
Finished Training
Total time taken: 6.730998992919922
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.686
[4,     1] loss: 0.651
[5,     1] loss: 0.635
[6,     1] loss: 0.591
[7,     1] loss: 0.570
[8,     1] loss: 0.562
[9,     1] loss: 0.554
[10,     1] loss: 0.497
[11,     1] loss: 0.467
[12,     1] loss: 0.464
[13,     1] loss: 0.383
[14,     1] loss: 0.357
[15,     1] loss: 0.377
[16,     1] loss: 0.308
[17,     1] loss: 0.286
[18,     1] loss: 0.269
[19,     1] loss: 0.317
[20,     1] loss: 0.326
[21,     1] loss: 0.305
[22,     1] loss: 0.253
[23,     1] loss: 0.297
[24,     1] loss: 0.242
[25,     1] loss: 0.248
[26,     1] loss: 0.266
[27,     1] loss: 0.265
[28,     1] loss: 0.282
[29,     1] loss: 0.292
[30,     1] loss: 0.171
[31,     1] loss: 0.248
[32,     1] loss: 0.191
[33,     1] loss: 0.229
[34,     1] loss: 0.261
[35,     1] loss: 0.243
[36,     1] loss: 0.154
[37,     1] loss: 0.180
[38,     1] loss: 0.205
[39,     1] loss: 0.163
[40,     1] loss: 0.154
[41,     1] loss: 0.165
[42,     1] loss: 0.155
[43,     1] loss: 0.173
[44,     1] loss: 0.110
[45,     1] loss: 0.139
[46,     1] loss: 0.127
[47,     1] loss: 0.105
[48,     1] loss: 0.110
[49,     1] loss: 0.112
[50,     1] loss: 0.104
[51,     1] loss: 0.101
[52,     1] loss: 0.098
[53,     1] loss: 0.085
[54,     1] loss: 0.081
[55,     1] loss: 0.111
[56,     1] loss: 0.068
[57,     1] loss: 0.059
[58,     1] loss: 0.096
[59,     1] loss: 0.121
[60,     1] loss: 0.115
[61,     1] loss: 0.143
[62,     1] loss: 0.242
[63,     1] loss: 0.123
[64,     1] loss: 0.164
[65,     1] loss: 0.080
[66,     1] loss: 0.162
[67,     1] loss: 0.098
[68,     1] loss: 0.090
[69,     1] loss: 0.122
[70,     1] loss: 0.145
[71,     1] loss: 0.122
[72,     1] loss: 0.116
[73,     1] loss: 0.088
Early stopping applied (best metric=0.16168643534183502)
Finished Training
Total time taken: 8.053001880645752
{'Hydroxylation-K Validation Accuracy': 0.8326950354609929, 'Hydroxylation-K Validation Sensitivity': 0.8111111111111111, 'Hydroxylation-K Validation Specificity': 0.8378947368421052, 'Hydroxylation-K Validation Precision': 0.5717492670727965, 'Hydroxylation-K AUC ROC': 0.838233918128655, 'Hydroxylation-K AUC PR': 0.6361617683644555, 'Hydroxylation-K MCC': 0.5792773961639176, 'Hydroxylation-K F1': 0.6652029165785127, 'Validation Loss (Hydroxylation-K)': 0.3500832961499691, 'Validation Loss (total)': 0.3500832961499691, 'TimeToTrain': 7.779409103393554}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003129958964185534,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1940694937,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.023934707774306}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.685
[3,     1] loss: 0.680
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030411819590480758,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1722924958,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.4674827635212}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.687
[3,     1] loss: 0.661
[4,     1] loss: 0.616
[5,     1] loss: 0.566
[6,     1] loss: 0.556
[7,     1] loss: 0.501
[8,     1] loss: 0.471
[9,     1] loss: 0.476
[10,     1] loss: 0.465
[11,     1] loss: 0.392
[12,     1] loss: 0.373
[13,     1] loss: 0.412
[14,     1] loss: 0.396
[15,     1] loss: 0.382
[16,     1] loss: 0.295
[17,     1] loss: 0.319
[18,     1] loss: 0.361
[19,     1] loss: 0.329
[20,     1] loss: 0.362
[21,     1] loss: 0.353
[22,     1] loss: 0.303
[23,     1] loss: 0.293
[24,     1] loss: 0.321
[25,     1] loss: 0.274
[26,     1] loss: 0.303
[27,     1] loss: 0.251
[28,     1] loss: 0.276
[29,     1] loss: 0.253
[30,     1] loss: 0.197
[31,     1] loss: 0.297
[32,     1] loss: 0.231
[33,     1] loss: 0.438
[34,     1] loss: 0.336
[35,     1] loss: 0.339
[36,     1] loss: 0.306
[37,     1] loss: 0.316
[38,     1] loss: 0.308
[39,     1] loss: 0.331
[40,     1] loss: 0.330
[41,     1] loss: 0.313
[42,     1] loss: 0.363
[43,     1] loss: 0.299
[44,     1] loss: 0.319
[45,     1] loss: 0.288
[46,     1] loss: 0.319
[47,     1] loss: 0.276
[48,     1] loss: 0.309
[49,     1] loss: 0.270
[50,     1] loss: 0.302
[51,     1] loss: 0.330
[52,     1] loss: 0.256
[53,     1] loss: 0.303
[54,     1] loss: 0.304
[55,     1] loss: 0.343
[56,     1] loss: 0.292
[57,     1] loss: 0.258
[58,     1] loss: 0.389
[59,     1] loss: 0.305
[60,     1] loss: 0.278
[61,     1] loss: 0.343
[62,     1] loss: 0.308
[63,     1] loss: 0.343
[64,     1] loss: 0.324
[65,     1] loss: 0.274
[66,     1] loss: 0.299
[67,     1] loss: 0.326
[68,     1] loss: 0.300
[69,     1] loss: 0.288
Early stopping applied (best metric=0.3621687591075897)
Finished Training
Total time taken: 7.597524642944336
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.675
[4,     1] loss: 0.643
[5,     1] loss: 0.615
[6,     1] loss: 0.590
[7,     1] loss: 0.573
[8,     1] loss: 0.536
[9,     1] loss: 0.544
[10,     1] loss: 0.533
[11,     1] loss: 0.461
[12,     1] loss: 0.449
[13,     1] loss: 0.430
[14,     1] loss: 0.435
[15,     1] loss: 0.404
[16,     1] loss: 0.369
[17,     1] loss: 0.357
[18,     1] loss: 0.374
[19,     1] loss: 0.368
[20,     1] loss: 0.349
[21,     1] loss: 0.360
[22,     1] loss: 0.300
[23,     1] loss: 0.306
[24,     1] loss: 0.311
[25,     1] loss: 0.354
[26,     1] loss: 0.355
[27,     1] loss: 0.358
[28,     1] loss: 0.294
[29,     1] loss: 0.317
[30,     1] loss: 0.328
[31,     1] loss: 0.333
[32,     1] loss: 0.319
[33,     1] loss: 0.307
[34,     1] loss: 0.272
[35,     1] loss: 0.350
[36,     1] loss: 0.301
[37,     1] loss: 0.257
[38,     1] loss: 0.256
[39,     1] loss: 0.283
[40,     1] loss: 0.296
[41,     1] loss: 0.389
[42,     1] loss: 0.390
[43,     1] loss: 0.328
[44,     1] loss: 0.323
[45,     1] loss: 0.305
[46,     1] loss: 0.239
[47,     1] loss: 0.249
[48,     1] loss: 0.272
[49,     1] loss: 0.213
[50,     1] loss: 0.210
[51,     1] loss: 0.238
[52,     1] loss: 0.198
[53,     1] loss: 0.245
[54,     1] loss: 0.228
[55,     1] loss: 0.254
[56,     1] loss: 0.210
[57,     1] loss: 0.318
[58,     1] loss: 0.373
[59,     1] loss: 0.388
[60,     1] loss: 0.271
[61,     1] loss: 0.260
[62,     1] loss: 0.237
[63,     1] loss: 0.214
[64,     1] loss: 0.241
[65,     1] loss: 0.255
[66,     1] loss: 0.234
[67,     1] loss: 0.241
[68,     1] loss: 0.237
[69,     1] loss: 0.197
[70,     1] loss: 0.219
[71,     1] loss: 0.210
[72,     1] loss: 0.180
[73,     1] loss: 0.213
[74,     1] loss: 0.237
[75,     1] loss: 0.236
[76,     1] loss: 0.258
[77,     1] loss: 0.204
[78,     1] loss: 0.190
[79,     1] loss: 0.190
[80,     1] loss: 0.218
[81,     1] loss: 0.188
[82,     1] loss: 0.165
[83,     1] loss: 0.189
[84,     1] loss: 0.214
[85,     1] loss: 0.221
[86,     1] loss: 0.259
[87,     1] loss: 0.177
[88,     1] loss: 0.269
[89,     1] loss: 0.273
[90,     1] loss: 0.227
[91,     1] loss: 0.196
[92,     1] loss: 0.162
[93,     1] loss: 0.206
[94,     1] loss: 0.205
[95,     1] loss: 0.188
[96,     1] loss: 0.183
[97,     1] loss: 0.221
[98,     1] loss: 0.288
[99,     1] loss: 0.199
[100,     1] loss: 0.220
[101,     1] loss: 0.186
[102,     1] loss: 0.195
[103,     1] loss: 0.245
[104,     1] loss: 0.193
[105,     1] loss: 0.212
[106,     1] loss: 0.222
[107,     1] loss: 0.192
[108,     1] loss: 0.194
[109,     1] loss: 0.161
[110,     1] loss: 0.176
[111,     1] loss: 0.193
[112,     1] loss: 0.181
[113,     1] loss: 0.233
[114,     1] loss: 0.198
[115,     1] loss: 0.213
[116,     1] loss: 0.182
[117,     1] loss: 0.183
[118,     1] loss: 0.175
[119,     1] loss: 0.223
[120,     1] loss: 0.216
[121,     1] loss: 0.177
[122,     1] loss: 0.187
[123,     1] loss: 0.280
[124,     1] loss: 0.200
[125,     1] loss: 0.223
[126,     1] loss: 0.217
[127,     1] loss: 0.209
[128,     1] loss: 0.247
[129,     1] loss: 0.184
[130,     1] loss: 0.194
[131,     1] loss: 0.218
[132,     1] loss: 0.202
[133,     1] loss: 0.218
[134,     1] loss: 0.217
[135,     1] loss: 0.355
[136,     1] loss: 0.253
[137,     1] loss: 0.252
[138,     1] loss: 0.224
[139,     1] loss: 0.266
[140,     1] loss: 0.228
[141,     1] loss: 0.189
[142,     1] loss: 0.219
[143,     1] loss: 0.196
[144,     1] loss: 0.192
[145,     1] loss: 0.212
[146,     1] loss: 0.185
[147,     1] loss: 0.173
[148,     1] loss: 0.201
[149,     1] loss: 0.170
[150,     1] loss: 0.168
[151,     1] loss: 0.203
[152,     1] loss: 0.201
[153,     1] loss: 0.167
[154,     1] loss: 0.168
[155,     1] loss: 0.186
[156,     1] loss: 0.171
[157,     1] loss: 0.166
[158,     1] loss: 0.189
[159,     1] loss: 0.179
[160,     1] loss: 0.190
[161,     1] loss: 0.476
[162,     1] loss: 0.289
[163,     1] loss: 0.185
[164,     1] loss: 0.191
[165,     1] loss: 0.173
[166,     1] loss: 0.255
[167,     1] loss: 0.204
[168,     1] loss: 0.221
[169,     1] loss: 0.214
[170,     1] loss: 0.331
[171,     1] loss: 0.173
[172,     1] loss: 0.261
[173,     1] loss: 0.181
Early stopping applied (best metric=0.2110351175069809)
Finished Training
Total time taken: 18.96099328994751
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.692
[3,     1] loss: 0.692
[4,     1] loss: 0.677
[5,     1] loss: 0.663
[6,     1] loss: 0.651
[7,     1] loss: 0.639
[8,     1] loss: 0.627
[9,     1] loss: 0.597
[10,     1] loss: 0.559
[11,     1] loss: 0.577
[12,     1] loss: 0.531
[13,     1] loss: 0.534
[14,     1] loss: 0.498
[15,     1] loss: 0.502
[16,     1] loss: 0.453
[17,     1] loss: 0.427
[18,     1] loss: 0.442
[19,     1] loss: 0.428
[20,     1] loss: 0.401
[21,     1] loss: 0.420
[22,     1] loss: 0.387
[23,     1] loss: 0.380
[24,     1] loss: 0.429
[25,     1] loss: 0.389
[26,     1] loss: 0.441
[27,     1] loss: 0.378
[28,     1] loss: 0.448
[29,     1] loss: 0.483
[30,     1] loss: 0.378
[31,     1] loss: 0.383
[32,     1] loss: 0.387
[33,     1] loss: 0.381
[34,     1] loss: 0.414
[35,     1] loss: 0.397
[36,     1] loss: 0.363
[37,     1] loss: 0.451
[38,     1] loss: 0.441
[39,     1] loss: 0.361
[40,     1] loss: 0.348
[41,     1] loss: 0.360
[42,     1] loss: 0.343
[43,     1] loss: 0.315
[44,     1] loss: 0.314
[45,     1] loss: 0.336
[46,     1] loss: 0.344
[47,     1] loss: 0.341
[48,     1] loss: 0.296
[49,     1] loss: 0.302
[50,     1] loss: 0.296
[51,     1] loss: 0.309
[52,     1] loss: 0.345
[53,     1] loss: 0.341
[54,     1] loss: 0.298
[55,     1] loss: 0.286
[56,     1] loss: 0.257
[57,     1] loss: 0.256
[58,     1] loss: 0.267
[59,     1] loss: 0.310
[60,     1] loss: 0.252
[61,     1] loss: 0.242
[62,     1] loss: 0.263
[63,     1] loss: 0.272
[64,     1] loss: 0.271
[65,     1] loss: 0.227
[66,     1] loss: 0.262
[67,     1] loss: 0.245
[68,     1] loss: 0.235
[69,     1] loss: 0.228
[70,     1] loss: 0.226
[71,     1] loss: 0.198
[72,     1] loss: 0.212
[73,     1] loss: 0.186
[74,     1] loss: 0.210
[75,     1] loss: 0.373
[76,     1] loss: 0.240
[77,     1] loss: 0.294
[78,     1] loss: 0.251
[79,     1] loss: 0.243
[80,     1] loss: 0.212
[81,     1] loss: 0.200
Early stopping applied (best metric=0.4278855323791504)
Finished Training
Total time taken: 8.89354658126831
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.690
[3,     1] loss: 0.684
[4,     1] loss: 0.682
[5,     1] loss: 0.665
[6,     1] loss: 0.651
[7,     1] loss: 0.636
[8,     1] loss: 0.629
[9,     1] loss: 0.612
[10,     1] loss: 0.597
[11,     1] loss: 0.586
[12,     1] loss: 0.576
[13,     1] loss: 0.541
[14,     1] loss: 0.539
[15,     1] loss: 0.517
[16,     1] loss: 0.503
[17,     1] loss: 0.475
[18,     1] loss: 0.482
[19,     1] loss: 0.451
[20,     1] loss: 0.438
[21,     1] loss: 0.424
[22,     1] loss: 0.404
[23,     1] loss: 0.411
[24,     1] loss: 0.441
[25,     1] loss: 0.374
[26,     1] loss: 0.393
[27,     1] loss: 0.395
[28,     1] loss: 0.371
[29,     1] loss: 0.376
[30,     1] loss: 0.359
[31,     1] loss: 0.341
[32,     1] loss: 0.330
[33,     1] loss: 0.344
[34,     1] loss: 0.358
[35,     1] loss: 0.312
[36,     1] loss: 0.308
[37,     1] loss: 0.344
[38,     1] loss: 0.312
[39,     1] loss: 0.317
[40,     1] loss: 0.307
[41,     1] loss: 0.304
[42,     1] loss: 0.353
[43,     1] loss: 0.409
[44,     1] loss: 0.354
[45,     1] loss: 0.327
[46,     1] loss: 0.364
[47,     1] loss: 0.411
[48,     1] loss: 0.345
[49,     1] loss: 0.380
[50,     1] loss: 0.373
[51,     1] loss: 0.344
[52,     1] loss: 0.379
[53,     1] loss: 0.323
[54,     1] loss: 0.335
[55,     1] loss: 0.407
[56,     1] loss: 0.334
[57,     1] loss: 0.348
[58,     1] loss: 0.344
[59,     1] loss: 0.342
[60,     1] loss: 0.298
[61,     1] loss: 0.380
[62,     1] loss: 0.336
[63,     1] loss: 0.335
[64,     1] loss: 0.316
[65,     1] loss: 0.327
[66,     1] loss: 0.357
[67,     1] loss: 0.344
[68,     1] loss: 0.400
[69,     1] loss: 0.395
[70,     1] loss: 0.305
[71,     1] loss: 0.344
[72,     1] loss: 0.370
[73,     1] loss: 0.367
[74,     1] loss: 0.359
[75,     1] loss: 0.362
[76,     1] loss: 0.327
[77,     1] loss: 0.337
[78,     1] loss: 0.308
[79,     1] loss: 0.322
[80,     1] loss: 0.341
[81,     1] loss: 0.316
[82,     1] loss: 0.315
[83,     1] loss: 0.298
[84,     1] loss: 0.333
[85,     1] loss: 0.332
[86,     1] loss: 0.386
[87,     1] loss: 0.322
[88,     1] loss: 0.295
[89,     1] loss: 0.382
[90,     1] loss: 0.319
[91,     1] loss: 0.308
[92,     1] loss: 0.318
[93,     1] loss: 0.365
[94,     1] loss: 0.324
[95,     1] loss: 0.304
[96,     1] loss: 0.310
[97,     1] loss: 0.318
[98,     1] loss: 0.358
[99,     1] loss: 0.300
[100,     1] loss: 0.309
[101,     1] loss: 0.326
[102,     1] loss: 0.325
[103,     1] loss: 0.346
[104,     1] loss: 0.336
[105,     1] loss: 0.290
[106,     1] loss: 0.317
[107,     1] loss: 0.283
Early stopping applied (best metric=0.4252351224422455)
Finished Training
Total time taken: 11.68052077293396
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.693
[3,     1] loss: 0.690
[4,     1] loss: 0.673
[5,     1] loss: 0.667
[6,     1] loss: 0.643
[7,     1] loss: 0.630
[8,     1] loss: 0.615
[9,     1] loss: 0.587
[10,     1] loss: 0.576
[11,     1] loss: 0.574
[12,     1] loss: 0.555
[13,     1] loss: 0.543
[14,     1] loss: 0.501
[15,     1] loss: 0.505
[16,     1] loss: 0.488
[17,     1] loss: 0.464
[18,     1] loss: 0.432
[19,     1] loss: 0.414
[20,     1] loss: 0.417
[21,     1] loss: 0.414
[22,     1] loss: 0.424
[23,     1] loss: 0.370
[24,     1] loss: 0.382
[25,     1] loss: 0.386
[26,     1] loss: 0.394
[27,     1] loss: 0.328
[28,     1] loss: 0.354
[29,     1] loss: 0.342
[30,     1] loss: 0.319
[31,     1] loss: 0.312
[32,     1] loss: 0.341
[33,     1] loss: 0.322
[34,     1] loss: 0.326
[35,     1] loss: 0.485
[36,     1] loss: 0.397
[37,     1] loss: 0.361
[38,     1] loss: 0.351
[39,     1] loss: 0.363
[40,     1] loss: 0.328
[41,     1] loss: 0.358
[42,     1] loss: 0.350
[43,     1] loss: 0.367
[44,     1] loss: 0.365
[45,     1] loss: 0.302
[46,     1] loss: 0.318
[47,     1] loss: 0.333
[48,     1] loss: 0.322
[49,     1] loss: 0.312
[50,     1] loss: 0.306
[51,     1] loss: 0.310
[52,     1] loss: 0.304
[53,     1] loss: 0.309
[54,     1] loss: 0.286
[55,     1] loss: 0.290
[56,     1] loss: 0.301
[57,     1] loss: 0.335
[58,     1] loss: 0.308
[59,     1] loss: 0.342
[60,     1] loss: 0.316
[61,     1] loss: 0.274
[62,     1] loss: 0.318
[63,     1] loss: 0.313
[64,     1] loss: 0.256
[65,     1] loss: 0.263
[66,     1] loss: 0.232
[67,     1] loss: 0.265
[68,     1] loss: 0.253
[69,     1] loss: 0.239
[70,     1] loss: 0.267
[71,     1] loss: 0.237
[72,     1] loss: 0.321
[73,     1] loss: 0.204
[74,     1] loss: 0.293
[75,     1] loss: 0.275
[76,     1] loss: 0.341
[77,     1] loss: 0.294
[78,     1] loss: 0.289
[79,     1] loss: 0.248
[80,     1] loss: 0.261
[81,     1] loss: 0.235
[82,     1] loss: 0.230
[83,     1] loss: 0.222
[84,     1] loss: 0.223
[85,     1] loss: 0.211
[86,     1] loss: 0.351
[87,     1] loss: 0.255
[88,     1] loss: 0.423
[89,     1] loss: 0.346
[90,     1] loss: 0.319
[91,     1] loss: 0.387
[92,     1] loss: 0.379
[93,     1] loss: 0.306
[94,     1] loss: 0.317
[95,     1] loss: 0.332
[96,     1] loss: 0.337
[97,     1] loss: 0.339
[98,     1] loss: 0.329
[99,     1] loss: 0.312
[100,     1] loss: 0.329
[101,     1] loss: 0.318
[102,     1] loss: 0.313
[103,     1] loss: 0.331
[104,     1] loss: 0.324
[105,     1] loss: 0.303
[106,     1] loss: 0.289
[107,     1] loss: 0.318
[108,     1] loss: 0.254
[109,     1] loss: 0.253
[110,     1] loss: 0.322
[111,     1] loss: 0.290
[112,     1] loss: 0.229
[113,     1] loss: 0.323
[114,     1] loss: 0.378
[115,     1] loss: 0.313
[116,     1] loss: 0.317
[117,     1] loss: 0.381
[118,     1] loss: 0.337
[119,     1] loss: 0.336
[120,     1] loss: 0.308
[121,     1] loss: 0.347
[122,     1] loss: 0.316
[123,     1] loss: 0.309
[124,     1] loss: 0.315
[125,     1] loss: 0.275
[126,     1] loss: 0.279
[127,     1] loss: 0.272
[128,     1] loss: 0.251
[129,     1] loss: 0.239
[130,     1] loss: 0.259
[131,     1] loss: 0.242
[132,     1] loss: 0.259
[133,     1] loss: 0.250
[134,     1] loss: 0.253
[135,     1] loss: 0.231
[136,     1] loss: 0.226
[137,     1] loss: 0.272
[138,     1] loss: 0.245
[139,     1] loss: 0.249
[140,     1] loss: 0.243
[141,     1] loss: 0.234
[142,     1] loss: 0.238
[143,     1] loss: 0.198
[144,     1] loss: 0.256
[145,     1] loss: 0.211
[146,     1] loss: 0.227
[147,     1] loss: 0.224
[148,     1] loss: 0.205
[149,     1] loss: 0.241
[150,     1] loss: 0.219
[151,     1] loss: 0.239
[152,     1] loss: 0.203
[153,     1] loss: 0.324
[154,     1] loss: 0.255
[155,     1] loss: 0.218
[156,     1] loss: 0.252
[157,     1] loss: 0.235
[158,     1] loss: 0.216
[159,     1] loss: 0.272
[160,     1] loss: 0.211
[161,     1] loss: 0.240
[162,     1] loss: 0.228
[163,     1] loss: 0.219
[164,     1] loss: 0.205
[165,     1] loss: 0.187
[166,     1] loss: 0.228
[167,     1] loss: 0.232
[168,     1] loss: 0.230
[169,     1] loss: 0.195
[170,     1] loss: 0.208
[171,     1] loss: 0.207
[172,     1] loss: 0.240
[173,     1] loss: 0.222
[174,     1] loss: 0.245
[175,     1] loss: 0.217
[176,     1] loss: 0.189
[177,     1] loss: 0.231
[178,     1] loss: 0.256
[179,     1] loss: 0.218
[180,     1] loss: 0.182
[181,     1] loss: 0.213
[182,     1] loss: 0.228
Early stopping applied (best metric=0.25006431341171265)
Finished Training
Total time taken: 19.911495208740234
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.681
[4,     1] loss: 0.662
[5,     1] loss: 0.641
[6,     1] loss: 0.619
[7,     1] loss: 0.597
[8,     1] loss: 0.560
[9,     1] loss: 0.539
[10,     1] loss: 0.521
[11,     1] loss: 0.480
[12,     1] loss: 0.453
[13,     1] loss: 0.443
[14,     1] loss: 0.475
[15,     1] loss: 0.460
[16,     1] loss: 0.381
[17,     1] loss: 0.380
[18,     1] loss: 0.419
[19,     1] loss: 0.406
[20,     1] loss: 0.322
[21,     1] loss: 0.402
[22,     1] loss: 0.374
[23,     1] loss: 0.375
[24,     1] loss: 0.319
[25,     1] loss: 0.381
[26,     1] loss: 0.302
[27,     1] loss: 0.321
[28,     1] loss: 0.360
[29,     1] loss: 0.308
[30,     1] loss: 0.333
[31,     1] loss: 0.346
[32,     1] loss: 0.289
[33,     1] loss: 0.274
[34,     1] loss: 0.323
[35,     1] loss: 0.315
[36,     1] loss: 0.316
[37,     1] loss: 0.304
[38,     1] loss: 0.334
[39,     1] loss: 0.308
[40,     1] loss: 0.251
[41,     1] loss: 0.274
[42,     1] loss: 0.283
[43,     1] loss: 0.313
[44,     1] loss: 0.337
[45,     1] loss: 0.313
[46,     1] loss: 0.345
[47,     1] loss: 0.345
[48,     1] loss: 0.303
[49,     1] loss: 0.280
[50,     1] loss: 0.320
[51,     1] loss: 0.261
[52,     1] loss: 0.257
[53,     1] loss: 0.236
[54,     1] loss: 0.256
[55,     1] loss: 0.248
[56,     1] loss: 0.296
[57,     1] loss: 0.274
[58,     1] loss: 0.272
[59,     1] loss: 0.257
[60,     1] loss: 0.236
[61,     1] loss: 0.259
[62,     1] loss: 0.270
[63,     1] loss: 0.213
[64,     1] loss: 0.241
[65,     1] loss: 0.221
[66,     1] loss: 0.237
[67,     1] loss: 0.214
[68,     1] loss: 0.200
[69,     1] loss: 0.283
[70,     1] loss: 0.210
[71,     1] loss: 0.240
[72,     1] loss: 0.289
[73,     1] loss: 0.217
[74,     1] loss: 0.194
[75,     1] loss: 0.164
[76,     1] loss: 0.336
[77,     1] loss: 0.475
[78,     1] loss: 0.327
[79,     1] loss: 0.367
[80,     1] loss: 0.273
[81,     1] loss: 0.289
[82,     1] loss: 0.268
[83,     1] loss: 0.275
[84,     1] loss: 0.292
[85,     1] loss: 0.258
[86,     1] loss: 0.278
[87,     1] loss: 0.295
[88,     1] loss: 0.285
[89,     1] loss: 0.237
[90,     1] loss: 0.277
[91,     1] loss: 0.262
[92,     1] loss: 0.281
[93,     1] loss: 0.269
[94,     1] loss: 0.242
[95,     1] loss: 0.262
[96,     1] loss: 0.259
[97,     1] loss: 0.209
[98,     1] loss: 0.224
[99,     1] loss: 0.245
[100,     1] loss: 0.229
[101,     1] loss: 0.219
[102,     1] loss: 0.222
[103,     1] loss: 0.241
[104,     1] loss: 0.239
[105,     1] loss: 0.204
[106,     1] loss: 0.237
[107,     1] loss: 0.217
[108,     1] loss: 0.260
[109,     1] loss: 0.276
[110,     1] loss: 0.253
[111,     1] loss: 0.257
[112,     1] loss: 0.180
[113,     1] loss: 0.216
[114,     1] loss: 0.215
[115,     1] loss: 0.216
[116,     1] loss: 0.192
[117,     1] loss: 0.199
[118,     1] loss: 0.237
[119,     1] loss: 0.170
[120,     1] loss: 0.191
[121,     1] loss: 0.207
[122,     1] loss: 0.193
[123,     1] loss: 0.202
[124,     1] loss: 0.212
[125,     1] loss: 0.184
[126,     1] loss: 0.184
[127,     1] loss: 0.195
[128,     1] loss: 0.216
[129,     1] loss: 0.174
[130,     1] loss: 0.185
[131,     1] loss: 0.197
[132,     1] loss: 0.241
[133,     1] loss: 0.358
[134,     1] loss: 0.251
[135,     1] loss: 0.399
[136,     1] loss: 0.238
[137,     1] loss: 0.307
[138,     1] loss: 0.290
[139,     1] loss: 0.262
[140,     1] loss: 0.313
[141,     1] loss: 0.251
[142,     1] loss: 0.251
[143,     1] loss: 0.248
[144,     1] loss: 0.245
[145,     1] loss: 0.280
[146,     1] loss: 0.214
[147,     1] loss: 0.248
[148,     1] loss: 0.253
[149,     1] loss: 0.229
[150,     1] loss: 0.227
[151,     1] loss: 0.209
[152,     1] loss: 0.194
[153,     1] loss: 0.232
[154,     1] loss: 0.235
[155,     1] loss: 0.220
[156,     1] loss: 0.231
[157,     1] loss: 0.207
[158,     1] loss: 0.265
[159,     1] loss: 0.251
[160,     1] loss: 0.248
[161,     1] loss: 0.222
[162,     1] loss: 0.245
[163,     1] loss: 0.275
[164,     1] loss: 0.253
[165,     1] loss: 0.271
[166,     1] loss: 0.211
[167,     1] loss: 0.200
[168,     1] loss: 0.213
[169,     1] loss: 0.186
[170,     1] loss: 0.217
[171,     1] loss: 0.207
[172,     1] loss: 0.197
[173,     1] loss: 0.183
[174,     1] loss: 0.170
[175,     1] loss: 0.180
[176,     1] loss: 0.181
[177,     1] loss: 0.191
[178,     1] loss: 0.206
[179,     1] loss: 0.196
[180,     1] loss: 0.211
[181,     1] loss: 0.401
[182,     1] loss: 0.308
[183,     1] loss: 0.271
[184,     1] loss: 0.286
[185,     1] loss: 0.262
[186,     1] loss: 0.233
[187,     1] loss: 0.257
[188,     1] loss: 0.246
[189,     1] loss: 0.241
[190,     1] loss: 0.258
[191,     1] loss: 0.201
[192,     1] loss: 0.207
[193,     1] loss: 0.220
[194,     1] loss: 0.213
[195,     1] loss: 0.180
[196,     1] loss: 0.197
[197,     1] loss: 0.201
[198,     1] loss: 0.208
[199,     1] loss: 0.241
[200,     1] loss: 0.226
Finished Training
Total time taken: 21.969748735427856
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.665
[4,     1] loss: 0.649
[5,     1] loss: 0.621
[6,     1] loss: 0.604
[7,     1] loss: 0.612
[8,     1] loss: 0.584
[9,     1] loss: 0.564
[10,     1] loss: 0.531
[11,     1] loss: 0.522
[12,     1] loss: 0.468
[13,     1] loss: 0.465
[14,     1] loss: 0.432
[15,     1] loss: 0.430
[16,     1] loss: 0.421
[17,     1] loss: 0.390
[18,     1] loss: 0.425
[19,     1] loss: 0.393
[20,     1] loss: 0.431
[21,     1] loss: 0.355
[22,     1] loss: 0.343
[23,     1] loss: 0.356
[24,     1] loss: 0.339
[25,     1] loss: 0.331
[26,     1] loss: 0.329
[27,     1] loss: 0.297
[28,     1] loss: 0.283
[29,     1] loss: 0.246
[30,     1] loss: 0.235
[31,     1] loss: 0.334
[32,     1] loss: 0.243
[33,     1] loss: 0.313
[34,     1] loss: 0.198
[35,     1] loss: 0.247
[36,     1] loss: 0.213
[37,     1] loss: 0.227
[38,     1] loss: 0.261
[39,     1] loss: 0.224
[40,     1] loss: 0.215
[41,     1] loss: 0.215
[42,     1] loss: 0.217
[43,     1] loss: 0.195
[44,     1] loss: 0.199
[45,     1] loss: 0.263
[46,     1] loss: 0.304
[47,     1] loss: 0.248
[48,     1] loss: 0.210
[49,     1] loss: 0.245
[50,     1] loss: 0.191
[51,     1] loss: 0.242
[52,     1] loss: 0.185
[53,     1] loss: 0.185
[54,     1] loss: 0.176
[55,     1] loss: 0.176
[56,     1] loss: 0.166
[57,     1] loss: 0.164
[58,     1] loss: 0.165
[59,     1] loss: 0.185
[60,     1] loss: 0.171
[61,     1] loss: 0.160
[62,     1] loss: 0.168
[63,     1] loss: 0.227
[64,     1] loss: 0.135
[65,     1] loss: 0.150
[66,     1] loss: 0.142
[67,     1] loss: 0.151
[68,     1] loss: 0.124
Early stopping applied (best metric=0.3772084414958954)
Finished Training
Total time taken: 7.461395740509033
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.685
[3,     1] loss: 0.670
[4,     1] loss: 0.631
[5,     1] loss: 0.618
[6,     1] loss: 0.592
[7,     1] loss: 0.597
[8,     1] loss: 0.557
[9,     1] loss: 0.516
[10,     1] loss: 0.483
[11,     1] loss: 0.478
[12,     1] loss: 0.460
[13,     1] loss: 0.468
[14,     1] loss: 0.459
[15,     1] loss: 0.383
[16,     1] loss: 0.427
[17,     1] loss: 0.418
[18,     1] loss: 0.379
[19,     1] loss: 0.461
[20,     1] loss: 0.463
[21,     1] loss: 0.421
[22,     1] loss: 0.411
[23,     1] loss: 0.375
[24,     1] loss: 0.338
[25,     1] loss: 0.377
[26,     1] loss: 0.339
[27,     1] loss: 0.375
[28,     1] loss: 0.383
[29,     1] loss: 0.385
[30,     1] loss: 0.306
[31,     1] loss: 0.406
[32,     1] loss: 0.293
[33,     1] loss: 0.342
[34,     1] loss: 0.359
[35,     1] loss: 0.358
[36,     1] loss: 0.341
[37,     1] loss: 0.345
[38,     1] loss: 0.336
[39,     1] loss: 0.340
[40,     1] loss: 0.315
[41,     1] loss: 0.295
[42,     1] loss: 0.310
[43,     1] loss: 0.351
[44,     1] loss: 0.352
[45,     1] loss: 0.393
[46,     1] loss: 0.316
[47,     1] loss: 0.358
[48,     1] loss: 0.321
[49,     1] loss: 0.319
[50,     1] loss: 0.297
[51,     1] loss: 0.357
[52,     1] loss: 0.314
[53,     1] loss: 0.297
[54,     1] loss: 0.262
[55,     1] loss: 0.307
[56,     1] loss: 0.298
[57,     1] loss: 0.306
[58,     1] loss: 0.297
[59,     1] loss: 0.236
[60,     1] loss: 0.340
[61,     1] loss: 0.317
[62,     1] loss: 0.265
[63,     1] loss: 0.270
[64,     1] loss: 0.273
[65,     1] loss: 0.284
[66,     1] loss: 0.274
[67,     1] loss: 0.272
[68,     1] loss: 0.249
[69,     1] loss: 0.278
[70,     1] loss: 0.210
[71,     1] loss: 0.209
[72,     1] loss: 0.238
[73,     1] loss: 0.456
[74,     1] loss: 0.402
[75,     1] loss: 0.402
[76,     1] loss: 0.277
[77,     1] loss: 0.359
[78,     1] loss: 0.322
[79,     1] loss: 0.291
[80,     1] loss: 0.342
[81,     1] loss: 0.276
[82,     1] loss: 0.274
[83,     1] loss: 0.247
[84,     1] loss: 0.259
[85,     1] loss: 0.250
[86,     1] loss: 0.205
[87,     1] loss: 0.211
[88,     1] loss: 0.224
[89,     1] loss: 0.175
[90,     1] loss: 0.214
[91,     1] loss: 0.215
[92,     1] loss: 0.190
[93,     1] loss: 0.206
[94,     1] loss: 0.182
[95,     1] loss: 0.202
[96,     1] loss: 0.193
[97,     1] loss: 0.139
[98,     1] loss: 0.199
[99,     1] loss: 0.192
[100,     1] loss: 0.246
[101,     1] loss: 0.182
[102,     1] loss: 0.145
[103,     1] loss: 0.158
[104,     1] loss: 0.165
[105,     1] loss: 0.200
[106,     1] loss: 0.159
[107,     1] loss: 0.141
[108,     1] loss: 0.150
[109,     1] loss: 0.222
[110,     1] loss: 0.194
[111,     1] loss: 0.175
[112,     1] loss: 0.282
[113,     1] loss: 0.194
[114,     1] loss: 0.169
[115,     1] loss: 0.172
[116,     1] loss: 0.245
[117,     1] loss: 0.229
[118,     1] loss: 0.235
[119,     1] loss: 0.236
[120,     1] loss: 0.207
[121,     1] loss: 0.171
[122,     1] loss: 0.166
[123,     1] loss: 0.184
[124,     1] loss: 0.171
[125,     1] loss: 0.153
[126,     1] loss: 0.182
[127,     1] loss: 0.155
[128,     1] loss: 0.182
[129,     1] loss: 0.182
[130,     1] loss: 0.188
[131,     1] loss: 0.165
[132,     1] loss: 0.168
[133,     1] loss: 0.152
[134,     1] loss: 0.172
[135,     1] loss: 0.131
[136,     1] loss: 0.122
[137,     1] loss: 0.122
[138,     1] loss: 0.140
[139,     1] loss: 0.128
[140,     1] loss: 0.134
[141,     1] loss: 0.146
[142,     1] loss: 0.139
[143,     1] loss: 0.122
[144,     1] loss: 0.177
[145,     1] loss: 0.368
[146,     1] loss: 0.256
[147,     1] loss: 0.299
[148,     1] loss: 0.292
[149,     1] loss: 0.180
[150,     1] loss: 0.273
[151,     1] loss: 0.189
[152,     1] loss: 0.161
[153,     1] loss: 0.181
[154,     1] loss: 0.191
Early stopping applied (best metric=0.23898528516292572)
Finished Training
Total time taken: 16.9816677570343
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.680
[4,     1] loss: 0.663
[5,     1] loss: 0.648
[6,     1] loss: 0.610
[7,     1] loss: 0.583
[8,     1] loss: 0.559
[9,     1] loss: 0.529
[10,     1] loss: 0.483
[11,     1] loss: 0.450
[12,     1] loss: 0.444
[13,     1] loss: 0.426
[14,     1] loss: 0.412
[15,     1] loss: 0.426
[16,     1] loss: 0.370
[17,     1] loss: 0.338
[18,     1] loss: 0.330
[19,     1] loss: 0.306
[20,     1] loss: 0.340
[21,     1] loss: 0.324
[22,     1] loss: 0.291
[23,     1] loss: 0.266
[24,     1] loss: 0.290
[25,     1] loss: 0.281
[26,     1] loss: 0.255
[27,     1] loss: 0.261
[28,     1] loss: 0.220
[29,     1] loss: 0.271
[30,     1] loss: 0.297
[31,     1] loss: 0.328
[32,     1] loss: 0.230
[33,     1] loss: 0.264
[34,     1] loss: 0.243
[35,     1] loss: 0.267
[36,     1] loss: 0.273
[37,     1] loss: 0.268
[38,     1] loss: 0.306
[39,     1] loss: 0.252
[40,     1] loss: 0.239
[41,     1] loss: 0.237
[42,     1] loss: 0.237
[43,     1] loss: 0.215
[44,     1] loss: 0.199
[45,     1] loss: 0.217
[46,     1] loss: 0.210
[47,     1] loss: 0.193
[48,     1] loss: 0.240
[49,     1] loss: 0.200
[50,     1] loss: 0.241
[51,     1] loss: 0.195
[52,     1] loss: 0.180
[53,     1] loss: 0.200
[54,     1] loss: 0.196
[55,     1] loss: 0.163
[56,     1] loss: 0.153
[57,     1] loss: 0.190
[58,     1] loss: 0.194
[59,     1] loss: 0.190
[60,     1] loss: 0.170
[61,     1] loss: 0.197
[62,     1] loss: 0.175
[63,     1] loss: 0.255
Early stopping applied (best metric=0.4055551588535309)
Finished Training
Total time taken: 6.99013352394104
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.694
[3,     1] loss: 0.689
[4,     1] loss: 0.679
[5,     1] loss: 0.663
[6,     1] loss: 0.643
[7,     1] loss: 0.629
[8,     1] loss: 0.610
[9,     1] loss: 0.594
[10,     1] loss: 0.592
[11,     1] loss: 0.551
[12,     1] loss: 0.523
[13,     1] loss: 0.511
[14,     1] loss: 0.475
[15,     1] loss: 0.474
[16,     1] loss: 0.425
[17,     1] loss: 0.421
[18,     1] loss: 0.400
[19,     1] loss: 0.416
[20,     1] loss: 0.376
[21,     1] loss: 0.378
[22,     1] loss: 0.398
[23,     1] loss: 0.353
[24,     1] loss: 0.412
[25,     1] loss: 0.365
[26,     1] loss: 0.394
[27,     1] loss: 0.409
[28,     1] loss: 0.371
[29,     1] loss: 0.353
[30,     1] loss: 0.296
[31,     1] loss: 0.284
[32,     1] loss: 0.310
[33,     1] loss: 0.306
[34,     1] loss: 0.296
[35,     1] loss: 0.306
[36,     1] loss: 0.316
[37,     1] loss: 0.286
[38,     1] loss: 0.428
[39,     1] loss: 0.301
[40,     1] loss: 0.342
[41,     1] loss: 0.407
[42,     1] loss: 0.299
[43,     1] loss: 0.304
[44,     1] loss: 0.330
[45,     1] loss: 0.268
[46,     1] loss: 0.307
[47,     1] loss: 0.325
[48,     1] loss: 0.322
[49,     1] loss: 0.316
[50,     1] loss: 0.288
[51,     1] loss: 0.332
[52,     1] loss: 0.292
[53,     1] loss: 0.249
[54,     1] loss: 0.294
[55,     1] loss: 0.249
[56,     1] loss: 0.314
[57,     1] loss: 0.273
[58,     1] loss: 0.283
[59,     1] loss: 0.255
[60,     1] loss: 0.267
[61,     1] loss: 0.240
[62,     1] loss: 0.239
[63,     1] loss: 0.242
[64,     1] loss: 0.249
[65,     1] loss: 0.268
[66,     1] loss: 0.279
[67,     1] loss: 0.232
[68,     1] loss: 0.287
[69,     1] loss: 0.282
[70,     1] loss: 0.211
[71,     1] loss: 0.234
[72,     1] loss: 0.245
[73,     1] loss: 0.249
[74,     1] loss: 0.187
[75,     1] loss: 0.246
[76,     1] loss: 0.357
[77,     1] loss: 0.329
[78,     1] loss: 0.332
[79,     1] loss: 0.301
[80,     1] loss: 0.331
[81,     1] loss: 0.297
Early stopping applied (best metric=0.3388579785823822)
Finished Training
Total time taken: 8.991571187973022
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.687
[3,     1] loss: 0.682
[4,     1] loss: 0.669
[5,     1] loss: 0.652
[6,     1] loss: 0.627
[7,     1] loss: 0.632
[8,     1] loss: 0.594
[9,     1] loss: 0.579
[10,     1] loss: 0.554
[11,     1] loss: 0.547
[12,     1] loss: 0.487
[13,     1] loss: 0.478
[14,     1] loss: 0.448
[15,     1] loss: 0.454
[16,     1] loss: 0.432
[17,     1] loss: 0.421
[18,     1] loss: 0.395
[19,     1] loss: 0.420
[20,     1] loss: 0.382
[21,     1] loss: 0.379
[22,     1] loss: 0.375
[23,     1] loss: 0.348
[24,     1] loss: 0.361
[25,     1] loss: 0.364
[26,     1] loss: 0.401
[27,     1] loss: 0.340
[28,     1] loss: 0.425
[29,     1] loss: 0.361
[30,     1] loss: 0.381
[31,     1] loss: 0.420
[32,     1] loss: 0.297
[33,     1] loss: 0.301
[34,     1] loss: 0.281
[35,     1] loss: 0.279
[36,     1] loss: 0.285
[37,     1] loss: 0.375
[38,     1] loss: 0.307
[39,     1] loss: 0.291
[40,     1] loss: 0.291
[41,     1] loss: 0.295
[42,     1] loss: 0.270
[43,     1] loss: 0.311
[44,     1] loss: 0.263
[45,     1] loss: 0.280
[46,     1] loss: 0.284
[47,     1] loss: 0.271
[48,     1] loss: 0.295
[49,     1] loss: 0.270
[50,     1] loss: 0.332
[51,     1] loss: 0.250
[52,     1] loss: 0.275
[53,     1] loss: 0.293
[54,     1] loss: 0.254
[55,     1] loss: 0.245
[56,     1] loss: 0.275
[57,     1] loss: 0.249
[58,     1] loss: 0.243
[59,     1] loss: 0.226
[60,     1] loss: 0.254
[61,     1] loss: 0.216
[62,     1] loss: 0.186
[63,     1] loss: 0.237
[64,     1] loss: 0.237
[65,     1] loss: 0.233
[66,     1] loss: 0.199
[67,     1] loss: 0.236
[68,     1] loss: 0.363
[69,     1] loss: 0.265
[70,     1] loss: 0.295
[71,     1] loss: 0.232
Early stopping applied (best metric=0.3304699659347534)
Finished Training
Total time taken: 8.010601997375488
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.694
[3,     1] loss: 0.675
[4,     1] loss: 0.651
[5,     1] loss: 0.626
[6,     1] loss: 0.591
[7,     1] loss: 0.545
[8,     1] loss: 0.551
[9,     1] loss: 0.523
[10,     1] loss: 0.481
[11,     1] loss: 0.442
[12,     1] loss: 0.416
[13,     1] loss: 0.433
[14,     1] loss: 0.409
[15,     1] loss: 0.386
[16,     1] loss: 0.328
[17,     1] loss: 0.314
[18,     1] loss: 0.366
[19,     1] loss: 0.338
[20,     1] loss: 0.313
[21,     1] loss: 0.270
[22,     1] loss: 0.305
[23,     1] loss: 0.329
[24,     1] loss: 0.292
[25,     1] loss: 0.269
[26,     1] loss: 0.301
[27,     1] loss: 0.254
[28,     1] loss: 0.302
[29,     1] loss: 0.250
[30,     1] loss: 0.255
[31,     1] loss: 0.343
[32,     1] loss: 0.287
[33,     1] loss: 0.299
[34,     1] loss: 0.276
[35,     1] loss: 0.331
[36,     1] loss: 0.275
[37,     1] loss: 0.277
[38,     1] loss: 0.274
[39,     1] loss: 0.247
[40,     1] loss: 0.247
[41,     1] loss: 0.228
[42,     1] loss: 0.211
[43,     1] loss: 0.206
[44,     1] loss: 0.241
[45,     1] loss: 0.184
[46,     1] loss: 0.183
[47,     1] loss: 0.175
[48,     1] loss: 0.206
[49,     1] loss: 0.282
[50,     1] loss: 0.297
[51,     1] loss: 0.241
[52,     1] loss: 0.273
[53,     1] loss: 0.225
[54,     1] loss: 0.280
[55,     1] loss: 0.278
[56,     1] loss: 0.193
[57,     1] loss: 0.214
[58,     1] loss: 0.243
[59,     1] loss: 0.246
[60,     1] loss: 0.253
[61,     1] loss: 0.220
[62,     1] loss: 0.199
[63,     1] loss: 0.197
[64,     1] loss: 0.206
[65,     1] loss: 0.201
[66,     1] loss: 0.180
[67,     1] loss: 0.193
[68,     1] loss: 0.186
[69,     1] loss: 0.157
[70,     1] loss: 0.153
[71,     1] loss: 0.160
[72,     1] loss: 0.204
[73,     1] loss: 0.291
[74,     1] loss: 0.305
[75,     1] loss: 0.290
[76,     1] loss: 0.296
[77,     1] loss: 0.291
[78,     1] loss: 0.365
[79,     1] loss: 0.310
[80,     1] loss: 0.324
[81,     1] loss: 0.336
[82,     1] loss: 0.285
[83,     1] loss: 0.309
[84,     1] loss: 0.278
[85,     1] loss: 0.279
[86,     1] loss: 0.264
[87,     1] loss: 0.290
[88,     1] loss: 0.281
[89,     1] loss: 0.272
[90,     1] loss: 0.220
[91,     1] loss: 0.267
[92,     1] loss: 0.280
[93,     1] loss: 0.261
[94,     1] loss: 0.283
[95,     1] loss: 0.261
[96,     1] loss: 0.230
[97,     1] loss: 0.300
[98,     1] loss: 0.310
[99,     1] loss: 0.238
[100,     1] loss: 0.232
[101,     1] loss: 0.195
[102,     1] loss: 0.183
[103,     1] loss: 0.222
[104,     1] loss: 0.231
[105,     1] loss: 0.198
[106,     1] loss: 0.219
[107,     1] loss: 0.207
[108,     1] loss: 0.163
[109,     1] loss: 0.242
[110,     1] loss: 0.167
[111,     1] loss: 0.231
[112,     1] loss: 0.282
[113,     1] loss: 0.304
[114,     1] loss: 0.223
[115,     1] loss: 0.277
[116,     1] loss: 0.185
[117,     1] loss: 0.205
[118,     1] loss: 0.218
[119,     1] loss: 0.193
[120,     1] loss: 0.193
[121,     1] loss: 0.184
[122,     1] loss: 0.207
[123,     1] loss: 0.205
[124,     1] loss: 0.170
[125,     1] loss: 0.196
[126,     1] loss: 0.181
[127,     1] loss: 0.219
[128,     1] loss: 0.175
[129,     1] loss: 0.180
[130,     1] loss: 0.175
[131,     1] loss: 0.209
[132,     1] loss: 0.161
[133,     1] loss: 0.205
[134,     1] loss: 0.268
[135,     1] loss: 0.167
[136,     1] loss: 0.229
[137,     1] loss: 0.175
[138,     1] loss: 0.162
[139,     1] loss: 0.168
[140,     1] loss: 0.269
[141,     1] loss: 0.214
[142,     1] loss: 0.202
[143,     1] loss: 0.195
[144,     1] loss: 0.211
[145,     1] loss: 0.215
[146,     1] loss: 0.176
[147,     1] loss: 0.165
[148,     1] loss: 0.164
[149,     1] loss: 0.152
[150,     1] loss: 0.193
[151,     1] loss: 0.269
[152,     1] loss: 0.170
[153,     1] loss: 0.232
[154,     1] loss: 0.240
[155,     1] loss: 0.170
[156,     1] loss: 0.223
[157,     1] loss: 0.270
[158,     1] loss: 0.248
Early stopping applied (best metric=0.26475486159324646)
Finished Training
Total time taken: 17.467854499816895
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.671
[4,     1] loss: 0.639
[5,     1] loss: 0.601
[6,     1] loss: 0.593
[7,     1] loss: 0.556
[8,     1] loss: 0.521
[9,     1] loss: 0.496
[10,     1] loss: 0.471
[11,     1] loss: 0.460
[12,     1] loss: 0.442
[13,     1] loss: 0.400
[14,     1] loss: 0.398
[15,     1] loss: 0.394
[16,     1] loss: 0.386
[17,     1] loss: 0.418
[18,     1] loss: 0.392
[19,     1] loss: 0.453
[20,     1] loss: 0.429
[21,     1] loss: 0.373
[22,     1] loss: 0.373
[23,     1] loss: 0.377
[24,     1] loss: 0.384
[25,     1] loss: 0.430
[26,     1] loss: 0.370
[27,     1] loss: 0.358
[28,     1] loss: 0.350
[29,     1] loss: 0.339
[30,     1] loss: 0.311
[31,     1] loss: 0.325
[32,     1] loss: 0.320
[33,     1] loss: 0.323
[34,     1] loss: 0.310
[35,     1] loss: 0.316
[36,     1] loss: 0.335
[37,     1] loss: 0.387
[38,     1] loss: 0.294
[39,     1] loss: 0.295
[40,     1] loss: 0.283
[41,     1] loss: 0.292
[42,     1] loss: 0.270
[43,     1] loss: 0.231
[44,     1] loss: 0.267
[45,     1] loss: 0.238
[46,     1] loss: 0.221
[47,     1] loss: 0.302
[48,     1] loss: 0.432
[49,     1] loss: 0.304
[50,     1] loss: 0.356
[51,     1] loss: 0.369
[52,     1] loss: 0.317
[53,     1] loss: 0.380
[54,     1] loss: 0.312
[55,     1] loss: 0.354
[56,     1] loss: 0.357
[57,     1] loss: 0.347
[58,     1] loss: 0.321
[59,     1] loss: 0.287
[60,     1] loss: 0.294
[61,     1] loss: 0.299
[62,     1] loss: 0.283
[63,     1] loss: 0.293
[64,     1] loss: 0.288
[65,     1] loss: 0.278
[66,     1] loss: 0.243
[67,     1] loss: 0.278
[68,     1] loss: 0.269
[69,     1] loss: 0.234
[70,     1] loss: 0.242
[71,     1] loss: 0.223
[72,     1] loss: 0.231
[73,     1] loss: 0.201
[74,     1] loss: 0.210
[75,     1] loss: 0.199
[76,     1] loss: 0.226
[77,     1] loss: 0.250
[78,     1] loss: 0.234
[79,     1] loss: 0.211
[80,     1] loss: 0.205
[81,     1] loss: 0.210
[82,     1] loss: 0.234
[83,     1] loss: 0.175
[84,     1] loss: 0.198
[85,     1] loss: 0.225
[86,     1] loss: 0.184
[87,     1] loss: 0.163
[88,     1] loss: 0.190
[89,     1] loss: 0.181
[90,     1] loss: 0.264
[91,     1] loss: 0.225
[92,     1] loss: 0.227
[93,     1] loss: 0.243
[94,     1] loss: 0.209
[95,     1] loss: 0.208
[96,     1] loss: 0.209
[97,     1] loss: 0.227
[98,     1] loss: 0.231
[99,     1] loss: 0.202
[100,     1] loss: 0.196
[101,     1] loss: 0.184
[102,     1] loss: 0.193
[103,     1] loss: 0.199
[104,     1] loss: 0.215
[105,     1] loss: 0.213
[106,     1] loss: 0.207
[107,     1] loss: 0.297
[108,     1] loss: 0.189
[109,     1] loss: 0.356
[110,     1] loss: 0.190
[111,     1] loss: 0.342
[112,     1] loss: 0.253
[113,     1] loss: 0.212
[114,     1] loss: 0.229
[115,     1] loss: 0.242
[116,     1] loss: 0.212
[117,     1] loss: 0.187
[118,     1] loss: 0.191
[119,     1] loss: 0.166
[120,     1] loss: 0.168
[121,     1] loss: 0.165
[122,     1] loss: 0.152
[123,     1] loss: 0.210
[124,     1] loss: 0.149
[125,     1] loss: 0.175
[126,     1] loss: 0.130
[127,     1] loss: 0.194
[128,     1] loss: 0.164
[129,     1] loss: 0.236
[130,     1] loss: 0.281
[131,     1] loss: 0.250
[132,     1] loss: 0.298
[133,     1] loss: 0.265
[134,     1] loss: 0.263
[135,     1] loss: 0.251
[136,     1] loss: 0.227
[137,     1] loss: 0.251
[138,     1] loss: 0.236
[139,     1] loss: 0.255
[140,     1] loss: 0.225
[141,     1] loss: 0.226
[142,     1] loss: 0.196
[143,     1] loss: 0.200
[144,     1] loss: 0.205
[145,     1] loss: 0.184
[146,     1] loss: 0.178
[147,     1] loss: 0.169
[148,     1] loss: 0.148
[149,     1] loss: 0.173
[150,     1] loss: 0.235
[151,     1] loss: 0.254
[152,     1] loss: 0.210
[153,     1] loss: 0.169
[154,     1] loss: 0.147
[155,     1] loss: 0.163
[156,     1] loss: 0.211
[157,     1] loss: 0.214
[158,     1] loss: 0.203
[159,     1] loss: 0.188
[160,     1] loss: 0.168
[161,     1] loss: 0.172
[162,     1] loss: 0.170
[163,     1] loss: 0.202
[164,     1] loss: 0.248
[165,     1] loss: 0.292
[166,     1] loss: 0.203
[167,     1] loss: 0.196
[168,     1] loss: 0.200
[169,     1] loss: 0.201
[170,     1] loss: 0.191
[171,     1] loss: 0.172
Early stopping applied (best metric=0.28329557180404663)
Finished Training
Total time taken: 19.33634877204895
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.667
[5,     1] loss: 0.639
[6,     1] loss: 0.619
[7,     1] loss: 0.611
[8,     1] loss: 0.569
[9,     1] loss: 0.568
[10,     1] loss: 0.535
[11,     1] loss: 0.515
[12,     1] loss: 0.495
[13,     1] loss: 0.450
[14,     1] loss: 0.435
[15,     1] loss: 0.462
[16,     1] loss: 0.394
[17,     1] loss: 0.389
[18,     1] loss: 0.347
[19,     1] loss: 0.353
[20,     1] loss: 0.373
[21,     1] loss: 0.377
[22,     1] loss: 0.373
[23,     1] loss: 0.397
[24,     1] loss: 0.289
[25,     1] loss: 0.344
[26,     1] loss: 0.323
[27,     1] loss: 0.330
[28,     1] loss: 0.356
[29,     1] loss: 0.289
[30,     1] loss: 0.300
[31,     1] loss: 0.285
[32,     1] loss: 0.363
[33,     1] loss: 0.248
[34,     1] loss: 0.309
[35,     1] loss: 0.296
[36,     1] loss: 0.337
[37,     1] loss: 0.312
[38,     1] loss: 0.225
[39,     1] loss: 0.280
[40,     1] loss: 0.278
[41,     1] loss: 0.279
[42,     1] loss: 0.324
[43,     1] loss: 0.284
[44,     1] loss: 0.301
[45,     1] loss: 0.252
[46,     1] loss: 0.296
[47,     1] loss: 0.260
[48,     1] loss: 0.234
[49,     1] loss: 0.320
[50,     1] loss: 0.247
[51,     1] loss: 0.314
[52,     1] loss: 0.263
[53,     1] loss: 0.277
[54,     1] loss: 0.216
[55,     1] loss: 0.273
[56,     1] loss: 0.273
[57,     1] loss: 0.261
[58,     1] loss: 0.243
[59,     1] loss: 0.245
[60,     1] loss: 0.258
[61,     1] loss: 0.255
[62,     1] loss: 0.262
[63,     1] loss: 0.249
[64,     1] loss: 0.269
[65,     1] loss: 0.278
[66,     1] loss: 0.223
Early stopping applied (best metric=0.41354072093963623)
Finished Training
Total time taken: 7.700545787811279
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.687
[2,     1] loss: 0.701
[3,     1] loss: 0.682
[4,     1] loss: 0.661
[5,     1] loss: 0.656
[6,     1] loss: 0.636
[7,     1] loss: 0.626
[8,     1] loss: 0.601
[9,     1] loss: 0.603
[10,     1] loss: 0.578
[11,     1] loss: 0.565
[12,     1] loss: 0.526
[13,     1] loss: 0.483
[14,     1] loss: 0.510
[15,     1] loss: 0.475
[16,     1] loss: 0.466
[17,     1] loss: 0.439
[18,     1] loss: 0.415
[19,     1] loss: 0.424
[20,     1] loss: 0.474
[21,     1] loss: 0.360
[22,     1] loss: 0.345
[23,     1] loss: 0.349
[24,     1] loss: 0.355
[25,     1] loss: 0.302
[26,     1] loss: 0.293
[27,     1] loss: 0.284
[28,     1] loss: 0.284
[29,     1] loss: 0.262
[30,     1] loss: 0.300
[31,     1] loss: 0.298
[32,     1] loss: 0.286
[33,     1] loss: 0.239
[34,     1] loss: 0.244
[35,     1] loss: 0.234
[36,     1] loss: 0.288
[37,     1] loss: 0.376
[38,     1] loss: 0.329
[39,     1] loss: 0.359
[40,     1] loss: 0.355
[41,     1] loss: 0.337
[42,     1] loss: 0.353
[43,     1] loss: 0.317
[44,     1] loss: 0.350
[45,     1] loss: 0.277
[46,     1] loss: 0.345
[47,     1] loss: 0.325
[48,     1] loss: 0.357
[49,     1] loss: 0.350
[50,     1] loss: 0.306
[51,     1] loss: 0.280
[52,     1] loss: 0.355
[53,     1] loss: 0.256
[54,     1] loss: 0.240
[55,     1] loss: 0.315
[56,     1] loss: 0.294
[57,     1] loss: 0.307
[58,     1] loss: 0.284
[59,     1] loss: 0.323
[60,     1] loss: 0.276
[61,     1] loss: 0.289
[62,     1] loss: 0.296
[63,     1] loss: 0.291
[64,     1] loss: 0.284
[65,     1] loss: 0.294
[66,     1] loss: 0.338
[67,     1] loss: 0.254
[68,     1] loss: 0.242
[69,     1] loss: 0.330
[70,     1] loss: 0.204
[71,     1] loss: 0.196
[72,     1] loss: 0.229
[73,     1] loss: 0.218
[74,     1] loss: 0.196
[75,     1] loss: 0.204
[76,     1] loss: 0.216
[77,     1] loss: 0.268
[78,     1] loss: 0.224
[79,     1] loss: 0.190
[80,     1] loss: 0.193
[81,     1] loss: 0.191
[82,     1] loss: 0.191
[83,     1] loss: 0.198
[84,     1] loss: 0.270
[85,     1] loss: 0.239
[86,     1] loss: 0.334
[87,     1] loss: 0.364
[88,     1] loss: 0.315
[89,     1] loss: 0.254
[90,     1] loss: 0.289
[91,     1] loss: 0.192
[92,     1] loss: 0.257
[93,     1] loss: 0.209
[94,     1] loss: 0.252
[95,     1] loss: 0.253
[96,     1] loss: 0.240
[97,     1] loss: 0.334
[98,     1] loss: 0.222
[99,     1] loss: 0.268
[100,     1] loss: 0.253
[101,     1] loss: 0.260
[102,     1] loss: 0.228
[103,     1] loss: 0.261
[104,     1] loss: 0.275
[105,     1] loss: 0.323
Early stopping applied (best metric=0.3490683138370514)
Finished Training
Total time taken: 12.089563131332397
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.688
[4,     1] loss: 0.671
[5,     1] loss: 0.643
[6,     1] loss: 0.623
[7,     1] loss: 0.596
[8,     1] loss: 0.573
[9,     1] loss: 0.549
[10,     1] loss: 0.515
[11,     1] loss: 0.497
[12,     1] loss: 0.460
[13,     1] loss: 0.456
[14,     1] loss: 0.419
[15,     1] loss: 0.402
[16,     1] loss: 0.401
[17,     1] loss: 0.357
[18,     1] loss: 0.339
[19,     1] loss: 0.344
[20,     1] loss: 0.355
[21,     1] loss: 0.333
[22,     1] loss: 0.330
[23,     1] loss: 0.322
[24,     1] loss: 0.316
[25,     1] loss: 0.352
[26,     1] loss: 0.362
[27,     1] loss: 0.359
[28,     1] loss: 0.375
[29,     1] loss: 0.334
[30,     1] loss: 0.268
[31,     1] loss: 0.301
[32,     1] loss: 0.326
[33,     1] loss: 0.355
[34,     1] loss: 0.319
[35,     1] loss: 0.293
[36,     1] loss: 0.303
[37,     1] loss: 0.305
[38,     1] loss: 0.276
[39,     1] loss: 0.266
[40,     1] loss: 0.276
[41,     1] loss: 0.256
[42,     1] loss: 0.281
[43,     1] loss: 0.267
[44,     1] loss: 0.255
[45,     1] loss: 0.326
[46,     1] loss: 0.277
[47,     1] loss: 0.259
[48,     1] loss: 0.296
[49,     1] loss: 0.302
[50,     1] loss: 0.250
[51,     1] loss: 0.227
[52,     1] loss: 0.229
[53,     1] loss: 0.227
[54,     1] loss: 0.213
[55,     1] loss: 0.243
[56,     1] loss: 0.213
[57,     1] loss: 0.214
[58,     1] loss: 0.185
[59,     1] loss: 0.189
[60,     1] loss: 0.198
[61,     1] loss: 0.190
[62,     1] loss: 0.182
[63,     1] loss: 0.196
[64,     1] loss: 0.168
[65,     1] loss: 0.174
[66,     1] loss: 0.195
[67,     1] loss: 0.191
[68,     1] loss: 0.193
[69,     1] loss: 0.202
[70,     1] loss: 0.194
Early stopping applied (best metric=0.38281306624412537)
Finished Training
Total time taken: 8.360376834869385
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.657
[4,     1] loss: 0.623
[5,     1] loss: 0.609
[6,     1] loss: 0.560
[7,     1] loss: 0.531
[8,     1] loss: 0.491
[9,     1] loss: 0.444
[10,     1] loss: 0.415
[11,     1] loss: 0.395
[12,     1] loss: 0.366
[13,     1] loss: 0.382
[14,     1] loss: 0.367
[15,     1] loss: 0.356
[16,     1] loss: 0.322
[17,     1] loss: 0.324
[18,     1] loss: 0.274
[19,     1] loss: 0.294
[20,     1] loss: 0.273
[21,     1] loss: 0.345
[22,     1] loss: 0.276
[23,     1] loss: 0.289
[24,     1] loss: 0.254
[25,     1] loss: 0.249
[26,     1] loss: 0.255
[27,     1] loss: 0.220
[28,     1] loss: 0.187
[29,     1] loss: 0.288
[30,     1] loss: 0.247
[31,     1] loss: 0.258
[32,     1] loss: 0.236
[33,     1] loss: 0.259
[34,     1] loss: 0.261
[35,     1] loss: 0.242
[36,     1] loss: 0.232
[37,     1] loss: 0.234
[38,     1] loss: 0.254
[39,     1] loss: 0.198
[40,     1] loss: 0.273
[41,     1] loss: 0.214
[42,     1] loss: 0.242
[43,     1] loss: 0.240
[44,     1] loss: 0.255
[45,     1] loss: 0.250
[46,     1] loss: 0.178
[47,     1] loss: 0.175
[48,     1] loss: 0.194
[49,     1] loss: 0.177
[50,     1] loss: 0.259
[51,     1] loss: 0.228
[52,     1] loss: 0.274
[53,     1] loss: 0.265
[54,     1] loss: 0.282
[55,     1] loss: 0.238
[56,     1] loss: 0.239
[57,     1] loss: 0.238
[58,     1] loss: 0.249
[59,     1] loss: 0.212
[60,     1] loss: 0.257
[61,     1] loss: 0.347
[62,     1] loss: 0.227
[63,     1] loss: 0.241
[64,     1] loss: 0.248
[65,     1] loss: 0.218
[66,     1] loss: 0.284
[67,     1] loss: 0.254
[68,     1] loss: 0.199
[69,     1] loss: 0.219
[70,     1] loss: 0.200
[71,     1] loss: 0.216
[72,     1] loss: 0.217
Early stopping applied (best metric=0.4610353410243988)
Finished Training
Total time taken: 8.508881568908691
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.685
[4,     1] loss: 0.656
[5,     1] loss: 0.628
[6,     1] loss: 0.627
[7,     1] loss: 0.581
[8,     1] loss: 0.553
[9,     1] loss: 0.528
[10,     1] loss: 0.503
[11,     1] loss: 0.458
[12,     1] loss: 0.435
[13,     1] loss: 0.408
[14,     1] loss: 0.342
[15,     1] loss: 0.372
[16,     1] loss: 0.354
[17,     1] loss: 0.349
[18,     1] loss: 0.337
[19,     1] loss: 0.301
[20,     1] loss: 0.321
[21,     1] loss: 0.298
[22,     1] loss: 0.289
[23,     1] loss: 0.312
[24,     1] loss: 0.379
[25,     1] loss: 0.271
[26,     1] loss: 0.318
[27,     1] loss: 0.316
[28,     1] loss: 0.263
[29,     1] loss: 0.268
[30,     1] loss: 0.256
[31,     1] loss: 0.252
[32,     1] loss: 0.239
[33,     1] loss: 0.250
[34,     1] loss: 0.237
[35,     1] loss: 0.228
[36,     1] loss: 0.247
[37,     1] loss: 0.189
[38,     1] loss: 0.252
[39,     1] loss: 0.236
[40,     1] loss: 0.245
[41,     1] loss: 0.183
[42,     1] loss: 0.181
[43,     1] loss: 0.202
[44,     1] loss: 0.208
[45,     1] loss: 0.257
[46,     1] loss: 0.217
[47,     1] loss: 0.283
[48,     1] loss: 0.244
[49,     1] loss: 0.306
[50,     1] loss: 0.252
[51,     1] loss: 0.372
[52,     1] loss: 0.292
[53,     1] loss: 0.202
[54,     1] loss: 0.231
[55,     1] loss: 0.273
[56,     1] loss: 0.249
[57,     1] loss: 0.190
[58,     1] loss: 0.211
[59,     1] loss: 0.228
[60,     1] loss: 0.243
[61,     1] loss: 0.235
Early stopping applied (best metric=0.413449764251709)
Finished Training
Total time taken: 7.076099634170532
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.701
[3,     1] loss: 0.679
[4,     1] loss: 0.651
[5,     1] loss: 0.625
[6,     1] loss: 0.597
[7,     1] loss: 0.580
[8,     1] loss: 0.556
[9,     1] loss: 0.527
[10,     1] loss: 0.496
[11,     1] loss: 0.455
[12,     1] loss: 0.441
[13,     1] loss: 0.442
[14,     1] loss: 0.429
[15,     1] loss: 0.417
[16,     1] loss: 0.360
[17,     1] loss: 0.416
[18,     1] loss: 0.357
[19,     1] loss: 0.376
[20,     1] loss: 0.342
[21,     1] loss: 0.333
[22,     1] loss: 0.387
[23,     1] loss: 0.308
[24,     1] loss: 0.408
[25,     1] loss: 0.339
[26,     1] loss: 0.366
[27,     1] loss: 0.289
[28,     1] loss: 0.363
[29,     1] loss: 0.330
[30,     1] loss: 0.305
[31,     1] loss: 0.334
[32,     1] loss: 0.315
[33,     1] loss: 0.289
[34,     1] loss: 0.295
[35,     1] loss: 0.301
[36,     1] loss: 0.287
[37,     1] loss: 0.246
[38,     1] loss: 0.250
[39,     1] loss: 0.273
[40,     1] loss: 0.256
[41,     1] loss: 0.267
[42,     1] loss: 0.275
[43,     1] loss: 0.246
[44,     1] loss: 0.252
[45,     1] loss: 0.222
[46,     1] loss: 0.325
[47,     1] loss: 0.249
[48,     1] loss: 0.243
[49,     1] loss: 0.249
[50,     1] loss: 0.236
[51,     1] loss: 0.222
[52,     1] loss: 0.216
[53,     1] loss: 0.198
[54,     1] loss: 0.195
[55,     1] loss: 0.184
[56,     1] loss: 0.226
[57,     1] loss: 0.199
[58,     1] loss: 0.290
[59,     1] loss: 0.296
[60,     1] loss: 0.343
[61,     1] loss: 0.306
[62,     1] loss: 0.361
[63,     1] loss: 0.355
[64,     1] loss: 0.319
[65,     1] loss: 0.264
[66,     1] loss: 0.254
[67,     1] loss: 0.267
[68,     1] loss: 0.272
[69,     1] loss: 0.243
[70,     1] loss: 0.228
[71,     1] loss: 0.247
[72,     1] loss: 0.221
[73,     1] loss: 0.206
[74,     1] loss: 0.206
[75,     1] loss: 0.179
[76,     1] loss: 0.220
[77,     1] loss: 0.277
[78,     1] loss: 0.210
[79,     1] loss: 0.308
[80,     1] loss: 0.242
[81,     1] loss: 0.377
[82,     1] loss: 0.245
[83,     1] loss: 0.354
[84,     1] loss: 0.239
[85,     1] loss: 0.216
[86,     1] loss: 0.276
[87,     1] loss: 0.229
[88,     1] loss: 0.206
[89,     1] loss: 0.277
[90,     1] loss: 0.246
[91,     1] loss: 0.223
[92,     1] loss: 0.193
[93,     1] loss: 0.194
[94,     1] loss: 0.193
[95,     1] loss: 0.182
[96,     1] loss: 0.181
[97,     1] loss: 0.213
[98,     1] loss: 0.215
[99,     1] loss: 0.175
[100,     1] loss: 0.175
[101,     1] loss: 0.158
[102,     1] loss: 0.214
[103,     1] loss: 0.170
[104,     1] loss: 0.169
[105,     1] loss: 0.183
[106,     1] loss: 0.162
[107,     1] loss: 0.151
[108,     1] loss: 0.173
[109,     1] loss: 0.161
[110,     1] loss: 0.238
[111,     1] loss: 0.223
[112,     1] loss: 0.155
[113,     1] loss: 0.176
[114,     1] loss: 0.185
[115,     1] loss: 0.201
[116,     1] loss: 0.145
[117,     1] loss: 0.156
Early stopping applied (best metric=0.42973968386650085)
Finished Training
Total time taken: 13.772690057754517
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.681
[4,     1] loss: 0.659
[5,     1] loss: 0.636
[6,     1] loss: 0.620
[7,     1] loss: 0.577
[8,     1] loss: 0.564
[9,     1] loss: 0.527
[10,     1] loss: 0.497
[11,     1] loss: 0.500
[12,     1] loss: 0.482
[13,     1] loss: 0.467
[14,     1] loss: 0.420
[15,     1] loss: 0.415
[16,     1] loss: 0.390
[17,     1] loss: 0.361
[18,     1] loss: 0.381
[19,     1] loss: 0.362
[20,     1] loss: 0.371
[21,     1] loss: 0.312
[22,     1] loss: 0.308
[23,     1] loss: 0.335
[24,     1] loss: 0.333
[25,     1] loss: 0.352
[26,     1] loss: 0.349
[27,     1] loss: 0.388
[28,     1] loss: 0.338
[29,     1] loss: 0.378
[30,     1] loss: 0.333
[31,     1] loss: 0.347
[32,     1] loss: 0.330
[33,     1] loss: 0.310
[34,     1] loss: 0.349
[35,     1] loss: 0.312
[36,     1] loss: 0.315
[37,     1] loss: 0.313
[38,     1] loss: 0.278
[39,     1] loss: 0.293
[40,     1] loss: 0.284
[41,     1] loss: 0.253
[42,     1] loss: 0.242
[43,     1] loss: 0.257
[44,     1] loss: 0.258
[45,     1] loss: 0.209
[46,     1] loss: 0.220
[47,     1] loss: 0.217
[48,     1] loss: 0.254
[49,     1] loss: 0.196
[50,     1] loss: 0.202
[51,     1] loss: 0.211
[52,     1] loss: 0.233
[53,     1] loss: 0.238
[54,     1] loss: 0.226
[55,     1] loss: 0.222
[56,     1] loss: 0.247
[57,     1] loss: 0.242
[58,     1] loss: 0.200
[59,     1] loss: 0.261
[60,     1] loss: 0.211
[61,     1] loss: 0.220
[62,     1] loss: 0.249
[63,     1] loss: 0.253
[64,     1] loss: 0.224
[65,     1] loss: 0.236
[66,     1] loss: 0.238
Early stopping applied (best metric=0.3628673851490021)
Finished Training
Total time taken: 8.669893264770508
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.691
[3,     1] loss: 0.684
[4,     1] loss: 0.664
[5,     1] loss: 0.627
[6,     1] loss: 0.593
[7,     1] loss: 0.567
[8,     1] loss: 0.551
[9,     1] loss: 0.513
[10,     1] loss: 0.523
[11,     1] loss: 0.477
[12,     1] loss: 0.432
[13,     1] loss: 0.425
[14,     1] loss: 0.426
[15,     1] loss: 0.382
[16,     1] loss: 0.420
[17,     1] loss: 0.401
[18,     1] loss: 0.344
[19,     1] loss: 0.346
[20,     1] loss: 0.334
[21,     1] loss: 0.344
[22,     1] loss: 0.350
[23,     1] loss: 0.329
[24,     1] loss: 0.313
[25,     1] loss: 0.345
[26,     1] loss: 0.363
[27,     1] loss: 0.334
[28,     1] loss: 0.266
[29,     1] loss: 0.348
[30,     1] loss: 0.333
[31,     1] loss: 0.314
[32,     1] loss: 0.297
[33,     1] loss: 0.286
[34,     1] loss: 0.352
[35,     1] loss: 0.285
[36,     1] loss: 0.315
[37,     1] loss: 0.297
[38,     1] loss: 0.290
[39,     1] loss: 0.285
[40,     1] loss: 0.244
[41,     1] loss: 0.288
[42,     1] loss: 0.265
[43,     1] loss: 0.241
[44,     1] loss: 0.237
[45,     1] loss: 0.222
[46,     1] loss: 0.245
[47,     1] loss: 0.236
[48,     1] loss: 0.274
[49,     1] loss: 0.276
[50,     1] loss: 0.270
[51,     1] loss: 0.247
[52,     1] loss: 0.216
[53,     1] loss: 0.185
[54,     1] loss: 0.222
[55,     1] loss: 0.189
[56,     1] loss: 0.204
[57,     1] loss: 0.212
[58,     1] loss: 0.250
[59,     1] loss: 0.235
[60,     1] loss: 0.203
[61,     1] loss: 0.235
[62,     1] loss: 0.214
[63,     1] loss: 0.246
[64,     1] loss: 0.231
[65,     1] loss: 0.191
[66,     1] loss: 0.194
[67,     1] loss: 0.206
[68,     1] loss: 0.207
[69,     1] loss: 0.196
[70,     1] loss: 0.248
[71,     1] loss: 0.169
[72,     1] loss: 0.237
[73,     1] loss: 0.172
[74,     1] loss: 0.236
[75,     1] loss: 0.161
[76,     1] loss: 0.181
[77,     1] loss: 0.182
[78,     1] loss: 0.223
[79,     1] loss: 0.188
[80,     1] loss: 0.225
[81,     1] loss: 0.225
[82,     1] loss: 0.238
[83,     1] loss: 0.240
[84,     1] loss: 0.257
[85,     1] loss: 0.197
[86,     1] loss: 0.237
[87,     1] loss: 0.205
[88,     1] loss: 0.235
[89,     1] loss: 0.203
[90,     1] loss: 0.208
[91,     1] loss: 0.193
[92,     1] loss: 0.176
[93,     1] loss: 0.192
[94,     1] loss: 0.230
[95,     1] loss: 0.306
[96,     1] loss: 0.309
[97,     1] loss: 0.244
[98,     1] loss: 0.252
[99,     1] loss: 0.252
[100,     1] loss: 0.192
[101,     1] loss: 0.226
[102,     1] loss: 0.252
[103,     1] loss: 0.224
[104,     1] loss: 0.248
[105,     1] loss: 0.209
[106,     1] loss: 0.202
[107,     1] loss: 0.229
[108,     1] loss: 0.187
[109,     1] loss: 0.192
[110,     1] loss: 0.177
[111,     1] loss: 0.180
[112,     1] loss: 0.244
[113,     1] loss: 0.193
[114,     1] loss: 0.225
[115,     1] loss: 0.390
[116,     1] loss: 0.201
[117,     1] loss: 0.237
[118,     1] loss: 0.309
[119,     1] loss: 0.311
[120,     1] loss: 0.311
[121,     1] loss: 0.252
[122,     1] loss: 0.281
[123,     1] loss: 0.256
[124,     1] loss: 0.230
[125,     1] loss: 0.275
[126,     1] loss: 0.237
[127,     1] loss: 0.249
[128,     1] loss: 0.264
[129,     1] loss: 0.256
[130,     1] loss: 0.225
[131,     1] loss: 0.229
[132,     1] loss: 0.212
[133,     1] loss: 0.222
[134,     1] loss: 0.200
[135,     1] loss: 0.214
[136,     1] loss: 0.187
[137,     1] loss: 0.189
[138,     1] loss: 0.200
[139,     1] loss: 0.178
[140,     1] loss: 0.170
Early stopping applied (best metric=0.2839522063732147)
Finished Training
Total time taken: 16.933133363723755
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.666
[4,     1] loss: 0.643
[5,     1] loss: 0.599
[6,     1] loss: 0.571
[7,     1] loss: 0.538
[8,     1] loss: 0.518
[9,     1] loss: 0.505
[10,     1] loss: 0.456
[11,     1] loss: 0.412
[12,     1] loss: 0.410
[13,     1] loss: 0.412
[14,     1] loss: 0.444
[15,     1] loss: 0.366
[16,     1] loss: 0.413
[17,     1] loss: 0.397
[18,     1] loss: 0.343
[19,     1] loss: 0.389
[20,     1] loss: 0.349
[21,     1] loss: 0.376
[22,     1] loss: 0.436
[23,     1] loss: 0.367
[24,     1] loss: 0.404
[25,     1] loss: 0.358
[26,     1] loss: 0.323
[27,     1] loss: 0.377
[28,     1] loss: 0.313
[29,     1] loss: 0.330
[30,     1] loss: 0.353
[31,     1] loss: 0.329
[32,     1] loss: 0.338
[33,     1] loss: 0.313
[34,     1] loss: 0.295
[35,     1] loss: 0.337
[36,     1] loss: 0.305
[37,     1] loss: 0.391
[38,     1] loss: 0.307
[39,     1] loss: 0.324
[40,     1] loss: 0.462
[41,     1] loss: 0.363
[42,     1] loss: 0.313
[43,     1] loss: 0.347
[44,     1] loss: 0.305
[45,     1] loss: 0.299
[46,     1] loss: 0.305
[47,     1] loss: 0.330
[48,     1] loss: 0.307
[49,     1] loss: 0.264
[50,     1] loss: 0.276
[51,     1] loss: 0.284
[52,     1] loss: 0.288
[53,     1] loss: 0.314
[54,     1] loss: 0.268
[55,     1] loss: 0.271
[56,     1] loss: 0.225
[57,     1] loss: 0.242
[58,     1] loss: 0.208
[59,     1] loss: 0.260
[60,     1] loss: 0.250
[61,     1] loss: 0.361
[62,     1] loss: 0.280
[63,     1] loss: 0.312
[64,     1] loss: 0.271
[65,     1] loss: 0.304
[66,     1] loss: 0.339
[67,     1] loss: 0.343
[68,     1] loss: 0.312
[69,     1] loss: 0.272
[70,     1] loss: 0.248
[71,     1] loss: 0.315
[72,     1] loss: 0.336
[73,     1] loss: 0.286
[74,     1] loss: 0.265
[75,     1] loss: 0.247
[76,     1] loss: 0.347
[77,     1] loss: 0.281
[78,     1] loss: 0.285
[79,     1] loss: 0.306
[80,     1] loss: 0.308
[81,     1] loss: 0.268
[82,     1] loss: 0.266
[83,     1] loss: 0.244
[84,     1] loss: 0.285
[85,     1] loss: 0.261
[86,     1] loss: 0.324
[87,     1] loss: 0.332
[88,     1] loss: 0.378
[89,     1] loss: 0.305
[90,     1] loss: 0.353
[91,     1] loss: 0.402
[92,     1] loss: 0.306
[93,     1] loss: 0.346
[94,     1] loss: 0.350
[95,     1] loss: 0.349
[96,     1] loss: 0.304
[97,     1] loss: 0.330
[98,     1] loss: 0.331
[99,     1] loss: 0.308
[100,     1] loss: 0.301
[101,     1] loss: 0.281
[102,     1] loss: 0.278
[103,     1] loss: 0.335
[104,     1] loss: 0.273
[105,     1] loss: 0.288
[106,     1] loss: 0.271
[107,     1] loss: 0.274
[108,     1] loss: 0.287
[109,     1] loss: 0.282
[110,     1] loss: 0.285
[111,     1] loss: 0.264
[112,     1] loss: 0.279
Early stopping applied (best metric=0.38849887251853943)
Finished Training
Total time taken: 12.975112199783325
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.691
[3,     1] loss: 0.663
[4,     1] loss: 0.613
[5,     1] loss: 0.582
[6,     1] loss: 0.570
[7,     1] loss: 0.528
[8,     1] loss: 0.516
[9,     1] loss: 0.486
[10,     1] loss: 0.445
[11,     1] loss: 0.441
[12,     1] loss: 0.373
[13,     1] loss: 0.331
[14,     1] loss: 0.353
[15,     1] loss: 0.336
[16,     1] loss: 0.353
[17,     1] loss: 0.329
[18,     1] loss: 0.432
[19,     1] loss: 0.299
[20,     1] loss: 0.394
[21,     1] loss: 0.366
[22,     1] loss: 0.329
[23,     1] loss: 0.379
[24,     1] loss: 0.368
[25,     1] loss: 0.399
[26,     1] loss: 0.300
[27,     1] loss: 0.347
[28,     1] loss: 0.330
[29,     1] loss: 0.313
[30,     1] loss: 0.352
[31,     1] loss: 0.322
[32,     1] loss: 0.344
[33,     1] loss: 0.319
[34,     1] loss: 0.313
[35,     1] loss: 0.297
[36,     1] loss: 0.246
[37,     1] loss: 0.267
[38,     1] loss: 0.300
[39,     1] loss: 0.259
[40,     1] loss: 0.239
[41,     1] loss: 0.237
[42,     1] loss: 0.248
[43,     1] loss: 0.187
[44,     1] loss: 0.230
[45,     1] loss: 0.179
[46,     1] loss: 0.188
[47,     1] loss: 0.206
[48,     1] loss: 0.146
[49,     1] loss: 0.140
[50,     1] loss: 0.173
[51,     1] loss: 0.183
[52,     1] loss: 0.310
[53,     1] loss: 0.186
[54,     1] loss: 0.201
[55,     1] loss: 0.164
[56,     1] loss: 0.224
[57,     1] loss: 0.260
[58,     1] loss: 0.234
[59,     1] loss: 0.230
Early stopping applied (best metric=0.47543543577194214)
Finished Training
Total time taken: 6.993093729019165
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.682
[4,     1] loss: 0.651
[5,     1] loss: 0.635
[6,     1] loss: 0.611
[7,     1] loss: 0.597
[8,     1] loss: 0.579
[9,     1] loss: 0.551
[10,     1] loss: 0.549
[11,     1] loss: 0.530
[12,     1] loss: 0.537
[13,     1] loss: 0.488
[14,     1] loss: 0.470
[15,     1] loss: 0.487
[16,     1] loss: 0.413
[17,     1] loss: 0.430
[18,     1] loss: 0.439
[19,     1] loss: 0.342
[20,     1] loss: 0.424
[21,     1] loss: 0.380
[22,     1] loss: 0.362
[23,     1] loss: 0.352
[24,     1] loss: 0.285
[25,     1] loss: 0.320
[26,     1] loss: 0.312
[27,     1] loss: 0.309
[28,     1] loss: 0.283
[29,     1] loss: 0.336
[30,     1] loss: 0.261
[31,     1] loss: 0.253
[32,     1] loss: 0.222
[33,     1] loss: 0.225
[34,     1] loss: 0.249
[35,     1] loss: 0.233
[36,     1] loss: 0.201
[37,     1] loss: 0.180
[38,     1] loss: 0.179
[39,     1] loss: 0.205
[40,     1] loss: 0.241
[41,     1] loss: 0.193
[42,     1] loss: 0.208
[43,     1] loss: 0.194
[44,     1] loss: 0.150
[45,     1] loss: 0.223
[46,     1] loss: 0.277
[47,     1] loss: 0.227
[48,     1] loss: 0.253
[49,     1] loss: 0.190
[50,     1] loss: 0.195
[51,     1] loss: 0.195
[52,     1] loss: 0.227
[53,     1] loss: 0.210
[54,     1] loss: 0.173
[55,     1] loss: 0.138
[56,     1] loss: 0.142
[57,     1] loss: 0.221
[58,     1] loss: 0.149
[59,     1] loss: 0.171
[60,     1] loss: 0.297
[61,     1] loss: 0.317
[62,     1] loss: 0.253
[63,     1] loss: 0.328
[64,     1] loss: 0.276
[65,     1] loss: 0.327
[66,     1] loss: 0.250
[67,     1] loss: 0.230
[68,     1] loss: 0.248
[69,     1] loss: 0.219
[70,     1] loss: 0.214
[71,     1] loss: 0.217
[72,     1] loss: 0.210
[73,     1] loss: 0.226
[74,     1] loss: 0.208
[75,     1] loss: 0.227
[76,     1] loss: 0.194
[77,     1] loss: 0.180
[78,     1] loss: 0.179
[79,     1] loss: 0.192
[80,     1] loss: 0.206
[81,     1] loss: 0.168
[82,     1] loss: 0.150
[83,     1] loss: 0.194
[84,     1] loss: 0.170
[85,     1] loss: 0.155
[86,     1] loss: 0.169
[87,     1] loss: 0.176
[88,     1] loss: 0.186
[89,     1] loss: 0.317
[90,     1] loss: 0.344
[91,     1] loss: 0.342
[92,     1] loss: 0.317
[93,     1] loss: 0.374
[94,     1] loss: 0.286
[95,     1] loss: 0.340
[96,     1] loss: 0.372
[97,     1] loss: 0.307
[98,     1] loss: 0.302
[99,     1] loss: 0.316
[100,     1] loss: 0.329
[101,     1] loss: 0.286
[102,     1] loss: 0.299
[103,     1] loss: 0.316
[104,     1] loss: 0.323
[105,     1] loss: 0.284
[106,     1] loss: 0.276
[107,     1] loss: 0.271
[108,     1] loss: 0.252
[109,     1] loss: 0.286
[110,     1] loss: 0.300
[111,     1] loss: 0.258
[112,     1] loss: 0.257
[113,     1] loss: 0.238
[114,     1] loss: 0.232
[115,     1] loss: 0.253
[116,     1] loss: 0.256
[117,     1] loss: 0.289
[118,     1] loss: 0.369
[119,     1] loss: 0.278
[120,     1] loss: 0.267
[121,     1] loss: 0.263
[122,     1] loss: 0.299
[123,     1] loss: 0.260
[124,     1] loss: 0.242
[125,     1] loss: 0.257
[126,     1] loss: 0.252
[127,     1] loss: 0.247
[128,     1] loss: 0.217
[129,     1] loss: 0.192
[130,     1] loss: 0.298
[131,     1] loss: 0.342
[132,     1] loss: 0.376
[133,     1] loss: 0.242
[134,     1] loss: 0.352
[135,     1] loss: 0.328
[136,     1] loss: 0.324
[137,     1] loss: 0.286
[138,     1] loss: 0.320
[139,     1] loss: 0.303
[140,     1] loss: 0.333
[141,     1] loss: 0.307
Early stopping applied (best metric=0.19015362858772278)
Finished Training
Total time taken: 17.785470485687256
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.687
[4,     1] loss: 0.662
[5,     1] loss: 0.655
[6,     1] loss: 0.627
[7,     1] loss: 0.613
[8,     1] loss: 0.593
[9,     1] loss: 0.562
[10,     1] loss: 0.568
[11,     1] loss: 0.535
[12,     1] loss: 0.511
[13,     1] loss: 0.492
[14,     1] loss: 0.451
[15,     1] loss: 0.443
[16,     1] loss: 0.508
[17,     1] loss: 0.408
[18,     1] loss: 0.441
[19,     1] loss: 0.407
[20,     1] loss: 0.372
[21,     1] loss: 0.390
[22,     1] loss: 0.355
[23,     1] loss: 0.371
[24,     1] loss: 0.405
[25,     1] loss: 0.396
[26,     1] loss: 0.418
[27,     1] loss: 0.415
[28,     1] loss: 0.365
[29,     1] loss: 0.430
[30,     1] loss: 0.342
[31,     1] loss: 0.398
[32,     1] loss: 0.366
[33,     1] loss: 0.382
[34,     1] loss: 0.321
[35,     1] loss: 0.318
[36,     1] loss: 0.315
[37,     1] loss: 0.322
[38,     1] loss: 0.355
[39,     1] loss: 0.298
[40,     1] loss: 0.347
[41,     1] loss: 0.323
[42,     1] loss: 0.325
[43,     1] loss: 0.271
[44,     1] loss: 0.332
[45,     1] loss: 0.329
[46,     1] loss: 0.292
[47,     1] loss: 0.294
[48,     1] loss: 0.297
[49,     1] loss: 0.238
[50,     1] loss: 0.225
[51,     1] loss: 0.279
[52,     1] loss: 0.245
[53,     1] loss: 0.212
[54,     1] loss: 0.229
[55,     1] loss: 0.280
[56,     1] loss: 0.340
[57,     1] loss: 0.380
[58,     1] loss: 0.295
[59,     1] loss: 0.433
[60,     1] loss: 0.327
[61,     1] loss: 0.351
[62,     1] loss: 0.383
[63,     1] loss: 0.324
[64,     1] loss: 0.345
[65,     1] loss: 0.319
[66,     1] loss: 0.307
[67,     1] loss: 0.292
[68,     1] loss: 0.279
[69,     1] loss: 0.314
[70,     1] loss: 0.310
[71,     1] loss: 0.270
[72,     1] loss: 0.268
[73,     1] loss: 0.254
[74,     1] loss: 0.236
[75,     1] loss: 0.259
[76,     1] loss: 0.265
[77,     1] loss: 0.262
[78,     1] loss: 0.234
[79,     1] loss: 0.213
[80,     1] loss: 0.242
[81,     1] loss: 0.234
[82,     1] loss: 0.230
Early stopping applied (best metric=0.24674494564533234)
Finished Training
Total time taken: 9.579270839691162
{'Hydroxylation-K Validation Accuracy': 0.8378723404255319, 'Hydroxylation-K Validation Sensitivity': 0.8333333333333334, 'Hydroxylation-K Validation Specificity': 0.8389473684210527, 'Hydroxylation-K Validation Precision': 0.5729535660418014, 'Hydroxylation-K AUC ROC': 0.8538362573099415, 'Hydroxylation-K AUC PR': 0.6160466698825133, 'Hydroxylation-K MCC': 0.5940023118557863, 'Hydroxylation-K F1': 0.6754516409601078, 'Validation Loss (Hydroxylation-K)': 0.3404422080516815, 'Validation Loss (total)': 0.3404422080516815, 'TimeToTrain': 12.187901344299316}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006993361604779623,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2265394867,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.84435163201047}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.670
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001207782360681311,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 618903063,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.942156102267486}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.692
[3,     1] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005778302064778513,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2678531591,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.147882504399055}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.694
[3,     1] loss: 0.682
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0066006964802577985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2984422612,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.372311937155149}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.687
[3,     1] loss: 0.697
[4,     1] loss: 0.650
[5,     1] loss: 0.622
[6,     1] loss: 0.578
[7,     1] loss: 0.509
[8,     1] loss: 0.474
[9,     1] loss: 0.437
[10,     1] loss: 0.394
[11,     1] loss: 0.372
[12,     1] loss: 0.318
[13,     1] loss: 0.346
[14,     1] loss: 0.354
[15,     1] loss: 0.292
[16,     1] loss: 0.381
[17,     1] loss: 0.300
[18,     1] loss: 0.254
[19,     1] loss: 0.261
[20,     1] loss: 0.278
[21,     1] loss: 0.311
[22,     1] loss: 0.236
[23,     1] loss: 0.307
[24,     1] loss: 0.340
[25,     1] loss: 0.215
[26,     1] loss: 0.215
[27,     1] loss: 0.284
[28,     1] loss: 0.326
[29,     1] loss: 0.299
[30,     1] loss: 0.232
[31,     1] loss: 0.274
[32,     1] loss: 0.459
[33,     1] loss: 0.257
[34,     1] loss: 0.255
[35,     1] loss: 0.288
[36,     1] loss: 0.223
[37,     1] loss: 0.293
[38,     1] loss: 0.210
[39,     1] loss: 0.232
[40,     1] loss: 0.207
[41,     1] loss: 0.209
[42,     1] loss: 0.175
[43,     1] loss: 0.234
[44,     1] loss: 0.108
[45,     1] loss: 0.225
[46,     1] loss: 0.107
[47,     1] loss: 0.182
[48,     1] loss: 0.166
[49,     1] loss: 0.108
[50,     1] loss: 0.103
[51,     1] loss: 0.121
[52,     1] loss: 0.098
[53,     1] loss: 0.082
[54,     1] loss: 0.060
[55,     1] loss: 0.197
[56,     1] loss: 0.198
[57,     1] loss: 0.114
[58,     1] loss: 0.320
[59,     1] loss: 0.225
[60,     1] loss: 0.280
[61,     1] loss: 0.270
[62,     1] loss: 0.262
[63,     1] loss: 0.269
[64,     1] loss: 0.253
[65,     1] loss: 0.227
[66,     1] loss: 0.215
[67,     1] loss: 0.198
[68,     1] loss: 0.200
[69,     1] loss: 0.149
[70,     1] loss: 0.210
[71,     1] loss: 0.139
[72,     1] loss: 0.130
[73,     1] loss: 0.175
[74,     1] loss: 0.188
[75,     1] loss: 0.123
[76,     1] loss: 0.152
[77,     1] loss: 0.079
[78,     1] loss: 0.118
[79,     1] loss: 0.113
Early stopping applied (best metric=0.3332371115684509)
Finished Training
Total time taken: 9.513514041900635
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.674
[4,     1] loss: 0.653
[5,     1] loss: 0.605
[6,     1] loss: 0.586
[7,     1] loss: 0.513
[8,     1] loss: 0.503
[9,     1] loss: 0.489
[10,     1] loss: 0.455
[11,     1] loss: 0.414
[12,     1] loss: 0.391
[13,     1] loss: 0.447
[14,     1] loss: 0.373
[15,     1] loss: 0.313
[16,     1] loss: 0.324
[17,     1] loss: 0.345
[18,     1] loss: 0.342
[19,     1] loss: 0.290
[20,     1] loss: 0.332
[21,     1] loss: 0.259
[22,     1] loss: 0.260
[23,     1] loss: 0.250
[24,     1] loss: 0.240
[25,     1] loss: 0.247
[26,     1] loss: 0.223
[27,     1] loss: 0.346
[28,     1] loss: 0.213
[29,     1] loss: 0.205
[30,     1] loss: 0.212
[31,     1] loss: 0.252
[32,     1] loss: 0.267
[33,     1] loss: 0.212
[34,     1] loss: 0.229
[35,     1] loss: 0.185
[36,     1] loss: 0.197
[37,     1] loss: 0.192
[38,     1] loss: 0.164
[39,     1] loss: 0.173
[40,     1] loss: 0.174
[41,     1] loss: 0.167
[42,     1] loss: 0.129
[43,     1] loss: 0.121
[44,     1] loss: 0.146
[45,     1] loss: 0.330
[46,     1] loss: 0.341
[47,     1] loss: 0.291
[48,     1] loss: 0.200
[49,     1] loss: 0.281
[50,     1] loss: 0.244
[51,     1] loss: 0.262
[52,     1] loss: 0.240
[53,     1] loss: 0.230
[54,     1] loss: 0.188
[55,     1] loss: 0.183
[56,     1] loss: 0.166
[57,     1] loss: 0.162
[58,     1] loss: 0.200
[59,     1] loss: 0.142
[60,     1] loss: 0.162
[61,     1] loss: 0.118
[62,     1] loss: 0.224
[63,     1] loss: 0.235
Early stopping applied (best metric=0.25211378931999207)
Finished Training
Total time taken: 7.451516151428223
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.696
[3,     1] loss: 0.687
[4,     1] loss: 0.682
[5,     1] loss: 0.657
[6,     1] loss: 0.635
[7,     1] loss: 0.598
[8,     1] loss: 0.558
[9,     1] loss: 0.540
[10,     1] loss: 0.487
[11,     1] loss: 0.509
[12,     1] loss: 0.439
[13,     1] loss: 0.373
[14,     1] loss: 0.333
[15,     1] loss: 0.422
[16,     1] loss: 0.364
[17,     1] loss: 0.311
[18,     1] loss: 0.354
[19,     1] loss: 0.326
[20,     1] loss: 0.375
[21,     1] loss: 0.337
[22,     1] loss: 0.343
[23,     1] loss: 0.263
[24,     1] loss: 0.284
[25,     1] loss: 0.289
[26,     1] loss: 0.264
[27,     1] loss: 0.306
[28,     1] loss: 0.260
[29,     1] loss: 0.324
[30,     1] loss: 0.299
[31,     1] loss: 0.283
[32,     1] loss: 0.239
[33,     1] loss: 0.291
[34,     1] loss: 0.263
[35,     1] loss: 0.257
[36,     1] loss: 0.216
[37,     1] loss: 0.257
[38,     1] loss: 0.248
[39,     1] loss: 0.258
[40,     1] loss: 0.209
[41,     1] loss: 0.235
[42,     1] loss: 0.258
[43,     1] loss: 0.200
[44,     1] loss: 0.187
[45,     1] loss: 0.178
[46,     1] loss: 0.194
[47,     1] loss: 0.157
[48,     1] loss: 0.203
[49,     1] loss: 0.144
[50,     1] loss: 0.149
[51,     1] loss: 0.203
[52,     1] loss: 0.147
[53,     1] loss: 0.198
[54,     1] loss: 0.132
[55,     1] loss: 0.160
[56,     1] loss: 0.134
[57,     1] loss: 0.247
[58,     1] loss: 0.220
[59,     1] loss: 0.245
[60,     1] loss: 0.145
[61,     1] loss: 0.189
[62,     1] loss: 0.154
[63,     1] loss: 0.151
[64,     1] loss: 0.174
[65,     1] loss: 0.158
[66,     1] loss: 0.162
[67,     1] loss: 0.113
[68,     1] loss: 0.283
[69,     1] loss: 0.243
[70,     1] loss: 0.207
[71,     1] loss: 0.175
[72,     1] loss: 0.233
[73,     1] loss: 0.161
[74,     1] loss: 0.159
[75,     1] loss: 0.184
[76,     1] loss: 0.185
[77,     1] loss: 0.165
[78,     1] loss: 0.155
[79,     1] loss: 0.107
[80,     1] loss: 0.092
[81,     1] loss: 0.131
[82,     1] loss: 0.113
[83,     1] loss: 0.112
[84,     1] loss: 0.078
[85,     1] loss: 0.105
[86,     1] loss: 0.126
[87,     1] loss: 0.129
[88,     1] loss: 0.133
[89,     1] loss: 0.144
[90,     1] loss: 0.131
[91,     1] loss: 0.165
[92,     1] loss: 0.120
[93,     1] loss: 0.109
[94,     1] loss: 0.130
[95,     1] loss: 0.125
[96,     1] loss: 0.132
[97,     1] loss: 0.144
[98,     1] loss: 0.126
[99,     1] loss: 0.154
[100,     1] loss: 0.196
[101,     1] loss: 0.136
[102,     1] loss: 0.169
[103,     1] loss: 0.142
[104,     1] loss: 0.153
Early stopping applied (best metric=0.1593300998210907)
Finished Training
Total time taken: 12.22208571434021
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.670
[4,     1] loss: 0.635
[5,     1] loss: 0.590
[6,     1] loss: 0.515
[7,     1] loss: 0.533
[8,     1] loss: 0.437
[9,     1] loss: 0.424
[10,     1] loss: 0.341
[11,     1] loss: 0.329
[12,     1] loss: 0.340
[13,     1] loss: 0.261
[14,     1] loss: 0.268
[15,     1] loss: 0.312
[16,     1] loss: 0.301
[17,     1] loss: 0.317
[18,     1] loss: 0.285
[19,     1] loss: 0.278
[20,     1] loss: 0.287
[21,     1] loss: 0.275
[22,     1] loss: 0.238
[23,     1] loss: 0.234
[24,     1] loss: 0.272
[25,     1] loss: 0.217
[26,     1] loss: 0.171
[27,     1] loss: 0.228
[28,     1] loss: 0.231
[29,     1] loss: 0.669
[30,     1] loss: 0.236
[31,     1] loss: 0.294
[32,     1] loss: 0.347
[33,     1] loss: 0.321
[34,     1] loss: 0.356
[35,     1] loss: 0.298
[36,     1] loss: 0.253
[37,     1] loss: 0.269
[38,     1] loss: 0.285
[39,     1] loss: 0.242
[40,     1] loss: 0.195
[41,     1] loss: 0.275
[42,     1] loss: 0.198
[43,     1] loss: 0.198
[44,     1] loss: 0.149
[45,     1] loss: 0.174
[46,     1] loss: 0.179
[47,     1] loss: 0.130
[48,     1] loss: 0.157
[49,     1] loss: 0.155
[50,     1] loss: 0.219
[51,     1] loss: 0.143
[52,     1] loss: 0.287
[53,     1] loss: 0.171
[54,     1] loss: 0.288
[55,     1] loss: 0.181
[56,     1] loss: 0.167
[57,     1] loss: 0.210
[58,     1] loss: 0.205
[59,     1] loss: 0.136
[60,     1] loss: 0.190
[61,     1] loss: 0.131
[62,     1] loss: 0.186
[63,     1] loss: 0.195
[64,     1] loss: 0.146
[65,     1] loss: 0.148
[66,     1] loss: 0.130
[67,     1] loss: 0.147
[68,     1] loss: 0.162
[69,     1] loss: 0.130
[70,     1] loss: 0.127
[71,     1] loss: 0.156
[72,     1] loss: 0.121
[73,     1] loss: 0.130
[74,     1] loss: 0.295
[75,     1] loss: 0.118
[76,     1] loss: 0.179
[77,     1] loss: 0.125
[78,     1] loss: 0.110
[79,     1] loss: 0.222
[80,     1] loss: 0.129
[81,     1] loss: 0.158
[82,     1] loss: 0.144
[83,     1] loss: 0.121
[84,     1] loss: 0.167
[85,     1] loss: 0.166
[86,     1] loss: 0.147
[87,     1] loss: 0.112
[88,     1] loss: 0.108
[89,     1] loss: 0.079
[90,     1] loss: 0.107
[91,     1] loss: 0.131
[92,     1] loss: 0.322
[93,     1] loss: 0.264
[94,     1] loss: 0.157
[95,     1] loss: 0.193
[96,     1] loss: 0.187
[97,     1] loss: 0.149
[98,     1] loss: 0.152
[99,     1] loss: 0.189
[100,     1] loss: 0.152
[101,     1] loss: 0.147
[102,     1] loss: 0.130
[103,     1] loss: 0.132
[104,     1] loss: 0.092
[105,     1] loss: 0.141
[106,     1] loss: 0.111
[107,     1] loss: 0.086
[108,     1] loss: 0.161
[109,     1] loss: 0.130
[110,     1] loss: 0.110
[111,     1] loss: 0.176
[112,     1] loss: 0.159
[113,     1] loss: 0.138
[114,     1] loss: 0.119
[115,     1] loss: 0.134
[116,     1] loss: 0.099
[117,     1] loss: 0.105
[118,     1] loss: 0.082
[119,     1] loss: 0.094
[120,     1] loss: 0.204
[121,     1] loss: 0.409
[122,     1] loss: 0.387
[123,     1] loss: 0.397
[124,     1] loss: 0.281
[125,     1] loss: 0.240
[126,     1] loss: 0.275
[127,     1] loss: 0.299
Early stopping applied (best metric=0.272602915763855)
Finished Training
Total time taken: 15.168133020401001
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.705
[3,     1] loss: 0.686
[4,     1] loss: 0.671
[5,     1] loss: 0.648
[6,     1] loss: 0.607
[7,     1] loss: 0.572
[8,     1] loss: 0.509
[9,     1] loss: 0.486
[10,     1] loss: 0.421
[11,     1] loss: 0.423
[12,     1] loss: 0.395
[13,     1] loss: 0.356
[14,     1] loss: 0.360
[15,     1] loss: 0.286
[16,     1] loss: 0.298
[17,     1] loss: 0.289
[18,     1] loss: 0.302
[19,     1] loss: 0.232
[20,     1] loss: 0.204
[21,     1] loss: 0.205
[22,     1] loss: 0.302
[23,     1] loss: 0.257
[24,     1] loss: 0.239
[25,     1] loss: 0.209
[26,     1] loss: 0.287
[27,     1] loss: 0.253
[28,     1] loss: 0.212
[29,     1] loss: 0.238
[30,     1] loss: 0.182
[31,     1] loss: 0.202
[32,     1] loss: 0.194
[33,     1] loss: 0.186
[34,     1] loss: 0.170
[35,     1] loss: 0.251
[36,     1] loss: 0.179
[37,     1] loss: 0.207
[38,     1] loss: 0.241
[39,     1] loss: 0.240
[40,     1] loss: 0.206
[41,     1] loss: 0.155
[42,     1] loss: 0.166
[43,     1] loss: 0.200
[44,     1] loss: 0.170
[45,     1] loss: 0.219
[46,     1] loss: 0.136
[47,     1] loss: 0.263
[48,     1] loss: 0.162
[49,     1] loss: 0.312
[50,     1] loss: 0.193
[51,     1] loss: 0.190
[52,     1] loss: 0.264
[53,     1] loss: 0.186
[54,     1] loss: 0.212
[55,     1] loss: 0.220
[56,     1] loss: 0.144
[57,     1] loss: 0.217
[58,     1] loss: 0.199
[59,     1] loss: 0.162
[60,     1] loss: 0.167
[61,     1] loss: 0.169
[62,     1] loss: 0.135
[63,     1] loss: 0.104
[64,     1] loss: 0.139
[65,     1] loss: 0.163
[66,     1] loss: 0.110
[67,     1] loss: 0.110
[68,     1] loss: 0.083
[69,     1] loss: 0.129
[70,     1] loss: 0.098
[71,     1] loss: 0.112
[72,     1] loss: 0.100
[73,     1] loss: 0.104
[74,     1] loss: 0.131
[75,     1] loss: 0.138
[76,     1] loss: 0.129
[77,     1] loss: 0.196
[78,     1] loss: 0.190
[79,     1] loss: 0.143
[80,     1] loss: 0.130
[81,     1] loss: 0.088
[82,     1] loss: 0.091
[83,     1] loss: 0.118
[84,     1] loss: 0.096
[85,     1] loss: 0.087
[86,     1] loss: 0.149
[87,     1] loss: 0.309
[88,     1] loss: 0.369
[89,     1] loss: 0.238
[90,     1] loss: 0.376
[91,     1] loss: 0.230
[92,     1] loss: 0.248
[93,     1] loss: 0.279
[94,     1] loss: 0.270
[95,     1] loss: 0.221
[96,     1] loss: 0.233
[97,     1] loss: 0.252
[98,     1] loss: 0.229
[99,     1] loss: 0.216
[100,     1] loss: 0.199
[101,     1] loss: 0.210
[102,     1] loss: 0.141
[103,     1] loss: 0.162
Early stopping applied (best metric=0.351747989654541)
Finished Training
Total time taken: 11.661592245101929
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.672
[4,     1] loss: 0.643
[5,     1] loss: 0.606
[6,     1] loss: 0.550
[7,     1] loss: 0.508
[8,     1] loss: 0.469
[9,     1] loss: 0.457
[10,     1] loss: 0.416
[11,     1] loss: 0.369
[12,     1] loss: 0.287
[13,     1] loss: 0.300
[14,     1] loss: 0.247
[15,     1] loss: 0.315
[16,     1] loss: 0.359
[17,     1] loss: 0.368
[18,     1] loss: 0.270
[19,     1] loss: 0.293
[20,     1] loss: 0.287
[21,     1] loss: 0.259
[22,     1] loss: 0.253
[23,     1] loss: 0.246
[24,     1] loss: 0.267
[25,     1] loss: 0.225
[26,     1] loss: 0.197
[27,     1] loss: 0.241
[28,     1] loss: 0.256
[29,     1] loss: 0.170
[30,     1] loss: 0.356
[31,     1] loss: 0.319
[32,     1] loss: 0.311
[33,     1] loss: 0.353
[34,     1] loss: 0.293
[35,     1] loss: 0.318
[36,     1] loss: 0.259
[37,     1] loss: 0.270
[38,     1] loss: 0.302
[39,     1] loss: 0.264
[40,     1] loss: 0.271
[41,     1] loss: 0.239
[42,     1] loss: 0.242
[43,     1] loss: 0.219
[44,     1] loss: 0.191
[45,     1] loss: 0.263
[46,     1] loss: 0.171
[47,     1] loss: 0.215
[48,     1] loss: 0.209
[49,     1] loss: 0.144
[50,     1] loss: 0.229
[51,     1] loss: 0.155
[52,     1] loss: 0.224
[53,     1] loss: 0.238
[54,     1] loss: 0.125
[55,     1] loss: 0.164
[56,     1] loss: 0.106
Early stopping applied (best metric=0.41324758529663086)
Finished Training
Total time taken: 6.558122158050537
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.697
[3,     1] loss: 0.691
[4,     1] loss: 0.690
[5,     1] loss: 0.693
[6,     1] loss: 0.688
[7,     1] loss: 0.680
[8,     1] loss: 0.672
[9,     1] loss: 0.648
[10,     1] loss: 0.626
[11,     1] loss: 0.617
[12,     1] loss: 0.576
[13,     1] loss: 0.541
[14,     1] loss: 0.524
[15,     1] loss: 0.520
[16,     1] loss: 0.486
[17,     1] loss: 0.405
[18,     1] loss: 0.445
[19,     1] loss: 0.465
[20,     1] loss: 0.372
[21,     1] loss: 0.405
[22,     1] loss: 0.363
[23,     1] loss: 0.380
[24,     1] loss: 0.333
[25,     1] loss: 0.302
[26,     1] loss: 0.342
[27,     1] loss: 0.320
[28,     1] loss: 0.277
[29,     1] loss: 0.252
[30,     1] loss: 0.369
[31,     1] loss: 0.227
[32,     1] loss: 0.288
[33,     1] loss: 0.282
[34,     1] loss: 0.323
[35,     1] loss: 0.248
[36,     1] loss: 0.238
[37,     1] loss: 0.250
[38,     1] loss: 0.263
[39,     1] loss: 0.289
[40,     1] loss: 0.250
[41,     1] loss: 0.246
[42,     1] loss: 0.244
[43,     1] loss: 0.235
[44,     1] loss: 0.189
[45,     1] loss: 0.257
[46,     1] loss: 0.198
[47,     1] loss: 0.196
[48,     1] loss: 0.228
[49,     1] loss: 0.251
[50,     1] loss: 0.200
[51,     1] loss: 0.150
[52,     1] loss: 0.185
[53,     1] loss: 0.183
[54,     1] loss: 0.169
[55,     1] loss: 0.207
[56,     1] loss: 0.148
[57,     1] loss: 0.207
[58,     1] loss: 0.238
[59,     1] loss: 0.242
[60,     1] loss: 0.155
[61,     1] loss: 0.179
[62,     1] loss: 0.170
[63,     1] loss: 0.150
[64,     1] loss: 0.198
[65,     1] loss: 0.175
[66,     1] loss: 0.174
[67,     1] loss: 0.135
[68,     1] loss: 0.136
[69,     1] loss: 0.129
[70,     1] loss: 0.112
[71,     1] loss: 0.234
Early stopping applied (best metric=0.3245486915111542)
Finished Training
Total time taken: 8.607700824737549
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.662
[4,     1] loss: 0.638
[5,     1] loss: 0.583
[6,     1] loss: 0.515
[7,     1] loss: 0.515
[8,     1] loss: 0.435
[9,     1] loss: 0.370
[10,     1] loss: 0.361
[11,     1] loss: 0.384
[12,     1] loss: 0.452
[13,     1] loss: 0.399
[14,     1] loss: 0.336
[15,     1] loss: 0.307
[16,     1] loss: 0.388
[17,     1] loss: 0.357
[18,     1] loss: 0.317
[19,     1] loss: 0.334
[20,     1] loss: 0.309
[21,     1] loss: 0.288
[22,     1] loss: 0.318
[23,     1] loss: 0.240
[24,     1] loss: 0.247
[25,     1] loss: 0.209
[26,     1] loss: 0.204
[27,     1] loss: 0.213
[28,     1] loss: 0.218
[29,     1] loss: 0.270
[30,     1] loss: 0.162
[31,     1] loss: 0.253
[32,     1] loss: 0.203
[33,     1] loss: 0.277
[34,     1] loss: 0.259
[35,     1] loss: 0.239
[36,     1] loss: 0.274
[37,     1] loss: 0.221
[38,     1] loss: 0.214
[39,     1] loss: 0.237
[40,     1] loss: 0.259
[41,     1] loss: 0.195
[42,     1] loss: 0.242
[43,     1] loss: 0.201
[44,     1] loss: 0.295
[45,     1] loss: 0.218
[46,     1] loss: 0.253
[47,     1] loss: 0.246
[48,     1] loss: 0.169
[49,     1] loss: 0.172
[50,     1] loss: 0.217
[51,     1] loss: 0.186
[52,     1] loss: 0.193
[53,     1] loss: 0.170
[54,     1] loss: 0.218
[55,     1] loss: 0.180
[56,     1] loss: 0.214
[57,     1] loss: 0.155
Early stopping applied (best metric=0.4000941514968872)
Finished Training
Total time taken: 7.124077796936035
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.700
[3,     1] loss: 0.689
[4,     1] loss: 0.678
[5,     1] loss: 0.645
[6,     1] loss: 0.623
[7,     1] loss: 0.579
[8,     1] loss: 0.551
[9,     1] loss: 0.517
[10,     1] loss: 0.417
[11,     1] loss: 0.432
[12,     1] loss: 0.442
[13,     1] loss: 0.400
[14,     1] loss: 0.295
[15,     1] loss: 0.321
[16,     1] loss: 0.259
[17,     1] loss: 0.330
[18,     1] loss: 0.280
[19,     1] loss: 0.254
[20,     1] loss: 0.294
[21,     1] loss: 0.224
[22,     1] loss: 0.321
[23,     1] loss: 0.263
[24,     1] loss: 0.311
[25,     1] loss: 0.340
[26,     1] loss: 0.232
[27,     1] loss: 0.310
[28,     1] loss: 0.230
[29,     1] loss: 0.213
[30,     1] loss: 0.325
[31,     1] loss: 0.224
[32,     1] loss: 0.192
[33,     1] loss: 0.218
[34,     1] loss: 0.194
[35,     1] loss: 0.193
[36,     1] loss: 0.187
[37,     1] loss: 0.245
[38,     1] loss: 0.177
[39,     1] loss: 0.155
[40,     1] loss: 0.160
[41,     1] loss: 0.187
[42,     1] loss: 0.154
[43,     1] loss: 0.155
[44,     1] loss: 0.188
[45,     1] loss: 0.173
[46,     1] loss: 0.124
[47,     1] loss: 0.117
[48,     1] loss: 0.107
[49,     1] loss: 0.158
[50,     1] loss: 0.203
[51,     1] loss: 0.151
[52,     1] loss: 0.323
[53,     1] loss: 0.257
[54,     1] loss: 0.266
[55,     1] loss: 0.220
[56,     1] loss: 0.242
[57,     1] loss: 0.225
[58,     1] loss: 0.215
[59,     1] loss: 0.241
[60,     1] loss: 0.220
[61,     1] loss: 0.193
[62,     1] loss: 0.172
[63,     1] loss: 0.162
[64,     1] loss: 0.219
[65,     1] loss: 0.157
[66,     1] loss: 0.194
[67,     1] loss: 0.181
[68,     1] loss: 0.190
[69,     1] loss: 0.213
[70,     1] loss: 0.139
[71,     1] loss: 0.154
[72,     1] loss: 0.133
[73,     1] loss: 0.174
[74,     1] loss: 0.149
[75,     1] loss: 0.132
[76,     1] loss: 0.154
[77,     1] loss: 0.126
[78,     1] loss: 0.101
[79,     1] loss: 0.147
[80,     1] loss: 0.122
[81,     1] loss: 0.137
[82,     1] loss: 0.163
[83,     1] loss: 0.193
[84,     1] loss: 0.129
[85,     1] loss: 0.163
[86,     1] loss: 0.112
[87,     1] loss: 0.146
[88,     1] loss: 0.187
[89,     1] loss: 0.146
[90,     1] loss: 0.114
[91,     1] loss: 0.117
[92,     1] loss: 0.117
[93,     1] loss: 0.131
[94,     1] loss: 0.127
[95,     1] loss: 0.177
Early stopping applied (best metric=0.21868658065795898)
Finished Training
Total time taken: 11.467469215393066
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.675
[4,     1] loss: 0.662
[5,     1] loss: 0.628
[6,     1] loss: 0.569
[7,     1] loss: 0.556
[8,     1] loss: 0.523
[9,     1] loss: 0.443
[10,     1] loss: 0.415
[11,     1] loss: 0.408
[12,     1] loss: 0.371
[13,     1] loss: 0.353
[14,     1] loss: 0.312
[15,     1] loss: 0.418
[16,     1] loss: 0.312
[17,     1] loss: 0.294
[18,     1] loss: 0.347
[19,     1] loss: 0.378
[20,     1] loss: 0.359
[21,     1] loss: 0.352
[22,     1] loss: 0.343
[23,     1] loss: 0.308
[24,     1] loss: 0.303
[25,     1] loss: 0.296
[26,     1] loss: 0.262
[27,     1] loss: 0.287
[28,     1] loss: 0.234
[29,     1] loss: 0.192
[30,     1] loss: 0.210
[31,     1] loss: 0.175
[32,     1] loss: 0.170
[33,     1] loss: 0.217
[34,     1] loss: 0.181
[35,     1] loss: 0.373
[36,     1] loss: 0.248
[37,     1] loss: 0.254
[38,     1] loss: 0.231
[39,     1] loss: 0.257
[40,     1] loss: 0.226
[41,     1] loss: 0.175
[42,     1] loss: 0.197
[43,     1] loss: 0.212
[44,     1] loss: 0.196
[45,     1] loss: 0.190
[46,     1] loss: 0.166
[47,     1] loss: 0.166
[48,     1] loss: 0.162
[49,     1] loss: 0.138
[50,     1] loss: 0.112
[51,     1] loss: 0.194
[52,     1] loss: 0.132
[53,     1] loss: 0.169
[54,     1] loss: 0.159
[55,     1] loss: 0.200
[56,     1] loss: 0.195
[57,     1] loss: 0.177
[58,     1] loss: 0.164
[59,     1] loss: 0.164
[60,     1] loss: 0.165
[61,     1] loss: 0.108
[62,     1] loss: 0.104
[63,     1] loss: 0.115
[64,     1] loss: 0.137
[65,     1] loss: 0.130
[66,     1] loss: 0.141
[67,     1] loss: 0.209
[68,     1] loss: 0.124
[69,     1] loss: 0.189
[70,     1] loss: 0.151
[71,     1] loss: 0.222
[72,     1] loss: 0.188
[73,     1] loss: 0.129
[74,     1] loss: 0.135
[75,     1] loss: 0.131
[76,     1] loss: 0.105
[77,     1] loss: 0.099
[78,     1] loss: 0.147
[79,     1] loss: 0.112
[80,     1] loss: 0.102
[81,     1] loss: 0.087
[82,     1] loss: 0.077
[83,     1] loss: 0.153
[84,     1] loss: 0.125
[85,     1] loss: 0.437
[86,     1] loss: 0.375
[87,     1] loss: 0.418
[88,     1] loss: 0.469
[89,     1] loss: 0.346
[90,     1] loss: 0.296
[91,     1] loss: 0.291
[92,     1] loss: 0.321
[93,     1] loss: 0.306
[94,     1] loss: 0.324
[95,     1] loss: 0.294
[96,     1] loss: 0.295
[97,     1] loss: 0.282
Early stopping applied (best metric=0.17604859173297882)
Finished Training
Total time taken: 11.771758079528809
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.695
[3,     1] loss: 0.680
[4,     1] loss: 0.654
[5,     1] loss: 0.625
[6,     1] loss: 0.583
[7,     1] loss: 0.509
[8,     1] loss: 0.461
[9,     1] loss: 0.420
[10,     1] loss: 0.385
[11,     1] loss: 0.384
[12,     1] loss: 0.327
[13,     1] loss: 0.283
[14,     1] loss: 0.335
[15,     1] loss: 0.331
[16,     1] loss: 0.324
[17,     1] loss: 0.325
[18,     1] loss: 0.391
[19,     1] loss: 0.364
[20,     1] loss: 0.230
[21,     1] loss: 0.259
[22,     1] loss: 0.260
[23,     1] loss: 0.219
[24,     1] loss: 0.249
[25,     1] loss: 0.226
[26,     1] loss: 0.218
[27,     1] loss: 0.176
[28,     1] loss: 0.174
[29,     1] loss: 0.213
[30,     1] loss: 0.171
[31,     1] loss: 0.184
[32,     1] loss: 0.154
[33,     1] loss: 0.221
[34,     1] loss: 0.182
[35,     1] loss: 0.100
[36,     1] loss: 0.193
[37,     1] loss: 0.129
[38,     1] loss: 0.203
[39,     1] loss: 0.194
[40,     1] loss: 0.229
[41,     1] loss: 0.199
[42,     1] loss: 0.158
[43,     1] loss: 0.176
[44,     1] loss: 0.190
[45,     1] loss: 0.178
[46,     1] loss: 0.175
[47,     1] loss: 0.117
[48,     1] loss: 0.155
[49,     1] loss: 0.129
[50,     1] loss: 0.110
[51,     1] loss: 0.122
[52,     1] loss: 0.088
[53,     1] loss: 0.121
[54,     1] loss: 0.086
[55,     1] loss: 0.066
[56,     1] loss: 0.070
[57,     1] loss: 0.072
[58,     1] loss: 0.079
[59,     1] loss: 0.081
[60,     1] loss: 0.257
[61,     1] loss: 0.258
[62,     1] loss: 0.201
[63,     1] loss: 0.108
[64,     1] loss: 0.201
[65,     1] loss: 0.144
[66,     1] loss: 0.168
[67,     1] loss: 0.165
[68,     1] loss: 0.134
[69,     1] loss: 0.136
[70,     1] loss: 0.145
[71,     1] loss: 0.141
[72,     1] loss: 0.120
[73,     1] loss: 0.090
Early stopping applied (best metric=0.2351267784833908)
Finished Training
Total time taken: 8.898781776428223
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.676
[4,     1] loss: 0.658
[5,     1] loss: 0.612
[6,     1] loss: 0.556
[7,     1] loss: 0.532
[8,     1] loss: 0.438
[9,     1] loss: 0.439
[10,     1] loss: 0.341
[11,     1] loss: 0.340
[12,     1] loss: 0.345
[13,     1] loss: 0.273
[14,     1] loss: 0.279
[15,     1] loss: 0.321
[16,     1] loss: 0.379
[17,     1] loss: 0.232
[18,     1] loss: 0.282
[19,     1] loss: 0.221
[20,     1] loss: 0.218
[21,     1] loss: 0.220
[22,     1] loss: 0.230
[23,     1] loss: 0.181
[24,     1] loss: 0.200
[25,     1] loss: 0.237
[26,     1] loss: 0.181
[27,     1] loss: 0.228
[28,     1] loss: 0.142
[29,     1] loss: 0.187
[30,     1] loss: 0.151
[31,     1] loss: 0.153
[32,     1] loss: 0.170
[33,     1] loss: 0.131
[34,     1] loss: 0.159
[35,     1] loss: 0.152
[36,     1] loss: 0.097
[37,     1] loss: 0.170
[38,     1] loss: 0.127
[39,     1] loss: 0.154
[40,     1] loss: 0.232
[41,     1] loss: 0.266
[42,     1] loss: 0.239
[43,     1] loss: 0.175
[44,     1] loss: 0.176
[45,     1] loss: 0.207
[46,     1] loss: 0.189
[47,     1] loss: 0.151
[48,     1] loss: 0.170
[49,     1] loss: 0.147
[50,     1] loss: 0.166
[51,     1] loss: 0.119
[52,     1] loss: 0.106
[53,     1] loss: 0.112
[54,     1] loss: 0.153
[55,     1] loss: 0.084
[56,     1] loss: 0.097
[57,     1] loss: 0.121
[58,     1] loss: 0.178
[59,     1] loss: 0.284
[60,     1] loss: 0.355
[61,     1] loss: 0.232
[62,     1] loss: 0.259
[63,     1] loss: 0.208
[64,     1] loss: 0.210
Early stopping applied (best metric=0.2600264251232147)
Finished Training
Total time taken: 7.519571542739868
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.699
[3,     1] loss: 0.674
[4,     1] loss: 0.641
[5,     1] loss: 0.605
[6,     1] loss: 0.570
[7,     1] loss: 0.523
[8,     1] loss: 0.489
[9,     1] loss: 0.510
[10,     1] loss: 0.403
[11,     1] loss: 0.598
[12,     1] loss: 0.352
[13,     1] loss: 0.348
[14,     1] loss: 0.338
[15,     1] loss: 0.327
[16,     1] loss: 0.331
[17,     1] loss: 0.319
[18,     1] loss: 0.293
[19,     1] loss: 0.297
[20,     1] loss: 0.272
[21,     1] loss: 0.276
[22,     1] loss: 0.259
[23,     1] loss: 0.281
[24,     1] loss: 0.244
[25,     1] loss: 0.275
[26,     1] loss: 0.232
[27,     1] loss: 0.264
[28,     1] loss: 0.226
[29,     1] loss: 0.216
[30,     1] loss: 0.264
[31,     1] loss: 0.193
[32,     1] loss: 0.214
[33,     1] loss: 0.261
[34,     1] loss: 0.246
[35,     1] loss: 0.205
[36,     1] loss: 0.169
[37,     1] loss: 0.188
[38,     1] loss: 0.216
[39,     1] loss: 0.154
[40,     1] loss: 0.137
[41,     1] loss: 0.139
[42,     1] loss: 0.184
[43,     1] loss: 0.326
[44,     1] loss: 0.187
[45,     1] loss: 0.162
[46,     1] loss: 0.155
[47,     1] loss: 0.169
[48,     1] loss: 0.149
[49,     1] loss: 0.178
[50,     1] loss: 0.172
[51,     1] loss: 0.132
[52,     1] loss: 0.175
[53,     1] loss: 0.135
[54,     1] loss: 0.205
[55,     1] loss: 0.117
[56,     1] loss: 0.093
[57,     1] loss: 0.133
[58,     1] loss: 0.150
[59,     1] loss: 0.120
[60,     1] loss: 0.219
[61,     1] loss: 0.125
[62,     1] loss: 0.127
[63,     1] loss: 0.077
[64,     1] loss: 0.158
[65,     1] loss: 0.133
[66,     1] loss: 0.128
[67,     1] loss: 0.189
[68,     1] loss: 0.128
[69,     1] loss: 0.111
[70,     1] loss: 0.105
[71,     1] loss: 0.157
[72,     1] loss: 0.287
[73,     1] loss: 0.229
[74,     1] loss: 0.131
[75,     1] loss: 0.190
[76,     1] loss: 0.174
[77,     1] loss: 0.132
[78,     1] loss: 0.151
[79,     1] loss: 0.121
[80,     1] loss: 0.108
[81,     1] loss: 0.140
[82,     1] loss: 0.095
[83,     1] loss: 0.088
[84,     1] loss: 0.094
[85,     1] loss: 0.226
[86,     1] loss: 0.372
[87,     1] loss: 0.199
[88,     1] loss: 0.359
[89,     1] loss: 0.346
[90,     1] loss: 0.292
[91,     1] loss: 0.260
[92,     1] loss: 0.265
[93,     1] loss: 0.254
[94,     1] loss: 0.264
[95,     1] loss: 0.273
[96,     1] loss: 0.245
[97,     1] loss: 0.217
[98,     1] loss: 0.204
[99,     1] loss: 0.164
[100,     1] loss: 0.165
[101,     1] loss: 0.180
[102,     1] loss: 0.178
[103,     1] loss: 0.173
[104,     1] loss: 0.139
[105,     1] loss: 0.154
[106,     1] loss: 0.130
[107,     1] loss: 0.124
[108,     1] loss: 0.081
[109,     1] loss: 0.117
[110,     1] loss: 0.119
[111,     1] loss: 0.137
[112,     1] loss: 0.135
[113,     1] loss: 0.149
[114,     1] loss: 0.113
[115,     1] loss: 0.175
[116,     1] loss: 0.122
[117,     1] loss: 0.225
[118,     1] loss: 0.110
[119,     1] loss: 0.321
[120,     1] loss: 0.252
[121,     1] loss: 0.240
[122,     1] loss: 0.231
[123,     1] loss: 0.218
[124,     1] loss: 0.183
[125,     1] loss: 0.178
[126,     1] loss: 0.159
[127,     1] loss: 0.180
[128,     1] loss: 0.107
[129,     1] loss: 0.110
[130,     1] loss: 0.101
[131,     1] loss: 0.114
[132,     1] loss: 0.112
[133,     1] loss: 0.125
[134,     1] loss: 0.121
[135,     1] loss: 0.106
[136,     1] loss: 0.134
[137,     1] loss: 0.085
[138,     1] loss: 0.122
[139,     1] loss: 0.122
[140,     1] loss: 0.098
[141,     1] loss: 0.088
[142,     1] loss: 0.193
Early stopping applied (best metric=0.27900633215904236)
Finished Training
Total time taken: 16.639183521270752
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.687
[2,     1] loss: 0.695
[3,     1] loss: 0.674
[4,     1] loss: 0.629
[5,     1] loss: 0.598
[6,     1] loss: 0.548
[7,     1] loss: 0.501
[8,     1] loss: 0.446
[9,     1] loss: 0.475
[10,     1] loss: 0.395
[11,     1] loss: 0.442
[12,     1] loss: 0.336
[13,     1] loss: 0.347
[14,     1] loss: 0.310
[15,     1] loss: 0.328
[16,     1] loss: 0.319
[17,     1] loss: 0.267
[18,     1] loss: 0.262
[19,     1] loss: 0.325
[20,     1] loss: 0.323
[21,     1] loss: 0.302
[22,     1] loss: 0.250
[23,     1] loss: 0.239
[24,     1] loss: 0.246
[25,     1] loss: 0.263
[26,     1] loss: 0.183
[27,     1] loss: 0.179
[28,     1] loss: 0.159
[29,     1] loss: 0.237
[30,     1] loss: 0.177
[31,     1] loss: 0.179
[32,     1] loss: 0.206
[33,     1] loss: 0.193
[34,     1] loss: 0.158
[35,     1] loss: 0.136
[36,     1] loss: 0.130
[37,     1] loss: 0.163
[38,     1] loss: 0.158
[39,     1] loss: 0.463
[40,     1] loss: 0.261
[41,     1] loss: 0.250
[42,     1] loss: 0.211
[43,     1] loss: 0.282
[44,     1] loss: 0.279
[45,     1] loss: 0.262
[46,     1] loss: 0.281
[47,     1] loss: 0.266
[48,     1] loss: 0.248
[49,     1] loss: 0.223
[50,     1] loss: 0.209
[51,     1] loss: 0.225
[52,     1] loss: 0.181
[53,     1] loss: 0.194
[54,     1] loss: 0.178
[55,     1] loss: 0.212
[56,     1] loss: 0.175
[57,     1] loss: 0.228
[58,     1] loss: 0.163
[59,     1] loss: 0.212
[60,     1] loss: 0.194
Early stopping applied (best metric=0.29936063289642334)
Finished Training
Total time taken: 7.172158241271973
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.698
[3,     1] loss: 0.684
[4,     1] loss: 0.672
[5,     1] loss: 0.639
[6,     1] loss: 0.606
[7,     1] loss: 0.588
[8,     1] loss: 0.547
[9,     1] loss: 0.487
[10,     1] loss: 0.446
[11,     1] loss: 0.375
[12,     1] loss: 0.353
[13,     1] loss: 0.337
[14,     1] loss: 0.319
[15,     1] loss: 0.292
[16,     1] loss: 0.314
[17,     1] loss: 0.344
[18,     1] loss: 0.271
[19,     1] loss: 0.223
[20,     1] loss: 0.265
[21,     1] loss: 0.263
[22,     1] loss: 0.237
[23,     1] loss: 0.330
[24,     1] loss: 0.233
[25,     1] loss: 0.267
[26,     1] loss: 0.309
[27,     1] loss: 0.258
[28,     1] loss: 0.224
[29,     1] loss: 0.256
[30,     1] loss: 0.228
[31,     1] loss: 0.269
[32,     1] loss: 0.223
[33,     1] loss: 0.204
[34,     1] loss: 0.209
[35,     1] loss: 0.190
[36,     1] loss: 0.176
[37,     1] loss: 0.229
[38,     1] loss: 0.166
[39,     1] loss: 0.255
[40,     1] loss: 0.204
[41,     1] loss: 0.191
[42,     1] loss: 0.241
[43,     1] loss: 0.180
[44,     1] loss: 0.197
[45,     1] loss: 0.194
[46,     1] loss: 0.164
[47,     1] loss: 0.178
[48,     1] loss: 0.172
[49,     1] loss: 0.197
[50,     1] loss: 0.156
[51,     1] loss: 0.179
[52,     1] loss: 0.131
[53,     1] loss: 0.196
[54,     1] loss: 0.138
[55,     1] loss: 0.194
[56,     1] loss: 0.129
[57,     1] loss: 0.204
[58,     1] loss: 0.201
[59,     1] loss: 0.235
[60,     1] loss: 0.135
[61,     1] loss: 0.248
[62,     1] loss: 0.207
[63,     1] loss: 0.204
[64,     1] loss: 0.174
[65,     1] loss: 0.203
[66,     1] loss: 0.133
[67,     1] loss: 0.147
[68,     1] loss: 0.119
[69,     1] loss: 0.112
[70,     1] loss: 0.154
[71,     1] loss: 0.158
[72,     1] loss: 0.227
[73,     1] loss: 0.198
[74,     1] loss: 0.123
[75,     1] loss: 0.202
[76,     1] loss: 0.213
[77,     1] loss: 0.191
Early stopping applied (best metric=0.3631601929664612)
Finished Training
Total time taken: 9.163757085800171
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.669
[4,     1] loss: 0.635
[5,     1] loss: 0.547
[6,     1] loss: 0.497
[7,     1] loss: 0.432
[8,     1] loss: 0.405
[9,     1] loss: 0.354
[10,     1] loss: 0.334
[11,     1] loss: 0.307
[12,     1] loss: 0.263
[13,     1] loss: 0.360
[14,     1] loss: 0.252
[15,     1] loss: 0.380
[16,     1] loss: 0.274
[17,     1] loss: 0.254
[18,     1] loss: 0.305
[19,     1] loss: 0.294
[20,     1] loss: 0.262
[21,     1] loss: 0.296
[22,     1] loss: 0.250
[23,     1] loss: 0.278
[24,     1] loss: 0.233
[25,     1] loss: 0.254
[26,     1] loss: 0.199
[27,     1] loss: 0.225
[28,     1] loss: 0.217
[29,     1] loss: 0.274
[30,     1] loss: 0.261
[31,     1] loss: 0.207
[32,     1] loss: 0.215
[33,     1] loss: 0.193
[34,     1] loss: 0.235
[35,     1] loss: 0.214
[36,     1] loss: 0.261
[37,     1] loss: 0.236
[38,     1] loss: 0.234
[39,     1] loss: 0.198
[40,     1] loss: 0.263
[41,     1] loss: 0.193
[42,     1] loss: 0.218
[43,     1] loss: 0.190
[44,     1] loss: 0.221
[45,     1] loss: 0.172
[46,     1] loss: 0.186
[47,     1] loss: 0.176
[48,     1] loss: 0.289
[49,     1] loss: 0.274
[50,     1] loss: 0.231
[51,     1] loss: 0.278
[52,     1] loss: 0.162
[53,     1] loss: 0.207
[54,     1] loss: 0.199
[55,     1] loss: 0.190
[56,     1] loss: 0.173
[57,     1] loss: 0.259
[58,     1] loss: 0.176
Early stopping applied (best metric=0.4153698682785034)
Finished Training
Total time taken: 6.7005908489227295
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.668
[4,     1] loss: 0.631
[5,     1] loss: 0.584
[6,     1] loss: 0.554
[7,     1] loss: 0.522
[8,     1] loss: 0.457
[9,     1] loss: 0.377
[10,     1] loss: 0.339
[11,     1] loss: 0.367
[12,     1] loss: 0.499
[13,     1] loss: 0.281
[14,     1] loss: 0.367
[15,     1] loss: 0.308
[16,     1] loss: 0.251
[17,     1] loss: 0.270
[18,     1] loss: 0.295
[19,     1] loss: 0.266
[20,     1] loss: 0.234
[21,     1] loss: 0.315
[22,     1] loss: 0.238
[23,     1] loss: 0.224
[24,     1] loss: 0.235
[25,     1] loss: 0.237
[26,     1] loss: 0.208
[27,     1] loss: 0.173
[28,     1] loss: 0.164
[29,     1] loss: 0.159
[30,     1] loss: 0.158
[31,     1] loss: 0.162
[32,     1] loss: 0.156
[33,     1] loss: 0.193
[34,     1] loss: 0.113
[35,     1] loss: 0.245
[36,     1] loss: 0.183
[37,     1] loss: 0.240
[38,     1] loss: 0.170
[39,     1] loss: 0.166
[40,     1] loss: 0.194
[41,     1] loss: 0.147
[42,     1] loss: 0.159
[43,     1] loss: 0.141
[44,     1] loss: 0.168
[45,     1] loss: 0.113
[46,     1] loss: 0.134
[47,     1] loss: 0.178
[48,     1] loss: 0.143
[49,     1] loss: 0.191
[50,     1] loss: 0.184
[51,     1] loss: 0.107
[52,     1] loss: 0.173
[53,     1] loss: 0.141
[54,     1] loss: 0.145
[55,     1] loss: 0.138
Early stopping applied (best metric=0.5126771330833435)
Finished Training
Total time taken: 6.217250823974609
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.694
[3,     1] loss: 0.692
[4,     1] loss: 0.675
[5,     1] loss: 0.654
[6,     1] loss: 0.622
[7,     1] loss: 0.573
[8,     1] loss: 0.525
[9,     1] loss: 0.498
[10,     1] loss: 0.439
[11,     1] loss: 0.422
[12,     1] loss: 0.403
[13,     1] loss: 0.390
[14,     1] loss: 0.471
[15,     1] loss: 0.324
[16,     1] loss: 0.425
[17,     1] loss: 0.374
[18,     1] loss: 0.410
[19,     1] loss: 0.369
[20,     1] loss: 0.398
[21,     1] loss: 0.390
[22,     1] loss: 0.386
[23,     1] loss: 0.317
[24,     1] loss: 0.286
[25,     1] loss: 0.309
[26,     1] loss: 0.280
[27,     1] loss: 0.330
[28,     1] loss: 0.285
[29,     1] loss: 0.234
[30,     1] loss: 0.244
[31,     1] loss: 0.311
[32,     1] loss: 0.283
[33,     1] loss: 0.218
[34,     1] loss: 0.233
[35,     1] loss: 0.188
[36,     1] loss: 0.183
[37,     1] loss: 0.173
[38,     1] loss: 0.150
[39,     1] loss: 0.208
[40,     1] loss: 0.178
[41,     1] loss: 0.161
[42,     1] loss: 0.157
[43,     1] loss: 0.149
[44,     1] loss: 0.166
[45,     1] loss: 0.171
[46,     1] loss: 0.160
[47,     1] loss: 0.217
[48,     1] loss: 0.202
[49,     1] loss: 0.156
[50,     1] loss: 0.156
[51,     1] loss: 0.175
[52,     1] loss: 0.187
[53,     1] loss: 0.257
[54,     1] loss: 0.254
[55,     1] loss: 0.287
[56,     1] loss: 0.216
[57,     1] loss: 0.188
[58,     1] loss: 0.246
[59,     1] loss: 0.188
[60,     1] loss: 0.171
[61,     1] loss: 0.179
[62,     1] loss: 0.195
[63,     1] loss: 0.151
[64,     1] loss: 0.166
[65,     1] loss: 0.165
[66,     1] loss: 0.106
[67,     1] loss: 0.141
[68,     1] loss: 0.202
[69,     1] loss: 0.218
[70,     1] loss: 0.144
[71,     1] loss: 0.143
[72,     1] loss: 0.118
[73,     1] loss: 0.099
[74,     1] loss: 0.126
[75,     1] loss: 0.149
[76,     1] loss: 0.124
[77,     1] loss: 0.121
[78,     1] loss: 0.170
[79,     1] loss: 0.151
[80,     1] loss: 0.119
[81,     1] loss: 0.109
[82,     1] loss: 0.145
[83,     1] loss: 0.099
[84,     1] loss: 0.120
[85,     1] loss: 0.137
[86,     1] loss: 0.254
[87,     1] loss: 0.328
[88,     1] loss: 0.367
[89,     1] loss: 0.239
[90,     1] loss: 0.245
[91,     1] loss: 0.224
[92,     1] loss: 0.233
[93,     1] loss: 0.232
[94,     1] loss: 0.264
[95,     1] loss: 0.208
[96,     1] loss: 0.262
[97,     1] loss: 0.172
[98,     1] loss: 0.223
[99,     1] loss: 0.250
[100,     1] loss: 0.175
[101,     1] loss: 0.195
[102,     1] loss: 0.191
[103,     1] loss: 0.131
[104,     1] loss: 0.156
[105,     1] loss: 0.173
[106,     1] loss: 0.163
[107,     1] loss: 0.140
Early stopping applied (best metric=0.4722619950771332)
Finished Training
Total time taken: 11.954918146133423
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.687
[2,     1] loss: 0.719
[3,     1] loss: 0.696
[4,     1] loss: 0.687
[5,     1] loss: 0.682
[6,     1] loss: 0.672
[7,     1] loss: 0.650
[8,     1] loss: 0.630
[9,     1] loss: 0.605
[10,     1] loss: 0.591
[11,     1] loss: 0.555
[12,     1] loss: 0.515
[13,     1] loss: 0.470
[14,     1] loss: 0.437
[15,     1] loss: 0.421
[16,     1] loss: 0.369
[17,     1] loss: 0.335
[18,     1] loss: 0.309
[19,     1] loss: 0.367
[20,     1] loss: 0.360
[21,     1] loss: 0.343
[22,     1] loss: 0.297
[23,     1] loss: 0.249
[24,     1] loss: 0.259
[25,     1] loss: 0.207
[26,     1] loss: 0.242
[27,     1] loss: 0.264
[28,     1] loss: 0.167
[29,     1] loss: 0.227
[30,     1] loss: 0.182
[31,     1] loss: 0.197
[32,     1] loss: 0.137
[33,     1] loss: 0.187
[34,     1] loss: 0.161
[35,     1] loss: 0.198
[36,     1] loss: 0.257
[37,     1] loss: 0.227
[38,     1] loss: 0.176
[39,     1] loss: 0.210
[40,     1] loss: 0.176
[41,     1] loss: 0.143
[42,     1] loss: 0.154
[43,     1] loss: 0.163
[44,     1] loss: 0.183
[45,     1] loss: 0.181
[46,     1] loss: 0.138
[47,     1] loss: 0.126
[48,     1] loss: 0.153
[49,     1] loss: 0.090
[50,     1] loss: 0.128
[51,     1] loss: 0.110
[52,     1] loss: 0.097
[53,     1] loss: 0.122
[54,     1] loss: 0.121
[55,     1] loss: 0.144
[56,     1] loss: 0.103
[57,     1] loss: 0.093
[58,     1] loss: 0.155
[59,     1] loss: 0.135
[60,     1] loss: 0.080
[61,     1] loss: 0.082
[62,     1] loss: 0.082
[63,     1] loss: 0.112
[64,     1] loss: 0.081
[65,     1] loss: 0.092
[66,     1] loss: 0.124
[67,     1] loss: 0.108
[68,     1] loss: 0.227
[69,     1] loss: 0.307
[70,     1] loss: 0.219
[71,     1] loss: 0.146
[72,     1] loss: 0.288
[73,     1] loss: 0.117
[74,     1] loss: 0.180
[75,     1] loss: 0.191
[76,     1] loss: 0.186
[77,     1] loss: 0.158
[78,     1] loss: 0.157
[79,     1] loss: 0.148
[80,     1] loss: 0.155
[81,     1] loss: 0.153
[82,     1] loss: 0.185
[83,     1] loss: 0.119
[84,     1] loss: 0.117
[85,     1] loss: 0.105
[86,     1] loss: 0.106
[87,     1] loss: 0.075
Early stopping applied (best metric=0.13404957950115204)
Finished Training
Total time taken: 9.92934799194336
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.687
[3,     1] loss: 0.673
[4,     1] loss: 0.641
[5,     1] loss: 0.610
[6,     1] loss: 0.583
[7,     1] loss: 0.553
[8,     1] loss: 0.539
[9,     1] loss: 0.454
[10,     1] loss: 0.473
[11,     1] loss: 0.418
[12,     1] loss: 0.426
[13,     1] loss: 0.382
[14,     1] loss: 0.338
[15,     1] loss: 0.305
[16,     1] loss: 0.296
[17,     1] loss: 0.286
[18,     1] loss: 0.301
[19,     1] loss: 0.303
[20,     1] loss: 0.623
[21,     1] loss: 0.318
[22,     1] loss: 0.297
[23,     1] loss: 0.362
[24,     1] loss: 0.321
[25,     1] loss: 0.307
[26,     1] loss: 0.320
[27,     1] loss: 0.344
[28,     1] loss: 0.295
[29,     1] loss: 0.300
[30,     1] loss: 0.312
[31,     1] loss: 0.296
[32,     1] loss: 0.309
[33,     1] loss: 0.297
[34,     1] loss: 0.255
[35,     1] loss: 0.245
[36,     1] loss: 0.217
[37,     1] loss: 0.212
[38,     1] loss: 0.233
[39,     1] loss: 0.190
[40,     1] loss: 0.168
[41,     1] loss: 0.228
[42,     1] loss: 0.215
[43,     1] loss: 0.204
[44,     1] loss: 0.138
[45,     1] loss: 0.173
[46,     1] loss: 0.137
[47,     1] loss: 0.208
[48,     1] loss: 0.177
[49,     1] loss: 0.213
[50,     1] loss: 0.201
[51,     1] loss: 0.317
[52,     1] loss: 0.274
[53,     1] loss: 0.155
[54,     1] loss: 0.144
[55,     1] loss: 0.208
[56,     1] loss: 0.241
[57,     1] loss: 0.183
[58,     1] loss: 0.212
[59,     1] loss: 0.194
[60,     1] loss: 0.190
[61,     1] loss: 0.158
[62,     1] loss: 0.173
[63,     1] loss: 0.144
[64,     1] loss: 0.144
[65,     1] loss: 0.109
[66,     1] loss: 0.124
[67,     1] loss: 0.129
[68,     1] loss: 0.119
[69,     1] loss: 0.168
[70,     1] loss: 0.114
[71,     1] loss: 0.174
[72,     1] loss: 0.149
[73,     1] loss: 0.132
[74,     1] loss: 0.107
[75,     1] loss: 0.208
[76,     1] loss: 0.171
[77,     1] loss: 0.181
[78,     1] loss: 0.159
[79,     1] loss: 0.139
[80,     1] loss: 0.200
[81,     1] loss: 0.131
[82,     1] loss: 0.139
[83,     1] loss: 0.141
[84,     1] loss: 0.184
[85,     1] loss: 0.151
[86,     1] loss: 0.124
[87,     1] loss: 0.165
[88,     1] loss: 0.145
[89,     1] loss: 0.146
[90,     1] loss: 0.114
[91,     1] loss: 0.113
[92,     1] loss: 0.192
[93,     1] loss: 0.146
Early stopping applied (best metric=0.229420006275177)
Finished Training
Total time taken: 11.409038782119751
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.690
[3,     1] loss: 0.667
[4,     1] loss: 0.591
[5,     1] loss: 0.559
[6,     1] loss: 0.524
[7,     1] loss: 0.454
[8,     1] loss: 0.473
[9,     1] loss: 0.412
[10,     1] loss: 0.357
[11,     1] loss: 0.346
[12,     1] loss: 0.541
[13,     1] loss: 0.307
[14,     1] loss: 0.343
[15,     1] loss: 0.321
[16,     1] loss: 0.291
[17,     1] loss: 0.285
[18,     1] loss: 0.275
[19,     1] loss: 0.246
[20,     1] loss: 0.238
[21,     1] loss: 0.251
[22,     1] loss: 0.203
[23,     1] loss: 0.251
[24,     1] loss: 0.205
[25,     1] loss: 0.193
[26,     1] loss: 0.169
[27,     1] loss: 0.189
[28,     1] loss: 0.183
[29,     1] loss: 0.236
[30,     1] loss: 0.186
[31,     1] loss: 0.184
[32,     1] loss: 0.163
[33,     1] loss: 0.212
[34,     1] loss: 0.166
[35,     1] loss: 0.165
[36,     1] loss: 0.260
[37,     1] loss: 0.392
[38,     1] loss: 0.163
[39,     1] loss: 0.178
[40,     1] loss: 0.214
[41,     1] loss: 0.146
[42,     1] loss: 0.245
[43,     1] loss: 0.185
[44,     1] loss: 0.150
[45,     1] loss: 0.206
[46,     1] loss: 0.202
[47,     1] loss: 0.205
[48,     1] loss: 0.176
[49,     1] loss: 0.147
[50,     1] loss: 0.153
[51,     1] loss: 0.155
[52,     1] loss: 0.132
[53,     1] loss: 0.174
[54,     1] loss: 0.149
[55,     1] loss: 0.228
[56,     1] loss: 0.131
[57,     1] loss: 0.130
[58,     1] loss: 0.104
[59,     1] loss: 0.110
[60,     1] loss: 0.115
Early stopping applied (best metric=0.3385953903198242)
Finished Training
Total time taken: 6.880093097686768
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.684
[3,     1] loss: 0.674
[4,     1] loss: 0.646
[5,     1] loss: 0.584
[6,     1] loss: 0.555
[7,     1] loss: 0.495
[8,     1] loss: 0.456
[9,     1] loss: 0.407
[10,     1] loss: 0.438
[11,     1] loss: 0.389
[12,     1] loss: 0.416
[13,     1] loss: 0.397
[14,     1] loss: 0.347
[15,     1] loss: 0.427
[16,     1] loss: 0.380
[17,     1] loss: 0.446
[18,     1] loss: 0.390
[19,     1] loss: 0.375
[20,     1] loss: 0.345
[21,     1] loss: 0.375
[22,     1] loss: 0.318
[23,     1] loss: 0.329
[24,     1] loss: 0.380
[25,     1] loss: 0.354
[26,     1] loss: 0.287
[27,     1] loss: 0.371
[28,     1] loss: 0.285
[29,     1] loss: 0.310
[30,     1] loss: 0.356
[31,     1] loss: 0.331
[32,     1] loss: 0.280
[33,     1] loss: 0.291
[34,     1] loss: 0.251
[35,     1] loss: 0.269
[36,     1] loss: 0.282
[37,     1] loss: 0.281
[38,     1] loss: 0.182
[39,     1] loss: 0.216
[40,     1] loss: 0.199
[41,     1] loss: 0.159
[42,     1] loss: 0.173
[43,     1] loss: 0.128
[44,     1] loss: 0.445
[45,     1] loss: 0.343
[46,     1] loss: 0.361
[47,     1] loss: 0.259
[48,     1] loss: 0.311
[49,     1] loss: 0.229
[50,     1] loss: 0.240
[51,     1] loss: 0.277
[52,     1] loss: 0.227
[53,     1] loss: 0.233
[54,     1] loss: 0.207
[55,     1] loss: 0.177
[56,     1] loss: 0.213
[57,     1] loss: 0.165
[58,     1] loss: 0.204
[59,     1] loss: 0.216
[60,     1] loss: 0.176
[61,     1] loss: 0.172
[62,     1] loss: 0.210
[63,     1] loss: 0.141
[64,     1] loss: 0.205
[65,     1] loss: 0.191
[66,     1] loss: 0.175
[67,     1] loss: 0.144
[68,     1] loss: 0.172
[69,     1] loss: 0.128
[70,     1] loss: 0.148
[71,     1] loss: 0.172
[72,     1] loss: 0.151
[73,     1] loss: 0.172
[74,     1] loss: 0.161
[75,     1] loss: 0.161
[76,     1] loss: 0.205
[77,     1] loss: 0.164
[78,     1] loss: 0.138
[79,     1] loss: 0.147
[80,     1] loss: 0.198
[81,     1] loss: 0.115
[82,     1] loss: 0.172
[83,     1] loss: 0.137
[84,     1] loss: 0.217
Early stopping applied (best metric=0.17899297177791595)
Finished Training
Total time taken: 9.936611890792847
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.677
[4,     1] loss: 0.648
[5,     1] loss: 0.601
[6,     1] loss: 0.565
[7,     1] loss: 0.514
[8,     1] loss: 0.508
[9,     1] loss: 0.446
[10,     1] loss: 0.412
[11,     1] loss: 0.388
[12,     1] loss: 0.391
[13,     1] loss: 0.403
[14,     1] loss: 0.456
[15,     1] loss: 0.393
[16,     1] loss: 0.369
[17,     1] loss: 0.339
[18,     1] loss: 0.376
[19,     1] loss: 0.367
[20,     1] loss: 0.343
[21,     1] loss: 0.341
[22,     1] loss: 0.293
[23,     1] loss: 0.338
[24,     1] loss: 0.369
[25,     1] loss: 0.286
[26,     1] loss: 0.215
[27,     1] loss: 0.220
[28,     1] loss: 0.179
[29,     1] loss: 0.216
[30,     1] loss: 0.138
[31,     1] loss: 0.125
[32,     1] loss: 0.141
[33,     1] loss: 0.148
[34,     1] loss: 0.242
[35,     1] loss: 0.161
[36,     1] loss: 0.210
[37,     1] loss: 0.240
[38,     1] loss: 0.185
[39,     1] loss: 0.193
[40,     1] loss: 0.180
[41,     1] loss: 0.175
[42,     1] loss: 0.180
[43,     1] loss: 0.140
[44,     1] loss: 0.140
[45,     1] loss: 0.132
[46,     1] loss: 0.120
[47,     1] loss: 0.128
[48,     1] loss: 0.136
[49,     1] loss: 0.157
[50,     1] loss: 0.162
[51,     1] loss: 0.127
[52,     1] loss: 0.128
[53,     1] loss: 0.094
[54,     1] loss: 0.137
[55,     1] loss: 0.104
[56,     1] loss: 0.209
[57,     1] loss: 0.137
[58,     1] loss: 0.168
[59,     1] loss: 0.199
[60,     1] loss: 0.121
[61,     1] loss: 0.179
[62,     1] loss: 0.235
[63,     1] loss: 0.115
[64,     1] loss: 0.143
[65,     1] loss: 0.146
[66,     1] loss: 0.145
[67,     1] loss: 0.116
[68,     1] loss: 0.133
Early stopping applied (best metric=0.41238659620285034)
Finished Training
Total time taken: 7.806779384613037
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.694
[3,     1] loss: 0.689
[4,     1] loss: 0.686
[5,     1] loss: 0.676
[6,     1] loss: 0.662
[7,     1] loss: 0.630
[8,     1] loss: 0.590
[9,     1] loss: 0.557
[10,     1] loss: 0.508
[11,     1] loss: 0.474
[12,     1] loss: 0.455
[13,     1] loss: 0.385
[14,     1] loss: 0.368
[15,     1] loss: 0.347
[16,     1] loss: 0.317
[17,     1] loss: 0.335
[18,     1] loss: 0.325
[19,     1] loss: 0.218
[20,     1] loss: 0.182
[21,     1] loss: 0.314
[22,     1] loss: 0.317
[23,     1] loss: 0.339
[24,     1] loss: 0.272
[25,     1] loss: 0.285
[26,     1] loss: 0.310
[27,     1] loss: 0.297
[28,     1] loss: 0.273
[29,     1] loss: 0.279
[30,     1] loss: 0.278
[31,     1] loss: 0.258
[32,     1] loss: 0.278
[33,     1] loss: 0.260
[34,     1] loss: 0.242
[35,     1] loss: 0.291
[36,     1] loss: 0.234
[37,     1] loss: 0.220
[38,     1] loss: 0.196
[39,     1] loss: 0.183
[40,     1] loss: 0.229
[41,     1] loss: 0.216
[42,     1] loss: 0.177
[43,     1] loss: 0.152
[44,     1] loss: 0.136
[45,     1] loss: 0.208
[46,     1] loss: 0.152
[47,     1] loss: 0.152
[48,     1] loss: 0.107
[49,     1] loss: 0.176
[50,     1] loss: 0.144
[51,     1] loss: 0.137
[52,     1] loss: 0.125
[53,     1] loss: 0.096
[54,     1] loss: 0.100
[55,     1] loss: 0.159
[56,     1] loss: 0.427
[57,     1] loss: 0.236
[58,     1] loss: 0.201
[59,     1] loss: 0.122
[60,     1] loss: 0.172
[61,     1] loss: 0.164
[62,     1] loss: 0.154
[63,     1] loss: 0.182
[64,     1] loss: 0.142
[65,     1] loss: 0.129
[66,     1] loss: 0.119
[67,     1] loss: 0.137
[68,     1] loss: 0.173
[69,     1] loss: 0.127
[70,     1] loss: 0.114
[71,     1] loss: 0.150
[72,     1] loss: 0.094
[73,     1] loss: 0.165
[74,     1] loss: 0.114
[75,     1] loss: 0.162
[76,     1] loss: 0.155
[77,     1] loss: 0.221
[78,     1] loss: 0.155
[79,     1] loss: 0.114
[80,     1] loss: 0.187
[81,     1] loss: 0.107
[82,     1] loss: 0.190
[83,     1] loss: 0.214
[84,     1] loss: 0.223
[85,     1] loss: 0.141
[86,     1] loss: 0.183
[87,     1] loss: 0.137
[88,     1] loss: 0.144
[89,     1] loss: 0.135
[90,     1] loss: 0.126
[91,     1] loss: 0.113
[92,     1] loss: 0.128
[93,     1] loss: 0.148
[94,     1] loss: 0.101
[95,     1] loss: 0.111
[96,     1] loss: 0.094
[97,     1] loss: 0.080
[98,     1] loss: 0.096
[99,     1] loss: 0.188
[100,     1] loss: 0.446
[101,     1] loss: 0.384
[102,     1] loss: 0.259
[103,     1] loss: 0.275
[104,     1] loss: 0.274
[105,     1] loss: 0.338
[106,     1] loss: 0.334
[107,     1] loss: 0.316
[108,     1] loss: 0.275
[109,     1] loss: 0.300
[110,     1] loss: 0.314
[111,     1] loss: 0.341
[112,     1] loss: 0.265
[113,     1] loss: 0.266
[114,     1] loss: 0.246
[115,     1] loss: 0.246
[116,     1] loss: 0.225
[117,     1] loss: 0.224
[118,     1] loss: 0.177
[119,     1] loss: 0.170
[120,     1] loss: 0.193
[121,     1] loss: 0.148
[122,     1] loss: 0.162
[123,     1] loss: 0.164
[124,     1] loss: 0.127
[125,     1] loss: 0.140
[126,     1] loss: 0.141
[127,     1] loss: 0.136
[128,     1] loss: 0.108
[129,     1] loss: 0.129
[130,     1] loss: 0.140
[131,     1] loss: 0.194
[132,     1] loss: 0.229
[133,     1] loss: 0.147
[134,     1] loss: 0.173
[135,     1] loss: 0.185
[136,     1] loss: 0.206
[137,     1] loss: 0.194
Early stopping applied (best metric=0.173343688249588)
Finished Training
Total time taken: 15.435508012771606
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.679
[4,     1] loss: 0.648
[5,     1] loss: 0.605
[6,     1] loss: 0.576
[7,     1] loss: 0.516
[8,     1] loss: 0.531
[9,     1] loss: 0.470
[10,     1] loss: 0.398
[11,     1] loss: 0.431
[12,     1] loss: 0.359
[13,     1] loss: 0.326
[14,     1] loss: 0.397
[15,     1] loss: 0.321
[16,     1] loss: 0.349
[17,     1] loss: 0.300
[18,     1] loss: 0.256
[19,     1] loss: 0.280
[20,     1] loss: 0.225
[21,     1] loss: 0.265
[22,     1] loss: 0.174
[23,     1] loss: 0.203
[24,     1] loss: 0.230
[25,     1] loss: 0.209
[26,     1] loss: 0.181
[27,     1] loss: 0.172
[28,     1] loss: 0.172
[29,     1] loss: 0.144
[30,     1] loss: 0.120
[31,     1] loss: 0.130
[32,     1] loss: 0.194
[33,     1] loss: 0.266
[34,     1] loss: 0.131
[35,     1] loss: 0.221
[36,     1] loss: 0.269
[37,     1] loss: 0.227
[38,     1] loss: 0.345
[39,     1] loss: 0.252
[40,     1] loss: 0.222
[41,     1] loss: 0.218
[42,     1] loss: 0.211
[43,     1] loss: 0.201
[44,     1] loss: 0.190
[45,     1] loss: 0.194
[46,     1] loss: 0.159
[47,     1] loss: 0.175
[48,     1] loss: 0.165
[49,     1] loss: 0.143
[50,     1] loss: 0.121
[51,     1] loss: 0.121
[52,     1] loss: 0.135
[53,     1] loss: 0.104
[54,     1] loss: 0.080
[55,     1] loss: 0.087
[56,     1] loss: 0.109
Early stopping applied (best metric=0.4738696217536926)
Finished Training
Total time taken: 6.34691309928894
{'Hydroxylation-K Validation Accuracy': 0.8497517730496453, 'Hydroxylation-K Validation Sensitivity': 0.8684444444444445, 'Hydroxylation-K Validation Specificity': 0.8452631578947368, 'Hydroxylation-K Validation Precision': 0.5976174769502943, 'Hydroxylation-K AUC ROC': 0.8780116959064327, 'Hydroxylation-K AUC PR': 0.6781745402955929, 'Hydroxylation-K MCC': 0.6312231846098142, 'Hydroxylation-K F1': 0.7038872146297697, 'Validation Loss (Hydroxylation-K)': 0.3071721887588501, 'Validation Loss (total)': 0.3071721887588501, 'TimeToTrain': 9.742258939743042}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018757647317228395,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1053251094,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.228850592416704}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00397239470288311,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3724210765,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.025907820295739}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.675
[3,     1] loss: 0.668
[4,     1] loss: 0.639
[5,     1] loss: 0.576
[6,     1] loss: 0.565
[7,     1] loss: 0.559
[8,     1] loss: 0.512
[9,     1] loss: 0.450
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007554593025410856,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3157401321,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.971810758368427}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.679
[4,     1] loss: 0.647
[5,     1] loss: 0.615
[6,     1] loss: 0.584
[7,     1] loss: 0.526
[8,     1] loss: 0.520
[9,     1] loss: 0.484
[10,     1] loss: 0.453
[11,     1] loss: 0.461
[12,     1] loss: 0.420
[13,     1] loss: 0.409
[14,     1] loss: 0.503
[15,     1] loss: 0.510
[16,     1] loss: 0.564
[17,     1] loss: 0.519
[18,     1] loss: 0.502
[19,     1] loss: 0.511
[20,     1] loss: 0.475
[21,     1] loss: 0.493
[22,     1] loss: 0.424
[23,     1] loss: 0.504
[24,     1] loss: 0.455
[25,     1] loss: 0.421
[26,     1] loss: 0.427
[27,     1] loss: 0.439
[28,     1] loss: 0.395
[29,     1] loss: 0.410
[30,     1] loss: 0.366
[31,     1] loss: 0.361
[32,     1] loss: 0.328
[33,     1] loss: 0.365
[34,     1] loss: 0.311
[35,     1] loss: 0.405
[36,     1] loss: 0.388
[37,     1] loss: 0.295
[38,     1] loss: 0.346
[39,     1] loss: 0.328
[40,     1] loss: 0.369
[41,     1] loss: 0.307
[42,     1] loss: 0.294
[43,     1] loss: 0.324
[44,     1] loss: 0.328
[45,     1] loss: 0.367
[46,     1] loss: 0.312
[47,     1] loss: 0.446
[48,     1] loss: 0.333
[49,     1] loss: 0.350
[50,     1] loss: 0.328
[51,     1] loss: 0.369
[52,     1] loss: 0.300
[53,     1] loss: 0.322
[54,     1] loss: 0.301
[55,     1] loss: 0.272
[56,     1] loss: 0.285
[57,     1] loss: 0.289
[58,     1] loss: 0.305
[59,     1] loss: 0.301
[60,     1] loss: 0.284
[61,     1] loss: 0.280
[62,     1] loss: 0.272
[63,     1] loss: 0.256
[64,     1] loss: 0.255
[65,     1] loss: 0.278
[66,     1] loss: 0.328
[67,     1] loss: 0.261
[68,     1] loss: 0.315
[69,     1] loss: 0.261
[70,     1] loss: 0.246
[71,     1] loss: 0.277
[72,     1] loss: 0.237
[73,     1] loss: 0.281
[74,     1] loss: 0.277
[75,     1] loss: 0.273
[76,     1] loss: 0.258
[77,     1] loss: 0.315
[78,     1] loss: 0.297
[79,     1] loss: 0.239
[80,     1] loss: 0.264
[81,     1] loss: 0.279
[82,     1] loss: 0.263
[83,     1] loss: 0.246
[84,     1] loss: 0.248
[85,     1] loss: 0.237
[86,     1] loss: 0.266
[87,     1] loss: 0.269
[88,     1] loss: 0.344
[89,     1] loss: 0.655
[90,     1] loss: 0.502
[91,     1] loss: 0.360
[92,     1] loss: 0.385
[93,     1] loss: 0.428
[94,     1] loss: 0.412
[95,     1] loss: 0.384
[96,     1] loss: 0.413
[97,     1] loss: 0.433
[98,     1] loss: 0.371
[99,     1] loss: 0.364
[100,     1] loss: 0.339
[101,     1] loss: 0.321
[102,     1] loss: 0.330
[103,     1] loss: 0.360
[104,     1] loss: 0.325
[105,     1] loss: 0.335
[106,     1] loss: 0.296
[107,     1] loss: 0.314
[108,     1] loss: 0.282
[109,     1] loss: 0.314
[110,     1] loss: 0.237
[111,     1] loss: 0.254
[112,     1] loss: 0.342
[113,     1] loss: 0.798
[114,     1] loss: 0.369
[115,     1] loss: 0.446
[116,     1] loss: 0.399
[117,     1] loss: 0.405
[118,     1] loss: 0.439
[119,     1] loss: 0.416
[120,     1] loss: 0.417
[121,     1] loss: 0.393
[122,     1] loss: 0.383
[123,     1] loss: 0.388
[124,     1] loss: 0.357
[125,     1] loss: 0.342
[126,     1] loss: 0.356
[127,     1] loss: 0.318
[128,     1] loss: 0.310
[129,     1] loss: 0.311
[130,     1] loss: 0.318
[131,     1] loss: 0.357
Early stopping applied (best metric=0.33604753017425537)
Finished Training
Total time taken: 15.506349086761475
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.644
[4,     1] loss: 0.617
[5,     1] loss: 0.565
[6,     1] loss: 0.534
[7,     1] loss: 0.495
[8,     1] loss: 0.469
[9,     1] loss: 0.459
[10,     1] loss: 0.441
[11,     1] loss: 0.381
[12,     1] loss: 0.417
[13,     1] loss: 0.653
[14,     1] loss: 0.405
[15,     1] loss: 0.502
[16,     1] loss: 0.436
[17,     1] loss: 0.414
[18,     1] loss: 0.419
[19,     1] loss: 0.422
[20,     1] loss: 0.430
[21,     1] loss: 0.504
[22,     1] loss: 0.378
[23,     1] loss: 0.421
[24,     1] loss: 0.398
[25,     1] loss: 0.423
[26,     1] loss: 0.396
[27,     1] loss: 0.357
[28,     1] loss: 0.359
[29,     1] loss: 0.375
[30,     1] loss: 0.380
[31,     1] loss: 0.378
[32,     1] loss: 0.370
[33,     1] loss: 0.355
[34,     1] loss: 0.365
[35,     1] loss: 0.350
[36,     1] loss: 0.309
[37,     1] loss: 0.363
[38,     1] loss: 0.485
[39,     1] loss: 0.482
[40,     1] loss: 0.456
[41,     1] loss: 0.375
[42,     1] loss: 0.397
[43,     1] loss: 0.368
[44,     1] loss: 0.372
[45,     1] loss: 0.355
[46,     1] loss: 0.372
[47,     1] loss: 0.366
[48,     1] loss: 0.347
[49,     1] loss: 0.406
[50,     1] loss: 0.334
[51,     1] loss: 0.318
[52,     1] loss: 0.342
[53,     1] loss: 0.337
[54,     1] loss: 0.325
[55,     1] loss: 0.343
[56,     1] loss: 0.294
[57,     1] loss: 0.310
[58,     1] loss: 0.373
[59,     1] loss: 0.363
[60,     1] loss: 0.382
[61,     1] loss: 0.536
Early stopping applied (best metric=0.4046976566314697)
Finished Training
Total time taken: 7.145063877105713
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.694
[3,     1] loss: 0.681
[4,     1] loss: 0.681
[5,     1] loss: 0.662
[6,     1] loss: 0.644
[7,     1] loss: 0.616
[8,     1] loss: 0.584
[9,     1] loss: 0.535
[10,     1] loss: 0.549
[11,     1] loss: 0.510
[12,     1] loss: 0.524
[13,     1] loss: 0.444
[14,     1] loss: 0.512
[15,     1] loss: 0.400
[16,     1] loss: 0.432
[17,     1] loss: 0.423
[18,     1] loss: 0.411
[19,     1] loss: 0.380
[20,     1] loss: 0.476
[21,     1] loss: 0.403
[22,     1] loss: 0.357
[23,     1] loss: 0.355
[24,     1] loss: 0.398
[25,     1] loss: 0.330
[26,     1] loss: 0.356
[27,     1] loss: 0.384
[28,     1] loss: 0.308
[29,     1] loss: 0.312
[30,     1] loss: 0.283
[31,     1] loss: 0.506
[32,     1] loss: 0.379
[33,     1] loss: 0.393
[34,     1] loss: 0.377
[35,     1] loss: 0.363
[36,     1] loss: 0.377
[37,     1] loss: 0.341
[38,     1] loss: 0.337
[39,     1] loss: 0.387
[40,     1] loss: 0.459
[41,     1] loss: 0.348
[42,     1] loss: 0.380
[43,     1] loss: 0.355
[44,     1] loss: 0.406
[45,     1] loss: 0.346
[46,     1] loss: 0.418
[47,     1] loss: 0.363
[48,     1] loss: 0.334
[49,     1] loss: 0.382
[50,     1] loss: 0.332
[51,     1] loss: 0.324
[52,     1] loss: 0.319
[53,     1] loss: 0.566
[54,     1] loss: 0.518
[55,     1] loss: 0.373
[56,     1] loss: 0.391
[57,     1] loss: 0.392
[58,     1] loss: 0.359
[59,     1] loss: 0.381
[60,     1] loss: 0.413
[61,     1] loss: 0.360
[62,     1] loss: 0.376
[63,     1] loss: 0.461
[64,     1] loss: 0.362
[65,     1] loss: 0.454
[66,     1] loss: 0.350
[67,     1] loss: 0.378
[68,     1] loss: 0.420
[69,     1] loss: 0.364
[70,     1] loss: 0.395
[71,     1] loss: 0.353
[72,     1] loss: 0.333
[73,     1] loss: 0.333
[74,     1] loss: 0.335
[75,     1] loss: 0.303
[76,     1] loss: 0.321
[77,     1] loss: 0.334
[78,     1] loss: 0.298
[79,     1] loss: 0.332
[80,     1] loss: 0.443
[81,     1] loss: 0.535
[82,     1] loss: 0.600
[83,     1] loss: 0.496
[84,     1] loss: 0.473
[85,     1] loss: 0.486
[86,     1] loss: 0.473
[87,     1] loss: 0.460
[88,     1] loss: 0.431
[89,     1] loss: 0.456
[90,     1] loss: 0.398
[91,     1] loss: 0.405
[92,     1] loss: 0.389
[93,     1] loss: 0.347
[94,     1] loss: 0.379
[95,     1] loss: 0.352
[96,     1] loss: 0.357
[97,     1] loss: 0.361
[98,     1] loss: 0.330
[99,     1] loss: 0.360
[100,     1] loss: 0.327
[101,     1] loss: 0.316
[102,     1] loss: 0.349
[103,     1] loss: 0.317
[104,     1] loss: 0.281
Early stopping applied (best metric=0.30585646629333496)
Finished Training
Total time taken: 12.667686700820923
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.671
[4,     1] loss: 0.647
[5,     1] loss: 0.612
[6,     1] loss: 0.562
[7,     1] loss: 0.533
[8,     1] loss: 0.525
[9,     1] loss: 0.496
[10,     1] loss: 0.461
[11,     1] loss: 0.456
[12,     1] loss: 0.395
[13,     1] loss: 0.400
[14,     1] loss: 0.406
[15,     1] loss: 0.402
[16,     1] loss: 0.397
[17,     1] loss: 0.385
[18,     1] loss: 0.360
[19,     1] loss: 0.420
[20,     1] loss: 0.363
[21,     1] loss: 0.321
[22,     1] loss: 0.298
[23,     1] loss: 0.368
[24,     1] loss: 0.342
[25,     1] loss: 0.364
[26,     1] loss: 0.322
[27,     1] loss: 0.384
[28,     1] loss: 0.332
[29,     1] loss: 0.343
[30,     1] loss: 0.384
[31,     1] loss: 0.366
[32,     1] loss: 0.403
[33,     1] loss: 0.353
[34,     1] loss: 0.361
[35,     1] loss: 0.359
[36,     1] loss: 0.374
[37,     1] loss: 0.357
[38,     1] loss: 0.341
[39,     1] loss: 0.319
[40,     1] loss: 0.330
[41,     1] loss: 0.337
[42,     1] loss: 0.377
[43,     1] loss: 0.588
[44,     1] loss: 0.432
[45,     1] loss: 0.393
[46,     1] loss: 0.370
[47,     1] loss: 0.376
[48,     1] loss: 0.365
[49,     1] loss: 0.377
[50,     1] loss: 0.347
[51,     1] loss: 0.326
[52,     1] loss: 0.351
[53,     1] loss: 0.317
[54,     1] loss: 0.327
[55,     1] loss: 0.308
[56,     1] loss: 0.332
[57,     1] loss: 0.294
[58,     1] loss: 0.285
[59,     1] loss: 0.314
[60,     1] loss: 0.317
[61,     1] loss: 0.336
[62,     1] loss: 0.289
[63,     1] loss: 0.306
[64,     1] loss: 0.315
[65,     1] loss: 0.544
[66,     1] loss: 0.516
[67,     1] loss: 0.403
[68,     1] loss: 0.409
[69,     1] loss: 0.441
[70,     1] loss: 0.404
[71,     1] loss: 0.370
[72,     1] loss: 0.364
[73,     1] loss: 0.355
[74,     1] loss: 0.343
[75,     1] loss: 0.353
[76,     1] loss: 0.322
[77,     1] loss: 0.336
[78,     1] loss: 0.333
[79,     1] loss: 0.376
[80,     1] loss: 0.474
[81,     1] loss: 0.364
[82,     1] loss: 0.396
[83,     1] loss: 0.353
[84,     1] loss: 0.349
[85,     1] loss: 0.339
[86,     1] loss: 0.339
[87,     1] loss: 0.338
[88,     1] loss: 0.314
[89,     1] loss: 0.336
[90,     1] loss: 0.301
[91,     1] loss: 0.319
[92,     1] loss: 0.276
[93,     1] loss: 0.310
[94,     1] loss: 0.298
[95,     1] loss: 0.576
[96,     1] loss: 0.572
[97,     1] loss: 0.398
[98,     1] loss: 0.383
[99,     1] loss: 0.450
[100,     1] loss: 0.408
[101,     1] loss: 0.358
[102,     1] loss: 0.390
[103,     1] loss: 0.370
[104,     1] loss: 0.363
[105,     1] loss: 0.361
[106,     1] loss: 0.340
[107,     1] loss: 0.332
[108,     1] loss: 0.331
[109,     1] loss: 0.316
[110,     1] loss: 0.332
[111,     1] loss: 0.334
[112,     1] loss: 0.307
[113,     1] loss: 0.303
[114,     1] loss: 0.332
[115,     1] loss: 0.310
[116,     1] loss: 0.307
[117,     1] loss: 0.322
[118,     1] loss: 0.410
[119,     1] loss: 0.373
[120,     1] loss: 0.354
[121,     1] loss: 0.328
[122,     1] loss: 0.324
[123,     1] loss: 0.327
[124,     1] loss: 0.297
[125,     1] loss: 0.347
[126,     1] loss: 0.329
[127,     1] loss: 0.306
[128,     1] loss: 0.323
[129,     1] loss: 0.344
[130,     1] loss: 0.317
[131,     1] loss: 0.318
[132,     1] loss: 0.339
[133,     1] loss: 0.344
[134,     1] loss: 0.334
[135,     1] loss: 0.320
[136,     1] loss: 0.298
[137,     1] loss: 0.314
[138,     1] loss: 0.365
[139,     1] loss: 0.319
[140,     1] loss: 0.307
[141,     1] loss: 0.346
[142,     1] loss: 0.320
[143,     1] loss: 0.293
[144,     1] loss: 0.286
[145,     1] loss: 0.313
[146,     1] loss: 0.307
[147,     1] loss: 0.390
[148,     1] loss: 0.730
[149,     1] loss: 0.533
[150,     1] loss: 0.455
[151,     1] loss: 0.428
[152,     1] loss: 0.442
[153,     1] loss: 0.448
[154,     1] loss: 0.390
[155,     1] loss: 0.411
[156,     1] loss: 0.369
[157,     1] loss: 0.359
Early stopping applied (best metric=0.40396207571029663)
Finished Training
Total time taken: 19.457170248031616
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.696
[3,     1] loss: 0.688
[4,     1] loss: 0.671
[5,     1] loss: 0.641
[6,     1] loss: 0.596
[7,     1] loss: 0.577
[8,     1] loss: 0.521
[9,     1] loss: 0.497
[10,     1] loss: 0.446
[11,     1] loss: 0.476
[12,     1] loss: 0.591
[13,     1] loss: 0.446
[14,     1] loss: 0.515
[15,     1] loss: 0.458
[16,     1] loss: 0.434
[17,     1] loss: 0.441
[18,     1] loss: 0.403
[19,     1] loss: 0.413
[20,     1] loss: 0.393
[21,     1] loss: 0.390
[22,     1] loss: 0.401
[23,     1] loss: 0.347
[24,     1] loss: 0.371
[25,     1] loss: 0.367
[26,     1] loss: 0.381
[27,     1] loss: 0.427
[28,     1] loss: 0.363
[29,     1] loss: 0.405
[30,     1] loss: 0.351
[31,     1] loss: 0.382
[32,     1] loss: 0.317
[33,     1] loss: 0.362
[34,     1] loss: 0.326
[35,     1] loss: 0.357
[36,     1] loss: 0.351
[37,     1] loss: 0.411
[38,     1] loss: 0.344
[39,     1] loss: 0.342
[40,     1] loss: 0.304
[41,     1] loss: 0.327
[42,     1] loss: 0.381
[43,     1] loss: 0.360
[44,     1] loss: 0.338
[45,     1] loss: 0.297
[46,     1] loss: 0.333
[47,     1] loss: 0.294
[48,     1] loss: 0.321
[49,     1] loss: 0.288
[50,     1] loss: 0.253
[51,     1] loss: 0.305
[52,     1] loss: 0.324
[53,     1] loss: 0.649
[54,     1] loss: 0.642
[55,     1] loss: 0.448
[56,     1] loss: 0.468
[57,     1] loss: 0.434
[58,     1] loss: 0.422
[59,     1] loss: 0.424
[60,     1] loss: 0.413
[61,     1] loss: 0.372
[62,     1] loss: 0.404
[63,     1] loss: 0.376
[64,     1] loss: 0.395
[65,     1] loss: 0.384
[66,     1] loss: 0.364
[67,     1] loss: 0.369
[68,     1] loss: 0.362
[69,     1] loss: 0.338
[70,     1] loss: 0.352
[71,     1] loss: 0.327
[72,     1] loss: 0.336
[73,     1] loss: 0.323
[74,     1] loss: 0.338
[75,     1] loss: 0.317
[76,     1] loss: 0.378
[77,     1] loss: 0.519
Early stopping applied (best metric=0.30777642130851746)
Finished Training
Total time taken: 8.950498580932617
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.686
[4,     1] loss: 0.677
[5,     1] loss: 0.645
[6,     1] loss: 0.620
[7,     1] loss: 0.584
[8,     1] loss: 0.564
[9,     1] loss: 0.531
[10,     1] loss: 0.503
[11,     1] loss: 0.471
[12,     1] loss: 0.481
[13,     1] loss: 0.423
[14,     1] loss: 0.452
[15,     1] loss: 0.439
[16,     1] loss: 0.457
[17,     1] loss: 0.387
[18,     1] loss: 0.640
[19,     1] loss: 0.407
[20,     1] loss: 0.438
[21,     1] loss: 0.411
[22,     1] loss: 0.404
[23,     1] loss: 0.454
[24,     1] loss: 0.418
[25,     1] loss: 0.388
[26,     1] loss: 0.389
[27,     1] loss: 0.406
[28,     1] loss: 0.412
[29,     1] loss: 0.394
[30,     1] loss: 0.379
[31,     1] loss: 0.364
[32,     1] loss: 0.366
[33,     1] loss: 0.396
[34,     1] loss: 0.395
[35,     1] loss: 0.391
[36,     1] loss: 0.361
[37,     1] loss: 0.363
[38,     1] loss: 0.357
[39,     1] loss: 0.380
[40,     1] loss: 0.357
[41,     1] loss: 0.374
[42,     1] loss: 0.458
[43,     1] loss: 0.345
[44,     1] loss: 0.376
[45,     1] loss: 0.351
[46,     1] loss: 0.426
[47,     1] loss: 0.491
[48,     1] loss: 0.352
[49,     1] loss: 0.361
[50,     1] loss: 0.380
[51,     1] loss: 0.373
[52,     1] loss: 0.392
[53,     1] loss: 0.367
[54,     1] loss: 0.345
[55,     1] loss: 0.346
[56,     1] loss: 0.346
[57,     1] loss: 0.356
[58,     1] loss: 0.397
[59,     1] loss: 0.345
[60,     1] loss: 0.355
[61,     1] loss: 0.388
[62,     1] loss: 0.339
[63,     1] loss: 0.370
[64,     1] loss: 0.334
[65,     1] loss: 0.347
[66,     1] loss: 0.349
[67,     1] loss: 0.357
[68,     1] loss: 0.328
[69,     1] loss: 0.321
[70,     1] loss: 0.318
[71,     1] loss: 0.347
[72,     1] loss: 0.318
[73,     1] loss: 0.322
[74,     1] loss: 0.345
[75,     1] loss: 0.408
[76,     1] loss: 0.427
[77,     1] loss: 0.363
[78,     1] loss: 0.374
[79,     1] loss: 0.428
[80,     1] loss: 0.360
[81,     1] loss: 0.385
[82,     1] loss: 0.368
[83,     1] loss: 0.327
[84,     1] loss: 0.314
[85,     1] loss: 0.328
[86,     1] loss: 0.346
[87,     1] loss: 0.436
[88,     1] loss: 0.591
[89,     1] loss: 0.389
[90,     1] loss: 0.401
[91,     1] loss: 0.391
[92,     1] loss: 0.383
[93,     1] loss: 0.422
[94,     1] loss: 0.363
[95,     1] loss: 0.357
[96,     1] loss: 0.337
[97,     1] loss: 0.394
[98,     1] loss: 0.345
[99,     1] loss: 0.343
[100,     1] loss: 0.356
[101,     1] loss: 0.354
[102,     1] loss: 0.363
[103,     1] loss: 0.314
[104,     1] loss: 0.333
[105,     1] loss: 0.316
[106,     1] loss: 0.362
[107,     1] loss: 0.299
[108,     1] loss: 0.296
[109,     1] loss: 0.333
[110,     1] loss: 0.364
[111,     1] loss: 0.456
[112,     1] loss: 0.472
[113,     1] loss: 0.457
[114,     1] loss: 0.391
[115,     1] loss: 0.404
[116,     1] loss: 0.413
[117,     1] loss: 0.406
[118,     1] loss: 0.377
[119,     1] loss: 0.343
[120,     1] loss: 0.359
[121,     1] loss: 0.355
Early stopping applied (best metric=0.333110511302948)
Finished Training
Total time taken: 14.011594772338867
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.698
[3,     1] loss: 0.690
[4,     1] loss: 0.678
[5,     1] loss: 0.659
[6,     1] loss: 0.648
[7,     1] loss: 0.622
[8,     1] loss: 0.592
[9,     1] loss: 0.558
[10,     1] loss: 0.548
[11,     1] loss: 0.527
[12,     1] loss: 0.529
[13,     1] loss: 0.500
[14,     1] loss: 0.507
[15,     1] loss: 0.431
[16,     1] loss: 0.473
[17,     1] loss: 0.586
[18,     1] loss: 0.551
[19,     1] loss: 0.509
[20,     1] loss: 0.456
[21,     1] loss: 0.493
[22,     1] loss: 0.453
[23,     1] loss: 0.449
[24,     1] loss: 0.455
[25,     1] loss: 0.451
[26,     1] loss: 0.408
[27,     1] loss: 0.404
[28,     1] loss: 0.380
[29,     1] loss: 0.392
[30,     1] loss: 0.385
[31,     1] loss: 0.400
[32,     1] loss: 0.370
[33,     1] loss: 0.344
[34,     1] loss: 0.335
[35,     1] loss: 0.336
[36,     1] loss: 0.362
[37,     1] loss: 0.347
[38,     1] loss: 0.322
[39,     1] loss: 0.346
[40,     1] loss: 0.372
[41,     1] loss: 0.397
[42,     1] loss: 0.351
[43,     1] loss: 0.387
[44,     1] loss: 0.357
[45,     1] loss: 0.353
[46,     1] loss: 0.337
[47,     1] loss: 0.328
[48,     1] loss: 0.341
[49,     1] loss: 0.326
[50,     1] loss: 0.351
[51,     1] loss: 0.356
[52,     1] loss: 0.328
[53,     1] loss: 0.315
[54,     1] loss: 0.337
[55,     1] loss: 0.313
[56,     1] loss: 0.312
[57,     1] loss: 0.370
[58,     1] loss: 0.689
[59,     1] loss: 0.365
[60,     1] loss: 0.399
[61,     1] loss: 0.430
[62,     1] loss: 0.402
[63,     1] loss: 0.393
[64,     1] loss: 0.393
[65,     1] loss: 0.369
[66,     1] loss: 0.364
[67,     1] loss: 0.366
[68,     1] loss: 0.359
[69,     1] loss: 0.367
[70,     1] loss: 0.366
[71,     1] loss: 0.352
[72,     1] loss: 0.318
[73,     1] loss: 0.355
[74,     1] loss: 0.419
[75,     1] loss: 0.357
[76,     1] loss: 0.377
[77,     1] loss: 0.352
[78,     1] loss: 0.318
[79,     1] loss: 0.326
[80,     1] loss: 0.333
[81,     1] loss: 0.356
[82,     1] loss: 0.334
[83,     1] loss: 0.400
[84,     1] loss: 0.566
[85,     1] loss: 0.436
[86,     1] loss: 0.368
[87,     1] loss: 0.377
[88,     1] loss: 0.375
[89,     1] loss: 0.393
[90,     1] loss: 0.384
[91,     1] loss: 0.386
[92,     1] loss: 0.390
[93,     1] loss: 0.364
[94,     1] loss: 0.369
[95,     1] loss: 0.335
[96,     1] loss: 0.329
[97,     1] loss: 0.354
[98,     1] loss: 0.377
[99,     1] loss: 0.350
[100,     1] loss: 0.324
[101,     1] loss: 0.341
[102,     1] loss: 0.337
[103,     1] loss: 0.348
[104,     1] loss: 0.304
[105,     1] loss: 0.295
[106,     1] loss: 0.314
[107,     1] loss: 0.322
[108,     1] loss: 0.348
[109,     1] loss: 0.675
[110,     1] loss: 0.390
[111,     1] loss: 0.383
[112,     1] loss: 0.387
[113,     1] loss: 0.364
[114,     1] loss: 0.373
[115,     1] loss: 0.399
[116,     1] loss: 0.378
[117,     1] loss: 0.364
[118,     1] loss: 0.349
[119,     1] loss: 0.350
[120,     1] loss: 0.347
[121,     1] loss: 0.351
[122,     1] loss: 0.368
Early stopping applied (best metric=0.4521515965461731)
Finished Training
Total time taken: 15.704577445983887
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.662
[4,     1] loss: 0.626
[5,     1] loss: 0.587
[6,     1] loss: 0.542
[7,     1] loss: 0.497
[8,     1] loss: 0.482
[9,     1] loss: 0.450
[10,     1] loss: 0.435
[11,     1] loss: 0.397
[12,     1] loss: 0.438
[13,     1] loss: 0.374
[14,     1] loss: 0.425
[15,     1] loss: 0.466
[16,     1] loss: 0.420
[17,     1] loss: 0.406
[18,     1] loss: 0.378
[19,     1] loss: 0.392
[20,     1] loss: 0.330
[21,     1] loss: 0.376
[22,     1] loss: 0.345
[23,     1] loss: 0.318
[24,     1] loss: 0.351
[25,     1] loss: 0.315
[26,     1] loss: 0.350
[27,     1] loss: 0.450
[28,     1] loss: 0.375
[29,     1] loss: 0.398
[30,     1] loss: 0.349
[31,     1] loss: 0.406
[32,     1] loss: 0.358
[33,     1] loss: 0.378
[34,     1] loss: 0.351
[35,     1] loss: 0.363
[36,     1] loss: 0.312
[37,     1] loss: 0.315
[38,     1] loss: 0.328
[39,     1] loss: 0.318
[40,     1] loss: 0.301
[41,     1] loss: 0.339
[42,     1] loss: 0.303
[43,     1] loss: 0.359
[44,     1] loss: 0.627
[45,     1] loss: 0.426
[46,     1] loss: 0.391
[47,     1] loss: 0.394
[48,     1] loss: 0.356
[49,     1] loss: 0.347
[50,     1] loss: 0.352
[51,     1] loss: 0.322
[52,     1] loss: 0.314
[53,     1] loss: 0.340
[54,     1] loss: 0.340
[55,     1] loss: 0.333
[56,     1] loss: 0.355
[57,     1] loss: 0.339
[58,     1] loss: 0.288
[59,     1] loss: 0.308
[60,     1] loss: 0.342
[61,     1] loss: 0.333
[62,     1] loss: 0.369
[63,     1] loss: 0.321
[64,     1] loss: 0.324
[65,     1] loss: 0.343
[66,     1] loss: 0.314
[67,     1] loss: 0.322
[68,     1] loss: 0.309
[69,     1] loss: 0.519
[70,     1] loss: 0.837
[71,     1] loss: 0.792
[72,     1] loss: 0.752
[73,     1] loss: 0.699
[74,     1] loss: 0.695
[75,     1] loss: 0.688
[76,     1] loss: 0.692
[77,     1] loss: 0.691
[78,     1] loss: 0.691
[79,     1] loss: 0.691
[80,     1] loss: 0.692
[81,     1] loss: 0.692
[82,     1] loss: 0.693
[83,     1] loss: 0.693
[84,     1] loss: 0.693
[85,     1] loss: 0.693
[86,     1] loss: 0.693
[87,     1] loss: 0.693
[88,     1] loss: 0.693
[89,     1] loss: 0.693
[90,     1] loss: 0.693
[91,     1] loss: 0.693
[92,     1] loss: 0.692
[93,     1] loss: 0.693
[94,     1] loss: 0.692
[95,     1] loss: 0.692
[96,     1] loss: 0.692
[97,     1] loss: 0.692
[98,     1] loss: 0.692
[99,     1] loss: 0.691
[100,     1] loss: 0.689
[101,     1] loss: 0.684
[102,     1] loss: 0.681
[103,     1] loss: 0.676
[104,     1] loss: 0.669
[105,     1] loss: 0.648
[106,     1] loss: 0.644
[107,     1] loss: 0.625
[108,     1] loss: 0.615
[109,     1] loss: 0.614
Early stopping applied (best metric=0.31726816296577454)
Finished Training
Total time taken: 12.830693006515503
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.669
[4,     1] loss: 0.648
[5,     1] loss: 0.620
[6,     1] loss: 0.565
[7,     1] loss: 0.544
[8,     1] loss: 0.519
[9,     1] loss: 0.494
[10,     1] loss: 0.441
[11,     1] loss: 0.417
[12,     1] loss: 0.462
[13,     1] loss: 0.411
[14,     1] loss: 0.415
[15,     1] loss: 0.374
[16,     1] loss: 0.413
[17,     1] loss: 0.370
[18,     1] loss: 0.361
[19,     1] loss: 0.429
[20,     1] loss: 0.384
[21,     1] loss: 0.539
[22,     1] loss: 0.467
[23,     1] loss: 0.403
[24,     1] loss: 0.383
[25,     1] loss: 0.476
[26,     1] loss: 0.464
[27,     1] loss: 0.406
[28,     1] loss: 0.467
[29,     1] loss: 0.395
[30,     1] loss: 0.386
[31,     1] loss: 0.365
[32,     1] loss: 0.414
[33,     1] loss: 0.370
[34,     1] loss: 0.387
[35,     1] loss: 0.363
[36,     1] loss: 0.365
[37,     1] loss: 0.375
[38,     1] loss: 0.472
[39,     1] loss: 0.341
[40,     1] loss: 0.344
[41,     1] loss: 0.368
[42,     1] loss: 0.340
[43,     1] loss: 0.428
[44,     1] loss: 0.566
[45,     1] loss: 0.396
[46,     1] loss: 0.448
[47,     1] loss: 0.430
[48,     1] loss: 0.391
[49,     1] loss: 0.371
[50,     1] loss: 0.392
[51,     1] loss: 0.383
[52,     1] loss: 0.378
[53,     1] loss: 0.386
[54,     1] loss: 0.381
[55,     1] loss: 0.337
[56,     1] loss: 0.328
[57,     1] loss: 0.309
[58,     1] loss: 0.322
[59,     1] loss: 0.306
[60,     1] loss: 0.369
[61,     1] loss: 0.384
[62,     1] loss: 0.495
[63,     1] loss: 0.469
[64,     1] loss: 0.441
[65,     1] loss: 0.452
[66,     1] loss: 0.392
[67,     1] loss: 0.439
[68,     1] loss: 0.396
[69,     1] loss: 0.348
[70,     1] loss: 0.340
[71,     1] loss: 0.375
[72,     1] loss: 0.335
[73,     1] loss: 0.359
[74,     1] loss: 0.332
[75,     1] loss: 0.275
[76,     1] loss: 0.314
[77,     1] loss: 0.272
[78,     1] loss: 0.303
[79,     1] loss: 0.315
[80,     1] loss: 0.691
[81,     1] loss: 0.585
[82,     1] loss: 0.447
[83,     1] loss: 0.409
[84,     1] loss: 0.466
[85,     1] loss: 0.422
[86,     1] loss: 0.431
[87,     1] loss: 0.435
[88,     1] loss: 0.456
[89,     1] loss: 0.417
[90,     1] loss: 0.437
[91,     1] loss: 0.412
[92,     1] loss: 0.398
[93,     1] loss: 0.396
[94,     1] loss: 0.352
[95,     1] loss: 0.358
[96,     1] loss: 0.357
[97,     1] loss: 0.357
[98,     1] loss: 0.657
[99,     1] loss: 0.335
[100,     1] loss: 0.337
[101,     1] loss: 0.306
[102,     1] loss: 0.332
[103,     1] loss: 0.336
[104,     1] loss: 0.345
[105,     1] loss: 0.285
[106,     1] loss: 0.287
[107,     1] loss: 0.269
[108,     1] loss: 0.286
[109,     1] loss: 0.254
[110,     1] loss: 0.298
[111,     1] loss: 0.318
[112,     1] loss: 0.243
[113,     1] loss: 0.445
[114,     1] loss: 0.610
[115,     1] loss: 0.376
[116,     1] loss: 0.535
[117,     1] loss: 0.397
[118,     1] loss: 0.432
[119,     1] loss: 0.383
[120,     1] loss: 0.405
[121,     1] loss: 0.416
[122,     1] loss: 0.391
[123,     1] loss: 0.383
[124,     1] loss: 0.376
[125,     1] loss: 0.364
[126,     1] loss: 0.378
[127,     1] loss: 0.333
[128,     1] loss: 0.377
[129,     1] loss: 0.335
[130,     1] loss: 0.294
[131,     1] loss: 0.325
[132,     1] loss: 0.322
[133,     1] loss: 0.351
[134,     1] loss: 0.635
[135,     1] loss: 0.339
[136,     1] loss: 0.397
[137,     1] loss: 0.334
[138,     1] loss: 0.357
[139,     1] loss: 0.362
[140,     1] loss: 0.401
[141,     1] loss: 0.353
[142,     1] loss: 0.418
[143,     1] loss: 0.346
[144,     1] loss: 0.316
[145,     1] loss: 0.320
[146,     1] loss: 0.334
[147,     1] loss: 0.281
[148,     1] loss: 0.346
[149,     1] loss: 0.345
[150,     1] loss: 0.281
[151,     1] loss: 0.343
[152,     1] loss: 0.321
[153,     1] loss: 0.341
[154,     1] loss: 0.345
[155,     1] loss: 0.332
[156,     1] loss: 0.284
[157,     1] loss: 0.339
[158,     1] loss: 0.324
[159,     1] loss: 0.330
[160,     1] loss: 0.290
[161,     1] loss: 0.314
[162,     1] loss: 0.315
[163,     1] loss: 0.311
[164,     1] loss: 0.298
[165,     1] loss: 0.342
[166,     1] loss: 0.356
[167,     1] loss: 0.287
[168,     1] loss: 0.366
[169,     1] loss: 0.326
[170,     1] loss: 0.340
[171,     1] loss: 0.357
[172,     1] loss: 0.354
[173,     1] loss: 0.301
[174,     1] loss: 0.295
[175,     1] loss: 0.331
[176,     1] loss: 0.294
[177,     1] loss: 0.270
[178,     1] loss: 0.301
[179,     1] loss: 0.264
[180,     1] loss: 0.547
[181,     1] loss: 0.522
[182,     1] loss: 0.405
Early stopping applied (best metric=0.21887771785259247)
Finished Training
Total time taken: 21.712916612625122
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.688
[3,     1] loss: 0.685
[4,     1] loss: 0.653
[5,     1] loss: 0.634
[6,     1] loss: 0.610
[7,     1] loss: 0.562
[8,     1] loss: 0.569
[9,     1] loss: 0.532
[10,     1] loss: 0.513
[11,     1] loss: 0.481
[12,     1] loss: 0.475
[13,     1] loss: 0.430
[14,     1] loss: 0.437
[15,     1] loss: 0.457
[16,     1] loss: 0.447
[17,     1] loss: 0.402
[18,     1] loss: 0.507
[19,     1] loss: 0.455
[20,     1] loss: 0.368
[21,     1] loss: 0.452
[22,     1] loss: 0.396
[23,     1] loss: 0.395
[24,     1] loss: 0.411
[25,     1] loss: 0.403
[26,     1] loss: 0.394
[27,     1] loss: 0.447
[28,     1] loss: 0.406
[29,     1] loss: 0.394
[30,     1] loss: 0.341
[31,     1] loss: 0.390
[32,     1] loss: 0.407
[33,     1] loss: 0.360
[34,     1] loss: 0.434
[35,     1] loss: 0.511
[36,     1] loss: 0.400
[37,     1] loss: 0.473
[38,     1] loss: 0.379
[39,     1] loss: 0.407
[40,     1] loss: 0.400
[41,     1] loss: 0.384
[42,     1] loss: 0.361
[43,     1] loss: 0.363
[44,     1] loss: 0.309
[45,     1] loss: 0.360
[46,     1] loss: 0.421
[47,     1] loss: 0.338
[48,     1] loss: 0.388
[49,     1] loss: 0.321
[50,     1] loss: 0.363
[51,     1] loss: 0.329
[52,     1] loss: 0.380
[53,     1] loss: 0.376
[54,     1] loss: 0.364
[55,     1] loss: 0.332
[56,     1] loss: 0.309
[57,     1] loss: 0.330
[58,     1] loss: 0.375
[59,     1] loss: 0.340
[60,     1] loss: 0.318
[61,     1] loss: 0.297
[62,     1] loss: 0.330
[63,     1] loss: 0.308
[64,     1] loss: 0.352
[65,     1] loss: 0.317
[66,     1] loss: 0.446
[67,     1] loss: 0.791
[68,     1] loss: 0.458
[69,     1] loss: 0.436
[70,     1] loss: 0.412
[71,     1] loss: 0.501
[72,     1] loss: 0.425
[73,     1] loss: 0.469
[74,     1] loss: 0.414
[75,     1] loss: 0.416
[76,     1] loss: 0.419
[77,     1] loss: 0.369
[78,     1] loss: 0.341
[79,     1] loss: 0.320
[80,     1] loss: 0.341
[81,     1] loss: 0.351
[82,     1] loss: 0.407
[83,     1] loss: 0.358
[84,     1] loss: 0.406
[85,     1] loss: 0.378
[86,     1] loss: 0.441
[87,     1] loss: 0.356
[88,     1] loss: 0.381
[89,     1] loss: 0.344
[90,     1] loss: 0.319
[91,     1] loss: 0.338
[92,     1] loss: 0.290
[93,     1] loss: 0.320
[94,     1] loss: 0.308
[95,     1] loss: 0.296
[96,     1] loss: 0.296
[97,     1] loss: 0.264
[98,     1] loss: 0.283
[99,     1] loss: 0.309
[100,     1] loss: 0.517
[101,     1] loss: 0.361
[102,     1] loss: 0.321
[103,     1] loss: 0.371
[104,     1] loss: 0.344
[105,     1] loss: 0.334
[106,     1] loss: 0.338
[107,     1] loss: 0.288
[108,     1] loss: 0.310
[109,     1] loss: 0.316
[110,     1] loss: 0.308
[111,     1] loss: 0.327
[112,     1] loss: 0.272
[113,     1] loss: 0.344
[114,     1] loss: 0.259
[115,     1] loss: 0.312
[116,     1] loss: 0.366
[117,     1] loss: 0.354
[118,     1] loss: 0.388
[119,     1] loss: 0.340
[120,     1] loss: 0.326
[121,     1] loss: 0.296
[122,     1] loss: 0.312
[123,     1] loss: 0.389
[124,     1] loss: 0.304
[125,     1] loss: 0.377
[126,     1] loss: 0.343
[127,     1] loss: 0.371
[128,     1] loss: 0.333
[129,     1] loss: 0.327
[130,     1] loss: 0.331
[131,     1] loss: 0.300
[132,     1] loss: 0.269
[133,     1] loss: 0.347
[134,     1] loss: 0.300
[135,     1] loss: 0.365
[136,     1] loss: 0.256
[137,     1] loss: 0.306
[138,     1] loss: 0.276
[139,     1] loss: 0.304
[140,     1] loss: 0.300
[141,     1] loss: 0.346
[142,     1] loss: 0.283
[143,     1] loss: 0.268
[144,     1] loss: 0.258
[145,     1] loss: 0.274
[146,     1] loss: 0.223
[147,     1] loss: 0.277
Early stopping applied (best metric=0.2338915765285492)
Finished Training
Total time taken: 17.234599828720093
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.697
[3,     1] loss: 0.689
[4,     1] loss: 0.677
[5,     1] loss: 0.656
[6,     1] loss: 0.638
[7,     1] loss: 0.615
[8,     1] loss: 0.568
[9,     1] loss: 0.546
[10,     1] loss: 0.513
[11,     1] loss: 0.482
[12,     1] loss: 0.516
[13,     1] loss: 0.615
[14,     1] loss: 0.510
[15,     1] loss: 0.540
[16,     1] loss: 0.503
[17,     1] loss: 0.509
[18,     1] loss: 0.495
[19,     1] loss: 0.444
[20,     1] loss: 0.432
[21,     1] loss: 0.436
[22,     1] loss: 0.418
[23,     1] loss: 0.416
[24,     1] loss: 0.419
[25,     1] loss: 0.401
[26,     1] loss: 0.459
[27,     1] loss: 0.454
[28,     1] loss: 0.467
[29,     1] loss: 0.372
[30,     1] loss: 0.448
[31,     1] loss: 0.370
[32,     1] loss: 0.401
[33,     1] loss: 0.386
[34,     1] loss: 0.458
[35,     1] loss: 0.359
[36,     1] loss: 0.406
[37,     1] loss: 0.374
[38,     1] loss: 0.370
[39,     1] loss: 0.378
[40,     1] loss: 0.536
[41,     1] loss: 0.454
[42,     1] loss: 0.549
[43,     1] loss: 0.443
[44,     1] loss: 0.511
[45,     1] loss: 0.463
[46,     1] loss: 0.483
[47,     1] loss: 0.483
[48,     1] loss: 0.442
[49,     1] loss: 0.448
[50,     1] loss: 0.442
[51,     1] loss: 0.436
[52,     1] loss: 0.425
[53,     1] loss: 0.391
[54,     1] loss: 0.433
[55,     1] loss: 0.372
[56,     1] loss: 0.458
[57,     1] loss: 0.484
[58,     1] loss: 0.379
[59,     1] loss: 0.472
[60,     1] loss: 0.393
[61,     1] loss: 0.361
[62,     1] loss: 0.401
[63,     1] loss: 0.353
[64,     1] loss: 0.373
[65,     1] loss: 0.390
[66,     1] loss: 0.373
[67,     1] loss: 0.367
[68,     1] loss: 0.346
[69,     1] loss: 0.368
[70,     1] loss: 0.433
[71,     1] loss: 0.421
[72,     1] loss: 0.397
[73,     1] loss: 0.349
[74,     1] loss: 0.341
[75,     1] loss: 0.307
[76,     1] loss: 0.473
[77,     1] loss: 0.427
[78,     1] loss: 0.339
[79,     1] loss: 0.327
[80,     1] loss: 0.352
[81,     1] loss: 0.350
[82,     1] loss: 0.380
[83,     1] loss: 0.330
[84,     1] loss: 0.382
Early stopping applied (best metric=0.3367066979408264)
Finished Training
Total time taken: 9.574528455734253
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.685
[3,     1] loss: 0.675
[4,     1] loss: 0.651
[5,     1] loss: 0.604
[6,     1] loss: 0.574
[7,     1] loss: 0.546
[8,     1] loss: 0.502
[9,     1] loss: 0.477
[10,     1] loss: 0.445
[11,     1] loss: 0.437
[12,     1] loss: 0.453
[13,     1] loss: 0.376
[14,     1] loss: 0.428
[15,     1] loss: 0.410
[16,     1] loss: 0.383
[17,     1] loss: 0.407
[18,     1] loss: 0.375
[19,     1] loss: 0.376
[20,     1] loss: 0.416
[21,     1] loss: 0.387
[22,     1] loss: 0.606
[23,     1] loss: 0.459
[24,     1] loss: 0.436
[25,     1] loss: 0.502
[26,     1] loss: 0.414
[27,     1] loss: 0.412
[28,     1] loss: 0.407
[29,     1] loss: 0.355
[30,     1] loss: 0.354
[31,     1] loss: 0.347
[32,     1] loss: 0.359
[33,     1] loss: 0.346
[34,     1] loss: 0.342
[35,     1] loss: 0.291
[36,     1] loss: 0.354
[37,     1] loss: 0.357
[38,     1] loss: 0.405
[39,     1] loss: 0.375
[40,     1] loss: 0.491
[41,     1] loss: 0.360
[42,     1] loss: 0.392
[43,     1] loss: 0.320
[44,     1] loss: 0.356
[45,     1] loss: 0.368
[46,     1] loss: 0.375
[47,     1] loss: 0.357
[48,     1] loss: 0.337
[49,     1] loss: 0.311
[50,     1] loss: 0.335
[51,     1] loss: 0.292
[52,     1] loss: 0.272
[53,     1] loss: 0.324
[54,     1] loss: 0.308
[55,     1] loss: 0.305
[56,     1] loss: 0.296
[57,     1] loss: 0.500
[58,     1] loss: 0.590
[59,     1] loss: 0.372
[60,     1] loss: 0.352
[61,     1] loss: 0.412
[62,     1] loss: 0.360
[63,     1] loss: 0.357
[64,     1] loss: 0.371
[65,     1] loss: 0.355
[66,     1] loss: 0.342
[67,     1] loss: 0.331
[68,     1] loss: 0.318
[69,     1] loss: 0.303
[70,     1] loss: 0.284
[71,     1] loss: 0.383
[72,     1] loss: 0.320
[73,     1] loss: 0.301
[74,     1] loss: 0.348
[75,     1] loss: 0.334
[76,     1] loss: 0.275
[77,     1] loss: 0.293
[78,     1] loss: 0.284
[79,     1] loss: 0.322
[80,     1] loss: 0.291
[81,     1] loss: 0.335
[82,     1] loss: 0.307
[83,     1] loss: 0.313
[84,     1] loss: 0.278
[85,     1] loss: 0.322
[86,     1] loss: 0.260
[87,     1] loss: 0.317
[88,     1] loss: 0.408
[89,     1] loss: 0.783
[90,     1] loss: 0.540
[91,     1] loss: 0.413
[92,     1] loss: 0.457
[93,     1] loss: 0.452
[94,     1] loss: 0.426
[95,     1] loss: 0.438
[96,     1] loss: 0.424
[97,     1] loss: 0.372
[98,     1] loss: 0.375
[99,     1] loss: 0.353
[100,     1] loss: 0.355
[101,     1] loss: 0.319
[102,     1] loss: 0.316
[103,     1] loss: 0.331
[104,     1] loss: 0.272
[105,     1] loss: 0.272
[106,     1] loss: 0.298
[107,     1] loss: 0.302
[108,     1] loss: 0.357
[109,     1] loss: 0.381
[110,     1] loss: 0.324
[111,     1] loss: 0.354
[112,     1] loss: 0.266
[113,     1] loss: 0.304
[114,     1] loss: 0.304
[115,     1] loss: 0.318
[116,     1] loss: 0.310
[117,     1] loss: 0.321
[118,     1] loss: 0.287
[119,     1] loss: 0.276
[120,     1] loss: 0.313
[121,     1] loss: 0.363
[122,     1] loss: 0.332
[123,     1] loss: 0.320
[124,     1] loss: 0.341
[125,     1] loss: 0.287
[126,     1] loss: 0.332
[127,     1] loss: 0.309
[128,     1] loss: 0.327
[129,     1] loss: 0.289
[130,     1] loss: 0.308
[131,     1] loss: 0.360
[132,     1] loss: 0.303
[133,     1] loss: 0.314
[134,     1] loss: 0.270
[135,     1] loss: 0.301
[136,     1] loss: 0.303
[137,     1] loss: 0.284
[138,     1] loss: 0.256
[139,     1] loss: 0.313
[140,     1] loss: 0.444
[141,     1] loss: 0.509
[142,     1] loss: 0.391
[143,     1] loss: 0.408
[144,     1] loss: 0.476
[145,     1] loss: 0.389
[146,     1] loss: 0.369
[147,     1] loss: 0.380
[148,     1] loss: 0.396
[149,     1] loss: 0.394
[150,     1] loss: 0.383
[151,     1] loss: 0.367
[152,     1] loss: 0.321
[153,     1] loss: 0.330
[154,     1] loss: 0.301
[155,     1] loss: 0.303
[156,     1] loss: 0.302
[157,     1] loss: 0.300
[158,     1] loss: 0.340
[159,     1] loss: 0.302
[160,     1] loss: 0.361
[161,     1] loss: 0.289
[162,     1] loss: 0.382
[163,     1] loss: 0.373
[164,     1] loss: 0.321
[165,     1] loss: 0.368
[166,     1] loss: 0.329
[167,     1] loss: 0.309
[168,     1] loss: 0.293
[169,     1] loss: 0.283
[170,     1] loss: 0.296
[171,     1] loss: 0.294
[172,     1] loss: 0.379
[173,     1] loss: 0.331
[174,     1] loss: 0.355
[175,     1] loss: 0.404
[176,     1] loss: 0.342
[177,     1] loss: 0.417
[178,     1] loss: 0.335
[179,     1] loss: 0.330
[180,     1] loss: 0.343
[181,     1] loss: 0.301
[182,     1] loss: 0.334
[183,     1] loss: 0.307
[184,     1] loss: 0.278
[185,     1] loss: 0.293
[186,     1] loss: 0.298
[187,     1] loss: 0.347
[188,     1] loss: 0.878
Early stopping applied (best metric=0.26532182097435)
Finished Training
Total time taken: 21.642009735107422
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.689
[3,     1] loss: 0.681
[4,     1] loss: 0.648
[5,     1] loss: 0.621
[6,     1] loss: 0.581
[7,     1] loss: 0.512
[8,     1] loss: 0.509
[9,     1] loss: 0.459
[10,     1] loss: 0.467
[11,     1] loss: 0.474
[12,     1] loss: 0.490
[13,     1] loss: 0.454
[14,     1] loss: 0.433
[15,     1] loss: 0.439
[16,     1] loss: 0.416
[17,     1] loss: 0.403
[18,     1] loss: 0.374
[19,     1] loss: 0.369
[20,     1] loss: 0.429
[21,     1] loss: 0.461
[22,     1] loss: 0.373
[23,     1] loss: 0.367
[24,     1] loss: 0.387
[25,     1] loss: 0.360
[26,     1] loss: 0.395
[27,     1] loss: 0.376
[28,     1] loss: 0.334
[29,     1] loss: 0.375
[30,     1] loss: 0.347
[31,     1] loss: 0.332
[32,     1] loss: 0.362
[33,     1] loss: 0.383
[34,     1] loss: 0.324
[35,     1] loss: 0.320
[36,     1] loss: 0.431
[37,     1] loss: 0.348
[38,     1] loss: 0.373
[39,     1] loss: 0.311
[40,     1] loss: 0.323
[41,     1] loss: 0.316
[42,     1] loss: 0.318
[43,     1] loss: 0.299
[44,     1] loss: 0.320
[45,     1] loss: 0.313
[46,     1] loss: 0.320
[47,     1] loss: 0.377
[48,     1] loss: 0.823
[49,     1] loss: 0.550
[50,     1] loss: 0.558
[51,     1] loss: 0.487
[52,     1] loss: 0.451
[53,     1] loss: 0.451
[54,     1] loss: 0.450
[55,     1] loss: 0.417
[56,     1] loss: 0.441
[57,     1] loss: 0.435
[58,     1] loss: 0.400
[59,     1] loss: 0.398
[60,     1] loss: 0.372
[61,     1] loss: 0.378
[62,     1] loss: 0.348
[63,     1] loss: 0.419
[64,     1] loss: 0.486
[65,     1] loss: 0.405
[66,     1] loss: 0.425
[67,     1] loss: 0.397
[68,     1] loss: 0.434
[69,     1] loss: 0.397
[70,     1] loss: 0.407
[71,     1] loss: 0.391
[72,     1] loss: 0.388
[73,     1] loss: 0.341
[74,     1] loss: 0.371
[75,     1] loss: 0.365
[76,     1] loss: 0.353
[77,     1] loss: 0.378
[78,     1] loss: 0.362
[79,     1] loss: 0.388
[80,     1] loss: 0.385
[81,     1] loss: 0.384
[82,     1] loss: 0.371
[83,     1] loss: 0.344
[84,     1] loss: 0.377
[85,     1] loss: 0.354
[86,     1] loss: 0.360
[87,     1] loss: 0.338
[88,     1] loss: 0.353
[89,     1] loss: 0.329
[90,     1] loss: 0.316
[91,     1] loss: 0.502
Early stopping applied (best metric=0.34798464179039)
Finished Training
Total time taken: 10.858965396881104
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.648
[5,     1] loss: 0.617
[6,     1] loss: 0.585
[7,     1] loss: 0.539
[8,     1] loss: 0.505
[9,     1] loss: 0.482
[10,     1] loss: 0.435
[11,     1] loss: 0.566
[12,     1] loss: 0.419
[13,     1] loss: 0.432
[14,     1] loss: 0.446
[15,     1] loss: 0.446
[16,     1] loss: 0.418
[17,     1] loss: 0.392
[18,     1] loss: 0.406
[19,     1] loss: 0.403
[20,     1] loss: 0.375
[21,     1] loss: 0.363
[22,     1] loss: 0.336
[23,     1] loss: 0.332
[24,     1] loss: 0.364
[25,     1] loss: 0.323
[26,     1] loss: 0.377
[27,     1] loss: 0.434
[28,     1] loss: 0.293
[29,     1] loss: 0.623
[30,     1] loss: 0.375
[31,     1] loss: 0.432
[32,     1] loss: 0.407
[33,     1] loss: 0.395
[34,     1] loss: 0.387
[35,     1] loss: 0.426
[36,     1] loss: 0.371
[37,     1] loss: 0.366
[38,     1] loss: 0.368
[39,     1] loss: 0.369
[40,     1] loss: 0.358
[41,     1] loss: 0.360
[42,     1] loss: 0.303
[43,     1] loss: 0.345
[44,     1] loss: 0.359
[45,     1] loss: 0.311
[46,     1] loss: 0.347
[47,     1] loss: 0.394
[48,     1] loss: 0.378
[49,     1] loss: 0.313
[50,     1] loss: 0.312
[51,     1] loss: 0.284
[52,     1] loss: 0.332
[53,     1] loss: 0.340
[54,     1] loss: 0.412
[55,     1] loss: 0.356
[56,     1] loss: 0.498
[57,     1] loss: 0.387
[58,     1] loss: 0.454
[59,     1] loss: 0.426
[60,     1] loss: 0.417
[61,     1] loss: 0.390
[62,     1] loss: 0.403
[63,     1] loss: 0.389
[64,     1] loss: 0.394
[65,     1] loss: 0.345
[66,     1] loss: 0.378
[67,     1] loss: 0.397
[68,     1] loss: 0.371
[69,     1] loss: 0.383
[70,     1] loss: 0.318
[71,     1] loss: 0.342
[72,     1] loss: 0.340
[73,     1] loss: 0.312
[74,     1] loss: 0.309
[75,     1] loss: 0.346
[76,     1] loss: 0.291
[77,     1] loss: 0.490
[78,     1] loss: 0.920
[79,     1] loss: 0.776
[80,     1] loss: 0.693
[81,     1] loss: 0.679
[82,     1] loss: 0.692
[83,     1] loss: 0.692
[84,     1] loss: 0.690
[85,     1] loss: 0.692
[86,     1] loss: 0.692
[87,     1] loss: 0.692
[88,     1] loss: 0.693
[89,     1] loss: 0.693
[90,     1] loss: 0.693
[91,     1] loss: 0.692
[92,     1] loss: 0.693
[93,     1] loss: 0.692
[94,     1] loss: 0.693
[95,     1] loss: 0.693
[96,     1] loss: 0.693
[97,     1] loss: 0.693
[98,     1] loss: 0.693
[99,     1] loss: 0.693
[100,     1] loss: 0.693
[101,     1] loss: 0.693
[102,     1] loss: 0.693
[103,     1] loss: 0.693
[104,     1] loss: 0.693
[105,     1] loss: 0.693
[106,     1] loss: 0.693
[107,     1] loss: 0.693
[108,     1] loss: 0.693
[109,     1] loss: 0.693
[110,     1] loss: 0.693
[111,     1] loss: 0.693
[112,     1] loss: 0.693
[113,     1] loss: 0.692
[114,     1] loss: 0.692
[115,     1] loss: 0.692
[116,     1] loss: 0.692
Early stopping applied (best metric=0.3973018527030945)
Finished Training
Total time taken: 13.735520362854004
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.649
[5,     1] loss: 0.622
[6,     1] loss: 0.586
[7,     1] loss: 0.538
[8,     1] loss: 0.508
[9,     1] loss: 0.480
[10,     1] loss: 0.455
[11,     1] loss: 0.487
[12,     1] loss: 0.417
[13,     1] loss: 0.404
[14,     1] loss: 0.429
[15,     1] loss: 0.380
[16,     1] loss: 0.362
[17,     1] loss: 0.363
[18,     1] loss: 0.350
[19,     1] loss: 0.579
[20,     1] loss: 0.486
[21,     1] loss: 0.435
[22,     1] loss: 0.423
[23,     1] loss: 0.437
[24,     1] loss: 0.455
[25,     1] loss: 0.394
[26,     1] loss: 0.398
[27,     1] loss: 0.384
[28,     1] loss: 0.359
[29,     1] loss: 0.353
[30,     1] loss: 0.389
[31,     1] loss: 0.340
[32,     1] loss: 0.331
[33,     1] loss: 0.342
[34,     1] loss: 0.317
[35,     1] loss: 0.386
[36,     1] loss: 0.375
[37,     1] loss: 0.365
[38,     1] loss: 0.360
[39,     1] loss: 0.387
[40,     1] loss: 0.302
[41,     1] loss: 0.358
[42,     1] loss: 0.307
[43,     1] loss: 0.323
[44,     1] loss: 0.352
[45,     1] loss: 0.335
[46,     1] loss: 0.525
[47,     1] loss: 0.557
[48,     1] loss: 0.403
[49,     1] loss: 0.439
[50,     1] loss: 0.436
[51,     1] loss: 0.406
[52,     1] loss: 0.402
[53,     1] loss: 0.389
[54,     1] loss: 0.375
[55,     1] loss: 0.365
[56,     1] loss: 0.338
[57,     1] loss: 0.325
[58,     1] loss: 0.385
[59,     1] loss: 0.346
[60,     1] loss: 0.344
[61,     1] loss: 0.316
[62,     1] loss: 0.335
[63,     1] loss: 0.287
[64,     1] loss: 0.331
[65,     1] loss: 0.293
[66,     1] loss: 0.303
[67,     1] loss: 0.652
[68,     1] loss: 0.663
[69,     1] loss: 0.558
[70,     1] loss: 0.493
[71,     1] loss: 0.472
[72,     1] loss: 0.493
[73,     1] loss: 0.469
[74,     1] loss: 0.444
[75,     1] loss: 0.435
[76,     1] loss: 0.438
[77,     1] loss: 0.417
[78,     1] loss: 0.414
[79,     1] loss: 0.441
[80,     1] loss: 0.410
[81,     1] loss: 0.405
[82,     1] loss: 0.388
Early stopping applied (best metric=0.3656501770019531)
Finished Training
Total time taken: 9.898643493652344
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.691
[3,     1] loss: 0.682
[4,     1] loss: 0.668
[5,     1] loss: 0.642
[6,     1] loss: 0.623
[7,     1] loss: 0.580
[8,     1] loss: 0.556
[9,     1] loss: 0.554
[10,     1] loss: 0.532
[11,     1] loss: 0.496
[12,     1] loss: 0.488
[13,     1] loss: 0.502
[14,     1] loss: 0.469
[15,     1] loss: 0.473
[16,     1] loss: 0.468
[17,     1] loss: 0.468
[18,     1] loss: 0.409
[19,     1] loss: 0.453
[20,     1] loss: 0.395
[21,     1] loss: 0.351
[22,     1] loss: 0.377
[23,     1] loss: 0.358
[24,     1] loss: 0.329
[25,     1] loss: 0.334
[26,     1] loss: 0.363
[27,     1] loss: 0.408
[28,     1] loss: 0.352
[29,     1] loss: 0.472
[30,     1] loss: 0.419
[31,     1] loss: 0.399
[32,     1] loss: 0.361
[33,     1] loss: 0.358
[34,     1] loss: 0.368
[35,     1] loss: 0.342
[36,     1] loss: 0.369
[37,     1] loss: 0.357
[38,     1] loss: 0.353
[39,     1] loss: 0.345
[40,     1] loss: 0.346
[41,     1] loss: 0.337
[42,     1] loss: 0.347
[43,     1] loss: 0.471
[44,     1] loss: 0.338
[45,     1] loss: 0.377
[46,     1] loss: 0.353
[47,     1] loss: 0.351
[48,     1] loss: 0.353
[49,     1] loss: 0.367
[50,     1] loss: 0.348
[51,     1] loss: 0.372
[52,     1] loss: 0.365
[53,     1] loss: 0.373
[54,     1] loss: 0.348
[55,     1] loss: 0.332
[56,     1] loss: 0.321
[57,     1] loss: 0.335
[58,     1] loss: 0.337
[59,     1] loss: 0.392
[60,     1] loss: 0.381
[61,     1] loss: 0.403
[62,     1] loss: 0.343
[63,     1] loss: 0.351
[64,     1] loss: 0.369
[65,     1] loss: 0.368
[66,     1] loss: 0.343
[67,     1] loss: 0.359
[68,     1] loss: 0.366
[69,     1] loss: 0.325
[70,     1] loss: 0.334
[71,     1] loss: 0.327
[72,     1] loss: 0.336
[73,     1] loss: 0.310
[74,     1] loss: 0.334
[75,     1] loss: 0.340
[76,     1] loss: 0.512
[77,     1] loss: 0.695
[78,     1] loss: 0.411
[79,     1] loss: 0.435
[80,     1] loss: 0.367
[81,     1] loss: 0.408
[82,     1] loss: 0.398
[83,     1] loss: 0.382
[84,     1] loss: 0.394
[85,     1] loss: 0.396
[86,     1] loss: 0.391
[87,     1] loss: 0.372
[88,     1] loss: 0.363
[89,     1] loss: 0.348
[90,     1] loss: 0.332
[91,     1] loss: 0.362
[92,     1] loss: 0.394
[93,     1] loss: 0.372
[94,     1] loss: 0.326
Early stopping applied (best metric=0.3069767653942108)
Finished Training
Total time taken: 11.197410106658936
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.694
[3,     1] loss: 0.692
[4,     1] loss: 0.682
[5,     1] loss: 0.663
[6,     1] loss: 0.642
[7,     1] loss: 0.623
[8,     1] loss: 0.601
[9,     1] loss: 0.559
[10,     1] loss: 0.546
[11,     1] loss: 0.517
[12,     1] loss: 0.524
[13,     1] loss: 0.521
[14,     1] loss: 0.498
[15,     1] loss: 0.478
[16,     1] loss: 0.479
[17,     1] loss: 0.455
[18,     1] loss: 0.413
[19,     1] loss: 0.467
[20,     1] loss: 0.393
[21,     1] loss: 0.417
[22,     1] loss: 0.374
[23,     1] loss: 0.412
[24,     1] loss: 0.415
[25,     1] loss: 0.388
[26,     1] loss: 0.400
[27,     1] loss: 0.431
[28,     1] loss: 0.354
[29,     1] loss: 0.398
[30,     1] loss: 0.368
[31,     1] loss: 0.361
[32,     1] loss: 0.378
[33,     1] loss: 0.375
[34,     1] loss: 0.407
[35,     1] loss: 0.733
[36,     1] loss: 0.449
[37,     1] loss: 0.458
[38,     1] loss: 0.479
[39,     1] loss: 0.478
[40,     1] loss: 0.449
[41,     1] loss: 0.446
[42,     1] loss: 0.427
[43,     1] loss: 0.452
[44,     1] loss: 0.440
[45,     1] loss: 0.414
[46,     1] loss: 0.399
[47,     1] loss: 0.388
[48,     1] loss: 0.423
[49,     1] loss: 0.459
[50,     1] loss: 0.434
[51,     1] loss: 0.400
[52,     1] loss: 0.406
[53,     1] loss: 0.387
[54,     1] loss: 0.386
[55,     1] loss: 0.379
[56,     1] loss: 0.364
[57,     1] loss: 0.339
[58,     1] loss: 0.412
[59,     1] loss: 0.360
[60,     1] loss: 0.398
[61,     1] loss: 0.369
[62,     1] loss: 0.357
[63,     1] loss: 0.358
[64,     1] loss: 0.374
[65,     1] loss: 0.412
[66,     1] loss: 0.367
[67,     1] loss: 0.360
[68,     1] loss: 0.360
[69,     1] loss: 0.343
[70,     1] loss: 0.386
[71,     1] loss: 0.372
[72,     1] loss: 0.415
[73,     1] loss: 0.366
[74,     1] loss: 0.352
[75,     1] loss: 0.414
[76,     1] loss: 0.367
[77,     1] loss: 0.352
[78,     1] loss: 0.354
[79,     1] loss: 0.417
[80,     1] loss: 0.467
[81,     1] loss: 0.391
[82,     1] loss: 0.438
[83,     1] loss: 0.422
[84,     1] loss: 0.392
[85,     1] loss: 0.417
[86,     1] loss: 0.383
[87,     1] loss: 0.440
[88,     1] loss: 0.378
[89,     1] loss: 0.385
[90,     1] loss: 0.396
[91,     1] loss: 0.371
[92,     1] loss: 0.367
[93,     1] loss: 0.361
[94,     1] loss: 0.378
[95,     1] loss: 0.362
[96,     1] loss: 0.349
[97,     1] loss: 0.378
[98,     1] loss: 0.393
[99,     1] loss: 0.337
[100,     1] loss: 0.364
[101,     1] loss: 0.358
[102,     1] loss: 0.317
[103,     1] loss: 0.358
[104,     1] loss: 0.418
[105,     1] loss: 0.457
[106,     1] loss: 0.392
[107,     1] loss: 0.427
[108,     1] loss: 0.421
[109,     1] loss: 0.391
[110,     1] loss: 0.412
[111,     1] loss: 0.428
[112,     1] loss: 0.403
[113,     1] loss: 0.406
Early stopping applied (best metric=0.3382076919078827)
Finished Training
Total time taken: 13.003071069717407
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.717
[3,     1] loss: 0.690
[4,     1] loss: 0.688
[5,     1] loss: 0.685
[6,     1] loss: 0.669
[7,     1] loss: 0.661
[8,     1] loss: 0.652
[9,     1] loss: 0.622
[10,     1] loss: 0.609
[11,     1] loss: 0.589
[12,     1] loss: 0.567
[13,     1] loss: 0.553
[14,     1] loss: 0.517
[15,     1] loss: 0.517
[16,     1] loss: 0.479
[17,     1] loss: 0.536
[18,     1] loss: 0.505
[19,     1] loss: 0.471
[20,     1] loss: 0.470
[21,     1] loss: 0.414
[22,     1] loss: 0.417
[23,     1] loss: 0.428
[24,     1] loss: 0.420
[25,     1] loss: 0.383
[26,     1] loss: 0.418
[27,     1] loss: 0.395
[28,     1] loss: 0.357
[29,     1] loss: 0.419
[30,     1] loss: 0.366
[31,     1] loss: 0.424
[32,     1] loss: 0.371
[33,     1] loss: 0.369
[34,     1] loss: 0.357
[35,     1] loss: 0.373
[36,     1] loss: 0.374
[37,     1] loss: 0.345
[38,     1] loss: 0.341
[39,     1] loss: 0.399
[40,     1] loss: 0.371
[41,     1] loss: 0.372
[42,     1] loss: 0.362
[43,     1] loss: 0.394
[44,     1] loss: 0.386
[45,     1] loss: 0.348
[46,     1] loss: 0.399
[47,     1] loss: 0.479
[48,     1] loss: 0.427
[49,     1] loss: 0.440
[50,     1] loss: 0.388
[51,     1] loss: 0.392
[52,     1] loss: 0.406
[53,     1] loss: 0.372
[54,     1] loss: 0.381
[55,     1] loss: 0.343
[56,     1] loss: 0.322
[57,     1] loss: 0.369
[58,     1] loss: 0.369
[59,     1] loss: 0.335
[60,     1] loss: 0.403
[61,     1] loss: 0.390
[62,     1] loss: 0.362
[63,     1] loss: 0.360
[64,     1] loss: 0.379
[65,     1] loss: 0.332
[66,     1] loss: 0.322
[67,     1] loss: 0.330
[68,     1] loss: 0.324
[69,     1] loss: 0.336
[70,     1] loss: 0.342
[71,     1] loss: 0.319
[72,     1] loss: 0.313
[73,     1] loss: 0.345
[74,     1] loss: 0.290
[75,     1] loss: 0.551
[76,     1] loss: 0.431
[77,     1] loss: 0.485
[78,     1] loss: 0.397
[79,     1] loss: 0.407
[80,     1] loss: 0.366
[81,     1] loss: 0.386
[82,     1] loss: 0.389
[83,     1] loss: 0.379
[84,     1] loss: 0.339
[85,     1] loss: 0.331
[86,     1] loss: 0.347
[87,     1] loss: 0.320
[88,     1] loss: 0.315
[89,     1] loss: 0.318
[90,     1] loss: 0.383
[91,     1] loss: 0.386
[92,     1] loss: 0.319
[93,     1] loss: 0.344
[94,     1] loss: 0.353
[95,     1] loss: 0.401
[96,     1] loss: 0.328
[97,     1] loss: 0.326
[98,     1] loss: 0.350
[99,     1] loss: 0.330
[100,     1] loss: 0.363
[101,     1] loss: 0.352
[102,     1] loss: 0.355
[103,     1] loss: 0.335
[104,     1] loss: 0.340
[105,     1] loss: 0.333
[106,     1] loss: 0.308
[107,     1] loss: 0.324
[108,     1] loss: 0.349
[109,     1] loss: 0.278
[110,     1] loss: 0.313
[111,     1] loss: 0.310
[112,     1] loss: 0.296
[113,     1] loss: 0.352
[114,     1] loss: 0.444
[115,     1] loss: 0.585
[116,     1] loss: 0.396
[117,     1] loss: 0.382
[118,     1] loss: 0.405
[119,     1] loss: 0.388
Early stopping applied (best metric=0.355540931224823)
Finished Training
Total time taken: 13.702127456665039
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.695
[3,     1] loss: 0.676
[4,     1] loss: 0.656
[5,     1] loss: 0.637
[6,     1] loss: 0.615
[7,     1] loss: 0.587
[8,     1] loss: 0.547
[9,     1] loss: 0.533
[10,     1] loss: 0.494
[11,     1] loss: 0.486
[12,     1] loss: 0.446
[13,     1] loss: 0.493
[14,     1] loss: 0.430
[15,     1] loss: 0.425
[16,     1] loss: 0.443
[17,     1] loss: 0.467
[18,     1] loss: 0.382
[19,     1] loss: 0.409
[20,     1] loss: 0.373
[21,     1] loss: 0.361
[22,     1] loss: 0.369
[23,     1] loss: 0.365
[24,     1] loss: 0.350
[25,     1] loss: 0.342
[26,     1] loss: 0.403
[27,     1] loss: 0.441
[28,     1] loss: 0.377
[29,     1] loss: 0.379
[30,     1] loss: 0.384
[31,     1] loss: 0.376
[32,     1] loss: 0.355
[33,     1] loss: 0.342
[34,     1] loss: 0.346
[35,     1] loss: 0.317
[36,     1] loss: 0.325
[37,     1] loss: 0.290
[38,     1] loss: 0.298
[39,     1] loss: 0.337
[40,     1] loss: 0.386
[41,     1] loss: 0.353
[42,     1] loss: 0.362
[43,     1] loss: 0.322
[44,     1] loss: 0.382
[45,     1] loss: 0.347
[46,     1] loss: 0.329
[47,     1] loss: 0.317
[48,     1] loss: 0.314
[49,     1] loss: 0.301
[50,     1] loss: 0.278
[51,     1] loss: 0.281
[52,     1] loss: 0.255
[53,     1] loss: 0.320
[54,     1] loss: 0.265
[55,     1] loss: 0.285
[56,     1] loss: 0.353
[57,     1] loss: 0.291
[58,     1] loss: 0.304
[59,     1] loss: 0.292
[60,     1] loss: 0.368
[61,     1] loss: 0.454
[62,     1] loss: 0.420
[63,     1] loss: 0.382
[64,     1] loss: 0.382
[65,     1] loss: 0.335
[66,     1] loss: 0.333
[67,     1] loss: 0.327
[68,     1] loss: 0.345
[69,     1] loss: 0.308
[70,     1] loss: 0.317
[71,     1] loss: 0.327
[72,     1] loss: 0.319
[73,     1] loss: 0.299
[74,     1] loss: 0.331
[75,     1] loss: 0.298
[76,     1] loss: 0.307
[77,     1] loss: 0.275
[78,     1] loss: 0.274
[79,     1] loss: 0.247
[80,     1] loss: 0.290
[81,     1] loss: 0.437
[82,     1] loss: 0.740
[83,     1] loss: 0.458
[84,     1] loss: 0.502
[85,     1] loss: 0.450
[86,     1] loss: 0.455
[87,     1] loss: 0.469
[88,     1] loss: 0.483
[89,     1] loss: 0.457
[90,     1] loss: 0.460
[91,     1] loss: 0.444
[92,     1] loss: 0.444
[93,     1] loss: 0.428
[94,     1] loss: 0.395
[95,     1] loss: 0.398
[96,     1] loss: 0.367
[97,     1] loss: 0.366
[98,     1] loss: 0.361
[99,     1] loss: 0.349
[100,     1] loss: 0.412
[101,     1] loss: 0.349
[102,     1] loss: 0.418
[103,     1] loss: 0.360
[104,     1] loss: 0.322
[105,     1] loss: 0.361
[106,     1] loss: 0.294
[107,     1] loss: 0.316
[108,     1] loss: 0.318
[109,     1] loss: 0.322
[110,     1] loss: 0.305
[111,     1] loss: 0.457
[112,     1] loss: 0.524
[113,     1] loss: 0.372
[114,     1] loss: 0.498
[115,     1] loss: 0.428
[116,     1] loss: 0.417
[117,     1] loss: 0.423
[118,     1] loss: 0.418
[119,     1] loss: 0.377
[120,     1] loss: 0.428
[121,     1] loss: 0.405
[122,     1] loss: 0.391
[123,     1] loss: 0.391
[124,     1] loss: 0.374
[125,     1] loss: 0.387
[126,     1] loss: 0.405
[127,     1] loss: 0.371
[128,     1] loss: 0.375
[129,     1] loss: 0.353
[130,     1] loss: 0.376
[131,     1] loss: 0.333
[132,     1] loss: 0.327
[133,     1] loss: 0.317
[134,     1] loss: 0.337
[135,     1] loss: 0.453
[136,     1] loss: 0.458
[137,     1] loss: 0.387
[138,     1] loss: 0.446
[139,     1] loss: 0.386
[140,     1] loss: 0.445
[141,     1] loss: 0.387
[142,     1] loss: 0.400
[143,     1] loss: 0.397
[144,     1] loss: 0.373
[145,     1] loss: 0.398
[146,     1] loss: 0.407
[147,     1] loss: 0.383
[148,     1] loss: 0.404
[149,     1] loss: 0.376
Early stopping applied (best metric=0.3166668117046356)
Finished Training
Total time taken: 17.20106554031372
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.691
[3,     1] loss: 0.668
[4,     1] loss: 0.623
[5,     1] loss: 0.583
[6,     1] loss: 0.531
[7,     1] loss: 0.501
[8,     1] loss: 0.458
[9,     1] loss: 0.466
[10,     1] loss: 0.462
[11,     1] loss: 0.406
[12,     1] loss: 0.408
[13,     1] loss: 0.403
[14,     1] loss: 0.377
[15,     1] loss: 0.354
[16,     1] loss: 0.423
[17,     1] loss: 0.357
[18,     1] loss: 0.512
[19,     1] loss: 0.446
[20,     1] loss: 0.363
[21,     1] loss: 0.396
[22,     1] loss: 0.365
[23,     1] loss: 0.384
[24,     1] loss: 0.408
[25,     1] loss: 0.369
[26,     1] loss: 0.348
[27,     1] loss: 0.337
[28,     1] loss: 0.343
[29,     1] loss: 0.365
[30,     1] loss: 0.405
[31,     1] loss: 0.379
[32,     1] loss: 0.353
[33,     1] loss: 0.414
[34,     1] loss: 0.321
[35,     1] loss: 0.368
[36,     1] loss: 0.357
[37,     1] loss: 0.373
[38,     1] loss: 0.350
[39,     1] loss: 0.496
[40,     1] loss: 0.403
[41,     1] loss: 0.382
[42,     1] loss: 0.398
[43,     1] loss: 0.367
[44,     1] loss: 0.400
[45,     1] loss: 0.386
[46,     1] loss: 0.374
[47,     1] loss: 0.337
[48,     1] loss: 0.364
[49,     1] loss: 0.349
[50,     1] loss: 0.328
[51,     1] loss: 0.328
[52,     1] loss: 0.349
[53,     1] loss: 0.387
[54,     1] loss: 0.447
[55,     1] loss: 0.387
[56,     1] loss: 0.396
[57,     1] loss: 0.366
[58,     1] loss: 0.401
[59,     1] loss: 0.359
[60,     1] loss: 0.364
[61,     1] loss: 0.356
[62,     1] loss: 0.375
[63,     1] loss: 0.317
[64,     1] loss: 0.339
[65,     1] loss: 0.327
[66,     1] loss: 0.349
[67,     1] loss: 0.374
[68,     1] loss: 0.328
[69,     1] loss: 0.375
[70,     1] loss: 0.386
[71,     1] loss: 0.331
[72,     1] loss: 0.315
[73,     1] loss: 0.395
[74,     1] loss: 0.850
[75,     1] loss: 0.569
[76,     1] loss: 0.460
[77,     1] loss: 0.472
[78,     1] loss: 0.488
[79,     1] loss: 0.483
[80,     1] loss: 0.466
[81,     1] loss: 0.471
[82,     1] loss: 0.467
[83,     1] loss: 0.448
[84,     1] loss: 0.429
[85,     1] loss: 0.434
[86,     1] loss: 0.420
[87,     1] loss: 0.426
Early stopping applied (best metric=0.3656834363937378)
Finished Training
Total time taken: 11.710784435272217
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.687
[2,     1] loss: 0.699
[3,     1] loss: 0.684
[4,     1] loss: 0.654
[5,     1] loss: 0.624
[6,     1] loss: 0.593
[7,     1] loss: 0.571
[8,     1] loss: 0.548
[9,     1] loss: 0.494
[10,     1] loss: 0.495
[11,     1] loss: 0.455
[12,     1] loss: 0.445
[13,     1] loss: 0.391
[14,     1] loss: 0.421
[15,     1] loss: 0.458
[16,     1] loss: 0.418
[17,     1] loss: 0.450
[18,     1] loss: 0.406
[19,     1] loss: 0.391
[20,     1] loss: 0.332
[21,     1] loss: 0.379
[22,     1] loss: 0.374
[23,     1] loss: 0.365
[24,     1] loss: 0.357
[25,     1] loss: 0.356
[26,     1] loss: 0.373
[27,     1] loss: 0.362
[28,     1] loss: 0.317
[29,     1] loss: 0.304
[30,     1] loss: 0.326
[31,     1] loss: 0.308
[32,     1] loss: 0.358
[33,     1] loss: 0.349
[34,     1] loss: 0.312
[35,     1] loss: 0.332
[36,     1] loss: 0.294
[37,     1] loss: 0.306
[38,     1] loss: 0.311
[39,     1] loss: 0.288
[40,     1] loss: 0.287
[41,     1] loss: 0.299
[42,     1] loss: 0.368
[43,     1] loss: 0.842
[44,     1] loss: 0.674
[45,     1] loss: 0.587
[46,     1] loss: 0.570
[47,     1] loss: 0.507
[48,     1] loss: 0.533
[49,     1] loss: 0.546
[50,     1] loss: 0.482
[51,     1] loss: 0.476
[52,     1] loss: 0.466
[53,     1] loss: 0.471
[54,     1] loss: 0.417
[55,     1] loss: 0.397
[56,     1] loss: 0.398
[57,     1] loss: 0.360
[58,     1] loss: 0.381
[59,     1] loss: 0.308
[60,     1] loss: 0.315
[61,     1] loss: 0.311
[62,     1] loss: 0.384
[63,     1] loss: 0.593
[64,     1] loss: 0.320
[65,     1] loss: 0.441
[66,     1] loss: 0.378
[67,     1] loss: 0.409
[68,     1] loss: 0.364
[69,     1] loss: 0.359
[70,     1] loss: 0.332
[71,     1] loss: 0.364
[72,     1] loss: 0.335
[73,     1] loss: 0.358
[74,     1] loss: 0.370
[75,     1] loss: 0.341
[76,     1] loss: 0.322
[77,     1] loss: 0.289
[78,     1] loss: 0.308
[79,     1] loss: 0.297
[80,     1] loss: 0.317
[81,     1] loss: 0.327
[82,     1] loss: 0.319
[83,     1] loss: 0.325
[84,     1] loss: 0.338
[85,     1] loss: 0.268
[86,     1] loss: 0.286
[87,     1] loss: 0.304
[88,     1] loss: 0.372
[89,     1] loss: 0.436
[90,     1] loss: 0.391
[91,     1] loss: 0.401
[92,     1] loss: 0.350
[93,     1] loss: 0.384
[94,     1] loss: 0.388
[95,     1] loss: 0.334
[96,     1] loss: 0.338
[97,     1] loss: 0.332
[98,     1] loss: 0.327
[99,     1] loss: 0.332
[100,     1] loss: 0.320
[101,     1] loss: 0.344
[102,     1] loss: 0.364
[103,     1] loss: 0.303
[104,     1] loss: 0.338
[105,     1] loss: 0.348
[106,     1] loss: 0.333
[107,     1] loss: 0.314
[108,     1] loss: 0.324
[109,     1] loss: 0.321
[110,     1] loss: 0.328
[111,     1] loss: 0.311
[112,     1] loss: 0.298
[113,     1] loss: 0.346
[114,     1] loss: 0.286
[115,     1] loss: 0.308
[116,     1] loss: 0.414
[117,     1] loss: 0.799
[118,     1] loss: 0.367
[119,     1] loss: 0.390
[120,     1] loss: 0.360
[121,     1] loss: 0.372
[122,     1] loss: 0.387
[123,     1] loss: 0.349
[124,     1] loss: 0.353
[125,     1] loss: 0.324
[126,     1] loss: 0.369
[127,     1] loss: 0.325
[128,     1] loss: 0.337
[129,     1] loss: 0.319
[130,     1] loss: 0.295
[131,     1] loss: 0.309
[132,     1] loss: 0.313
Early stopping applied (best metric=0.41730865836143494)
Finished Training
Total time taken: 16.46751642227173
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.689
[4,     1] loss: 0.674
[5,     1] loss: 0.647
[6,     1] loss: 0.642
[7,     1] loss: 0.601
[8,     1] loss: 0.564
[9,     1] loss: 0.553
[10,     1] loss: 0.517
[11,     1] loss: 0.475
[12,     1] loss: 0.473
[13,     1] loss: 0.457
[14,     1] loss: 0.431
[15,     1] loss: 0.394
[16,     1] loss: 0.391
[17,     1] loss: 0.596
[18,     1] loss: 0.362
[19,     1] loss: 0.467
[20,     1] loss: 0.399
[21,     1] loss: 0.401
[22,     1] loss: 0.408
[23,     1] loss: 0.431
[24,     1] loss: 0.363
[25,     1] loss: 0.391
[26,     1] loss: 0.373
[27,     1] loss: 0.339
[28,     1] loss: 0.342
[29,     1] loss: 0.356
[30,     1] loss: 0.378
[31,     1] loss: 0.340
[32,     1] loss: 0.528
[33,     1] loss: 0.593
[34,     1] loss: 0.378
[35,     1] loss: 0.500
[36,     1] loss: 0.397
[37,     1] loss: 0.418
[38,     1] loss: 0.399
[39,     1] loss: 0.427
[40,     1] loss: 0.380
[41,     1] loss: 0.331
[42,     1] loss: 0.382
[43,     1] loss: 0.349
[44,     1] loss: 0.380
[45,     1] loss: 0.371
[46,     1] loss: 0.330
[47,     1] loss: 0.348
[48,     1] loss: 0.377
[49,     1] loss: 0.339
[50,     1] loss: 0.318
[51,     1] loss: 0.343
[52,     1] loss: 0.320
[53,     1] loss: 0.355
[54,     1] loss: 0.575
[55,     1] loss: 0.429
[56,     1] loss: 0.466
[57,     1] loss: 0.411
[58,     1] loss: 0.397
[59,     1] loss: 0.435
[60,     1] loss: 0.398
[61,     1] loss: 0.422
[62,     1] loss: 0.383
[63,     1] loss: 0.393
[64,     1] loss: 0.408
[65,     1] loss: 0.330
[66,     1] loss: 0.362
[67,     1] loss: 0.350
[68,     1] loss: 0.330
[69,     1] loss: 0.355
[70,     1] loss: 0.346
[71,     1] loss: 0.325
[72,     1] loss: 0.458
[73,     1] loss: 0.633
[74,     1] loss: 0.462
[75,     1] loss: 0.408
[76,     1] loss: 0.419
[77,     1] loss: 0.423
[78,     1] loss: 0.406
[79,     1] loss: 0.371
[80,     1] loss: 0.370
[81,     1] loss: 0.373
[82,     1] loss: 0.366
[83,     1] loss: 0.371
[84,     1] loss: 0.371
[85,     1] loss: 0.345
[86,     1] loss: 0.336
[87,     1] loss: 0.372
[88,     1] loss: 0.367
[89,     1] loss: 0.323
[90,     1] loss: 0.339
[91,     1] loss: 0.364
[92,     1] loss: 0.350
[93,     1] loss: 0.407
[94,     1] loss: 0.417
[95,     1] loss: 0.395
[96,     1] loss: 0.386
[97,     1] loss: 0.417
[98,     1] loss: 0.387
[99,     1] loss: 0.383
[100,     1] loss: 0.363
[101,     1] loss: 0.337
[102,     1] loss: 0.359
Early stopping applied (best metric=0.314813494682312)
Finished Training
Total time taken: 11.903015375137329
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.698
[3,     1] loss: 0.691
[4,     1] loss: 0.683
[5,     1] loss: 0.678
[6,     1] loss: 0.665
[7,     1] loss: 0.641
[8,     1] loss: 0.616
[9,     1] loss: 0.597
[10,     1] loss: 0.563
[11,     1] loss: 0.541
[12,     1] loss: 0.532
[13,     1] loss: 0.505
[14,     1] loss: 0.468
[15,     1] loss: 0.494
[16,     1] loss: 0.427
[17,     1] loss: 0.467
[18,     1] loss: 0.499
[19,     1] loss: 0.403
[20,     1] loss: 0.510
[21,     1] loss: 0.467
[22,     1] loss: 0.422
[23,     1] loss: 0.384
[24,     1] loss: 0.410
[25,     1] loss: 0.406
[26,     1] loss: 0.370
[27,     1] loss: 0.336
[28,     1] loss: 0.315
[29,     1] loss: 0.335
[30,     1] loss: 0.341
[31,     1] loss: 0.310
[32,     1] loss: 0.329
[33,     1] loss: 0.314
[34,     1] loss: 0.315
[35,     1] loss: 0.297
[36,     1] loss: 0.312
[37,     1] loss: 0.304
[38,     1] loss: 0.341
[39,     1] loss: 0.763
[40,     1] loss: 0.368
[41,     1] loss: 0.446
[42,     1] loss: 0.384
[43,     1] loss: 0.401
[44,     1] loss: 0.395
[45,     1] loss: 0.353
[46,     1] loss: 0.364
[47,     1] loss: 0.365
[48,     1] loss: 0.371
[49,     1] loss: 0.344
[50,     1] loss: 0.322
[51,     1] loss: 0.353
[52,     1] loss: 0.334
[53,     1] loss: 0.329
[54,     1] loss: 0.286
[55,     1] loss: 0.310
[56,     1] loss: 0.297
[57,     1] loss: 0.344
[58,     1] loss: 0.272
[59,     1] loss: 0.289
[60,     1] loss: 0.295
[61,     1] loss: 0.366
[62,     1] loss: 0.328
[63,     1] loss: 0.354
[64,     1] loss: 0.346
[65,     1] loss: 0.317
[66,     1] loss: 0.293
[67,     1] loss: 0.338
[68,     1] loss: 0.300
[69,     1] loss: 0.330
[70,     1] loss: 0.325
[71,     1] loss: 0.325
[72,     1] loss: 0.336
[73,     1] loss: 0.300
[74,     1] loss: 0.318
[75,     1] loss: 0.357
[76,     1] loss: 0.590
[77,     1] loss: 0.349
[78,     1] loss: 0.352
[79,     1] loss: 0.365
[80,     1] loss: 0.384
[81,     1] loss: 0.356
[82,     1] loss: 0.356
[83,     1] loss: 0.324
[84,     1] loss: 0.318
[85,     1] loss: 0.332
Early stopping applied (best metric=0.3529825806617737)
Finished Training
Total time taken: 11.41517186164856
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.685
[2,     1] loss: 0.715
[3,     1] loss: 0.692
[4,     1] loss: 0.685
[5,     1] loss: 0.675
[6,     1] loss: 0.661
[7,     1] loss: 0.643
[8,     1] loss: 0.637
[9,     1] loss: 0.610
[10,     1] loss: 0.591
[11,     1] loss: 0.556
[12,     1] loss: 0.552
[13,     1] loss: 0.501
[14,     1] loss: 0.548
[15,     1] loss: 0.506
[16,     1] loss: 0.517
[17,     1] loss: 0.515
[18,     1] loss: 0.535
[19,     1] loss: 0.447
[20,     1] loss: 0.450
[21,     1] loss: 0.454
[22,     1] loss: 0.444
[23,     1] loss: 0.424
[24,     1] loss: 0.431
[25,     1] loss: 0.432
[26,     1] loss: 0.417
[27,     1] loss: 0.444
[28,     1] loss: 0.429
[29,     1] loss: 0.406
[30,     1] loss: 0.394
[31,     1] loss: 0.372
[32,     1] loss: 0.384
[33,     1] loss: 0.357
[34,     1] loss: 0.316
[35,     1] loss: 0.336
[36,     1] loss: 0.410
[37,     1] loss: 0.368
[38,     1] loss: 0.376
[39,     1] loss: 0.361
[40,     1] loss: 0.350
[41,     1] loss: 0.372
[42,     1] loss: 0.353
[43,     1] loss: 0.309
[44,     1] loss: 0.349
[45,     1] loss: 0.342
[46,     1] loss: 0.344
[47,     1] loss: 0.335
[48,     1] loss: 0.325
[49,     1] loss: 0.331
[50,     1] loss: 0.322
[51,     1] loss: 0.399
[52,     1] loss: 0.730
[53,     1] loss: 0.482
[54,     1] loss: 0.408
[55,     1] loss: 0.550
[56,     1] loss: 0.457
[57,     1] loss: 0.479
[58,     1] loss: 0.461
[59,     1] loss: 0.434
[60,     1] loss: 0.420
[61,     1] loss: 0.405
[62,     1] loss: 0.412
[63,     1] loss: 0.376
[64,     1] loss: 0.377
[65,     1] loss: 0.379
[66,     1] loss: 0.338
[67,     1] loss: 0.370
[68,     1] loss: 0.344
[69,     1] loss: 0.376
[70,     1] loss: 0.360
[71,     1] loss: 0.317
[72,     1] loss: 0.322
[73,     1] loss: 0.338
[74,     1] loss: 0.324
[75,     1] loss: 0.315
[76,     1] loss: 0.319
[77,     1] loss: 0.460
[78,     1] loss: 0.360
[79,     1] loss: 0.403
[80,     1] loss: 0.401
[81,     1] loss: 0.359
[82,     1] loss: 0.358
[83,     1] loss: 0.350
[84,     1] loss: 0.379
[85,     1] loss: 0.451
[86,     1] loss: 0.478
[87,     1] loss: 0.417
[88,     1] loss: 0.390
[89,     1] loss: 0.409
[90,     1] loss: 0.444
[91,     1] loss: 0.427
[92,     1] loss: 0.379
[93,     1] loss: 0.456
[94,     1] loss: 0.402
[95,     1] loss: 0.362
[96,     1] loss: 0.402
[97,     1] loss: 0.407
[98,     1] loss: 0.381
[99,     1] loss: 0.357
[100,     1] loss: 0.337
[101,     1] loss: 0.324
[102,     1] loss: 0.344
[103,     1] loss: 0.312
[104,     1] loss: 0.348
[105,     1] loss: 0.364
[106,     1] loss: 0.342
[107,     1] loss: 0.334
[108,     1] loss: 0.324
[109,     1] loss: 0.347
[110,     1] loss: 0.345
[111,     1] loss: 0.428
[112,     1] loss: 0.763
[113,     1] loss: 0.659
[114,     1] loss: 0.666
[115,     1] loss: 0.646
[116,     1] loss: 0.628
[117,     1] loss: 0.633
[118,     1] loss: 0.627
[119,     1] loss: 0.569
[120,     1] loss: 0.552
[121,     1] loss: 0.580
[122,     1] loss: 0.519
[123,     1] loss: 0.532
[124,     1] loss: 0.511
[125,     1] loss: 0.456
[126,     1] loss: 0.449
[127,     1] loss: 0.431
[128,     1] loss: 0.403
Early stopping applied (best metric=0.3258143663406372)
Finished Training
Total time taken: 16.316259384155273
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.676
[4,     1] loss: 0.648
[5,     1] loss: 0.602
[6,     1] loss: 0.586
[7,     1] loss: 0.580
[8,     1] loss: 0.535
[9,     1] loss: 0.506
[10,     1] loss: 0.491
[11,     1] loss: 0.465
[12,     1] loss: 0.470
[13,     1] loss: 0.461
[14,     1] loss: 0.469
[15,     1] loss: 0.447
[16,     1] loss: 0.429
[17,     1] loss: 0.408
[18,     1] loss: 0.412
[19,     1] loss: 0.383
[20,     1] loss: 0.451
[21,     1] loss: 0.365
[22,     1] loss: 0.424
[23,     1] loss: 0.386
[24,     1] loss: 0.440
[25,     1] loss: 0.334
[26,     1] loss: 0.365
[27,     1] loss: 0.353
[28,     1] loss: 0.360
[29,     1] loss: 0.356
[30,     1] loss: 0.338
[31,     1] loss: 0.365
[32,     1] loss: 0.312
[33,     1] loss: 0.526
[34,     1] loss: 0.304
[35,     1] loss: 0.408
[36,     1] loss: 0.370
[37,     1] loss: 0.349
[38,     1] loss: 0.356
[39,     1] loss: 0.367
[40,     1] loss: 0.375
[41,     1] loss: 0.315
[42,     1] loss: 0.389
[43,     1] loss: 0.334
[44,     1] loss: 0.347
[45,     1] loss: 0.343
[46,     1] loss: 0.318
[47,     1] loss: 0.325
[48,     1] loss: 0.309
[49,     1] loss: 0.355
[50,     1] loss: 0.538
[51,     1] loss: 0.554
[52,     1] loss: 0.466
[53,     1] loss: 0.415
[54,     1] loss: 0.408
[55,     1] loss: 0.429
[56,     1] loss: 0.438
[57,     1] loss: 0.406
[58,     1] loss: 0.433
[59,     1] loss: 0.413
[60,     1] loss: 0.392
[61,     1] loss: 0.380
[62,     1] loss: 0.393
[63,     1] loss: 0.364
[64,     1] loss: 0.391
[65,     1] loss: 0.378
[66,     1] loss: 0.381
[67,     1] loss: 0.349
[68,     1] loss: 0.371
[69,     1] loss: 0.325
[70,     1] loss: 0.359
[71,     1] loss: 0.333
[72,     1] loss: 0.361
[73,     1] loss: 0.347
[74,     1] loss: 0.360
[75,     1] loss: 0.320
[76,     1] loss: 0.356
[77,     1] loss: 0.306
[78,     1] loss: 0.344
[79,     1] loss: 0.408
[80,     1] loss: 0.408
[81,     1] loss: 0.321
[82,     1] loss: 0.380
[83,     1] loss: 0.376
[84,     1] loss: 0.316
[85,     1] loss: 0.351
[86,     1] loss: 0.336
[87,     1] loss: 0.348
[88,     1] loss: 0.362
[89,     1] loss: 0.305
[90,     1] loss: 0.344
[91,     1] loss: 0.326
[92,     1] loss: 0.364
[93,     1] loss: 0.318
[94,     1] loss: 0.322
[95,     1] loss: 0.401
[96,     1] loss: 0.355
[97,     1] loss: 0.333
[98,     1] loss: 0.350
[99,     1] loss: 0.332
[100,     1] loss: 0.385
[101,     1] loss: 0.432
[102,     1] loss: 0.347
[103,     1] loss: 0.450
[104,     1] loss: 0.358
[105,     1] loss: 0.434
[106,     1] loss: 0.412
[107,     1] loss: 0.355
[108,     1] loss: 0.399
[109,     1] loss: 0.395
[110,     1] loss: 0.325
[111,     1] loss: 0.367
[112,     1] loss: 0.365
[113,     1] loss: 0.386
[114,     1] loss: 0.372
[115,     1] loss: 0.361
[116,     1] loss: 0.323
[117,     1] loss: 0.324
[118,     1] loss: 0.331
[119,     1] loss: 0.327
[120,     1] loss: 0.435
[121,     1] loss: 0.449
[122,     1] loss: 0.418
[123,     1] loss: 0.365
[124,     1] loss: 0.442
[125,     1] loss: 0.409
[126,     1] loss: 0.403
Early stopping applied (best metric=0.3257753252983093)
Finished Training
Total time taken: 17.222437143325806
{'Hydroxylation-K Validation Accuracy': 0.8554078014184398, 'Hydroxylation-K Validation Sensitivity': 0.8711111111111111, 'Hydroxylation-K Validation Specificity': 0.8515789473684211, 'Hydroxylation-K Validation Precision': 0.6092297600098112, 'Hydroxylation-K AUC ROC': 0.8535964912280701, 'Hydroxylation-K AUC PR': 0.6748412154413128, 'Hydroxylation-K MCC': 0.6429140964029402, 'Hydroxylation-K F1': 0.7117984926506665, 'Validation Loss (Hydroxylation-K)': 0.3378549987077713, 'Validation Loss (total)': 0.3378549987077713, 'TimeToTrain': 14.042787055969239}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029097170549755133,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 468540378,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.17584708570887}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.683
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0075763473484711154,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1618181017,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.7757238142484829}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.688
[3,     1] loss: 0.662
[4,     1] loss: 0.614
[5,     1] loss: 0.506
[6,     1] loss: 0.492
[7,     1] loss: 0.462
[8,     1] loss: 0.418
[9,     1] loss: 0.408
[10,     1] loss: 0.384
[11,     1] loss: 0.403
[12,     1] loss: 0.345
[13,     1] loss: 0.287
[14,     1] loss: 0.405
[15,     1] loss: 0.375
[16,     1] loss: 0.307
[17,     1] loss: 0.321
[18,     1] loss: 0.269
[19,     1] loss: 0.296
[20,     1] loss: 0.234
[21,     1] loss: 0.265
[22,     1] loss: 0.224
[23,     1] loss: 0.308
[24,     1] loss: 0.248
[25,     1] loss: 0.268
[26,     1] loss: 0.210
[27,     1] loss: 0.261
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00839028476488111,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1175539183,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.593672458166242}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.697
[3,     1] loss: 0.680
[4,     1] loss: 0.641
[5,     1] loss: 0.588
[6,     1] loss: 0.551
[7,     1] loss: 0.489
[8,     1] loss: 0.495
[9,     1] loss: 0.465
[10,     1] loss: 0.413
[11,     1] loss: 0.389
[12,     1] loss: 0.449
[13,     1] loss: 0.333
[14,     1] loss: 0.347
[15,     1] loss: 0.300
[16,     1] loss: 0.284
[17,     1] loss: 0.286
[18,     1] loss: 0.221
[19,     1] loss: 0.212
[20,     1] loss: 0.267
[21,     1] loss: 0.229
[22,     1] loss: 0.259
[23,     1] loss: 0.184
[24,     1] loss: 0.225
[25,     1] loss: 0.240
[26,     1] loss: 0.208
[27,     1] loss: 0.261
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008605283431450054,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 474129963,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.831515390131211}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.694
[3,     1] loss: 0.682
[4,     1] loss: 0.677
[5,     1] loss: 0.668
[6,     1] loss: 0.668
[7,     1] loss: 0.646
[8,     1] loss: 0.624
[9,     1] loss: 0.622
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004470004256684594,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2786392337,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.460933003350027}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.683
[4,     1] loss: 0.666
[5,     1] loss: 0.638
[6,     1] loss: 0.620
[7,     1] loss: 0.606
[8,     1] loss: 0.571
[9,     1] loss: 0.537
[10,     1] loss: 0.497
[11,     1] loss: 0.497
[12,     1] loss: 0.428
[13,     1] loss: 0.381
[14,     1] loss: 0.396
[15,     1] loss: 0.420
[16,     1] loss: 0.384
[17,     1] loss: 0.378
[18,     1] loss: 0.483
[19,     1] loss: 0.393
[20,     1] loss: 0.406
[21,     1] loss: 0.383
[22,     1] loss: 0.310
[23,     1] loss: 0.392
[24,     1] loss: 0.309
[25,     1] loss: 0.305
[26,     1] loss: 0.307
[27,     1] loss: 0.304
[28,     1] loss: 0.244
[29,     1] loss: 0.274
[30,     1] loss: 0.243
[31,     1] loss: 0.259
[32,     1] loss: 0.234
[33,     1] loss: 0.248
[34,     1] loss: 0.372
[35,     1] loss: 0.274
[36,     1] loss: 0.297
[37,     1] loss: 0.253
[38,     1] loss: 0.288
[39,     1] loss: 0.311
[40,     1] loss: 0.255
[41,     1] loss: 0.232
[42,     1] loss: 0.235
[43,     1] loss: 0.240
[44,     1] loss: 0.223
[45,     1] loss: 0.224
[46,     1] loss: 0.159
[47,     1] loss: 0.208
[48,     1] loss: 0.216
[49,     1] loss: 0.226
[50,     1] loss: 0.215
[51,     1] loss: 0.244
[52,     1] loss: 0.196
[53,     1] loss: 0.218
[54,     1] loss: 0.236
[55,     1] loss: 0.179
[56,     1] loss: 0.300
[57,     1] loss: 0.207
[58,     1] loss: 0.223
[59,     1] loss: 0.180
[60,     1] loss: 0.218
[61,     1] loss: 0.195
[62,     1] loss: 0.170
[63,     1] loss: 0.189
[64,     1] loss: 0.207
[65,     1] loss: 0.248
[66,     1] loss: 0.164
[67,     1] loss: 0.196
[68,     1] loss: 0.296
[69,     1] loss: 0.226
[70,     1] loss: 0.314
[71,     1] loss: 0.260
[72,     1] loss: 0.302
[73,     1] loss: 0.219
[74,     1] loss: 0.241
[75,     1] loss: 0.226
[76,     1] loss: 0.246
[77,     1] loss: 0.236
[78,     1] loss: 0.216
[79,     1] loss: 0.200
[80,     1] loss: 0.212
[81,     1] loss: 0.223
[82,     1] loss: 0.192
[83,     1] loss: 0.201
[84,     1] loss: 0.246
[85,     1] loss: 0.268
[86,     1] loss: 0.261
[87,     1] loss: 0.226
[88,     1] loss: 0.264
[89,     1] loss: 0.228
[90,     1] loss: 0.202
[91,     1] loss: 0.198
[92,     1] loss: 0.220
[93,     1] loss: 0.224
[94,     1] loss: 0.223
[95,     1] loss: 0.200
[96,     1] loss: 0.174
[97,     1] loss: 0.158
[98,     1] loss: 0.204
[99,     1] loss: 0.255
[100,     1] loss: 0.182
[101,     1] loss: 0.224
[102,     1] loss: 0.181
[103,     1] loss: 0.177
[104,     1] loss: 0.178
[105,     1] loss: 0.155
[106,     1] loss: 0.147
[107,     1] loss: 0.131
[108,     1] loss: 0.167
[109,     1] loss: 0.274
[110,     1] loss: 0.402
[111,     1] loss: 0.280
[112,     1] loss: 0.363
[113,     1] loss: 0.319
Early stopping applied (best metric=0.33639979362487793)
Finished Training
Total time taken: 14.727706909179688
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.688
[3,     1] loss: 0.680
[4,     1] loss: 0.654
[5,     1] loss: 0.610
[6,     1] loss: 0.574
[7,     1] loss: 0.550
[8,     1] loss: 0.515
[9,     1] loss: 0.511
[10,     1] loss: 0.460
[11,     1] loss: 0.465
[12,     1] loss: 0.398
[13,     1] loss: 0.394
[14,     1] loss: 0.387
[15,     1] loss: 0.346
[16,     1] loss: 0.391
[17,     1] loss: 0.292
[18,     1] loss: 0.339
[19,     1] loss: 0.319
[20,     1] loss: 0.455
[21,     1] loss: 0.337
[22,     1] loss: 0.366
[23,     1] loss: 0.308
[24,     1] loss: 0.326
[25,     1] loss: 0.346
[26,     1] loss: 0.271
[27,     1] loss: 0.303
[28,     1] loss: 0.273
[29,     1] loss: 0.241
[30,     1] loss: 0.208
[31,     1] loss: 0.216
[32,     1] loss: 0.233
[33,     1] loss: 0.258
[34,     1] loss: 0.226
[35,     1] loss: 0.226
[36,     1] loss: 0.290
[37,     1] loss: 0.226
[38,     1] loss: 0.237
[39,     1] loss: 0.284
[40,     1] loss: 0.298
[41,     1] loss: 0.249
[42,     1] loss: 0.287
[43,     1] loss: 0.216
[44,     1] loss: 0.200
[45,     1] loss: 0.220
[46,     1] loss: 0.181
[47,     1] loss: 0.192
[48,     1] loss: 0.142
[49,     1] loss: 0.145
[50,     1] loss: 0.166
[51,     1] loss: 0.135
[52,     1] loss: 0.159
[53,     1] loss: 0.245
[54,     1] loss: 0.174
[55,     1] loss: 0.167
[56,     1] loss: 0.137
[57,     1] loss: 0.162
[58,     1] loss: 0.156
[59,     1] loss: 0.297
[60,     1] loss: 0.263
[61,     1] loss: 0.223
[62,     1] loss: 0.308
[63,     1] loss: 0.266
[64,     1] loss: 0.240
[65,     1] loss: 0.242
[66,     1] loss: 0.237
[67,     1] loss: 0.229
[68,     1] loss: 0.222
[69,     1] loss: 0.240
[70,     1] loss: 0.223
[71,     1] loss: 0.194
[72,     1] loss: 0.219
[73,     1] loss: 0.205
[74,     1] loss: 0.191
[75,     1] loss: 0.201
[76,     1] loss: 0.204
[77,     1] loss: 0.153
Early stopping applied (best metric=0.3996376395225525)
Finished Training
Total time taken: 10.395132064819336
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.672
[4,     1] loss: 0.645
[5,     1] loss: 0.617
[6,     1] loss: 0.569
[7,     1] loss: 0.531
[8,     1] loss: 0.522
[9,     1] loss: 0.504
[10,     1] loss: 0.446
[11,     1] loss: 0.418
[12,     1] loss: 0.413
[13,     1] loss: 0.374
[14,     1] loss: 0.434
[15,     1] loss: 0.386
[16,     1] loss: 0.399
[17,     1] loss: 0.437
[18,     1] loss: 0.339
[19,     1] loss: 0.368
[20,     1] loss: 0.403
[21,     1] loss: 0.317
[22,     1] loss: 0.346
[23,     1] loss: 0.385
[24,     1] loss: 0.373
[25,     1] loss: 0.313
[26,     1] loss: 0.362
[27,     1] loss: 0.309
[28,     1] loss: 0.375
[29,     1] loss: 0.328
[30,     1] loss: 0.298
[31,     1] loss: 0.311
[32,     1] loss: 0.285
[33,     1] loss: 0.272
[34,     1] loss: 0.283
[35,     1] loss: 0.248
[36,     1] loss: 0.246
[37,     1] loss: 0.237
[38,     1] loss: 0.249
[39,     1] loss: 0.194
[40,     1] loss: 0.221
[41,     1] loss: 0.268
[42,     1] loss: 0.226
[43,     1] loss: 0.347
[44,     1] loss: 0.233
[45,     1] loss: 0.235
[46,     1] loss: 0.298
[47,     1] loss: 0.280
[48,     1] loss: 0.241
[49,     1] loss: 0.284
[50,     1] loss: 0.250
[51,     1] loss: 0.224
[52,     1] loss: 0.195
[53,     1] loss: 0.205
[54,     1] loss: 0.215
[55,     1] loss: 0.197
[56,     1] loss: 0.242
[57,     1] loss: 0.338
[58,     1] loss: 0.184
[59,     1] loss: 0.185
[60,     1] loss: 0.307
[61,     1] loss: 0.263
[62,     1] loss: 0.256
[63,     1] loss: 0.199
[64,     1] loss: 0.222
[65,     1] loss: 0.229
[66,     1] loss: 0.225
[67,     1] loss: 0.171
[68,     1] loss: 0.187
[69,     1] loss: 0.213
[70,     1] loss: 0.175
[71,     1] loss: 0.254
[72,     1] loss: 0.161
[73,     1] loss: 0.180
[74,     1] loss: 0.210
[75,     1] loss: 0.235
[76,     1] loss: 0.235
[77,     1] loss: 0.195
[78,     1] loss: 0.274
[79,     1] loss: 0.228
[80,     1] loss: 0.285
[81,     1] loss: 0.211
[82,     1] loss: 0.255
[83,     1] loss: 0.171
[84,     1] loss: 0.195
[85,     1] loss: 0.236
[86,     1] loss: 0.251
[87,     1] loss: 0.218
[88,     1] loss: 0.206
[89,     1] loss: 0.177
[90,     1] loss: 0.212
[91,     1] loss: 0.200
[92,     1] loss: 0.200
[93,     1] loss: 0.171
[94,     1] loss: 0.202
[95,     1] loss: 0.204
[96,     1] loss: 0.246
[97,     1] loss: 0.174
[98,     1] loss: 0.162
[99,     1] loss: 0.273
[100,     1] loss: 0.206
[101,     1] loss: 0.208
[102,     1] loss: 0.196
[103,     1] loss: 0.203
[104,     1] loss: 0.221
[105,     1] loss: 0.181
[106,     1] loss: 0.205
[107,     1] loss: 0.188
[108,     1] loss: 0.220
[109,     1] loss: 0.158
[110,     1] loss: 0.244
[111,     1] loss: 0.187
[112,     1] loss: 0.153
[113,     1] loss: 0.169
[114,     1] loss: 0.178
[115,     1] loss: 0.152
Early stopping applied (best metric=0.2726866602897644)
Finished Training
Total time taken: 15.913623571395874
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.639
[5,     1] loss: 0.609
[6,     1] loss: 0.562
[7,     1] loss: 0.560
[8,     1] loss: 0.510
[9,     1] loss: 0.473
[10,     1] loss: 0.404
[11,     1] loss: 0.451
[12,     1] loss: 0.447
[13,     1] loss: 0.353
[14,     1] loss: 0.380
[15,     1] loss: 0.378
[16,     1] loss: 0.519
[17,     1] loss: 0.344
[18,     1] loss: 0.374
[19,     1] loss: 0.344
[20,     1] loss: 0.376
[21,     1] loss: 0.344
[22,     1] loss: 0.340
[23,     1] loss: 0.317
[24,     1] loss: 0.368
[25,     1] loss: 0.316
[26,     1] loss: 0.293
[27,     1] loss: 0.249
[28,     1] loss: 0.224
[29,     1] loss: 0.281
[30,     1] loss: 0.211
[31,     1] loss: 0.217
[32,     1] loss: 0.220
[33,     1] loss: 0.208
[34,     1] loss: 0.203
[35,     1] loss: 0.186
[36,     1] loss: 0.234
[37,     1] loss: 0.231
[38,     1] loss: 0.365
[39,     1] loss: 0.246
[40,     1] loss: 0.373
[41,     1] loss: 0.436
[42,     1] loss: 0.370
[43,     1] loss: 0.301
[44,     1] loss: 0.381
[45,     1] loss: 0.347
[46,     1] loss: 0.301
[47,     1] loss: 0.321
[48,     1] loss: 0.328
[49,     1] loss: 0.354
[50,     1] loss: 0.295
[51,     1] loss: 0.297
[52,     1] loss: 0.286
[53,     1] loss: 0.277
[54,     1] loss: 0.295
[55,     1] loss: 0.229
[56,     1] loss: 0.242
[57,     1] loss: 0.216
[58,     1] loss: 0.207
[59,     1] loss: 0.227
[60,     1] loss: 0.197
[61,     1] loss: 0.198
[62,     1] loss: 0.242
[63,     1] loss: 0.191
[64,     1] loss: 0.232
[65,     1] loss: 0.228
[66,     1] loss: 0.196
[67,     1] loss: 0.305
[68,     1] loss: 0.252
[69,     1] loss: 0.205
[70,     1] loss: 0.221
[71,     1] loss: 0.213
[72,     1] loss: 0.200
[73,     1] loss: 0.247
[74,     1] loss: 0.226
[75,     1] loss: 0.299
[76,     1] loss: 0.244
[77,     1] loss: 0.227
[78,     1] loss: 0.218
[79,     1] loss: 0.209
[80,     1] loss: 0.197
[81,     1] loss: 0.225
[82,     1] loss: 0.208
[83,     1] loss: 0.203
Early stopping applied (best metric=0.1824524849653244)
Finished Training
Total time taken: 13.967191457748413
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.678
[4,     1] loss: 0.662
[5,     1] loss: 0.635
[6,     1] loss: 0.600
[7,     1] loss: 0.583
[8,     1] loss: 0.557
[9,     1] loss: 0.507
[10,     1] loss: 0.495
[11,     1] loss: 0.469
[12,     1] loss: 0.438
[13,     1] loss: 0.426
[14,     1] loss: 0.419
[15,     1] loss: 0.409
[16,     1] loss: 0.400
[17,     1] loss: 0.344
[18,     1] loss: 0.342
[19,     1] loss: 0.367
[20,     1] loss: 0.377
[21,     1] loss: 0.374
[22,     1] loss: 0.383
[23,     1] loss: 0.336
[24,     1] loss: 0.366
[25,     1] loss: 0.352
[26,     1] loss: 0.327
[27,     1] loss: 0.308
[28,     1] loss: 0.375
[29,     1] loss: 0.344
[30,     1] loss: 0.312
[31,     1] loss: 0.324
[32,     1] loss: 0.282
[33,     1] loss: 0.264
[34,     1] loss: 0.282
[35,     1] loss: 0.262
[36,     1] loss: 0.250
[37,     1] loss: 0.206
[38,     1] loss: 0.299
[39,     1] loss: 0.342
[40,     1] loss: 0.232
[41,     1] loss: 0.229
[42,     1] loss: 0.252
[43,     1] loss: 0.259
[44,     1] loss: 0.224
[45,     1] loss: 0.223
[46,     1] loss: 0.213
[47,     1] loss: 0.244
[48,     1] loss: 0.203
[49,     1] loss: 0.214
[50,     1] loss: 0.238
[51,     1] loss: 0.213
[52,     1] loss: 0.239
[53,     1] loss: 0.236
[54,     1] loss: 0.260
[55,     1] loss: 0.234
[56,     1] loss: 0.344
[57,     1] loss: 0.224
[58,     1] loss: 0.253
[59,     1] loss: 0.302
[60,     1] loss: 0.188
[61,     1] loss: 0.226
[62,     1] loss: 0.249
[63,     1] loss: 0.249
[64,     1] loss: 0.235
[65,     1] loss: 0.214
[66,     1] loss: 0.281
[67,     1] loss: 0.265
[68,     1] loss: 0.212
[69,     1] loss: 0.169
[70,     1] loss: 0.166
[71,     1] loss: 0.200
[72,     1] loss: 0.199
[73,     1] loss: 0.169
[74,     1] loss: 0.183
[75,     1] loss: 0.161
[76,     1] loss: 0.175
[77,     1] loss: 0.170
[78,     1] loss: 0.181
[79,     1] loss: 0.154
[80,     1] loss: 0.146
[81,     1] loss: 0.174
[82,     1] loss: 0.159
[83,     1] loss: 0.269
[84,     1] loss: 0.224
[85,     1] loss: 0.172
[86,     1] loss: 0.278
[87,     1] loss: 0.162
[88,     1] loss: 0.167
[89,     1] loss: 0.235
[90,     1] loss: 0.215
[91,     1] loss: 0.202
[92,     1] loss: 0.179
[93,     1] loss: 0.281
[94,     1] loss: 0.253
[95,     1] loss: 0.202
[96,     1] loss: 0.281
[97,     1] loss: 0.181
[98,     1] loss: 0.201
[99,     1] loss: 0.183
[100,     1] loss: 0.199
[101,     1] loss: 0.211
[102,     1] loss: 0.201
[103,     1] loss: 0.165
[104,     1] loss: 0.206
Early stopping applied (best metric=0.28835657238960266)
Finished Training
Total time taken: 16.992411851882935
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.678
[4,     1] loss: 0.646
[5,     1] loss: 0.608
[6,     1] loss: 0.595
[7,     1] loss: 0.551
[8,     1] loss: 0.498
[9,     1] loss: 0.500
[10,     1] loss: 0.492
[11,     1] loss: 0.438
[12,     1] loss: 0.435
[13,     1] loss: 0.403
[14,     1] loss: 0.401
[15,     1] loss: 0.328
[16,     1] loss: 0.376
[17,     1] loss: 0.372
[18,     1] loss: 0.300
[19,     1] loss: 0.384
[20,     1] loss: 0.290
[21,     1] loss: 0.309
[22,     1] loss: 0.270
[23,     1] loss: 0.243
[24,     1] loss: 0.270
[25,     1] loss: 0.236
[26,     1] loss: 0.299
[27,     1] loss: 0.270
[28,     1] loss: 0.211
[29,     1] loss: 0.240
[30,     1] loss: 0.289
[31,     1] loss: 0.265
[32,     1] loss: 0.245
[33,     1] loss: 0.225
[34,     1] loss: 0.231
[35,     1] loss: 0.180
[36,     1] loss: 0.210
[37,     1] loss: 0.143
[38,     1] loss: 0.204
[39,     1] loss: 0.231
[40,     1] loss: 0.236
[41,     1] loss: 0.186
[42,     1] loss: 0.167
[43,     1] loss: 0.200
[44,     1] loss: 0.177
[45,     1] loss: 0.193
[46,     1] loss: 0.214
[47,     1] loss: 0.181
[48,     1] loss: 0.186
[49,     1] loss: 0.206
[50,     1] loss: 0.194
[51,     1] loss: 0.160
[52,     1] loss: 0.183
[53,     1] loss: 0.168
[54,     1] loss: 0.176
[55,     1] loss: 0.158
[56,     1] loss: 0.137
[57,     1] loss: 0.175
[58,     1] loss: 0.216
[59,     1] loss: 0.243
[60,     1] loss: 0.288
[61,     1] loss: 0.339
[62,     1] loss: 0.260
[63,     1] loss: 0.271
[64,     1] loss: 0.256
[65,     1] loss: 0.254
[66,     1] loss: 0.196
[67,     1] loss: 0.257
[68,     1] loss: 0.260
[69,     1] loss: 0.226
[70,     1] loss: 0.245
[71,     1] loss: 0.209
[72,     1] loss: 0.196
[73,     1] loss: 0.187
[74,     1] loss: 0.205
[75,     1] loss: 0.205
[76,     1] loss: 0.239
[77,     1] loss: 0.252
[78,     1] loss: 0.246
[79,     1] loss: 0.245
[80,     1] loss: 0.189
[81,     1] loss: 0.184
[82,     1] loss: 0.176
[83,     1] loss: 0.214
[84,     1] loss: 0.181
[85,     1] loss: 0.185
[86,     1] loss: 0.149
[87,     1] loss: 0.144
[88,     1] loss: 0.134
[89,     1] loss: 0.242
[90,     1] loss: 0.345
[91,     1] loss: 0.324
[92,     1] loss: 0.361
[93,     1] loss: 0.358
[94,     1] loss: 0.287
[95,     1] loss: 0.349
[96,     1] loss: 0.285
[97,     1] loss: 0.299
[98,     1] loss: 0.249
[99,     1] loss: 0.290
[100,     1] loss: 0.272
[101,     1] loss: 0.285
[102,     1] loss: 0.282
[103,     1] loss: 0.242
[104,     1] loss: 0.236
[105,     1] loss: 0.237
[106,     1] loss: 0.220
[107,     1] loss: 0.179
[108,     1] loss: 0.205
[109,     1] loss: 0.181
[110,     1] loss: 0.196
[111,     1] loss: 0.169
[112,     1] loss: 0.171
[113,     1] loss: 0.240
[114,     1] loss: 0.190
[115,     1] loss: 0.181
[116,     1] loss: 0.160
[117,     1] loss: 0.162
[118,     1] loss: 0.187
[119,     1] loss: 0.160
[120,     1] loss: 0.144
[121,     1] loss: 0.255
[122,     1] loss: 0.404
[123,     1] loss: 0.258
[124,     1] loss: 0.268
[125,     1] loss: 0.286
[126,     1] loss: 0.244
[127,     1] loss: 0.305
[128,     1] loss: 0.226
[129,     1] loss: 0.259
[130,     1] loss: 0.297
[131,     1] loss: 0.219
[132,     1] loss: 0.235
[133,     1] loss: 0.239
[134,     1] loss: 0.225
[135,     1] loss: 0.219
[136,     1] loss: 0.207
[137,     1] loss: 0.199
[138,     1] loss: 0.160
[139,     1] loss: 0.185
[140,     1] loss: 0.203
[141,     1] loss: 0.161
[142,     1] loss: 0.170
[143,     1] loss: 0.244
[144,     1] loss: 0.300
[145,     1] loss: 0.315
[146,     1] loss: 0.268
[147,     1] loss: 0.238
[148,     1] loss: 0.238
[149,     1] loss: 0.258
[150,     1] loss: 0.248
[151,     1] loss: 0.238
[152,     1] loss: 0.239
[153,     1] loss: 0.220
[154,     1] loss: 0.194
[155,     1] loss: 0.236
[156,     1] loss: 0.197
[157,     1] loss: 0.193
[158,     1] loss: 0.163
[159,     1] loss: 0.188
[160,     1] loss: 0.238
[161,     1] loss: 0.183
[162,     1] loss: 0.250
[163,     1] loss: 0.169
[164,     1] loss: 0.222
[165,     1] loss: 0.286
[166,     1] loss: 0.228
[167,     1] loss: 0.270
[168,     1] loss: 0.301
[169,     1] loss: 0.232
[170,     1] loss: 0.219
[171,     1] loss: 0.234
[172,     1] loss: 0.224
[173,     1] loss: 0.229
[174,     1] loss: 0.193
[175,     1] loss: 0.187
Early stopping applied (best metric=0.2835709750652313)
Finished Training
Total time taken: 28.40600824356079
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.692
[3,     1] loss: 0.687
[4,     1] loss: 0.679
[5,     1] loss: 0.659
[6,     1] loss: 0.636
[7,     1] loss: 0.603
[8,     1] loss: 0.601
[9,     1] loss: 0.554
[10,     1] loss: 0.526
[11,     1] loss: 0.503
[12,     1] loss: 0.517
[13,     1] loss: 0.455
[14,     1] loss: 0.462
[15,     1] loss: 0.455
[16,     1] loss: 0.375
[17,     1] loss: 0.421
[18,     1] loss: 0.373
[19,     1] loss: 0.426
[20,     1] loss: 0.460
[21,     1] loss: 0.377
[22,     1] loss: 0.434
[23,     1] loss: 0.352
[24,     1] loss: 0.370
[25,     1] loss: 0.358
[26,     1] loss: 0.378
[27,     1] loss: 0.371
[28,     1] loss: 0.295
[29,     1] loss: 0.339
[30,     1] loss: 0.336
[31,     1] loss: 0.307
[32,     1] loss: 0.300
[33,     1] loss: 0.309
[34,     1] loss: 0.294
[35,     1] loss: 0.335
[36,     1] loss: 0.395
[37,     1] loss: 0.342
[38,     1] loss: 0.324
[39,     1] loss: 0.335
[40,     1] loss: 0.375
[41,     1] loss: 0.364
[42,     1] loss: 0.373
[43,     1] loss: 0.318
[44,     1] loss: 0.372
[45,     1] loss: 0.339
[46,     1] loss: 0.304
[47,     1] loss: 0.320
[48,     1] loss: 0.296
[49,     1] loss: 0.264
[50,     1] loss: 0.274
[51,     1] loss: 0.253
[52,     1] loss: 0.309
[53,     1] loss: 0.291
[54,     1] loss: 0.250
[55,     1] loss: 0.296
[56,     1] loss: 0.236
[57,     1] loss: 0.282
[58,     1] loss: 0.289
[59,     1] loss: 0.296
[60,     1] loss: 0.254
[61,     1] loss: 0.209
[62,     1] loss: 0.242
[63,     1] loss: 0.257
[64,     1] loss: 0.213
[65,     1] loss: 0.248
[66,     1] loss: 0.231
[67,     1] loss: 0.238
[68,     1] loss: 0.221
[69,     1] loss: 0.259
[70,     1] loss: 0.203
[71,     1] loss: 0.258
[72,     1] loss: 0.235
[73,     1] loss: 0.265
[74,     1] loss: 0.282
[75,     1] loss: 0.290
[76,     1] loss: 0.290
[77,     1] loss: 0.331
[78,     1] loss: 0.337
[79,     1] loss: 0.299
[80,     1] loss: 0.325
[81,     1] loss: 0.290
[82,     1] loss: 0.337
[83,     1] loss: 0.274
[84,     1] loss: 0.284
[85,     1] loss: 0.264
[86,     1] loss: 0.272
[87,     1] loss: 0.258
[88,     1] loss: 0.229
Early stopping applied (best metric=0.4487139880657196)
Finished Training
Total time taken: 14.431586027145386
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.693
[3,     1] loss: 0.680
[4,     1] loss: 0.662
[5,     1] loss: 0.631
[6,     1] loss: 0.594
[7,     1] loss: 0.579
[8,     1] loss: 0.563
[9,     1] loss: 0.528
[10,     1] loss: 0.471
[11,     1] loss: 0.456
[12,     1] loss: 0.454
[13,     1] loss: 0.406
[14,     1] loss: 0.379
[15,     1] loss: 0.388
[16,     1] loss: 0.319
[17,     1] loss: 0.372
[18,     1] loss: 0.330
[19,     1] loss: 0.319
[20,     1] loss: 0.316
[21,     1] loss: 0.255
[22,     1] loss: 0.261
[23,     1] loss: 0.233
[24,     1] loss: 0.259
[25,     1] loss: 0.283
[26,     1] loss: 0.225
[27,     1] loss: 0.257
[28,     1] loss: 0.313
[29,     1] loss: 0.278
[30,     1] loss: 0.224
[31,     1] loss: 0.206
[32,     1] loss: 0.217
[33,     1] loss: 0.219
[34,     1] loss: 0.186
[35,     1] loss: 0.297
[36,     1] loss: 0.258
[37,     1] loss: 0.220
[38,     1] loss: 0.242
[39,     1] loss: 0.234
[40,     1] loss: 0.206
[41,     1] loss: 0.221
[42,     1] loss: 0.301
[43,     1] loss: 0.219
[44,     1] loss: 0.277
[45,     1] loss: 0.305
[46,     1] loss: 0.285
[47,     1] loss: 0.300
[48,     1] loss: 0.278
[49,     1] loss: 0.275
[50,     1] loss: 0.257
[51,     1] loss: 0.262
[52,     1] loss: 0.258
[53,     1] loss: 0.253
[54,     1] loss: 0.260
[55,     1] loss: 0.234
[56,     1] loss: 0.274
[57,     1] loss: 0.257
[58,     1] loss: 0.268
[59,     1] loss: 0.225
[60,     1] loss: 0.263
[61,     1] loss: 0.233
[62,     1] loss: 0.275
[63,     1] loss: 0.284
[64,     1] loss: 0.253
[65,     1] loss: 0.248
[66,     1] loss: 0.275
[67,     1] loss: 0.265
[68,     1] loss: 0.261
[69,     1] loss: 0.281
[70,     1] loss: 0.265
[71,     1] loss: 0.240
[72,     1] loss: 0.258
[73,     1] loss: 0.216
[74,     1] loss: 0.217
[75,     1] loss: 0.233
[76,     1] loss: 0.230
[77,     1] loss: 0.198
[78,     1] loss: 0.226
[79,     1] loss: 0.227
[80,     1] loss: 0.208
[81,     1] loss: 0.184
[82,     1] loss: 0.217
[83,     1] loss: 0.199
[84,     1] loss: 0.217
[85,     1] loss: 0.208
[86,     1] loss: 0.253
[87,     1] loss: 0.449
[88,     1] loss: 0.338
[89,     1] loss: 0.328
[90,     1] loss: 0.282
[91,     1] loss: 0.273
[92,     1] loss: 0.284
[93,     1] loss: 0.259
[94,     1] loss: 0.292
[95,     1] loss: 0.242
[96,     1] loss: 0.294
[97,     1] loss: 0.288
[98,     1] loss: 0.256
[99,     1] loss: 0.309
[100,     1] loss: 0.296
[101,     1] loss: 0.237
[102,     1] loss: 0.270
[103,     1] loss: 0.254
[104,     1] loss: 0.221
[105,     1] loss: 0.264
[106,     1] loss: 0.226
[107,     1] loss: 0.287
[108,     1] loss: 0.266
[109,     1] loss: 0.233
[110,     1] loss: 0.278
[111,     1] loss: 0.230
[112,     1] loss: 0.210
[113,     1] loss: 0.256
Early stopping applied (best metric=0.38326817750930786)
Finished Training
Total time taken: 19.24752426147461
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.695
[3,     1] loss: 0.685
[4,     1] loss: 0.651
[5,     1] loss: 0.624
[6,     1] loss: 0.594
[7,     1] loss: 0.565
[8,     1] loss: 0.535
[9,     1] loss: 0.499
[10,     1] loss: 0.489
[11,     1] loss: 0.454
[12,     1] loss: 0.412
[13,     1] loss: 0.371
[14,     1] loss: 0.369
[15,     1] loss: 0.412
[16,     1] loss: 0.371
[17,     1] loss: 0.374
[18,     1] loss: 0.329
[19,     1] loss: 0.304
[20,     1] loss: 0.330
[21,     1] loss: 0.264
[22,     1] loss: 0.244
[23,     1] loss: 0.325
[24,     1] loss: 0.286
[25,     1] loss: 0.302
[26,     1] loss: 0.221
[27,     1] loss: 0.391
[28,     1] loss: 0.309
[29,     1] loss: 0.391
[30,     1] loss: 0.368
[31,     1] loss: 0.361
[32,     1] loss: 0.328
[33,     1] loss: 0.304
[34,     1] loss: 0.341
[35,     1] loss: 0.365
[36,     1] loss: 0.322
[37,     1] loss: 0.303
[38,     1] loss: 0.301
[39,     1] loss: 0.287
[40,     1] loss: 0.266
[41,     1] loss: 0.295
[42,     1] loss: 0.261
[43,     1] loss: 0.252
[44,     1] loss: 0.247
[45,     1] loss: 0.283
[46,     1] loss: 0.226
[47,     1] loss: 0.308
[48,     1] loss: 0.274
[49,     1] loss: 0.271
[50,     1] loss: 0.265
[51,     1] loss: 0.269
[52,     1] loss: 0.231
[53,     1] loss: 0.253
[54,     1] loss: 0.279
[55,     1] loss: 0.275
[56,     1] loss: 0.252
[57,     1] loss: 0.265
[58,     1] loss: 0.268
[59,     1] loss: 0.246
[60,     1] loss: 0.200
[61,     1] loss: 0.237
[62,     1] loss: 0.256
[63,     1] loss: 0.206
[64,     1] loss: 0.195
[65,     1] loss: 0.407
[66,     1] loss: 0.404
[67,     1] loss: 0.264
[68,     1] loss: 0.313
[69,     1] loss: 0.419
[70,     1] loss: 0.285
[71,     1] loss: 0.305
[72,     1] loss: 0.324
[73,     1] loss: 0.284
[74,     1] loss: 0.299
[75,     1] loss: 0.345
[76,     1] loss: 0.265
[77,     1] loss: 0.275
[78,     1] loss: 0.292
[79,     1] loss: 0.273
[80,     1] loss: 0.263
[81,     1] loss: 0.252
[82,     1] loss: 0.236
[83,     1] loss: 0.222
[84,     1] loss: 0.205
[85,     1] loss: 0.241
[86,     1] loss: 0.207
[87,     1] loss: 0.258
[88,     1] loss: 0.288
[89,     1] loss: 0.334
[90,     1] loss: 0.421
[91,     1] loss: 0.285
[92,     1] loss: 0.298
[93,     1] loss: 0.347
[94,     1] loss: 0.285
[95,     1] loss: 0.344
[96,     1] loss: 0.305
[97,     1] loss: 0.277
[98,     1] loss: 0.259
[99,     1] loss: 0.322
[100,     1] loss: 0.311
[101,     1] loss: 0.273
[102,     1] loss: 0.292
[103,     1] loss: 0.249
[104,     1] loss: 0.254
[105,     1] loss: 0.238
[106,     1] loss: 0.205
[107,     1] loss: 0.229
[108,     1] loss: 0.225
[109,     1] loss: 0.248
[110,     1] loss: 0.195
[111,     1] loss: 0.238
[112,     1] loss: 0.205
[113,     1] loss: 0.212
[114,     1] loss: 0.185
[115,     1] loss: 0.260
[116,     1] loss: 0.215
[117,     1] loss: 0.257
[118,     1] loss: 0.190
[119,     1] loss: 0.196
[120,     1] loss: 0.165
[121,     1] loss: 0.190
[122,     1] loss: 0.186
[123,     1] loss: 0.170
[124,     1] loss: 0.247
[125,     1] loss: 0.198
[126,     1] loss: 0.210
[127,     1] loss: 0.193
[128,     1] loss: 0.206
[129,     1] loss: 0.169
[130,     1] loss: 0.231
[131,     1] loss: 0.243
[132,     1] loss: 0.189
[133,     1] loss: 0.189
[134,     1] loss: 0.172
[135,     1] loss: 0.220
[136,     1] loss: 0.166
[137,     1] loss: 0.215
[138,     1] loss: 0.149
[139,     1] loss: 0.239
[140,     1] loss: 0.217
[141,     1] loss: 0.261
[142,     1] loss: 0.196
[143,     1] loss: 0.156
[144,     1] loss: 0.185
[145,     1] loss: 0.166
[146,     1] loss: 0.176
[147,     1] loss: 0.174
[148,     1] loss: 0.157
[149,     1] loss: 0.215
[150,     1] loss: 0.156
[151,     1] loss: 0.217
[152,     1] loss: 0.159
[153,     1] loss: 0.275
[154,     1] loss: 0.227
[155,     1] loss: 0.225
[156,     1] loss: 0.190
[157,     1] loss: 0.190
[158,     1] loss: 0.179
[159,     1] loss: 0.183
[160,     1] loss: 0.201
[161,     1] loss: 0.176
[162,     1] loss: 0.199
[163,     1] loss: 0.154
[164,     1] loss: 0.217
[165,     1] loss: 0.192
[166,     1] loss: 0.153
[167,     1] loss: 0.182
[168,     1] loss: 0.220
[169,     1] loss: 0.206
[170,     1] loss: 0.222
[171,     1] loss: 0.251
[172,     1] loss: 0.198
[173,     1] loss: 0.258
[174,     1] loss: 0.238
[175,     1] loss: 0.199
[176,     1] loss: 0.200
[177,     1] loss: 0.233
[178,     1] loss: 0.214
[179,     1] loss: 0.215
[180,     1] loss: 0.218
[181,     1] loss: 0.161
[182,     1] loss: 0.164
[183,     1] loss: 0.181
[184,     1] loss: 0.198
[185,     1] loss: 0.177
[186,     1] loss: 0.188
[187,     1] loss: 0.150
[188,     1] loss: 0.316
[189,     1] loss: 0.462
[190,     1] loss: 0.325
[191,     1] loss: 0.411
[192,     1] loss: 0.327
[193,     1] loss: 0.337
[194,     1] loss: 0.324
[195,     1] loss: 0.360
[196,     1] loss: 0.276
[197,     1] loss: 0.274
[198,     1] loss: 0.308
Early stopping applied (best metric=0.18159699440002441)
Finished Training
Total time taken: 32.83417463302612
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.687
[3,     1] loss: 0.689
[4,     1] loss: 0.671
[5,     1] loss: 0.646
[6,     1] loss: 0.615
[7,     1] loss: 0.592
[8,     1] loss: 0.563
[9,     1] loss: 0.530
[10,     1] loss: 0.492
[11,     1] loss: 0.463
[12,     1] loss: 0.420
[13,     1] loss: 0.393
[14,     1] loss: 0.384
[15,     1] loss: 0.367
[16,     1] loss: 0.351
[17,     1] loss: 0.366
[18,     1] loss: 0.297
[19,     1] loss: 0.325
[20,     1] loss: 0.272
[21,     1] loss: 0.294
[22,     1] loss: 0.256
[23,     1] loss: 0.262
[24,     1] loss: 0.290
[25,     1] loss: 0.288
[26,     1] loss: 0.246
[27,     1] loss: 0.248
[28,     1] loss: 0.246
[29,     1] loss: 0.257
[30,     1] loss: 0.186
[31,     1] loss: 0.181
[32,     1] loss: 0.189
[33,     1] loss: 0.189
[34,     1] loss: 0.158
[35,     1] loss: 0.150
[36,     1] loss: 0.162
[37,     1] loss: 0.176
[38,     1] loss: 0.226
[39,     1] loss: 0.313
[40,     1] loss: 0.254
[41,     1] loss: 0.276
[42,     1] loss: 0.259
[43,     1] loss: 0.229
[44,     1] loss: 0.221
[45,     1] loss: 0.195
[46,     1] loss: 0.222
[47,     1] loss: 0.219
[48,     1] loss: 0.179
[49,     1] loss: 0.229
[50,     1] loss: 0.199
[51,     1] loss: 0.196
[52,     1] loss: 0.179
[53,     1] loss: 0.211
[54,     1] loss: 0.214
[55,     1] loss: 0.179
[56,     1] loss: 0.168
[57,     1] loss: 0.200
[58,     1] loss: 0.232
[59,     1] loss: 0.236
[60,     1] loss: 0.293
[61,     1] loss: 0.215
[62,     1] loss: 0.182
Early stopping applied (best metric=0.3449787199497223)
Finished Training
Total time taken: 10.16846776008606
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.669
[4,     1] loss: 0.630
[5,     1] loss: 0.576
[6,     1] loss: 0.583
[7,     1] loss: 0.520
[8,     1] loss: 0.504
[9,     1] loss: 0.468
[10,     1] loss: 0.438
[11,     1] loss: 0.385
[12,     1] loss: 0.409
[13,     1] loss: 0.424
[14,     1] loss: 0.374
[15,     1] loss: 0.372
[16,     1] loss: 0.362
[17,     1] loss: 0.344
[18,     1] loss: 0.305
[19,     1] loss: 0.307
[20,     1] loss: 0.271
[21,     1] loss: 0.287
[22,     1] loss: 0.289
[23,     1] loss: 0.277
[24,     1] loss: 0.285
[25,     1] loss: 0.228
[26,     1] loss: 0.231
[27,     1] loss: 0.264
[28,     1] loss: 0.197
[29,     1] loss: 0.275
[30,     1] loss: 0.224
[31,     1] loss: 0.244
[32,     1] loss: 0.255
[33,     1] loss: 0.284
[34,     1] loss: 0.293
[35,     1] loss: 0.230
[36,     1] loss: 0.236
[37,     1] loss: 0.216
[38,     1] loss: 0.223
[39,     1] loss: 0.245
[40,     1] loss: 0.226
[41,     1] loss: 0.182
[42,     1] loss: 0.238
[43,     1] loss: 0.174
[44,     1] loss: 0.330
[45,     1] loss: 0.392
[46,     1] loss: 0.348
[47,     1] loss: 0.294
[48,     1] loss: 0.337
[49,     1] loss: 0.316
[50,     1] loss: 0.217
[51,     1] loss: 0.257
[52,     1] loss: 0.267
[53,     1] loss: 0.233
[54,     1] loss: 0.256
[55,     1] loss: 0.235
[56,     1] loss: 0.238
[57,     1] loss: 0.203
[58,     1] loss: 0.217
[59,     1] loss: 0.211
[60,     1] loss: 0.231
[61,     1] loss: 0.252
[62,     1] loss: 0.169
[63,     1] loss: 0.206
[64,     1] loss: 0.205
[65,     1] loss: 0.183
[66,     1] loss: 0.168
[67,     1] loss: 0.182
[68,     1] loss: 0.264
[69,     1] loss: 0.175
[70,     1] loss: 0.168
[71,     1] loss: 0.176
[72,     1] loss: 0.223
[73,     1] loss: 0.210
[74,     1] loss: 0.175
[75,     1] loss: 0.265
[76,     1] loss: 0.202
[77,     1] loss: 0.226
[78,     1] loss: 0.226
[79,     1] loss: 0.184
[80,     1] loss: 0.159
[81,     1] loss: 0.238
[82,     1] loss: 0.200
[83,     1] loss: 0.262
[84,     1] loss: 0.189
[85,     1] loss: 0.248
[86,     1] loss: 0.176
[87,     1] loss: 0.206
[88,     1] loss: 0.233
[89,     1] loss: 0.205
[90,     1] loss: 0.181
[91,     1] loss: 0.191
[92,     1] loss: 0.194
[93,     1] loss: 0.211
[94,     1] loss: 0.179
[95,     1] loss: 0.204
[96,     1] loss: 0.179
[97,     1] loss: 0.176
[98,     1] loss: 0.156
[99,     1] loss: 0.227
[100,     1] loss: 0.161
[101,     1] loss: 0.256
[102,     1] loss: 0.216
[103,     1] loss: 0.180
[104,     1] loss: 0.180
[105,     1] loss: 0.183
[106,     1] loss: 0.185
[107,     1] loss: 0.144
[108,     1] loss: 0.142
[109,     1] loss: 0.142
[110,     1] loss: 0.179
[111,     1] loss: 0.147
[112,     1] loss: 0.165
[113,     1] loss: 0.139
[114,     1] loss: 0.125
[115,     1] loss: 0.134
[116,     1] loss: 0.124
[117,     1] loss: 0.161
[118,     1] loss: 0.147
[119,     1] loss: 0.112
[120,     1] loss: 0.121
[121,     1] loss: 0.144
[122,     1] loss: 0.258
[123,     1] loss: 0.161
[124,     1] loss: 0.190
[125,     1] loss: 0.163
[126,     1] loss: 0.170
[127,     1] loss: 0.175
[128,     1] loss: 0.152
[129,     1] loss: 0.161
[130,     1] loss: 0.175
[131,     1] loss: 0.142
[132,     1] loss: 0.141
[133,     1] loss: 0.156
[134,     1] loss: 0.161
[135,     1] loss: 0.175
[136,     1] loss: 0.159
[137,     1] loss: 0.162
[138,     1] loss: 0.160
[139,     1] loss: 0.169
[140,     1] loss: 0.125
[141,     1] loss: 0.199
[142,     1] loss: 0.141
[143,     1] loss: 0.148
[144,     1] loss: 0.276
[145,     1] loss: 0.466
[146,     1] loss: 0.275
[147,     1] loss: 0.501
[148,     1] loss: 0.305
[149,     1] loss: 0.245
[150,     1] loss: 0.321
[151,     1] loss: 0.331
[152,     1] loss: 0.295
[153,     1] loss: 0.289
[154,     1] loss: 0.278
[155,     1] loss: 0.274
[156,     1] loss: 0.257
[157,     1] loss: 0.228
[158,     1] loss: 0.241
[159,     1] loss: 0.219
[160,     1] loss: 0.237
[161,     1] loss: 0.197
[162,     1] loss: 0.195
[163,     1] loss: 0.208
[164,     1] loss: 0.177
[165,     1] loss: 0.189
[166,     1] loss: 0.170
[167,     1] loss: 0.192
[168,     1] loss: 0.193
[169,     1] loss: 0.193
[170,     1] loss: 0.172
[171,     1] loss: 0.155
[172,     1] loss: 0.185
[173,     1] loss: 0.197
[174,     1] loss: 0.173
[175,     1] loss: 0.168
[176,     1] loss: 0.177
[177,     1] loss: 0.202
[178,     1] loss: 0.203
[179,     1] loss: 0.194
[180,     1] loss: 0.214
[181,     1] loss: 0.179
[182,     1] loss: 0.172
[183,     1] loss: 0.173
[184,     1] loss: 0.153
[185,     1] loss: 0.177
[186,     1] loss: 0.166
[187,     1] loss: 0.162
[188,     1] loss: 0.176
[189,     1] loss: 0.150
[190,     1] loss: 0.153
[191,     1] loss: 0.160
[192,     1] loss: 0.143
Early stopping applied (best metric=0.2840730547904968)
Finished Training
Total time taken: 32.009994983673096
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.682
[4,     1] loss: 0.657
[5,     1] loss: 0.637
[6,     1] loss: 0.607
[7,     1] loss: 0.568
[8,     1] loss: 0.551
[9,     1] loss: 0.538
[10,     1] loss: 0.478
[11,     1] loss: 0.463
[12,     1] loss: 0.430
[13,     1] loss: 0.414
[14,     1] loss: 0.374
[15,     1] loss: 0.363
[16,     1] loss: 0.410
[17,     1] loss: 0.388
[18,     1] loss: 0.374
[19,     1] loss: 0.382
[20,     1] loss: 0.391
[21,     1] loss: 0.346
[22,     1] loss: 0.317
[23,     1] loss: 0.323
[24,     1] loss: 0.345
[25,     1] loss: 0.319
[26,     1] loss: 0.258
[27,     1] loss: 0.273
[28,     1] loss: 0.267
[29,     1] loss: 0.309
[30,     1] loss: 0.328
[31,     1] loss: 0.270
[32,     1] loss: 0.290
[33,     1] loss: 0.257
[34,     1] loss: 0.279
[35,     1] loss: 0.267
[36,     1] loss: 0.288
[37,     1] loss: 0.236
[38,     1] loss: 0.260
[39,     1] loss: 0.244
[40,     1] loss: 0.233
[41,     1] loss: 0.220
[42,     1] loss: 0.181
[43,     1] loss: 0.191
[44,     1] loss: 0.227
[45,     1] loss: 0.189
[46,     1] loss: 0.209
[47,     1] loss: 0.180
[48,     1] loss: 0.203
[49,     1] loss: 0.193
[50,     1] loss: 0.225
[51,     1] loss: 0.165
[52,     1] loss: 0.190
[53,     1] loss: 0.216
[54,     1] loss: 0.235
[55,     1] loss: 0.181
[56,     1] loss: 0.145
[57,     1] loss: 0.186
[58,     1] loss: 0.180
[59,     1] loss: 0.207
[60,     1] loss: 0.177
[61,     1] loss: 0.189
[62,     1] loss: 0.207
[63,     1] loss: 0.134
[64,     1] loss: 0.180
[65,     1] loss: 0.176
[66,     1] loss: 0.166
[67,     1] loss: 0.186
[68,     1] loss: 0.153
[69,     1] loss: 0.167
[70,     1] loss: 0.165
[71,     1] loss: 0.128
Early stopping applied (best metric=0.46210145950317383)
Finished Training
Total time taken: 11.940533876419067
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.673
[4,     1] loss: 0.632
[5,     1] loss: 0.598
[6,     1] loss: 0.541
[7,     1] loss: 0.501
[8,     1] loss: 0.459
[9,     1] loss: 0.447
[10,     1] loss: 0.411
[11,     1] loss: 0.407
[12,     1] loss: 0.369
[13,     1] loss: 0.335
[14,     1] loss: 0.360
[15,     1] loss: 0.369
[16,     1] loss: 0.357
[17,     1] loss: 0.319
[18,     1] loss: 0.331
[19,     1] loss: 0.297
[20,     1] loss: 0.315
[21,     1] loss: 0.295
[22,     1] loss: 0.361
[23,     1] loss: 0.309
[24,     1] loss: 0.355
[25,     1] loss: 0.325
[26,     1] loss: 0.308
[27,     1] loss: 0.265
[28,     1] loss: 0.267
[29,     1] loss: 0.280
[30,     1] loss: 0.256
[31,     1] loss: 0.265
[32,     1] loss: 0.270
[33,     1] loss: 0.231
[34,     1] loss: 0.254
[35,     1] loss: 0.213
[36,     1] loss: 0.283
[37,     1] loss: 0.272
[38,     1] loss: 0.358
[39,     1] loss: 0.317
[40,     1] loss: 0.316
[41,     1] loss: 0.256
[42,     1] loss: 0.283
[43,     1] loss: 0.308
[44,     1] loss: 0.284
[45,     1] loss: 0.313
[46,     1] loss: 0.293
[47,     1] loss: 0.274
[48,     1] loss: 0.302
[49,     1] loss: 0.295
[50,     1] loss: 0.257
[51,     1] loss: 0.279
[52,     1] loss: 0.287
[53,     1] loss: 0.251
[54,     1] loss: 0.268
[55,     1] loss: 0.221
[56,     1] loss: 0.214
[57,     1] loss: 0.314
[58,     1] loss: 0.199
[59,     1] loss: 0.362
[60,     1] loss: 0.242
[61,     1] loss: 0.321
[62,     1] loss: 0.189
[63,     1] loss: 0.266
[64,     1] loss: 0.323
[65,     1] loss: 0.257
[66,     1] loss: 0.269
[67,     1] loss: 0.331
[68,     1] loss: 0.352
[69,     1] loss: 0.227
[70,     1] loss: 0.275
[71,     1] loss: 0.274
[72,     1] loss: 0.271
[73,     1] loss: 0.264
[74,     1] loss: 0.288
[75,     1] loss: 0.282
[76,     1] loss: 0.245
[77,     1] loss: 0.277
[78,     1] loss: 0.250
[79,     1] loss: 0.254
[80,     1] loss: 0.303
[81,     1] loss: 0.305
[82,     1] loss: 0.321
[83,     1] loss: 0.268
[84,     1] loss: 0.277
[85,     1] loss: 0.355
[86,     1] loss: 0.270
[87,     1] loss: 0.282
[88,     1] loss: 0.296
[89,     1] loss: 0.285
[90,     1] loss: 0.277
[91,     1] loss: 0.268
[92,     1] loss: 0.289
[93,     1] loss: 0.295
[94,     1] loss: 0.261
[95,     1] loss: 0.234
[96,     1] loss: 0.255
[97,     1] loss: 0.256
[98,     1] loss: 0.294
[99,     1] loss: 0.284
[100,     1] loss: 0.251
[101,     1] loss: 0.257
[102,     1] loss: 0.270
Early stopping applied (best metric=0.3406680226325989)
Finished Training
Total time taken: 17.41954469680786
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.689
[3,     1] loss: 0.673
[4,     1] loss: 0.653
[5,     1] loss: 0.644
[6,     1] loss: 0.612
[7,     1] loss: 0.596
[8,     1] loss: 0.557
[9,     1] loss: 0.534
[10,     1] loss: 0.518
[11,     1] loss: 0.505
[12,     1] loss: 0.423
[13,     1] loss: 0.469
[14,     1] loss: 0.474
[15,     1] loss: 0.365
[16,     1] loss: 0.381
[17,     1] loss: 0.385
[18,     1] loss: 0.356
[19,     1] loss: 0.343
[20,     1] loss: 0.354
[21,     1] loss: 0.308
[22,     1] loss: 0.300
[23,     1] loss: 0.336
[24,     1] loss: 0.312
[25,     1] loss: 0.260
[26,     1] loss: 0.281
[27,     1] loss: 0.374
[28,     1] loss: 0.296
[29,     1] loss: 0.286
[30,     1] loss: 0.309
[31,     1] loss: 0.303
[32,     1] loss: 0.290
[33,     1] loss: 0.288
[34,     1] loss: 0.269
[35,     1] loss: 0.262
[36,     1] loss: 0.311
[37,     1] loss: 0.339
[38,     1] loss: 0.238
[39,     1] loss: 0.263
[40,     1] loss: 0.220
[41,     1] loss: 0.203
[42,     1] loss: 0.198
[43,     1] loss: 0.197
[44,     1] loss: 0.177
[45,     1] loss: 0.140
[46,     1] loss: 0.210
[47,     1] loss: 0.358
[48,     1] loss: 0.428
[49,     1] loss: 0.241
[50,     1] loss: 0.278
[51,     1] loss: 0.265
[52,     1] loss: 0.232
[53,     1] loss: 0.232
[54,     1] loss: 0.293
[55,     1] loss: 0.213
[56,     1] loss: 0.232
[57,     1] loss: 0.233
[58,     1] loss: 0.223
[59,     1] loss: 0.187
[60,     1] loss: 0.197
[61,     1] loss: 0.197
[62,     1] loss: 0.170
Early stopping applied (best metric=0.4182572662830353)
Finished Training
Total time taken: 10.556403636932373
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.694
[3,     1] loss: 0.681
[4,     1] loss: 0.666
[5,     1] loss: 0.643
[6,     1] loss: 0.615
[7,     1] loss: 0.595
[8,     1] loss: 0.573
[9,     1] loss: 0.556
[10,     1] loss: 0.513
[11,     1] loss: 0.486
[12,     1] loss: 0.489
[13,     1] loss: 0.424
[14,     1] loss: 0.426
[15,     1] loss: 0.372
[16,     1] loss: 0.358
[17,     1] loss: 0.337
[18,     1] loss: 0.312
[19,     1] loss: 0.290
[20,     1] loss: 0.321
[21,     1] loss: 0.333
[22,     1] loss: 0.341
[23,     1] loss: 0.321
[24,     1] loss: 0.286
[25,     1] loss: 0.297
[26,     1] loss: 0.307
[27,     1] loss: 0.327
[28,     1] loss: 0.335
[29,     1] loss: 0.309
[30,     1] loss: 0.320
[31,     1] loss: 0.298
[32,     1] loss: 0.303
[33,     1] loss: 0.305
[34,     1] loss: 0.310
[35,     1] loss: 0.303
[36,     1] loss: 0.296
[37,     1] loss: 0.310
[38,     1] loss: 0.236
[39,     1] loss: 0.268
[40,     1] loss: 0.268
[41,     1] loss: 0.285
[42,     1] loss: 0.310
[43,     1] loss: 0.275
[44,     1] loss: 0.267
[45,     1] loss: 0.259
[46,     1] loss: 0.239
[47,     1] loss: 0.246
[48,     1] loss: 0.271
[49,     1] loss: 0.265
[50,     1] loss: 0.268
[51,     1] loss: 0.312
[52,     1] loss: 0.255
[53,     1] loss: 0.250
[54,     1] loss: 0.239
[55,     1] loss: 0.218
[56,     1] loss: 0.237
[57,     1] loss: 0.233
[58,     1] loss: 0.254
[59,     1] loss: 0.230
Early stopping applied (best metric=0.48300185799598694)
Finished Training
Total time taken: 9.950630903244019
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.679
[4,     1] loss: 0.641
[5,     1] loss: 0.601
[6,     1] loss: 0.595
[7,     1] loss: 0.546
[8,     1] loss: 0.497
[9,     1] loss: 0.460
[10,     1] loss: 0.428
[11,     1] loss: 0.405
[12,     1] loss: 0.425
[13,     1] loss: 0.373
[14,     1] loss: 0.445
[15,     1] loss: 0.395
[16,     1] loss: 0.397
[17,     1] loss: 0.331
[18,     1] loss: 0.326
[19,     1] loss: 0.409
[20,     1] loss: 0.326
[21,     1] loss: 0.343
[22,     1] loss: 0.377
[23,     1] loss: 0.382
[24,     1] loss: 0.331
[25,     1] loss: 0.464
[26,     1] loss: 0.390
[27,     1] loss: 0.351
[28,     1] loss: 0.343
[29,     1] loss: 0.347
[30,     1] loss: 0.359
[31,     1] loss: 0.302
[32,     1] loss: 0.330
[33,     1] loss: 0.322
[34,     1] loss: 0.277
[35,     1] loss: 0.299
[36,     1] loss: 0.344
[37,     1] loss: 0.267
[38,     1] loss: 0.283
[39,     1] loss: 0.310
[40,     1] loss: 0.279
[41,     1] loss: 0.311
[42,     1] loss: 0.188
[43,     1] loss: 0.258
[44,     1] loss: 0.246
[45,     1] loss: 0.229
[46,     1] loss: 0.250
[47,     1] loss: 0.226
[48,     1] loss: 0.217
[49,     1] loss: 0.194
[50,     1] loss: 0.196
[51,     1] loss: 0.213
[52,     1] loss: 0.232
[53,     1] loss: 0.313
[54,     1] loss: 0.203
[55,     1] loss: 0.244
[56,     1] loss: 0.207
[57,     1] loss: 0.259
[58,     1] loss: 0.295
[59,     1] loss: 0.242
[60,     1] loss: 0.190
[61,     1] loss: 0.227
[62,     1] loss: 0.212
[63,     1] loss: 0.191
[64,     1] loss: 0.172
[65,     1] loss: 0.202
[66,     1] loss: 0.199
[67,     1] loss: 0.161
[68,     1] loss: 0.156
[69,     1] loss: 0.172
[70,     1] loss: 0.140
[71,     1] loss: 0.163
[72,     1] loss: 0.131
[73,     1] loss: 0.167
[74,     1] loss: 0.125
[75,     1] loss: 0.165
[76,     1] loss: 0.107
[77,     1] loss: 0.161
[78,     1] loss: 0.115
[79,     1] loss: 0.195
[80,     1] loss: 0.650
[81,     1] loss: 0.312
[82,     1] loss: 0.277
[83,     1] loss: 0.262
[84,     1] loss: 0.240
[85,     1] loss: 0.294
[86,     1] loss: 0.265
[87,     1] loss: 0.221
[88,     1] loss: 0.184
[89,     1] loss: 0.197
[90,     1] loss: 0.195
[91,     1] loss: 0.197
[92,     1] loss: 0.184
[93,     1] loss: 0.200
[94,     1] loss: 0.222
[95,     1] loss: 0.173
[96,     1] loss: 0.220
[97,     1] loss: 0.179
[98,     1] loss: 0.236
[99,     1] loss: 0.187
[100,     1] loss: 0.165
[101,     1] loss: 0.194
[102,     1] loss: 0.160
[103,     1] loss: 0.166
[104,     1] loss: 0.156
[105,     1] loss: 0.156
[106,     1] loss: 0.112
[107,     1] loss: 0.129
[108,     1] loss: 0.151
[109,     1] loss: 0.129
[110,     1] loss: 0.167
[111,     1] loss: 0.166
[112,     1] loss: 0.173
[113,     1] loss: 0.153
[114,     1] loss: 0.129
[115,     1] loss: 0.127
[116,     1] loss: 0.131
[117,     1] loss: 0.131
[118,     1] loss: 0.157
[119,     1] loss: 0.156
[120,     1] loss: 0.251
[121,     1] loss: 0.449
[122,     1] loss: 0.213
[123,     1] loss: 0.390
[124,     1] loss: 0.278
Early stopping applied (best metric=0.18042194843292236)
Finished Training
Total time taken: 20.529728174209595
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.683
[4,     1] loss: 0.668
[5,     1] loss: 0.638
[6,     1] loss: 0.620
[7,     1] loss: 0.596
[8,     1] loss: 0.556
[9,     1] loss: 0.540
[10,     1] loss: 0.538
[11,     1] loss: 0.470
[12,     1] loss: 0.499
[13,     1] loss: 0.453
[14,     1] loss: 0.466
[15,     1] loss: 0.436
[16,     1] loss: 0.416
[17,     1] loss: 0.385
[18,     1] loss: 0.351
[19,     1] loss: 0.368
[20,     1] loss: 0.380
[21,     1] loss: 0.334
[22,     1] loss: 0.331
[23,     1] loss: 0.362
[24,     1] loss: 0.342
[25,     1] loss: 0.285
[26,     1] loss: 0.276
[27,     1] loss: 0.292
[28,     1] loss: 0.302
[29,     1] loss: 0.250
[30,     1] loss: 0.279
[31,     1] loss: 0.271
[32,     1] loss: 0.229
[33,     1] loss: 0.315
[34,     1] loss: 0.626
[35,     1] loss: 0.344
[36,     1] loss: 0.314
[37,     1] loss: 0.368
[38,     1] loss: 0.321
[39,     1] loss: 0.341
[40,     1] loss: 0.290
[41,     1] loss: 0.309
[42,     1] loss: 0.296
[43,     1] loss: 0.265
[44,     1] loss: 0.285
[45,     1] loss: 0.273
[46,     1] loss: 0.289
[47,     1] loss: 0.218
[48,     1] loss: 0.293
[49,     1] loss: 0.246
[50,     1] loss: 0.228
[51,     1] loss: 0.213
[52,     1] loss: 0.226
[53,     1] loss: 0.217
[54,     1] loss: 0.208
[55,     1] loss: 0.209
[56,     1] loss: 0.202
[57,     1] loss: 0.201
[58,     1] loss: 0.216
[59,     1] loss: 0.158
[60,     1] loss: 0.207
[61,     1] loss: 0.157
[62,     1] loss: 0.195
[63,     1] loss: 0.157
[64,     1] loss: 0.168
[65,     1] loss: 0.185
[66,     1] loss: 0.250
[67,     1] loss: 0.264
[68,     1] loss: 0.384
[69,     1] loss: 0.328
[70,     1] loss: 0.263
[71,     1] loss: 0.209
[72,     1] loss: 0.252
[73,     1] loss: 0.212
[74,     1] loss: 0.273
[75,     1] loss: 0.216
[76,     1] loss: 0.218
[77,     1] loss: 0.286
[78,     1] loss: 0.278
[79,     1] loss: 0.260
[80,     1] loss: 0.270
[81,     1] loss: 0.213
[82,     1] loss: 0.204
[83,     1] loss: 0.271
[84,     1] loss: 0.193
[85,     1] loss: 0.215
[86,     1] loss: 0.239
[87,     1] loss: 0.233
[88,     1] loss: 0.269
[89,     1] loss: 0.196
[90,     1] loss: 0.254
Early stopping applied (best metric=0.35253065824508667)
Finished Training
Total time taken: 14.953805685043335
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.666
[4,     1] loss: 0.641
[5,     1] loss: 0.606
[6,     1] loss: 0.606
[7,     1] loss: 0.565
[8,     1] loss: 0.537
[9,     1] loss: 0.513
[10,     1] loss: 0.522
[11,     1] loss: 0.454
[12,     1] loss: 0.459
[13,     1] loss: 0.434
[14,     1] loss: 0.482
[15,     1] loss: 0.372
[16,     1] loss: 0.357
[17,     1] loss: 0.372
[18,     1] loss: 0.351
[19,     1] loss: 0.375
[20,     1] loss: 0.331
[21,     1] loss: 0.333
[22,     1] loss: 0.300
[23,     1] loss: 0.280
[24,     1] loss: 0.300
[25,     1] loss: 0.259
[26,     1] loss: 0.337
[27,     1] loss: 0.304
[28,     1] loss: 0.345
[29,     1] loss: 0.304
[30,     1] loss: 0.289
[31,     1] loss: 0.246
[32,     1] loss: 0.316
[33,     1] loss: 0.262
[34,     1] loss: 0.289
[35,     1] loss: 0.254
[36,     1] loss: 0.211
[37,     1] loss: 0.184
[38,     1] loss: 0.254
[39,     1] loss: 0.219
[40,     1] loss: 0.249
[41,     1] loss: 0.194
[42,     1] loss: 0.218
[43,     1] loss: 0.209
[44,     1] loss: 0.239
[45,     1] loss: 0.202
[46,     1] loss: 0.248
[47,     1] loss: 0.200
[48,     1] loss: 0.212
[49,     1] loss: 0.219
[50,     1] loss: 0.207
[51,     1] loss: 0.230
[52,     1] loss: 0.189
[53,     1] loss: 0.208
[54,     1] loss: 0.405
[55,     1] loss: 0.349
[56,     1] loss: 0.313
[57,     1] loss: 0.353
[58,     1] loss: 0.311
[59,     1] loss: 0.238
[60,     1] loss: 0.262
[61,     1] loss: 0.260
[62,     1] loss: 0.260
[63,     1] loss: 0.229
[64,     1] loss: 0.249
[65,     1] loss: 0.197
[66,     1] loss: 0.178
[67,     1] loss: 0.174
Early stopping applied (best metric=0.4600136876106262)
Finished Training
Total time taken: 11.113247632980347
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.680
[4,     1] loss: 0.648
[5,     1] loss: 0.602
[6,     1] loss: 0.592
[7,     1] loss: 0.538
[8,     1] loss: 0.520
[9,     1] loss: 0.472
[10,     1] loss: 0.454
[11,     1] loss: 0.474
[12,     1] loss: 0.448
[13,     1] loss: 0.381
[14,     1] loss: 0.383
[15,     1] loss: 0.388
[16,     1] loss: 0.383
[17,     1] loss: 0.341
[18,     1] loss: 0.404
[19,     1] loss: 0.328
[20,     1] loss: 0.348
[21,     1] loss: 0.422
[22,     1] loss: 0.341
[23,     1] loss: 0.400
[24,     1] loss: 0.399
[25,     1] loss: 0.319
[26,     1] loss: 0.349
[27,     1] loss: 0.396
[28,     1] loss: 0.341
[29,     1] loss: 0.328
[30,     1] loss: 0.382
[31,     1] loss: 0.308
[32,     1] loss: 0.325
[33,     1] loss: 0.332
[34,     1] loss: 0.297
[35,     1] loss: 0.294
[36,     1] loss: 0.316
[37,     1] loss: 0.333
[38,     1] loss: 0.323
[39,     1] loss: 0.305
[40,     1] loss: 0.355
[41,     1] loss: 0.345
[42,     1] loss: 0.351
[43,     1] loss: 0.332
[44,     1] loss: 0.376
[45,     1] loss: 0.328
[46,     1] loss: 0.301
[47,     1] loss: 0.315
[48,     1] loss: 0.298
[49,     1] loss: 0.304
[50,     1] loss: 0.283
[51,     1] loss: 0.303
[52,     1] loss: 0.303
[53,     1] loss: 0.253
[54,     1] loss: 0.295
[55,     1] loss: 0.351
[56,     1] loss: 0.251
[57,     1] loss: 0.306
[58,     1] loss: 0.310
[59,     1] loss: 0.253
[60,     1] loss: 0.260
[61,     1] loss: 0.278
[62,     1] loss: 0.348
[63,     1] loss: 0.327
[64,     1] loss: 0.309
[65,     1] loss: 0.292
[66,     1] loss: 0.285
[67,     1] loss: 0.311
[68,     1] loss: 0.279
[69,     1] loss: 0.268
[70,     1] loss: 0.309
[71,     1] loss: 0.311
[72,     1] loss: 0.299
[73,     1] loss: 0.303
[74,     1] loss: 0.262
[75,     1] loss: 0.224
[76,     1] loss: 0.235
[77,     1] loss: 0.222
[78,     1] loss: 0.221
[79,     1] loss: 0.206
[80,     1] loss: 0.206
[81,     1] loss: 0.203
[82,     1] loss: 0.235
[83,     1] loss: 0.265
[84,     1] loss: 0.246
[85,     1] loss: 0.215
Early stopping applied (best metric=0.30011680722236633)
Finished Training
Total time taken: 14.110520839691162
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.678
[4,     1] loss: 0.646
[5,     1] loss: 0.615
[6,     1] loss: 0.589
[7,     1] loss: 0.547
[8,     1] loss: 0.527
[9,     1] loss: 0.502
[10,     1] loss: 0.452
[11,     1] loss: 0.430
[12,     1] loss: 0.418
[13,     1] loss: 0.398
[14,     1] loss: 0.374
[15,     1] loss: 0.375
[16,     1] loss: 0.322
[17,     1] loss: 0.306
[18,     1] loss: 0.275
[19,     1] loss: 0.293
[20,     1] loss: 0.268
[21,     1] loss: 0.328
[22,     1] loss: 0.362
[23,     1] loss: 0.310
[24,     1] loss: 0.286
[25,     1] loss: 0.333
[26,     1] loss: 0.331
[27,     1] loss: 0.284
[28,     1] loss: 0.298
[29,     1] loss: 0.306
[30,     1] loss: 0.277
[31,     1] loss: 0.296
[32,     1] loss: 0.285
[33,     1] loss: 0.299
[34,     1] loss: 0.295
[35,     1] loss: 0.250
[36,     1] loss: 0.260
[37,     1] loss: 0.252
[38,     1] loss: 0.269
[39,     1] loss: 0.221
[40,     1] loss: 0.267
[41,     1] loss: 0.327
[42,     1] loss: 0.281
[43,     1] loss: 0.274
[44,     1] loss: 0.267
[45,     1] loss: 0.245
[46,     1] loss: 0.220
[47,     1] loss: 0.229
[48,     1] loss: 0.258
[49,     1] loss: 0.321
[50,     1] loss: 0.255
[51,     1] loss: 0.270
[52,     1] loss: 0.293
[53,     1] loss: 0.274
[54,     1] loss: 0.317
[55,     1] loss: 0.359
[56,     1] loss: 0.303
[57,     1] loss: 0.310
[58,     1] loss: 0.281
[59,     1] loss: 0.256
[60,     1] loss: 0.276
[61,     1] loss: 0.297
[62,     1] loss: 0.258
Early stopping applied (best metric=0.4057449996471405)
Finished Training
Total time taken: 10.481530666351318
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.695
[3,     1] loss: 0.678
[4,     1] loss: 0.650
[5,     1] loss: 0.617
[6,     1] loss: 0.573
[7,     1] loss: 0.541
[8,     1] loss: 0.516
[9,     1] loss: 0.451
[10,     1] loss: 0.433
[11,     1] loss: 0.429
[12,     1] loss: 0.393
[13,     1] loss: 0.443
[14,     1] loss: 0.361
[15,     1] loss: 0.326
[16,     1] loss: 0.336
[17,     1] loss: 0.304
[18,     1] loss: 0.321
[19,     1] loss: 0.380
[20,     1] loss: 0.306
[21,     1] loss: 0.345
[22,     1] loss: 0.281
[23,     1] loss: 0.304
[24,     1] loss: 0.299
[25,     1] loss: 0.263
[26,     1] loss: 0.255
[27,     1] loss: 0.242
[28,     1] loss: 0.248
[29,     1] loss: 0.237
[30,     1] loss: 0.201
[31,     1] loss: 0.208
[32,     1] loss: 0.281
[33,     1] loss: 0.239
[34,     1] loss: 0.278
[35,     1] loss: 0.270
[36,     1] loss: 0.230
[37,     1] loss: 0.247
[38,     1] loss: 0.250
[39,     1] loss: 0.208
[40,     1] loss: 0.216
[41,     1] loss: 0.223
[42,     1] loss: 0.207
[43,     1] loss: 0.196
[44,     1] loss: 0.252
[45,     1] loss: 0.234
[46,     1] loss: 0.312
[47,     1] loss: 0.244
[48,     1] loss: 0.335
[49,     1] loss: 0.277
[50,     1] loss: 0.256
[51,     1] loss: 0.232
[52,     1] loss: 0.221
[53,     1] loss: 0.244
[54,     1] loss: 0.208
[55,     1] loss: 0.229
[56,     1] loss: 0.195
[57,     1] loss: 0.183
[58,     1] loss: 0.172
[59,     1] loss: 0.197
[60,     1] loss: 0.161
[61,     1] loss: 0.230
[62,     1] loss: 0.213
Early stopping applied (best metric=0.29237812757492065)
Finished Training
Total time taken: 10.345512628555298
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.647
[4,     1] loss: 0.629
[5,     1] loss: 0.597
[6,     1] loss: 0.539
[7,     1] loss: 0.514
[8,     1] loss: 0.489
[9,     1] loss: 0.471
[10,     1] loss: 0.454
[11,     1] loss: 0.438
[12,     1] loss: 0.352
[13,     1] loss: 0.324
[14,     1] loss: 0.362
[15,     1] loss: 0.306
[16,     1] loss: 0.322
[17,     1] loss: 0.269
[18,     1] loss: 0.341
[19,     1] loss: 0.305
[20,     1] loss: 0.310
[21,     1] loss: 0.272
[22,     1] loss: 0.322
[23,     1] loss: 0.263
[24,     1] loss: 0.338
[25,     1] loss: 0.216
[26,     1] loss: 0.305
[27,     1] loss: 0.232
[28,     1] loss: 0.277
[29,     1] loss: 0.210
[30,     1] loss: 0.244
[31,     1] loss: 0.235
[32,     1] loss: 0.206
[33,     1] loss: 0.199
[34,     1] loss: 0.170
[35,     1] loss: 0.248
[36,     1] loss: 0.313
[37,     1] loss: 0.314
[38,     1] loss: 0.312
[39,     1] loss: 0.299
[40,     1] loss: 0.246
[41,     1] loss: 0.250
[42,     1] loss: 0.218
[43,     1] loss: 0.232
[44,     1] loss: 0.222
[45,     1] loss: 0.190
[46,     1] loss: 0.180
[47,     1] loss: 0.187
[48,     1] loss: 0.181
[49,     1] loss: 0.169
[50,     1] loss: 0.242
[51,     1] loss: 0.220
[52,     1] loss: 0.164
[53,     1] loss: 0.181
[54,     1] loss: 0.167
[55,     1] loss: 0.212
[56,     1] loss: 0.259
[57,     1] loss: 0.266
[58,     1] loss: 0.179
[59,     1] loss: 0.229
[60,     1] loss: 0.173
[61,     1] loss: 0.214
[62,     1] loss: 0.181
[63,     1] loss: 0.140
[64,     1] loss: 0.169
[65,     1] loss: 0.164
[66,     1] loss: 0.160
Early stopping applied (best metric=0.3328878879547119)
Finished Training
Total time taken: 11.39779782295227
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.677
[4,     1] loss: 0.645
[5,     1] loss: 0.612
[6,     1] loss: 0.597
[7,     1] loss: 0.547
[8,     1] loss: 0.524
[9,     1] loss: 0.467
[10,     1] loss: 0.456
[11,     1] loss: 0.427
[12,     1] loss: 0.404
[13,     1] loss: 0.398
[14,     1] loss: 0.376
[15,     1] loss: 0.363
[16,     1] loss: 0.347
[17,     1] loss: 0.455
[18,     1] loss: 0.305
[19,     1] loss: 0.365
[20,     1] loss: 0.380
[21,     1] loss: 0.416
[22,     1] loss: 0.377
[23,     1] loss: 0.386
[24,     1] loss: 0.310
[25,     1] loss: 0.393
[26,     1] loss: 0.389
[27,     1] loss: 0.339
[28,     1] loss: 0.346
[29,     1] loss: 0.321
[30,     1] loss: 0.299
[31,     1] loss: 0.270
[32,     1] loss: 0.276
[33,     1] loss: 0.311
[34,     1] loss: 0.248
[35,     1] loss: 0.271
[36,     1] loss: 0.241
[37,     1] loss: 0.188
[38,     1] loss: 0.289
[39,     1] loss: 0.258
[40,     1] loss: 0.435
[41,     1] loss: 0.319
[42,     1] loss: 0.314
[43,     1] loss: 0.317
[44,     1] loss: 0.317
[45,     1] loss: 0.270
[46,     1] loss: 0.300
[47,     1] loss: 0.251
[48,     1] loss: 0.304
[49,     1] loss: 0.260
[50,     1] loss: 0.268
[51,     1] loss: 0.252
[52,     1] loss: 0.260
[53,     1] loss: 0.237
[54,     1] loss: 0.242
[55,     1] loss: 0.231
[56,     1] loss: 0.243
[57,     1] loss: 0.290
[58,     1] loss: 0.213
[59,     1] loss: 0.323
[60,     1] loss: 0.210
[61,     1] loss: 0.419
[62,     1] loss: 0.226
[63,     1] loss: 0.217
[64,     1] loss: 0.250
[65,     1] loss: 0.193
Early stopping applied (best metric=0.45932814478874207)
Finished Training
Total time taken: 10.434628009796143
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.670
[4,     1] loss: 0.633
[5,     1] loss: 0.588
[6,     1] loss: 0.558
[7,     1] loss: 0.495
[8,     1] loss: 0.492
[9,     1] loss: 0.494
[10,     1] loss: 0.445
[11,     1] loss: 0.429
[12,     1] loss: 0.427
[13,     1] loss: 0.352
[14,     1] loss: 0.368
[15,     1] loss: 0.335
[16,     1] loss: 0.342
[17,     1] loss: 0.303
[18,     1] loss: 0.311
[19,     1] loss: 0.314
[20,     1] loss: 0.406
[21,     1] loss: 0.337
[22,     1] loss: 0.286
[23,     1] loss: 0.343
[24,     1] loss: 0.339
[25,     1] loss: 0.307
[26,     1] loss: 0.345
[27,     1] loss: 0.280
[28,     1] loss: 0.316
[29,     1] loss: 0.319
[30,     1] loss: 0.299
[31,     1] loss: 0.271
[32,     1] loss: 0.273
[33,     1] loss: 0.289
[34,     1] loss: 0.256
[35,     1] loss: 0.236
[36,     1] loss: 0.224
[37,     1] loss: 0.234
[38,     1] loss: 0.226
[39,     1] loss: 0.245
[40,     1] loss: 0.282
[41,     1] loss: 0.251
[42,     1] loss: 0.339
[43,     1] loss: 0.300
[44,     1] loss: 0.262
[45,     1] loss: 0.294
[46,     1] loss: 0.319
[47,     1] loss: 0.262
[48,     1] loss: 0.237
[49,     1] loss: 0.227
[50,     1] loss: 0.237
[51,     1] loss: 0.239
[52,     1] loss: 0.325
[53,     1] loss: 0.352
[54,     1] loss: 0.291
[55,     1] loss: 0.287
[56,     1] loss: 0.308
[57,     1] loss: 0.299
[58,     1] loss: 0.349
[59,     1] loss: 0.302
[60,     1] loss: 0.295
[61,     1] loss: 0.333
Early stopping applied (best metric=0.3253791928291321)
Finished Training
Total time taken: 7.709143877029419
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.675
[4,     1] loss: 0.652
[5,     1] loss: 0.631
[6,     1] loss: 0.590
[7,     1] loss: 0.569
[8,     1] loss: 0.547
[9,     1] loss: 0.514
[10,     1] loss: 0.509
[11,     1] loss: 0.467
[12,     1] loss: 0.456
[13,     1] loss: 0.425
[14,     1] loss: 0.435
[15,     1] loss: 0.381
[16,     1] loss: 0.333
[17,     1] loss: 0.338
[18,     1] loss: 0.332
[19,     1] loss: 0.313
[20,     1] loss: 0.308
[21,     1] loss: 0.275
[22,     1] loss: 0.321
[23,     1] loss: 0.330
[24,     1] loss: 0.320
[25,     1] loss: 0.253
[26,     1] loss: 0.320
[27,     1] loss: 0.230
[28,     1] loss: 0.223
[29,     1] loss: 0.279
[30,     1] loss: 0.205
[31,     1] loss: 0.215
[32,     1] loss: 0.217
[33,     1] loss: 0.222
[34,     1] loss: 0.208
[35,     1] loss: 0.241
[36,     1] loss: 0.269
[37,     1] loss: 0.248
[38,     1] loss: 0.231
[39,     1] loss: 0.232
[40,     1] loss: 0.211
[41,     1] loss: 0.192
[42,     1] loss: 0.179
[43,     1] loss: 0.203
[44,     1] loss: 0.237
[45,     1] loss: 0.192
[46,     1] loss: 0.184
[47,     1] loss: 0.241
[48,     1] loss: 0.196
[49,     1] loss: 0.479
[50,     1] loss: 0.194
[51,     1] loss: 0.239
[52,     1] loss: 0.239
[53,     1] loss: 0.226
[54,     1] loss: 0.207
[55,     1] loss: 0.176
[56,     1] loss: 0.234
[57,     1] loss: 0.210
[58,     1] loss: 0.203
[59,     1] loss: 0.200
[60,     1] loss: 0.199
[61,     1] loss: 0.196
[62,     1] loss: 0.204
[63,     1] loss: 0.188
[64,     1] loss: 0.218
Early stopping applied (best metric=0.3816320300102234)
Finished Training
Total time taken: 7.479556083679199
{'Hydroxylation-K Validation Accuracy': 0.8428723404255319, 'Hydroxylation-K Validation Sensitivity': 0.8333333333333334, 'Hydroxylation-K Validation Specificity': 0.8452631578947368, 'Hydroxylation-K Validation Precision': 0.5892381895710069, 'Hydroxylation-K AUC ROC': 0.8434444444444444, 'Hydroxylation-K AUC PR': 0.6029345608109211, 'Hydroxylation-K MCC': 0.6058051951313849, 'Hydroxylation-K F1': 0.6849515611824457, 'Validation Loss (Hydroxylation-K)': 0.3440078860521317, 'Validation Loss (total)': 0.3440078860521317, 'TimeToTrain': 15.100656251907349}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011645201197078593,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2442430593,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.83434838346635}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008125124157647334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2328340061,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.605943736796194}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.686
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004964750453232782,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 985170538,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.8150911262445963}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.689
[3,     1] loss: 0.682
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004915384284014045,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1322920574,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.128885643017839}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.701
[3,     1] loss: 0.681
[4,     1] loss: 0.645
[5,     1] loss: 0.607
[6,     1] loss: 0.572
[7,     1] loss: 0.533
[8,     1] loss: 0.509
[9,     1] loss: 0.435
[10,     1] loss: 0.447
[11,     1] loss: 0.374
[12,     1] loss: 0.372
[13,     1] loss: 0.333
[14,     1] loss: 0.333
[15,     1] loss: 0.313
[16,     1] loss: 0.327
[17,     1] loss: 0.324
[18,     1] loss: 0.261
[19,     1] loss: 0.294
[20,     1] loss: 0.295
[21,     1] loss: 0.264
[22,     1] loss: 0.203
[23,     1] loss: 0.242
[24,     1] loss: 0.202
[25,     1] loss: 0.167
[26,     1] loss: 0.177
[27,     1] loss: 0.189
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013311619799867302,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1707864636,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.0320405998278455}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.693
[3,     1] loss: 0.684
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009590329688456314,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2771963870,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9958084552309199}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.667
[4,     1] loss: 0.635
[5,     1] loss: 0.591
[6,     1] loss: 0.519
[7,     1] loss: 0.494
[8,     1] loss: 0.456
[9,     1] loss: 0.439
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036407038000375288,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1148263985,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.878661983628632}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.664
[4,     1] loss: 0.652
[5,     1] loss: 0.628
[6,     1] loss: 0.600
[7,     1] loss: 0.572
[8,     1] loss: 0.545
[9,     1] loss: 0.534
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005362167962117599,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3172755615,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.693140069873637}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.689
[3,     1] loss: 0.668
[4,     1] loss: 0.617
[5,     1] loss: 0.574
[6,     1] loss: 0.553
[7,     1] loss: 0.456
[8,     1] loss: 0.421
[9,     1] loss: 0.381
[10,     1] loss: 0.412
[11,     1] loss: 0.384
[12,     1] loss: 0.365
[13,     1] loss: 0.433
[14,     1] loss: 0.367
[15,     1] loss: 0.323
[16,     1] loss: 0.357
[17,     1] loss: 0.297
[18,     1] loss: 0.362
[19,     1] loss: 0.246
[20,     1] loss: 0.332
[21,     1] loss: 0.290
[22,     1] loss: 0.310
[23,     1] loss: 0.261
[24,     1] loss: 0.308
[25,     1] loss: 0.291
[26,     1] loss: 0.287
[27,     1] loss: 0.269
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004687281542018497,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2202987315,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.912025288137983}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.694
[3,     1] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012020187877469007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3455873973,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.911544880885337}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.688
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008009618379810134,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3744964566,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.245540670870357}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.692
[3,     1] loss: 0.674
[4,     1] loss: 0.649
[5,     1] loss: 0.592
[6,     1] loss: 0.534
[7,     1] loss: 0.488
[8,     1] loss: 0.442
[9,     1] loss: 0.423
[10,     1] loss: 0.323
[11,     1] loss: 0.371
[12,     1] loss: 0.392
[13,     1] loss: 0.373
[14,     1] loss: 0.230
[15,     1] loss: 0.304
[16,     1] loss: 0.285
[17,     1] loss: 0.340
[18,     1] loss: 0.264
[19,     1] loss: 0.410
[20,     1] loss: 0.242
[21,     1] loss: 0.303
[22,     1] loss: 0.284
[23,     1] loss: 0.323
[24,     1] loss: 0.272
[25,     1] loss: 0.300
[26,     1] loss: 0.251
[27,     1] loss: 0.234
[28,     1] loss: 0.264
[29,     1] loss: 0.252
[30,     1] loss: 0.211
[31,     1] loss: 0.223
[32,     1] loss: 0.174
[33,     1] loss: 0.151
[34,     1] loss: 0.247
[35,     1] loss: 0.189
[36,     1] loss: 0.173
[37,     1] loss: 0.165
[38,     1] loss: 0.167
[39,     1] loss: 0.226
[40,     1] loss: 0.188
[41,     1] loss: 0.180
[42,     1] loss: 0.173
[43,     1] loss: 0.210
[44,     1] loss: 0.170
[45,     1] loss: 0.144
[46,     1] loss: 0.161
[47,     1] loss: 0.137
[48,     1] loss: 0.110
[49,     1] loss: 0.096
[50,     1] loss: 0.111
[51,     1] loss: 0.622
[52,     1] loss: 0.465
[53,     1] loss: 0.216
[54,     1] loss: 0.341
[55,     1] loss: 0.271
[56,     1] loss: 0.294
[57,     1] loss: 0.253
[58,     1] loss: 0.286
[59,     1] loss: 0.307
[60,     1] loss: 0.241
[61,     1] loss: 0.255
[62,     1] loss: 0.225
[63,     1] loss: 0.194
[64,     1] loss: 0.199
[65,     1] loss: 0.183
[66,     1] loss: 0.156
[67,     1] loss: 0.161
[68,     1] loss: 0.115
[69,     1] loss: 0.157
[70,     1] loss: 0.143
[71,     1] loss: 0.146
[72,     1] loss: 0.118
[73,     1] loss: 0.104
[74,     1] loss: 0.124
[75,     1] loss: 0.108
[76,     1] loss: 0.139
[77,     1] loss: 0.100
[78,     1] loss: 0.088
[79,     1] loss: 0.101
[80,     1] loss: 0.095
[81,     1] loss: 0.111
[82,     1] loss: 0.111
[83,     1] loss: 0.119
[84,     1] loss: 0.147
[85,     1] loss: 0.346
[86,     1] loss: 0.377
[87,     1] loss: 0.132
[88,     1] loss: 0.181
[89,     1] loss: 0.238
[90,     1] loss: 0.223
[91,     1] loss: 0.205
[92,     1] loss: 0.195
[93,     1] loss: 0.190
[94,     1] loss: 0.168
[95,     1] loss: 0.174
[96,     1] loss: 0.140
[97,     1] loss: 0.163
[98,     1] loss: 0.145
[99,     1] loss: 0.123
[100,     1] loss: 0.124
[101,     1] loss: 0.135
[102,     1] loss: 0.150
[103,     1] loss: 0.216
[104,     1] loss: 0.227
[105,     1] loss: 0.320
[106,     1] loss: 0.207
[107,     1] loss: 0.322
[108,     1] loss: 0.242
[109,     1] loss: 0.200
[110,     1] loss: 0.257
[111,     1] loss: 0.215
[112,     1] loss: 0.162
[113,     1] loss: 0.200
[114,     1] loss: 0.190
Early stopping applied (best metric=0.1793755292892456)
Finished Training
Total time taken: 13.141541242599487
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.664
[4,     1] loss: 0.637
[5,     1] loss: 0.586
[6,     1] loss: 0.527
[7,     1] loss: 0.483
[8,     1] loss: 0.451
[9,     1] loss: 0.413
[10,     1] loss: 0.383
[11,     1] loss: 0.434
[12,     1] loss: 0.379
[13,     1] loss: 0.385
[14,     1] loss: 0.362
[15,     1] loss: 0.327
[16,     1] loss: 0.399
[17,     1] loss: 0.276
[18,     1] loss: 0.327
[19,     1] loss: 0.326
[20,     1] loss: 0.277
[21,     1] loss: 0.216
[22,     1] loss: 0.284
[23,     1] loss: 0.237
[24,     1] loss: 0.260
[25,     1] loss: 0.239
[26,     1] loss: 0.301
[27,     1] loss: 0.251
[28,     1] loss: 0.260
[29,     1] loss: 0.330
[30,     1] loss: 0.233
[31,     1] loss: 0.229
[32,     1] loss: 0.287
[33,     1] loss: 0.236
[34,     1] loss: 0.229
[35,     1] loss: 0.197
[36,     1] loss: 0.240
[37,     1] loss: 0.367
[38,     1] loss: 0.250
[39,     1] loss: 0.246
[40,     1] loss: 0.230
[41,     1] loss: 0.226
[42,     1] loss: 0.286
[43,     1] loss: 0.222
[44,     1] loss: 0.205
[45,     1] loss: 0.198
[46,     1] loss: 0.177
[47,     1] loss: 0.154
[48,     1] loss: 0.165
[49,     1] loss: 0.152
[50,     1] loss: 0.170
[51,     1] loss: 0.227
[52,     1] loss: 0.113
[53,     1] loss: 0.214
[54,     1] loss: 0.151
[55,     1] loss: 0.201
[56,     1] loss: 0.136
[57,     1] loss: 0.138
[58,     1] loss: 0.155
[59,     1] loss: 0.110
[60,     1] loss: 0.119
[61,     1] loss: 0.124
[62,     1] loss: 0.097
[63,     1] loss: 0.128
[64,     1] loss: 0.174
[65,     1] loss: 0.128
[66,     1] loss: 0.173
[67,     1] loss: 0.108
[68,     1] loss: 0.137
[69,     1] loss: 0.098
[70,     1] loss: 0.100
[71,     1] loss: 0.137
[72,     1] loss: 0.107
[73,     1] loss: 0.118
[74,     1] loss: 0.144
[75,     1] loss: 0.342
[76,     1] loss: 0.270
[77,     1] loss: 0.200
[78,     1] loss: 0.195
[79,     1] loss: 0.254
[80,     1] loss: 0.219
[81,     1] loss: 0.182
[82,     1] loss: 0.206
[83,     1] loss: 0.161
[84,     1] loss: 0.194
[85,     1] loss: 0.143
[86,     1] loss: 0.110
[87,     1] loss: 0.127
[88,     1] loss: 0.132
[89,     1] loss: 0.122
[90,     1] loss: 0.199
[91,     1] loss: 0.089
[92,     1] loss: 0.161
[93,     1] loss: 0.212
[94,     1] loss: 0.150
Early stopping applied (best metric=0.49871110916137695)
Finished Training
Total time taken: 10.660949468612671
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.686
[4,     1] loss: 0.668
[5,     1] loss: 0.619
[6,     1] loss: 0.577
[7,     1] loss: 0.519
[8,     1] loss: 0.472
[9,     1] loss: 0.419
[10,     1] loss: 0.416
[11,     1] loss: 0.354
[12,     1] loss: 0.345
[13,     1] loss: 0.303
[14,     1] loss: 0.251
[15,     1] loss: 0.296
[16,     1] loss: 0.241
[17,     1] loss: 0.325
[18,     1] loss: 0.261
[19,     1] loss: 0.226
[20,     1] loss: 0.245
[21,     1] loss: 0.236
[22,     1] loss: 0.222
[23,     1] loss: 0.227
[24,     1] loss: 0.209
[25,     1] loss: 0.242
[26,     1] loss: 0.178
[27,     1] loss: 0.194
[28,     1] loss: 0.178
[29,     1] loss: 0.183
[30,     1] loss: 0.173
[31,     1] loss: 0.232
[32,     1] loss: 0.191
[33,     1] loss: 0.167
[34,     1] loss: 0.146
[35,     1] loss: 0.278
[36,     1] loss: 0.234
[37,     1] loss: 0.200
[38,     1] loss: 0.200
[39,     1] loss: 0.260
[40,     1] loss: 0.215
[41,     1] loss: 0.207
[42,     1] loss: 0.202
[43,     1] loss: 0.199
[44,     1] loss: 0.194
[45,     1] loss: 0.149
[46,     1] loss: 0.146
[47,     1] loss: 0.153
[48,     1] loss: 0.172
[49,     1] loss: 0.142
[50,     1] loss: 0.194
[51,     1] loss: 0.144
[52,     1] loss: 0.146
[53,     1] loss: 0.193
[54,     1] loss: 0.128
[55,     1] loss: 0.149
[56,     1] loss: 0.164
[57,     1] loss: 0.135
[58,     1] loss: 0.133
[59,     1] loss: 0.142
[60,     1] loss: 0.151
[61,     1] loss: 0.078
[62,     1] loss: 0.145
[63,     1] loss: 0.326
[64,     1] loss: 0.356
Early stopping applied (best metric=0.3570464253425598)
Finished Training
Total time taken: 7.209325313568115
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.645
[5,     1] loss: 0.604
[6,     1] loss: 0.541
[7,     1] loss: 0.506
[8,     1] loss: 0.493
[9,     1] loss: 0.468
[10,     1] loss: 0.413
[11,     1] loss: 0.410
[12,     1] loss: 0.365
[13,     1] loss: 0.348
[14,     1] loss: 0.407
[15,     1] loss: 0.325
[16,     1] loss: 0.350
[17,     1] loss: 0.330
[18,     1] loss: 0.313
[19,     1] loss: 0.259
[20,     1] loss: 0.291
[21,     1] loss: 0.254
[22,     1] loss: 0.263
[23,     1] loss: 0.316
[24,     1] loss: 0.249
[25,     1] loss: 0.247
[26,     1] loss: 0.229
[27,     1] loss: 0.309
[28,     1] loss: 0.178
[29,     1] loss: 0.293
[30,     1] loss: 0.231
[31,     1] loss: 0.181
[32,     1] loss: 0.198
[33,     1] loss: 0.170
[34,     1] loss: 0.207
[35,     1] loss: 0.214
[36,     1] loss: 0.184
[37,     1] loss: 0.246
[38,     1] loss: 0.292
[39,     1] loss: 0.196
[40,     1] loss: 0.181
[41,     1] loss: 0.184
[42,     1] loss: 0.158
[43,     1] loss: 0.167
[44,     1] loss: 0.126
[45,     1] loss: 0.157
[46,     1] loss: 0.116
[47,     1] loss: 0.171
[48,     1] loss: 0.103
[49,     1] loss: 0.113
[50,     1] loss: 0.120
[51,     1] loss: 0.133
[52,     1] loss: 0.172
[53,     1] loss: 0.332
[54,     1] loss: 0.483
[55,     1] loss: 0.244
[56,     1] loss: 0.389
[57,     1] loss: 0.239
[58,     1] loss: 0.279
[59,     1] loss: 0.302
[60,     1] loss: 0.297
[61,     1] loss: 0.254
[62,     1] loss: 0.243
[63,     1] loss: 0.208
[64,     1] loss: 0.203
[65,     1] loss: 0.181
[66,     1] loss: 0.179
[67,     1] loss: 0.194
[68,     1] loss: 0.154
[69,     1] loss: 0.204
[70,     1] loss: 0.127
[71,     1] loss: 0.147
[72,     1] loss: 0.131
[73,     1] loss: 0.148
[74,     1] loss: 0.122
[75,     1] loss: 0.208
[76,     1] loss: 0.169
[77,     1] loss: 0.145
[78,     1] loss: 0.138
[79,     1] loss: 0.160
[80,     1] loss: 0.105
[81,     1] loss: 0.081
[82,     1] loss: 0.118
[83,     1] loss: 0.128
[84,     1] loss: 0.322
[85,     1] loss: 0.330
[86,     1] loss: 0.411
[87,     1] loss: 0.257
[88,     1] loss: 0.298
[89,     1] loss: 0.246
[90,     1] loss: 0.297
[91,     1] loss: 0.256
[92,     1] loss: 0.250
[93,     1] loss: 0.256
Early stopping applied (best metric=0.11973875015974045)
Finished Training
Total time taken: 10.472032308578491
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.695
[3,     1] loss: 0.689
[4,     1] loss: 0.677
[5,     1] loss: 0.658
[6,     1] loss: 0.633
[7,     1] loss: 0.597
[8,     1] loss: 0.550
[9,     1] loss: 0.475
[10,     1] loss: 0.488
[11,     1] loss: 0.473
[12,     1] loss: 0.362
[13,     1] loss: 0.385
[14,     1] loss: 0.384
[15,     1] loss: 0.328
[16,     1] loss: 0.291
[17,     1] loss: 0.330
[18,     1] loss: 0.268
[19,     1] loss: 0.272
[20,     1] loss: 0.248
[21,     1] loss: 0.241
[22,     1] loss: 0.238
[23,     1] loss: 0.217
[24,     1] loss: 0.308
[25,     1] loss: 0.431
[26,     1] loss: 0.334
[27,     1] loss: 0.298
[28,     1] loss: 0.285
[29,     1] loss: 0.299
[30,     1] loss: 0.303
[31,     1] loss: 0.316
[32,     1] loss: 0.300
[33,     1] loss: 0.255
[34,     1] loss: 0.311
[35,     1] loss: 0.218
[36,     1] loss: 0.252
[37,     1] loss: 0.225
[38,     1] loss: 0.203
[39,     1] loss: 0.191
[40,     1] loss: 0.182
[41,     1] loss: 0.159
[42,     1] loss: 0.199
[43,     1] loss: 0.202
[44,     1] loss: 0.274
[45,     1] loss: 0.240
[46,     1] loss: 0.200
[47,     1] loss: 0.257
[48,     1] loss: 0.219
[49,     1] loss: 0.171
[50,     1] loss: 0.153
[51,     1] loss: 0.189
[52,     1] loss: 0.154
[53,     1] loss: 0.164
[54,     1] loss: 0.219
[55,     1] loss: 0.143
[56,     1] loss: 0.176
[57,     1] loss: 0.194
[58,     1] loss: 0.153
[59,     1] loss: 0.152
[60,     1] loss: 0.155
[61,     1] loss: 0.157
[62,     1] loss: 0.176
[63,     1] loss: 0.265
[64,     1] loss: 0.229
[65,     1] loss: 0.196
[66,     1] loss: 0.165
[67,     1] loss: 0.287
[68,     1] loss: 0.208
[69,     1] loss: 0.250
[70,     1] loss: 0.196
[71,     1] loss: 0.209
[72,     1] loss: 0.228
[73,     1] loss: 0.185
[74,     1] loss: 0.119
[75,     1] loss: 0.186
[76,     1] loss: 0.133
[77,     1] loss: 0.125
[78,     1] loss: 0.128
[79,     1] loss: 0.125
[80,     1] loss: 0.111
[81,     1] loss: 0.132
[82,     1] loss: 0.107
[83,     1] loss: 0.136
[84,     1] loss: 0.126
[85,     1] loss: 0.166
[86,     1] loss: 0.123
[87,     1] loss: 0.143
[88,     1] loss: 0.100
[89,     1] loss: 0.124
[90,     1] loss: 0.091
[91,     1] loss: 0.116
[92,     1] loss: 0.162
[93,     1] loss: 0.108
[94,     1] loss: 0.148
[95,     1] loss: 0.141
[96,     1] loss: 0.180
[97,     1] loss: 0.142
[98,     1] loss: 0.172
[99,     1] loss: 0.211
[100,     1] loss: 0.169
[101,     1] loss: 0.161
[102,     1] loss: 0.192
[103,     1] loss: 0.199
[104,     1] loss: 0.163
[105,     1] loss: 0.147
[106,     1] loss: 0.150
[107,     1] loss: 0.181
[108,     1] loss: 0.160
[109,     1] loss: 0.155
[110,     1] loss: 0.132
[111,     1] loss: 0.188
[112,     1] loss: 0.115
[113,     1] loss: 0.127
[114,     1] loss: 0.167
[115,     1] loss: 0.194
[116,     1] loss: 0.126
[117,     1] loss: 0.140
[118,     1] loss: 0.141
Early stopping applied (best metric=0.22421588003635406)
Finished Training
Total time taken: 13.212127685546875
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.716
[3,     1] loss: 0.691
[4,     1] loss: 0.689
[5,     1] loss: 0.677
[6,     1] loss: 0.668
[7,     1] loss: 0.640
[8,     1] loss: 0.615
[9,     1] loss: 0.584
[10,     1] loss: 0.546
[11,     1] loss: 0.520
[12,     1] loss: 0.492
[13,     1] loss: 0.453
[14,     1] loss: 0.427
[15,     1] loss: 0.374
[16,     1] loss: 0.420
[17,     1] loss: 0.402
[18,     1] loss: 0.348
[19,     1] loss: 0.347
[20,     1] loss: 0.281
[21,     1] loss: 0.328
[22,     1] loss: 0.280
[23,     1] loss: 0.338
[24,     1] loss: 0.312
[25,     1] loss: 0.330
[26,     1] loss: 0.281
[27,     1] loss: 0.326
[28,     1] loss: 0.283
[29,     1] loss: 0.261
[30,     1] loss: 0.228
[31,     1] loss: 0.263
[32,     1] loss: 0.306
[33,     1] loss: 0.244
[34,     1] loss: 0.241
[35,     1] loss: 0.237
[36,     1] loss: 0.268
[37,     1] loss: 0.244
[38,     1] loss: 0.229
[39,     1] loss: 0.257
[40,     1] loss: 0.263
[41,     1] loss: 0.211
[42,     1] loss: 0.169
[43,     1] loss: 0.213
[44,     1] loss: 0.211
[45,     1] loss: 0.210
[46,     1] loss: 0.211
[47,     1] loss: 0.238
[48,     1] loss: 0.256
[49,     1] loss: 0.230
[50,     1] loss: 0.205
[51,     1] loss: 0.188
[52,     1] loss: 0.239
[53,     1] loss: 0.163
[54,     1] loss: 0.196
[55,     1] loss: 0.189
[56,     1] loss: 0.152
[57,     1] loss: 0.169
[58,     1] loss: 0.170
[59,     1] loss: 0.387
[60,     1] loss: 0.483
[61,     1] loss: 0.319
[62,     1] loss: 0.277
[63,     1] loss: 0.315
[64,     1] loss: 0.297
[65,     1] loss: 0.314
[66,     1] loss: 0.297
[67,     1] loss: 0.319
[68,     1] loss: 0.283
[69,     1] loss: 0.250
[70,     1] loss: 0.247
[71,     1] loss: 0.201
[72,     1] loss: 0.197
[73,     1] loss: 0.218
[74,     1] loss: 0.291
[75,     1] loss: 0.144
[76,     1] loss: 0.226
[77,     1] loss: 0.187
[78,     1] loss: 0.222
[79,     1] loss: 0.182
[80,     1] loss: 0.204
Early stopping applied (best metric=0.2551794946193695)
Finished Training
Total time taken: 9.010514259338379
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.695
[3,     1] loss: 0.682
[4,     1] loss: 0.666
[5,     1] loss: 0.626
[6,     1] loss: 0.590
[7,     1] loss: 0.561
[8,     1] loss: 0.484
[9,     1] loss: 0.420
[10,     1] loss: 0.390
[11,     1] loss: 0.387
[12,     1] loss: 0.327
[13,     1] loss: 0.277
[14,     1] loss: 0.331
[15,     1] loss: 0.221
[16,     1] loss: 0.331
[17,     1] loss: 0.222
[18,     1] loss: 0.268
[19,     1] loss: 0.190
[20,     1] loss: 0.207
[21,     1] loss: 0.183
[22,     1] loss: 0.207
[23,     1] loss: 0.153
[24,     1] loss: 0.146
[25,     1] loss: 0.161
[26,     1] loss: 0.178
[27,     1] loss: 0.199
[28,     1] loss: 0.160
[29,     1] loss: 0.196
[30,     1] loss: 0.133
[31,     1] loss: 0.159
[32,     1] loss: 0.176
[33,     1] loss: 0.158
[34,     1] loss: 0.137
[35,     1] loss: 0.162
[36,     1] loss: 0.167
[37,     1] loss: 0.132
[38,     1] loss: 0.439
[39,     1] loss: 0.282
[40,     1] loss: 0.207
[41,     1] loss: 0.295
[42,     1] loss: 0.266
[43,     1] loss: 0.234
[44,     1] loss: 0.271
[45,     1] loss: 0.226
[46,     1] loss: 0.222
[47,     1] loss: 0.232
[48,     1] loss: 0.155
[49,     1] loss: 0.176
[50,     1] loss: 0.128
[51,     1] loss: 0.141
[52,     1] loss: 0.143
[53,     1] loss: 0.133
[54,     1] loss: 0.106
[55,     1] loss: 0.133
[56,     1] loss: 0.103
[57,     1] loss: 0.131
[58,     1] loss: 0.122
[59,     1] loss: 0.097
Early stopping applied (best metric=0.31908416748046875)
Finished Training
Total time taken: 6.6310014724731445
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.692
[3,     1] loss: 0.682
[4,     1] loss: 0.669
[5,     1] loss: 0.653
[6,     1] loss: 0.608
[7,     1] loss: 0.574
[8,     1] loss: 0.515
[9,     1] loss: 0.469
[10,     1] loss: 0.500
[11,     1] loss: 0.433
[12,     1] loss: 0.409
[13,     1] loss: 0.309
[14,     1] loss: 0.383
[15,     1] loss: 0.478
[16,     1] loss: 0.296
[17,     1] loss: 0.330
[18,     1] loss: 0.374
[19,     1] loss: 0.331
[20,     1] loss: 0.322
[21,     1] loss: 0.324
[22,     1] loss: 0.270
[23,     1] loss: 0.273
[24,     1] loss: 0.265
[25,     1] loss: 0.217
[26,     1] loss: 0.229
[27,     1] loss: 0.216
[28,     1] loss: 0.169
[29,     1] loss: 0.349
[30,     1] loss: 0.239
[31,     1] loss: 0.288
[32,     1] loss: 0.278
[33,     1] loss: 0.259
[34,     1] loss: 0.242
[35,     1] loss: 0.281
[36,     1] loss: 0.218
[37,     1] loss: 0.215
[38,     1] loss: 0.179
[39,     1] loss: 0.214
[40,     1] loss: 0.246
[41,     1] loss: 0.199
[42,     1] loss: 0.224
[43,     1] loss: 0.162
[44,     1] loss: 0.210
[45,     1] loss: 0.239
[46,     1] loss: 0.203
[47,     1] loss: 0.179
[48,     1] loss: 0.205
[49,     1] loss: 0.181
[50,     1] loss: 0.255
[51,     1] loss: 0.286
[52,     1] loss: 0.284
[53,     1] loss: 0.203
[54,     1] loss: 0.221
[55,     1] loss: 0.268
[56,     1] loss: 0.216
[57,     1] loss: 0.274
[58,     1] loss: 0.232
[59,     1] loss: 0.220
[60,     1] loss: 0.191
[61,     1] loss: 0.176
[62,     1] loss: 0.209
[63,     1] loss: 0.165
[64,     1] loss: 0.171
[65,     1] loss: 0.184
[66,     1] loss: 0.188
[67,     1] loss: 0.169
[68,     1] loss: 0.165
[69,     1] loss: 0.170
[70,     1] loss: 0.163
[71,     1] loss: 0.111
[72,     1] loss: 0.105
[73,     1] loss: 0.114
[74,     1] loss: 0.098
[75,     1] loss: 0.141
[76,     1] loss: 0.254
[77,     1] loss: 0.600
[78,     1] loss: 0.366
[79,     1] loss: 0.487
[80,     1] loss: 0.362
[81,     1] loss: 0.273
[82,     1] loss: 0.335
[83,     1] loss: 0.307
[84,     1] loss: 0.317
[85,     1] loss: 0.302
[86,     1] loss: 0.270
[87,     1] loss: 0.235
[88,     1] loss: 0.261
[89,     1] loss: 0.219
[90,     1] loss: 0.202
[91,     1] loss: 0.182
[92,     1] loss: 0.167
[93,     1] loss: 0.179
[94,     1] loss: 0.227
[95,     1] loss: 0.161
[96,     1] loss: 0.237
[97,     1] loss: 0.183
[98,     1] loss: 0.156
[99,     1] loss: 0.161
[100,     1] loss: 0.203
[101,     1] loss: 0.163
[102,     1] loss: 0.165
[103,     1] loss: 0.163
[104,     1] loss: 0.166
[105,     1] loss: 0.198
[106,     1] loss: 0.263
[107,     1] loss: 0.188
[108,     1] loss: 0.227
[109,     1] loss: 0.239
[110,     1] loss: 0.175
[111,     1] loss: 0.210
[112,     1] loss: 0.203
[113,     1] loss: 0.182
[114,     1] loss: 0.216
[115,     1] loss: 0.187
[116,     1] loss: 0.241
[117,     1] loss: 0.174
[118,     1] loss: 0.151
[119,     1] loss: 0.192
[120,     1] loss: 0.143
[121,     1] loss: 0.145
[122,     1] loss: 0.142
[123,     1] loss: 0.119
[124,     1] loss: 0.142
[125,     1] loss: 0.179
[126,     1] loss: 0.159
[127,     1] loss: 0.149
[128,     1] loss: 0.167
[129,     1] loss: 0.149
[130,     1] loss: 0.169
[131,     1] loss: 0.174
[132,     1] loss: 0.148
[133,     1] loss: 0.184
[134,     1] loss: 0.126
[135,     1] loss: 0.177
[136,     1] loss: 0.171
[137,     1] loss: 0.154
[138,     1] loss: 0.128
[139,     1] loss: 0.167
[140,     1] loss: 0.130
[141,     1] loss: 0.172
[142,     1] loss: 0.147
[143,     1] loss: 0.128
[144,     1] loss: 0.131
[145,     1] loss: 0.136
[146,     1] loss: 0.123
[147,     1] loss: 0.123
[148,     1] loss: 0.122
[149,     1] loss: 0.179
[150,     1] loss: 0.159
[151,     1] loss: 0.159
[152,     1] loss: 0.132
[153,     1] loss: 0.144
[154,     1] loss: 0.137
[155,     1] loss: 0.161
[156,     1] loss: 0.182
[157,     1] loss: 0.134
[158,     1] loss: 0.132
[159,     1] loss: 0.144
[160,     1] loss: 0.153
[161,     1] loss: 0.172
[162,     1] loss: 0.174
[163,     1] loss: 0.159
[164,     1] loss: 0.126
[165,     1] loss: 0.169
[166,     1] loss: 0.147
[167,     1] loss: 0.153
[168,     1] loss: 0.100
[169,     1] loss: 0.175
[170,     1] loss: 0.108
[171,     1] loss: 0.249
[172,     1] loss: 0.160
[173,     1] loss: 0.176
[174,     1] loss: 0.148
[175,     1] loss: 0.120
[176,     1] loss: 0.191
[177,     1] loss: 0.177
[178,     1] loss: 0.186
[179,     1] loss: 0.156
[180,     1] loss: 0.128
[181,     1] loss: 0.147
[182,     1] loss: 0.145
[183,     1] loss: 0.179
[184,     1] loss: 0.106
[185,     1] loss: 0.349
[186,     1] loss: 0.239
[187,     1] loss: 0.262
[188,     1] loss: 0.179
[189,     1] loss: 0.240
[190,     1] loss: 0.229
[191,     1] loss: 0.166
[192,     1] loss: 0.228
[193,     1] loss: 0.191
[194,     1] loss: 0.178
Early stopping applied (best metric=0.28336280584335327)
Finished Training
Total time taken: 22.766648530960083
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.702
[3,     1] loss: 0.690
[4,     1] loss: 0.677
[5,     1] loss: 0.656
[6,     1] loss: 0.637
[7,     1] loss: 0.613
[8,     1] loss: 0.592
[9,     1] loss: 0.553
[10,     1] loss: 0.503
[11,     1] loss: 0.522
[12,     1] loss: 0.451
[13,     1] loss: 0.414
[14,     1] loss: 0.430
[15,     1] loss: 0.351
[16,     1] loss: 0.389
[17,     1] loss: 0.383
[18,     1] loss: 0.392
[19,     1] loss: 0.390
[20,     1] loss: 0.285
[21,     1] loss: 0.432
[22,     1] loss: 0.275
[23,     1] loss: 0.342
[24,     1] loss: 0.300
[25,     1] loss: 0.260
[26,     1] loss: 0.244
[27,     1] loss: 0.380
[28,     1] loss: 0.242
[29,     1] loss: 0.282
[30,     1] loss: 0.272
[31,     1] loss: 0.270
[32,     1] loss: 0.272
[33,     1] loss: 0.266
[34,     1] loss: 0.248
[35,     1] loss: 0.245
[36,     1] loss: 0.284
[37,     1] loss: 0.263
[38,     1] loss: 0.196
[39,     1] loss: 0.171
[40,     1] loss: 0.184
[41,     1] loss: 0.171
[42,     1] loss: 0.122
[43,     1] loss: 0.138
[44,     1] loss: 0.174
[45,     1] loss: 0.141
[46,     1] loss: 0.188
[47,     1] loss: 0.173
[48,     1] loss: 0.261
[49,     1] loss: 0.193
[50,     1] loss: 0.193
[51,     1] loss: 0.168
[52,     1] loss: 0.167
[53,     1] loss: 0.201
[54,     1] loss: 0.175
[55,     1] loss: 0.194
[56,     1] loss: 0.149
[57,     1] loss: 0.212
[58,     1] loss: 0.169
[59,     1] loss: 0.229
[60,     1] loss: 0.163
[61,     1] loss: 0.224
[62,     1] loss: 0.237
[63,     1] loss: 0.178
[64,     1] loss: 0.184
[65,     1] loss: 0.221
[66,     1] loss: 0.161
[67,     1] loss: 0.177
[68,     1] loss: 0.182
[69,     1] loss: 0.214
[70,     1] loss: 0.231
[71,     1] loss: 0.162
[72,     1] loss: 0.258
[73,     1] loss: 0.350
[74,     1] loss: 0.362
[75,     1] loss: 0.332
[76,     1] loss: 0.349
[77,     1] loss: 0.350
[78,     1] loss: 0.313
[79,     1] loss: 0.308
[80,     1] loss: 0.301
[81,     1] loss: 0.269
[82,     1] loss: 0.302
[83,     1] loss: 0.321
[84,     1] loss: 0.305
[85,     1] loss: 0.247
[86,     1] loss: 0.246
[87,     1] loss: 0.224
[88,     1] loss: 0.234
[89,     1] loss: 0.251
[90,     1] loss: 0.239
[91,     1] loss: 0.204
[92,     1] loss: 0.255
[93,     1] loss: 0.205
[94,     1] loss: 0.259
[95,     1] loss: 0.207
[96,     1] loss: 0.192
[97,     1] loss: 0.205
[98,     1] loss: 0.171
[99,     1] loss: 0.198
[100,     1] loss: 0.255
[101,     1] loss: 0.196
[102,     1] loss: 0.210
[103,     1] loss: 0.199
[104,     1] loss: 0.180
[105,     1] loss: 0.178
[106,     1] loss: 0.172
[107,     1] loss: 0.157
[108,     1] loss: 0.128
[109,     1] loss: 0.139
[110,     1] loss: 0.150
[111,     1] loss: 0.593
[112,     1] loss: 0.391
[113,     1] loss: 0.343
[114,     1] loss: 0.378
[115,     1] loss: 0.310
[116,     1] loss: 0.294
[117,     1] loss: 0.298
[118,     1] loss: 0.333
[119,     1] loss: 0.293
[120,     1] loss: 0.324
[121,     1] loss: 0.295
[122,     1] loss: 0.243
[123,     1] loss: 0.272
[124,     1] loss: 0.218
[125,     1] loss: 0.246
[126,     1] loss: 0.256
[127,     1] loss: 0.177
[128,     1] loss: 0.215
[129,     1] loss: 0.205
[130,     1] loss: 0.171
[131,     1] loss: 0.188
[132,     1] loss: 0.179
[133,     1] loss: 0.205
[134,     1] loss: 0.215
[135,     1] loss: 0.210
[136,     1] loss: 0.181
[137,     1] loss: 0.167
[138,     1] loss: 0.146
[139,     1] loss: 0.175
[140,     1] loss: 0.191
[141,     1] loss: 0.192
[142,     1] loss: 0.157
[143,     1] loss: 0.168
[144,     1] loss: 0.184
[145,     1] loss: 0.138
[146,     1] loss: 0.201
[147,     1] loss: 0.175
[148,     1] loss: 0.149
[149,     1] loss: 0.134
[150,     1] loss: 0.189
[151,     1] loss: 0.369
[152,     1] loss: 0.422
[153,     1] loss: 0.317
[154,     1] loss: 0.313
[155,     1] loss: 0.390
[156,     1] loss: 0.304
[157,     1] loss: 0.275
[158,     1] loss: 0.290
[159,     1] loss: 0.309
[160,     1] loss: 0.285
[161,     1] loss: 0.278
[162,     1] loss: 0.266
[163,     1] loss: 0.228
[164,     1] loss: 0.228
[165,     1] loss: 0.207
[166,     1] loss: 0.249
[167,     1] loss: 0.188
[168,     1] loss: 0.194
[169,     1] loss: 0.175
[170,     1] loss: 0.189
[171,     1] loss: 0.176
[172,     1] loss: 0.181
[173,     1] loss: 0.151
[174,     1] loss: 0.154
[175,     1] loss: 0.176
[176,     1] loss: 0.221
[177,     1] loss: 0.413
[178,     1] loss: 0.151
[179,     1] loss: 0.282
[180,     1] loss: 0.184
[181,     1] loss: 0.244
[182,     1] loss: 0.224
[183,     1] loss: 0.246
[184,     1] loss: 0.224
[185,     1] loss: 0.178
[186,     1] loss: 0.146
[187,     1] loss: 0.146
[188,     1] loss: 0.161
[189,     1] loss: 0.161
[190,     1] loss: 0.217
[191,     1] loss: 0.108
[192,     1] loss: 0.203
[193,     1] loss: 0.200
Early stopping applied (best metric=0.09415058046579361)
Finished Training
Total time taken: 22.49859118461609
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.676
[4,     1] loss: 0.656
[5,     1] loss: 0.588
[6,     1] loss: 0.560
[7,     1] loss: 0.511
[8,     1] loss: 0.498
[9,     1] loss: 0.409
[10,     1] loss: 0.417
[11,     1] loss: 0.405
[12,     1] loss: 0.336
[13,     1] loss: 0.370
[14,     1] loss: 0.326
[15,     1] loss: 0.388
[16,     1] loss: 0.337
[17,     1] loss: 0.395
[18,     1] loss: 0.346
[19,     1] loss: 0.369
[20,     1] loss: 0.414
[21,     1] loss: 0.316
[22,     1] loss: 0.321
[23,     1] loss: 0.338
[24,     1] loss: 0.395
[25,     1] loss: 0.348
[26,     1] loss: 0.323
[27,     1] loss: 0.430
[28,     1] loss: 0.349
[29,     1] loss: 0.265
[30,     1] loss: 0.321
[31,     1] loss: 0.330
[32,     1] loss: 0.335
[33,     1] loss: 0.351
[34,     1] loss: 0.390
[35,     1] loss: 0.293
[36,     1] loss: 0.300
[37,     1] loss: 0.323
[38,     1] loss: 0.265
[39,     1] loss: 0.328
[40,     1] loss: 0.348
[41,     1] loss: 0.368
[42,     1] loss: 0.317
[43,     1] loss: 0.288
[44,     1] loss: 0.267
[45,     1] loss: 0.271
[46,     1] loss: 0.241
[47,     1] loss: 0.267
[48,     1] loss: 0.415
[49,     1] loss: 0.273
[50,     1] loss: 0.310
[51,     1] loss: 0.314
[52,     1] loss: 0.291
[53,     1] loss: 0.342
[54,     1] loss: 0.258
[55,     1] loss: 0.291
[56,     1] loss: 0.290
[57,     1] loss: 0.310
[58,     1] loss: 0.243
[59,     1] loss: 0.277
[60,     1] loss: 0.218
[61,     1] loss: 0.314
[62,     1] loss: 0.207
[63,     1] loss: 0.300
[64,     1] loss: 0.204
[65,     1] loss: 0.284
[66,     1] loss: 0.235
[67,     1] loss: 0.199
[68,     1] loss: 0.228
[69,     1] loss: 0.244
[70,     1] loss: 0.193
[71,     1] loss: 0.171
[72,     1] loss: 0.217
[73,     1] loss: 0.264
[74,     1] loss: 0.196
[75,     1] loss: 0.196
[76,     1] loss: 0.184
[77,     1] loss: 0.174
[78,     1] loss: 0.138
[79,     1] loss: 0.130
[80,     1] loss: 0.132
[81,     1] loss: 0.175
[82,     1] loss: 0.133
[83,     1] loss: 0.147
[84,     1] loss: 0.407
[85,     1] loss: 0.674
[86,     1] loss: 0.294
[87,     1] loss: 0.302
[88,     1] loss: 0.338
[89,     1] loss: 0.270
[90,     1] loss: 0.297
[91,     1] loss: 0.323
[92,     1] loss: 0.290
[93,     1] loss: 0.290
[94,     1] loss: 0.280
[95,     1] loss: 0.259
[96,     1] loss: 0.292
[97,     1] loss: 0.226
[98,     1] loss: 0.223
[99,     1] loss: 0.179
[100,     1] loss: 0.182
[101,     1] loss: 0.163
Early stopping applied (best metric=0.34913748502731323)
Finished Training
Total time taken: 11.959615468978882
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.705
[3,     1] loss: 0.682
[4,     1] loss: 0.681
[5,     1] loss: 0.676
[6,     1] loss: 0.664
[7,     1] loss: 0.634
[8,     1] loss: 0.620
[9,     1] loss: 0.580
[10,     1] loss: 0.562
[11,     1] loss: 0.475
[12,     1] loss: 0.419
[13,     1] loss: 0.422
[14,     1] loss: 0.334
[15,     1] loss: 0.345
[16,     1] loss: 0.327
[17,     1] loss: 0.314
[18,     1] loss: 0.328
[19,     1] loss: 0.323
[20,     1] loss: 0.276
[21,     1] loss: 0.317
[22,     1] loss: 0.296
[23,     1] loss: 0.275
[24,     1] loss: 0.312
[25,     1] loss: 0.241
[26,     1] loss: 0.241
[27,     1] loss: 0.277
[28,     1] loss: 0.199
[29,     1] loss: 0.291
[30,     1] loss: 0.207
[31,     1] loss: 0.221
[32,     1] loss: 0.225
[33,     1] loss: 0.235
[34,     1] loss: 0.233
[35,     1] loss: 0.199
[36,     1] loss: 0.236
[37,     1] loss: 0.196
[38,     1] loss: 0.174
[39,     1] loss: 0.205
[40,     1] loss: 0.165
[41,     1] loss: 0.192
[42,     1] loss: 0.176
[43,     1] loss: 0.130
[44,     1] loss: 0.182
[45,     1] loss: 0.109
[46,     1] loss: 0.208
[47,     1] loss: 0.182
[48,     1] loss: 0.201
[49,     1] loss: 0.148
[50,     1] loss: 0.252
[51,     1] loss: 0.178
[52,     1] loss: 0.174
[53,     1] loss: 0.176
[54,     1] loss: 0.258
[55,     1] loss: 0.119
[56,     1] loss: 0.197
[57,     1] loss: 0.141
[58,     1] loss: 0.205
[59,     1] loss: 0.211
[60,     1] loss: 0.196
[61,     1] loss: 0.147
[62,     1] loss: 0.253
[63,     1] loss: 0.175
[64,     1] loss: 0.169
[65,     1] loss: 0.183
[66,     1] loss: 0.158
Early stopping applied (best metric=0.3042293190956116)
Finished Training
Total time taken: 7.57246470451355
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.695
[3,     1] loss: 0.684
[4,     1] loss: 0.672
[5,     1] loss: 0.649
[6,     1] loss: 0.620
[7,     1] loss: 0.590
[8,     1] loss: 0.534
[9,     1] loss: 0.497
[10,     1] loss: 0.448
[11,     1] loss: 0.426
[12,     1] loss: 0.349
[13,     1] loss: 0.314
[14,     1] loss: 0.284
[15,     1] loss: 0.287
[16,     1] loss: 0.262
[17,     1] loss: 0.286
[18,     1] loss: 0.287
[19,     1] loss: 0.265
[20,     1] loss: 0.222
[21,     1] loss: 0.242
[22,     1] loss: 0.232
[23,     1] loss: 0.266
[24,     1] loss: 0.247
[25,     1] loss: 0.244
[26,     1] loss: 0.268
[27,     1] loss: 0.308
[28,     1] loss: 0.255
[29,     1] loss: 0.213
[30,     1] loss: 0.207
[31,     1] loss: 0.173
[32,     1] loss: 0.239
[33,     1] loss: 0.219
[34,     1] loss: 0.204
[35,     1] loss: 0.214
[36,     1] loss: 0.159
[37,     1] loss: 0.249
[38,     1] loss: 0.220
[39,     1] loss: 0.354
[40,     1] loss: 0.266
[41,     1] loss: 0.306
[42,     1] loss: 0.250
[43,     1] loss: 0.232
[44,     1] loss: 0.250
[45,     1] loss: 0.210
[46,     1] loss: 0.218
[47,     1] loss: 0.212
[48,     1] loss: 0.216
[49,     1] loss: 0.232
[50,     1] loss: 0.164
[51,     1] loss: 0.187
[52,     1] loss: 0.191
[53,     1] loss: 0.137
[54,     1] loss: 0.151
[55,     1] loss: 0.141
[56,     1] loss: 0.136
[57,     1] loss: 0.295
[58,     1] loss: 0.274
[59,     1] loss: 0.301
[60,     1] loss: 0.270
Early stopping applied (best metric=0.43424832820892334)
Finished Training
Total time taken: 7.132381916046143
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.685
[4,     1] loss: 0.653
[5,     1] loss: 0.622
[6,     1] loss: 0.591
[7,     1] loss: 0.550
[8,     1] loss: 0.471
[9,     1] loss: 0.448
[10,     1] loss: 0.391
[11,     1] loss: 0.357
[12,     1] loss: 0.361
[13,     1] loss: 0.397
[14,     1] loss: 0.347
[15,     1] loss: 0.378
[16,     1] loss: 0.299
[17,     1] loss: 0.349
[18,     1] loss: 0.314
[19,     1] loss: 0.333
[20,     1] loss: 0.323
[21,     1] loss: 0.333
[22,     1] loss: 0.292
[23,     1] loss: 0.280
[24,     1] loss: 0.241
[25,     1] loss: 0.246
[26,     1] loss: 0.220
[27,     1] loss: 0.222
[28,     1] loss: 0.256
[29,     1] loss: 0.189
[30,     1] loss: 0.272
[31,     1] loss: 0.252
[32,     1] loss: 0.182
[33,     1] loss: 0.199
[34,     1] loss: 0.178
[35,     1] loss: 0.215
[36,     1] loss: 0.210
[37,     1] loss: 0.186
[38,     1] loss: 0.167
[39,     1] loss: 0.190
[40,     1] loss: 0.157
[41,     1] loss: 0.172
[42,     1] loss: 0.165
[43,     1] loss: 0.167
[44,     1] loss: 0.162
[45,     1] loss: 0.149
[46,     1] loss: 0.410
[47,     1] loss: 0.425
[48,     1] loss: 0.256
[49,     1] loss: 0.257
[50,     1] loss: 0.289
[51,     1] loss: 0.315
[52,     1] loss: 0.255
[53,     1] loss: 0.265
[54,     1] loss: 0.261
[55,     1] loss: 0.253
[56,     1] loss: 0.207
[57,     1] loss: 0.196
[58,     1] loss: 0.170
[59,     1] loss: 0.138
[60,     1] loss: 0.179
[61,     1] loss: 0.135
[62,     1] loss: 0.131
[63,     1] loss: 0.162
[64,     1] loss: 0.144
[65,     1] loss: 0.123
[66,     1] loss: 0.128
[67,     1] loss: 0.103
[68,     1] loss: 0.119
[69,     1] loss: 0.196
[70,     1] loss: 0.510
[71,     1] loss: 0.226
[72,     1] loss: 0.291
[73,     1] loss: 0.237
[74,     1] loss: 0.284
[75,     1] loss: 0.351
[76,     1] loss: 0.241
[77,     1] loss: 0.240
[78,     1] loss: 0.339
[79,     1] loss: 0.275
[80,     1] loss: 0.247
[81,     1] loss: 0.213
[82,     1] loss: 0.240
[83,     1] loss: 0.231
[84,     1] loss: 0.192
[85,     1] loss: 0.195
[86,     1] loss: 0.191
[87,     1] loss: 0.165
[88,     1] loss: 0.190
[89,     1] loss: 0.123
[90,     1] loss: 0.150
[91,     1] loss: 0.126
[92,     1] loss: 0.121
[93,     1] loss: 0.208
[94,     1] loss: 0.160
[95,     1] loss: 0.147
[96,     1] loss: 0.163
[97,     1] loss: 0.134
[98,     1] loss: 0.132
[99,     1] loss: 0.156
[100,     1] loss: 0.137
[101,     1] loss: 0.145
[102,     1] loss: 0.109
[103,     1] loss: 0.150
[104,     1] loss: 0.179
[105,     1] loss: 0.108
[106,     1] loss: 0.172
[107,     1] loss: 0.111
[108,     1] loss: 0.185
[109,     1] loss: 0.109
[110,     1] loss: 0.124
[111,     1] loss: 0.115
[112,     1] loss: 0.097
[113,     1] loss: 0.219
[114,     1] loss: 0.495
[115,     1] loss: 0.355
[116,     1] loss: 0.368
[117,     1] loss: 0.257
[118,     1] loss: 0.264
[119,     1] loss: 0.258
[120,     1] loss: 0.274
[121,     1] loss: 0.262
[122,     1] loss: 0.289
[123,     1] loss: 0.250
[124,     1] loss: 0.231
[125,     1] loss: 0.253
[126,     1] loss: 0.242
[127,     1] loss: 0.173
[128,     1] loss: 0.198
[129,     1] loss: 0.193
[130,     1] loss: 0.187
Early stopping applied (best metric=0.29042547941207886)
Finished Training
Total time taken: 15.632386445999146
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.689
[3,     1] loss: 0.682
[4,     1] loss: 0.665
[5,     1] loss: 0.624
[6,     1] loss: 0.552
[7,     1] loss: 0.538
[8,     1] loss: 0.524
[9,     1] loss: 0.458
[10,     1] loss: 0.443
[11,     1] loss: 0.435
[12,     1] loss: 0.362
[13,     1] loss: 0.326
[14,     1] loss: 0.345
[15,     1] loss: 0.273
[16,     1] loss: 0.269
[17,     1] loss: 0.246
[18,     1] loss: 0.272
[19,     1] loss: 0.238
[20,     1] loss: 0.206
[21,     1] loss: 0.213
[22,     1] loss: 0.341
[23,     1] loss: 0.256
[24,     1] loss: 0.325
[25,     1] loss: 0.337
[26,     1] loss: 0.291
[27,     1] loss: 0.301
[28,     1] loss: 0.329
[29,     1] loss: 0.273
[30,     1] loss: 0.268
[31,     1] loss: 0.280
[32,     1] loss: 0.267
[33,     1] loss: 0.277
[34,     1] loss: 0.294
[35,     1] loss: 0.209
[36,     1] loss: 0.251
[37,     1] loss: 0.236
[38,     1] loss: 0.226
[39,     1] loss: 0.190
[40,     1] loss: 0.242
[41,     1] loss: 0.257
[42,     1] loss: 0.193
[43,     1] loss: 0.207
[44,     1] loss: 0.173
[45,     1] loss: 0.213
[46,     1] loss: 0.207
[47,     1] loss: 0.194
[48,     1] loss: 0.169
[49,     1] loss: 0.246
[50,     1] loss: 0.212
[51,     1] loss: 0.211
[52,     1] loss: 0.210
[53,     1] loss: 0.199
[54,     1] loss: 0.224
[55,     1] loss: 0.185
[56,     1] loss: 0.174
[57,     1] loss: 0.129
[58,     1] loss: 0.211
[59,     1] loss: 0.219
[60,     1] loss: 0.255
[61,     1] loss: 0.223
[62,     1] loss: 0.164
[63,     1] loss: 0.189
[64,     1] loss: 0.181
[65,     1] loss: 0.177
[66,     1] loss: 0.183
[67,     1] loss: 0.166
[68,     1] loss: 0.205
[69,     1] loss: 0.145
[70,     1] loss: 0.169
[71,     1] loss: 0.183
[72,     1] loss: 0.128
[73,     1] loss: 0.160
[74,     1] loss: 0.173
[75,     1] loss: 0.147
[76,     1] loss: 0.140
[77,     1] loss: 0.156
[78,     1] loss: 0.133
[79,     1] loss: 0.157
[80,     1] loss: 0.152
[81,     1] loss: 0.179
[82,     1] loss: 0.132
[83,     1] loss: 0.130
[84,     1] loss: 0.150
[85,     1] loss: 0.217
[86,     1] loss: 0.263
[87,     1] loss: 0.393
[88,     1] loss: 0.236
[89,     1] loss: 0.304
[90,     1] loss: 0.235
[91,     1] loss: 0.281
[92,     1] loss: 0.227
[93,     1] loss: 0.253
[94,     1] loss: 0.232
[95,     1] loss: 0.224
[96,     1] loss: 0.211
[97,     1] loss: 0.171
[98,     1] loss: 0.206
[99,     1] loss: 0.186
[100,     1] loss: 0.146
[101,     1] loss: 0.183
[102,     1] loss: 0.130
[103,     1] loss: 0.198
[104,     1] loss: 0.192
[105,     1] loss: 0.190
[106,     1] loss: 0.121
[107,     1] loss: 0.132
[108,     1] loss: 0.100
[109,     1] loss: 0.129
[110,     1] loss: 0.155
[111,     1] loss: 0.114
Early stopping applied (best metric=0.13493354618549347)
Finished Training
Total time taken: 13.12544560432434
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.685
[3,     1] loss: 0.665
[4,     1] loss: 0.610
[5,     1] loss: 0.589
[6,     1] loss: 0.551
[7,     1] loss: 0.480
[8,     1] loss: 0.430
[9,     1] loss: 0.431
[10,     1] loss: 0.395
[11,     1] loss: 0.410
[12,     1] loss: 0.410
[13,     1] loss: 0.403
[14,     1] loss: 0.339
[15,     1] loss: 0.336
[16,     1] loss: 0.355
[17,     1] loss: 0.319
[18,     1] loss: 0.334
[19,     1] loss: 0.328
[20,     1] loss: 0.285
[21,     1] loss: 0.280
[22,     1] loss: 0.286
[23,     1] loss: 0.331
[24,     1] loss: 0.219
[25,     1] loss: 0.261
[26,     1] loss: 0.248
[27,     1] loss: 0.191
[28,     1] loss: 0.232
[29,     1] loss: 0.289
[30,     1] loss: 0.251
[31,     1] loss: 0.189
[32,     1] loss: 0.193
[33,     1] loss: 0.172
[34,     1] loss: 0.251
[35,     1] loss: 0.183
[36,     1] loss: 0.195
[37,     1] loss: 0.158
[38,     1] loss: 0.229
[39,     1] loss: 0.169
[40,     1] loss: 0.195
[41,     1] loss: 0.144
[42,     1] loss: 0.165
[43,     1] loss: 0.210
[44,     1] loss: 0.188
[45,     1] loss: 0.269
[46,     1] loss: 0.476
[47,     1] loss: 0.290
[48,     1] loss: 0.388
[49,     1] loss: 0.239
[50,     1] loss: 0.373
[51,     1] loss: 0.309
[52,     1] loss: 0.292
[53,     1] loss: 0.303
[54,     1] loss: 0.377
[55,     1] loss: 0.290
[56,     1] loss: 0.282
[57,     1] loss: 0.244
[58,     1] loss: 0.244
Early stopping applied (best metric=0.2920357584953308)
Finished Training
Total time taken: 6.967054843902588
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.694
[3,     1] loss: 0.685
[4,     1] loss: 0.677
[5,     1] loss: 0.654
[6,     1] loss: 0.637
[7,     1] loss: 0.580
[8,     1] loss: 0.551
[9,     1] loss: 0.535
[10,     1] loss: 0.419
[11,     1] loss: 0.470
[12,     1] loss: 0.397
[13,     1] loss: 0.356
[14,     1] loss: 0.412
[15,     1] loss: 0.396
[16,     1] loss: 0.316
[17,     1] loss: 0.428
[18,     1] loss: 0.370
[19,     1] loss: 0.374
[20,     1] loss: 0.340
[21,     1] loss: 0.322
[22,     1] loss: 0.285
[23,     1] loss: 0.324
[24,     1] loss: 0.279
[25,     1] loss: 0.243
[26,     1] loss: 0.273
[27,     1] loss: 0.307
[28,     1] loss: 0.224
[29,     1] loss: 0.377
[30,     1] loss: 0.341
[31,     1] loss: 0.424
[32,     1] loss: 0.293
[33,     1] loss: 0.334
[34,     1] loss: 0.385
[35,     1] loss: 0.400
[36,     1] loss: 0.381
[37,     1] loss: 0.348
[38,     1] loss: 0.337
[39,     1] loss: 0.364
[40,     1] loss: 0.339
[41,     1] loss: 0.351
[42,     1] loss: 0.298
[43,     1] loss: 0.275
[44,     1] loss: 0.258
[45,     1] loss: 0.272
[46,     1] loss: 0.239
[47,     1] loss: 0.230
[48,     1] loss: 0.215
[49,     1] loss: 0.226
[50,     1] loss: 0.251
[51,     1] loss: 0.247
[52,     1] loss: 0.203
[53,     1] loss: 0.310
[54,     1] loss: 0.185
[55,     1] loss: 0.213
[56,     1] loss: 0.261
[57,     1] loss: 0.220
[58,     1] loss: 0.204
[59,     1] loss: 0.182
[60,     1] loss: 0.184
[61,     1] loss: 0.191
[62,     1] loss: 0.156
[63,     1] loss: 0.189
[64,     1] loss: 0.160
[65,     1] loss: 0.119
[66,     1] loss: 0.172
[67,     1] loss: 0.119
[68,     1] loss: 0.167
[69,     1] loss: 0.224
[70,     1] loss: 0.172
[71,     1] loss: 0.247
[72,     1] loss: 0.195
[73,     1] loss: 0.200
[74,     1] loss: 0.184
[75,     1] loss: 0.194
[76,     1] loss: 0.173
[77,     1] loss: 0.214
[78,     1] loss: 0.157
[79,     1] loss: 0.180
[80,     1] loss: 0.238
[81,     1] loss: 0.194
[82,     1] loss: 0.193
[83,     1] loss: 0.127
[84,     1] loss: 0.186
[85,     1] loss: 0.174
[86,     1] loss: 0.154
[87,     1] loss: 0.154
[88,     1] loss: 0.193
[89,     1] loss: 0.149
[90,     1] loss: 0.109
[91,     1] loss: 0.156
[92,     1] loss: 0.218
[93,     1] loss: 0.164
[94,     1] loss: 0.131
[95,     1] loss: 0.143
[96,     1] loss: 0.157
[97,     1] loss: 0.113
[98,     1] loss: 0.162
[99,     1] loss: 0.123
[100,     1] loss: 0.124
[101,     1] loss: 0.137
[102,     1] loss: 0.092
[103,     1] loss: 0.162
[104,     1] loss: 0.199
[105,     1] loss: 0.315
[106,     1] loss: 0.207
[107,     1] loss: 0.167
[108,     1] loss: 0.218
[109,     1] loss: 0.178
[110,     1] loss: 0.188
[111,     1] loss: 0.151
[112,     1] loss: 0.160
[113,     1] loss: 0.150
[114,     1] loss: 0.141
[115,     1] loss: 0.141
[116,     1] loss: 0.140
[117,     1] loss: 0.152
[118,     1] loss: 0.165
[119,     1] loss: 0.143
[120,     1] loss: 0.153
[121,     1] loss: 0.164
Early stopping applied (best metric=0.2840026915073395)
Finished Training
Total time taken: 14.530681133270264
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.712
[3,     1] loss: 0.687
[4,     1] loss: 0.693
[5,     1] loss: 0.693
[6,     1] loss: 0.688
[7,     1] loss: 0.689
[8,     1] loss: 0.675
[9,     1] loss: 0.671
[10,     1] loss: 0.648
[11,     1] loss: 0.626
[12,     1] loss: 0.616
[13,     1] loss: 0.572
[14,     1] loss: 0.519
[15,     1] loss: 0.471
[16,     1] loss: 0.416
[17,     1] loss: 0.406
[18,     1] loss: 0.378
[19,     1] loss: 0.397
[20,     1] loss: 0.302
[21,     1] loss: 0.328
[22,     1] loss: 0.388
[23,     1] loss: 0.360
[24,     1] loss: 0.254
[25,     1] loss: 0.297
[26,     1] loss: 0.338
[27,     1] loss: 0.328
[28,     1] loss: 0.306
[29,     1] loss: 0.357
[30,     1] loss: 0.254
[31,     1] loss: 0.288
[32,     1] loss: 0.261
[33,     1] loss: 0.225
[34,     1] loss: 0.235
[35,     1] loss: 0.312
[36,     1] loss: 0.247
[37,     1] loss: 0.257
[38,     1] loss: 0.238
[39,     1] loss: 0.221
[40,     1] loss: 0.206
[41,     1] loss: 0.239
[42,     1] loss: 0.197
[43,     1] loss: 0.215
[44,     1] loss: 0.237
[45,     1] loss: 0.267
[46,     1] loss: 0.210
[47,     1] loss: 0.227
[48,     1] loss: 0.182
[49,     1] loss: 0.171
[50,     1] loss: 0.164
[51,     1] loss: 0.210
[52,     1] loss: 0.155
[53,     1] loss: 0.204
[54,     1] loss: 0.215
[55,     1] loss: 0.349
[56,     1] loss: 0.312
[57,     1] loss: 0.226
[58,     1] loss: 0.245
[59,     1] loss: 0.229
[60,     1] loss: 0.218
[61,     1] loss: 0.258
[62,     1] loss: 0.257
[63,     1] loss: 0.205
[64,     1] loss: 0.257
[65,     1] loss: 0.205
[66,     1] loss: 0.204
Early stopping applied (best metric=0.4254237413406372)
Finished Training
Total time taken: 7.773525953292847
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.663
[4,     1] loss: 0.611
[5,     1] loss: 0.576
[6,     1] loss: 0.529
[7,     1] loss: 0.474
[8,     1] loss: 0.395
[9,     1] loss: 0.417
[10,     1] loss: 0.428
[11,     1] loss: 0.369
[12,     1] loss: 0.352
[13,     1] loss: 0.403
[14,     1] loss: 0.397
[15,     1] loss: 0.384
[16,     1] loss: 0.278
[17,     1] loss: 0.308
[18,     1] loss: 0.312
[19,     1] loss: 0.233
[20,     1] loss: 0.274
[21,     1] loss: 0.281
[22,     1] loss: 0.212
[23,     1] loss: 0.227
[24,     1] loss: 0.234
[25,     1] loss: 0.193
[26,     1] loss: 0.219
[27,     1] loss: 0.203
[28,     1] loss: 0.193
[29,     1] loss: 0.183
[30,     1] loss: 0.216
[31,     1] loss: 0.250
[32,     1] loss: 0.184
[33,     1] loss: 0.210
[34,     1] loss: 0.257
[35,     1] loss: 0.254
[36,     1] loss: 0.269
[37,     1] loss: 0.231
[38,     1] loss: 0.227
[39,     1] loss: 0.190
[40,     1] loss: 0.175
[41,     1] loss: 0.186
[42,     1] loss: 0.205
[43,     1] loss: 0.159
[44,     1] loss: 0.326
[45,     1] loss: 0.266
[46,     1] loss: 0.397
[47,     1] loss: 0.276
[48,     1] loss: 0.331
[49,     1] loss: 0.306
[50,     1] loss: 0.288
[51,     1] loss: 0.237
[52,     1] loss: 0.249
[53,     1] loss: 0.271
[54,     1] loss: 0.238
[55,     1] loss: 0.268
[56,     1] loss: 0.219
[57,     1] loss: 0.307
[58,     1] loss: 0.217
[59,     1] loss: 0.174
[60,     1] loss: 0.195
[61,     1] loss: 0.182
[62,     1] loss: 0.218
[63,     1] loss: 0.198
[64,     1] loss: 0.200
Early stopping applied (best metric=0.3602863550186157)
Finished Training
Total time taken: 7.581088304519653
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.651
[5,     1] loss: 0.603
[6,     1] loss: 0.539
[7,     1] loss: 0.516
[8,     1] loss: 0.451
[9,     1] loss: 0.397
[10,     1] loss: 0.398
[11,     1] loss: 0.306
[12,     1] loss: 0.344
[13,     1] loss: 0.264
[14,     1] loss: 0.446
[15,     1] loss: 0.284
[16,     1] loss: 0.322
[17,     1] loss: 0.268
[18,     1] loss: 0.284
[19,     1] loss: 0.285
[20,     1] loss: 0.231
[21,     1] loss: 0.209
[22,     1] loss: 0.214
[23,     1] loss: 0.238
[24,     1] loss: 0.218
[25,     1] loss: 0.261
[26,     1] loss: 0.258
[27,     1] loss: 0.206
[28,     1] loss: 0.214
[29,     1] loss: 0.190
[30,     1] loss: 0.231
[31,     1] loss: 0.246
[32,     1] loss: 0.260
[33,     1] loss: 0.216
[34,     1] loss: 0.146
[35,     1] loss: 0.193
[36,     1] loss: 0.221
[37,     1] loss: 0.235
[38,     1] loss: 0.229
[39,     1] loss: 0.184
[40,     1] loss: 0.149
[41,     1] loss: 0.175
[42,     1] loss: 0.238
[43,     1] loss: 0.283
[44,     1] loss: 0.290
[45,     1] loss: 0.182
[46,     1] loss: 0.229
[47,     1] loss: 0.224
[48,     1] loss: 0.226
[49,     1] loss: 0.181
[50,     1] loss: 0.199
[51,     1] loss: 0.178
[52,     1] loss: 0.123
[53,     1] loss: 0.166
[54,     1] loss: 0.130
[55,     1] loss: 0.218
[56,     1] loss: 0.291
[57,     1] loss: 0.257
[58,     1] loss: 0.172
[59,     1] loss: 0.234
[60,     1] loss: 0.200
[61,     1] loss: 0.169
[62,     1] loss: 0.212
[63,     1] loss: 0.171
[64,     1] loss: 0.136
[65,     1] loss: 0.146
[66,     1] loss: 0.121
[67,     1] loss: 0.176
[68,     1] loss: 0.147
[69,     1] loss: 0.123
[70,     1] loss: 0.177
[71,     1] loss: 0.128
[72,     1] loss: 0.167
[73,     1] loss: 0.183
[74,     1] loss: 0.193
[75,     1] loss: 0.095
[76,     1] loss: 0.127
[77,     1] loss: 0.152
[78,     1] loss: 0.146
[79,     1] loss: 0.188
[80,     1] loss: 0.176
[81,     1] loss: 0.119
[82,     1] loss: 0.138
[83,     1] loss: 0.107
[84,     1] loss: 0.221
[85,     1] loss: 0.163
[86,     1] loss: 0.159
[87,     1] loss: 0.177
[88,     1] loss: 0.143
[89,     1] loss: 0.219
[90,     1] loss: 0.193
Early stopping applied (best metric=0.3165764808654785)
Finished Training
Total time taken: 11.047645330429077
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.706
[3,     1] loss: 0.686
[4,     1] loss: 0.675
[5,     1] loss: 0.662
[6,     1] loss: 0.639
[7,     1] loss: 0.617
[8,     1] loss: 0.582
[9,     1] loss: 0.521
[10,     1] loss: 0.478
[11,     1] loss: 0.482
[12,     1] loss: 0.448
[13,     1] loss: 0.403
[14,     1] loss: 0.362
[15,     1] loss: 0.326
[16,     1] loss: 0.299
[17,     1] loss: 0.306
[18,     1] loss: 0.307
[19,     1] loss: 0.242
[20,     1] loss: 0.391
[21,     1] loss: 0.219
[22,     1] loss: 0.226
[23,     1] loss: 0.227
[24,     1] loss: 0.271
[25,     1] loss: 0.253
[26,     1] loss: 0.287
[27,     1] loss: 0.286
[28,     1] loss: 0.289
[29,     1] loss: 0.232
[30,     1] loss: 0.207
[31,     1] loss: 0.191
[32,     1] loss: 0.239
[33,     1] loss: 0.202
[34,     1] loss: 0.247
[35,     1] loss: 0.213
[36,     1] loss: 0.350
[37,     1] loss: 0.216
[38,     1] loss: 0.233
[39,     1] loss: 0.198
[40,     1] loss: 0.280
[41,     1] loss: 0.272
[42,     1] loss: 0.250
[43,     1] loss: 0.223
[44,     1] loss: 0.183
[45,     1] loss: 0.201
[46,     1] loss: 0.244
[47,     1] loss: 0.190
[48,     1] loss: 0.168
[49,     1] loss: 0.181
[50,     1] loss: 0.151
[51,     1] loss: 0.150
[52,     1] loss: 0.361
[53,     1] loss: 0.590
[54,     1] loss: 0.311
[55,     1] loss: 0.406
[56,     1] loss: 0.468
[57,     1] loss: 0.330
[58,     1] loss: 0.307
[59,     1] loss: 0.309
[60,     1] loss: 0.363
[61,     1] loss: 0.302
[62,     1] loss: 0.270
[63,     1] loss: 0.267
[64,     1] loss: 0.248
[65,     1] loss: 0.290
[66,     1] loss: 0.233
[67,     1] loss: 0.281
[68,     1] loss: 0.243
[69,     1] loss: 0.207
[70,     1] loss: 0.227
[71,     1] loss: 0.205
[72,     1] loss: 0.164
[73,     1] loss: 0.161
[74,     1] loss: 0.193
[75,     1] loss: 0.204
[76,     1] loss: 0.194
[77,     1] loss: 0.157
[78,     1] loss: 0.121
[79,     1] loss: 0.141
[80,     1] loss: 0.136
[81,     1] loss: 0.182
[82,     1] loss: 0.235
[83,     1] loss: 0.267
[84,     1] loss: 0.219
[85,     1] loss: 0.191
[86,     1] loss: 0.202
[87,     1] loss: 0.219
[88,     1] loss: 0.169
[89,     1] loss: 0.186
[90,     1] loss: 0.202
[91,     1] loss: 0.194
[92,     1] loss: 0.154
[93,     1] loss: 0.186
[94,     1] loss: 0.153
[95,     1] loss: 0.165
[96,     1] loss: 0.161
[97,     1] loss: 0.172
[98,     1] loss: 0.150
[99,     1] loss: 0.143
[100,     1] loss: 0.134
[101,     1] loss: 0.154
[102,     1] loss: 0.153
[103,     1] loss: 0.182
[104,     1] loss: 0.327
[105,     1] loss: 0.295
[106,     1] loss: 0.295
[107,     1] loss: 0.220
[108,     1] loss: 0.256
[109,     1] loss: 0.206
[110,     1] loss: 0.205
[111,     1] loss: 0.205
[112,     1] loss: 0.189
[113,     1] loss: 0.145
[114,     1] loss: 0.168
[115,     1] loss: 0.155
[116,     1] loss: 0.191
[117,     1] loss: 0.164
[118,     1] loss: 0.184
[119,     1] loss: 0.140
Early stopping applied (best metric=0.2145051658153534)
Finished Training
Total time taken: 14.321367740631104
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.668
[4,     1] loss: 0.632
[5,     1] loss: 0.577
[6,     1] loss: 0.534
[7,     1] loss: 0.492
[8,     1] loss: 0.413
[9,     1] loss: 0.373
[10,     1] loss: 0.399
[11,     1] loss: 0.333
[12,     1] loss: 0.266
[13,     1] loss: 0.277
[14,     1] loss: 0.236
[15,     1] loss: 0.205
[16,     1] loss: 0.193
[17,     1] loss: 0.213
[18,     1] loss: 0.174
[19,     1] loss: 0.286
[20,     1] loss: 0.199
[21,     1] loss: 0.195
[22,     1] loss: 0.204
[23,     1] loss: 0.180
[24,     1] loss: 0.213
[25,     1] loss: 0.214
[26,     1] loss: 0.157
[27,     1] loss: 0.177
[28,     1] loss: 0.163
[29,     1] loss: 0.142
[30,     1] loss: 0.122
[31,     1] loss: 0.224
[32,     1] loss: 0.182
[33,     1] loss: 0.173
[34,     1] loss: 0.145
[35,     1] loss: 0.165
[36,     1] loss: 0.156
[37,     1] loss: 0.118
[38,     1] loss: 0.108
[39,     1] loss: 0.126
[40,     1] loss: 0.143
[41,     1] loss: 0.301
[42,     1] loss: 0.300
[43,     1] loss: 0.364
[44,     1] loss: 0.256
[45,     1] loss: 0.250
[46,     1] loss: 0.282
[47,     1] loss: 0.206
[48,     1] loss: 0.192
[49,     1] loss: 0.259
[50,     1] loss: 0.245
[51,     1] loss: 0.202
[52,     1] loss: 0.215
[53,     1] loss: 0.169
[54,     1] loss: 0.114
[55,     1] loss: 0.140
[56,     1] loss: 0.106
[57,     1] loss: 0.104
[58,     1] loss: 0.096
[59,     1] loss: 0.192
[60,     1] loss: 0.223
Early stopping applied (best metric=0.37518584728240967)
Finished Training
Total time taken: 7.2926952838897705
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.672
[5,     1] loss: 0.636
[6,     1] loss: 0.627
[7,     1] loss: 0.573
[8,     1] loss: 0.526
[9,     1] loss: 0.493
[10,     1] loss: 0.434
[11,     1] loss: 0.425
[12,     1] loss: 0.421
[13,     1] loss: 0.357
[14,     1] loss: 0.382
[15,     1] loss: 0.304
[16,     1] loss: 0.375
[17,     1] loss: 0.251
[18,     1] loss: 0.296
[19,     1] loss: 0.230
[20,     1] loss: 0.230
[21,     1] loss: 0.219
[22,     1] loss: 0.353
[23,     1] loss: 0.335
[24,     1] loss: 0.357
[25,     1] loss: 0.195
[26,     1] loss: 0.225
[27,     1] loss: 0.295
[28,     1] loss: 0.262
[29,     1] loss: 0.292
[30,     1] loss: 0.275
[31,     1] loss: 0.221
[32,     1] loss: 0.233
[33,     1] loss: 0.201
[34,     1] loss: 0.210
[35,     1] loss: 0.164
[36,     1] loss: 0.224
[37,     1] loss: 0.272
[38,     1] loss: 0.197
[39,     1] loss: 0.199
[40,     1] loss: 0.194
[41,     1] loss: 0.158
[42,     1] loss: 0.145
[43,     1] loss: 0.192
[44,     1] loss: 0.236
[45,     1] loss: 0.149
[46,     1] loss: 0.177
[47,     1] loss: 0.166
[48,     1] loss: 0.196
[49,     1] loss: 0.135
[50,     1] loss: 0.120
[51,     1] loss: 0.207
[52,     1] loss: 0.235
[53,     1] loss: 0.326
[54,     1] loss: 0.307
[55,     1] loss: 0.152
[56,     1] loss: 0.243
[57,     1] loss: 0.246
[58,     1] loss: 0.239
[59,     1] loss: 0.236
[60,     1] loss: 0.185
[61,     1] loss: 0.224
[62,     1] loss: 0.162
[63,     1] loss: 0.203
[64,     1] loss: 0.211
[65,     1] loss: 0.170
[66,     1] loss: 0.245
[67,     1] loss: 0.205
[68,     1] loss: 0.168
[69,     1] loss: 0.137
[70,     1] loss: 0.173
[71,     1] loss: 0.157
[72,     1] loss: 0.155
[73,     1] loss: 0.138
[74,     1] loss: 0.141
[75,     1] loss: 0.139
[76,     1] loss: 0.182
[77,     1] loss: 0.128
[78,     1] loss: 0.151
[79,     1] loss: 0.130
[80,     1] loss: 0.198
[81,     1] loss: 0.142
[82,     1] loss: 0.093
[83,     1] loss: 0.139
[84,     1] loss: 0.161
[85,     1] loss: 0.111
[86,     1] loss: 0.170
[87,     1] loss: 0.099
[88,     1] loss: 0.159
[89,     1] loss: 0.134
[90,     1] loss: 0.140
[91,     1] loss: 0.184
[92,     1] loss: 0.210
[93,     1] loss: 0.143
[94,     1] loss: 0.144
[95,     1] loss: 0.325
[96,     1] loss: 0.633
[97,     1] loss: 0.344
[98,     1] loss: 0.342
[99,     1] loss: 0.280
[100,     1] loss: 0.324
[101,     1] loss: 0.338
[102,     1] loss: 0.348
[103,     1] loss: 0.313
[104,     1] loss: 0.282
[105,     1] loss: 0.245
[106,     1] loss: 0.271
[107,     1] loss: 0.253
[108,     1] loss: 0.222
[109,     1] loss: 0.194
[110,     1] loss: 0.207
[111,     1] loss: 0.207
[112,     1] loss: 0.185
[113,     1] loss: 0.179
[114,     1] loss: 0.172
[115,     1] loss: 0.182
[116,     1] loss: 0.179
[117,     1] loss: 0.128
[118,     1] loss: 0.159
[119,     1] loss: 0.131
[120,     1] loss: 0.141
[121,     1] loss: 0.129
[122,     1] loss: 0.128
[123,     1] loss: 0.138
[124,     1] loss: 0.173
[125,     1] loss: 0.126
[126,     1] loss: 0.273
[127,     1] loss: 0.241
[128,     1] loss: 0.274
[129,     1] loss: 0.222
[130,     1] loss: 0.167
[131,     1] loss: 0.238
[132,     1] loss: 0.205
[133,     1] loss: 0.206
[134,     1] loss: 0.199
[135,     1] loss: 0.189
[136,     1] loss: 0.188
[137,     1] loss: 0.160
[138,     1] loss: 0.219
[139,     1] loss: 0.163
[140,     1] loss: 0.154
[141,     1] loss: 0.174
[142,     1] loss: 0.189
[143,     1] loss: 0.210
[144,     1] loss: 0.192
[145,     1] loss: 0.206
[146,     1] loss: 0.159
[147,     1] loss: 0.156
[148,     1] loss: 0.137
[149,     1] loss: 0.151
[150,     1] loss: 0.186
[151,     1] loss: 0.190
[152,     1] loss: 0.206
[153,     1] loss: 0.215
[154,     1] loss: 0.208
[155,     1] loss: 0.200
[156,     1] loss: 0.152
[157,     1] loss: 0.194
[158,     1] loss: 0.181
[159,     1] loss: 0.177
[160,     1] loss: 0.154
[161,     1] loss: 0.161
[162,     1] loss: 0.134
[163,     1] loss: 0.140
[164,     1] loss: 0.155
[165,     1] loss: 0.138
[166,     1] loss: 0.154
[167,     1] loss: 0.230
[168,     1] loss: 0.154
[169,     1] loss: 0.114
Early stopping applied (best metric=0.10497190803289413)
Finished Training
Total time taken: 20.440091848373413
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.704
[3,     1] loss: 0.684
[4,     1] loss: 0.672
[5,     1] loss: 0.643
[6,     1] loss: 0.606
[7,     1] loss: 0.558
[8,     1] loss: 0.506
[9,     1] loss: 0.429
[10,     1] loss: 0.429
[11,     1] loss: 0.371
[12,     1] loss: 0.321
[13,     1] loss: 0.323
[14,     1] loss: 0.283
[15,     1] loss: 0.285
[16,     1] loss: 0.262
[17,     1] loss: 0.262
[18,     1] loss: 0.229
[19,     1] loss: 0.218
[20,     1] loss: 0.214
[21,     1] loss: 0.240
[22,     1] loss: 0.226
[23,     1] loss: 0.338
[24,     1] loss: 0.217
[25,     1] loss: 0.363
[26,     1] loss: 0.221
[27,     1] loss: 0.216
[28,     1] loss: 0.222
[29,     1] loss: 0.243
[30,     1] loss: 0.221
[31,     1] loss: 0.204
[32,     1] loss: 0.199
[33,     1] loss: 0.167
[34,     1] loss: 0.208
[35,     1] loss: 0.177
[36,     1] loss: 0.154
[37,     1] loss: 0.272
[38,     1] loss: 0.198
[39,     1] loss: 0.231
[40,     1] loss: 0.137
[41,     1] loss: 0.190
[42,     1] loss: 0.196
[43,     1] loss: 0.208
[44,     1] loss: 0.205
[45,     1] loss: 0.225
[46,     1] loss: 0.187
[47,     1] loss: 0.223
[48,     1] loss: 0.149
[49,     1] loss: 0.187
[50,     1] loss: 0.182
[51,     1] loss: 0.149
[52,     1] loss: 0.217
[53,     1] loss: 0.300
[54,     1] loss: 0.170
[55,     1] loss: 0.174
[56,     1] loss: 0.232
[57,     1] loss: 0.168
[58,     1] loss: 0.213
[59,     1] loss: 0.164
Early stopping applied (best metric=0.34234482049942017)
Finished Training
Total time taken: 7.3351850509643555
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.676
[4,     1] loss: 0.640
[5,     1] loss: 0.575
[6,     1] loss: 0.509
[7,     1] loss: 0.467
[8,     1] loss: 0.402
[9,     1] loss: 0.358
[10,     1] loss: 0.321
[11,     1] loss: 0.345
[12,     1] loss: 0.332
[13,     1] loss: 0.252
[14,     1] loss: 0.287
[15,     1] loss: 0.250
[16,     1] loss: 0.187
[17,     1] loss: 0.343
[18,     1] loss: 0.226
[19,     1] loss: 0.272
[20,     1] loss: 0.185
[21,     1] loss: 0.197
[22,     1] loss: 0.246
[23,     1] loss: 0.237
[24,     1] loss: 0.216
[25,     1] loss: 0.173
[26,     1] loss: 0.199
[27,     1] loss: 0.144
[28,     1] loss: 0.164
[29,     1] loss: 0.149
[30,     1] loss: 0.121
[31,     1] loss: 0.133
[32,     1] loss: 0.175
[33,     1] loss: 0.135
[34,     1] loss: 0.154
[35,     1] loss: 0.117
[36,     1] loss: 0.093
[37,     1] loss: 0.226
[38,     1] loss: 0.382
[39,     1] loss: 0.178
[40,     1] loss: 0.302
[41,     1] loss: 0.241
[42,     1] loss: 0.175
[43,     1] loss: 0.219
[44,     1] loss: 0.215
[45,     1] loss: 0.228
[46,     1] loss: 0.201
[47,     1] loss: 0.234
[48,     1] loss: 0.170
[49,     1] loss: 0.250
[50,     1] loss: 0.220
[51,     1] loss: 0.183
[52,     1] loss: 0.162
[53,     1] loss: 0.197
[54,     1] loss: 0.105
[55,     1] loss: 0.216
[56,     1] loss: 0.181
[57,     1] loss: 0.157
[58,     1] loss: 0.253
[59,     1] loss: 0.116
[60,     1] loss: 0.286
[61,     1] loss: 0.181
[62,     1] loss: 0.138
[63,     1] loss: 0.160
[64,     1] loss: 0.173
[65,     1] loss: 0.147
[66,     1] loss: 0.206
[67,     1] loss: 0.137
[68,     1] loss: 0.099
[69,     1] loss: 0.154
[70,     1] loss: 0.179
[71,     1] loss: 0.203
[72,     1] loss: 0.101
[73,     1] loss: 0.182
[74,     1] loss: 0.122
[75,     1] loss: 0.188
[76,     1] loss: 0.173
[77,     1] loss: 0.126
[78,     1] loss: 0.210
[79,     1] loss: 0.086
[80,     1] loss: 0.122
[81,     1] loss: 0.163
[82,     1] loss: 0.140
[83,     1] loss: 0.345
[84,     1] loss: 0.209
[85,     1] loss: 0.410
[86,     1] loss: 0.294
[87,     1] loss: 0.255
[88,     1] loss: 0.268
[89,     1] loss: 0.256
[90,     1] loss: 0.250
[91,     1] loss: 0.259
[92,     1] loss: 0.282
[93,     1] loss: 0.257
[94,     1] loss: 0.214
[95,     1] loss: 0.247
[96,     1] loss: 0.203
[97,     1] loss: 0.192
[98,     1] loss: 0.153
[99,     1] loss: 0.114
[100,     1] loss: 0.157
Early stopping applied (best metric=0.352275013923645)
Finished Training
Total time taken: 11.601745843887329
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.679
[4,     1] loss: 0.652
[5,     1] loss: 0.629
[6,     1] loss: 0.573
[7,     1] loss: 0.550
[8,     1] loss: 0.449
[9,     1] loss: 0.457
[10,     1] loss: 0.339
[11,     1] loss: 0.329
[12,     1] loss: 0.370
[13,     1] loss: 0.400
[14,     1] loss: 0.317
[15,     1] loss: 0.221
[16,     1] loss: 0.446
[17,     1] loss: 0.323
[18,     1] loss: 0.269
[19,     1] loss: 0.268
[20,     1] loss: 0.340
[21,     1] loss: 0.295
[22,     1] loss: 0.293
[23,     1] loss: 0.297
[24,     1] loss: 0.249
[25,     1] loss: 0.220
[26,     1] loss: 0.234
[27,     1] loss: 0.226
[28,     1] loss: 0.224
[29,     1] loss: 0.207
[30,     1] loss: 0.263
[31,     1] loss: 0.307
[32,     1] loss: 0.232
[33,     1] loss: 0.241
[34,     1] loss: 0.209
[35,     1] loss: 0.194
[36,     1] loss: 0.217
[37,     1] loss: 0.185
[38,     1] loss: 0.172
[39,     1] loss: 0.250
[40,     1] loss: 0.199
[41,     1] loss: 0.222
[42,     1] loss: 0.148
[43,     1] loss: 0.145
[44,     1] loss: 0.227
[45,     1] loss: 0.219
[46,     1] loss: 0.186
[47,     1] loss: 0.200
[48,     1] loss: 0.154
[49,     1] loss: 0.175
[50,     1] loss: 0.143
[51,     1] loss: 0.165
[52,     1] loss: 0.207
[53,     1] loss: 0.252
[54,     1] loss: 0.164
[55,     1] loss: 0.174
[56,     1] loss: 0.211
[57,     1] loss: 0.340
[58,     1] loss: 0.269
[59,     1] loss: 0.263
Early stopping applied (best metric=0.34216469526290894)
Finished Training
Total time taken: 6.875633955001831
{'Hydroxylation-K Validation Accuracy': 0.8699113475177305, 'Hydroxylation-K Validation Sensitivity': 0.8804444444444445, 'Hydroxylation-K Validation Specificity': 0.8673684210526316, 'Hydroxylation-K Validation Precision': 0.6422339335930667, 'Hydroxylation-K AUC ROC': 0.887859649122807, 'Hydroxylation-K AUC PR': 0.7341761847217481, 'Hydroxylation-K MCC': 0.6737171234525431, 'Hydroxylation-K F1': 0.7372032399069407, 'Validation Loss (Hydroxylation-K)': 0.29014445513486864, 'Validation Loss (total)': 0.29014445513486864, 'TimeToTrain': 11.471669635772706}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004800817072027099,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 374574842,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.784711222553948}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.696
[3,     1] loss: 0.670
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003776301760052938,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 900961082,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.308148851005146}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.691
[3,     1] loss: 0.685
[4,     1] loss: 0.669
[5,     1] loss: 0.643
[6,     1] loss: 0.613
[7,     1] loss: 0.596
[8,     1] loss: 0.576
[9,     1] loss: 0.532
[10,     1] loss: 0.528
[11,     1] loss: 0.489
[12,     1] loss: 0.483
[13,     1] loss: 0.448
[14,     1] loss: 0.439
[15,     1] loss: 0.419
[16,     1] loss: 0.432
[17,     1] loss: 0.354
[18,     1] loss: 0.377
[19,     1] loss: 0.347
[20,     1] loss: 0.381
[21,     1] loss: 0.339
[22,     1] loss: 0.369
[23,     1] loss: 0.340
[24,     1] loss: 0.317
[25,     1] loss: 0.347
[26,     1] loss: 0.301
[27,     1] loss: 0.341
[28,     1] loss: 0.287
[29,     1] loss: 0.279
[30,     1] loss: 0.339
[31,     1] loss: 0.322
[32,     1] loss: 0.305
[33,     1] loss: 0.283
[34,     1] loss: 0.253
[35,     1] loss: 0.299
[36,     1] loss: 0.315
[37,     1] loss: 0.294
[38,     1] loss: 0.290
[39,     1] loss: 0.303
[40,     1] loss: 0.261
[41,     1] loss: 0.256
[42,     1] loss: 0.287
[43,     1] loss: 0.284
[44,     1] loss: 0.305
[45,     1] loss: 0.323
[46,     1] loss: 0.289
[47,     1] loss: 0.364
[48,     1] loss: 0.348
[49,     1] loss: 0.332
[50,     1] loss: 0.315
[51,     1] loss: 0.299
[52,     1] loss: 0.290
[53,     1] loss: 0.294
[54,     1] loss: 0.286
[55,     1] loss: 0.290
[56,     1] loss: 0.365
[57,     1] loss: 0.369
[58,     1] loss: 0.267
[59,     1] loss: 0.333
[60,     1] loss: 0.363
[61,     1] loss: 0.336
[62,     1] loss: 0.348
[63,     1] loss: 0.320
[64,     1] loss: 0.288
[65,     1] loss: 0.294
[66,     1] loss: 0.366
[67,     1] loss: 0.347
[68,     1] loss: 0.338
[69,     1] loss: 0.311
[70,     1] loss: 0.301
[71,     1] loss: 0.301
[72,     1] loss: 0.317
[73,     1] loss: 0.293
[74,     1] loss: 0.267
[75,     1] loss: 0.278
[76,     1] loss: 0.329
[77,     1] loss: 0.257
[78,     1] loss: 0.328
[79,     1] loss: 0.291
[80,     1] loss: 0.345
[81,     1] loss: 0.284
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021344847599388973,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3737117892,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.6041607361558}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.689
[3,     1] loss: 0.680
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017939567173309034,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2992730498,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.218858416999446}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.689
[3,     1] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007434869213841082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1303847196,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.47011113750689}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.677
[4,     1] loss: 0.670
[5,     1] loss: 0.645
[6,     1] loss: 0.609
[7,     1] loss: 0.579
[8,     1] loss: 0.550
[9,     1] loss: 0.538
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005654593149865506,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1420829013,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.46341348574388}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.697
[3,     1] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0053534847904217,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1505212836,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.107710088500287}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.692
[3,     1] loss: 0.674
[4,     1] loss: 0.631
[5,     1] loss: 0.601
[6,     1] loss: 0.557
[7,     1] loss: 0.495
[8,     1] loss: 0.472
[9,     1] loss: 0.444
[10,     1] loss: 0.404
[11,     1] loss: 0.393
[12,     1] loss: 0.327
[13,     1] loss: 0.331
[14,     1] loss: 0.298
[15,     1] loss: 0.387
[16,     1] loss: 0.282
[17,     1] loss: 0.255
[18,     1] loss: 0.255
[19,     1] loss: 0.236
[20,     1] loss: 0.209
[21,     1] loss: 0.222
[22,     1] loss: 0.280
[23,     1] loss: 0.231
[24,     1] loss: 0.236
[25,     1] loss: 0.203
[26,     1] loss: 0.336
[27,     1] loss: 0.218
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009527305975997016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2898471398,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.62317155720843}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008832287507883627,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 498131276,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.390389720737513}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.687
[4,     1] loss: 0.677
[5,     1] loss: 0.665
[6,     1] loss: 0.635
[7,     1] loss: 0.607
[8,     1] loss: 0.590
[9,     1] loss: 0.548
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019067906599432704,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1877386200,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.292903972717511}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.684
[4,     1] loss: 0.677
[5,     1] loss: 0.659
[6,     1] loss: 0.642
[7,     1] loss: 0.628
[8,     1] loss: 0.611
[9,     1] loss: 0.593
[10,     1] loss: 0.570
[11,     1] loss: 0.553
[12,     1] loss: 0.547
[13,     1] loss: 0.530
[14,     1] loss: 0.513
[15,     1] loss: 0.484
[16,     1] loss: 0.468
[17,     1] loss: 0.429
[18,     1] loss: 0.410
[19,     1] loss: 0.425
[20,     1] loss: 0.392
[21,     1] loss: 0.404
[22,     1] loss: 0.424
[23,     1] loss: 0.334
[24,     1] loss: 0.327
[25,     1] loss: 0.323
[26,     1] loss: 0.327
[27,     1] loss: 0.280
[28,     1] loss: 0.288
[29,     1] loss: 0.197
[30,     1] loss: 0.208
[31,     1] loss: 0.236
[32,     1] loss: 0.215
[33,     1] loss: 0.219
[34,     1] loss: 0.166
[35,     1] loss: 0.207
[36,     1] loss: 0.216
[37,     1] loss: 0.195
[38,     1] loss: 0.231
[39,     1] loss: 0.196
[40,     1] loss: 0.171
[41,     1] loss: 0.167
[42,     1] loss: 0.148
[43,     1] loss: 0.224
[44,     1] loss: 0.216
[45,     1] loss: 0.236
[46,     1] loss: 0.136
[47,     1] loss: 0.153
[48,     1] loss: 0.170
[49,     1] loss: 0.149
[50,     1] loss: 0.140
[51,     1] loss: 0.150
[52,     1] loss: 0.136
[53,     1] loss: 0.127
[54,     1] loss: 0.116
[55,     1] loss: 0.148
[56,     1] loss: 0.105
[57,     1] loss: 0.106
[58,     1] loss: 0.103
[59,     1] loss: 0.275
[60,     1] loss: 0.079
[61,     1] loss: 0.109
[62,     1] loss: 0.106
[63,     1] loss: 0.095
[64,     1] loss: 0.112
[65,     1] loss: 0.128
[66,     1] loss: 0.127
[67,     1] loss: 0.106
[68,     1] loss: 0.102
[69,     1] loss: 0.085
Early stopping applied (best metric=0.34161481261253357)
Finished Training
Total time taken: 8.016559362411499
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.690
[4,     1] loss: 0.663
[5,     1] loss: 0.651
[6,     1] loss: 0.618
[7,     1] loss: 0.589
[8,     1] loss: 0.562
[9,     1] loss: 0.548
[10,     1] loss: 0.532
[11,     1] loss: 0.490
[12,     1] loss: 0.478
[13,     1] loss: 0.511
[14,     1] loss: 0.449
[15,     1] loss: 0.429
[16,     1] loss: 0.409
[17,     1] loss: 0.429
[18,     1] loss: 0.420
[19,     1] loss: 0.382
[20,     1] loss: 0.391
[21,     1] loss: 0.369
[22,     1] loss: 0.368
[23,     1] loss: 0.342
[24,     1] loss: 0.407
[25,     1] loss: 0.314
[26,     1] loss: 0.271
[27,     1] loss: 0.328
[28,     1] loss: 0.286
[29,     1] loss: 0.347
[30,     1] loss: 0.302
[31,     1] loss: 0.328
[32,     1] loss: 0.280
[33,     1] loss: 0.290
[34,     1] loss: 0.230
[35,     1] loss: 0.210
[36,     1] loss: 0.237
[37,     1] loss: 0.253
[38,     1] loss: 0.317
[39,     1] loss: 0.246
[40,     1] loss: 0.283
[41,     1] loss: 0.252
[42,     1] loss: 0.257
[43,     1] loss: 0.212
[44,     1] loss: 0.280
[45,     1] loss: 0.155
[46,     1] loss: 0.227
[47,     1] loss: 0.234
[48,     1] loss: 0.197
[49,     1] loss: 0.229
[50,     1] loss: 0.279
[51,     1] loss: 0.184
[52,     1] loss: 0.197
[53,     1] loss: 0.190
[54,     1] loss: 0.269
[55,     1] loss: 0.203
[56,     1] loss: 0.216
[57,     1] loss: 0.211
[58,     1] loss: 0.164
[59,     1] loss: 0.148
[60,     1] loss: 0.157
[61,     1] loss: 0.157
[62,     1] loss: 0.182
[63,     1] loss: 0.176
[64,     1] loss: 0.164
[65,     1] loss: 0.255
[66,     1] loss: 0.142
Early stopping applied (best metric=0.4408871531486511)
Finished Training
Total time taken: 7.672841548919678
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.671
[4,     1] loss: 0.645
[5,     1] loss: 0.629
[6,     1] loss: 0.594
[7,     1] loss: 0.598
[8,     1] loss: 0.552
[9,     1] loss: 0.548
[10,     1] loss: 0.514
[11,     1] loss: 0.492
[12,     1] loss: 0.447
[13,     1] loss: 0.443
[14,     1] loss: 0.415
[15,     1] loss: 0.424
[16,     1] loss: 0.390
[17,     1] loss: 0.359
[18,     1] loss: 0.342
[19,     1] loss: 0.327
[20,     1] loss: 0.357
[21,     1] loss: 0.353
[22,     1] loss: 0.287
[23,     1] loss: 0.330
[24,     1] loss: 0.278
[25,     1] loss: 0.294
[26,     1] loss: 0.249
[27,     1] loss: 0.297
[28,     1] loss: 0.244
[29,     1] loss: 0.211
[30,     1] loss: 0.263
[31,     1] loss: 0.297
[32,     1] loss: 0.204
[33,     1] loss: 0.251
[34,     1] loss: 0.260
[35,     1] loss: 0.227
[36,     1] loss: 0.259
[37,     1] loss: 0.222
[38,     1] loss: 0.172
[39,     1] loss: 0.227
[40,     1] loss: 0.203
[41,     1] loss: 0.156
[42,     1] loss: 0.202
[43,     1] loss: 0.145
[44,     1] loss: 0.247
[45,     1] loss: 0.213
[46,     1] loss: 0.213
[47,     1] loss: 0.189
[48,     1] loss: 0.170
[49,     1] loss: 0.241
[50,     1] loss: 0.156
[51,     1] loss: 0.157
[52,     1] loss: 0.187
[53,     1] loss: 0.177
[54,     1] loss: 0.144
[55,     1] loss: 0.166
[56,     1] loss: 0.187
[57,     1] loss: 0.173
[58,     1] loss: 0.121
[59,     1] loss: 0.174
[60,     1] loss: 0.134
[61,     1] loss: 0.153
[62,     1] loss: 0.105
[63,     1] loss: 0.124
[64,     1] loss: 0.165
[65,     1] loss: 0.115
[66,     1] loss: 0.101
[67,     1] loss: 0.078
[68,     1] loss: 0.100
[69,     1] loss: 0.079
[70,     1] loss: 0.046
[71,     1] loss: 0.092
[72,     1] loss: 0.051
[73,     1] loss: 0.043
[74,     1] loss: 0.129
[75,     1] loss: 0.061
[76,     1] loss: 0.138
[77,     1] loss: 0.041
[78,     1] loss: 0.052
[79,     1] loss: 0.085
[80,     1] loss: 0.047
[81,     1] loss: 0.044
[82,     1] loss: 0.041
[83,     1] loss: 0.037
[84,     1] loss: 0.066
[85,     1] loss: 0.038
[86,     1] loss: 0.034
[87,     1] loss: 0.033
[88,     1] loss: 0.035
[89,     1] loss: 0.034
[90,     1] loss: 0.032
[91,     1] loss: 0.030
[92,     1] loss: 0.059
[93,     1] loss: 0.033
[94,     1] loss: 0.038
[95,     1] loss: 0.019
[96,     1] loss: 0.023
[97,     1] loss: 0.065
[98,     1] loss: 0.063
[99,     1] loss: 0.138
[100,     1] loss: 0.037
[101,     1] loss: 0.222
[102,     1] loss: 0.037
[103,     1] loss: 0.133
Early stopping applied (best metric=0.4424302577972412)
Finished Training
Total time taken: 11.991096258163452
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.683
[4,     1] loss: 0.673
[5,     1] loss: 0.647
[6,     1] loss: 0.633
[7,     1] loss: 0.601
[8,     1] loss: 0.587
[9,     1] loss: 0.556
[10,     1] loss: 0.556
[11,     1] loss: 0.534
[12,     1] loss: 0.494
[13,     1] loss: 0.478
[14,     1] loss: 0.470
[15,     1] loss: 0.387
[16,     1] loss: 0.414
[17,     1] loss: 0.401
[18,     1] loss: 0.406
[19,     1] loss: 0.347
[20,     1] loss: 0.384
[21,     1] loss: 0.311
[22,     1] loss: 0.311
[23,     1] loss: 0.370
[24,     1] loss: 0.373
[25,     1] loss: 0.256
[26,     1] loss: 0.344
[27,     1] loss: 0.348
[28,     1] loss: 0.280
[29,     1] loss: 0.230
[30,     1] loss: 0.301
[31,     1] loss: 0.245
[32,     1] loss: 0.317
[33,     1] loss: 0.235
[34,     1] loss: 0.231
[35,     1] loss: 0.255
[36,     1] loss: 0.274
[37,     1] loss: 0.230
[38,     1] loss: 0.340
[39,     1] loss: 0.227
[40,     1] loss: 0.273
[41,     1] loss: 0.230
[42,     1] loss: 0.224
[43,     1] loss: 0.280
[44,     1] loss: 0.208
[45,     1] loss: 0.288
[46,     1] loss: 0.215
[47,     1] loss: 0.219
[48,     1] loss: 0.183
[49,     1] loss: 0.176
[50,     1] loss: 0.211
[51,     1] loss: 0.190
[52,     1] loss: 0.179
[53,     1] loss: 0.170
[54,     1] loss: 0.172
[55,     1] loss: 0.163
[56,     1] loss: 0.145
[57,     1] loss: 0.117
[58,     1] loss: 0.105
[59,     1] loss: 0.107
[60,     1] loss: 0.105
[61,     1] loss: 0.097
[62,     1] loss: 0.082
[63,     1] loss: 0.090
[64,     1] loss: 0.090
[65,     1] loss: 0.065
[66,     1] loss: 0.074
Early stopping applied (best metric=0.2942706346511841)
Finished Training
Total time taken: 7.858638763427734
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.684
[3,     1] loss: 0.666
[4,     1] loss: 0.645
[5,     1] loss: 0.613
[6,     1] loss: 0.595
[7,     1] loss: 0.545
[8,     1] loss: 0.516
[9,     1] loss: 0.482
[10,     1] loss: 0.422
[11,     1] loss: 0.458
[12,     1] loss: 0.461
[13,     1] loss: 0.412
[14,     1] loss: 0.417
[15,     1] loss: 0.334
[16,     1] loss: 0.356
[17,     1] loss: 0.350
[18,     1] loss: 0.307
[19,     1] loss: 0.286
[20,     1] loss: 0.342
[21,     1] loss: 0.277
[22,     1] loss: 0.375
[23,     1] loss: 0.302
[24,     1] loss: 0.270
[25,     1] loss: 0.283
[26,     1] loss: 0.351
[27,     1] loss: 0.343
[28,     1] loss: 0.289
[29,     1] loss: 0.204
[30,     1] loss: 0.192
[31,     1] loss: 0.248
[32,     1] loss: 0.204
[33,     1] loss: 0.210
[34,     1] loss: 0.264
[35,     1] loss: 0.190
[36,     1] loss: 0.196
[37,     1] loss: 0.194
[38,     1] loss: 0.172
[39,     1] loss: 0.180
[40,     1] loss: 0.208
[41,     1] loss: 0.160
[42,     1] loss: 0.200
[43,     1] loss: 0.217
[44,     1] loss: 0.215
[45,     1] loss: 0.143
[46,     1] loss: 0.164
[47,     1] loss: 0.160
[48,     1] loss: 0.148
[49,     1] loss: 0.152
[50,     1] loss: 0.138
[51,     1] loss: 0.206
[52,     1] loss: 0.135
[53,     1] loss: 0.114
[54,     1] loss: 0.125
[55,     1] loss: 0.155
[56,     1] loss: 0.101
[57,     1] loss: 0.153
[58,     1] loss: 0.141
[59,     1] loss: 0.152
[60,     1] loss: 0.114
[61,     1] loss: 0.112
[62,     1] loss: 0.127
[63,     1] loss: 0.100
[64,     1] loss: 0.093
Early stopping applied (best metric=0.3774193227291107)
Finished Training
Total time taken: 7.3925135135650635
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.683
[4,     1] loss: 0.669
[5,     1] loss: 0.647
[6,     1] loss: 0.621
[7,     1] loss: 0.608
[8,     1] loss: 0.598
[9,     1] loss: 0.548
[10,     1] loss: 0.524
[11,     1] loss: 0.514
[12,     1] loss: 0.490
[13,     1] loss: 0.454
[14,     1] loss: 0.421
[15,     1] loss: 0.402
[16,     1] loss: 0.380
[17,     1] loss: 0.351
[18,     1] loss: 0.330
[19,     1] loss: 0.373
[20,     1] loss: 0.303
[21,     1] loss: 0.306
[22,     1] loss: 0.260
[23,     1] loss: 0.251
[24,     1] loss: 0.255
[25,     1] loss: 0.225
[26,     1] loss: 0.244
[27,     1] loss: 0.277
[28,     1] loss: 0.237
[29,     1] loss: 0.236
[30,     1] loss: 0.224
[31,     1] loss: 0.284
[32,     1] loss: 0.166
[33,     1] loss: 0.219
[34,     1] loss: 0.203
[35,     1] loss: 0.201
[36,     1] loss: 0.150
[37,     1] loss: 0.141
[38,     1] loss: 0.155
[39,     1] loss: 0.155
[40,     1] loss: 0.153
[41,     1] loss: 0.147
[42,     1] loss: 0.147
[43,     1] loss: 0.171
[44,     1] loss: 0.122
[45,     1] loss: 0.123
[46,     1] loss: 0.116
[47,     1] loss: 0.106
[48,     1] loss: 0.077
[49,     1] loss: 0.098
[50,     1] loss: 0.162
[51,     1] loss: 0.088
[52,     1] loss: 0.117
[53,     1] loss: 0.142
[54,     1] loss: 0.087
[55,     1] loss: 0.122
[56,     1] loss: 0.133
[57,     1] loss: 0.096
[58,     1] loss: 0.092
[59,     1] loss: 0.106
[60,     1] loss: 0.122
Early stopping applied (best metric=0.3616243004798889)
Finished Training
Total time taken: 6.953049659729004
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.660
[4,     1] loss: 0.619
[5,     1] loss: 0.579
[6,     1] loss: 0.559
[7,     1] loss: 0.526
[8,     1] loss: 0.513
[9,     1] loss: 0.446
[10,     1] loss: 0.460
[11,     1] loss: 0.426
[12,     1] loss: 0.408
[13,     1] loss: 0.376
[14,     1] loss: 0.327
[15,     1] loss: 0.374
[16,     1] loss: 0.292
[17,     1] loss: 0.323
[18,     1] loss: 0.303
[19,     1] loss: 0.322
[20,     1] loss: 0.316
[21,     1] loss: 0.263
[22,     1] loss: 0.250
[23,     1] loss: 0.295
[24,     1] loss: 0.273
[25,     1] loss: 0.305
[26,     1] loss: 0.212
[27,     1] loss: 0.169
[28,     1] loss: 0.286
[29,     1] loss: 0.190
[30,     1] loss: 0.257
[31,     1] loss: 0.196
[32,     1] loss: 0.163
[33,     1] loss: 0.186
[34,     1] loss: 0.183
[35,     1] loss: 0.198
[36,     1] loss: 0.202
[37,     1] loss: 0.179
[38,     1] loss: 0.157
[39,     1] loss: 0.176
[40,     1] loss: 0.158
[41,     1] loss: 0.137
[42,     1] loss: 0.166
[43,     1] loss: 0.174
[44,     1] loss: 0.133
[45,     1] loss: 0.137
[46,     1] loss: 0.171
[47,     1] loss: 0.157
[48,     1] loss: 0.129
[49,     1] loss: 0.131
[50,     1] loss: 0.135
[51,     1] loss: 0.181
[52,     1] loss: 0.142
[53,     1] loss: 0.102
[54,     1] loss: 0.138
[55,     1] loss: 0.128
Early stopping applied (best metric=0.5149821639060974)
Finished Training
Total time taken: 6.395955562591553
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.674
[4,     1] loss: 0.653
[5,     1] loss: 0.633
[6,     1] loss: 0.612
[7,     1] loss: 0.599
[8,     1] loss: 0.573
[9,     1] loss: 0.528
[10,     1] loss: 0.557
[11,     1] loss: 0.509
[12,     1] loss: 0.458
[13,     1] loss: 0.416
[14,     1] loss: 0.408
[15,     1] loss: 0.428
[16,     1] loss: 0.385
[17,     1] loss: 0.427
[18,     1] loss: 0.365
[19,     1] loss: 0.358
[20,     1] loss: 0.384
[21,     1] loss: 0.328
[22,     1] loss: 0.371
[23,     1] loss: 0.333
[24,     1] loss: 0.305
[25,     1] loss: 0.277
[26,     1] loss: 0.346
[27,     1] loss: 0.309
[28,     1] loss: 0.280
[29,     1] loss: 0.311
[30,     1] loss: 0.320
[31,     1] loss: 0.352
[32,     1] loss: 0.332
[33,     1] loss: 0.230
[34,     1] loss: 0.231
[35,     1] loss: 0.223
[36,     1] loss: 0.228
[37,     1] loss: 0.273
[38,     1] loss: 0.220
[39,     1] loss: 0.255
[40,     1] loss: 0.184
[41,     1] loss: 0.193
[42,     1] loss: 0.165
[43,     1] loss: 0.156
[44,     1] loss: 0.165
[45,     1] loss: 0.146
[46,     1] loss: 0.144
[47,     1] loss: 0.156
[48,     1] loss: 0.126
[49,     1] loss: 0.093
[50,     1] loss: 0.185
[51,     1] loss: 0.115
[52,     1] loss: 0.112
[53,     1] loss: 0.082
[54,     1] loss: 0.107
[55,     1] loss: 0.125
[56,     1] loss: 0.084
[57,     1] loss: 0.053
[58,     1] loss: 0.058
[59,     1] loss: 0.109
[60,     1] loss: 0.104
[61,     1] loss: 0.125
[62,     1] loss: 0.112
[63,     1] loss: 0.092
[64,     1] loss: 0.073
[65,     1] loss: 0.078
[66,     1] loss: 0.072
[67,     1] loss: 0.080
[68,     1] loss: 0.059
[69,     1] loss: 0.054
[70,     1] loss: 0.051
[71,     1] loss: 0.042
[72,     1] loss: 0.066
[73,     1] loss: 0.061
[74,     1] loss: 0.041
[75,     1] loss: 0.058
Early stopping applied (best metric=0.431135356426239)
Finished Training
Total time taken: 8.706682920455933
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.697
[3,     1] loss: 0.684
[4,     1] loss: 0.671
[5,     1] loss: 0.646
[6,     1] loss: 0.635
[7,     1] loss: 0.605
[8,     1] loss: 0.589
[9,     1] loss: 0.590
[10,     1] loss: 0.549
[11,     1] loss: 0.525
[12,     1] loss: 0.525
[13,     1] loss: 0.529
[14,     1] loss: 0.472
[15,     1] loss: 0.430
[16,     1] loss: 0.440
[17,     1] loss: 0.430
[18,     1] loss: 0.416
[19,     1] loss: 0.400
[20,     1] loss: 0.411
[21,     1] loss: 0.400
[22,     1] loss: 0.310
[23,     1] loss: 0.369
[24,     1] loss: 0.297
[25,     1] loss: 0.366
[26,     1] loss: 0.334
[27,     1] loss: 0.383
[28,     1] loss: 0.294
[29,     1] loss: 0.241
[30,     1] loss: 0.245
[31,     1] loss: 0.255
[32,     1] loss: 0.266
[33,     1] loss: 0.240
[34,     1] loss: 0.260
[35,     1] loss: 0.366
[36,     1] loss: 0.261
[37,     1] loss: 0.279
[38,     1] loss: 0.248
[39,     1] loss: 0.324
[40,     1] loss: 0.278
[41,     1] loss: 0.247
[42,     1] loss: 0.206
[43,     1] loss: 0.232
[44,     1] loss: 0.201
[45,     1] loss: 0.268
[46,     1] loss: 0.170
[47,     1] loss: 0.202
[48,     1] loss: 0.248
[49,     1] loss: 0.168
[50,     1] loss: 0.151
[51,     1] loss: 0.167
[52,     1] loss: 0.166
[53,     1] loss: 0.139
[54,     1] loss: 0.129
[55,     1] loss: 0.126
[56,     1] loss: 0.148
[57,     1] loss: 0.133
[58,     1] loss: 0.169
[59,     1] loss: 0.134
[60,     1] loss: 0.216
[61,     1] loss: 0.154
[62,     1] loss: 0.090
[63,     1] loss: 0.230
[64,     1] loss: 0.120
[65,     1] loss: 0.105
[66,     1] loss: 0.182
[67,     1] loss: 0.152
[68,     1] loss: 0.099
[69,     1] loss: 0.085
[70,     1] loss: 0.141
[71,     1] loss: 0.105
[72,     1] loss: 0.073
[73,     1] loss: 0.107
[74,     1] loss: 0.108
[75,     1] loss: 0.122
[76,     1] loss: 0.130
[77,     1] loss: 0.073
Early stopping applied (best metric=0.3220435380935669)
Finished Training
Total time taken: 8.857723951339722
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.688
[3,     1] loss: 0.677
[4,     1] loss: 0.648
[5,     1] loss: 0.616
[6,     1] loss: 0.587
[7,     1] loss: 0.582
[8,     1] loss: 0.525
[9,     1] loss: 0.492
[10,     1] loss: 0.496
[11,     1] loss: 0.437
[12,     1] loss: 0.454
[13,     1] loss: 0.402
[14,     1] loss: 0.372
[15,     1] loss: 0.343
[16,     1] loss: 0.394
[17,     1] loss: 0.343
[18,     1] loss: 0.349
[19,     1] loss: 0.308
[20,     1] loss: 0.252
[21,     1] loss: 0.272
[22,     1] loss: 0.298
[23,     1] loss: 0.260
[24,     1] loss: 0.208
[25,     1] loss: 0.229
[26,     1] loss: 0.252
[27,     1] loss: 0.249
[28,     1] loss: 0.262
[29,     1] loss: 0.205
[30,     1] loss: 0.189
[31,     1] loss: 0.252
[32,     1] loss: 0.261
[33,     1] loss: 0.279
[34,     1] loss: 0.268
[35,     1] loss: 0.099
[36,     1] loss: 0.287
[37,     1] loss: 0.264
[38,     1] loss: 0.237
[39,     1] loss: 0.208
[40,     1] loss: 0.194
[41,     1] loss: 0.218
[42,     1] loss: 0.181
[43,     1] loss: 0.155
[44,     1] loss: 0.216
[45,     1] loss: 0.245
[46,     1] loss: 0.200
[47,     1] loss: 0.187
[48,     1] loss: 0.179
[49,     1] loss: 0.152
[50,     1] loss: 0.131
[51,     1] loss: 0.196
[52,     1] loss: 0.170
[53,     1] loss: 0.119
[54,     1] loss: 0.099
[55,     1] loss: 0.147
[56,     1] loss: 0.156
[57,     1] loss: 0.101
[58,     1] loss: 0.178
[59,     1] loss: 0.125
[60,     1] loss: 0.219
[61,     1] loss: 0.152
[62,     1] loss: 0.122
[63,     1] loss: 0.103
Early stopping applied (best metric=0.46162140369415283)
Finished Training
Total time taken: 7.295443534851074
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.695
[3,     1] loss: 0.675
[4,     1] loss: 0.660
[5,     1] loss: 0.625
[6,     1] loss: 0.611
[7,     1] loss: 0.594
[8,     1] loss: 0.553
[9,     1] loss: 0.528
[10,     1] loss: 0.492
[11,     1] loss: 0.485
[12,     1] loss: 0.440
[13,     1] loss: 0.455
[14,     1] loss: 0.435
[15,     1] loss: 0.419
[16,     1] loss: 0.410
[17,     1] loss: 0.359
[18,     1] loss: 0.321
[19,     1] loss: 0.353
[20,     1] loss: 0.362
[21,     1] loss: 0.374
[22,     1] loss: 0.301
[23,     1] loss: 0.304
[24,     1] loss: 0.336
[25,     1] loss: 0.262
[26,     1] loss: 0.294
[27,     1] loss: 0.285
[28,     1] loss: 0.257
[29,     1] loss: 0.209
[30,     1] loss: 0.287
[31,     1] loss: 0.216
[32,     1] loss: 0.237
[33,     1] loss: 0.221
[34,     1] loss: 0.218
[35,     1] loss: 0.258
[36,     1] loss: 0.209
[37,     1] loss: 0.214
[38,     1] loss: 0.124
[39,     1] loss: 0.168
[40,     1] loss: 0.127
[41,     1] loss: 0.104
[42,     1] loss: 0.170
[43,     1] loss: 0.138
[44,     1] loss: 0.101
[45,     1] loss: 0.107
[46,     1] loss: 0.095
[47,     1] loss: 0.139
[48,     1] loss: 0.106
[49,     1] loss: 0.090
[50,     1] loss: 0.146
[51,     1] loss: 0.079
[52,     1] loss: 0.092
[53,     1] loss: 0.062
[54,     1] loss: 0.062
[55,     1] loss: 0.062
[56,     1] loss: 0.087
[57,     1] loss: 0.078
[58,     1] loss: 0.071
[59,     1] loss: 0.060
[60,     1] loss: 0.058
[61,     1] loss: 0.063
[62,     1] loss: 0.045
[63,     1] loss: 0.080
[64,     1] loss: 0.049
[65,     1] loss: 0.030
[66,     1] loss: 0.051
[67,     1] loss: 0.043
[68,     1] loss: 0.040
[69,     1] loss: 0.039
[70,     1] loss: 0.040
[71,     1] loss: 0.035
[72,     1] loss: 0.044
[73,     1] loss: 0.042
[74,     1] loss: 0.045
[75,     1] loss: 0.030
[76,     1] loss: 0.025
[77,     1] loss: 0.030
[78,     1] loss: 0.042
Early stopping applied (best metric=0.32882341742515564)
Finished Training
Total time taken: 9.110169887542725
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.677
[4,     1] loss: 0.667
[5,     1] loss: 0.644
[6,     1] loss: 0.611
[7,     1] loss: 0.602
[8,     1] loss: 0.563
[9,     1] loss: 0.542
[10,     1] loss: 0.515
[11,     1] loss: 0.520
[12,     1] loss: 0.483
[13,     1] loss: 0.469
[14,     1] loss: 0.447
[15,     1] loss: 0.415
[16,     1] loss: 0.386
[17,     1] loss: 0.350
[18,     1] loss: 0.354
[19,     1] loss: 0.334
[20,     1] loss: 0.321
[21,     1] loss: 0.342
[22,     1] loss: 0.287
[23,     1] loss: 0.315
[24,     1] loss: 0.267
[25,     1] loss: 0.290
[26,     1] loss: 0.217
[27,     1] loss: 0.239
[28,     1] loss: 0.284
[29,     1] loss: 0.236
[30,     1] loss: 0.325
[31,     1] loss: 0.226
[32,     1] loss: 0.266
[33,     1] loss: 0.306
[34,     1] loss: 0.176
[35,     1] loss: 0.195
[36,     1] loss: 0.250
[37,     1] loss: 0.224
[38,     1] loss: 0.157
[39,     1] loss: 0.154
[40,     1] loss: 0.237
[41,     1] loss: 0.136
[42,     1] loss: 0.139
[43,     1] loss: 0.147
[44,     1] loss: 0.178
[45,     1] loss: 0.147
[46,     1] loss: 0.149
[47,     1] loss: 0.115
[48,     1] loss: 0.102
[49,     1] loss: 0.116
[50,     1] loss: 0.132
[51,     1] loss: 0.122
[52,     1] loss: 0.111
[53,     1] loss: 0.110
[54,     1] loss: 0.112
[55,     1] loss: 0.114
[56,     1] loss: 0.127
[57,     1] loss: 0.071
[58,     1] loss: 0.162
[59,     1] loss: 0.092
[60,     1] loss: 0.132
[61,     1] loss: 0.068
[62,     1] loss: 0.117
[63,     1] loss: 0.147
[64,     1] loss: 0.088
[65,     1] loss: 0.119
[66,     1] loss: 0.075
[67,     1] loss: 0.078
[68,     1] loss: 0.044
[69,     1] loss: 0.107
[70,     1] loss: 0.052
[71,     1] loss: 0.082
[72,     1] loss: 0.068
[73,     1] loss: 0.083
[74,     1] loss: 0.059
[75,     1] loss: 0.091
[76,     1] loss: 0.080
[77,     1] loss: 0.052
[78,     1] loss: 0.072
[79,     1] loss: 0.067
[80,     1] loss: 0.097
[81,     1] loss: 0.077
[82,     1] loss: 0.080
[83,     1] loss: 0.045
[84,     1] loss: 0.089
[85,     1] loss: 0.043
[86,     1] loss: 0.049
[87,     1] loss: 0.043
[88,     1] loss: 0.066
[89,     1] loss: 0.030
[90,     1] loss: 0.058
[91,     1] loss: 0.034
[92,     1] loss: 0.058
[93,     1] loss: 0.035
[94,     1] loss: 0.101
[95,     1] loss: 0.076
[96,     1] loss: 0.032
[97,     1] loss: 0.048
[98,     1] loss: 0.047
[99,     1] loss: 0.095
[100,     1] loss: 0.071
[101,     1] loss: 0.078
[102,     1] loss: 0.026
[103,     1] loss: 0.041
[104,     1] loss: 0.030
[105,     1] loss: 0.071
[106,     1] loss: 0.083
[107,     1] loss: 0.033
[108,     1] loss: 0.115
[109,     1] loss: 0.047
[110,     1] loss: 0.059
[111,     1] loss: 0.029
[112,     1] loss: 0.029
[113,     1] loss: 0.053
[114,     1] loss: 0.043
[115,     1] loss: 0.066
[116,     1] loss: 0.042
[117,     1] loss: 0.058
[118,     1] loss: 0.033
[119,     1] loss: 0.051
[120,     1] loss: 0.058
[121,     1] loss: 0.058
[122,     1] loss: 0.034
[123,     1] loss: 0.023
[124,     1] loss: 0.042
[125,     1] loss: 0.058
[126,     1] loss: 0.065
[127,     1] loss: 0.027
[128,     1] loss: 0.056
[129,     1] loss: 0.056
[130,     1] loss: 0.079
[131,     1] loss: 0.041
[132,     1] loss: 0.114
[133,     1] loss: 0.040
[134,     1] loss: 0.044
[135,     1] loss: 0.077
[136,     1] loss: 0.054
[137,     1] loss: 0.043
[138,     1] loss: 0.054
[139,     1] loss: 0.036
[140,     1] loss: 0.029
[141,     1] loss: 0.037
[142,     1] loss: 0.038
[143,     1] loss: 0.020
[144,     1] loss: 0.030
[145,     1] loss: 0.079
[146,     1] loss: 0.025
[147,     1] loss: 0.051
[148,     1] loss: 0.087
[149,     1] loss: 0.028
[150,     1] loss: 0.065
[151,     1] loss: 0.034
[152,     1] loss: 0.028
[153,     1] loss: 0.041
[154,     1] loss: 0.053
[155,     1] loss: 0.042
[156,     1] loss: 0.031
[157,     1] loss: 0.032
[158,     1] loss: 0.025
[159,     1] loss: 0.027
[160,     1] loss: 0.025
[161,     1] loss: 0.032
[162,     1] loss: 0.022
[163,     1] loss: 0.027
[164,     1] loss: 0.020
[165,     1] loss: 0.037
[166,     1] loss: 0.049
[167,     1] loss: 0.050
[168,     1] loss: 0.015
[169,     1] loss: 0.034
[170,     1] loss: 0.049
[171,     1] loss: 0.044
[172,     1] loss: 0.044
[173,     1] loss: 0.053
[174,     1] loss: 0.039
[175,     1] loss: 0.026
[176,     1] loss: 0.036
[177,     1] loss: 0.032
[178,     1] loss: 0.052
[179,     1] loss: 0.035
[180,     1] loss: 0.023
[181,     1] loss: 0.025
[182,     1] loss: 0.037
[183,     1] loss: 0.024
[184,     1] loss: 0.036
Early stopping applied (best metric=0.10797461122274399)
Finished Training
Total time taken: 22.06835675239563
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.678
[4,     1] loss: 0.656
[5,     1] loss: 0.629
[6,     1] loss: 0.609
[7,     1] loss: 0.604
[8,     1] loss: 0.582
[9,     1] loss: 0.538
[10,     1] loss: 0.505
[11,     1] loss: 0.498
[12,     1] loss: 0.444
[13,     1] loss: 0.413
[14,     1] loss: 0.441
[15,     1] loss: 0.399
[16,     1] loss: 0.341
[17,     1] loss: 0.339
[18,     1] loss: 0.338
[19,     1] loss: 0.282
[20,     1] loss: 0.254
[21,     1] loss: 0.296
[22,     1] loss: 0.259
[23,     1] loss: 0.322
[24,     1] loss: 0.260
[25,     1] loss: 0.242
[26,     1] loss: 0.232
[27,     1] loss: 0.230
[28,     1] loss: 0.306
[29,     1] loss: 0.290
[30,     1] loss: 0.237
[31,     1] loss: 0.198
[32,     1] loss: 0.276
[33,     1] loss: 0.232
[34,     1] loss: 0.264
[35,     1] loss: 0.257
[36,     1] loss: 0.219
[37,     1] loss: 0.180
[38,     1] loss: 0.172
[39,     1] loss: 0.217
[40,     1] loss: 0.175
[41,     1] loss: 0.145
[42,     1] loss: 0.163
[43,     1] loss: 0.129
[44,     1] loss: 0.173
[45,     1] loss: 0.179
[46,     1] loss: 0.114
[47,     1] loss: 0.181
[48,     1] loss: 0.127
[49,     1] loss: 0.122
[50,     1] loss: 0.124
[51,     1] loss: 0.107
[52,     1] loss: 0.165
[53,     1] loss: 0.172
[54,     1] loss: 0.141
[55,     1] loss: 0.162
[56,     1] loss: 0.100
[57,     1] loss: 0.109
[58,     1] loss: 0.087
[59,     1] loss: 0.081
[60,     1] loss: 0.179
[61,     1] loss: 0.085
[62,     1] loss: 0.151
[63,     1] loss: 0.154
[64,     1] loss: 0.129
[65,     1] loss: 0.081
[66,     1] loss: 0.086
[67,     1] loss: 0.114
[68,     1] loss: 0.071
[69,     1] loss: 0.126
[70,     1] loss: 0.084
[71,     1] loss: 0.110
[72,     1] loss: 0.066
[73,     1] loss: 0.060
[74,     1] loss: 0.056
[75,     1] loss: 0.079
[76,     1] loss: 0.059
[77,     1] loss: 0.063
[78,     1] loss: 0.074
[79,     1] loss: 0.082
[80,     1] loss: 0.032
[81,     1] loss: 0.043
[82,     1] loss: 0.038
[83,     1] loss: 0.051
[84,     1] loss: 0.080
[85,     1] loss: 0.032
[86,     1] loss: 0.046
[87,     1] loss: 0.039
[88,     1] loss: 0.049
[89,     1] loss: 0.091
[90,     1] loss: 0.084
Early stopping applied (best metric=0.301174134016037)
Finished Training
Total time taken: 10.75787878036499
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.689
[3,     1] loss: 0.692
[4,     1] loss: 0.667
[5,     1] loss: 0.654
[6,     1] loss: 0.622
[7,     1] loss: 0.611
[8,     1] loss: 0.582
[9,     1] loss: 0.571
[10,     1] loss: 0.545
[11,     1] loss: 0.505
[12,     1] loss: 0.489
[13,     1] loss: 0.486
[14,     1] loss: 0.453
[15,     1] loss: 0.411
[16,     1] loss: 0.393
[17,     1] loss: 0.400
[18,     1] loss: 0.361
[19,     1] loss: 0.350
[20,     1] loss: 0.409
[21,     1] loss: 0.317
[22,     1] loss: 0.317
[23,     1] loss: 0.271
[24,     1] loss: 0.273
[25,     1] loss: 0.273
[26,     1] loss: 0.307
[27,     1] loss: 0.271
[28,     1] loss: 0.244
[29,     1] loss: 0.213
[30,     1] loss: 0.264
[31,     1] loss: 0.182
[32,     1] loss: 0.236
[33,     1] loss: 0.200
[34,     1] loss: 0.164
[35,     1] loss: 0.290
[36,     1] loss: 0.242
[37,     1] loss: 0.165
[38,     1] loss: 0.197
[39,     1] loss: 0.275
[40,     1] loss: 0.234
[41,     1] loss: 0.191
[42,     1] loss: 0.179
[43,     1] loss: 0.129
[44,     1] loss: 0.179
[45,     1] loss: 0.168
[46,     1] loss: 0.170
[47,     1] loss: 0.193
[48,     1] loss: 0.169
[49,     1] loss: 0.143
[50,     1] loss: 0.170
[51,     1] loss: 0.138
[52,     1] loss: 0.133
[53,     1] loss: 0.116
[54,     1] loss: 0.130
[55,     1] loss: 0.095
[56,     1] loss: 0.132
[57,     1] loss: 0.108
[58,     1] loss: 0.100
[59,     1] loss: 0.076
[60,     1] loss: 0.098
[61,     1] loss: 0.083
[62,     1] loss: 0.062
[63,     1] loss: 0.069
[64,     1] loss: 0.102
[65,     1] loss: 0.066
[66,     1] loss: 0.126
[67,     1] loss: 0.090
[68,     1] loss: 0.052
Early stopping applied (best metric=0.3596850335597992)
Finished Training
Total time taken: 7.974810838699341
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.692
[3,     1] loss: 0.678
[4,     1] loss: 0.658
[5,     1] loss: 0.636
[6,     1] loss: 0.602
[7,     1] loss: 0.586
[8,     1] loss: 0.536
[9,     1] loss: 0.523
[10,     1] loss: 0.478
[11,     1] loss: 0.460
[12,     1] loss: 0.431
[13,     1] loss: 0.443
[14,     1] loss: 0.394
[15,     1] loss: 0.345
[16,     1] loss: 0.338
[17,     1] loss: 0.313
[18,     1] loss: 0.285
[19,     1] loss: 0.278
[20,     1] loss: 0.240
[21,     1] loss: 0.289
[22,     1] loss: 0.249
[23,     1] loss: 0.301
[24,     1] loss: 0.216
[25,     1] loss: 0.219
[26,     1] loss: 0.193
[27,     1] loss: 0.153
[28,     1] loss: 0.244
[29,     1] loss: 0.197
[30,     1] loss: 0.236
[31,     1] loss: 0.181
[32,     1] loss: 0.164
[33,     1] loss: 0.143
[34,     1] loss: 0.157
[35,     1] loss: 0.148
[36,     1] loss: 0.181
[37,     1] loss: 0.122
[38,     1] loss: 0.178
[39,     1] loss: 0.125
[40,     1] loss: 0.139
[41,     1] loss: 0.117
[42,     1] loss: 0.175
[43,     1] loss: 0.140
[44,     1] loss: 0.111
[45,     1] loss: 0.092
[46,     1] loss: 0.143
[47,     1] loss: 0.143
[48,     1] loss: 0.148
[49,     1] loss: 0.112
[50,     1] loss: 0.123
[51,     1] loss: 0.088
[52,     1] loss: 0.070
[53,     1] loss: 0.096
[54,     1] loss: 0.116
[55,     1] loss: 0.100
[56,     1] loss: 0.074
[57,     1] loss: 0.065
[58,     1] loss: 0.089
[59,     1] loss: 0.064
[60,     1] loss: 0.078
Early stopping applied (best metric=0.39375215768814087)
Finished Training
Total time taken: 7.043589353561401
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.683
[4,     1] loss: 0.664
[5,     1] loss: 0.635
[6,     1] loss: 0.610
[7,     1] loss: 0.585
[8,     1] loss: 0.580
[9,     1] loss: 0.572
[10,     1] loss: 0.556
[11,     1] loss: 0.518
[12,     1] loss: 0.467
[13,     1] loss: 0.467
[14,     1] loss: 0.492
[15,     1] loss: 0.415
[16,     1] loss: 0.424
[17,     1] loss: 0.378
[18,     1] loss: 0.416
[19,     1] loss: 0.407
[20,     1] loss: 0.384
[21,     1] loss: 0.375
[22,     1] loss: 0.355
[23,     1] loss: 0.403
[24,     1] loss: 0.357
[25,     1] loss: 0.347
[26,     1] loss: 0.328
[27,     1] loss: 0.405
[28,     1] loss: 0.308
[29,     1] loss: 0.274
[30,     1] loss: 0.230
[31,     1] loss: 0.292
[32,     1] loss: 0.307
[33,     1] loss: 0.283
[34,     1] loss: 0.253
[35,     1] loss: 0.258
[36,     1] loss: 0.215
[37,     1] loss: 0.238
[38,     1] loss: 0.236
[39,     1] loss: 0.218
[40,     1] loss: 0.211
[41,     1] loss: 0.184
[42,     1] loss: 0.213
[43,     1] loss: 0.194
[44,     1] loss: 0.216
[45,     1] loss: 0.184
[46,     1] loss: 0.183
[47,     1] loss: 0.125
[48,     1] loss: 0.138
[49,     1] loss: 0.159
[50,     1] loss: 0.131
[51,     1] loss: 0.124
[52,     1] loss: 0.117
[53,     1] loss: 0.140
[54,     1] loss: 0.103
[55,     1] loss: 0.105
[56,     1] loss: 0.101
[57,     1] loss: 0.090
[58,     1] loss: 0.143
[59,     1] loss: 0.055
[60,     1] loss: 0.097
[61,     1] loss: 0.075
[62,     1] loss: 0.081
[63,     1] loss: 0.103
[64,     1] loss: 0.082
[65,     1] loss: 0.208
[66,     1] loss: 0.065
[67,     1] loss: 0.084
[68,     1] loss: 0.140
[69,     1] loss: 0.120
[70,     1] loss: 0.121
[71,     1] loss: 0.092
[72,     1] loss: 0.179
[73,     1] loss: 0.067
[74,     1] loss: 0.156
[75,     1] loss: 0.174
[76,     1] loss: 0.113
[77,     1] loss: 0.115
[78,     1] loss: 0.088
[79,     1] loss: 0.141
[80,     1] loss: 0.106
[81,     1] loss: 0.073
[82,     1] loss: 0.075
[83,     1] loss: 0.049
[84,     1] loss: 0.081
[85,     1] loss: 0.079
[86,     1] loss: 0.048
[87,     1] loss: 0.073
[88,     1] loss: 0.062
[89,     1] loss: 0.052
[90,     1] loss: 0.051
[91,     1] loss: 0.056
[92,     1] loss: 0.077
[93,     1] loss: 0.037
[94,     1] loss: 0.066
[95,     1] loss: 0.040
[96,     1] loss: 0.043
[97,     1] loss: 0.077
[98,     1] loss: 0.034
[99,     1] loss: 0.064
[100,     1] loss: 0.050
[101,     1] loss: 0.042
[102,     1] loss: 0.038
[103,     1] loss: 0.052
[104,     1] loss: 0.042
[105,     1] loss: 0.025
[106,     1] loss: 0.042
[107,     1] loss: 0.034
[108,     1] loss: 0.041
[109,     1] loss: 0.048
[110,     1] loss: 0.049
[111,     1] loss: 0.022
[112,     1] loss: 0.024
[113,     1] loss: 0.034
[114,     1] loss: 0.043
[115,     1] loss: 0.034
[116,     1] loss: 0.052
[117,     1] loss: 0.020
[118,     1] loss: 0.077
[119,     1] loss: 0.044
[120,     1] loss: 0.076
[121,     1] loss: 0.088
[122,     1] loss: 0.034
Early stopping applied (best metric=0.22823232412338257)
Finished Training
Total time taken: 14.672369718551636
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.696
[3,     1] loss: 0.687
[4,     1] loss: 0.675
[5,     1] loss: 0.655
[6,     1] loss: 0.633
[7,     1] loss: 0.612
[8,     1] loss: 0.604
[9,     1] loss: 0.556
[10,     1] loss: 0.563
[11,     1] loss: 0.550
[12,     1] loss: 0.519
[13,     1] loss: 0.504
[14,     1] loss: 0.488
[15,     1] loss: 0.461
[16,     1] loss: 0.407
[17,     1] loss: 0.401
[18,     1] loss: 0.415
[19,     1] loss: 0.355
[20,     1] loss: 0.359
[21,     1] loss: 0.341
[22,     1] loss: 0.355
[23,     1] loss: 0.319
[24,     1] loss: 0.225
[25,     1] loss: 0.251
[26,     1] loss: 0.274
[27,     1] loss: 0.249
[28,     1] loss: 0.242
[29,     1] loss: 0.220
[30,     1] loss: 0.229
[31,     1] loss: 0.220
[32,     1] loss: 0.189
[33,     1] loss: 0.170
[34,     1] loss: 0.195
[35,     1] loss: 0.222
[36,     1] loss: 0.205
[37,     1] loss: 0.212
[38,     1] loss: 0.205
[39,     1] loss: 0.157
[40,     1] loss: 0.170
[41,     1] loss: 0.161
[42,     1] loss: 0.249
[43,     1] loss: 0.167
[44,     1] loss: 0.105
[45,     1] loss: 0.116
[46,     1] loss: 0.140
[47,     1] loss: 0.130
[48,     1] loss: 0.128
[49,     1] loss: 0.098
[50,     1] loss: 0.190
[51,     1] loss: 0.087
[52,     1] loss: 0.189
[53,     1] loss: 0.112
[54,     1] loss: 0.104
[55,     1] loss: 0.117
[56,     1] loss: 0.162
[57,     1] loss: 0.060
[58,     1] loss: 0.127
[59,     1] loss: 0.106
[60,     1] loss: 0.126
[61,     1] loss: 0.151
[62,     1] loss: 0.080
[63,     1] loss: 0.092
[64,     1] loss: 0.121
[65,     1] loss: 0.077
[66,     1] loss: 0.058
[67,     1] loss: 0.061
[68,     1] loss: 0.065
[69,     1] loss: 0.058
[70,     1] loss: 0.061
Early stopping applied (best metric=0.2797883152961731)
Finished Training
Total time taken: 8.368499517440796
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.694
[3,     1] loss: 0.688
[4,     1] loss: 0.667
[5,     1] loss: 0.639
[6,     1] loss: 0.613
[7,     1] loss: 0.614
[8,     1] loss: 0.581
[9,     1] loss: 0.562
[10,     1] loss: 0.535
[11,     1] loss: 0.506
[12,     1] loss: 0.472
[13,     1] loss: 0.438
[14,     1] loss: 0.386
[15,     1] loss: 0.392
[16,     1] loss: 0.380
[17,     1] loss: 0.355
[18,     1] loss: 0.314
[19,     1] loss: 0.328
[20,     1] loss: 0.247
[21,     1] loss: 0.308
[22,     1] loss: 0.263
[23,     1] loss: 0.197
[24,     1] loss: 0.219
[25,     1] loss: 0.171
[26,     1] loss: 0.247
[27,     1] loss: 0.164
[28,     1] loss: 0.152
[29,     1] loss: 0.167
[30,     1] loss: 0.134
[31,     1] loss: 0.151
[32,     1] loss: 0.162
[33,     1] loss: 0.125
[34,     1] loss: 0.213
[35,     1] loss: 0.126
[36,     1] loss: 0.114
[37,     1] loss: 0.146
[38,     1] loss: 0.129
[39,     1] loss: 0.104
[40,     1] loss: 0.127
[41,     1] loss: 0.108
[42,     1] loss: 0.088
[43,     1] loss: 0.126
[44,     1] loss: 0.123
[45,     1] loss: 0.132
[46,     1] loss: 0.098
[47,     1] loss: 0.102
[48,     1] loss: 0.130
[49,     1] loss: 0.097
[50,     1] loss: 0.190
[51,     1] loss: 0.076
[52,     1] loss: 0.160
[53,     1] loss: 0.168
[54,     1] loss: 0.130
[55,     1] loss: 0.099
[56,     1] loss: 0.137
[57,     1] loss: 0.108
[58,     1] loss: 0.108
[59,     1] loss: 0.125
[60,     1] loss: 0.075
[61,     1] loss: 0.088
[62,     1] loss: 0.123
[63,     1] loss: 0.104
[64,     1] loss: 0.070
[65,     1] loss: 0.079
Early stopping applied (best metric=0.37973451614379883)
Finished Training
Total time taken: 7.631402492523193
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.669
[4,     1] loss: 0.641
[5,     1] loss: 0.627
[6,     1] loss: 0.597
[7,     1] loss: 0.575
[8,     1] loss: 0.548
[9,     1] loss: 0.541
[10,     1] loss: 0.484
[11,     1] loss: 0.461
[12,     1] loss: 0.422
[13,     1] loss: 0.410
[14,     1] loss: 0.373
[15,     1] loss: 0.360
[16,     1] loss: 0.327
[17,     1] loss: 0.362
[18,     1] loss: 0.289
[19,     1] loss: 0.285
[20,     1] loss: 0.255
[21,     1] loss: 0.244
[22,     1] loss: 0.258
[23,     1] loss: 0.304
[24,     1] loss: 0.246
[25,     1] loss: 0.204
[26,     1] loss: 0.214
[27,     1] loss: 0.140
[28,     1] loss: 0.172
[29,     1] loss: 0.240
[30,     1] loss: 0.219
[31,     1] loss: 0.166
[32,     1] loss: 0.134
[33,     1] loss: 0.128
[34,     1] loss: 0.155
[35,     1] loss: 0.136
[36,     1] loss: 0.172
[37,     1] loss: 0.149
[38,     1] loss: 0.143
[39,     1] loss: 0.116
[40,     1] loss: 0.156
[41,     1] loss: 0.087
[42,     1] loss: 0.092
[43,     1] loss: 0.086
[44,     1] loss: 0.104
[45,     1] loss: 0.078
[46,     1] loss: 0.122
[47,     1] loss: 0.119
[48,     1] loss: 0.087
[49,     1] loss: 0.103
[50,     1] loss: 0.123
[51,     1] loss: 0.056
[52,     1] loss: 0.061
[53,     1] loss: 0.162
[54,     1] loss: 0.077
[55,     1] loss: 0.156
[56,     1] loss: 0.165
[57,     1] loss: 0.069
[58,     1] loss: 0.078
Early stopping applied (best metric=0.48826131224632263)
Finished Training
Total time taken: 6.812314510345459
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.683
[4,     1] loss: 0.666
[5,     1] loss: 0.635
[6,     1] loss: 0.615
[7,     1] loss: 0.588
[8,     1] loss: 0.569
[9,     1] loss: 0.542
[10,     1] loss: 0.523
[11,     1] loss: 0.464
[12,     1] loss: 0.451
[13,     1] loss: 0.424
[14,     1] loss: 0.440
[15,     1] loss: 0.386
[16,     1] loss: 0.391
[17,     1] loss: 0.348
[18,     1] loss: 0.327
[19,     1] loss: 0.332
[20,     1] loss: 0.372
[21,     1] loss: 0.311
[22,     1] loss: 0.227
[23,     1] loss: 0.271
[24,     1] loss: 0.309
[25,     1] loss: 0.275
[26,     1] loss: 0.302
[27,     1] loss: 0.299
[28,     1] loss: 0.238
[29,     1] loss: 0.242
[30,     1] loss: 0.271
[31,     1] loss: 0.150
[32,     1] loss: 0.212
[33,     1] loss: 0.199
[34,     1] loss: 0.180
[35,     1] loss: 0.188
[36,     1] loss: 0.192
[37,     1] loss: 0.170
[38,     1] loss: 0.124
[39,     1] loss: 0.124
[40,     1] loss: 0.201
[41,     1] loss: 0.163
[42,     1] loss: 0.150
[43,     1] loss: 0.139
[44,     1] loss: 0.161
[45,     1] loss: 0.133
[46,     1] loss: 0.099
[47,     1] loss: 0.130
[48,     1] loss: 0.160
[49,     1] loss: 0.099
[50,     1] loss: 0.116
[51,     1] loss: 0.144
[52,     1] loss: 0.100
[53,     1] loss: 0.117
[54,     1] loss: 0.108
[55,     1] loss: 0.146
[56,     1] loss: 0.109
[57,     1] loss: 0.088
[58,     1] loss: 0.085
[59,     1] loss: 0.105
[60,     1] loss: 0.099
[61,     1] loss: 0.084
[62,     1] loss: 0.095
[63,     1] loss: 0.069
[64,     1] loss: 0.080
[65,     1] loss: 0.063
[66,     1] loss: 0.104
[67,     1] loss: 0.082
[68,     1] loss: 0.059
Early stopping applied (best metric=0.2806002199649811)
Finished Training
Total time taken: 7.968193769454956
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.686
[3,     1] loss: 0.686
[4,     1] loss: 0.664
[5,     1] loss: 0.638
[6,     1] loss: 0.607
[7,     1] loss: 0.599
[8,     1] loss: 0.585
[9,     1] loss: 0.569
[10,     1] loss: 0.531
[11,     1] loss: 0.517
[12,     1] loss: 0.491
[13,     1] loss: 0.424
[14,     1] loss: 0.438
[15,     1] loss: 0.406
[16,     1] loss: 0.421
[17,     1] loss: 0.370
[18,     1] loss: 0.347
[19,     1] loss: 0.342
[20,     1] loss: 0.368
[21,     1] loss: 0.340
[22,     1] loss: 0.334
[23,     1] loss: 0.331
[24,     1] loss: 0.297
[25,     1] loss: 0.285
[26,     1] loss: 0.268
[27,     1] loss: 0.354
[28,     1] loss: 0.283
[29,     1] loss: 0.261
[30,     1] loss: 0.313
[31,     1] loss: 0.312
[32,     1] loss: 0.236
[33,     1] loss: 0.232
[34,     1] loss: 0.223
[35,     1] loss: 0.320
[36,     1] loss: 0.169
[37,     1] loss: 0.261
[38,     1] loss: 0.297
[39,     1] loss: 0.271
[40,     1] loss: 0.273
[41,     1] loss: 0.228
[42,     1] loss: 0.239
[43,     1] loss: 0.243
[44,     1] loss: 0.205
[45,     1] loss: 0.184
[46,     1] loss: 0.254
[47,     1] loss: 0.212
[48,     1] loss: 0.198
[49,     1] loss: 0.161
[50,     1] loss: 0.237
[51,     1] loss: 0.168
[52,     1] loss: 0.159
[53,     1] loss: 0.115
[54,     1] loss: 0.155
[55,     1] loss: 0.134
[56,     1] loss: 0.126
[57,     1] loss: 0.121
[58,     1] loss: 0.153
[59,     1] loss: 0.091
[60,     1] loss: 0.089
[61,     1] loss: 0.153
[62,     1] loss: 0.112
[63,     1] loss: 0.111
[64,     1] loss: 0.120
[65,     1] loss: 0.065
[66,     1] loss: 0.121
[67,     1] loss: 0.123
[68,     1] loss: 0.096
[69,     1] loss: 0.071
[70,     1] loss: 0.097
Early stopping applied (best metric=0.39951252937316895)
Finished Training
Total time taken: 8.758330345153809
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.687
[3,     1] loss: 0.682
[4,     1] loss: 0.658
[5,     1] loss: 0.642
[6,     1] loss: 0.628
[7,     1] loss: 0.589
[8,     1] loss: 0.578
[9,     1] loss: 0.553
[10,     1] loss: 0.549
[11,     1] loss: 0.506
[12,     1] loss: 0.496
[13,     1] loss: 0.473
[14,     1] loss: 0.443
[15,     1] loss: 0.423
[16,     1] loss: 0.392
[17,     1] loss: 0.415
[18,     1] loss: 0.407
[19,     1] loss: 0.363
[20,     1] loss: 0.353
[21,     1] loss: 0.357
[22,     1] loss: 0.368
[23,     1] loss: 0.291
[24,     1] loss: 0.340
[25,     1] loss: 0.253
[26,     1] loss: 0.292
[27,     1] loss: 0.280
[28,     1] loss: 0.259
[29,     1] loss: 0.238
[30,     1] loss: 0.264
[31,     1] loss: 0.275
[32,     1] loss: 0.266
[33,     1] loss: 0.213
[34,     1] loss: 0.254
[35,     1] loss: 0.190
[36,     1] loss: 0.326
[37,     1] loss: 0.195
[38,     1] loss: 0.155
[39,     1] loss: 0.230
[40,     1] loss: 0.189
[41,     1] loss: 0.212
[42,     1] loss: 0.267
[43,     1] loss: 0.231
[44,     1] loss: 0.190
[45,     1] loss: 0.154
[46,     1] loss: 0.168
[47,     1] loss: 0.186
[48,     1] loss: 0.228
[49,     1] loss: 0.136
[50,     1] loss: 0.222
[51,     1] loss: 0.172
[52,     1] loss: 0.145
[53,     1] loss: 0.129
[54,     1] loss: 0.126
[55,     1] loss: 0.158
[56,     1] loss: 0.156
[57,     1] loss: 0.147
[58,     1] loss: 0.139
[59,     1] loss: 0.144
[60,     1] loss: 0.162
[61,     1] loss: 0.106
[62,     1] loss: 0.086
[63,     1] loss: 0.063
[64,     1] loss: 0.058
[65,     1] loss: 0.127
[66,     1] loss: 0.083
[67,     1] loss: 0.064
[68,     1] loss: 0.057
[69,     1] loss: 0.052
[70,     1] loss: 0.081
[71,     1] loss: 0.055
[72,     1] loss: 0.064
[73,     1] loss: 0.044
[74,     1] loss: 0.035
[75,     1] loss: 0.038
Early stopping applied (best metric=0.37587594985961914)
Finished Training
Total time taken: 9.113725900650024
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.665
[4,     1] loss: 0.646
[5,     1] loss: 0.619
[6,     1] loss: 0.589
[7,     1] loss: 0.553
[8,     1] loss: 0.531
[9,     1] loss: 0.515
[10,     1] loss: 0.481
[11,     1] loss: 0.434
[12,     1] loss: 0.413
[13,     1] loss: 0.364
[14,     1] loss: 0.403
[15,     1] loss: 0.360
[16,     1] loss: 0.340
[17,     1] loss: 0.327
[18,     1] loss: 0.305
[19,     1] loss: 0.325
[20,     1] loss: 0.325
[21,     1] loss: 0.296
[22,     1] loss: 0.246
[23,     1] loss: 0.329
[24,     1] loss: 0.305
[25,     1] loss: 0.283
[26,     1] loss: 0.377
[27,     1] loss: 0.244
[28,     1] loss: 0.247
[29,     1] loss: 0.211
[30,     1] loss: 0.208
[31,     1] loss: 0.269
[32,     1] loss: 0.317
[33,     1] loss: 0.234
[34,     1] loss: 0.213
[35,     1] loss: 0.210
[36,     1] loss: 0.201
[37,     1] loss: 0.209
[38,     1] loss: 0.214
[39,     1] loss: 0.137
[40,     1] loss: 0.187
[41,     1] loss: 0.149
[42,     1] loss: 0.144
[43,     1] loss: 0.138
[44,     1] loss: 0.186
[45,     1] loss: 0.158
[46,     1] loss: 0.097
[47,     1] loss: 0.129
[48,     1] loss: 0.175
[49,     1] loss: 0.128
[50,     1] loss: 0.113
[51,     1] loss: 0.123
[52,     1] loss: 0.123
[53,     1] loss: 0.082
[54,     1] loss: 0.092
[55,     1] loss: 0.081
[56,     1] loss: 0.102
[57,     1] loss: 0.081
[58,     1] loss: 0.112
[59,     1] loss: 0.062
Early stopping applied (best metric=0.4736413359642029)
Finished Training
Total time taken: 7.286000490188599
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.686
[3,     1] loss: 0.664
[4,     1] loss: 0.644
[5,     1] loss: 0.600
[6,     1] loss: 0.566
[7,     1] loss: 0.548
[8,     1] loss: 0.519
[9,     1] loss: 0.487
[10,     1] loss: 0.480
[11,     1] loss: 0.489
[12,     1] loss: 0.435
[13,     1] loss: 0.397
[14,     1] loss: 0.365
[15,     1] loss: 0.364
[16,     1] loss: 0.296
[17,     1] loss: 0.316
[18,     1] loss: 0.346
[19,     1] loss: 0.344
[20,     1] loss: 0.328
[21,     1] loss: 0.241
[22,     1] loss: 0.297
[23,     1] loss: 0.298
[24,     1] loss: 0.263
[25,     1] loss: 0.268
[26,     1] loss: 0.310
[27,     1] loss: 0.235
[28,     1] loss: 0.317
[29,     1] loss: 0.309
[30,     1] loss: 0.212
[31,     1] loss: 0.218
[32,     1] loss: 0.241
[33,     1] loss: 0.203
[34,     1] loss: 0.161
[35,     1] loss: 0.212
[36,     1] loss: 0.219
[37,     1] loss: 0.160
[38,     1] loss: 0.212
[39,     1] loss: 0.182
[40,     1] loss: 0.183
[41,     1] loss: 0.130
[42,     1] loss: 0.152
[43,     1] loss: 0.171
[44,     1] loss: 0.149
[45,     1] loss: 0.138
[46,     1] loss: 0.139
[47,     1] loss: 0.195
[48,     1] loss: 0.148
[49,     1] loss: 0.152
[50,     1] loss: 0.160
[51,     1] loss: 0.125
[52,     1] loss: 0.102
[53,     1] loss: 0.128
[54,     1] loss: 0.144
[55,     1] loss: 0.110
[56,     1] loss: 0.116
Early stopping applied (best metric=0.5146488547325134)
Finished Training
Total time taken: 7.003869533538818
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.668
[4,     1] loss: 0.647
[5,     1] loss: 0.636
[6,     1] loss: 0.605
[7,     1] loss: 0.610
[8,     1] loss: 0.563
[9,     1] loss: 0.555
[10,     1] loss: 0.519
[11,     1] loss: 0.505
[12,     1] loss: 0.486
[13,     1] loss: 0.462
[14,     1] loss: 0.422
[15,     1] loss: 0.411
[16,     1] loss: 0.359
[17,     1] loss: 0.369
[18,     1] loss: 0.412
[19,     1] loss: 0.313
[20,     1] loss: 0.403
[21,     1] loss: 0.315
[22,     1] loss: 0.395
[23,     1] loss: 0.306
[24,     1] loss: 0.305
[25,     1] loss: 0.302
[26,     1] loss: 0.324
[27,     1] loss: 0.324
[28,     1] loss: 0.292
[29,     1] loss: 0.311
[30,     1] loss: 0.235
[31,     1] loss: 0.254
[32,     1] loss: 0.260
[33,     1] loss: 0.311
[34,     1] loss: 0.206
[35,     1] loss: 0.255
[36,     1] loss: 0.250
[37,     1] loss: 0.233
[38,     1] loss: 0.321
[39,     1] loss: 0.286
[40,     1] loss: 0.221
[41,     1] loss: 0.305
[42,     1] loss: 0.213
[43,     1] loss: 0.267
[44,     1] loss: 0.194
[45,     1] loss: 0.235
[46,     1] loss: 0.209
[47,     1] loss: 0.220
[48,     1] loss: 0.284
[49,     1] loss: 0.168
[50,     1] loss: 0.218
[51,     1] loss: 0.221
[52,     1] loss: 0.138
[53,     1] loss: 0.152
[54,     1] loss: 0.102
[55,     1] loss: 0.150
[56,     1] loss: 0.160
[57,     1] loss: 0.132
[58,     1] loss: 0.148
[59,     1] loss: 0.115
[60,     1] loss: 0.143
[61,     1] loss: 0.137
[62,     1] loss: 0.133
[63,     1] loss: 0.118
[64,     1] loss: 0.118
[65,     1] loss: 0.092
[66,     1] loss: 0.175
[67,     1] loss: 0.128
[68,     1] loss: 0.273
[69,     1] loss: 0.117
Early stopping applied (best metric=0.4016205966472626)
Finished Training
Total time taken: 7.855000019073486
{'Hydroxylation-K Validation Accuracy': 0.8352127659574469, 'Hydroxylation-K Validation Sensitivity': 0.7906666666666666, 'Hydroxylation-K Validation Specificity': 0.8463157894736842, 'Hydroxylation-K Validation Precision': 0.5867364682101525, 'Hydroxylation-K AUC ROC': 0.8283391812865497, 'Hydroxylation-K AUC PR': 0.6088901380314611, 'Hydroxylation-K MCC': 0.5785431741130234, 'Hydroxylation-K F1': 0.6670566296780452, 'Validation Loss (Hydroxylation-K)': 0.3720541700720787, 'Validation Loss (total)': 0.3720541700720787, 'TimeToTrain': 8.942600679397582}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003236883477407875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3823693516,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.261068657413189}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.687
[3,     1] loss: 0.697
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006355657597048509,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2439117823,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.1966062742761867}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.694
[3,     1] loss: 0.683
[4,     1] loss: 0.665
[5,     1] loss: 0.632
[6,     1] loss: 0.592
[7,     1] loss: 0.536
[8,     1] loss: 0.463
[9,     1] loss: 0.394
[10,     1] loss: 0.346
[11,     1] loss: 0.303
[12,     1] loss: 0.277
[13,     1] loss: 0.262
[14,     1] loss: 0.190
[15,     1] loss: 0.270
[16,     1] loss: 0.215
[17,     1] loss: 0.195
[18,     1] loss: 0.242
[19,     1] loss: 0.216
[20,     1] loss: 0.306
[21,     1] loss: 0.219
[22,     1] loss: 0.224
[23,     1] loss: 0.275
[24,     1] loss: 0.193
[25,     1] loss: 0.150
[26,     1] loss: 0.273
[27,     1] loss: 0.238
[28,     1] loss: 0.207
[29,     1] loss: 0.201
[30,     1] loss: 0.193
[31,     1] loss: 0.200
[32,     1] loss: 0.223
[33,     1] loss: 0.150
[34,     1] loss: 0.189
[35,     1] loss: 0.224
[36,     1] loss: 0.192
[37,     1] loss: 0.140
[38,     1] loss: 0.181
[39,     1] loss: 0.150
[40,     1] loss: 0.161
[41,     1] loss: 0.152
[42,     1] loss: 0.139
[43,     1] loss: 0.106
[44,     1] loss: 0.197
[45,     1] loss: 0.115
[46,     1] loss: 0.131
[47,     1] loss: 0.176
[48,     1] loss: 0.158
[49,     1] loss: 0.134
[50,     1] loss: 0.157
[51,     1] loss: 0.169
[52,     1] loss: 0.113
[53,     1] loss: 0.132
[54,     1] loss: 0.128
[55,     1] loss: 0.142
[56,     1] loss: 0.097
[57,     1] loss: 0.094
Early stopping applied (best metric=0.4595285654067993)
Finished Training
Total time taken: 6.668997764587402
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.691
[3,     1] loss: 0.680
[4,     1] loss: 0.649
[5,     1] loss: 0.607
[6,     1] loss: 0.563
[7,     1] loss: 0.506
[8,     1] loss: 0.479
[9,     1] loss: 0.424
[10,     1] loss: 0.377
[11,     1] loss: 0.343
[12,     1] loss: 0.376
[13,     1] loss: 0.335
[14,     1] loss: 0.311
[15,     1] loss: 0.281
[16,     1] loss: 0.287
[17,     1] loss: 0.229
[18,     1] loss: 0.144
[19,     1] loss: 0.190
[20,     1] loss: 0.204
[21,     1] loss: 0.163
[22,     1] loss: 0.252
[23,     1] loss: 0.174
[24,     1] loss: 0.138
[25,     1] loss: 0.235
[26,     1] loss: 0.170
[27,     1] loss: 0.253
[28,     1] loss: 0.166
[29,     1] loss: 0.190
[30,     1] loss: 0.246
[31,     1] loss: 0.181
[32,     1] loss: 0.173
[33,     1] loss: 0.197
[34,     1] loss: 0.169
[35,     1] loss: 0.203
[36,     1] loss: 0.203
[37,     1] loss: 0.157
[38,     1] loss: 0.207
[39,     1] loss: 0.165
[40,     1] loss: 0.173
[41,     1] loss: 0.157
[42,     1] loss: 0.127
[43,     1] loss: 0.107
[44,     1] loss: 0.121
[45,     1] loss: 0.128
[46,     1] loss: 0.116
[47,     1] loss: 0.087
[48,     1] loss: 0.128
[49,     1] loss: 0.144
[50,     1] loss: 0.144
[51,     1] loss: 0.072
[52,     1] loss: 0.151
[53,     1] loss: 0.112
[54,     1] loss: 0.089
[55,     1] loss: 0.091
[56,     1] loss: 0.121
[57,     1] loss: 0.090
[58,     1] loss: 0.133
[59,     1] loss: 0.251
[60,     1] loss: 0.143
[61,     1] loss: 0.290
Early stopping applied (best metric=0.24729424715042114)
Finished Training
Total time taken: 7.0260009765625
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.692
[3,     1] loss: 0.689
[4,     1] loss: 0.670
[5,     1] loss: 0.647
[6,     1] loss: 0.605
[7,     1] loss: 0.582
[8,     1] loss: 0.535
[9,     1] loss: 0.475
[10,     1] loss: 0.433
[11,     1] loss: 0.377
[12,     1] loss: 0.392
[13,     1] loss: 0.368
[14,     1] loss: 0.449
[15,     1] loss: 0.395
[16,     1] loss: 0.309
[17,     1] loss: 0.379
[18,     1] loss: 0.458
[19,     1] loss: 0.382
[20,     1] loss: 0.362
[21,     1] loss: 0.320
[22,     1] loss: 0.316
[23,     1] loss: 0.287
[24,     1] loss: 0.351
[25,     1] loss: 0.332
[26,     1] loss: 0.330
[27,     1] loss: 0.236
[28,     1] loss: 0.341
[29,     1] loss: 0.281
[30,     1] loss: 0.193
[31,     1] loss: 0.262
[32,     1] loss: 0.203
[33,     1] loss: 0.238
[34,     1] loss: 0.147
[35,     1] loss: 0.266
[36,     1] loss: 0.249
[37,     1] loss: 0.209
[38,     1] loss: 0.178
[39,     1] loss: 0.199
[40,     1] loss: 0.101
[41,     1] loss: 0.167
[42,     1] loss: 0.172
[43,     1] loss: 0.186
[44,     1] loss: 0.160
[45,     1] loss: 0.122
[46,     1] loss: 0.151
[47,     1] loss: 0.132
[48,     1] loss: 0.169
[49,     1] loss: 0.126
[50,     1] loss: 0.118
[51,     1] loss: 0.126
[52,     1] loss: 0.125
[53,     1] loss: 0.114
[54,     1] loss: 0.139
[55,     1] loss: 0.112
[56,     1] loss: 0.112
[57,     1] loss: 0.155
[58,     1] loss: 0.089
[59,     1] loss: 0.102
[60,     1] loss: 0.073
[61,     1] loss: 0.161
[62,     1] loss: 0.070
[63,     1] loss: 0.189
[64,     1] loss: 0.090
[65,     1] loss: 0.156
[66,     1] loss: 0.141
[67,     1] loss: 0.141
[68,     1] loss: 0.122
[69,     1] loss: 0.168
[70,     1] loss: 0.111
[71,     1] loss: 0.084
[72,     1] loss: 0.150
[73,     1] loss: 0.126
[74,     1] loss: 0.132
[75,     1] loss: 0.096
[76,     1] loss: 0.069
Early stopping applied (best metric=0.346541166305542)
Finished Training
Total time taken: 8.612999200820923
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.695
[3,     1] loss: 0.663
[4,     1] loss: 0.607
[5,     1] loss: 0.533
[6,     1] loss: 0.484
[7,     1] loss: 0.432
[8,     1] loss: 0.384
[9,     1] loss: 0.274
[10,     1] loss: 0.335
[11,     1] loss: 0.333
[12,     1] loss: 0.272
[13,     1] loss: 0.326
[14,     1] loss: 0.213
[15,     1] loss: 0.211
[16,     1] loss: 0.210
[17,     1] loss: 0.204
[18,     1] loss: 0.250
[19,     1] loss: 0.267
[20,     1] loss: 0.195
[21,     1] loss: 0.244
[22,     1] loss: 0.235
[23,     1] loss: 0.157
[24,     1] loss: 0.195
[25,     1] loss: 0.209
[26,     1] loss: 0.216
[27,     1] loss: 0.173
[28,     1] loss: 0.197
[29,     1] loss: 0.197
[30,     1] loss: 0.203
[31,     1] loss: 0.148
[32,     1] loss: 0.179
[33,     1] loss: 0.156
[34,     1] loss: 0.183
[35,     1] loss: 0.170
[36,     1] loss: 0.139
[37,     1] loss: 0.175
[38,     1] loss: 0.139
[39,     1] loss: 0.127
[40,     1] loss: 0.169
[41,     1] loss: 0.118
[42,     1] loss: 0.083
[43,     1] loss: 0.111
[44,     1] loss: 0.131
[45,     1] loss: 0.137
[46,     1] loss: 0.141
[47,     1] loss: 0.113
[48,     1] loss: 0.118
[49,     1] loss: 0.181
[50,     1] loss: 0.125
[51,     1] loss: 0.083
[52,     1] loss: 0.126
[53,     1] loss: 0.140
[54,     1] loss: 0.084
Early stopping applied (best metric=0.49076592922210693)
Finished Training
Total time taken: 6.378000020980835
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.678
[3,     1] loss: 0.666
[4,     1] loss: 0.623
[5,     1] loss: 0.570
[6,     1] loss: 0.517
[7,     1] loss: 0.475
[8,     1] loss: 0.471
[9,     1] loss: 0.431
[10,     1] loss: 0.351
[11,     1] loss: 0.289
[12,     1] loss: 0.310
[13,     1] loss: 0.283
[14,     1] loss: 0.237
[15,     1] loss: 0.231
[16,     1] loss: 0.229
[17,     1] loss: 0.158
[18,     1] loss: 0.177
[19,     1] loss: 0.146
[20,     1] loss: 0.213
[21,     1] loss: 0.178
[22,     1] loss: 0.169
[23,     1] loss: 0.190
[24,     1] loss: 0.201
[25,     1] loss: 0.185
[26,     1] loss: 0.191
[27,     1] loss: 0.181
[28,     1] loss: 0.201
[29,     1] loss: 0.208
[30,     1] loss: 0.161
[31,     1] loss: 0.117
[32,     1] loss: 0.166
[33,     1] loss: 0.226
[34,     1] loss: 0.165
[35,     1] loss: 0.149
[36,     1] loss: 0.200
[37,     1] loss: 0.145
[38,     1] loss: 0.130
[39,     1] loss: 0.117
[40,     1] loss: 0.135
[41,     1] loss: 0.149
[42,     1] loss: 0.136
[43,     1] loss: 0.134
[44,     1] loss: 0.130
[45,     1] loss: 0.284
[46,     1] loss: 0.124
[47,     1] loss: 0.182
[48,     1] loss: 0.179
[49,     1] loss: 0.114
[50,     1] loss: 0.122
[51,     1] loss: 0.171
[52,     1] loss: 0.144
[53,     1] loss: 0.203
[54,     1] loss: 0.121
[55,     1] loss: 0.097
[56,     1] loss: 0.093
[57,     1] loss: 0.054
[58,     1] loss: 0.161
[59,     1] loss: 0.080
[60,     1] loss: 0.106
[61,     1] loss: 0.126
Early stopping applied (best metric=0.4541959762573242)
Finished Training
Total time taken: 8.004004716873169
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.702
[3,     1] loss: 0.681
[4,     1] loss: 0.671
[5,     1] loss: 0.640
[6,     1] loss: 0.588
[7,     1] loss: 0.525
[8,     1] loss: 0.539
[9,     1] loss: 0.483
[10,     1] loss: 0.399
[11,     1] loss: 0.409
[12,     1] loss: 0.382
[13,     1] loss: 0.370
[14,     1] loss: 0.353
[15,     1] loss: 0.315
[16,     1] loss: 0.286
[17,     1] loss: 0.259
[18,     1] loss: 0.226
[19,     1] loss: 0.303
[20,     1] loss: 0.264
[21,     1] loss: 0.198
[22,     1] loss: 0.238
[23,     1] loss: 0.251
[24,     1] loss: 0.192
[25,     1] loss: 0.185
[26,     1] loss: 0.182
[27,     1] loss: 0.228
[28,     1] loss: 0.193
[29,     1] loss: 0.148
[30,     1] loss: 0.182
[31,     1] loss: 0.181
[32,     1] loss: 0.150
[33,     1] loss: 0.186
[34,     1] loss: 0.187
[35,     1] loss: 0.156
[36,     1] loss: 0.122
[37,     1] loss: 0.113
[38,     1] loss: 0.118
[39,     1] loss: 0.134
[40,     1] loss: 0.131
[41,     1] loss: 0.133
[42,     1] loss: 0.155
[43,     1] loss: 0.084
[44,     1] loss: 0.180
[45,     1] loss: 0.212
[46,     1] loss: 0.146
[47,     1] loss: 0.116
[48,     1] loss: 0.132
[49,     1] loss: 0.133
[50,     1] loss: 0.150
[51,     1] loss: 0.110
[52,     1] loss: 0.185
[53,     1] loss: 0.115
[54,     1] loss: 0.090
[55,     1] loss: 0.117
[56,     1] loss: 0.134
[57,     1] loss: 0.083
Early stopping applied (best metric=0.3496983051300049)
Finished Training
Total time taken: 6.982996940612793
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.649
[4,     1] loss: 0.586
[5,     1] loss: 0.524
[6,     1] loss: 0.480
[7,     1] loss: 0.475
[8,     1] loss: 0.421
[9,     1] loss: 0.384
[10,     1] loss: 0.390
[11,     1] loss: 0.359
[12,     1] loss: 0.338
[13,     1] loss: 0.348
[14,     1] loss: 0.303
[15,     1] loss: 0.288
[16,     1] loss: 0.270
[17,     1] loss: 0.263
[18,     1] loss: 0.263
[19,     1] loss: 0.257
[20,     1] loss: 0.230
[21,     1] loss: 0.176
[22,     1] loss: 0.205
[23,     1] loss: 0.188
[24,     1] loss: 0.231
[25,     1] loss: 0.188
[26,     1] loss: 0.149
[27,     1] loss: 0.163
[28,     1] loss: 0.197
[29,     1] loss: 0.170
[30,     1] loss: 0.140
[31,     1] loss: 0.161
[32,     1] loss: 0.147
[33,     1] loss: 0.145
[34,     1] loss: 0.175
[35,     1] loss: 0.141
[36,     1] loss: 0.138
[37,     1] loss: 0.132
[38,     1] loss: 0.148
[39,     1] loss: 0.172
[40,     1] loss: 0.136
[41,     1] loss: 0.172
[42,     1] loss: 0.140
[43,     1] loss: 0.121
[44,     1] loss: 0.121
[45,     1] loss: 0.096
[46,     1] loss: 0.123
[47,     1] loss: 0.126
[48,     1] loss: 0.092
[49,     1] loss: 0.109
[50,     1] loss: 0.108
[51,     1] loss: 0.078
[52,     1] loss: 0.137
[53,     1] loss: 0.094
[54,     1] loss: 0.080
Early stopping applied (best metric=0.4743521213531494)
Finished Training
Total time taken: 7.22951602935791
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.695
[3,     1] loss: 0.689
[4,     1] loss: 0.672
[5,     1] loss: 0.648
[6,     1] loss: 0.631
[7,     1] loss: 0.563
[8,     1] loss: 0.512
[9,     1] loss: 0.457
[10,     1] loss: 0.398
[11,     1] loss: 0.415
[12,     1] loss: 0.343
[13,     1] loss: 0.340
[14,     1] loss: 0.397
[15,     1] loss: 0.279
[16,     1] loss: 0.320
[17,     1] loss: 0.288
[18,     1] loss: 0.360
[19,     1] loss: 0.357
[20,     1] loss: 0.272
[21,     1] loss: 0.232
[22,     1] loss: 0.257
[23,     1] loss: 0.272
[24,     1] loss: 0.272
[25,     1] loss: 0.235
[26,     1] loss: 0.237
[27,     1] loss: 0.230
[28,     1] loss: 0.202
[29,     1] loss: 0.188
[30,     1] loss: 0.189
[31,     1] loss: 0.252
[32,     1] loss: 0.186
[33,     1] loss: 0.204
[34,     1] loss: 0.148
[35,     1] loss: 0.173
[36,     1] loss: 0.121
[37,     1] loss: 0.165
[38,     1] loss: 0.137
[39,     1] loss: 0.125
[40,     1] loss: 0.159
[41,     1] loss: 0.119
[42,     1] loss: 0.100
[43,     1] loss: 0.120
[44,     1] loss: 0.143
[45,     1] loss: 0.148
[46,     1] loss: 0.126
[47,     1] loss: 0.122
[48,     1] loss: 0.125
[49,     1] loss: 0.146
[50,     1] loss: 0.115
[51,     1] loss: 0.162
[52,     1] loss: 0.081
[53,     1] loss: 0.153
[54,     1] loss: 0.110
[55,     1] loss: 0.102
[56,     1] loss: 0.063
[57,     1] loss: 0.078
[58,     1] loss: 0.154
[59,     1] loss: 0.127
Early stopping applied (best metric=0.3998403549194336)
Finished Training
Total time taken: 7.177129745483398
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.694
[3,     1] loss: 0.674
[4,     1] loss: 0.634
[5,     1] loss: 0.578
[6,     1] loss: 0.508
[7,     1] loss: 0.477
[8,     1] loss: 0.449
[9,     1] loss: 0.411
[10,     1] loss: 0.369
[11,     1] loss: 0.338
[12,     1] loss: 0.342
[13,     1] loss: 0.293
[14,     1] loss: 0.248
[15,     1] loss: 0.259
[16,     1] loss: 0.252
[17,     1] loss: 0.299
[18,     1] loss: 0.290
[19,     1] loss: 0.245
[20,     1] loss: 0.344
[21,     1] loss: 0.274
[22,     1] loss: 0.313
[23,     1] loss: 0.237
[24,     1] loss: 0.256
[25,     1] loss: 0.220
[26,     1] loss: 0.260
[27,     1] loss: 0.166
[28,     1] loss: 0.201
[29,     1] loss: 0.147
[30,     1] loss: 0.178
[31,     1] loss: 0.196
[32,     1] loss: 0.163
[33,     1] loss: 0.099
[34,     1] loss: 0.119
[35,     1] loss: 0.124
[36,     1] loss: 0.127
[37,     1] loss: 0.150
[38,     1] loss: 0.102
[39,     1] loss: 0.088
[40,     1] loss: 0.086
[41,     1] loss: 0.143
[42,     1] loss: 0.160
[43,     1] loss: 0.093
[44,     1] loss: 0.073
[45,     1] loss: 0.132
[46,     1] loss: 0.096
[47,     1] loss: 0.081
[48,     1] loss: 0.107
[49,     1] loss: 0.202
[50,     1] loss: 0.108
[51,     1] loss: 0.105
[52,     1] loss: 0.106
[53,     1] loss: 0.089
[54,     1] loss: 0.124
[55,     1] loss: 0.072
[56,     1] loss: 0.081
[57,     1] loss: 0.089
[58,     1] loss: 0.102
[59,     1] loss: 0.056
Early stopping applied (best metric=0.34060388803482056)
Finished Training
Total time taken: 7.1891093254089355
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.679
[4,     1] loss: 0.652
[5,     1] loss: 0.633
[6,     1] loss: 0.552
[7,     1] loss: 0.513
[8,     1] loss: 0.444
[9,     1] loss: 0.503
[10,     1] loss: 0.471
[11,     1] loss: 0.380
[12,     1] loss: 0.429
[13,     1] loss: 0.348
[14,     1] loss: 0.360
[15,     1] loss: 0.313
[16,     1] loss: 0.378
[17,     1] loss: 0.374
[18,     1] loss: 0.354
[19,     1] loss: 0.327
[20,     1] loss: 0.314
[21,     1] loss: 0.330
[22,     1] loss: 0.292
[23,     1] loss: 0.274
[24,     1] loss: 0.236
[25,     1] loss: 0.264
[26,     1] loss: 0.305
[27,     1] loss: 0.240
[28,     1] loss: 0.161
[29,     1] loss: 0.226
[30,     1] loss: 0.190
[31,     1] loss: 0.143
[32,     1] loss: 0.177
[33,     1] loss: 0.118
[34,     1] loss: 0.148
[35,     1] loss: 0.222
[36,     1] loss: 0.081
[37,     1] loss: 0.185
[38,     1] loss: 0.123
[39,     1] loss: 0.149
[40,     1] loss: 0.183
[41,     1] loss: 0.157
[42,     1] loss: 0.206
[43,     1] loss: 0.139
[44,     1] loss: 0.120
[45,     1] loss: 0.110
[46,     1] loss: 0.112
[47,     1] loss: 0.111
[48,     1] loss: 0.093
[49,     1] loss: 0.064
[50,     1] loss: 0.077
[51,     1] loss: 0.079
[52,     1] loss: 0.129
[53,     1] loss: 0.079
[54,     1] loss: 0.070
[55,     1] loss: 0.093
[56,     1] loss: 0.077
[57,     1] loss: 0.082
[58,     1] loss: 0.047
[59,     1] loss: 0.099
[60,     1] loss: 0.055
[61,     1] loss: 0.082
[62,     1] loss: 0.051
[63,     1] loss: 0.095
[64,     1] loss: 0.066
[65,     1] loss: 0.134
[66,     1] loss: 0.098
[67,     1] loss: 0.088
[68,     1] loss: 0.067
[69,     1] loss: 0.059
[70,     1] loss: 0.083
[71,     1] loss: 0.040
Early stopping applied (best metric=0.39029520750045776)
Finished Training
Total time taken: 8.094643592834473
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.678
[4,     1] loss: 0.631
[5,     1] loss: 0.586
[6,     1] loss: 0.525
[7,     1] loss: 0.493
[8,     1] loss: 0.418
[9,     1] loss: 0.381
[10,     1] loss: 0.302
[11,     1] loss: 0.339
[12,     1] loss: 0.390
[13,     1] loss: 0.287
[14,     1] loss: 0.282
[15,     1] loss: 0.272
[16,     1] loss: 0.255
[17,     1] loss: 0.255
[18,     1] loss: 0.263
[19,     1] loss: 0.262
[20,     1] loss: 0.242
[21,     1] loss: 0.240
[22,     1] loss: 0.277
[23,     1] loss: 0.228
[24,     1] loss: 0.257
[25,     1] loss: 0.213
[26,     1] loss: 0.185
[27,     1] loss: 0.200
[28,     1] loss: 0.178
[29,     1] loss: 0.154
[30,     1] loss: 0.180
[31,     1] loss: 0.167
[32,     1] loss: 0.112
[33,     1] loss: 0.220
[34,     1] loss: 0.109
[35,     1] loss: 0.253
[36,     1] loss: 0.086
[37,     1] loss: 0.172
[38,     1] loss: 0.132
[39,     1] loss: 0.122
[40,     1] loss: 0.122
[41,     1] loss: 0.113
[42,     1] loss: 0.131
[43,     1] loss: 0.136
[44,     1] loss: 0.087
[45,     1] loss: 0.105
[46,     1] loss: 0.129
[47,     1] loss: 0.076
[48,     1] loss: 0.093
[49,     1] loss: 0.080
[50,     1] loss: 0.118
[51,     1] loss: 0.096
[52,     1] loss: 0.066
[53,     1] loss: 0.119
[54,     1] loss: 0.064
[55,     1] loss: 0.124
[56,     1] loss: 0.151
Early stopping applied (best metric=0.42225414514541626)
Finished Training
Total time taken: 6.259001016616821
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.675
[4,     1] loss: 0.650
[5,     1] loss: 0.611
[6,     1] loss: 0.548
[7,     1] loss: 0.514
[8,     1] loss: 0.461
[9,     1] loss: 0.431
[10,     1] loss: 0.379
[11,     1] loss: 0.311
[12,     1] loss: 0.291
[13,     1] loss: 0.356
[14,     1] loss: 0.377
[15,     1] loss: 0.314
[16,     1] loss: 0.312
[17,     1] loss: 0.312
[18,     1] loss: 0.259
[19,     1] loss: 0.259
[20,     1] loss: 0.244
[21,     1] loss: 0.246
[22,     1] loss: 0.221
[23,     1] loss: 0.160
[24,     1] loss: 0.156
[25,     1] loss: 0.182
[26,     1] loss: 0.137
[27,     1] loss: 0.193
[28,     1] loss: 0.106
[29,     1] loss: 0.115
[30,     1] loss: 0.133
[31,     1] loss: 0.069
[32,     1] loss: 0.087
[33,     1] loss: 0.082
[34,     1] loss: 0.104
[35,     1] loss: 0.073
[36,     1] loss: 0.069
[37,     1] loss: 0.121
[38,     1] loss: 0.056
[39,     1] loss: 0.168
[40,     1] loss: 0.113
[41,     1] loss: 0.095
[42,     1] loss: 0.212
[43,     1] loss: 0.090
[44,     1] loss: 0.149
[45,     1] loss: 0.111
[46,     1] loss: 0.101
[47,     1] loss: 0.115
[48,     1] loss: 0.149
[49,     1] loss: 0.080
[50,     1] loss: 0.130
[51,     1] loss: 0.247
[52,     1] loss: 0.130
[53,     1] loss: 0.094
[54,     1] loss: 0.151
[55,     1] loss: 0.095
[56,     1] loss: 0.100
[57,     1] loss: 0.122
[58,     1] loss: 0.107
[59,     1] loss: 0.094
[60,     1] loss: 0.099
Early stopping applied (best metric=0.2602773904800415)
Finished Training
Total time taken: 6.716996669769287
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.674
[4,     1] loss: 0.649
[5,     1] loss: 0.620
[6,     1] loss: 0.564
[7,     1] loss: 0.562
[8,     1] loss: 0.511
[9,     1] loss: 0.436
[10,     1] loss: 0.408
[11,     1] loss: 0.354
[12,     1] loss: 0.335
[13,     1] loss: 0.273
[14,     1] loss: 0.228
[15,     1] loss: 0.279
[16,     1] loss: 0.231
[17,     1] loss: 0.221
[18,     1] loss: 0.261
[19,     1] loss: 0.241
[20,     1] loss: 0.163
[21,     1] loss: 0.254
[22,     1] loss: 0.229
[23,     1] loss: 0.202
[24,     1] loss: 0.245
[25,     1] loss: 0.238
[26,     1] loss: 0.247
[27,     1] loss: 0.239
[28,     1] loss: 0.263
[29,     1] loss: 0.186
[30,     1] loss: 0.177
[31,     1] loss: 0.219
[32,     1] loss: 0.230
[33,     1] loss: 0.196
[34,     1] loss: 0.208
[35,     1] loss: 0.181
[36,     1] loss: 0.168
[37,     1] loss: 0.185
[38,     1] loss: 0.164
[39,     1] loss: 0.181
[40,     1] loss: 0.179
[41,     1] loss: 0.167
[42,     1] loss: 0.135
[43,     1] loss: 0.184
[44,     1] loss: 0.184
[45,     1] loss: 0.152
[46,     1] loss: 0.297
[47,     1] loss: 0.201
[48,     1] loss: 0.185
[49,     1] loss: 0.175
[50,     1] loss: 0.195
[51,     1] loss: 0.166
[52,     1] loss: 0.159
[53,     1] loss: 0.170
[54,     1] loss: 0.243
[55,     1] loss: 0.178
[56,     1] loss: 0.201
[57,     1] loss: 0.138
[58,     1] loss: 0.129
[59,     1] loss: 0.150
[60,     1] loss: 0.129
Early stopping applied (best metric=0.3534702956676483)
Finished Training
Total time taken: 6.77900242805481
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.695
[3,     1] loss: 0.681
[4,     1] loss: 0.663
[5,     1] loss: 0.639
[6,     1] loss: 0.589
[7,     1] loss: 0.581
[8,     1] loss: 0.502
[9,     1] loss: 0.509
[10,     1] loss: 0.447
[11,     1] loss: 0.438
[12,     1] loss: 0.458
[13,     1] loss: 0.353
[14,     1] loss: 0.349
[15,     1] loss: 0.316
[16,     1] loss: 0.368
[17,     1] loss: 0.325
[18,     1] loss: 0.382
[19,     1] loss: 0.309
[20,     1] loss: 0.334
[21,     1] loss: 0.254
[22,     1] loss: 0.287
[23,     1] loss: 0.219
[24,     1] loss: 0.245
[25,     1] loss: 0.333
[26,     1] loss: 0.211
[27,     1] loss: 0.180
[28,     1] loss: 0.187
[29,     1] loss: 0.211
[30,     1] loss: 0.152
[31,     1] loss: 0.166
[32,     1] loss: 0.153
[33,     1] loss: 0.122
[34,     1] loss: 0.126
[35,     1] loss: 0.119
[36,     1] loss: 0.137
[37,     1] loss: 0.273
[38,     1] loss: 0.154
[39,     1] loss: 0.272
[40,     1] loss: 0.320
[41,     1] loss: 0.262
[42,     1] loss: 0.215
[43,     1] loss: 0.153
[44,     1] loss: 0.221
[45,     1] loss: 0.192
[46,     1] loss: 0.193
[47,     1] loss: 0.179
[48,     1] loss: 0.226
[49,     1] loss: 0.191
[50,     1] loss: 0.179
[51,     1] loss: 0.186
[52,     1] loss: 0.140
[53,     1] loss: 0.150
[54,     1] loss: 0.120
[55,     1] loss: 0.135
[56,     1] loss: 0.079
[57,     1] loss: 0.106
[58,     1] loss: 0.106
[59,     1] loss: 0.133
[60,     1] loss: 0.132
[61,     1] loss: 0.113
[62,     1] loss: 0.089
[63,     1] loss: 0.103
[64,     1] loss: 0.126
[65,     1] loss: 0.071
[66,     1] loss: 0.152
[67,     1] loss: 0.157
[68,     1] loss: 0.043
[69,     1] loss: 0.110
[70,     1] loss: 0.095
[71,     1] loss: 0.094
[72,     1] loss: 0.113
[73,     1] loss: 0.088
[74,     1] loss: 0.127
[75,     1] loss: 0.083
Early stopping applied (best metric=0.4282342195510864)
Finished Training
Total time taken: 8.724000453948975
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.698
[3,     1] loss: 0.683
[4,     1] loss: 0.656
[5,     1] loss: 0.618
[6,     1] loss: 0.590
[7,     1] loss: 0.542
[8,     1] loss: 0.489
[9,     1] loss: 0.510
[10,     1] loss: 0.404
[11,     1] loss: 0.409
[12,     1] loss: 0.377
[13,     1] loss: 0.357
[14,     1] loss: 0.368
[15,     1] loss: 0.379
[16,     1] loss: 0.346
[17,     1] loss: 0.336
[18,     1] loss: 0.313
[19,     1] loss: 0.282
[20,     1] loss: 0.273
[21,     1] loss: 0.240
[22,     1] loss: 0.254
[23,     1] loss: 0.185
[24,     1] loss: 0.194
[25,     1] loss: 0.152
[26,     1] loss: 0.175
[27,     1] loss: 0.239
[28,     1] loss: 0.174
[29,     1] loss: 0.078
[30,     1] loss: 0.179
[31,     1] loss: 0.230
[32,     1] loss: 0.176
[33,     1] loss: 0.190
[34,     1] loss: 0.165
[35,     1] loss: 0.120
[36,     1] loss: 0.161
[37,     1] loss: 0.145
[38,     1] loss: 0.133
[39,     1] loss: 0.151
[40,     1] loss: 0.132
[41,     1] loss: 0.129
[42,     1] loss: 0.114
[43,     1] loss: 0.111
[44,     1] loss: 0.203
[45,     1] loss: 0.151
[46,     1] loss: 0.154
[47,     1] loss: 0.103
[48,     1] loss: 0.093
[49,     1] loss: 0.085
[50,     1] loss: 0.161
[51,     1] loss: 0.098
[52,     1] loss: 0.090
[53,     1] loss: 0.130
[54,     1] loss: 0.113
[55,     1] loss: 0.076
[56,     1] loss: 0.119
Early stopping applied (best metric=0.48335134983062744)
Finished Training
Total time taken: 6.393000841140747
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.698
[3,     1] loss: 0.682
[4,     1] loss: 0.679
[5,     1] loss: 0.648
[6,     1] loss: 0.630
[7,     1] loss: 0.600
[8,     1] loss: 0.561
[9,     1] loss: 0.541
[10,     1] loss: 0.494
[11,     1] loss: 0.437
[12,     1] loss: 0.462
[13,     1] loss: 0.409
[14,     1] loss: 0.429
[15,     1] loss: 0.336
[16,     1] loss: 0.399
[17,     1] loss: 0.370
[18,     1] loss: 0.297
[19,     1] loss: 0.357
[20,     1] loss: 0.335
[21,     1] loss: 0.332
[22,     1] loss: 0.276
[23,     1] loss: 0.311
[24,     1] loss: 0.260
[25,     1] loss: 0.309
[26,     1] loss: 0.281
[27,     1] loss: 0.297
[28,     1] loss: 0.264
[29,     1] loss: 0.246
[30,     1] loss: 0.206
[31,     1] loss: 0.188
[32,     1] loss: 0.253
[33,     1] loss: 0.222
[34,     1] loss: 0.198
[35,     1] loss: 0.166
[36,     1] loss: 0.154
[37,     1] loss: 0.142
[38,     1] loss: 0.151
[39,     1] loss: 0.214
[40,     1] loss: 0.138
[41,     1] loss: 0.189
[42,     1] loss: 0.166
[43,     1] loss: 0.153
[44,     1] loss: 0.233
[45,     1] loss: 0.255
[46,     1] loss: 0.170
[47,     1] loss: 0.128
[48,     1] loss: 0.174
[49,     1] loss: 0.175
[50,     1] loss: 0.135
[51,     1] loss: 0.162
[52,     1] loss: 0.143
[53,     1] loss: 0.162
[54,     1] loss: 0.186
[55,     1] loss: 0.203
[56,     1] loss: 0.086
[57,     1] loss: 0.143
[58,     1] loss: 0.136
[59,     1] loss: 0.104
[60,     1] loss: 0.133
[61,     1] loss: 0.106
[62,     1] loss: 0.128
[63,     1] loss: 0.105
[64,     1] loss: 0.123
[65,     1] loss: 0.209
[66,     1] loss: 0.157
[67,     1] loss: 0.160
[68,     1] loss: 0.100
[69,     1] loss: 0.155
Early stopping applied (best metric=0.2668668031692505)
Finished Training
Total time taken: 7.980999708175659
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.698
[3,     1] loss: 0.680
[4,     1] loss: 0.663
[5,     1] loss: 0.612
[6,     1] loss: 0.583
[7,     1] loss: 0.517
[8,     1] loss: 0.464
[9,     1] loss: 0.434
[10,     1] loss: 0.392
[11,     1] loss: 0.319
[12,     1] loss: 0.355
[13,     1] loss: 0.308
[14,     1] loss: 0.317
[15,     1] loss: 0.374
[16,     1] loss: 0.401
[17,     1] loss: 0.281
[18,     1] loss: 0.280
[19,     1] loss: 0.365
[20,     1] loss: 0.276
[21,     1] loss: 0.282
[22,     1] loss: 0.291
[23,     1] loss: 0.191
[24,     1] loss: 0.230
[25,     1] loss: 0.192
[26,     1] loss: 0.166
[27,     1] loss: 0.173
[28,     1] loss: 0.202
[29,     1] loss: 0.158
[30,     1] loss: 0.136
[31,     1] loss: 0.135
[32,     1] loss: 0.120
[33,     1] loss: 0.076
[34,     1] loss: 0.101
[35,     1] loss: 0.144
[36,     1] loss: 0.102
[37,     1] loss: 0.138
[38,     1] loss: 0.093
[39,     1] loss: 0.123
[40,     1] loss: 0.090
[41,     1] loss: 0.068
[42,     1] loss: 0.098
[43,     1] loss: 0.071
[44,     1] loss: 0.088
[45,     1] loss: 0.107
[46,     1] loss: 0.102
[47,     1] loss: 0.072
[48,     1] loss: 0.127
[49,     1] loss: 0.116
[50,     1] loss: 0.093
[51,     1] loss: 0.102
[52,     1] loss: 0.075
[53,     1] loss: 0.104
[54,     1] loss: 0.122
[55,     1] loss: 0.179
[56,     1] loss: 0.062
Early stopping applied (best metric=0.39104944467544556)
Finished Training
Total time taken: 6.242999076843262
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.679
[4,     1] loss: 0.648
[5,     1] loss: 0.597
[6,     1] loss: 0.548
[7,     1] loss: 0.487
[8,     1] loss: 0.451
[9,     1] loss: 0.372
[10,     1] loss: 0.369
[11,     1] loss: 0.375
[12,     1] loss: 0.268
[13,     1] loss: 0.332
[14,     1] loss: 0.318
[15,     1] loss: 0.369
[16,     1] loss: 0.281
[17,     1] loss: 0.321
[18,     1] loss: 0.350
[19,     1] loss: 0.251
[20,     1] loss: 0.187
[21,     1] loss: 0.227
[22,     1] loss: 0.240
[23,     1] loss: 0.222
[24,     1] loss: 0.191
[25,     1] loss: 0.214
[26,     1] loss: 0.170
[27,     1] loss: 0.210
[28,     1] loss: 0.194
[29,     1] loss: 0.148
[30,     1] loss: 0.158
[31,     1] loss: 0.223
[32,     1] loss: 0.179
[33,     1] loss: 0.157
[34,     1] loss: 0.232
[35,     1] loss: 0.194
[36,     1] loss: 0.157
[37,     1] loss: 0.130
[38,     1] loss: 0.154
[39,     1] loss: 0.141
[40,     1] loss: 0.151
[41,     1] loss: 0.170
[42,     1] loss: 0.153
[43,     1] loss: 0.136
[44,     1] loss: 0.134
[45,     1] loss: 0.113
[46,     1] loss: 0.148
[47,     1] loss: 0.149
[48,     1] loss: 0.190
[49,     1] loss: 0.083
[50,     1] loss: 0.162
[51,     1] loss: 0.125
[52,     1] loss: 0.101
[53,     1] loss: 0.078
[54,     1] loss: 0.101
[55,     1] loss: 0.133
[56,     1] loss: 0.116
[57,     1] loss: 0.078
Early stopping applied (best metric=0.3941989839076996)
Finished Training
Total time taken: 6.39300012588501
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.688
[3,     1] loss: 0.667
[4,     1] loss: 0.631
[5,     1] loss: 0.595
[6,     1] loss: 0.549
[7,     1] loss: 0.489
[8,     1] loss: 0.422
[9,     1] loss: 0.383
[10,     1] loss: 0.373
[11,     1] loss: 0.395
[12,     1] loss: 0.316
[13,     1] loss: 0.320
[14,     1] loss: 0.275
[15,     1] loss: 0.309
[16,     1] loss: 0.344
[17,     1] loss: 0.282
[18,     1] loss: 0.221
[19,     1] loss: 0.279
[20,     1] loss: 0.211
[21,     1] loss: 0.253
[22,     1] loss: 0.287
[23,     1] loss: 0.219
[24,     1] loss: 0.250
[25,     1] loss: 0.252
[26,     1] loss: 0.204
[27,     1] loss: 0.244
[28,     1] loss: 0.253
[29,     1] loss: 0.197
[30,     1] loss: 0.202
[31,     1] loss: 0.180
[32,     1] loss: 0.127
[33,     1] loss: 0.139
[34,     1] loss: 0.149
[35,     1] loss: 0.162
[36,     1] loss: 0.116
[37,     1] loss: 0.106
[38,     1] loss: 0.079
[39,     1] loss: 0.068
[40,     1] loss: 0.082
[41,     1] loss: 0.052
[42,     1] loss: 0.110
[43,     1] loss: 0.043
[44,     1] loss: 0.117
[45,     1] loss: 0.083
[46,     1] loss: 0.076
[47,     1] loss: 0.065
[48,     1] loss: 0.098
[49,     1] loss: 0.128
[50,     1] loss: 0.052
[51,     1] loss: 0.136
[52,     1] loss: 0.076
[53,     1] loss: 0.065
[54,     1] loss: 0.100
[55,     1] loss: 0.094
[56,     1] loss: 0.067
Early stopping applied (best metric=0.41572272777557373)
Finished Training
Total time taken: 6.219998598098755
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.696
[3,     1] loss: 0.658
[4,     1] loss: 0.624
[5,     1] loss: 0.601
[6,     1] loss: 0.522
[7,     1] loss: 0.507
[8,     1] loss: 0.463
[9,     1] loss: 0.419
[10,     1] loss: 0.408
[11,     1] loss: 0.343
[12,     1] loss: 0.391
[13,     1] loss: 0.366
[14,     1] loss: 0.284
[15,     1] loss: 0.297
[16,     1] loss: 0.294
[17,     1] loss: 0.253
[18,     1] loss: 0.329
[19,     1] loss: 0.259
[20,     1] loss: 0.306
[21,     1] loss: 0.252
[22,     1] loss: 0.262
[23,     1] loss: 0.233
[24,     1] loss: 0.256
[25,     1] loss: 0.182
[26,     1] loss: 0.230
[27,     1] loss: 0.188
[28,     1] loss: 0.226
[29,     1] loss: 0.225
[30,     1] loss: 0.170
[31,     1] loss: 0.221
[32,     1] loss: 0.147
[33,     1] loss: 0.142
[34,     1] loss: 0.200
[35,     1] loss: 0.171
[36,     1] loss: 0.188
[37,     1] loss: 0.197
[38,     1] loss: 0.108
[39,     1] loss: 0.159
[40,     1] loss: 0.108
[41,     1] loss: 0.101
[42,     1] loss: 0.125
[43,     1] loss: 0.142
[44,     1] loss: 0.134
[45,     1] loss: 0.157
[46,     1] loss: 0.110
[47,     1] loss: 0.118
[48,     1] loss: 0.105
[49,     1] loss: 0.069
[50,     1] loss: 0.081
[51,     1] loss: 0.120
[52,     1] loss: 0.134
[53,     1] loss: 0.069
[54,     1] loss: 0.083
[55,     1] loss: 0.144
[56,     1] loss: 0.083
[57,     1] loss: 0.125
[58,     1] loss: 0.082
[59,     1] loss: 0.080
[60,     1] loss: 0.071
[61,     1] loss: 0.086
[62,     1] loss: 0.090
[63,     1] loss: 0.069
[64,     1] loss: 0.194
[65,     1] loss: 0.039
[66,     1] loss: 0.118
[67,     1] loss: 0.083
[68,     1] loss: 0.061
[69,     1] loss: 0.049
[70,     1] loss: 0.058
[71,     1] loss: 0.066
[72,     1] loss: 0.069
[73,     1] loss: 0.052
[74,     1] loss: 0.054
[75,     1] loss: 0.044
[76,     1] loss: 0.067
[77,     1] loss: 0.050
[78,     1] loss: 0.066
[79,     1] loss: 0.055
[80,     1] loss: 0.077
[81,     1] loss: 0.043
[82,     1] loss: 0.060
[83,     1] loss: 0.090
[84,     1] loss: 0.087
[85,     1] loss: 0.144
[86,     1] loss: 0.334
[87,     1] loss: 0.070
[88,     1] loss: 0.109
[89,     1] loss: 0.142
[90,     1] loss: 0.049
[91,     1] loss: 0.087
[92,     1] loss: 0.158
Early stopping applied (best metric=0.2116314023733139)
Finished Training
Total time taken: 10.1975257396698
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.671
[4,     1] loss: 0.626
[5,     1] loss: 0.572
[6,     1] loss: 0.546
[7,     1] loss: 0.467
[8,     1] loss: 0.430
[9,     1] loss: 0.357
[10,     1] loss: 0.296
[11,     1] loss: 0.347
[12,     1] loss: 0.328
[13,     1] loss: 0.243
[14,     1] loss: 0.270
[15,     1] loss: 0.433
[16,     1] loss: 0.347
[17,     1] loss: 0.314
[18,     1] loss: 0.226
[19,     1] loss: 0.254
[20,     1] loss: 0.335
[21,     1] loss: 0.302
[22,     1] loss: 0.255
[23,     1] loss: 0.220
[24,     1] loss: 0.252
[25,     1] loss: 0.239
[26,     1] loss: 0.239
[27,     1] loss: 0.319
[28,     1] loss: 0.208
[29,     1] loss: 0.242
[30,     1] loss: 0.226
[31,     1] loss: 0.189
[32,     1] loss: 0.184
[33,     1] loss: 0.149
[34,     1] loss: 0.183
[35,     1] loss: 0.216
[36,     1] loss: 0.211
[37,     1] loss: 0.178
[38,     1] loss: 0.220
[39,     1] loss: 0.181
[40,     1] loss: 0.196
[41,     1] loss: 0.172
[42,     1] loss: 0.135
[43,     1] loss: 0.147
[44,     1] loss: 0.208
[45,     1] loss: 0.149
[46,     1] loss: 0.123
[47,     1] loss: 0.169
[48,     1] loss: 0.125
[49,     1] loss: 0.163
[50,     1] loss: 0.178
[51,     1] loss: 0.146
[52,     1] loss: 0.180
[53,     1] loss: 0.152
[54,     1] loss: 0.100
[55,     1] loss: 0.162
[56,     1] loss: 0.135
[57,     1] loss: 0.102
Early stopping applied (best metric=0.39566072821617126)
Finished Training
Total time taken: 6.393002033233643
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.677
[4,     1] loss: 0.650
[5,     1] loss: 0.614
[6,     1] loss: 0.576
[7,     1] loss: 0.519
[8,     1] loss: 0.472
[9,     1] loss: 0.406
[10,     1] loss: 0.430
[11,     1] loss: 0.340
[12,     1] loss: 0.373
[13,     1] loss: 0.378
[14,     1] loss: 0.338
[15,     1] loss: 0.374
[16,     1] loss: 0.303
[17,     1] loss: 0.280
[18,     1] loss: 0.329
[19,     1] loss: 0.286
[20,     1] loss: 0.344
[21,     1] loss: 0.300
[22,     1] loss: 0.235
[23,     1] loss: 0.185
[24,     1] loss: 0.223
[25,     1] loss: 0.223
[26,     1] loss: 0.253
[27,     1] loss: 0.226
[28,     1] loss: 0.200
[29,     1] loss: 0.198
[30,     1] loss: 0.222
[31,     1] loss: 0.217
[32,     1] loss: 0.139
[33,     1] loss: 0.232
[34,     1] loss: 0.125
[35,     1] loss: 0.138
[36,     1] loss: 0.205
[37,     1] loss: 0.146
[38,     1] loss: 0.119
[39,     1] loss: 0.120
[40,     1] loss: 0.092
[41,     1] loss: 0.130
[42,     1] loss: 0.129
[43,     1] loss: 0.108
[44,     1] loss: 0.094
[45,     1] loss: 0.091
[46,     1] loss: 0.159
[47,     1] loss: 0.083
[48,     1] loss: 0.110
[49,     1] loss: 0.073
[50,     1] loss: 0.203
[51,     1] loss: 0.150
[52,     1] loss: 0.102
[53,     1] loss: 0.218
[54,     1] loss: 0.087
[55,     1] loss: 0.117
[56,     1] loss: 0.169
[57,     1] loss: 0.139
[58,     1] loss: 0.149
[59,     1] loss: 0.148
[60,     1] loss: 0.100
[61,     1] loss: 0.094
[62,     1] loss: 0.070
[63,     1] loss: 0.100
[64,     1] loss: 0.093
[65,     1] loss: 0.068
[66,     1] loss: 0.076
Early stopping applied (best metric=0.33445292711257935)
Finished Training
Total time taken: 7.929887771606445
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.668
[4,     1] loss: 0.633
[5,     1] loss: 0.590
[6,     1] loss: 0.552
[7,     1] loss: 0.525
[8,     1] loss: 0.468
[9,     1] loss: 0.447
[10,     1] loss: 0.459
[11,     1] loss: 0.459
[12,     1] loss: 0.362
[13,     1] loss: 0.409
[14,     1] loss: 0.387
[15,     1] loss: 0.379
[16,     1] loss: 0.466
[17,     1] loss: 0.323
[18,     1] loss: 0.396
[19,     1] loss: 0.349
[20,     1] loss: 0.316
[21,     1] loss: 0.325
[22,     1] loss: 0.300
[23,     1] loss: 0.329
[24,     1] loss: 0.319
[25,     1] loss: 0.302
[26,     1] loss: 0.265
[27,     1] loss: 0.297
[28,     1] loss: 0.256
[29,     1] loss: 0.208
[30,     1] loss: 0.266
[31,     1] loss: 0.241
[32,     1] loss: 0.209
[33,     1] loss: 0.177
[34,     1] loss: 0.233
[35,     1] loss: 0.158
[36,     1] loss: 0.252
[37,     1] loss: 0.187
[38,     1] loss: 0.184
[39,     1] loss: 0.185
[40,     1] loss: 0.238
[41,     1] loss: 0.157
[42,     1] loss: 0.128
[43,     1] loss: 0.168
[44,     1] loss: 0.140
[45,     1] loss: 0.118
[46,     1] loss: 0.099
[47,     1] loss: 0.110
[48,     1] loss: 0.102
[49,     1] loss: 0.083
[50,     1] loss: 0.111
[51,     1] loss: 0.052
[52,     1] loss: 0.102
[53,     1] loss: 0.084
[54,     1] loss: 0.066
[55,     1] loss: 0.041
[56,     1] loss: 0.056
[57,     1] loss: 0.046
[58,     1] loss: 0.062
[59,     1] loss: 0.113
[60,     1] loss: 0.058
[61,     1] loss: 0.060
[62,     1] loss: 0.062
[63,     1] loss: 0.112
[64,     1] loss: 0.065
[65,     1] loss: 0.066
[66,     1] loss: 0.097
[67,     1] loss: 0.100
[68,     1] loss: 0.072
[69,     1] loss: 0.137
[70,     1] loss: 0.046
[71,     1] loss: 0.089
[72,     1] loss: 0.092
[73,     1] loss: 0.065
[74,     1] loss: 0.105
[75,     1] loss: 0.102
[76,     1] loss: 0.156
[77,     1] loss: 0.052
[78,     1] loss: 0.059
[79,     1] loss: 0.072
[80,     1] loss: 0.043
[81,     1] loss: 0.068
[82,     1] loss: 0.045
[83,     1] loss: 0.054
[84,     1] loss: 0.045
[85,     1] loss: 0.067
[86,     1] loss: 0.043
[87,     1] loss: 0.042
[88,     1] loss: 0.057
[89,     1] loss: 0.030
[90,     1] loss: 0.046
[91,     1] loss: 0.056
[92,     1] loss: 0.037
[93,     1] loss: 0.033
Early stopping applied (best metric=0.3379001319408417)
Finished Training
Total time taken: 11.487732887268066
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.672
[4,     1] loss: 0.617
[5,     1] loss: 0.572
[6,     1] loss: 0.509
[7,     1] loss: 0.487
[8,     1] loss: 0.526
[9,     1] loss: 0.395
[10,     1] loss: 0.356
[11,     1] loss: 0.342
[12,     1] loss: 0.361
[13,     1] loss: 0.268
[14,     1] loss: 0.291
[15,     1] loss: 0.263
[16,     1] loss: 0.397
[17,     1] loss: 0.286
[18,     1] loss: 0.261
[19,     1] loss: 0.239
[20,     1] loss: 0.269
[21,     1] loss: 0.214
[22,     1] loss: 0.255
[23,     1] loss: 0.262
[24,     1] loss: 0.230
[25,     1] loss: 0.213
[26,     1] loss: 0.206
[27,     1] loss: 0.188
[28,     1] loss: 0.175
[29,     1] loss: 0.212
[30,     1] loss: 0.199
[31,     1] loss: 0.151
[32,     1] loss: 0.167
[33,     1] loss: 0.176
[34,     1] loss: 0.187
[35,     1] loss: 0.153
[36,     1] loss: 0.160
[37,     1] loss: 0.187
[38,     1] loss: 0.132
[39,     1] loss: 0.134
[40,     1] loss: 0.186
[41,     1] loss: 0.131
[42,     1] loss: 0.140
[43,     1] loss: 0.119
[44,     1] loss: 0.166
[45,     1] loss: 0.109
[46,     1] loss: 0.174
[47,     1] loss: 0.148
[48,     1] loss: 0.111
[49,     1] loss: 0.122
[50,     1] loss: 0.105
[51,     1] loss: 0.140
[52,     1] loss: 0.117
[53,     1] loss: 0.115
[54,     1] loss: 0.146
[55,     1] loss: 0.133
[56,     1] loss: 0.139
[57,     1] loss: 0.166
Early stopping applied (best metric=0.36129435896873474)
Finished Training
Total time taken: 7.483080148696899
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.689
[3,     1] loss: 0.665
[4,     1] loss: 0.620
[5,     1] loss: 0.569
[6,     1] loss: 0.478
[7,     1] loss: 0.415
[8,     1] loss: 0.322
[9,     1] loss: 0.368
[10,     1] loss: 0.253
[11,     1] loss: 0.219
[12,     1] loss: 0.175
[13,     1] loss: 0.193
[14,     1] loss: 0.213
[15,     1] loss: 0.193
[16,     1] loss: 0.245
[17,     1] loss: 0.156
[18,     1] loss: 0.154
[19,     1] loss: 0.180
[20,     1] loss: 0.200
[21,     1] loss: 0.135
[22,     1] loss: 0.122
[23,     1] loss: 0.166
[24,     1] loss: 0.180
[25,     1] loss: 0.199
[26,     1] loss: 0.135
[27,     1] loss: 0.124
[28,     1] loss: 0.160
[29,     1] loss: 0.117
[30,     1] loss: 0.191
[31,     1] loss: 0.123
[32,     1] loss: 0.149
[33,     1] loss: 0.204
[34,     1] loss: 0.141
[35,     1] loss: 0.115
[36,     1] loss: 0.139
[37,     1] loss: 0.100
[38,     1] loss: 0.101
[39,     1] loss: 0.133
[40,     1] loss: 0.117
[41,     1] loss: 0.075
[42,     1] loss: 0.077
[43,     1] loss: 0.101
[44,     1] loss: 0.133
[45,     1] loss: 0.109
[46,     1] loss: 0.074
[47,     1] loss: 0.100
[48,     1] loss: 0.118
[49,     1] loss: 0.112
[50,     1] loss: 0.070
[51,     1] loss: 0.171
[52,     1] loss: 0.072
[53,     1] loss: 0.179
[54,     1] loss: 0.141
[55,     1] loss: 0.192
Early stopping applied (best metric=0.39639872312545776)
Finished Training
Total time taken: 6.627879858016968
{'Hydroxylation-K Validation Accuracy': 0.8207801418439716, 'Hydroxylation-K Validation Sensitivity': 0.8204444444444444, 'Hydroxylation-K Validation Specificity': 0.8210526315789474, 'Hydroxylation-K Validation Precision': 0.5438654936394874, 'Hydroxylation-K AUC ROC': 0.8378713450292398, 'Hydroxylation-K AUC PR': 0.5833445412736216, 'Hydroxylation-K MCC': 0.5613071393994923, 'Hydroxylation-K F1': 0.6499283008372253, 'Validation Loss (Hydroxylation-K)': 0.3762351757287979, 'Validation Loss (total)': 0.3762351757287979, 'TimeToTrain': 7.4076602268219}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005996844503968113,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3460580804,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.5431973782129147}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.695
[3,     1] loss: 0.693
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007533295144031897,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3141557228,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.064273693050987}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.645
[5,     1] loss: 0.600
[6,     1] loss: 0.551
[7,     1] loss: 0.471
[8,     1] loss: 0.425
[9,     1] loss: 0.419
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001101983184228928,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1474075702,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.229983240762373}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.699
[3,     1] loss: 0.688
[4,     1] loss: 0.684
[5,     1] loss: 0.676
[6,     1] loss: 0.670
[7,     1] loss: 0.660
[8,     1] loss: 0.656
[9,     1] loss: 0.637
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0048064950877669274,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3153308664,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.717905495892231}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.690
[3,     1] loss: 0.668
[4,     1] loss: 0.638
[5,     1] loss: 0.595
[6,     1] loss: 0.561
[7,     1] loss: 0.490
[8,     1] loss: 0.451
[9,     1] loss: 0.415
[10,     1] loss: 0.381
[11,     1] loss: 0.390
[12,     1] loss: 0.333
[13,     1] loss: 0.400
[14,     1] loss: 0.314
[15,     1] loss: 0.271
[16,     1] loss: 0.316
[17,     1] loss: 0.312
[18,     1] loss: 0.262
[19,     1] loss: 0.507
[20,     1] loss: 0.236
[21,     1] loss: 0.313
[22,     1] loss: 0.233
[23,     1] loss: 0.316
[24,     1] loss: 0.289
[25,     1] loss: 0.236
[26,     1] loss: 0.257
[27,     1] loss: 0.318
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004080190834686521,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1860440989,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.3793779755135618}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.705
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006114247251143445,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 966717489,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.3089692166544955}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.688
[3,     1] loss: 0.656
[4,     1] loss: 0.630
[5,     1] loss: 0.560
[6,     1] loss: 0.531
[7,     1] loss: 0.478
[8,     1] loss: 0.415
[9,     1] loss: 0.350
[10,     1] loss: 0.401
[11,     1] loss: 0.365
[12,     1] loss: 0.350
[13,     1] loss: 0.386
[14,     1] loss: 0.376
[15,     1] loss: 0.286
[16,     1] loss: 0.279
[17,     1] loss: 0.381
[18,     1] loss: 0.320
[19,     1] loss: 0.290
[20,     1] loss: 0.290
[21,     1] loss: 0.264
[22,     1] loss: 0.269
[23,     1] loss: 0.240
[24,     1] loss: 0.245
[25,     1] loss: 0.249
[26,     1] loss: 0.170
[27,     1] loss: 0.155
[28,     1] loss: 0.211
[29,     1] loss: 0.290
[30,     1] loss: 0.207
[31,     1] loss: 0.165
[32,     1] loss: 0.189
[33,     1] loss: 0.146
[34,     1] loss: 0.231
[35,     1] loss: 0.177
[36,     1] loss: 0.193
[37,     1] loss: 0.158
[38,     1] loss: 0.174
[39,     1] loss: 0.165
[40,     1] loss: 0.209
[41,     1] loss: 0.145
[42,     1] loss: 0.161
[43,     1] loss: 0.123
[44,     1] loss: 0.295
[45,     1] loss: 0.235
[46,     1] loss: 0.278
[47,     1] loss: 0.263
[48,     1] loss: 0.299
[49,     1] loss: 0.241
[50,     1] loss: 0.256
[51,     1] loss: 0.230
[52,     1] loss: 0.254
[53,     1] loss: 0.227
[54,     1] loss: 0.212
[55,     1] loss: 0.223
[56,     1] loss: 0.187
[57,     1] loss: 0.211
[58,     1] loss: 0.166
[59,     1] loss: 0.217
[60,     1] loss: 0.148
[61,     1] loss: 0.177
[62,     1] loss: 0.180
[63,     1] loss: 0.154
[64,     1] loss: 0.214
Early stopping applied (best metric=0.2631732225418091)
Finished Training
Total time taken: 8.262027025222778
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.665
[4,     1] loss: 0.633
[5,     1] loss: 0.581
[6,     1] loss: 0.560
[7,     1] loss: 0.501
[8,     1] loss: 0.501
[9,     1] loss: 0.402
[10,     1] loss: 0.382
[11,     1] loss: 0.468
[12,     1] loss: 0.403
[13,     1] loss: 0.399
[14,     1] loss: 0.352
[15,     1] loss: 0.368
[16,     1] loss: 0.398
[17,     1] loss: 0.297
[18,     1] loss: 0.364
[19,     1] loss: 0.311
[20,     1] loss: 0.309
[21,     1] loss: 0.267
[22,     1] loss: 0.300
[23,     1] loss: 0.278
[24,     1] loss: 0.283
[25,     1] loss: 0.267
[26,     1] loss: 0.253
[27,     1] loss: 0.279
[28,     1] loss: 0.216
[29,     1] loss: 0.251
[30,     1] loss: 0.212
[31,     1] loss: 0.264
[32,     1] loss: 0.265
[33,     1] loss: 0.200
[34,     1] loss: 0.200
[35,     1] loss: 0.254
[36,     1] loss: 0.206
[37,     1] loss: 0.159
[38,     1] loss: 0.207
[39,     1] loss: 0.227
[40,     1] loss: 0.216
[41,     1] loss: 0.191
[42,     1] loss: 0.409
[43,     1] loss: 0.251
[44,     1] loss: 0.295
[45,     1] loss: 0.287
[46,     1] loss: 0.207
[47,     1] loss: 0.313
[48,     1] loss: 0.247
[49,     1] loss: 0.255
[50,     1] loss: 0.230
[51,     1] loss: 0.286
[52,     1] loss: 0.247
[53,     1] loss: 0.243
[54,     1] loss: 0.195
[55,     1] loss: 0.204
[56,     1] loss: 0.224
[57,     1] loss: 0.237
[58,     1] loss: 0.150
[59,     1] loss: 0.196
[60,     1] loss: 0.182
[61,     1] loss: 0.211
[62,     1] loss: 0.180
[63,     1] loss: 0.186
[64,     1] loss: 0.247
[65,     1] loss: 0.252
[66,     1] loss: 0.214
[67,     1] loss: 0.203
[68,     1] loss: 0.166
[69,     1] loss: 0.217
[70,     1] loss: 0.175
[71,     1] loss: 0.193
[72,     1] loss: 0.200
[73,     1] loss: 0.131
[74,     1] loss: 0.152
[75,     1] loss: 0.157
[76,     1] loss: 0.134
[77,     1] loss: 0.160
[78,     1] loss: 0.163
[79,     1] loss: 0.117
[80,     1] loss: 0.133
[81,     1] loss: 0.125
[82,     1] loss: 0.092
Early stopping applied (best metric=0.2609339654445648)
Finished Training
Total time taken: 10.53699803352356
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.695
[3,     1] loss: 0.675
[4,     1] loss: 0.643
[5,     1] loss: 0.615
[6,     1] loss: 0.555
[7,     1] loss: 0.525
[8,     1] loss: 0.467
[9,     1] loss: 0.399
[10,     1] loss: 0.424
[11,     1] loss: 0.386
[12,     1] loss: 0.330
[13,     1] loss: 0.326
[14,     1] loss: 0.422
[15,     1] loss: 0.271
[16,     1] loss: 0.284
[17,     1] loss: 0.365
[18,     1] loss: 0.291
[19,     1] loss: 0.345
[20,     1] loss: 0.306
[21,     1] loss: 0.294
[22,     1] loss: 0.271
[23,     1] loss: 0.261
[24,     1] loss: 0.226
[25,     1] loss: 0.253
[26,     1] loss: 0.236
[27,     1] loss: 0.343
[28,     1] loss: 0.197
[29,     1] loss: 0.197
[30,     1] loss: 0.246
[31,     1] loss: 0.280
[32,     1] loss: 0.239
[33,     1] loss: 0.202
[34,     1] loss: 0.232
[35,     1] loss: 0.208
[36,     1] loss: 0.238
[37,     1] loss: 0.167
[38,     1] loss: 0.216
[39,     1] loss: 0.173
[40,     1] loss: 0.178
[41,     1] loss: 0.186
[42,     1] loss: 0.154
[43,     1] loss: 0.168
[44,     1] loss: 0.151
[45,     1] loss: 0.267
[46,     1] loss: 0.116
[47,     1] loss: 0.134
[48,     1] loss: 0.136
[49,     1] loss: 0.158
[50,     1] loss: 0.179
[51,     1] loss: 0.131
[52,     1] loss: 0.160
[53,     1] loss: 0.122
[54,     1] loss: 0.140
[55,     1] loss: 0.112
[56,     1] loss: 0.108
[57,     1] loss: 0.131
[58,     1] loss: 0.136
[59,     1] loss: 0.143
[60,     1] loss: 0.117
[61,     1] loss: 0.153
[62,     1] loss: 0.112
[63,     1] loss: 0.148
[64,     1] loss: 0.098
[65,     1] loss: 0.152
[66,     1] loss: 0.114
[67,     1] loss: 0.149
[68,     1] loss: 0.091
[69,     1] loss: 0.063
[70,     1] loss: 0.107
[71,     1] loss: 0.375
[72,     1] loss: 0.255
[73,     1] loss: 0.231
[74,     1] loss: 0.238
[75,     1] loss: 0.205
[76,     1] loss: 0.207
[77,     1] loss: 0.246
[78,     1] loss: 0.223
[79,     1] loss: 0.237
[80,     1] loss: 0.217
[81,     1] loss: 0.186
[82,     1] loss: 0.169
[83,     1] loss: 0.177
[84,     1] loss: 0.189
[85,     1] loss: 0.160
[86,     1] loss: 0.144
[87,     1] loss: 0.141
[88,     1] loss: 0.126
[89,     1] loss: 0.130
[90,     1] loss: 0.107
[91,     1] loss: 0.147
[92,     1] loss: 0.129
[93,     1] loss: 0.098
[94,     1] loss: 0.152
[95,     1] loss: 0.127
[96,     1] loss: 0.145
[97,     1] loss: 0.106
[98,     1] loss: 0.139
[99,     1] loss: 0.187
[100,     1] loss: 0.108
[101,     1] loss: 0.187
[102,     1] loss: 0.131
[103,     1] loss: 0.149
[104,     1] loss: 0.121
[105,     1] loss: 0.115
[106,     1] loss: 0.088
[107,     1] loss: 0.117
[108,     1] loss: 0.147
[109,     1] loss: 0.083
[110,     1] loss: 0.126
[111,     1] loss: 0.114
[112,     1] loss: 0.099
[113,     1] loss: 0.311
[114,     1] loss: 0.183
[115,     1] loss: 0.233
[116,     1] loss: 0.163
[117,     1] loss: 0.227
[118,     1] loss: 0.174
Early stopping applied (best metric=0.08156444132328033)
Finished Training
Total time taken: 14.990139722824097
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.692
[3,     1] loss: 0.699
[4,     1] loss: 0.689
[5,     1] loss: 0.681
[6,     1] loss: 0.656
[7,     1] loss: 0.631
[8,     1] loss: 0.614
[9,     1] loss: 0.566
[10,     1] loss: 0.575
[11,     1] loss: 0.517
[12,     1] loss: 0.536
[13,     1] loss: 0.471
[14,     1] loss: 0.485
[15,     1] loss: 0.410
[16,     1] loss: 0.474
[17,     1] loss: 0.425
[18,     1] loss: 0.434
[19,     1] loss: 0.397
[20,     1] loss: 0.372
[21,     1] loss: 0.374
[22,     1] loss: 0.375
[23,     1] loss: 0.295
[24,     1] loss: 0.307
[25,     1] loss: 0.331
[26,     1] loss: 0.333
[27,     1] loss: 0.364
[28,     1] loss: 0.342
[29,     1] loss: 0.336
[30,     1] loss: 0.391
[31,     1] loss: 0.384
[32,     1] loss: 0.331
[33,     1] loss: 0.316
[34,     1] loss: 0.378
[35,     1] loss: 0.373
[36,     1] loss: 0.297
[37,     1] loss: 0.303
[38,     1] loss: 0.351
[39,     1] loss: 0.294
[40,     1] loss: 0.303
[41,     1] loss: 0.257
[42,     1] loss: 0.226
[43,     1] loss: 0.254
[44,     1] loss: 0.246
[45,     1] loss: 0.234
[46,     1] loss: 0.272
[47,     1] loss: 0.212
[48,     1] loss: 0.212
[49,     1] loss: 0.235
[50,     1] loss: 0.178
[51,     1] loss: 0.245
[52,     1] loss: 0.210
[53,     1] loss: 0.199
[54,     1] loss: 0.204
[55,     1] loss: 0.208
[56,     1] loss: 0.181
[57,     1] loss: 0.196
[58,     1] loss: 0.242
[59,     1] loss: 0.136
[60,     1] loss: 0.211
[61,     1] loss: 0.184
[62,     1] loss: 0.380
[63,     1] loss: 0.169
[64,     1] loss: 0.283
[65,     1] loss: 0.176
Early stopping applied (best metric=0.4035375714302063)
Finished Training
Total time taken: 8.30251669883728
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.676
[4,     1] loss: 0.646
[5,     1] loss: 0.589
[6,     1] loss: 0.547
[7,     1] loss: 0.499
[8,     1] loss: 0.438
[9,     1] loss: 0.419
[10,     1] loss: 0.379
[11,     1] loss: 0.385
[12,     1] loss: 0.349
[13,     1] loss: 0.370
[14,     1] loss: 0.360
[15,     1] loss: 0.371
[16,     1] loss: 0.346
[17,     1] loss: 0.315
[18,     1] loss: 0.264
[19,     1] loss: 0.312
[20,     1] loss: 0.222
[21,     1] loss: 0.261
[22,     1] loss: 0.282
[23,     1] loss: 0.281
[24,     1] loss: 0.222
[25,     1] loss: 0.190
[26,     1] loss: 0.201
[27,     1] loss: 0.202
[28,     1] loss: 0.164
[29,     1] loss: 0.174
[30,     1] loss: 0.169
[31,     1] loss: 0.174
[32,     1] loss: 0.140
[33,     1] loss: 0.134
[34,     1] loss: 0.147
[35,     1] loss: 0.138
[36,     1] loss: 0.179
[37,     1] loss: 0.128
[38,     1] loss: 0.152
[39,     1] loss: 0.142
[40,     1] loss: 0.156
[41,     1] loss: 0.145
[42,     1] loss: 0.142
[43,     1] loss: 0.232
[44,     1] loss: 0.156
[45,     1] loss: 0.218
[46,     1] loss: 0.201
[47,     1] loss: 0.165
[48,     1] loss: 0.233
[49,     1] loss: 0.153
[50,     1] loss: 0.190
[51,     1] loss: 0.184
[52,     1] loss: 0.202
[53,     1] loss: 0.193
[54,     1] loss: 0.168
[55,     1] loss: 0.175
[56,     1] loss: 0.187
Early stopping applied (best metric=0.4295502007007599)
Finished Training
Total time taken: 7.187117099761963
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.685
[4,     1] loss: 0.669
[5,     1] loss: 0.632
[6,     1] loss: 0.595
[7,     1] loss: 0.556
[8,     1] loss: 0.496
[9,     1] loss: 0.459
[10,     1] loss: 0.399
[11,     1] loss: 0.299
[12,     1] loss: 0.282
[13,     1] loss: 0.293
[14,     1] loss: 0.326
[15,     1] loss: 0.288
[16,     1] loss: 0.214
[17,     1] loss: 0.259
[18,     1] loss: 0.250
[19,     1] loss: 0.257
[20,     1] loss: 0.194
[21,     1] loss: 0.172
[22,     1] loss: 0.162
[23,     1] loss: 0.174
[24,     1] loss: 0.158
[25,     1] loss: 0.183
[26,     1] loss: 0.168
[27,     1] loss: 0.155
[28,     1] loss: 0.130
[29,     1] loss: 0.154
[30,     1] loss: 0.123
[31,     1] loss: 0.172
[32,     1] loss: 0.111
[33,     1] loss: 0.130
[34,     1] loss: 0.188
[35,     1] loss: 0.178
[36,     1] loss: 0.192
[37,     1] loss: 0.209
[38,     1] loss: 0.274
[39,     1] loss: 0.190
[40,     1] loss: 0.176
[41,     1] loss: 0.206
[42,     1] loss: 0.179
[43,     1] loss: 0.172
[44,     1] loss: 0.147
[45,     1] loss: 0.164
[46,     1] loss: 0.132
[47,     1] loss: 0.148
[48,     1] loss: 0.155
[49,     1] loss: 0.162
[50,     1] loss: 0.158
[51,     1] loss: 0.132
[52,     1] loss: 0.159
[53,     1] loss: 0.122
[54,     1] loss: 0.128
[55,     1] loss: 0.133
[56,     1] loss: 0.084
[57,     1] loss: 0.150
[58,     1] loss: 0.162
[59,     1] loss: 0.104
Early stopping applied (best metric=0.44201621413230896)
Finished Training
Total time taken: 7.532636642456055
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.689
[3,     1] loss: 0.672
[4,     1] loss: 0.629
[5,     1] loss: 0.593
[6,     1] loss: 0.541
[7,     1] loss: 0.484
[8,     1] loss: 0.459
[9,     1] loss: 0.387
[10,     1] loss: 0.383
[11,     1] loss: 0.341
[12,     1] loss: 0.265
[13,     1] loss: 0.308
[14,     1] loss: 0.239
[15,     1] loss: 0.389
[16,     1] loss: 0.291
[17,     1] loss: 0.369
[18,     1] loss: 0.329
[19,     1] loss: 0.242
[20,     1] loss: 0.335
[21,     1] loss: 0.243
[22,     1] loss: 0.239
[23,     1] loss: 0.261
[24,     1] loss: 0.236
[25,     1] loss: 0.242
[26,     1] loss: 0.260
[27,     1] loss: 0.227
[28,     1] loss: 0.265
[29,     1] loss: 0.226
[30,     1] loss: 0.244
[31,     1] loss: 0.196
[32,     1] loss: 0.200
[33,     1] loss: 0.253
[34,     1] loss: 0.227
[35,     1] loss: 0.187
[36,     1] loss: 0.164
[37,     1] loss: 0.209
[38,     1] loss: 0.187
[39,     1] loss: 0.196
[40,     1] loss: 0.178
[41,     1] loss: 0.166
[42,     1] loss: 0.156
[43,     1] loss: 0.209
[44,     1] loss: 0.168
[45,     1] loss: 0.257
[46,     1] loss: 0.162
[47,     1] loss: 0.399
[48,     1] loss: 0.181
[49,     1] loss: 0.210
[50,     1] loss: 0.200
[51,     1] loss: 0.253
[52,     1] loss: 0.192
[53,     1] loss: 0.222
[54,     1] loss: 0.203
[55,     1] loss: 0.194
Early stopping applied (best metric=0.4586044251918793)
Finished Training
Total time taken: 7.013515949249268
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.695
[3,     1] loss: 0.672
[4,     1] loss: 0.650
[5,     1] loss: 0.607
[6,     1] loss: 0.559
[7,     1] loss: 0.490
[8,     1] loss: 0.446
[9,     1] loss: 0.466
[10,     1] loss: 0.398
[11,     1] loss: 0.368
[12,     1] loss: 0.298
[13,     1] loss: 0.302
[14,     1] loss: 0.313
[15,     1] loss: 0.354
[16,     1] loss: 0.261
[17,     1] loss: 0.306
[18,     1] loss: 0.292
[19,     1] loss: 0.388
[20,     1] loss: 0.297
[21,     1] loss: 0.270
[22,     1] loss: 0.270
[23,     1] loss: 0.284
[24,     1] loss: 0.275
[25,     1] loss: 0.270
[26,     1] loss: 0.275
[27,     1] loss: 0.280
[28,     1] loss: 0.279
[29,     1] loss: 0.258
[30,     1] loss: 0.224
[31,     1] loss: 0.198
[32,     1] loss: 0.233
[33,     1] loss: 0.242
[34,     1] loss: 0.228
[35,     1] loss: 0.210
[36,     1] loss: 0.181
[37,     1] loss: 0.247
[38,     1] loss: 0.178
[39,     1] loss: 0.293
[40,     1] loss: 0.200
[41,     1] loss: 0.352
[42,     1] loss: 0.267
[43,     1] loss: 0.254
[44,     1] loss: 0.213
[45,     1] loss: 0.262
[46,     1] loss: 0.266
[47,     1] loss: 0.246
[48,     1] loss: 0.266
[49,     1] loss: 0.237
[50,     1] loss: 0.200
[51,     1] loss: 0.252
[52,     1] loss: 0.188
[53,     1] loss: 0.194
[54,     1] loss: 0.161
[55,     1] loss: 0.229
[56,     1] loss: 0.262
[57,     1] loss: 0.241
[58,     1] loss: 0.213
[59,     1] loss: 0.212
[60,     1] loss: 0.209
[61,     1] loss: 0.196
[62,     1] loss: 0.176
[63,     1] loss: 0.244
[64,     1] loss: 0.204
Early stopping applied (best metric=0.39474281668663025)
Finished Training
Total time taken: 8.21299934387207
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.641
[5,     1] loss: 0.590
[6,     1] loss: 0.541
[7,     1] loss: 0.492
[8,     1] loss: 0.466
[9,     1] loss: 0.417
[10,     1] loss: 0.353
[11,     1] loss: 0.312
[12,     1] loss: 0.352
[13,     1] loss: 0.310
[14,     1] loss: 0.386
[15,     1] loss: 0.299
[16,     1] loss: 0.234
[17,     1] loss: 0.254
[18,     1] loss: 0.308
[19,     1] loss: 0.263
[20,     1] loss: 0.312
[21,     1] loss: 0.224
[22,     1] loss: 0.258
[23,     1] loss: 0.233
[24,     1] loss: 0.284
[25,     1] loss: 0.279
[26,     1] loss: 0.148
[27,     1] loss: 0.275
[28,     1] loss: 0.266
[29,     1] loss: 0.225
[30,     1] loss: 0.163
[31,     1] loss: 0.236
[32,     1] loss: 0.240
[33,     1] loss: 0.195
[34,     1] loss: 0.207
[35,     1] loss: 0.172
[36,     1] loss: 0.222
[37,     1] loss: 0.173
[38,     1] loss: 0.136
[39,     1] loss: 0.168
[40,     1] loss: 0.100
[41,     1] loss: 0.149
[42,     1] loss: 0.187
[43,     1] loss: 0.205
[44,     1] loss: 0.149
[45,     1] loss: 0.185
[46,     1] loss: 0.157
[47,     1] loss: 0.150
[48,     1] loss: 0.149
[49,     1] loss: 0.173
[50,     1] loss: 0.150
[51,     1] loss: 0.184
[52,     1] loss: 0.154
[53,     1] loss: 0.103
[54,     1] loss: 0.122
[55,     1] loss: 0.139
[56,     1] loss: 0.287
[57,     1] loss: 0.265
[58,     1] loss: 0.267
[59,     1] loss: 0.386
[60,     1] loss: 0.298
[61,     1] loss: 0.282
[62,     1] loss: 0.266
[63,     1] loss: 0.320
[64,     1] loss: 0.281
[65,     1] loss: 0.311
[66,     1] loss: 0.285
[67,     1] loss: 0.248
[68,     1] loss: 0.233
[69,     1] loss: 0.239
[70,     1] loss: 0.238
[71,     1] loss: 0.238
[72,     1] loss: 0.154
[73,     1] loss: 0.210
[74,     1] loss: 0.213
[75,     1] loss: 0.196
[76,     1] loss: 0.181
[77,     1] loss: 0.138
[78,     1] loss: 0.143
[79,     1] loss: 0.160
[80,     1] loss: 0.154
[81,     1] loss: 0.209
[82,     1] loss: 0.158
[83,     1] loss: 0.178
[84,     1] loss: 0.131
[85,     1] loss: 0.122
[86,     1] loss: 0.162
[87,     1] loss: 0.144
[88,     1] loss: 0.142
[89,     1] loss: 0.131
[90,     1] loss: 0.147
[91,     1] loss: 0.212
[92,     1] loss: 0.141
[93,     1] loss: 0.143
[94,     1] loss: 0.155
[95,     1] loss: 0.128
[96,     1] loss: 0.196
[97,     1] loss: 0.166
[98,     1] loss: 0.165
[99,     1] loss: 0.198
[100,     1] loss: 0.164
[101,     1] loss: 0.120
[102,     1] loss: 0.207
Early stopping applied (best metric=0.3321189880371094)
Finished Training
Total time taken: 13.023035764694214
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.694
[3,     1] loss: 0.689
[4,     1] loss: 0.682
[5,     1] loss: 0.655
[6,     1] loss: 0.639
[7,     1] loss: 0.584
[8,     1] loss: 0.551
[9,     1] loss: 0.564
[10,     1] loss: 0.495
[11,     1] loss: 0.412
[12,     1] loss: 0.407
[13,     1] loss: 0.338
[14,     1] loss: 0.366
[15,     1] loss: 0.335
[16,     1] loss: 0.322
[17,     1] loss: 0.381
[18,     1] loss: 0.360
[19,     1] loss: 0.265
[20,     1] loss: 0.269
[21,     1] loss: 0.291
[22,     1] loss: 0.251
[23,     1] loss: 0.299
[24,     1] loss: 0.208
[25,     1] loss: 0.241
[26,     1] loss: 0.257
[27,     1] loss: 0.188
[28,     1] loss: 0.192
[29,     1] loss: 0.199
[30,     1] loss: 0.124
[31,     1] loss: 0.121
[32,     1] loss: 0.194
[33,     1] loss: 0.227
[34,     1] loss: 0.396
[35,     1] loss: 0.271
[36,     1] loss: 0.533
[37,     1] loss: 0.222
[38,     1] loss: 0.315
[39,     1] loss: 0.337
[40,     1] loss: 0.301
[41,     1] loss: 0.286
[42,     1] loss: 0.274
[43,     1] loss: 0.263
[44,     1] loss: 0.288
[45,     1] loss: 0.293
[46,     1] loss: 0.256
[47,     1] loss: 0.256
[48,     1] loss: 0.212
[49,     1] loss: 0.232
[50,     1] loss: 0.204
[51,     1] loss: 0.205
[52,     1] loss: 0.233
[53,     1] loss: 0.185
[54,     1] loss: 0.216
[55,     1] loss: 0.212
[56,     1] loss: 0.235
[57,     1] loss: 0.183
[58,     1] loss: 0.199
[59,     1] loss: 0.171
[60,     1] loss: 0.139
[61,     1] loss: 0.167
[62,     1] loss: 0.196
[63,     1] loss: 0.130
[64,     1] loss: 0.180
[65,     1] loss: 0.185
Early stopping applied (best metric=0.31256550550460815)
Finished Training
Total time taken: 8.325515031814575
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.699
[3,     1] loss: 0.683
[4,     1] loss: 0.659
[5,     1] loss: 0.629
[6,     1] loss: 0.583
[7,     1] loss: 0.541
[8,     1] loss: 0.499
[9,     1] loss: 0.489
[10,     1] loss: 0.454
[11,     1] loss: 0.404
[12,     1] loss: 0.351
[13,     1] loss: 0.347
[14,     1] loss: 0.331
[15,     1] loss: 0.388
[16,     1] loss: 0.339
[17,     1] loss: 0.450
[18,     1] loss: 0.387
[19,     1] loss: 0.387
[20,     1] loss: 0.345
[21,     1] loss: 0.394
[22,     1] loss: 0.360
[23,     1] loss: 0.357
[24,     1] loss: 0.324
[25,     1] loss: 0.331
[26,     1] loss: 0.337
[27,     1] loss: 0.311
[28,     1] loss: 0.291
[29,     1] loss: 0.264
[30,     1] loss: 0.265
[31,     1] loss: 0.227
[32,     1] loss: 0.241
[33,     1] loss: 0.348
[34,     1] loss: 0.201
[35,     1] loss: 0.233
[36,     1] loss: 0.270
[37,     1] loss: 0.225
[38,     1] loss: 0.248
[39,     1] loss: 0.251
[40,     1] loss: 0.207
[41,     1] loss: 0.211
[42,     1] loss: 0.266
[43,     1] loss: 0.196
[44,     1] loss: 0.221
[45,     1] loss: 0.193
[46,     1] loss: 0.170
[47,     1] loss: 0.166
[48,     1] loss: 0.206
[49,     1] loss: 0.214
[50,     1] loss: 0.253
[51,     1] loss: 0.224
[52,     1] loss: 0.291
[53,     1] loss: 0.202
[54,     1] loss: 0.209
[55,     1] loss: 0.277
[56,     1] loss: 0.195
[57,     1] loss: 0.254
[58,     1] loss: 0.296
[59,     1] loss: 0.218
[60,     1] loss: 0.218
Early stopping applied (best metric=0.3187916874885559)
Finished Training
Total time taken: 7.664522171020508
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.688
[3,     1] loss: 0.646
[4,     1] loss: 0.603
[5,     1] loss: 0.529
[6,     1] loss: 0.482
[7,     1] loss: 0.424
[8,     1] loss: 0.404
[9,     1] loss: 0.376
[10,     1] loss: 0.416
[11,     1] loss: 0.383
[12,     1] loss: 0.314
[13,     1] loss: 0.294
[14,     1] loss: 0.282
[15,     1] loss: 0.350
[16,     1] loss: 0.334
[17,     1] loss: 0.298
[18,     1] loss: 0.273
[19,     1] loss: 0.306
[20,     1] loss: 0.311
[21,     1] loss: 0.287
[22,     1] loss: 0.367
[23,     1] loss: 0.265
[24,     1] loss: 0.225
[25,     1] loss: 0.283
[26,     1] loss: 0.183
[27,     1] loss: 0.283
[28,     1] loss: 0.166
[29,     1] loss: 0.214
[30,     1] loss: 0.161
[31,     1] loss: 0.155
[32,     1] loss: 0.178
[33,     1] loss: 0.149
[34,     1] loss: 0.172
[35,     1] loss: 0.132
[36,     1] loss: 0.136
[37,     1] loss: 0.091
[38,     1] loss: 0.119
[39,     1] loss: 0.105
[40,     1] loss: 0.088
[41,     1] loss: 0.097
[42,     1] loss: 0.183
[43,     1] loss: 0.296
[44,     1] loss: 0.195
[45,     1] loss: 0.222
[46,     1] loss: 0.225
[47,     1] loss: 0.302
[48,     1] loss: 0.179
[49,     1] loss: 0.234
[50,     1] loss: 0.228
[51,     1] loss: 0.189
[52,     1] loss: 0.139
[53,     1] loss: 0.150
[54,     1] loss: 0.186
[55,     1] loss: 0.125
[56,     1] loss: 0.159
[57,     1] loss: 0.153
[58,     1] loss: 0.146
[59,     1] loss: 0.102
[60,     1] loss: 0.135
[61,     1] loss: 0.106
[62,     1] loss: 0.097
[63,     1] loss: 0.126
[64,     1] loss: 0.137
[65,     1] loss: 0.074
[66,     1] loss: 0.081
[67,     1] loss: 0.083
[68,     1] loss: 0.078
[69,     1] loss: 0.118
[70,     1] loss: 0.274
[71,     1] loss: 0.356
[72,     1] loss: 0.340
[73,     1] loss: 0.229
[74,     1] loss: 0.199
[75,     1] loss: 0.252
[76,     1] loss: 0.211
Early stopping applied (best metric=0.2243819236755371)
Finished Training
Total time taken: 9.66961932182312
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.687
[2,     1] loss: 0.684
[3,     1] loss: 0.683
[4,     1] loss: 0.656
[5,     1] loss: 0.613
[6,     1] loss: 0.593
[7,     1] loss: 0.551
[8,     1] loss: 0.548
[9,     1] loss: 0.457
[10,     1] loss: 0.466
[11,     1] loss: 0.366
[12,     1] loss: 0.390
[13,     1] loss: 0.389
[14,     1] loss: 0.305
[15,     1] loss: 0.313
[16,     1] loss: 0.330
[17,     1] loss: 0.246
[18,     1] loss: 0.289
[19,     1] loss: 0.238
[20,     1] loss: 0.247
[21,     1] loss: 0.234
[22,     1] loss: 0.234
[23,     1] loss: 0.254
[24,     1] loss: 0.184
[25,     1] loss: 0.198
[26,     1] loss: 0.160
[27,     1] loss: 0.256
[28,     1] loss: 0.237
[29,     1] loss: 0.202
[30,     1] loss: 0.229
[31,     1] loss: 0.225
[32,     1] loss: 0.186
[33,     1] loss: 0.195
[34,     1] loss: 0.215
[35,     1] loss: 0.188
[36,     1] loss: 0.203
[37,     1] loss: 0.183
[38,     1] loss: 0.422
[39,     1] loss: 0.186
[40,     1] loss: 0.324
[41,     1] loss: 0.383
[42,     1] loss: 0.288
[43,     1] loss: 0.225
[44,     1] loss: 0.352
[45,     1] loss: 0.246
[46,     1] loss: 0.296
[47,     1] loss: 0.299
[48,     1] loss: 0.252
[49,     1] loss: 0.242
[50,     1] loss: 0.235
[51,     1] loss: 0.259
[52,     1] loss: 0.244
[53,     1] loss: 0.225
[54,     1] loss: 0.200
[55,     1] loss: 0.204
[56,     1] loss: 0.154
[57,     1] loss: 0.167
[58,     1] loss: 0.206
[59,     1] loss: 0.233
[60,     1] loss: 0.265
[61,     1] loss: 0.231
[62,     1] loss: 0.146
[63,     1] loss: 0.228
[64,     1] loss: 0.158
[65,     1] loss: 0.192
[66,     1] loss: 0.204
[67,     1] loss: 0.183
[68,     1] loss: 0.180
[69,     1] loss: 0.133
[70,     1] loss: 0.174
[71,     1] loss: 0.114
[72,     1] loss: 0.134
[73,     1] loss: 0.128
[74,     1] loss: 0.148
[75,     1] loss: 0.136
[76,     1] loss: 0.135
[77,     1] loss: 0.126
[78,     1] loss: 0.134
[79,     1] loss: 0.146
[80,     1] loss: 0.184
[81,     1] loss: 0.140
[82,     1] loss: 0.216
[83,     1] loss: 0.296
[84,     1] loss: 0.188
[85,     1] loss: 0.212
[86,     1] loss: 0.156
[87,     1] loss: 0.188
[88,     1] loss: 0.227
[89,     1] loss: 0.180
[90,     1] loss: 0.136
Early stopping applied (best metric=0.3256635069847107)
Finished Training
Total time taken: 10.972534656524658
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.673
[4,     1] loss: 0.638
[5,     1] loss: 0.618
[6,     1] loss: 0.552
[7,     1] loss: 0.516
[8,     1] loss: 0.456
[9,     1] loss: 0.428
[10,     1] loss: 0.464
[11,     1] loss: 0.376
[12,     1] loss: 0.445
[13,     1] loss: 0.329
[14,     1] loss: 0.426
[15,     1] loss: 0.381
[16,     1] loss: 0.302
[17,     1] loss: 0.290
[18,     1] loss: 0.293
[19,     1] loss: 0.336
[20,     1] loss: 0.423
[21,     1] loss: 0.273
[22,     1] loss: 0.264
[23,     1] loss: 0.327
[24,     1] loss: 0.268
[25,     1] loss: 0.289
[26,     1] loss: 0.228
[27,     1] loss: 0.201
[28,     1] loss: 0.222
[29,     1] loss: 0.172
[30,     1] loss: 0.188
[31,     1] loss: 0.160
[32,     1] loss: 0.213
[33,     1] loss: 0.095
[34,     1] loss: 0.098
[35,     1] loss: 0.123
[36,     1] loss: 0.111
[37,     1] loss: 0.145
[38,     1] loss: 0.135
[39,     1] loss: 0.142
[40,     1] loss: 0.182
[41,     1] loss: 0.111
[42,     1] loss: 0.100
[43,     1] loss: 0.188
[44,     1] loss: 0.235
[45,     1] loss: 0.257
[46,     1] loss: 0.339
[47,     1] loss: 0.195
[48,     1] loss: 0.193
[49,     1] loss: 0.205
[50,     1] loss: 0.225
[51,     1] loss: 0.200
[52,     1] loss: 0.147
[53,     1] loss: 0.139
[54,     1] loss: 0.127
[55,     1] loss: 0.133
[56,     1] loss: 0.147
[57,     1] loss: 0.135
[58,     1] loss: 0.185
[59,     1] loss: 0.078
[60,     1] loss: 0.134
[61,     1] loss: 0.189
[62,     1] loss: 0.150
[63,     1] loss: 0.128
[64,     1] loss: 0.138
[65,     1] loss: 0.129
[66,     1] loss: 0.112
[67,     1] loss: 0.130
[68,     1] loss: 0.088
[69,     1] loss: 0.101
[70,     1] loss: 0.111
[71,     1] loss: 0.112
[72,     1] loss: 0.088
[73,     1] loss: 0.095
[74,     1] loss: 0.069
[75,     1] loss: 0.098
[76,     1] loss: 0.110
[77,     1] loss: 0.321
[78,     1] loss: 0.147
[79,     1] loss: 0.144
[80,     1] loss: 0.116
[81,     1] loss: 0.131
[82,     1] loss: 0.133
[83,     1] loss: 0.142
[84,     1] loss: 0.131
[85,     1] loss: 0.118
[86,     1] loss: 0.089
[87,     1] loss: 0.113
[88,     1] loss: 0.088
[89,     1] loss: 0.059
[90,     1] loss: 0.118
[91,     1] loss: 0.091
[92,     1] loss: 0.078
[93,     1] loss: 0.096
[94,     1] loss: 0.169
[95,     1] loss: 0.075
[96,     1] loss: 0.201
[97,     1] loss: 0.223
[98,     1] loss: 0.264
[99,     1] loss: 0.161
[100,     1] loss: 0.198
[101,     1] loss: 0.120
[102,     1] loss: 0.148
[103,     1] loss: 0.142
[104,     1] loss: 0.134
[105,     1] loss: 0.129
[106,     1] loss: 0.120
[107,     1] loss: 0.119
[108,     1] loss: 0.112
[109,     1] loss: 0.102
[110,     1] loss: 0.108
[111,     1] loss: 0.099
[112,     1] loss: 0.088
[113,     1] loss: 0.078
[114,     1] loss: 0.102
[115,     1] loss: 0.105
[116,     1] loss: 0.066
[117,     1] loss: 0.078
[118,     1] loss: 0.074
[119,     1] loss: 0.095
[120,     1] loss: 0.106
[121,     1] loss: 0.115
[122,     1] loss: 0.133
[123,     1] loss: 0.125
[124,     1] loss: 0.108
[125,     1] loss: 0.211
[126,     1] loss: 0.155
[127,     1] loss: 0.140
[128,     1] loss: 0.112
[129,     1] loss: 0.146
[130,     1] loss: 0.122
[131,     1] loss: 0.129
[132,     1] loss: 0.137
[133,     1] loss: 0.121
[134,     1] loss: 0.116
[135,     1] loss: 0.132
[136,     1] loss: 0.106
[137,     1] loss: 0.200
[138,     1] loss: 0.118
[139,     1] loss: 0.128
[140,     1] loss: 0.113
[141,     1] loss: 0.102
[142,     1] loss: 0.118
[143,     1] loss: 0.104
[144,     1] loss: 0.100
[145,     1] loss: 0.118
[146,     1] loss: 0.082
[147,     1] loss: 0.086
[148,     1] loss: 0.059
[149,     1] loss: 0.186
[150,     1] loss: 0.355
[151,     1] loss: 0.345
[152,     1] loss: 0.259
[153,     1] loss: 0.183
[154,     1] loss: 0.178
[155,     1] loss: 0.161
[156,     1] loss: 0.205
Early stopping applied (best metric=0.1427227109670639)
Finished Training
Total time taken: 18.68333911895752
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.686
[3,     1] loss: 0.661
[4,     1] loss: 0.604
[5,     1] loss: 0.558
[6,     1] loss: 0.495
[7,     1] loss: 0.486
[8,     1] loss: 0.426
[9,     1] loss: 0.400
[10,     1] loss: 0.323
[11,     1] loss: 0.298
[12,     1] loss: 0.292
[13,     1] loss: 0.306
[14,     1] loss: 0.296
[15,     1] loss: 0.249
[16,     1] loss: 0.228
[17,     1] loss: 0.280
[18,     1] loss: 0.272
[19,     1] loss: 0.317
[20,     1] loss: 0.282
[21,     1] loss: 0.230
[22,     1] loss: 0.229
[23,     1] loss: 0.239
[24,     1] loss: 0.228
[25,     1] loss: 0.275
[26,     1] loss: 0.224
[27,     1] loss: 0.192
[28,     1] loss: 0.219
[29,     1] loss: 0.242
[30,     1] loss: 0.277
[31,     1] loss: 0.298
[32,     1] loss: 0.244
[33,     1] loss: 0.199
[34,     1] loss: 0.198
[35,     1] loss: 0.247
[36,     1] loss: 0.241
[37,     1] loss: 0.167
[38,     1] loss: 0.219
[39,     1] loss: 0.207
[40,     1] loss: 0.221
[41,     1] loss: 0.219
[42,     1] loss: 0.166
[43,     1] loss: 0.160
[44,     1] loss: 0.187
[45,     1] loss: 0.161
[46,     1] loss: 0.124
[47,     1] loss: 0.151
[48,     1] loss: 0.180
[49,     1] loss: 0.180
[50,     1] loss: 0.159
[51,     1] loss: 0.156
[52,     1] loss: 0.173
[53,     1] loss: 0.141
[54,     1] loss: 0.155
[55,     1] loss: 0.166
Early stopping applied (best metric=0.422535240650177)
Finished Training
Total time taken: 6.59947395324707
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.706
[3,     1] loss: 0.688
[4,     1] loss: 0.676
[5,     1] loss: 0.671
[6,     1] loss: 0.649
[7,     1] loss: 0.625
[8,     1] loss: 0.608
[9,     1] loss: 0.553
[10,     1] loss: 0.558
[11,     1] loss: 0.494
[12,     1] loss: 0.452
[13,     1] loss: 0.448
[14,     1] loss: 0.389
[15,     1] loss: 0.388
[16,     1] loss: 0.432
[17,     1] loss: 0.358
[18,     1] loss: 0.367
[19,     1] loss: 0.364
[20,     1] loss: 0.345
[21,     1] loss: 0.463
[22,     1] loss: 0.368
[23,     1] loss: 0.412
[24,     1] loss: 0.312
[25,     1] loss: 0.369
[26,     1] loss: 0.344
[27,     1] loss: 0.377
[28,     1] loss: 0.359
[29,     1] loss: 0.328
[30,     1] loss: 0.324
[31,     1] loss: 0.298
[32,     1] loss: 0.290
[33,     1] loss: 0.295
[34,     1] loss: 0.291
[35,     1] loss: 0.277
[36,     1] loss: 0.226
[37,     1] loss: 0.199
[38,     1] loss: 0.208
[39,     1] loss: 0.164
[40,     1] loss: 0.207
[41,     1] loss: 0.200
[42,     1] loss: 0.334
[43,     1] loss: 0.229
[44,     1] loss: 0.276
[45,     1] loss: 0.221
[46,     1] loss: 0.210
[47,     1] loss: 0.229
[48,     1] loss: 0.252
[49,     1] loss: 0.217
[50,     1] loss: 0.201
[51,     1] loss: 0.204
[52,     1] loss: 0.177
[53,     1] loss: 0.177
[54,     1] loss: 0.201
[55,     1] loss: 0.150
[56,     1] loss: 0.165
[57,     1] loss: 0.127
[58,     1] loss: 0.129
[59,     1] loss: 0.184
[60,     1] loss: 0.178
[61,     1] loss: 0.186
[62,     1] loss: 0.120
[63,     1] loss: 0.143
[64,     1] loss: 0.151
[65,     1] loss: 0.159
[66,     1] loss: 0.173
[67,     1] loss: 0.232
[68,     1] loss: 0.160
[69,     1] loss: 0.172
[70,     1] loss: 0.149
[71,     1] loss: 0.126
[72,     1] loss: 0.132
[73,     1] loss: 0.134
[74,     1] loss: 0.123
[75,     1] loss: 0.178
[76,     1] loss: 0.104
Early stopping applied (best metric=0.3200411796569824)
Finished Training
Total time taken: 9.118967294692993
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.693
[3,     1] loss: 0.662
[4,     1] loss: 0.625
[5,     1] loss: 0.567
[6,     1] loss: 0.501
[7,     1] loss: 0.448
[8,     1] loss: 0.399
[9,     1] loss: 0.391
[10,     1] loss: 0.376
[11,     1] loss: 0.309
[12,     1] loss: 0.282
[13,     1] loss: 0.287
[14,     1] loss: 0.261
[15,     1] loss: 0.241
[16,     1] loss: 0.226
[17,     1] loss: 0.226
[18,     1] loss: 0.184
[19,     1] loss: 0.248
[20,     1] loss: 0.256
[21,     1] loss: 0.279
[22,     1] loss: 0.218
[23,     1] loss: 0.215
[24,     1] loss: 0.262
[25,     1] loss: 0.204
[26,     1] loss: 0.281
[27,     1] loss: 0.233
[28,     1] loss: 0.223
[29,     1] loss: 0.220
[30,     1] loss: 0.207
[31,     1] loss: 0.227
[32,     1] loss: 0.163
[33,     1] loss: 0.255
[34,     1] loss: 0.185
[35,     1] loss: 0.237
[36,     1] loss: 0.183
[37,     1] loss: 0.206
[38,     1] loss: 0.179
[39,     1] loss: 0.155
[40,     1] loss: 0.230
[41,     1] loss: 0.174
[42,     1] loss: 0.202
[43,     1] loss: 0.234
[44,     1] loss: 0.127
[45,     1] loss: 0.261
[46,     1] loss: 0.290
[47,     1] loss: 0.244
[48,     1] loss: 0.290
[49,     1] loss: 0.315
[50,     1] loss: 0.250
[51,     1] loss: 0.266
[52,     1] loss: 0.251
[53,     1] loss: 0.212
[54,     1] loss: 0.249
[55,     1] loss: 0.194
[56,     1] loss: 0.161
Early stopping applied (best metric=0.3785773515701294)
Finished Training
Total time taken: 6.745173931121826
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.692
[3,     1] loss: 0.685
[4,     1] loss: 0.669
[5,     1] loss: 0.631
[6,     1] loss: 0.609
[7,     1] loss: 0.563
[8,     1] loss: 0.526
[9,     1] loss: 0.479
[10,     1] loss: 0.424
[11,     1] loss: 0.460
[12,     1] loss: 0.436
[13,     1] loss: 0.303
[14,     1] loss: 0.384
[15,     1] loss: 0.334
[16,     1] loss: 0.331
[17,     1] loss: 0.333
[18,     1] loss: 0.438
[19,     1] loss: 0.389
[20,     1] loss: 0.345
[21,     1] loss: 0.367
[22,     1] loss: 0.279
[23,     1] loss: 0.301
[24,     1] loss: 0.296
[25,     1] loss: 0.319
[26,     1] loss: 0.326
[27,     1] loss: 0.239
[28,     1] loss: 0.245
[29,     1] loss: 0.230
[30,     1] loss: 0.226
[31,     1] loss: 0.197
[32,     1] loss: 0.206
[33,     1] loss: 0.199
[34,     1] loss: 0.165
[35,     1] loss: 0.207
[36,     1] loss: 0.196
[37,     1] loss: 0.202
[38,     1] loss: 0.185
[39,     1] loss: 0.215
[40,     1] loss: 0.199
[41,     1] loss: 0.198
[42,     1] loss: 0.208
[43,     1] loss: 0.171
[44,     1] loss: 0.174
[45,     1] loss: 0.161
[46,     1] loss: 0.128
[47,     1] loss: 0.166
[48,     1] loss: 0.196
[49,     1] loss: 0.139
[50,     1] loss: 0.126
[51,     1] loss: 0.163
[52,     1] loss: 0.163
[53,     1] loss: 0.243
[54,     1] loss: 0.207
[55,     1] loss: 0.182
[56,     1] loss: 0.163
[57,     1] loss: 0.152
[58,     1] loss: 0.167
[59,     1] loss: 0.177
[60,     1] loss: 0.152
[61,     1] loss: 0.114
[62,     1] loss: 0.139
[63,     1] loss: 0.135
[64,     1] loss: 0.119
[65,     1] loss: 0.128
[66,     1] loss: 0.150
[67,     1] loss: 0.258
[68,     1] loss: 0.441
[69,     1] loss: 0.267
[70,     1] loss: 0.398
[71,     1] loss: 0.445
[72,     1] loss: 0.267
[73,     1] loss: 0.301
[74,     1] loss: 0.327
[75,     1] loss: 0.254
[76,     1] loss: 0.282
[77,     1] loss: 0.301
[78,     1] loss: 0.260
[79,     1] loss: 0.225
[80,     1] loss: 0.255
[81,     1] loss: 0.240
[82,     1] loss: 0.237
[83,     1] loss: 0.189
[84,     1] loss: 0.219
[85,     1] loss: 0.226
[86,     1] loss: 0.178
[87,     1] loss: 0.143
[88,     1] loss: 0.144
[89,     1] loss: 0.136
[90,     1] loss: 0.145
[91,     1] loss: 0.112
[92,     1] loss: 0.169
[93,     1] loss: 0.183
[94,     1] loss: 0.141
[95,     1] loss: 0.182
[96,     1] loss: 0.171
[97,     1] loss: 0.139
[98,     1] loss: 0.133
[99,     1] loss: 0.144
[100,     1] loss: 0.116
[101,     1] loss: 0.120
[102,     1] loss: 0.137
[103,     1] loss: 0.207
[104,     1] loss: 0.197
[105,     1] loss: 0.209
[106,     1] loss: 0.278
[107,     1] loss: 0.234
[108,     1] loss: 0.183
[109,     1] loss: 0.195
[110,     1] loss: 0.169
[111,     1] loss: 0.178
[112,     1] loss: 0.207
[113,     1] loss: 0.199
[114,     1] loss: 0.197
[115,     1] loss: 0.177
[116,     1] loss: 0.153
[117,     1] loss: 0.130
[118,     1] loss: 0.163
[119,     1] loss: 0.155
[120,     1] loss: 0.137
[121,     1] loss: 0.138
[122,     1] loss: 0.141
[123,     1] loss: 0.148
[124,     1] loss: 0.116
[125,     1] loss: 0.216
[126,     1] loss: 0.165
[127,     1] loss: 0.241
[128,     1] loss: 0.206
[129,     1] loss: 0.168
[130,     1] loss: 0.139
[131,     1] loss: 0.163
[132,     1] loss: 0.165
[133,     1] loss: 0.156
[134,     1] loss: 0.155
[135,     1] loss: 0.147
[136,     1] loss: 0.136
[137,     1] loss: 0.142
[138,     1] loss: 0.166
[139,     1] loss: 0.178
[140,     1] loss: 0.224
Early stopping applied (best metric=0.16531693935394287)
Finished Training
Total time taken: 17.524713039398193
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.688
[4,     1] loss: 0.665
[5,     1] loss: 0.635
[6,     1] loss: 0.624
[7,     1] loss: 0.587
[8,     1] loss: 0.538
[9,     1] loss: 0.511
[10,     1] loss: 0.506
[11,     1] loss: 0.465
[12,     1] loss: 0.466
[13,     1] loss: 0.428
[14,     1] loss: 0.395
[15,     1] loss: 0.419
[16,     1] loss: 0.383
[17,     1] loss: 0.448
[18,     1] loss: 0.378
[19,     1] loss: 0.354
[20,     1] loss: 0.402
[21,     1] loss: 0.287
[22,     1] loss: 0.325
[23,     1] loss: 0.320
[24,     1] loss: 0.376
[25,     1] loss: 0.324
[26,     1] loss: 0.300
[27,     1] loss: 0.269
[28,     1] loss: 0.295
[29,     1] loss: 0.213
[30,     1] loss: 0.297
[31,     1] loss: 0.234
[32,     1] loss: 0.268
[33,     1] loss: 0.272
[34,     1] loss: 0.300
[35,     1] loss: 0.236
[36,     1] loss: 0.202
[37,     1] loss: 0.203
[38,     1] loss: 0.214
[39,     1] loss: 0.219
[40,     1] loss: 0.250
[41,     1] loss: 0.170
[42,     1] loss: 0.159
[43,     1] loss: 0.202
[44,     1] loss: 0.166
[45,     1] loss: 0.236
[46,     1] loss: 0.237
[47,     1] loss: 0.258
[48,     1] loss: 0.230
[49,     1] loss: 0.137
[50,     1] loss: 0.189
[51,     1] loss: 0.213
[52,     1] loss: 0.268
[53,     1] loss: 0.166
[54,     1] loss: 0.165
[55,     1] loss: 0.224
[56,     1] loss: 0.159
[57,     1] loss: 0.188
[58,     1] loss: 0.138
[59,     1] loss: 0.161
[60,     1] loss: 0.140
[61,     1] loss: 0.099
[62,     1] loss: 0.097
[63,     1] loss: 0.136
[64,     1] loss: 0.110
[65,     1] loss: 0.145
[66,     1] loss: 0.172
[67,     1] loss: 0.085
[68,     1] loss: 0.142
[69,     1] loss: 0.099
[70,     1] loss: 0.112
[71,     1] loss: 0.113
[72,     1] loss: 0.119
[73,     1] loss: 0.109
[74,     1] loss: 0.105
[75,     1] loss: 0.126
[76,     1] loss: 0.069
Early stopping applied (best metric=0.27792447805404663)
Finished Training
Total time taken: 10.916883945465088
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.693
[3,     1] loss: 0.678
[4,     1] loss: 0.651
[5,     1] loss: 0.604
[6,     1] loss: 0.574
[7,     1] loss: 0.534
[8,     1] loss: 0.517
[9,     1] loss: 0.448
[10,     1] loss: 0.480
[11,     1] loss: 0.386
[12,     1] loss: 0.319
[13,     1] loss: 0.367
[14,     1] loss: 0.396
[15,     1] loss: 0.314
[16,     1] loss: 0.316
[17,     1] loss: 0.283
[18,     1] loss: 0.321
[19,     1] loss: 0.259
[20,     1] loss: 0.252
[21,     1] loss: 0.170
[22,     1] loss: 0.205
[23,     1] loss: 0.173
[24,     1] loss: 0.275
[25,     1] loss: 0.177
[26,     1] loss: 0.127
[27,     1] loss: 0.224
[28,     1] loss: 0.187
[29,     1] loss: 0.279
[30,     1] loss: 0.221
[31,     1] loss: 0.367
[32,     1] loss: 0.255
[33,     1] loss: 0.229
[34,     1] loss: 0.281
[35,     1] loss: 0.232
[36,     1] loss: 0.234
[37,     1] loss: 0.229
[38,     1] loss: 0.239
[39,     1] loss: 0.220
[40,     1] loss: 0.238
[41,     1] loss: 0.232
[42,     1] loss: 0.198
[43,     1] loss: 0.179
[44,     1] loss: 0.206
[45,     1] loss: 0.194
[46,     1] loss: 0.224
[47,     1] loss: 0.187
[48,     1] loss: 0.169
[49,     1] loss: 0.140
[50,     1] loss: 0.142
[51,     1] loss: 0.116
[52,     1] loss: 0.120
[53,     1] loss: 0.167
[54,     1] loss: 0.131
[55,     1] loss: 0.170
[56,     1] loss: 0.143
[57,     1] loss: 0.213
[58,     1] loss: 0.172
[59,     1] loss: 0.249
[60,     1] loss: 0.222
[61,     1] loss: 0.319
[62,     1] loss: 0.148
[63,     1] loss: 0.172
[64,     1] loss: 0.157
[65,     1] loss: 0.152
[66,     1] loss: 0.210
[67,     1] loss: 0.161
[68,     1] loss: 0.155
[69,     1] loss: 0.167
[70,     1] loss: 0.124
[71,     1] loss: 0.151
[72,     1] loss: 0.141
[73,     1] loss: 0.127
[74,     1] loss: 0.136
[75,     1] loss: 0.123
[76,     1] loss: 0.381
[77,     1] loss: 0.201
[78,     1] loss: 0.289
[79,     1] loss: 0.228
[80,     1] loss: 0.339
[81,     1] loss: 0.194
[82,     1] loss: 0.213
[83,     1] loss: 0.204
[84,     1] loss: 0.196
[85,     1] loss: 0.191
[86,     1] loss: 0.177
[87,     1] loss: 0.185
[88,     1] loss: 0.165
[89,     1] loss: 0.151
[90,     1] loss: 0.146
[91,     1] loss: 0.138
[92,     1] loss: 0.114
[93,     1] loss: 0.115
[94,     1] loss: 0.160
[95,     1] loss: 0.092
[96,     1] loss: 0.139
[97,     1] loss: 0.123
[98,     1] loss: 0.128
[99,     1] loss: 0.119
[100,     1] loss: 0.125
[101,     1] loss: 0.087
[102,     1] loss: 0.127
[103,     1] loss: 0.107
[104,     1] loss: 0.126
[105,     1] loss: 0.173
[106,     1] loss: 0.121
[107,     1] loss: 0.144
[108,     1] loss: 0.134
[109,     1] loss: 0.163
[110,     1] loss: 0.138
[111,     1] loss: 0.155
[112,     1] loss: 0.146
[113,     1] loss: 0.169
[114,     1] loss: 0.167
[115,     1] loss: 0.134
[116,     1] loss: 0.109
[117,     1] loss: 0.113
[118,     1] loss: 0.137
[119,     1] loss: 0.132
[120,     1] loss: 0.109
[121,     1] loss: 0.114
[122,     1] loss: 0.107
[123,     1] loss: 0.128
[124,     1] loss: 0.109
[125,     1] loss: 0.111
[126,     1] loss: 0.182
[127,     1] loss: 0.201
[128,     1] loss: 0.220
[129,     1] loss: 0.150
[130,     1] loss: 0.272
[131,     1] loss: 0.169
[132,     1] loss: 0.157
[133,     1] loss: 0.185
[134,     1] loss: 0.183
[135,     1] loss: 0.204
[136,     1] loss: 0.217
[137,     1] loss: 0.134
[138,     1] loss: 0.155
[139,     1] loss: 0.113
[140,     1] loss: 0.160
[141,     1] loss: 0.114
[142,     1] loss: 0.129
[143,     1] loss: 0.130
[144,     1] loss: 0.090
[145,     1] loss: 0.137
[146,     1] loss: 0.104
[147,     1] loss: 0.143
[148,     1] loss: 0.117
[149,     1] loss: 0.071
[150,     1] loss: 0.100
[151,     1] loss: 0.114
[152,     1] loss: 0.165
[153,     1] loss: 0.093
[154,     1] loss: 0.086
[155,     1] loss: 0.119
[156,     1] loss: 0.106
[157,     1] loss: 0.150
[158,     1] loss: 0.106
[159,     1] loss: 0.122
[160,     1] loss: 0.125
[161,     1] loss: 0.083
[162,     1] loss: 0.072
[163,     1] loss: 0.108
[164,     1] loss: 0.141
[165,     1] loss: 0.143
[166,     1] loss: 0.246
[167,     1] loss: 0.242
[168,     1] loss: 0.180
[169,     1] loss: 0.176
[170,     1] loss: 0.129
[171,     1] loss: 0.135
[172,     1] loss: 0.160
[173,     1] loss: 0.125
Early stopping applied (best metric=0.1502280831336975)
Finished Training
Total time taken: 21.376558542251587
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.687
[3,     1] loss: 0.683
[4,     1] loss: 0.662
[5,     1] loss: 0.613
[6,     1] loss: 0.586
[7,     1] loss: 0.513
[8,     1] loss: 0.522
[9,     1] loss: 0.468
[10,     1] loss: 0.443
[11,     1] loss: 0.368
[12,     1] loss: 0.385
[13,     1] loss: 0.362
[14,     1] loss: 0.368
[15,     1] loss: 0.320
[16,     1] loss: 0.309
[17,     1] loss: 0.368
[18,     1] loss: 0.261
[19,     1] loss: 0.367
[20,     1] loss: 0.226
[21,     1] loss: 0.250
[22,     1] loss: 0.235
[23,     1] loss: 0.249
[24,     1] loss: 0.184
[25,     1] loss: 0.250
[26,     1] loss: 0.214
[27,     1] loss: 0.298
[28,     1] loss: 0.281
[29,     1] loss: 0.288
[30,     1] loss: 0.250
[31,     1] loss: 0.259
[32,     1] loss: 0.199
[33,     1] loss: 0.198
[34,     1] loss: 0.260
[35,     1] loss: 0.166
[36,     1] loss: 0.168
[37,     1] loss: 0.172
[38,     1] loss: 0.155
[39,     1] loss: 0.176
[40,     1] loss: 0.129
[41,     1] loss: 0.189
[42,     1] loss: 0.170
[43,     1] loss: 0.163
[44,     1] loss: 0.143
[45,     1] loss: 0.152
[46,     1] loss: 0.137
[47,     1] loss: 0.166
[48,     1] loss: 0.098
[49,     1] loss: 0.144
[50,     1] loss: 0.146
[51,     1] loss: 0.114
[52,     1] loss: 0.104
[53,     1] loss: 0.128
[54,     1] loss: 0.116
[55,     1] loss: 0.075
[56,     1] loss: 0.132
[57,     1] loss: 0.101
[58,     1] loss: 0.112
[59,     1] loss: 0.154
[60,     1] loss: 0.321
Early stopping applied (best metric=0.331570565700531)
Finished Training
Total time taken: 7.801593065261841
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.679
[4,     1] loss: 0.651
[5,     1] loss: 0.630
[6,     1] loss: 0.592
[7,     1] loss: 0.534
[8,     1] loss: 0.504
[9,     1] loss: 0.499
[10,     1] loss: 0.428
[11,     1] loss: 0.403
[12,     1] loss: 0.437
[13,     1] loss: 0.296
[14,     1] loss: 0.441
[15,     1] loss: 0.382
[16,     1] loss: 0.352
[17,     1] loss: 0.353
[18,     1] loss: 0.313
[19,     1] loss: 0.284
[20,     1] loss: 0.324
[21,     1] loss: 0.251
[22,     1] loss: 0.322
[23,     1] loss: 0.277
[24,     1] loss: 0.237
[25,     1] loss: 0.277
[26,     1] loss: 0.198
[27,     1] loss: 0.217
[28,     1] loss: 0.254
[29,     1] loss: 0.318
[30,     1] loss: 0.313
[31,     1] loss: 0.328
[32,     1] loss: 0.198
[33,     1] loss: 0.217
[34,     1] loss: 0.187
[35,     1] loss: 0.178
[36,     1] loss: 0.215
[37,     1] loss: 0.165
[38,     1] loss: 0.184
[39,     1] loss: 0.164
[40,     1] loss: 0.190
[41,     1] loss: 0.184
[42,     1] loss: 0.126
[43,     1] loss: 0.144
[44,     1] loss: 0.124
[45,     1] loss: 0.119
[46,     1] loss: 0.167
[47,     1] loss: 0.197
[48,     1] loss: 0.080
[49,     1] loss: 0.118
[50,     1] loss: 0.110
[51,     1] loss: 0.130
[52,     1] loss: 0.093
[53,     1] loss: 0.112
[54,     1] loss: 0.103
[55,     1] loss: 0.094
[56,     1] loss: 0.266
[57,     1] loss: 0.363
[58,     1] loss: 0.192
[59,     1] loss: 0.211
[60,     1] loss: 0.150
[61,     1] loss: 0.156
[62,     1] loss: 0.142
[63,     1] loss: 0.194
[64,     1] loss: 0.134
[65,     1] loss: 0.171
[66,     1] loss: 0.142
[67,     1] loss: 0.116
[68,     1] loss: 0.128
[69,     1] loss: 0.122
[70,     1] loss: 0.116
[71,     1] loss: 0.134
[72,     1] loss: 0.105
[73,     1] loss: 0.127
[74,     1] loss: 0.114
[75,     1] loss: 0.100
[76,     1] loss: 0.101
[77,     1] loss: 0.093
[78,     1] loss: 0.118
[79,     1] loss: 0.197
[80,     1] loss: 0.503
[81,     1] loss: 0.306
[82,     1] loss: 0.292
[83,     1] loss: 0.315
[84,     1] loss: 0.233
[85,     1] loss: 0.221
[86,     1] loss: 0.292
[87,     1] loss: 0.282
[88,     1] loss: 0.229
[89,     1] loss: 0.202
[90,     1] loss: 0.215
[91,     1] loss: 0.175
[92,     1] loss: 0.169
[93,     1] loss: 0.175
[94,     1] loss: 0.164
[95,     1] loss: 0.158
[96,     1] loss: 0.133
[97,     1] loss: 0.101
[98,     1] loss: 0.132
[99,     1] loss: 0.113
[100,     1] loss: 0.125
[101,     1] loss: 0.072
[102,     1] loss: 0.110
[103,     1] loss: 0.100
[104,     1] loss: 0.113
[105,     1] loss: 0.083
[106,     1] loss: 0.111
[107,     1] loss: 0.066
[108,     1] loss: 0.088
[109,     1] loss: 0.117
[110,     1] loss: 0.096
[111,     1] loss: 0.079
[112,     1] loss: 0.088
[113,     1] loss: 0.105
[114,     1] loss: 0.108
[115,     1] loss: 0.092
[116,     1] loss: 0.081
[117,     1] loss: 0.055
[118,     1] loss: 0.060
[119,     1] loss: 0.113
[120,     1] loss: 0.182
[121,     1] loss: 0.278
[122,     1] loss: 0.394
[123,     1] loss: 0.204
[124,     1] loss: 0.210
[125,     1] loss: 0.242
[126,     1] loss: 0.204
[127,     1] loss: 0.196
[128,     1] loss: 0.182
[129,     1] loss: 0.202
[130,     1] loss: 0.179
[131,     1] loss: 0.188
[132,     1] loss: 0.163
[133,     1] loss: 0.168
[134,     1] loss: 0.164
[135,     1] loss: 0.164
[136,     1] loss: 0.178
[137,     1] loss: 0.122
[138,     1] loss: 0.171
[139,     1] loss: 0.116
[140,     1] loss: 0.113
[141,     1] loss: 0.121
[142,     1] loss: 0.120
[143,     1] loss: 0.140
[144,     1] loss: 0.147
[145,     1] loss: 0.156
[146,     1] loss: 0.137
[147,     1] loss: 0.114
[148,     1] loss: 0.109
[149,     1] loss: 0.136
[150,     1] loss: 0.095
[151,     1] loss: 0.159
[152,     1] loss: 0.107
[153,     1] loss: 0.173
[154,     1] loss: 0.141
[155,     1] loss: 0.224
[156,     1] loss: 0.095
[157,     1] loss: 0.193
[158,     1] loss: 0.192
[159,     1] loss: 0.134
[160,     1] loss: 0.134
[161,     1] loss: 0.098
[162,     1] loss: 0.125
[163,     1] loss: 0.153
[164,     1] loss: 0.095
[165,     1] loss: 0.118
[166,     1] loss: 0.124
Early stopping applied (best metric=0.1272067129611969)
Finished Training
Total time taken: 20.116299390792847
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.692
[3,     1] loss: 0.673
[4,     1] loss: 0.635
[5,     1] loss: 0.598
[6,     1] loss: 0.557
[7,     1] loss: 0.483
[8,     1] loss: 0.439
[9,     1] loss: 0.403
[10,     1] loss: 0.429
[11,     1] loss: 0.327
[12,     1] loss: 0.349
[13,     1] loss: 0.275
[14,     1] loss: 0.295
[15,     1] loss: 0.285
[16,     1] loss: 0.239
[17,     1] loss: 0.252
[18,     1] loss: 0.252
[19,     1] loss: 0.277
[20,     1] loss: 0.250
[21,     1] loss: 0.190
[22,     1] loss: 0.209
[23,     1] loss: 0.185
[24,     1] loss: 0.195
[25,     1] loss: 0.194
[26,     1] loss: 0.170
[27,     1] loss: 0.221
[28,     1] loss: 0.169
[29,     1] loss: 0.141
[30,     1] loss: 0.167
[31,     1] loss: 0.134
[32,     1] loss: 0.115
[33,     1] loss: 0.113
[34,     1] loss: 0.170
[35,     1] loss: 0.103
[36,     1] loss: 0.163
[37,     1] loss: 0.263
[38,     1] loss: 0.319
[39,     1] loss: 0.160
[40,     1] loss: 0.239
[41,     1] loss: 0.159
[42,     1] loss: 0.158
[43,     1] loss: 0.177
[44,     1] loss: 0.196
[45,     1] loss: 0.138
[46,     1] loss: 0.147
[47,     1] loss: 0.128
[48,     1] loss: 0.162
[49,     1] loss: 0.110
[50,     1] loss: 0.119
[51,     1] loss: 0.090
[52,     1] loss: 0.082
[53,     1] loss: 0.082
[54,     1] loss: 0.118
[55,     1] loss: 0.122
[56,     1] loss: 0.142
[57,     1] loss: 0.110
[58,     1] loss: 0.152
[59,     1] loss: 0.125
[60,     1] loss: 0.319
[61,     1] loss: 0.266
[62,     1] loss: 0.395
[63,     1] loss: 0.212
[64,     1] loss: 0.343
[65,     1] loss: 0.285
[66,     1] loss: 0.199
[67,     1] loss: 0.241
[68,     1] loss: 0.267
[69,     1] loss: 0.231
[70,     1] loss: 0.172
[71,     1] loss: 0.173
Early stopping applied (best metric=0.1859988570213318)
Finished Training
Total time taken: 8.581517457962036
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.679
[4,     1] loss: 0.628
[5,     1] loss: 0.590
[6,     1] loss: 0.529
[7,     1] loss: 0.496
[8,     1] loss: 0.460
[9,     1] loss: 0.435
[10,     1] loss: 0.355
[11,     1] loss: 0.354
[12,     1] loss: 0.375
[13,     1] loss: 0.267
[14,     1] loss: 0.326
[15,     1] loss: 0.339
[16,     1] loss: 0.286
[17,     1] loss: 0.305
[18,     1] loss: 0.264
[19,     1] loss: 0.348
[20,     1] loss: 0.278
[21,     1] loss: 0.266
[22,     1] loss: 0.310
[23,     1] loss: 0.257
[24,     1] loss: 0.257
[25,     1] loss: 0.235
[26,     1] loss: 0.216
[27,     1] loss: 0.217
[28,     1] loss: 0.214
[29,     1] loss: 0.203
[30,     1] loss: 0.201
[31,     1] loss: 0.325
[32,     1] loss: 0.172
[33,     1] loss: 0.202
[34,     1] loss: 0.192
[35,     1] loss: 0.183
[36,     1] loss: 0.199
[37,     1] loss: 0.191
[38,     1] loss: 0.145
[39,     1] loss: 0.128
[40,     1] loss: 0.129
[41,     1] loss: 0.237
[42,     1] loss: 0.255
[43,     1] loss: 0.278
[44,     1] loss: 0.218
[45,     1] loss: 0.404
[46,     1] loss: 0.179
[47,     1] loss: 0.204
[48,     1] loss: 0.223
[49,     1] loss: 0.212
[50,     1] loss: 0.211
[51,     1] loss: 0.188
[52,     1] loss: 0.181
[53,     1] loss: 0.187
[54,     1] loss: 0.173
[55,     1] loss: 0.134
[56,     1] loss: 0.155
[57,     1] loss: 0.127
[58,     1] loss: 0.100
[59,     1] loss: 0.119
[60,     1] loss: 0.186
Early stopping applied (best metric=0.34450411796569824)
Finished Training
Total time taken: 7.27487587928772
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.676
[4,     1] loss: 0.638
[5,     1] loss: 0.593
[6,     1] loss: 0.542
[7,     1] loss: 0.497
[8,     1] loss: 0.462
[9,     1] loss: 0.440
[10,     1] loss: 0.388
[11,     1] loss: 0.371
[12,     1] loss: 0.332
[13,     1] loss: 0.314
[14,     1] loss: 0.384
[15,     1] loss: 0.313
[16,     1] loss: 0.340
[17,     1] loss: 0.260
[18,     1] loss: 0.213
[19,     1] loss: 0.296
[20,     1] loss: 0.212
[21,     1] loss: 0.217
[22,     1] loss: 0.235
[23,     1] loss: 0.201
[24,     1] loss: 0.188
[25,     1] loss: 0.171
[26,     1] loss: 0.154
[27,     1] loss: 0.179
[28,     1] loss: 0.193
[29,     1] loss: 0.219
[30,     1] loss: 0.204
[31,     1] loss: 0.221
[32,     1] loss: 0.173
[33,     1] loss: 0.150
[34,     1] loss: 0.178
[35,     1] loss: 0.143
[36,     1] loss: 0.139
[37,     1] loss: 0.118
[38,     1] loss: 0.132
[39,     1] loss: 0.140
[40,     1] loss: 0.109
[41,     1] loss: 0.148
[42,     1] loss: 0.139
[43,     1] loss: 0.217
[44,     1] loss: 0.252
[45,     1] loss: 0.146
[46,     1] loss: 0.268
[47,     1] loss: 0.316
[48,     1] loss: 0.214
[49,     1] loss: 0.272
[50,     1] loss: 0.252
[51,     1] loss: 0.234
[52,     1] loss: 0.210
[53,     1] loss: 0.205
[54,     1] loss: 0.200
[55,     1] loss: 0.152
[56,     1] loss: 0.186
[57,     1] loss: 0.165
[58,     1] loss: 0.176
[59,     1] loss: 0.238
[60,     1] loss: 0.184
[61,     1] loss: 0.257
[62,     1] loss: 0.157
Early stopping applied (best metric=0.2745838463306427)
Finished Training
Total time taken: 7.499147891998291
{'Hydroxylation-K Validation Accuracy': 0.8536347517730496, 'Hydroxylation-K Validation Sensitivity': 0.8791111111111111, 'Hydroxylation-K Validation Specificity': 0.8473684210526315, 'Hydroxylation-K Validation Precision': 0.6099949544779266, 'Hydroxylation-K AUC ROC': 0.8820818713450292, 'Hydroxylation-K AUC PR': 0.7048369919367261, 'Hydroxylation-K MCC': 0.6454790660519456, 'Hydroxylation-K F1': 0.7142915033103939, 'Validation Loss (Hydroxylation-K)': 0.294754182100296, 'Validation Loss (total)': 0.294754182100296, 'TimeToTrain': 10.557268838882447}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009470517752036983,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3780993558,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.343663196460245}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002662182909944016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2956418347,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.65302790384966}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.679
[4,     1] loss: 0.659
[5,     1] loss: 0.649
[6,     1] loss: 0.635
[7,     1] loss: 0.596
[8,     1] loss: 0.602
[9,     1] loss: 0.557
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007234492885147024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1100482608,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.263638599524418}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.694
[3,     1] loss: 0.678
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009479436200649964,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3462186434,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.06559253581280622}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009087739573928952,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2697020064,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.820403874305203}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.700
[3,     1] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003946012696210439,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 230971969,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.717945159079262}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.687
[3,     1] loss: 0.664
[4,     1] loss: 0.613
[5,     1] loss: 0.585
[6,     1] loss: 0.555
[7,     1] loss: 0.543
[8,     1] loss: 0.496
[9,     1] loss: 0.472
[10,     1] loss: 0.400
[11,     1] loss: 0.459
[12,     1] loss: 0.395
[13,     1] loss: 0.346
[14,     1] loss: 0.378
[15,     1] loss: 0.376
[16,     1] loss: 0.389
[17,     1] loss: 0.377
[18,     1] loss: 0.421
[19,     1] loss: 0.375
[20,     1] loss: 0.362
[21,     1] loss: 0.384
[22,     1] loss: 0.358
[23,     1] loss: 0.295
[24,     1] loss: 0.369
[25,     1] loss: 0.312
[26,     1] loss: 0.305
[27,     1] loss: 0.403
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006372864384140656,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4092438457,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.65109575259828}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006945518110372403,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 620697673,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.04018020490421}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.669
[4,     1] loss: 0.635
[5,     1] loss: 0.593
[6,     1] loss: 0.552
[7,     1] loss: 0.496
[8,     1] loss: 0.448
[9,     1] loss: 0.401
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024434170783975005,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 416464005,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.62891157775341}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.689
[3,     1] loss: 0.674
[4,     1] loss: 0.652
[5,     1] loss: 0.631
[6,     1] loss: 0.607
[7,     1] loss: 0.587
[8,     1] loss: 0.555
[9,     1] loss: 0.531
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009886891778750143,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3400876576,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.5688905081081747}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.698
[3,     1] loss: 0.691
[4,     1] loss: 0.670
[5,     1] loss: 0.655
[6,     1] loss: 0.637
[7,     1] loss: 0.567
[8,     1] loss: 0.556
[9,     1] loss: 0.503
[10,     1] loss: 0.434
[11,     1] loss: 0.381
[12,     1] loss: 0.358
[13,     1] loss: 0.430
[14,     1] loss: 0.368
[15,     1] loss: 0.301
[16,     1] loss: 0.353
[17,     1] loss: 0.295
[18,     1] loss: 0.274
[19,     1] loss: 0.217
[20,     1] loss: 0.270
[21,     1] loss: 0.311
[22,     1] loss: 0.208
[23,     1] loss: 0.302
[24,     1] loss: 0.348
[25,     1] loss: 0.236
[26,     1] loss: 0.250
[27,     1] loss: 0.235
[28,     1] loss: 0.252
[29,     1] loss: 0.236
[30,     1] loss: 0.236
[31,     1] loss: 0.198
[32,     1] loss: 0.242
[33,     1] loss: 0.229
[34,     1] loss: 0.203
[35,     1] loss: 0.192
[36,     1] loss: 0.209
[37,     1] loss: 0.212
[38,     1] loss: 0.196
[39,     1] loss: 0.225
[40,     1] loss: 0.208
[41,     1] loss: 0.204
[42,     1] loss: 0.188
[43,     1] loss: 0.187
[44,     1] loss: 0.213
[45,     1] loss: 0.137
[46,     1] loss: 0.196
[47,     1] loss: 0.162
[48,     1] loss: 0.162
[49,     1] loss: 0.203
[50,     1] loss: 0.146
[51,     1] loss: 0.158
[52,     1] loss: 0.138
[53,     1] loss: 0.179
[54,     1] loss: 0.144
[55,     1] loss: 0.184
[56,     1] loss: 0.236
[57,     1] loss: 0.139
[58,     1] loss: 0.165
[59,     1] loss: 0.117
[60,     1] loss: 0.157
[61,     1] loss: 0.156
[62,     1] loss: 0.167
[63,     1] loss: 0.158
[64,     1] loss: 0.189
[65,     1] loss: 0.135
[66,     1] loss: 0.184
[67,     1] loss: 0.134
[68,     1] loss: 0.146
[69,     1] loss: 0.160
[70,     1] loss: 0.143
[71,     1] loss: 0.148
[72,     1] loss: 0.171
[73,     1] loss: 0.173
[74,     1] loss: 0.109
Early stopping applied (best metric=0.2489701509475708)
Finished Training
Total time taken: 8.921488046646118
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.710
[3,     1] loss: 0.687
[4,     1] loss: 0.686
[5,     1] loss: 0.672
[6,     1] loss: 0.660
[7,     1] loss: 0.643
[8,     1] loss: 0.598
[9,     1] loss: 0.551
[10,     1] loss: 0.530
[11,     1] loss: 0.484
[12,     1] loss: 0.419
[13,     1] loss: 0.519
[14,     1] loss: 0.467
[15,     1] loss: 0.443
[16,     1] loss: 0.340
[17,     1] loss: 0.419
[18,     1] loss: 0.339
[19,     1] loss: 0.333
[20,     1] loss: 0.354
[21,     1] loss: 0.291
[22,     1] loss: 0.273
[23,     1] loss: 0.283
[24,     1] loss: 0.261
[25,     1] loss: 0.276
[26,     1] loss: 0.378
[27,     1] loss: 0.232
[28,     1] loss: 0.214
[29,     1] loss: 0.234
[30,     1] loss: 0.213
[31,     1] loss: 0.207
[32,     1] loss: 0.176
[33,     1] loss: 0.164
[34,     1] loss: 0.206
[35,     1] loss: 0.142
[36,     1] loss: 0.133
[37,     1] loss: 0.164
[38,     1] loss: 0.184
[39,     1] loss: 0.171
[40,     1] loss: 0.180
[41,     1] loss: 0.203
[42,     1] loss: 0.153
[43,     1] loss: 0.169
[44,     1] loss: 0.188
[45,     1] loss: 0.168
[46,     1] loss: 0.186
[47,     1] loss: 0.168
[48,     1] loss: 0.145
[49,     1] loss: 0.124
[50,     1] loss: 0.182
[51,     1] loss: 0.134
[52,     1] loss: 0.147
[53,     1] loss: 0.142
[54,     1] loss: 0.155
[55,     1] loss: 0.142
[56,     1] loss: 0.155
[57,     1] loss: 0.155
[58,     1] loss: 0.158
[59,     1] loss: 0.141
Early stopping applied (best metric=0.5008542537689209)
Finished Training
Total time taken: 7.594298839569092
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.694
[3,     1] loss: 0.680
[4,     1] loss: 0.666
[5,     1] loss: 0.617
[6,     1] loss: 0.564
[7,     1] loss: 0.488
[8,     1] loss: 0.455
[9,     1] loss: 0.479
[10,     1] loss: 0.429
[11,     1] loss: 0.364
[12,     1] loss: 0.341
[13,     1] loss: 0.319
[14,     1] loss: 0.304
[15,     1] loss: 0.331
[16,     1] loss: 0.220
[17,     1] loss: 0.289
[18,     1] loss: 0.311
[19,     1] loss: 0.280
[20,     1] loss: 0.266
[21,     1] loss: 0.266
[22,     1] loss: 0.247
[23,     1] loss: 0.222
[24,     1] loss: 0.308
[25,     1] loss: 0.233
[26,     1] loss: 0.188
[27,     1] loss: 0.206
[28,     1] loss: 0.175
[29,     1] loss: 0.214
[30,     1] loss: 0.213
[31,     1] loss: 0.295
[32,     1] loss: 0.209
[33,     1] loss: 0.146
[34,     1] loss: 0.195
[35,     1] loss: 0.203
[36,     1] loss: 0.187
[37,     1] loss: 0.241
[38,     1] loss: 0.203
[39,     1] loss: 0.116
[40,     1] loss: 0.208
[41,     1] loss: 0.186
[42,     1] loss: 0.146
[43,     1] loss: 0.187
[44,     1] loss: 0.234
[45,     1] loss: 0.151
[46,     1] loss: 0.176
[47,     1] loss: 0.191
[48,     1] loss: 0.128
[49,     1] loss: 0.132
[50,     1] loss: 0.206
[51,     1] loss: 0.218
[52,     1] loss: 0.163
[53,     1] loss: 0.207
[54,     1] loss: 0.154
[55,     1] loss: 0.166
[56,     1] loss: 0.220
[57,     1] loss: 0.143
[58,     1] loss: 0.154
[59,     1] loss: 0.145
[60,     1] loss: 0.169
[61,     1] loss: 0.159
Early stopping applied (best metric=0.36923515796661377)
Finished Training
Total time taken: 7.599938631057739
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.695
[3,     1] loss: 0.687
[4,     1] loss: 0.683
[5,     1] loss: 0.646
[6,     1] loss: 0.620
[7,     1] loss: 0.579
[8,     1] loss: 0.541
[9,     1] loss: 0.523
[10,     1] loss: 0.389
[11,     1] loss: 0.377
[12,     1] loss: 0.465
[13,     1] loss: 0.375
[14,     1] loss: 0.350
[15,     1] loss: 0.374
[16,     1] loss: 0.316
[17,     1] loss: 0.262
[18,     1] loss: 0.219
[19,     1] loss: 0.216
[20,     1] loss: 0.236
[21,     1] loss: 0.237
[22,     1] loss: 0.189
[23,     1] loss: 0.138
[24,     1] loss: 0.202
[25,     1] loss: 0.195
[26,     1] loss: 0.217
[27,     1] loss: 0.201
[28,     1] loss: 0.157
[29,     1] loss: 0.251
[30,     1] loss: 0.154
[31,     1] loss: 0.111
[32,     1] loss: 0.297
[33,     1] loss: 0.225
[34,     1] loss: 0.169
[35,     1] loss: 0.147
[36,     1] loss: 0.232
[37,     1] loss: 0.169
[38,     1] loss: 0.155
[39,     1] loss: 0.194
[40,     1] loss: 0.148
[41,     1] loss: 0.203
[42,     1] loss: 0.166
[43,     1] loss: 0.149
[44,     1] loss: 0.175
[45,     1] loss: 0.152
[46,     1] loss: 0.200
[47,     1] loss: 0.176
[48,     1] loss: 0.162
[49,     1] loss: 0.199
[50,     1] loss: 0.169
[51,     1] loss: 0.138
[52,     1] loss: 0.137
[53,     1] loss: 0.129
[54,     1] loss: 0.159
[55,     1] loss: 0.174
[56,     1] loss: 0.113
[57,     1] loss: 0.246
Early stopping applied (best metric=0.43301475048065186)
Finished Training
Total time taken: 6.985679864883423
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.695
[3,     1] loss: 0.683
[4,     1] loss: 0.673
[5,     1] loss: 0.660
[6,     1] loss: 0.624
[7,     1] loss: 0.570
[8,     1] loss: 0.539
[9,     1] loss: 0.523
[10,     1] loss: 0.462
[11,     1] loss: 0.496
[12,     1] loss: 0.411
[13,     1] loss: 0.385
[14,     1] loss: 0.345
[15,     1] loss: 0.326
[16,     1] loss: 0.313
[17,     1] loss: 0.263
[18,     1] loss: 0.292
[19,     1] loss: 0.259
[20,     1] loss: 0.301
[21,     1] loss: 0.210
[22,     1] loss: 0.308
[23,     1] loss: 0.244
[24,     1] loss: 0.276
[25,     1] loss: 0.214
[26,     1] loss: 0.275
[27,     1] loss: 0.225
[28,     1] loss: 0.198
[29,     1] loss: 0.218
[30,     1] loss: 0.258
[31,     1] loss: 0.286
[32,     1] loss: 0.149
[33,     1] loss: 0.216
[34,     1] loss: 0.203
[35,     1] loss: 0.221
[36,     1] loss: 0.217
[37,     1] loss: 0.193
[38,     1] loss: 0.175
[39,     1] loss: 0.179
[40,     1] loss: 0.195
[41,     1] loss: 0.148
[42,     1] loss: 0.146
[43,     1] loss: 0.240
[44,     1] loss: 0.251
[45,     1] loss: 0.216
[46,     1] loss: 0.217
[47,     1] loss: 0.136
[48,     1] loss: 0.218
[49,     1] loss: 0.203
[50,     1] loss: 0.274
[51,     1] loss: 0.144
[52,     1] loss: 0.211
[53,     1] loss: 0.203
[54,     1] loss: 0.189
[55,     1] loss: 0.185
[56,     1] loss: 0.200
[57,     1] loss: 0.162
[58,     1] loss: 0.209
[59,     1] loss: 0.158
[60,     1] loss: 0.202
[61,     1] loss: 0.169
[62,     1] loss: 0.165
[63,     1] loss: 0.194
[64,     1] loss: 0.166
[65,     1] loss: 0.131
[66,     1] loss: 0.191
[67,     1] loss: 0.159
[68,     1] loss: 0.228
[69,     1] loss: 0.174
[70,     1] loss: 0.152
[71,     1] loss: 0.170
[72,     1] loss: 0.140
[73,     1] loss: 0.164
[74,     1] loss: 0.160
[75,     1] loss: 0.128
[76,     1] loss: 0.206
[77,     1] loss: 0.218
[78,     1] loss: 0.146
[79,     1] loss: 0.169
[80,     1] loss: 0.218
[81,     1] loss: 0.203
[82,     1] loss: 0.168
[83,     1] loss: 0.141
[84,     1] loss: 0.120
[85,     1] loss: 0.170
[86,     1] loss: 0.168
[87,     1] loss: 0.168
[88,     1] loss: 0.151
[89,     1] loss: 0.168
[90,     1] loss: 0.169
[91,     1] loss: 0.211
[92,     1] loss: 0.194
[93,     1] loss: 0.156
[94,     1] loss: 0.142
[95,     1] loss: 0.150
Early stopping applied (best metric=0.0856001004576683)
Finished Training
Total time taken: 11.533284664154053
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.689
[2,     1] loss: 0.690
[3,     1] loss: 0.663
[4,     1] loss: 0.636
[5,     1] loss: 0.561
[6,     1] loss: 0.487
[7,     1] loss: 0.438
[8,     1] loss: 0.414
[9,     1] loss: 0.477
[10,     1] loss: 0.385
[11,     1] loss: 0.371
[12,     1] loss: 0.408
[13,     1] loss: 0.369
[14,     1] loss: 0.273
[15,     1] loss: 0.313
[16,     1] loss: 0.277
[17,     1] loss: 0.251
[18,     1] loss: 0.248
[19,     1] loss: 0.266
[20,     1] loss: 0.323
[21,     1] loss: 0.339
[22,     1] loss: 0.252
[23,     1] loss: 0.200
[24,     1] loss: 0.174
[25,     1] loss: 0.262
[26,     1] loss: 0.234
[27,     1] loss: 0.169
[28,     1] loss: 0.170
[29,     1] loss: 0.153
[30,     1] loss: 0.242
[31,     1] loss: 0.191
[32,     1] loss: 0.168
[33,     1] loss: 0.207
[34,     1] loss: 0.192
[35,     1] loss: 0.149
[36,     1] loss: 0.160
[37,     1] loss: 0.195
[38,     1] loss: 0.181
[39,     1] loss: 0.163
[40,     1] loss: 0.190
[41,     1] loss: 0.182
[42,     1] loss: 0.192
[43,     1] loss: 0.211
[44,     1] loss: 0.139
[45,     1] loss: 0.141
[46,     1] loss: 0.202
[47,     1] loss: 0.154
[48,     1] loss: 0.204
[49,     1] loss: 0.171
[50,     1] loss: 0.182
[51,     1] loss: 0.133
[52,     1] loss: 0.126
[53,     1] loss: 0.165
[54,     1] loss: 0.168
Early stopping applied (best metric=0.4489485025405884)
Finished Training
Total time taken: 6.561249732971191
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.701
[3,     1] loss: 0.686
[4,     1] loss: 0.669
[5,     1] loss: 0.635
[6,     1] loss: 0.578
[7,     1] loss: 0.542
[8,     1] loss: 0.507
[9,     1] loss: 0.479
[10,     1] loss: 0.438
[11,     1] loss: 0.509
[12,     1] loss: 0.389
[13,     1] loss: 0.405
[14,     1] loss: 0.342
[15,     1] loss: 0.329
[16,     1] loss: 0.332
[17,     1] loss: 0.400
[18,     1] loss: 0.351
[19,     1] loss: 0.316
[20,     1] loss: 0.302
[21,     1] loss: 0.282
[22,     1] loss: 0.286
[23,     1] loss: 0.228
[24,     1] loss: 0.271
[25,     1] loss: 0.207
[26,     1] loss: 0.219
[27,     1] loss: 0.233
[28,     1] loss: 0.220
[29,     1] loss: 0.160
[30,     1] loss: 0.201
[31,     1] loss: 0.230
[32,     1] loss: 0.179
[33,     1] loss: 0.241
[34,     1] loss: 0.194
[35,     1] loss: 0.196
[36,     1] loss: 0.172
[37,     1] loss: 0.249
[38,     1] loss: 0.177
[39,     1] loss: 0.242
[40,     1] loss: 0.189
[41,     1] loss: 0.242
[42,     1] loss: 0.198
[43,     1] loss: 0.211
[44,     1] loss: 0.197
[45,     1] loss: 0.163
[46,     1] loss: 0.193
[47,     1] loss: 0.209
[48,     1] loss: 0.217
[49,     1] loss: 0.161
[50,     1] loss: 0.154
[51,     1] loss: 0.160
[52,     1] loss: 0.191
[53,     1] loss: 0.210
[54,     1] loss: 0.132
[55,     1] loss: 0.185
[56,     1] loss: 0.154
[57,     1] loss: 0.205
[58,     1] loss: 0.204
[59,     1] loss: 0.170
[60,     1] loss: 0.179
[61,     1] loss: 0.209
[62,     1] loss: 0.159
[63,     1] loss: 0.158
[64,     1] loss: 0.213
[65,     1] loss: 0.195
[66,     1] loss: 0.152
[67,     1] loss: 0.138
Early stopping applied (best metric=0.3963342607021332)
Finished Training
Total time taken: 8.122665882110596
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.707
[3,     1] loss: 0.687
[4,     1] loss: 0.695
[5,     1] loss: 0.684
[6,     1] loss: 0.678
[7,     1] loss: 0.663
[8,     1] loss: 0.634
[9,     1] loss: 0.609
[10,     1] loss: 0.565
[11,     1] loss: 0.517
[12,     1] loss: 0.500
[13,     1] loss: 0.463
[14,     1] loss: 0.430
[15,     1] loss: 0.394
[16,     1] loss: 0.421
[17,     1] loss: 0.340
[18,     1] loss: 0.378
[19,     1] loss: 0.312
[20,     1] loss: 0.255
[21,     1] loss: 0.396
[22,     1] loss: 0.353
[23,     1] loss: 0.344
[24,     1] loss: 0.312
[25,     1] loss: 0.329
[26,     1] loss: 0.283
[27,     1] loss: 0.344
[28,     1] loss: 0.246
[29,     1] loss: 0.279
[30,     1] loss: 0.261
[31,     1] loss: 0.242
[32,     1] loss: 0.231
[33,     1] loss: 0.211
[34,     1] loss: 0.211
[35,     1] loss: 0.225
[36,     1] loss: 0.169
[37,     1] loss: 0.189
[38,     1] loss: 0.191
[39,     1] loss: 0.179
[40,     1] loss: 0.174
[41,     1] loss: 0.214
[42,     1] loss: 0.178
[43,     1] loss: 0.143
[44,     1] loss: 0.233
[45,     1] loss: 0.171
[46,     1] loss: 0.167
[47,     1] loss: 0.132
[48,     1] loss: 0.143
[49,     1] loss: 0.194
[50,     1] loss: 0.199
[51,     1] loss: 0.242
[52,     1] loss: 0.165
[53,     1] loss: 0.175
[54,     1] loss: 0.164
[55,     1] loss: 0.209
[56,     1] loss: 0.157
[57,     1] loss: 0.180
[58,     1] loss: 0.124
[59,     1] loss: 0.149
[60,     1] loss: 0.198
[61,     1] loss: 0.144
[62,     1] loss: 0.131
[63,     1] loss: 0.179
[64,     1] loss: 0.161
[65,     1] loss: 0.139
[66,     1] loss: 0.121
[67,     1] loss: 0.207
[68,     1] loss: 0.138
[69,     1] loss: 0.128
[70,     1] loss: 0.160
[71,     1] loss: 0.112
[72,     1] loss: 0.148
[73,     1] loss: 0.137
[74,     1] loss: 0.144
[75,     1] loss: 0.154
[76,     1] loss: 0.091
[77,     1] loss: 0.155
[78,     1] loss: 0.126
[79,     1] loss: 0.143
[80,     1] loss: 0.102
[81,     1] loss: 0.099
[82,     1] loss: 0.142
[83,     1] loss: 0.121
[84,     1] loss: 0.116
[85,     1] loss: 0.165
[86,     1] loss: 0.110
[87,     1] loss: 0.156
[88,     1] loss: 0.151
[89,     1] loss: 0.146
[90,     1] loss: 0.153
[91,     1] loss: 0.149
[92,     1] loss: 0.138
[93,     1] loss: 0.122
[94,     1] loss: 0.125
[95,     1] loss: 0.083
[96,     1] loss: 0.108
[97,     1] loss: 0.115
[98,     1] loss: 0.096
[99,     1] loss: 0.175
[100,     1] loss: 0.110
[101,     1] loss: 0.111
[102,     1] loss: 0.095
[103,     1] loss: 0.105
[104,     1] loss: 0.123
[105,     1] loss: 0.116
[106,     1] loss: 0.166
[107,     1] loss: 0.086
[108,     1] loss: 0.125
[109,     1] loss: 0.111
[110,     1] loss: 0.123
[111,     1] loss: 0.132
[112,     1] loss: 0.111
[113,     1] loss: 0.134
[114,     1] loss: 0.100
[115,     1] loss: 0.152
[116,     1] loss: 0.114
[117,     1] loss: 0.139
[118,     1] loss: 0.113
[119,     1] loss: 0.078
[120,     1] loss: 0.079
[121,     1] loss: 0.122
[122,     1] loss: 0.100
[123,     1] loss: 0.127
[124,     1] loss: 0.105
[125,     1] loss: 0.108
[126,     1] loss: 0.132
[127,     1] loss: 0.143
[128,     1] loss: 0.117
[129,     1] loss: 0.121
[130,     1] loss: 0.097
[131,     1] loss: 0.092
[132,     1] loss: 0.100
[133,     1] loss: 0.104
[134,     1] loss: 0.092
[135,     1] loss: 0.134
[136,     1] loss: 0.110
[137,     1] loss: 0.100
[138,     1] loss: 0.107
[139,     1] loss: 0.155
[140,     1] loss: 0.107
[141,     1] loss: 0.086
[142,     1] loss: 0.089
[143,     1] loss: 0.082
[144,     1] loss: 0.101
[145,     1] loss: 0.084
[146,     1] loss: 0.128
[147,     1] loss: 0.155
[148,     1] loss: 0.087
[149,     1] loss: 0.120
[150,     1] loss: 0.121
[151,     1] loss: 0.096
[152,     1] loss: 0.111
[153,     1] loss: 0.099
[154,     1] loss: 0.131
[155,     1] loss: 0.145
[156,     1] loss: 0.156
[157,     1] loss: 0.167
[158,     1] loss: 0.121
[159,     1] loss: 0.127
[160,     1] loss: 0.169
[161,     1] loss: 0.144
[162,     1] loss: 0.129
[163,     1] loss: 0.141
[164,     1] loss: 0.152
[165,     1] loss: 0.133
[166,     1] loss: 0.133
[167,     1] loss: 0.144
[168,     1] loss: 0.163
[169,     1] loss: 0.166
[170,     1] loss: 0.118
[171,     1] loss: 0.174
[172,     1] loss: 0.145
[173,     1] loss: 0.170
[174,     1] loss: 0.152
[175,     1] loss: 0.123
[176,     1] loss: 0.147
Early stopping applied (best metric=0.09724734723567963)
Finished Training
Total time taken: 21.30887746810913
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.691
[4,     1] loss: 0.659
[5,     1] loss: 0.640
[6,     1] loss: 0.620
[7,     1] loss: 0.538
[8,     1] loss: 0.484
[9,     1] loss: 0.469
[10,     1] loss: 0.385
[11,     1] loss: 0.421
[12,     1] loss: 0.368
[13,     1] loss: 0.405
[14,     1] loss: 0.342
[15,     1] loss: 0.303
[16,     1] loss: 0.303
[17,     1] loss: 0.323
[18,     1] loss: 0.339
[19,     1] loss: 0.275
[20,     1] loss: 0.247
[21,     1] loss: 0.275
[22,     1] loss: 0.264
[23,     1] loss: 0.191
[24,     1] loss: 0.257
[25,     1] loss: 0.268
[26,     1] loss: 0.212
[27,     1] loss: 0.282
[28,     1] loss: 0.201
[29,     1] loss: 0.204
[30,     1] loss: 0.194
[31,     1] loss: 0.203
[32,     1] loss: 0.209
[33,     1] loss: 0.168
[34,     1] loss: 0.195
[35,     1] loss: 0.186
[36,     1] loss: 0.192
[37,     1] loss: 0.138
[38,     1] loss: 0.193
[39,     1] loss: 0.191
[40,     1] loss: 0.155
[41,     1] loss: 0.145
[42,     1] loss: 0.143
[43,     1] loss: 0.165
[44,     1] loss: 0.147
[45,     1] loss: 0.221
[46,     1] loss: 0.194
[47,     1] loss: 0.174
[48,     1] loss: 0.208
[49,     1] loss: 0.227
[50,     1] loss: 0.173
[51,     1] loss: 0.164
[52,     1] loss: 0.214
[53,     1] loss: 0.165
[54,     1] loss: 0.145
[55,     1] loss: 0.156
[56,     1] loss: 0.174
[57,     1] loss: 0.148
[58,     1] loss: 0.121
[59,     1] loss: 0.131
[60,     1] loss: 0.184
[61,     1] loss: 0.171
[62,     1] loss: 0.189
[63,     1] loss: 0.127
[64,     1] loss: 0.192
[65,     1] loss: 0.205
[66,     1] loss: 0.168
[67,     1] loss: 0.101
[68,     1] loss: 0.173
[69,     1] loss: 0.203
[70,     1] loss: 0.180
[71,     1] loss: 0.117
[72,     1] loss: 0.104
[73,     1] loss: 0.166
[74,     1] loss: 0.170
[75,     1] loss: 0.122
[76,     1] loss: 0.116
[77,     1] loss: 0.179
[78,     1] loss: 0.173
[79,     1] loss: 0.154
[80,     1] loss: 0.127
[81,     1] loss: 0.157
[82,     1] loss: 0.147
[83,     1] loss: 0.154
[84,     1] loss: 0.131
[85,     1] loss: 0.160
[86,     1] loss: 0.129
[87,     1] loss: 0.147
[88,     1] loss: 0.082
[89,     1] loss: 0.122
[90,     1] loss: 0.107
[91,     1] loss: 0.099
[92,     1] loss: 0.112
[93,     1] loss: 0.106
[94,     1] loss: 0.111
[95,     1] loss: 0.102
[96,     1] loss: 0.101
[97,     1] loss: 0.127
[98,     1] loss: 0.144
[99,     1] loss: 0.132
[100,     1] loss: 0.134
[101,     1] loss: 0.108
[102,     1] loss: 0.132
[103,     1] loss: 0.115
[104,     1] loss: 0.132
[105,     1] loss: 0.079
[106,     1] loss: 0.108
[107,     1] loss: 0.136
[108,     1] loss: 0.104
[109,     1] loss: 0.098
[110,     1] loss: 0.108
[111,     1] loss: 0.145
[112,     1] loss: 0.108
[113,     1] loss: 0.071
[114,     1] loss: 0.100
[115,     1] loss: 0.088
[116,     1] loss: 0.123
[117,     1] loss: 0.127
[118,     1] loss: 0.134
[119,     1] loss: 0.120
[120,     1] loss: 0.119
[121,     1] loss: 0.095
[122,     1] loss: 0.120
[123,     1] loss: 0.120
[124,     1] loss: 0.109
[125,     1] loss: 0.131
[126,     1] loss: 0.101
[127,     1] loss: 0.073
[128,     1] loss: 0.105
[129,     1] loss: 0.111
[130,     1] loss: 0.074
[131,     1] loss: 0.130
[132,     1] loss: 0.110
[133,     1] loss: 0.141
[134,     1] loss: 0.109
[135,     1] loss: 0.102
[136,     1] loss: 0.145
[137,     1] loss: 0.095
[138,     1] loss: 0.125
[139,     1] loss: 0.112
[140,     1] loss: 0.105
[141,     1] loss: 0.155
[142,     1] loss: 0.107
[143,     1] loss: 0.140
[144,     1] loss: 0.134
[145,     1] loss: 0.131
[146,     1] loss: 0.106
[147,     1] loss: 0.128
[148,     1] loss: 0.146
[149,     1] loss: 0.167
[150,     1] loss: 0.131
[151,     1] loss: 0.163
[152,     1] loss: 0.117
[153,     1] loss: 0.148
[154,     1] loss: 0.104
[155,     1] loss: 0.115
[156,     1] loss: 0.116
[157,     1] loss: 0.120
[158,     1] loss: 0.120
[159,     1] loss: 0.087
[160,     1] loss: 0.124
[161,     1] loss: 0.072
[162,     1] loss: 0.121
[163,     1] loss: 0.131
[164,     1] loss: 0.138
[165,     1] loss: 0.074
[166,     1] loss: 0.137
[167,     1] loss: 0.115
[168,     1] loss: 0.143
[169,     1] loss: 0.112
[170,     1] loss: 0.096
[171,     1] loss: 0.093
[172,     1] loss: 0.143
[173,     1] loss: 0.150
[174,     1] loss: 0.104
[175,     1] loss: 0.127
[176,     1] loss: 0.134
[177,     1] loss: 0.121
[178,     1] loss: 0.085
[179,     1] loss: 0.130
[180,     1] loss: 0.154
Early stopping applied (best metric=0.14578017592430115)
Finished Training
Total time taken: 21.71657133102417
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.698
[3,     1] loss: 0.677
[4,     1] loss: 0.638
[5,     1] loss: 0.603
[6,     1] loss: 0.521
[7,     1] loss: 0.453
[8,     1] loss: 0.471
[9,     1] loss: 0.297
[10,     1] loss: 0.358
[11,     1] loss: 0.439
[12,     1] loss: 0.296
[13,     1] loss: 0.321
[14,     1] loss: 0.304
[15,     1] loss: 0.343
[16,     1] loss: 0.252
[17,     1] loss: 0.275
[18,     1] loss: 0.256
[19,     1] loss: 0.284
[20,     1] loss: 0.265
[21,     1] loss: 0.199
[22,     1] loss: 0.245
[23,     1] loss: 0.201
[24,     1] loss: 0.171
[25,     1] loss: 0.149
[26,     1] loss: 0.138
[27,     1] loss: 0.238
[28,     1] loss: 0.171
[29,     1] loss: 0.162
[30,     1] loss: 0.199
[31,     1] loss: 0.158
[32,     1] loss: 0.165
[33,     1] loss: 0.163
[34,     1] loss: 0.150
[35,     1] loss: 0.138
[36,     1] loss: 0.217
[37,     1] loss: 0.169
[38,     1] loss: 0.172
[39,     1] loss: 0.170
[40,     1] loss: 0.143
[41,     1] loss: 0.169
[42,     1] loss: 0.182
[43,     1] loss: 0.222
[44,     1] loss: 0.167
[45,     1] loss: 0.129
[46,     1] loss: 0.208
[47,     1] loss: 0.147
[48,     1] loss: 0.142
[49,     1] loss: 0.128
[50,     1] loss: 0.129
[51,     1] loss: 0.151
[52,     1] loss: 0.116
[53,     1] loss: 0.133
[54,     1] loss: 0.116
[55,     1] loss: 0.105
[56,     1] loss: 0.131
[57,     1] loss: 0.148
[58,     1] loss: 0.148
[59,     1] loss: 0.176
[60,     1] loss: 0.107
[61,     1] loss: 0.162
[62,     1] loss: 0.179
[63,     1] loss: 0.129
[64,     1] loss: 0.126
[65,     1] loss: 0.181
[66,     1] loss: 0.090
[67,     1] loss: 0.095
[68,     1] loss: 0.115
[69,     1] loss: 0.117
[70,     1] loss: 0.143
[71,     1] loss: 0.147
[72,     1] loss: 0.134
[73,     1] loss: 0.109
[74,     1] loss: 0.135
[75,     1] loss: 0.178
[76,     1] loss: 0.155
[77,     1] loss: 0.097
[78,     1] loss: 0.135
[79,     1] loss: 0.156
[80,     1] loss: 0.155
[81,     1] loss: 0.121
[82,     1] loss: 0.124
[83,     1] loss: 0.169
[84,     1] loss: 0.159
[85,     1] loss: 0.133
[86,     1] loss: 0.115
[87,     1] loss: 0.126
[88,     1] loss: 0.139
[89,     1] loss: 0.110
[90,     1] loss: 0.113
[91,     1] loss: 0.139
[92,     1] loss: 0.132
[93,     1] loss: 0.118
Early stopping applied (best metric=0.27131399512290955)
Finished Training
Total time taken: 11.230812311172485
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.717
[2,     1] loss: 0.699
[3,     1] loss: 0.688
[4,     1] loss: 0.685
[5,     1] loss: 0.686
[6,     1] loss: 0.681
[7,     1] loss: 0.667
[8,     1] loss: 0.640
[9,     1] loss: 0.610
[10,     1] loss: 0.545
[11,     1] loss: 0.523
[12,     1] loss: 0.501
[13,     1] loss: 0.459
[14,     1] loss: 0.442
[15,     1] loss: 0.441
[16,     1] loss: 0.400
[17,     1] loss: 0.336
[18,     1] loss: 0.332
[19,     1] loss: 0.346
[20,     1] loss: 0.272
[21,     1] loss: 0.295
[22,     1] loss: 0.218
[23,     1] loss: 0.286
[24,     1] loss: 0.287
[25,     1] loss: 0.236
[26,     1] loss: 0.177
[27,     1] loss: 0.235
[28,     1] loss: 0.228
[29,     1] loss: 0.204
[30,     1] loss: 0.247
[31,     1] loss: 0.193
[32,     1] loss: 0.205
[33,     1] loss: 0.261
[34,     1] loss: 0.182
[35,     1] loss: 0.217
[36,     1] loss: 0.200
[37,     1] loss: 0.135
[38,     1] loss: 0.159
[39,     1] loss: 0.169
[40,     1] loss: 0.192
[41,     1] loss: 0.203
[42,     1] loss: 0.161
[43,     1] loss: 0.164
[44,     1] loss: 0.198
[45,     1] loss: 0.176
[46,     1] loss: 0.160
[47,     1] loss: 0.191
[48,     1] loss: 0.153
[49,     1] loss: 0.151
[50,     1] loss: 0.114
[51,     1] loss: 0.170
[52,     1] loss: 0.150
[53,     1] loss: 0.166
[54,     1] loss: 0.137
[55,     1] loss: 0.186
[56,     1] loss: 0.170
[57,     1] loss: 0.137
[58,     1] loss: 0.132
[59,     1] loss: 0.176
[60,     1] loss: 0.142
[61,     1] loss: 0.168
[62,     1] loss: 0.119
[63,     1] loss: 0.138
[64,     1] loss: 0.143
[65,     1] loss: 0.138
[66,     1] loss: 0.118
[67,     1] loss: 0.156
[68,     1] loss: 0.116
[69,     1] loss: 0.135
[70,     1] loss: 0.098
[71,     1] loss: 0.136
[72,     1] loss: 0.147
[73,     1] loss: 0.157
[74,     1] loss: 0.131
[75,     1] loss: 0.155
[76,     1] loss: 0.109
[77,     1] loss: 0.214
[78,     1] loss: 0.192
[79,     1] loss: 0.103
[80,     1] loss: 0.164
[81,     1] loss: 0.181
[82,     1] loss: 0.148
[83,     1] loss: 0.070
[84,     1] loss: 0.150
[85,     1] loss: 0.117
[86,     1] loss: 0.133
[87,     1] loss: 0.199
[88,     1] loss: 0.131
[89,     1] loss: 0.112
[90,     1] loss: 0.092
[91,     1] loss: 0.113
[92,     1] loss: 0.097
[93,     1] loss: 0.139
[94,     1] loss: 0.135
[95,     1] loss: 0.104
[96,     1] loss: 0.113
Early stopping applied (best metric=0.2341965287923813)
Finished Training
Total time taken: 11.58625864982605
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.661
[4,     1] loss: 0.615
[5,     1] loss: 0.557
[6,     1] loss: 0.452
[7,     1] loss: 0.463
[8,     1] loss: 0.460
[9,     1] loss: 0.361
[10,     1] loss: 0.480
[11,     1] loss: 0.415
[12,     1] loss: 0.347
[13,     1] loss: 0.429
[14,     1] loss: 0.330
[15,     1] loss: 0.383
[16,     1] loss: 0.340
[17,     1] loss: 0.270
[18,     1] loss: 0.276
[19,     1] loss: 0.276
[20,     1] loss: 0.254
[21,     1] loss: 0.209
[22,     1] loss: 0.261
[23,     1] loss: 0.236
[24,     1] loss: 0.204
[25,     1] loss: 0.201
[26,     1] loss: 0.246
[27,     1] loss: 0.249
[28,     1] loss: 0.197
[29,     1] loss: 0.225
[30,     1] loss: 0.210
[31,     1] loss: 0.230
[32,     1] loss: 0.225
[33,     1] loss: 0.216
[34,     1] loss: 0.198
[35,     1] loss: 0.221
[36,     1] loss: 0.192
[37,     1] loss: 0.170
[38,     1] loss: 0.206
[39,     1] loss: 0.182
[40,     1] loss: 0.198
[41,     1] loss: 0.217
[42,     1] loss: 0.197
[43,     1] loss: 0.226
[44,     1] loss: 0.218
[45,     1] loss: 0.226
[46,     1] loss: 0.220
[47,     1] loss: 0.248
[48,     1] loss: 0.174
[49,     1] loss: 0.208
[50,     1] loss: 0.255
[51,     1] loss: 0.280
[52,     1] loss: 0.150
[53,     1] loss: 0.125
[54,     1] loss: 0.196
[55,     1] loss: 0.213
[56,     1] loss: 0.186
[57,     1] loss: 0.200
[58,     1] loss: 0.170
[59,     1] loss: 0.165
[60,     1] loss: 0.164
[61,     1] loss: 0.144
[62,     1] loss: 0.147
[63,     1] loss: 0.175
[64,     1] loss: 0.151
[65,     1] loss: 0.181
[66,     1] loss: 0.138
[67,     1] loss: 0.165
[68,     1] loss: 0.146
[69,     1] loss: 0.147
[70,     1] loss: 0.172
[71,     1] loss: 0.163
[72,     1] loss: 0.199
[73,     1] loss: 0.148
[74,     1] loss: 0.150
[75,     1] loss: 0.125
[76,     1] loss: 0.169
[77,     1] loss: 0.126
[78,     1] loss: 0.101
[79,     1] loss: 0.150
[80,     1] loss: 0.110
[81,     1] loss: 0.143
[82,     1] loss: 0.106
[83,     1] loss: 0.150
[84,     1] loss: 0.116
[85,     1] loss: 0.130
[86,     1] loss: 0.121
[87,     1] loss: 0.089
[88,     1] loss: 0.113
[89,     1] loss: 0.106
[90,     1] loss: 0.207
[91,     1] loss: 0.102
[92,     1] loss: 0.123
[93,     1] loss: 0.142
[94,     1] loss: 0.141
[95,     1] loss: 0.118
[96,     1] loss: 0.117
[97,     1] loss: 0.100
[98,     1] loss: 0.132
[99,     1] loss: 0.165
[100,     1] loss: 0.113
[101,     1] loss: 0.096
[102,     1] loss: 0.128
[103,     1] loss: 0.136
[104,     1] loss: 0.151
[105,     1] loss: 0.110
[106,     1] loss: 0.190
[107,     1] loss: 0.113
[108,     1] loss: 0.114
[109,     1] loss: 0.181
[110,     1] loss: 0.192
[111,     1] loss: 0.159
[112,     1] loss: 0.138
[113,     1] loss: 0.109
[114,     1] loss: 0.125
[115,     1] loss: 0.151
[116,     1] loss: 0.115
[117,     1] loss: 0.144
[118,     1] loss: 0.136
[119,     1] loss: 0.110
[120,     1] loss: 0.133
[121,     1] loss: 0.103
[122,     1] loss: 0.098
[123,     1] loss: 0.083
[124,     1] loss: 0.097
Early stopping applied (best metric=0.1061527281999588)
Finished Training
Total time taken: 14.950012683868408
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.658
[4,     1] loss: 0.604
[5,     1] loss: 0.514
[6,     1] loss: 0.445
[7,     1] loss: 0.420
[8,     1] loss: 0.326
[9,     1] loss: 0.300
[10,     1] loss: 0.257
[11,     1] loss: 0.341
[12,     1] loss: 0.218
[13,     1] loss: 0.258
[14,     1] loss: 0.278
[15,     1] loss: 0.278
[16,     1] loss: 0.237
[17,     1] loss: 0.250
[18,     1] loss: 0.269
[19,     1] loss: 0.264
[20,     1] loss: 0.190
[21,     1] loss: 0.183
[22,     1] loss: 0.225
[23,     1] loss: 0.181
[24,     1] loss: 0.241
[25,     1] loss: 0.160
[26,     1] loss: 0.223
[27,     1] loss: 0.174
[28,     1] loss: 0.198
[29,     1] loss: 0.176
[30,     1] loss: 0.207
[31,     1] loss: 0.260
[32,     1] loss: 0.215
[33,     1] loss: 0.199
[34,     1] loss: 0.155
[35,     1] loss: 0.213
[36,     1] loss: 0.246
[37,     1] loss: 0.215
[38,     1] loss: 0.196
[39,     1] loss: 0.172
[40,     1] loss: 0.209
[41,     1] loss: 0.156
[42,     1] loss: 0.202
[43,     1] loss: 0.217
[44,     1] loss: 0.187
[45,     1] loss: 0.140
[46,     1] loss: 0.167
[47,     1] loss: 0.162
[48,     1] loss: 0.156
[49,     1] loss: 0.209
[50,     1] loss: 0.182
[51,     1] loss: 0.170
[52,     1] loss: 0.202
[53,     1] loss: 0.231
[54,     1] loss: 0.211
[55,     1] loss: 0.203
Early stopping applied (best metric=0.4342855215072632)
Finished Training
Total time taken: 6.674779891967773
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.689
[4,     1] loss: 0.668
[5,     1] loss: 0.638
[6,     1] loss: 0.582
[7,     1] loss: 0.550
[8,     1] loss: 0.479
[9,     1] loss: 0.454
[10,     1] loss: 0.451
[11,     1] loss: 0.397
[12,     1] loss: 0.397
[13,     1] loss: 0.481
[14,     1] loss: 0.439
[15,     1] loss: 0.473
[16,     1] loss: 0.348
[17,     1] loss: 0.468
[18,     1] loss: 0.348
[19,     1] loss: 0.358
[20,     1] loss: 0.347
[21,     1] loss: 0.394
[22,     1] loss: 0.300
[23,     1] loss: 0.299
[24,     1] loss: 0.326
[25,     1] loss: 0.366
[26,     1] loss: 0.261
[27,     1] loss: 0.263
[28,     1] loss: 0.253
[29,     1] loss: 0.254
[30,     1] loss: 0.209
[31,     1] loss: 0.259
[32,     1] loss: 0.224
[33,     1] loss: 0.167
[34,     1] loss: 0.191
[35,     1] loss: 0.195
[36,     1] loss: 0.172
[37,     1] loss: 0.188
[38,     1] loss: 0.129
[39,     1] loss: 0.201
[40,     1] loss: 0.221
[41,     1] loss: 0.206
[42,     1] loss: 0.142
[43,     1] loss: 0.141
[44,     1] loss: 0.237
[45,     1] loss: 0.202
[46,     1] loss: 0.097
[47,     1] loss: 0.182
[48,     1] loss: 0.231
[49,     1] loss: 0.168
[50,     1] loss: 0.225
[51,     1] loss: 0.146
[52,     1] loss: 0.161
[53,     1] loss: 0.181
[54,     1] loss: 0.145
[55,     1] loss: 0.206
[56,     1] loss: 0.195
[57,     1] loss: 0.154
[58,     1] loss: 0.139
Early stopping applied (best metric=0.4085615277290344)
Finished Training
Total time taken: 7.038617372512817
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.677
[4,     1] loss: 0.641
[5,     1] loss: 0.589
[6,     1] loss: 0.565
[7,     1] loss: 0.444
[8,     1] loss: 0.437
[9,     1] loss: 0.397
[10,     1] loss: 0.355
[11,     1] loss: 0.344
[12,     1] loss: 0.272
[13,     1] loss: 0.256
[14,     1] loss: 0.253
[15,     1] loss: 0.219
[16,     1] loss: 0.204
[17,     1] loss: 0.184
[18,     1] loss: 0.188
[19,     1] loss: 0.225
[20,     1] loss: 0.201
[21,     1] loss: 0.223
[22,     1] loss: 0.145
[23,     1] loss: 0.179
[24,     1] loss: 0.137
[25,     1] loss: 0.194
[26,     1] loss: 0.159
[27,     1] loss: 0.191
[28,     1] loss: 0.154
[29,     1] loss: 0.189
[30,     1] loss: 0.132
[31,     1] loss: 0.159
[32,     1] loss: 0.146
[33,     1] loss: 0.182
[34,     1] loss: 0.162
[35,     1] loss: 0.161
[36,     1] loss: 0.200
[37,     1] loss: 0.135
[38,     1] loss: 0.171
[39,     1] loss: 0.164
[40,     1] loss: 0.126
[41,     1] loss: 0.167
[42,     1] loss: 0.163
[43,     1] loss: 0.137
[44,     1] loss: 0.141
[45,     1] loss: 0.164
[46,     1] loss: 0.154
[47,     1] loss: 0.146
[48,     1] loss: 0.151
[49,     1] loss: 0.142
[50,     1] loss: 0.125
[51,     1] loss: 0.148
[52,     1] loss: 0.207
[53,     1] loss: 0.145
[54,     1] loss: 0.135
[55,     1] loss: 0.176
[56,     1] loss: 0.106
[57,     1] loss: 0.165
Early stopping applied (best metric=0.3679467439651489)
Finished Training
Total time taken: 7.002702474594116
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.708
[3,     1] loss: 0.689
[4,     1] loss: 0.690
[5,     1] loss: 0.692
[6,     1] loss: 0.686
[7,     1] loss: 0.681
[8,     1] loss: 0.668
[9,     1] loss: 0.652
[10,     1] loss: 0.631
[11,     1] loss: 0.580
[12,     1] loss: 0.536
[13,     1] loss: 0.552
[14,     1] loss: 0.484
[15,     1] loss: 0.484
[16,     1] loss: 0.493
[17,     1] loss: 0.550
[18,     1] loss: 0.391
[19,     1] loss: 0.349
[20,     1] loss: 0.359
[21,     1] loss: 0.427
[22,     1] loss: 0.431
[23,     1] loss: 0.432
[24,     1] loss: 0.385
[25,     1] loss: 0.334
[26,     1] loss: 0.360
[27,     1] loss: 0.388
[28,     1] loss: 0.300
[29,     1] loss: 0.394
[30,     1] loss: 0.359
[31,     1] loss: 0.370
[32,     1] loss: 0.382
[33,     1] loss: 0.417
[34,     1] loss: 0.318
[35,     1] loss: 0.301
[36,     1] loss: 0.305
[37,     1] loss: 0.278
[38,     1] loss: 0.263
[39,     1] loss: 0.272
[40,     1] loss: 0.275
[41,     1] loss: 0.264
[42,     1] loss: 0.221
[43,     1] loss: 0.251
[44,     1] loss: 0.249
[45,     1] loss: 0.290
[46,     1] loss: 0.284
[47,     1] loss: 0.215
[48,     1] loss: 0.185
[49,     1] loss: 0.195
[50,     1] loss: 0.219
[51,     1] loss: 0.135
[52,     1] loss: 0.205
[53,     1] loss: 0.194
[54,     1] loss: 0.194
[55,     1] loss: 0.164
[56,     1] loss: 0.139
[57,     1] loss: 0.165
[58,     1] loss: 0.154
[59,     1] loss: 0.116
[60,     1] loss: 0.137
[61,     1] loss: 0.176
[62,     1] loss: 0.153
[63,     1] loss: 0.178
[64,     1] loss: 0.142
[65,     1] loss: 0.150
[66,     1] loss: 0.168
[67,     1] loss: 0.122
[68,     1] loss: 0.174
[69,     1] loss: 0.198
[70,     1] loss: 0.117
[71,     1] loss: 0.150
[72,     1] loss: 0.168
[73,     1] loss: 0.108
[74,     1] loss: 0.195
[75,     1] loss: 0.156
[76,     1] loss: 0.148
[77,     1] loss: 0.141
[78,     1] loss: 0.155
[79,     1] loss: 0.126
Early stopping applied (best metric=0.4544599652290344)
Finished Training
Total time taken: 9.67452096939087
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.669
[4,     1] loss: 0.644
[5,     1] loss: 0.563
[6,     1] loss: 0.521
[7,     1] loss: 0.440
[8,     1] loss: 0.365
[9,     1] loss: 0.453
[10,     1] loss: 0.340
[11,     1] loss: 0.428
[12,     1] loss: 0.323
[13,     1] loss: 0.266
[14,     1] loss: 0.283
[15,     1] loss: 0.201
[16,     1] loss: 0.266
[17,     1] loss: 0.252
[18,     1] loss: 0.198
[19,     1] loss: 0.240
[20,     1] loss: 0.286
[21,     1] loss: 0.191
[22,     1] loss: 0.239
[23,     1] loss: 0.221
[24,     1] loss: 0.198
[25,     1] loss: 0.216
[26,     1] loss: 0.215
[27,     1] loss: 0.233
[28,     1] loss: 0.140
[29,     1] loss: 0.170
[30,     1] loss: 0.207
[31,     1] loss: 0.190
[32,     1] loss: 0.251
[33,     1] loss: 0.183
[34,     1] loss: 0.170
[35,     1] loss: 0.204
[36,     1] loss: 0.136
[37,     1] loss: 0.204
[38,     1] loss: 0.137
[39,     1] loss: 0.170
[40,     1] loss: 0.164
[41,     1] loss: 0.147
[42,     1] loss: 0.170
[43,     1] loss: 0.162
[44,     1] loss: 0.139
[45,     1] loss: 0.225
[46,     1] loss: 0.196
[47,     1] loss: 0.180
[48,     1] loss: 0.150
[49,     1] loss: 0.110
[50,     1] loss: 0.147
[51,     1] loss: 0.124
[52,     1] loss: 0.092
[53,     1] loss: 0.103
[54,     1] loss: 0.133
[55,     1] loss: 0.098
[56,     1] loss: 0.105
[57,     1] loss: 0.115
Early stopping applied (best metric=0.27488309144973755)
Finished Training
Total time taken: 6.942682981491089
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.684
[4,     1] loss: 0.652
[5,     1] loss: 0.590
[6,     1] loss: 0.552
[7,     1] loss: 0.474
[8,     1] loss: 0.454
[9,     1] loss: 0.452
[10,     1] loss: 0.333
[11,     1] loss: 0.457
[12,     1] loss: 0.412
[13,     1] loss: 0.297
[14,     1] loss: 0.207
[15,     1] loss: 0.312
[16,     1] loss: 0.280
[17,     1] loss: 0.265
[18,     1] loss: 0.274
[19,     1] loss: 0.220
[20,     1] loss: 0.244
[21,     1] loss: 0.253
[22,     1] loss: 0.224
[23,     1] loss: 0.228
[24,     1] loss: 0.221
[25,     1] loss: 0.171
[26,     1] loss: 0.149
[27,     1] loss: 0.171
[28,     1] loss: 0.148
[29,     1] loss: 0.215
[30,     1] loss: 0.176
[31,     1] loss: 0.220
[32,     1] loss: 0.153
[33,     1] loss: 0.186
[34,     1] loss: 0.198
[35,     1] loss: 0.170
[36,     1] loss: 0.239
[37,     1] loss: 0.215
[38,     1] loss: 0.234
[39,     1] loss: 0.167
[40,     1] loss: 0.174
[41,     1] loss: 0.165
[42,     1] loss: 0.125
[43,     1] loss: 0.161
[44,     1] loss: 0.163
[45,     1] loss: 0.199
[46,     1] loss: 0.159
[47,     1] loss: 0.146
[48,     1] loss: 0.138
[49,     1] loss: 0.205
[50,     1] loss: 0.204
[51,     1] loss: 0.163
[52,     1] loss: 0.179
[53,     1] loss: 0.154
[54,     1] loss: 0.153
[55,     1] loss: 0.160
[56,     1] loss: 0.164
[57,     1] loss: 0.175
[58,     1] loss: 0.184
[59,     1] loss: 0.157
[60,     1] loss: 0.175
[61,     1] loss: 0.160
[62,     1] loss: 0.093
[63,     1] loss: 0.177
[64,     1] loss: 0.177
[65,     1] loss: 0.118
[66,     1] loss: 0.155
[67,     1] loss: 0.197
[68,     1] loss: 0.191
[69,     1] loss: 0.139
[70,     1] loss: 0.129
[71,     1] loss: 0.130
[72,     1] loss: 0.173
[73,     1] loss: 0.201
[74,     1] loss: 0.152
[75,     1] loss: 0.167
[76,     1] loss: 0.124
[77,     1] loss: 0.215
[78,     1] loss: 0.161
[79,     1] loss: 0.174
[80,     1] loss: 0.112
[81,     1] loss: 0.152
[82,     1] loss: 0.103
[83,     1] loss: 0.110
[84,     1] loss: 0.137
[85,     1] loss: 0.121
[86,     1] loss: 0.146
[87,     1] loss: 0.098
[88,     1] loss: 0.123
[89,     1] loss: 0.172
[90,     1] loss: 0.147
[91,     1] loss: 0.158
Early stopping applied (best metric=0.39627140760421753)
Finished Training
Total time taken: 10.999647855758667
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.695
[3,     1] loss: 0.691
[4,     1] loss: 0.682
[5,     1] loss: 0.669
[6,     1] loss: 0.657
[7,     1] loss: 0.613
[8,     1] loss: 0.573
[9,     1] loss: 0.552
[10,     1] loss: 0.470
[11,     1] loss: 0.440
[12,     1] loss: 0.459
[13,     1] loss: 0.389
[14,     1] loss: 0.416
[15,     1] loss: 0.320
[16,     1] loss: 0.365
[17,     1] loss: 0.333
[18,     1] loss: 0.260
[19,     1] loss: 0.299
[20,     1] loss: 0.256
[21,     1] loss: 0.216
[22,     1] loss: 0.264
[23,     1] loss: 0.200
[24,     1] loss: 0.248
[25,     1] loss: 0.210
[26,     1] loss: 0.309
[27,     1] loss: 0.225
[28,     1] loss: 0.263
[29,     1] loss: 0.247
[30,     1] loss: 0.254
[31,     1] loss: 0.219
[32,     1] loss: 0.183
[33,     1] loss: 0.329
[34,     1] loss: 0.231
[35,     1] loss: 0.155
[36,     1] loss: 0.253
[37,     1] loss: 0.198
[38,     1] loss: 0.204
[39,     1] loss: 0.243
[40,     1] loss: 0.214
[41,     1] loss: 0.203
[42,     1] loss: 0.158
[43,     1] loss: 0.206
[44,     1] loss: 0.215
[45,     1] loss: 0.239
[46,     1] loss: 0.212
[47,     1] loss: 0.176
[48,     1] loss: 0.195
[49,     1] loss: 0.154
[50,     1] loss: 0.166
[51,     1] loss: 0.145
[52,     1] loss: 0.202
[53,     1] loss: 0.199
[54,     1] loss: 0.190
[55,     1] loss: 0.183
[56,     1] loss: 0.185
[57,     1] loss: 0.211
[58,     1] loss: 0.155
[59,     1] loss: 0.229
[60,     1] loss: 0.156
[61,     1] loss: 0.160
[62,     1] loss: 0.194
[63,     1] loss: 0.186
[64,     1] loss: 0.179
[65,     1] loss: 0.200
[66,     1] loss: 0.153
[67,     1] loss: 0.168
[68,     1] loss: 0.172
[69,     1] loss: 0.159
[70,     1] loss: 0.148
[71,     1] loss: 0.088
[72,     1] loss: 0.133
[73,     1] loss: 0.188
[74,     1] loss: 0.098
[75,     1] loss: 0.118
[76,     1] loss: 0.164
[77,     1] loss: 0.211
[78,     1] loss: 0.205
[79,     1] loss: 0.161
[80,     1] loss: 0.157
[81,     1] loss: 0.139
[82,     1] loss: 0.195
[83,     1] loss: 0.176
[84,     1] loss: 0.151
[85,     1] loss: 0.159
[86,     1] loss: 0.162
[87,     1] loss: 0.155
[88,     1] loss: 0.172
[89,     1] loss: 0.136
[90,     1] loss: 0.150
[91,     1] loss: 0.138
[92,     1] loss: 0.144
[93,     1] loss: 0.159
[94,     1] loss: 0.147
[95,     1] loss: 0.145
[96,     1] loss: 0.096
[97,     1] loss: 0.149
[98,     1] loss: 0.148
[99,     1] loss: 0.175
[100,     1] loss: 0.140
[101,     1] loss: 0.161
Early stopping applied (best metric=0.13999731838703156)
Finished Training
Total time taken: 12.198896169662476
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.651
[4,     1] loss: 0.604
[5,     1] loss: 0.547
[6,     1] loss: 0.483
[7,     1] loss: 0.400
[8,     1] loss: 0.423
[9,     1] loss: 0.404
[10,     1] loss: 0.360
[11,     1] loss: 0.316
[12,     1] loss: 0.242
[13,     1] loss: 0.277
[14,     1] loss: 0.234
[15,     1] loss: 0.272
[16,     1] loss: 0.229
[17,     1] loss: 0.242
[18,     1] loss: 0.217
[19,     1] loss: 0.195
[20,     1] loss: 0.200
[21,     1] loss: 0.251
[22,     1] loss: 0.180
[23,     1] loss: 0.207
[24,     1] loss: 0.182
[25,     1] loss: 0.211
[26,     1] loss: 0.231
[27,     1] loss: 0.179
[28,     1] loss: 0.208
[29,     1] loss: 0.192
[30,     1] loss: 0.201
[31,     1] loss: 0.149
[32,     1] loss: 0.178
[33,     1] loss: 0.175
[34,     1] loss: 0.181
[35,     1] loss: 0.164
[36,     1] loss: 0.194
[37,     1] loss: 0.185
[38,     1] loss: 0.198
[39,     1] loss: 0.205
[40,     1] loss: 0.154
[41,     1] loss: 0.156
[42,     1] loss: 0.162
[43,     1] loss: 0.231
[44,     1] loss: 0.179
[45,     1] loss: 0.135
[46,     1] loss: 0.157
[47,     1] loss: 0.192
[48,     1] loss: 0.177
[49,     1] loss: 0.147
[50,     1] loss: 0.177
[51,     1] loss: 0.142
[52,     1] loss: 0.165
[53,     1] loss: 0.170
[54,     1] loss: 0.123
[55,     1] loss: 0.140
[56,     1] loss: 0.158
[57,     1] loss: 0.155
[58,     1] loss: 0.186
[59,     1] loss: 0.146
[60,     1] loss: 0.196
Early stopping applied (best metric=0.23598869144916534)
Finished Training
Total time taken: 7.307312250137329
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.642
[4,     1] loss: 0.592
[5,     1] loss: 0.530
[6,     1] loss: 0.520
[7,     1] loss: 0.452
[8,     1] loss: 0.353
[9,     1] loss: 0.351
[10,     1] loss: 0.369
[11,     1] loss: 0.250
[12,     1] loss: 0.241
[13,     1] loss: 0.249
[14,     1] loss: 0.220
[15,     1] loss: 0.210
[16,     1] loss: 0.236
[17,     1] loss: 0.215
[18,     1] loss: 0.202
[19,     1] loss: 0.217
[20,     1] loss: 0.180
[21,     1] loss: 0.231
[22,     1] loss: 0.187
[23,     1] loss: 0.219
[24,     1] loss: 0.194
[25,     1] loss: 0.181
[26,     1] loss: 0.181
[27,     1] loss: 0.164
[28,     1] loss: 0.160
[29,     1] loss: 0.200
[30,     1] loss: 0.203
[31,     1] loss: 0.184
[32,     1] loss: 0.185
[33,     1] loss: 0.181
[34,     1] loss: 0.239
[35,     1] loss: 0.145
[36,     1] loss: 0.167
[37,     1] loss: 0.131
[38,     1] loss: 0.205
[39,     1] loss: 0.180
[40,     1] loss: 0.170
[41,     1] loss: 0.244
[42,     1] loss: 0.195
[43,     1] loss: 0.152
[44,     1] loss: 0.137
[45,     1] loss: 0.209
[46,     1] loss: 0.170
[47,     1] loss: 0.119
[48,     1] loss: 0.174
[49,     1] loss: 0.177
[50,     1] loss: 0.151
[51,     1] loss: 0.128
[52,     1] loss: 0.171
[53,     1] loss: 0.150
[54,     1] loss: 0.147
[55,     1] loss: 0.145
[56,     1] loss: 0.085
[57,     1] loss: 0.147
Early stopping applied (best metric=0.32135209441185)
Finished Training
Total time taken: 6.93516731262207
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.697
[3,     1] loss: 0.670
[4,     1] loss: 0.647
[5,     1] loss: 0.572
[6,     1] loss: 0.500
[7,     1] loss: 0.443
[8,     1] loss: 0.417
[9,     1] loss: 0.349
[10,     1] loss: 0.348
[11,     1] loss: 0.312
[12,     1] loss: 0.254
[13,     1] loss: 0.323
[14,     1] loss: 0.255
[15,     1] loss: 0.272
[16,     1] loss: 0.275
[17,     1] loss: 0.250
[18,     1] loss: 0.255
[19,     1] loss: 0.207
[20,     1] loss: 0.225
[21,     1] loss: 0.199
[22,     1] loss: 0.234
[23,     1] loss: 0.253
[24,     1] loss: 0.252
[25,     1] loss: 0.158
[26,     1] loss: 0.231
[27,     1] loss: 0.210
[28,     1] loss: 0.218
[29,     1] loss: 0.209
[30,     1] loss: 0.223
[31,     1] loss: 0.178
[32,     1] loss: 0.179
[33,     1] loss: 0.184
[34,     1] loss: 0.166
[35,     1] loss: 0.118
[36,     1] loss: 0.170
[37,     1] loss: 0.183
[38,     1] loss: 0.139
[39,     1] loss: 0.134
[40,     1] loss: 0.173
[41,     1] loss: 0.161
[42,     1] loss: 0.142
[43,     1] loss: 0.204
[44,     1] loss: 0.162
[45,     1] loss: 0.124
[46,     1] loss: 0.195
[47,     1] loss: 0.266
[48,     1] loss: 0.213
[49,     1] loss: 0.176
[50,     1] loss: 0.156
[51,     1] loss: 0.166
[52,     1] loss: 0.161
[53,     1] loss: 0.144
[54,     1] loss: 0.211
[55,     1] loss: 0.146
Early stopping applied (best metric=0.45818960666656494)
Finished Training
Total time taken: 6.720840692520142
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.694
[3,     1] loss: 0.690
[4,     1] loss: 0.679
[5,     1] loss: 0.655
[6,     1] loss: 0.607
[7,     1] loss: 0.572
[8,     1] loss: 0.559
[9,     1] loss: 0.506
[10,     1] loss: 0.478
[11,     1] loss: 0.455
[12,     1] loss: 0.389
[13,     1] loss: 0.330
[14,     1] loss: 0.397
[15,     1] loss: 0.233
[16,     1] loss: 0.314
[17,     1] loss: 0.375
[18,     1] loss: 0.298
[19,     1] loss: 0.266
[20,     1] loss: 0.268
[21,     1] loss: 0.288
[22,     1] loss: 0.264
[23,     1] loss: 0.217
[24,     1] loss: 0.245
[25,     1] loss: 0.251
[26,     1] loss: 0.245
[27,     1] loss: 0.201
[28,     1] loss: 0.263
[29,     1] loss: 0.249
[30,     1] loss: 0.211
[31,     1] loss: 0.217
[32,     1] loss: 0.213
[33,     1] loss: 0.215
[34,     1] loss: 0.228
[35,     1] loss: 0.153
[36,     1] loss: 0.207
[37,     1] loss: 0.174
[38,     1] loss: 0.197
[39,     1] loss: 0.164
[40,     1] loss: 0.151
[41,     1] loss: 0.189
[42,     1] loss: 0.218
[43,     1] loss: 0.175
[44,     1] loss: 0.197
[45,     1] loss: 0.143
[46,     1] loss: 0.164
[47,     1] loss: 0.146
[48,     1] loss: 0.145
[49,     1] loss: 0.169
[50,     1] loss: 0.153
[51,     1] loss: 0.142
[52,     1] loss: 0.138
[53,     1] loss: 0.163
[54,     1] loss: 0.175
[55,     1] loss: 0.230
[56,     1] loss: 0.152
[57,     1] loss: 0.137
[58,     1] loss: 0.154
[59,     1] loss: 0.156
[60,     1] loss: 0.210
[61,     1] loss: 0.210
Early stopping applied (best metric=0.36962756514549255)
Finished Training
Total time taken: 7.427558183670044
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.697
[3,     1] loss: 0.683
[4,     1] loss: 0.670
[5,     1] loss: 0.644
[6,     1] loss: 0.612
[7,     1] loss: 0.554
[8,     1] loss: 0.506
[9,     1] loss: 0.490
[10,     1] loss: 0.436
[11,     1] loss: 0.418
[12,     1] loss: 0.470
[13,     1] loss: 0.422
[14,     1] loss: 0.361
[15,     1] loss: 0.280
[16,     1] loss: 0.291
[17,     1] loss: 0.256
[18,     1] loss: 0.277
[19,     1] loss: 0.279
[20,     1] loss: 0.292
[21,     1] loss: 0.216
[22,     1] loss: 0.261
[23,     1] loss: 0.217
[24,     1] loss: 0.242
[25,     1] loss: 0.207
[26,     1] loss: 0.168
[27,     1] loss: 0.229
[28,     1] loss: 0.236
[29,     1] loss: 0.285
[30,     1] loss: 0.175
[31,     1] loss: 0.257
[32,     1] loss: 0.211
[33,     1] loss: 0.187
[34,     1] loss: 0.227
[35,     1] loss: 0.195
[36,     1] loss: 0.245
[37,     1] loss: 0.137
[38,     1] loss: 0.172
[39,     1] loss: 0.188
[40,     1] loss: 0.209
[41,     1] loss: 0.176
[42,     1] loss: 0.190
[43,     1] loss: 0.206
[44,     1] loss: 0.164
[45,     1] loss: 0.242
[46,     1] loss: 0.137
[47,     1] loss: 0.171
[48,     1] loss: 0.143
[49,     1] loss: 0.175
[50,     1] loss: 0.117
[51,     1] loss: 0.155
[52,     1] loss: 0.204
[53,     1] loss: 0.157
[54,     1] loss: 0.158
[55,     1] loss: 0.161
[56,     1] loss: 0.140
Early stopping applied (best metric=0.45644962787628174)
Finished Training
Total time taken: 6.777579069137573
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.708
[3,     1] loss: 0.689
[4,     1] loss: 0.664
[5,     1] loss: 0.634
[6,     1] loss: 0.594
[7,     1] loss: 0.530
[8,     1] loss: 0.479
[9,     1] loss: 0.496
[10,     1] loss: 0.349
[11,     1] loss: 0.411
[12,     1] loss: 0.412
[13,     1] loss: 0.326
[14,     1] loss: 0.400
[15,     1] loss: 0.335
[16,     1] loss: 0.241
[17,     1] loss: 0.340
[18,     1] loss: 0.321
[19,     1] loss: 0.375
[20,     1] loss: 0.273
[21,     1] loss: 0.258
[22,     1] loss: 0.261
[23,     1] loss: 0.225
[24,     1] loss: 0.241
[25,     1] loss: 0.243
[26,     1] loss: 0.306
[27,     1] loss: 0.248
[28,     1] loss: 0.198
[29,     1] loss: 0.220
[30,     1] loss: 0.172
[31,     1] loss: 0.188
[32,     1] loss: 0.244
[33,     1] loss: 0.210
[34,     1] loss: 0.160
[35,     1] loss: 0.226
[36,     1] loss: 0.299
[37,     1] loss: 0.207
[38,     1] loss: 0.234
[39,     1] loss: 0.245
[40,     1] loss: 0.184
[41,     1] loss: 0.148
[42,     1] loss: 0.160
[43,     1] loss: 0.172
[44,     1] loss: 0.213
[45,     1] loss: 0.230
[46,     1] loss: 0.290
[47,     1] loss: 0.239
[48,     1] loss: 0.206
[49,     1] loss: 0.193
[50,     1] loss: 0.183
[51,     1] loss: 0.180
[52,     1] loss: 0.204
[53,     1] loss: 0.207
[54,     1] loss: 0.216
[55,     1] loss: 0.210
[56,     1] loss: 0.210
[57,     1] loss: 0.172
[58,     1] loss: 0.196
[59,     1] loss: 0.168
[60,     1] loss: 0.233
[61,     1] loss: 0.184
[62,     1] loss: 0.139
[63,     1] loss: 0.183
[64,     1] loss: 0.165
Early stopping applied (best metric=0.364618182182312)
Finished Training
Total time taken: 7.69838809967041
{'Hydroxylation-K Validation Accuracy': 0.8151418439716313, 'Hydroxylation-K Validation Sensitivity': 0.88, 'Hydroxylation-K Validation Specificity': 0.7989473684210526, 'Hydroxylation-K Validation Precision': 0.5435041846698193, 'Hydroxylation-K AUC ROC': 0.8728070175438597, 'Hydroxylation-K AUC PR': 0.6839880302697621, 'Hydroxylation-K MCC': 0.585805553241834, 'Hydroxylation-K F1': 0.6661968047732907, 'Validation Loss (Hydroxylation-K)': 0.3208111718297005, 'Validation Loss (total)': 0.3208111718297005, 'TimeToTrain': 9.660393257141113}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005796670477445612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3019300765,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.51647836748491}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.685
[3,     1] loss: 0.657
[4,     1] loss: 0.608
[5,     1] loss: 0.568
[6,     1] loss: 0.520
[7,     1] loss: 0.449
[8,     1] loss: 0.424
[9,     1] loss: 0.420
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025663895000869585,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4199526992,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.19912097713017}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.690
[3,     1] loss: 0.677
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005780790918779906,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1021343772,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.981076104088874}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.689
[4,     1] loss: 0.687
[5,     1] loss: 0.682
[6,     1] loss: 0.677
[7,     1] loss: 0.674
[8,     1] loss: 0.660
[9,     1] loss: 0.655
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009487593854656013,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3671137782,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.515209160208368}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.705
[3,     1] loss: 0.700
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0048202241735565855,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1811834611,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.076394529652309}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.644
[5,     1] loss: 0.602
[6,     1] loss: 0.558
[7,     1] loss: 0.530
[8,     1] loss: 0.473
[9,     1] loss: 0.438
[10,     1] loss: 0.457
[11,     1] loss: 0.409
[12,     1] loss: 0.328
[13,     1] loss: 0.396
[14,     1] loss: 0.387
[15,     1] loss: 0.315
[16,     1] loss: 0.376
[17,     1] loss: 0.280
[18,     1] loss: 0.344
[19,     1] loss: 0.273
[20,     1] loss: 0.372
[21,     1] loss: 0.295
[22,     1] loss: 0.347
[23,     1] loss: 0.313
[24,     1] loss: 0.288
[25,     1] loss: 0.293
[26,     1] loss: 0.305
[27,     1] loss: 0.304
[28,     1] loss: 0.243
[29,     1] loss: 0.286
[30,     1] loss: 0.236
[31,     1] loss: 0.242
[32,     1] loss: 0.225
[33,     1] loss: 0.324
[34,     1] loss: 0.216
[35,     1] loss: 0.233
[36,     1] loss: 0.215
[37,     1] loss: 0.298
[38,     1] loss: 0.149
[39,     1] loss: 0.256
[40,     1] loss: 0.215
[41,     1] loss: 0.257
[42,     1] loss: 0.245
[43,     1] loss: 0.187
[44,     1] loss: 0.212
[45,     1] loss: 0.177
[46,     1] loss: 0.184
[47,     1] loss: 0.190
[48,     1] loss: 0.185
[49,     1] loss: 0.170
[50,     1] loss: 0.191
[51,     1] loss: 0.151
[52,     1] loss: 0.149
[53,     1] loss: 0.124
[54,     1] loss: 0.191
[55,     1] loss: 0.108
[56,     1] loss: 0.226
[57,     1] loss: 0.135
[58,     1] loss: 0.150
[59,     1] loss: 0.213
[60,     1] loss: 0.109
[61,     1] loss: 0.144
[62,     1] loss: 0.177
[63,     1] loss: 0.167
[64,     1] loss: 0.220
[65,     1] loss: 0.135
[66,     1] loss: 0.188
[67,     1] loss: 0.168
[68,     1] loss: 0.148
[69,     1] loss: 0.100
[70,     1] loss: 0.141
[71,     1] loss: 0.140
[72,     1] loss: 0.145
[73,     1] loss: 0.125
[74,     1] loss: 0.106
[75,     1] loss: 0.125
[76,     1] loss: 0.108
[77,     1] loss: 0.096
[78,     1] loss: 0.071
[79,     1] loss: 0.057
[80,     1] loss: 0.180
[81,     1] loss: 0.073
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004054894673434169,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2425379737,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.234552453349685}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.680
[4,     1] loss: 0.659
[5,     1] loss: 0.630
[6,     1] loss: 0.606
[7,     1] loss: 0.569
[8,     1] loss: 0.535
[9,     1] loss: 0.471
[10,     1] loss: 0.449
[11,     1] loss: 0.393
[12,     1] loss: 0.415
[13,     1] loss: 0.353
[14,     1] loss: 0.317
[15,     1] loss: 0.286
[16,     1] loss: 0.239
[17,     1] loss: 0.308
[18,     1] loss: 0.257
[19,     1] loss: 0.206
[20,     1] loss: 0.204
[21,     1] loss: 0.155
[22,     1] loss: 0.223
[23,     1] loss: 0.210
[24,     1] loss: 0.106
[25,     1] loss: 0.127
[26,     1] loss: 0.106
[27,     1] loss: 0.097
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007652188550237044,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2310775277,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.6524719497965794}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.686
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0054362945774525375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2785183508,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.840256237221242}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005587687483693334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2107384044,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.5031988525455}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.692
[3,     1] loss: 0.665
[4,     1] loss: 0.621
[5,     1] loss: 0.572
[6,     1] loss: 0.517
[7,     1] loss: 0.466
[8,     1] loss: 0.430
[9,     1] loss: 0.377
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018217296933191155,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 332231079,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.586559071170495}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.675
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00273003784797859,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2483522345,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.280628019814737}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.676
[4,     1] loss: 0.656
[5,     1] loss: 0.623
[6,     1] loss: 0.621
[7,     1] loss: 0.580
[8,     1] loss: 0.543
[9,     1] loss: 0.546
[10,     1] loss: 0.496
[11,     1] loss: 0.458
[12,     1] loss: 0.452
[13,     1] loss: 0.401
[14,     1] loss: 0.385
[15,     1] loss: 0.369
[16,     1] loss: 0.405
[17,     1] loss: 0.318
[18,     1] loss: 0.304
[19,     1] loss: 0.305
[20,     1] loss: 0.348
[21,     1] loss: 0.329
[22,     1] loss: 0.259
[23,     1] loss: 0.287
[24,     1] loss: 0.311
[25,     1] loss: 0.242
[26,     1] loss: 0.261
[27,     1] loss: 0.234
[28,     1] loss: 0.207
[29,     1] loss: 0.235
[30,     1] loss: 0.174
[31,     1] loss: 0.186
[32,     1] loss: 0.193
[33,     1] loss: 0.246
[34,     1] loss: 0.202
[35,     1] loss: 0.210
[36,     1] loss: 0.184
[37,     1] loss: 0.179
[38,     1] loss: 0.156
[39,     1] loss: 0.152
[40,     1] loss: 0.191
[41,     1] loss: 0.226
[42,     1] loss: 0.140
[43,     1] loss: 0.199
[44,     1] loss: 0.147
[45,     1] loss: 0.135
[46,     1] loss: 0.156
[47,     1] loss: 0.168
[48,     1] loss: 0.142
[49,     1] loss: 0.170
[50,     1] loss: 0.167
[51,     1] loss: 0.105
[52,     1] loss: 0.149
[53,     1] loss: 0.156
[54,     1] loss: 0.153
[55,     1] loss: 0.146
[56,     1] loss: 0.120
[57,     1] loss: 0.178
[58,     1] loss: 0.138
[59,     1] loss: 0.201
[60,     1] loss: 0.131
[61,     1] loss: 0.163
[62,     1] loss: 0.133
[63,     1] loss: 0.149
[64,     1] loss: 0.136
[65,     1] loss: 0.108
[66,     1] loss: 0.082
[67,     1] loss: 0.173
[68,     1] loss: 0.129
[69,     1] loss: 0.141
[70,     1] loss: 0.122
[71,     1] loss: 0.099
[72,     1] loss: 0.087
[73,     1] loss: 0.105
[74,     1] loss: 0.073
[75,     1] loss: 0.082
[76,     1] loss: 0.056
[77,     1] loss: 0.123
[78,     1] loss: 0.163
[79,     1] loss: 0.186
[80,     1] loss: 0.054
[81,     1] loss: 0.097
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00797481765003507,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1458375871,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 18.216722041270295}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002007140963897817,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4004212852,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.482146952944857}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.666
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036097875471958795,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 191159079,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.742438344820316}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.689
[3,     1] loss: 0.669
[4,     1] loss: 0.641
[5,     1] loss: 0.595
[6,     1] loss: 0.539
[7,     1] loss: 0.522
[8,     1] loss: 0.480
[9,     1] loss: 0.450
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031453276999169628,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3572579174,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.726412020026046}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.675
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021204358372699807,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3166185046,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.56898011553774}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.679
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001255469175175011,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1126629126,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.551388300672041}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.693
[3,     1] loss: 0.682
[4,     1] loss: 0.669
[5,     1] loss: 0.658
[6,     1] loss: 0.630
[7,     1] loss: 0.601
[8,     1] loss: 0.598
[9,     1] loss: 0.581
[10,     1] loss: 0.574
[11,     1] loss: 0.553
[12,     1] loss: 0.534
[13,     1] loss: 0.543
[14,     1] loss: 0.520
[15,     1] loss: 0.516
[16,     1] loss: 0.452
[17,     1] loss: 0.472
[18,     1] loss: 0.435
[19,     1] loss: 0.439
[20,     1] loss: 0.431
[21,     1] loss: 0.401
[22,     1] loss: 0.385
[23,     1] loss: 0.373
[24,     1] loss: 0.368
[25,     1] loss: 0.364
[26,     1] loss: 0.346
[27,     1] loss: 0.369
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006781703550352606,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 296544700,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.553283938559135}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.697
[3,     1] loss: 0.674
[4,     1] loss: 0.652
[5,     1] loss: 0.618
[6,     1] loss: 0.560
[7,     1] loss: 0.501
[8,     1] loss: 0.471
[9,     1] loss: 0.386
[10,     1] loss: 0.376
[11,     1] loss: 0.381
[12,     1] loss: 0.397
[13,     1] loss: 0.337
[14,     1] loss: 0.458
[15,     1] loss: 0.278
[16,     1] loss: 0.316
[17,     1] loss: 0.285
[18,     1] loss: 0.301
[19,     1] loss: 0.317
[20,     1] loss: 0.284
[21,     1] loss: 0.297
[22,     1] loss: 0.279
[23,     1] loss: 0.251
[24,     1] loss: 0.236
[25,     1] loss: 0.250
[26,     1] loss: 0.235
[27,     1] loss: 0.236
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009689516222416789,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 565082335,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.653808250393299}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.662
[4,     1] loss: 0.653
[5,     1] loss: 0.585
[6,     1] loss: 0.535
[7,     1] loss: 0.470
[8,     1] loss: 0.482
[9,     1] loss: 0.487
[10,     1] loss: 0.432
[11,     1] loss: 0.444
[12,     1] loss: 0.429
[13,     1] loss: 0.378
[14,     1] loss: 0.365
[15,     1] loss: 0.411
[16,     1] loss: 0.342
[17,     1] loss: 0.384
[18,     1] loss: 0.370
[19,     1] loss: 0.353
[20,     1] loss: 0.389
[21,     1] loss: 0.406
[22,     1] loss: 0.339
[23,     1] loss: 0.401
[24,     1] loss: 0.375
[25,     1] loss: 0.388
[26,     1] loss: 0.366
[27,     1] loss: 0.370
[28,     1] loss: 0.327
[29,     1] loss: 0.312
[30,     1] loss: 0.298
[31,     1] loss: 0.309
[32,     1] loss: 0.282
[33,     1] loss: 0.322
[34,     1] loss: 0.301
[35,     1] loss: 0.336
[36,     1] loss: 0.361
[37,     1] loss: 0.672
[38,     1] loss: 0.485
[39,     1] loss: 0.419
[40,     1] loss: 0.390
[41,     1] loss: 0.423
[42,     1] loss: 0.405
[43,     1] loss: 0.372
[44,     1] loss: 0.367
[45,     1] loss: 0.365
[46,     1] loss: 0.334
[47,     1] loss: 0.334
[48,     1] loss: 0.322
[49,     1] loss: 0.291
[50,     1] loss: 0.338
[51,     1] loss: 0.295
[52,     1] loss: 0.255
[53,     1] loss: 0.249
[54,     1] loss: 0.312
[55,     1] loss: 0.288
[56,     1] loss: 0.283
[57,     1] loss: 0.328
[58,     1] loss: 0.312
[59,     1] loss: 0.279
[60,     1] loss: 0.292
[61,     1] loss: 0.392
[62,     1] loss: 0.377
[63,     1] loss: 0.352
[64,     1] loss: 0.318
[65,     1] loss: 0.366
[66,     1] loss: 0.308
[67,     1] loss: 0.326
[68,     1] loss: 0.362
[69,     1] loss: 0.337
[70,     1] loss: 0.372
[71,     1] loss: 0.331
[72,     1] loss: 0.292
[73,     1] loss: 0.311
[74,     1] loss: 0.302
[75,     1] loss: 0.306
[76,     1] loss: 0.292
[77,     1] loss: 0.291
[78,     1] loss: 0.277
[79,     1] loss: 0.340
[80,     1] loss: 0.273
[81,     1] loss: 0.293
[82,     1] loss: 0.328
[83,     1] loss: 0.279
[84,     1] loss: 0.274
[85,     1] loss: 0.324
Early stopping applied (best metric=0.2539871335029602)
Finished Training
Total time taken: 10.330312013626099
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.693
[3,     1] loss: 0.668
[4,     1] loss: 0.637
[5,     1] loss: 0.573
[6,     1] loss: 0.551
[7,     1] loss: 0.506
[8,     1] loss: 0.493
[9,     1] loss: 0.530
[10,     1] loss: 0.457
[11,     1] loss: 0.441
[12,     1] loss: 0.510
[13,     1] loss: 0.428
[14,     1] loss: 0.451
[15,     1] loss: 0.437
[16,     1] loss: 0.347
[17,     1] loss: 0.384
[18,     1] loss: 0.427
[19,     1] loss: 0.343
[20,     1] loss: 0.372
[21,     1] loss: 0.320
[22,     1] loss: 0.326
[23,     1] loss: 0.366
[24,     1] loss: 0.295
[25,     1] loss: 0.360
[26,     1] loss: 0.318
[27,     1] loss: 0.344
[28,     1] loss: 0.315
[29,     1] loss: 0.276
[30,     1] loss: 0.268
[31,     1] loss: 0.303
[32,     1] loss: 0.292
[33,     1] loss: 0.303
[34,     1] loss: 0.283
[35,     1] loss: 0.280
[36,     1] loss: 0.305
[37,     1] loss: 0.414
[38,     1] loss: 0.629
[39,     1] loss: 0.298
[40,     1] loss: 0.430
[41,     1] loss: 0.353
[42,     1] loss: 0.394
[43,     1] loss: 0.372
[44,     1] loss: 0.328
[45,     1] loss: 0.357
[46,     1] loss: 0.332
[47,     1] loss: 0.294
[48,     1] loss: 0.343
[49,     1] loss: 0.263
[50,     1] loss: 0.292
[51,     1] loss: 0.266
[52,     1] loss: 0.272
[53,     1] loss: 0.260
[54,     1] loss: 0.381
[55,     1] loss: 0.545
[56,     1] loss: 0.337
[57,     1] loss: 0.394
[58,     1] loss: 0.333
[59,     1] loss: 0.375
[60,     1] loss: 0.355
[61,     1] loss: 0.393
[62,     1] loss: 0.346
[63,     1] loss: 0.326
[64,     1] loss: 0.335
[65,     1] loss: 0.342
[66,     1] loss: 0.304
[67,     1] loss: 0.307
[68,     1] loss: 0.302
Early stopping applied (best metric=0.3086665868759155)
Finished Training
Total time taken: 8.240020275115967
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.692
[3,     1] loss: 0.679
[4,     1] loss: 0.669
[5,     1] loss: 0.632
[6,     1] loss: 0.596
[7,     1] loss: 0.572
[8,     1] loss: 0.532
[9,     1] loss: 0.493
[10,     1] loss: 0.470
[11,     1] loss: 0.443
[12,     1] loss: 0.416
[13,     1] loss: 0.362
[14,     1] loss: 0.410
[15,     1] loss: 0.371
[16,     1] loss: 0.358
[17,     1] loss: 0.404
[18,     1] loss: 0.337
[19,     1] loss: 0.292
[20,     1] loss: 0.303
[21,     1] loss: 0.351
[22,     1] loss: 0.388
[23,     1] loss: 0.625
[24,     1] loss: 0.356
[25,     1] loss: 0.370
[26,     1] loss: 0.368
[27,     1] loss: 0.371
[28,     1] loss: 0.348
[29,     1] loss: 0.338
[30,     1] loss: 0.369
[31,     1] loss: 0.365
[32,     1] loss: 0.317
[33,     1] loss: 0.294
[34,     1] loss: 0.292
[35,     1] loss: 0.320
[36,     1] loss: 0.348
[37,     1] loss: 0.318
[38,     1] loss: 0.299
[39,     1] loss: 0.235
[40,     1] loss: 0.268
[41,     1] loss: 0.244
[42,     1] loss: 0.358
[43,     1] loss: 0.648
[44,     1] loss: 0.353
[45,     1] loss: 0.493
[46,     1] loss: 0.399
[47,     1] loss: 0.457
[48,     1] loss: 0.428
[49,     1] loss: 0.404
[50,     1] loss: 0.399
[51,     1] loss: 0.364
[52,     1] loss: 0.359
[53,     1] loss: 0.367
[54,     1] loss: 0.311
[55,     1] loss: 0.350
[56,     1] loss: 0.299
[57,     1] loss: 0.302
[58,     1] loss: 0.307
[59,     1] loss: 0.325
[60,     1] loss: 0.287
[61,     1] loss: 0.302
[62,     1] loss: 0.242
[63,     1] loss: 0.282
Early stopping applied (best metric=0.43080878257751465)
Finished Training
Total time taken: 7.594317674636841
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.692
[3,     1] loss: 0.691
[4,     1] loss: 0.681
[5,     1] loss: 0.658
[6,     1] loss: 0.627
[7,     1] loss: 0.605
[8,     1] loss: 0.583
[9,     1] loss: 0.534
[10,     1] loss: 0.511
[11,     1] loss: 0.495
[12,     1] loss: 0.450
[13,     1] loss: 0.470
[14,     1] loss: 0.471
[15,     1] loss: 0.423
[16,     1] loss: 0.471
[17,     1] loss: 0.437
[18,     1] loss: 0.400
[19,     1] loss: 0.453
[20,     1] loss: 0.401
[21,     1] loss: 0.375
[22,     1] loss: 0.439
[23,     1] loss: 0.405
[24,     1] loss: 0.394
[25,     1] loss: 0.407
[26,     1] loss: 0.361
[27,     1] loss: 0.346
[28,     1] loss: 0.366
[29,     1] loss: 0.368
[30,     1] loss: 0.359
[31,     1] loss: 0.350
[32,     1] loss: 0.388
[33,     1] loss: 0.328
[34,     1] loss: 0.403
[35,     1] loss: 0.404
[36,     1] loss: 0.358
[37,     1] loss: 0.408
[38,     1] loss: 0.341
[39,     1] loss: 0.284
[40,     1] loss: 0.281
[41,     1] loss: 0.262
[42,     1] loss: 0.283
[43,     1] loss: 0.297
[44,     1] loss: 0.256
[45,     1] loss: 0.452
[46,     1] loss: 0.480
[47,     1] loss: 0.454
[48,     1] loss: 0.405
[49,     1] loss: 0.396
[50,     1] loss: 0.347
[51,     1] loss: 0.391
[52,     1] loss: 0.360
[53,     1] loss: 0.359
[54,     1] loss: 0.380
[55,     1] loss: 0.333
[56,     1] loss: 0.317
[57,     1] loss: 0.297
[58,     1] loss: 0.318
[59,     1] loss: 0.326
[60,     1] loss: 0.293
[61,     1] loss: 0.300
[62,     1] loss: 0.313
[63,     1] loss: 0.325
[64,     1] loss: 0.299
[65,     1] loss: 0.314
[66,     1] loss: 0.354
[67,     1] loss: 0.582
[68,     1] loss: 0.436
[69,     1] loss: 0.403
[70,     1] loss: 0.391
[71,     1] loss: 0.392
[72,     1] loss: 0.384
[73,     1] loss: 0.360
[74,     1] loss: 0.315
Early stopping applied (best metric=0.35766366124153137)
Finished Training
Total time taken: 8.919818639755249
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.696
[3,     1] loss: 0.686
[4,     1] loss: 0.666
[5,     1] loss: 0.641
[6,     1] loss: 0.617
[7,     1] loss: 0.584
[8,     1] loss: 0.528
[9,     1] loss: 0.475
[10,     1] loss: 0.477
[11,     1] loss: 0.444
[12,     1] loss: 0.388
[13,     1] loss: 0.402
[14,     1] loss: 0.343
[15,     1] loss: 0.389
[16,     1] loss: 0.356
[17,     1] loss: 0.351
[18,     1] loss: 0.304
[19,     1] loss: 0.393
[20,     1] loss: 0.321
[21,     1] loss: 0.393
[22,     1] loss: 0.401
[23,     1] loss: 0.353
[24,     1] loss: 0.404
[25,     1] loss: 0.350
[26,     1] loss: 0.325
[27,     1] loss: 0.355
[28,     1] loss: 0.333
[29,     1] loss: 0.315
[30,     1] loss: 0.380
[31,     1] loss: 0.339
[32,     1] loss: 0.379
[33,     1] loss: 0.525
[34,     1] loss: 0.406
[35,     1] loss: 0.363
[36,     1] loss: 0.350
[37,     1] loss: 0.373
[38,     1] loss: 0.370
[39,     1] loss: 0.331
[40,     1] loss: 0.330
[41,     1] loss: 0.350
[42,     1] loss: 0.302
[43,     1] loss: 0.299
[44,     1] loss: 0.290
[45,     1] loss: 0.270
[46,     1] loss: 0.321
[47,     1] loss: 0.350
[48,     1] loss: 0.249
[49,     1] loss: 0.224
[50,     1] loss: 0.293
[51,     1] loss: 0.275
[52,     1] loss: 0.317
[53,     1] loss: 0.276
[54,     1] loss: 0.324
[55,     1] loss: 0.295
[56,     1] loss: 0.317
[57,     1] loss: 0.287
[58,     1] loss: 0.350
[59,     1] loss: 0.590
[60,     1] loss: 0.416
[61,     1] loss: 0.423
[62,     1] loss: 0.456
[63,     1] loss: 0.395
[64,     1] loss: 0.437
[65,     1] loss: 0.364
[66,     1] loss: 0.323
[67,     1] loss: 0.339
[68,     1] loss: 0.288
[69,     1] loss: 0.299
[70,     1] loss: 0.302
[71,     1] loss: 0.245
[72,     1] loss: 0.224
[73,     1] loss: 0.240
[74,     1] loss: 0.212
[75,     1] loss: 0.184
[76,     1] loss: 0.226
[77,     1] loss: 0.226
[78,     1] loss: 0.175
[79,     1] loss: 0.192
[80,     1] loss: 0.739
[81,     1] loss: 1.176
[82,     1] loss: 0.562
[83,     1] loss: 0.520
[84,     1] loss: 0.533
[85,     1] loss: 0.547
[86,     1] loss: 0.572
[87,     1] loss: 0.568
[88,     1] loss: 0.542
[89,     1] loss: 0.542
[90,     1] loss: 0.553
[91,     1] loss: 0.539
[92,     1] loss: 0.501
[93,     1] loss: 0.476
[94,     1] loss: 0.469
[95,     1] loss: 0.427
[96,     1] loss: 0.398
[97,     1] loss: 0.342
[98,     1] loss: 0.366
[99,     1] loss: 0.369
[100,     1] loss: 0.307
[101,     1] loss: 0.294
[102,     1] loss: 0.248
[103,     1] loss: 0.270
[104,     1] loss: 0.298
[105,     1] loss: 0.314
[106,     1] loss: 0.332
[107,     1] loss: 0.301
[108,     1] loss: 0.319
[109,     1] loss: 0.334
[110,     1] loss: 0.313
[111,     1] loss: 0.307
[112,     1] loss: 0.286
[113,     1] loss: 0.351
[114,     1] loss: 0.403
[115,     1] loss: 0.409
[116,     1] loss: 0.354
[117,     1] loss: 0.351
[118,     1] loss: 0.349
[119,     1] loss: 0.404
[120,     1] loss: 0.343
[121,     1] loss: 0.306
[122,     1] loss: 0.297
[123,     1] loss: 0.307
[124,     1] loss: 0.294
[125,     1] loss: 0.301
[126,     1] loss: 0.273
[127,     1] loss: 0.275
[128,     1] loss: 0.277
[129,     1] loss: 0.274
[130,     1] loss: 0.274
[131,     1] loss: 0.226
[132,     1] loss: 0.250
[133,     1] loss: 0.268
[134,     1] loss: 0.274
[135,     1] loss: 0.501
[136,     1] loss: 0.685
[137,     1] loss: 0.360
[138,     1] loss: 0.411
[139,     1] loss: 0.441
[140,     1] loss: 0.384
[141,     1] loss: 0.381
[142,     1] loss: 0.410
[143,     1] loss: 0.397
[144,     1] loss: 0.372
[145,     1] loss: 0.340
[146,     1] loss: 0.333
[147,     1] loss: 0.345
[148,     1] loss: 0.312
Early stopping applied (best metric=0.36225929856300354)
Finished Training
Total time taken: 17.863259077072144
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.690
[3,     1] loss: 0.681
[4,     1] loss: 0.664
[5,     1] loss: 0.612
[6,     1] loss: 0.596
[7,     1] loss: 0.561
[8,     1] loss: 0.518
[9,     1] loss: 0.464
[10,     1] loss: 0.437
[11,     1] loss: 0.435
[12,     1] loss: 0.401
[13,     1] loss: 0.539
[14,     1] loss: 0.342
[15,     1] loss: 0.394
[16,     1] loss: 0.411
[17,     1] loss: 0.448
[18,     1] loss: 0.367
[19,     1] loss: 0.354
[20,     1] loss: 0.354
[21,     1] loss: 0.403
[22,     1] loss: 0.441
[23,     1] loss: 0.375
[24,     1] loss: 0.358
[25,     1] loss: 0.351
[26,     1] loss: 0.333
[27,     1] loss: 0.300
[28,     1] loss: 0.280
[29,     1] loss: 0.288
[30,     1] loss: 0.249
[31,     1] loss: 0.237
[32,     1] loss: 0.254
[33,     1] loss: 0.428
[34,     1] loss: 0.349
[35,     1] loss: 0.289
[36,     1] loss: 0.332
[37,     1] loss: 0.348
[38,     1] loss: 0.273
[39,     1] loss: 0.303
[40,     1] loss: 0.256
[41,     1] loss: 0.273
[42,     1] loss: 0.269
[43,     1] loss: 0.467
[44,     1] loss: 0.316
[45,     1] loss: 0.405
[46,     1] loss: 0.331
[47,     1] loss: 0.413
[48,     1] loss: 0.339
[49,     1] loss: 0.342
[50,     1] loss: 0.315
[51,     1] loss: 0.323
[52,     1] loss: 0.336
[53,     1] loss: 0.289
[54,     1] loss: 0.290
[55,     1] loss: 0.269
[56,     1] loss: 0.241
[57,     1] loss: 0.265
[58,     1] loss: 0.245
[59,     1] loss: 0.369
[60,     1] loss: 0.404
[61,     1] loss: 0.307
[62,     1] loss: 0.312
[63,     1] loss: 0.322
[64,     1] loss: 0.355
[65,     1] loss: 0.326
Early stopping applied (best metric=0.3012198805809021)
Finished Training
Total time taken: 7.868360757827759
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.685
[2,     1] loss: 0.723
[3,     1] loss: 0.689
[4,     1] loss: 0.687
[5,     1] loss: 0.679
[6,     1] loss: 0.660
[7,     1] loss: 0.640
[8,     1] loss: 0.600
[9,     1] loss: 0.574
[10,     1] loss: 0.545
[11,     1] loss: 0.522
[12,     1] loss: 0.458
[13,     1] loss: 0.425
[14,     1] loss: 0.444
[15,     1] loss: 0.421
[16,     1] loss: 0.434
[17,     1] loss: 0.537
[18,     1] loss: 0.395
[19,     1] loss: 0.442
[20,     1] loss: 0.447
[21,     1] loss: 0.423
[22,     1] loss: 0.391
[23,     1] loss: 0.343
[24,     1] loss: 0.503
[25,     1] loss: 0.409
[26,     1] loss: 0.429
[27,     1] loss: 0.398
[28,     1] loss: 0.383
[29,     1] loss: 0.383
[30,     1] loss: 0.363
[31,     1] loss: 0.385
[32,     1] loss: 0.329
[33,     1] loss: 0.361
[34,     1] loss: 0.307
[35,     1] loss: 0.305
[36,     1] loss: 0.319
[37,     1] loss: 0.350
[38,     1] loss: 0.322
[39,     1] loss: 0.434
[40,     1] loss: 0.345
[41,     1] loss: 0.311
[42,     1] loss: 0.289
[43,     1] loss: 0.286
[44,     1] loss: 0.383
[45,     1] loss: 0.349
[46,     1] loss: 0.385
[47,     1] loss: 0.413
[48,     1] loss: 0.328
[49,     1] loss: 0.385
[50,     1] loss: 0.336
[51,     1] loss: 0.398
[52,     1] loss: 0.304
[53,     1] loss: 0.332
[54,     1] loss: 0.309
[55,     1] loss: 0.311
[56,     1] loss: 0.343
[57,     1] loss: 0.291
[58,     1] loss: 0.304
[59,     1] loss: 0.354
[60,     1] loss: 0.419
[61,     1] loss: 0.353
[62,     1] loss: 0.371
[63,     1] loss: 0.446
[64,     1] loss: 0.472
[65,     1] loss: 0.369
[66,     1] loss: 0.355
[67,     1] loss: 0.363
[68,     1] loss: 0.325
[69,     1] loss: 0.341
[70,     1] loss: 0.330
[71,     1] loss: 0.308
[72,     1] loss: 0.303
[73,     1] loss: 0.289
[74,     1] loss: 0.291
[75,     1] loss: 0.221
[76,     1] loss: 0.278
[77,     1] loss: 0.258
[78,     1] loss: 0.535
[79,     1] loss: 0.987
[80,     1] loss: 0.421
[81,     1] loss: 0.465
[82,     1] loss: 0.502
[83,     1] loss: 0.462
[84,     1] loss: 0.493
[85,     1] loss: 0.449
[86,     1] loss: 0.467
[87,     1] loss: 0.421
[88,     1] loss: 0.439
[89,     1] loss: 0.404
[90,     1] loss: 0.375
[91,     1] loss: 0.365
[92,     1] loss: 0.313
[93,     1] loss: 0.320
[94,     1] loss: 0.321
[95,     1] loss: 0.332
[96,     1] loss: 0.419
[97,     1] loss: 0.616
[98,     1] loss: 0.370
[99,     1] loss: 0.366
[100,     1] loss: 0.400
[101,     1] loss: 0.444
[102,     1] loss: 0.356
[103,     1] loss: 0.406
[104,     1] loss: 0.341
[105,     1] loss: 0.345
[106,     1] loss: 0.359
[107,     1] loss: 0.342
[108,     1] loss: 0.325
[109,     1] loss: 0.300
[110,     1] loss: 0.268
[111,     1] loss: 0.265
[112,     1] loss: 0.282
[113,     1] loss: 0.282
[114,     1] loss: 0.288
[115,     1] loss: 0.425
[116,     1] loss: 0.727
[117,     1] loss: 0.452
[118,     1] loss: 0.452
[119,     1] loss: 0.472
[120,     1] loss: 0.480
[121,     1] loss: 0.441
Early stopping applied (best metric=0.24182598292827606)
Finished Training
Total time taken: 14.563687562942505
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.697
[3,     1] loss: 0.684
[4,     1] loss: 0.671
[5,     1] loss: 0.646
[6,     1] loss: 0.631
[7,     1] loss: 0.600
[8,     1] loss: 0.568
[9,     1] loss: 0.531
[10,     1] loss: 0.502
[11,     1] loss: 0.463
[12,     1] loss: 0.517
[13,     1] loss: 0.446
[14,     1] loss: 0.500
[15,     1] loss: 0.449
[16,     1] loss: 0.407
[17,     1] loss: 0.415
[18,     1] loss: 0.387
[19,     1] loss: 0.412
[20,     1] loss: 0.389
[21,     1] loss: 0.399
[22,     1] loss: 0.376
[23,     1] loss: 0.369
[24,     1] loss: 0.369
[25,     1] loss: 0.347
[26,     1] loss: 0.357
[27,     1] loss: 0.367
[28,     1] loss: 0.432
[29,     1] loss: 0.353
[30,     1] loss: 0.337
[31,     1] loss: 0.309
[32,     1] loss: 0.348
[33,     1] loss: 0.326
[34,     1] loss: 0.297
[35,     1] loss: 0.311
[36,     1] loss: 0.274
[37,     1] loss: 0.304
[38,     1] loss: 0.284
[39,     1] loss: 0.278
[40,     1] loss: 0.310
[41,     1] loss: 0.306
[42,     1] loss: 0.387
[43,     1] loss: 0.291
[44,     1] loss: 0.302
[45,     1] loss: 0.346
[46,     1] loss: 0.311
[47,     1] loss: 0.352
[48,     1] loss: 0.310
[49,     1] loss: 0.290
[50,     1] loss: 0.366
[51,     1] loss: 0.377
[52,     1] loss: 0.341
[53,     1] loss: 0.397
[54,     1] loss: 0.408
[55,     1] loss: 0.478
[56,     1] loss: 0.344
[57,     1] loss: 0.315
[58,     1] loss: 0.313
[59,     1] loss: 0.307
[60,     1] loss: 0.311
[61,     1] loss: 0.351
[62,     1] loss: 0.312
[63,     1] loss: 0.303
[64,     1] loss: 0.343
[65,     1] loss: 0.290
[66,     1] loss: 0.289
[67,     1] loss: 0.309
[68,     1] loss: 0.301
[69,     1] loss: 0.406
[70,     1] loss: 0.297
[71,     1] loss: 0.349
[72,     1] loss: 0.317
[73,     1] loss: 0.279
[74,     1] loss: 0.325
[75,     1] loss: 0.305
[76,     1] loss: 0.289
[77,     1] loss: 0.292
[78,     1] loss: 0.289
[79,     1] loss: 0.266
[80,     1] loss: 0.239
[81,     1] loss: 0.287
[82,     1] loss: 0.282
[83,     1] loss: 0.306
[84,     1] loss: 0.652
[85,     1] loss: 0.588
[86,     1] loss: 0.348
[87,     1] loss: 0.382
[88,     1] loss: 0.419
[89,     1] loss: 0.373
[90,     1] loss: 0.395
[91,     1] loss: 0.366
[92,     1] loss: 0.366
[93,     1] loss: 0.310
[94,     1] loss: 0.332
[95,     1] loss: 0.343
[96,     1] loss: 0.323
[97,     1] loss: 0.305
[98,     1] loss: 0.317
[99,     1] loss: 0.299
[100,     1] loss: 0.268
[101,     1] loss: 0.275
[102,     1] loss: 0.278
[103,     1] loss: 0.319
[104,     1] loss: 0.271
[105,     1] loss: 0.222
[106,     1] loss: 0.247
[107,     1] loss: 0.291
[108,     1] loss: 0.255
[109,     1] loss: 0.390
[110,     1] loss: 0.424
[111,     1] loss: 0.255
[112,     1] loss: 0.333
[113,     1] loss: 0.346
[114,     1] loss: 0.311
[115,     1] loss: 0.312
[116,     1] loss: 0.347
[117,     1] loss: 0.268
[118,     1] loss: 0.294
[119,     1] loss: 0.268
[120,     1] loss: 0.279
[121,     1] loss: 0.325
[122,     1] loss: 0.896
[123,     1] loss: 0.509
[124,     1] loss: 0.421
[125,     1] loss: 0.430
[126,     1] loss: 0.415
[127,     1] loss: 0.421
[128,     1] loss: 0.387
[129,     1] loss: 0.389
[130,     1] loss: 0.371
[131,     1] loss: 0.358
[132,     1] loss: 0.349
[133,     1] loss: 0.319
[134,     1] loss: 0.302
[135,     1] loss: 0.291
[136,     1] loss: 0.306
[137,     1] loss: 0.358
[138,     1] loss: 0.293
[139,     1] loss: 0.686
[140,     1] loss: 0.841
[141,     1] loss: 0.608
[142,     1] loss: 0.611
[143,     1] loss: 0.567
Early stopping applied (best metric=0.2932649552822113)
Finished Training
Total time taken: 17.231842756271362
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.692
[3,     1] loss: 0.686
[4,     1] loss: 0.662
[5,     1] loss: 0.633
[6,     1] loss: 0.592
[7,     1] loss: 0.535
[8,     1] loss: 0.490
[9,     1] loss: 0.477
[10,     1] loss: 0.490
[11,     1] loss: 0.420
[12,     1] loss: 0.406
[13,     1] loss: 0.396
[14,     1] loss: 0.386
[15,     1] loss: 0.387
[16,     1] loss: 0.368
[17,     1] loss: 0.384
[18,     1] loss: 0.367
[19,     1] loss: 0.362
[20,     1] loss: 0.319
[21,     1] loss: 0.310
[22,     1] loss: 0.365
[23,     1] loss: 0.307
[24,     1] loss: 0.323
[25,     1] loss: 0.307
[26,     1] loss: 0.324
[27,     1] loss: 0.329
[28,     1] loss: 0.354
[29,     1] loss: 0.436
[30,     1] loss: 0.324
[31,     1] loss: 0.428
[32,     1] loss: 0.325
[33,     1] loss: 0.381
[34,     1] loss: 0.339
[35,     1] loss: 0.394
[36,     1] loss: 0.311
[37,     1] loss: 0.395
[38,     1] loss: 0.354
[39,     1] loss: 0.291
[40,     1] loss: 0.380
[41,     1] loss: 0.271
[42,     1] loss: 0.273
[43,     1] loss: 0.291
[44,     1] loss: 0.277
[45,     1] loss: 0.276
[46,     1] loss: 0.241
[47,     1] loss: 0.272
[48,     1] loss: 0.253
[49,     1] loss: 0.267
[50,     1] loss: 0.406
[51,     1] loss: 0.246
[52,     1] loss: 0.362
[53,     1] loss: 0.320
[54,     1] loss: 0.302
[55,     1] loss: 0.313
[56,     1] loss: 0.284
[57,     1] loss: 0.319
[58,     1] loss: 0.270
[59,     1] loss: 0.303
[60,     1] loss: 0.265
[61,     1] loss: 0.248
[62,     1] loss: 0.260
[63,     1] loss: 0.311
[64,     1] loss: 0.234
[65,     1] loss: 0.243
[66,     1] loss: 0.272
[67,     1] loss: 0.224
[68,     1] loss: 0.219
[69,     1] loss: 0.233
[70,     1] loss: 0.248
[71,     1] loss: 0.240
[72,     1] loss: 0.351
[73,     1] loss: 0.846
[74,     1] loss: 0.455
[75,     1] loss: 0.455
[76,     1] loss: 0.442
[77,     1] loss: 0.422
[78,     1] loss: 0.427
[79,     1] loss: 0.404
[80,     1] loss: 0.423
[81,     1] loss: 0.379
[82,     1] loss: 0.351
[83,     1] loss: 0.326
[84,     1] loss: 0.316
[85,     1] loss: 0.310
[86,     1] loss: 0.279
[87,     1] loss: 0.286
[88,     1] loss: 0.256
[89,     1] loss: 0.252
[90,     1] loss: 0.255
[91,     1] loss: 0.257
[92,     1] loss: 0.246
[93,     1] loss: 0.239
[94,     1] loss: 0.229
[95,     1] loss: 0.203
[96,     1] loss: 0.300
[97,     1] loss: 0.259
[98,     1] loss: 0.253
[99,     1] loss: 0.286
[100,     1] loss: 0.232
[101,     1] loss: 0.247
[102,     1] loss: 0.266
[103,     1] loss: 0.226
[104,     1] loss: 0.227
[105,     1] loss: 0.316
[106,     1] loss: 0.546
[107,     1] loss: 0.340
[108,     1] loss: 0.412
[109,     1] loss: 0.309
[110,     1] loss: 0.357
[111,     1] loss: 0.395
Early stopping applied (best metric=0.30826690793037415)
Finished Training
Total time taken: 13.305846691131592
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.682
[4,     1] loss: 0.684
[5,     1] loss: 0.664
[6,     1] loss: 0.628
[7,     1] loss: 0.587
[8,     1] loss: 0.553
[9,     1] loss: 0.508
[10,     1] loss: 0.468
[11,     1] loss: 0.453
[12,     1] loss: 0.404
[13,     1] loss: 0.395
[14,     1] loss: 0.324
[15,     1] loss: 0.317
[16,     1] loss: 0.312
[17,     1] loss: 0.327
[18,     1] loss: 0.260
[19,     1] loss: 0.281
[20,     1] loss: 0.277
[21,     1] loss: 0.276
[22,     1] loss: 0.258
[23,     1] loss: 0.380
[24,     1] loss: 0.827
[25,     1] loss: 0.374
[26,     1] loss: 0.419
[27,     1] loss: 0.451
[28,     1] loss: 0.429
[29,     1] loss: 0.409
[30,     1] loss: 0.395
[31,     1] loss: 0.399
[32,     1] loss: 0.397
[33,     1] loss: 0.371
[34,     1] loss: 0.372
[35,     1] loss: 0.349
[36,     1] loss: 0.337
[37,     1] loss: 0.302
[38,     1] loss: 0.327
[39,     1] loss: 0.322
[40,     1] loss: 0.256
[41,     1] loss: 0.277
[42,     1] loss: 0.270
[43,     1] loss: 0.261
[44,     1] loss: 0.279
[45,     1] loss: 0.315
[46,     1] loss: 0.289
[47,     1] loss: 0.281
[48,     1] loss: 0.286
[49,     1] loss: 0.291
[50,     1] loss: 0.289
[51,     1] loss: 0.230
[52,     1] loss: 0.268
[53,     1] loss: 0.312
[54,     1] loss: 0.247
[55,     1] loss: 0.344
[56,     1] loss: 0.616
[57,     1] loss: 0.290
[58,     1] loss: 0.316
[59,     1] loss: 0.334
[60,     1] loss: 0.302
[61,     1] loss: 0.292
[62,     1] loss: 0.308
[63,     1] loss: 0.280
[64,     1] loss: 0.296
[65,     1] loss: 0.283
[66,     1] loss: 0.250
Early stopping applied (best metric=0.4114300608634949)
Finished Training
Total time taken: 7.986922264099121
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.654
[5,     1] loss: 0.636
[6,     1] loss: 0.591
[7,     1] loss: 0.568
[8,     1] loss: 0.494
[9,     1] loss: 0.456
[10,     1] loss: 0.454
[11,     1] loss: 0.385
[12,     1] loss: 0.401
[13,     1] loss: 0.592
[14,     1] loss: 0.666
[15,     1] loss: 0.430
[16,     1] loss: 0.462
[17,     1] loss: 0.483
[18,     1] loss: 0.480
[19,     1] loss: 0.428
[20,     1] loss: 0.412
[21,     1] loss: 0.422
[22,     1] loss: 0.410
[23,     1] loss: 0.383
[24,     1] loss: 0.376
[25,     1] loss: 0.326
[26,     1] loss: 0.347
[27,     1] loss: 0.416
[28,     1] loss: 0.328
[29,     1] loss: 0.346
[30,     1] loss: 0.325
[31,     1] loss: 0.348
[32,     1] loss: 0.270
[33,     1] loss: 0.273
[34,     1] loss: 0.437
[35,     1] loss: 0.608
[36,     1] loss: 0.315
[37,     1] loss: 0.419
[38,     1] loss: 0.391
[39,     1] loss: 0.377
[40,     1] loss: 0.391
[41,     1] loss: 0.338
[42,     1] loss: 0.361
[43,     1] loss: 0.354
[44,     1] loss: 0.339
[45,     1] loss: 0.311
[46,     1] loss: 0.314
[47,     1] loss: 0.311
[48,     1] loss: 0.307
[49,     1] loss: 0.333
[50,     1] loss: 0.249
[51,     1] loss: 0.325
[52,     1] loss: 0.298
[53,     1] loss: 0.349
[54,     1] loss: 0.323
[55,     1] loss: 0.302
[56,     1] loss: 0.292
[57,     1] loss: 0.285
[58,     1] loss: 0.256
[59,     1] loss: 0.274
[60,     1] loss: 0.225
[61,     1] loss: 0.261
[62,     1] loss: 0.242
[63,     1] loss: 0.257
[64,     1] loss: 0.233
[65,     1] loss: 0.297
[66,     1] loss: 0.247
[67,     1] loss: 0.456
[68,     1] loss: 0.807
[69,     1] loss: 0.306
[70,     1] loss: 0.368
[71,     1] loss: 0.386
[72,     1] loss: 0.385
[73,     1] loss: 0.354
[74,     1] loss: 0.355
[75,     1] loss: 0.379
[76,     1] loss: 0.321
[77,     1] loss: 0.294
[78,     1] loss: 0.327
[79,     1] loss: 0.294
[80,     1] loss: 0.327
[81,     1] loss: 0.281
[82,     1] loss: 0.273
[83,     1] loss: 0.249
[84,     1] loss: 0.255
[85,     1] loss: 0.248
[86,     1] loss: 0.237
[87,     1] loss: 0.239
[88,     1] loss: 0.613
[89,     1] loss: 0.368
[90,     1] loss: 0.544
[91,     1] loss: 0.375
[92,     1] loss: 0.405
[93,     1] loss: 0.389
[94,     1] loss: 0.373
[95,     1] loss: 0.375
[96,     1] loss: 0.372
[97,     1] loss: 0.415
[98,     1] loss: 0.354
[99,     1] loss: 0.346
[100,     1] loss: 0.310
[101,     1] loss: 0.323
[102,     1] loss: 0.302
[103,     1] loss: 0.289
[104,     1] loss: 0.347
[105,     1] loss: 0.273
[106,     1] loss: 0.300
[107,     1] loss: 0.350
[108,     1] loss: 0.309
[109,     1] loss: 0.321
[110,     1] loss: 0.326
[111,     1] loss: 0.309
[112,     1] loss: 0.275
[113,     1] loss: 0.279
[114,     1] loss: 0.287
[115,     1] loss: 0.275
[116,     1] loss: 0.299
[117,     1] loss: 0.289
[118,     1] loss: 0.305
[119,     1] loss: 0.286
[120,     1] loss: 0.332
[121,     1] loss: 0.316
[122,     1] loss: 0.274
[123,     1] loss: 0.253
[124,     1] loss: 0.294
[125,     1] loss: 0.259
[126,     1] loss: 0.264
[127,     1] loss: 0.276
[128,     1] loss: 0.306
[129,     1] loss: 0.248
[130,     1] loss: 0.255
[131,     1] loss: 0.260
[132,     1] loss: 0.281
[133,     1] loss: 0.254
[134,     1] loss: 0.269
[135,     1] loss: 0.328
Early stopping applied (best metric=0.3557697534561157)
Finished Training
Total time taken: 16.284287691116333
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.707
[3,     1] loss: 0.694
[4,     1] loss: 0.693
[5,     1] loss: 0.691
[6,     1] loss: 0.687
[7,     1] loss: 0.681
[8,     1] loss: 0.670
[9,     1] loss: 0.656
[10,     1] loss: 0.642
[11,     1] loss: 0.627
[12,     1] loss: 0.601
[13,     1] loss: 0.592
[14,     1] loss: 0.562
[15,     1] loss: 0.546
[16,     1] loss: 0.523
[17,     1] loss: 0.488
[18,     1] loss: 0.503
[19,     1] loss: 0.506
[20,     1] loss: 0.508
[21,     1] loss: 0.439
[22,     1] loss: 0.417
[23,     1] loss: 0.424
[24,     1] loss: 0.363
[25,     1] loss: 0.379
[26,     1] loss: 0.383
[27,     1] loss: 0.331
[28,     1] loss: 0.335
[29,     1] loss: 0.344
[30,     1] loss: 0.302
[31,     1] loss: 0.315
[32,     1] loss: 0.317
[33,     1] loss: 0.279
[34,     1] loss: 0.286
[35,     1] loss: 0.317
[36,     1] loss: 0.309
[37,     1] loss: 0.395
[38,     1] loss: 0.699
[39,     1] loss: 0.419
[40,     1] loss: 0.415
[41,     1] loss: 0.352
[42,     1] loss: 0.411
[43,     1] loss: 0.381
[44,     1] loss: 0.396
[45,     1] loss: 0.377
[46,     1] loss: 0.341
[47,     1] loss: 0.352
[48,     1] loss: 0.308
[49,     1] loss: 0.336
[50,     1] loss: 0.288
[51,     1] loss: 0.301
[52,     1] loss: 0.285
[53,     1] loss: 0.287
[54,     1] loss: 0.269
[55,     1] loss: 0.346
[56,     1] loss: 0.277
[57,     1] loss: 0.274
[58,     1] loss: 0.268
[59,     1] loss: 0.277
[60,     1] loss: 0.282
[61,     1] loss: 0.325
[62,     1] loss: 0.554
[63,     1] loss: 0.342
[64,     1] loss: 0.496
[65,     1] loss: 0.397
[66,     1] loss: 0.412
[67,     1] loss: 0.418
[68,     1] loss: 0.440
[69,     1] loss: 0.392
[70,     1] loss: 0.347
[71,     1] loss: 0.359
[72,     1] loss: 0.388
[73,     1] loss: 0.325
[74,     1] loss: 0.361
[75,     1] loss: 0.310
[76,     1] loss: 0.306
[77,     1] loss: 0.295
[78,     1] loss: 0.297
[79,     1] loss: 0.380
[80,     1] loss: 0.365
[81,     1] loss: 0.336
[82,     1] loss: 0.308
[83,     1] loss: 0.331
[84,     1] loss: 0.366
[85,     1] loss: 0.317
[86,     1] loss: 0.339
[87,     1] loss: 0.307
[88,     1] loss: 0.328
[89,     1] loss: 0.279
[90,     1] loss: 0.314
Early stopping applied (best metric=0.4154864251613617)
Finished Training
Total time taken: 10.940393924713135
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.703
[3,     1] loss: 0.690
[4,     1] loss: 0.689
[5,     1] loss: 0.681
[6,     1] loss: 0.663
[7,     1] loss: 0.642
[8,     1] loss: 0.614
[9,     1] loss: 0.601
[10,     1] loss: 0.555
[11,     1] loss: 0.513
[12,     1] loss: 0.472
[13,     1] loss: 0.446
[14,     1] loss: 0.404
[15,     1] loss: 0.482
[16,     1] loss: 0.510
[17,     1] loss: 0.415
[18,     1] loss: 0.453
[19,     1] loss: 0.397
[20,     1] loss: 0.386
[21,     1] loss: 0.387
[22,     1] loss: 0.384
[23,     1] loss: 0.322
[24,     1] loss: 0.349
[25,     1] loss: 0.353
[26,     1] loss: 0.334
[27,     1] loss: 0.325
[28,     1] loss: 0.376
[29,     1] loss: 0.402
[30,     1] loss: 0.342
[31,     1] loss: 0.355
[32,     1] loss: 0.347
[33,     1] loss: 0.315
[34,     1] loss: 0.332
[35,     1] loss: 0.329
[36,     1] loss: 0.323
[37,     1] loss: 0.321
[38,     1] loss: 0.319
[39,     1] loss: 0.412
[40,     1] loss: 0.369
[41,     1] loss: 0.335
[42,     1] loss: 0.407
[43,     1] loss: 0.344
[44,     1] loss: 0.346
[45,     1] loss: 0.319
[46,     1] loss: 0.304
[47,     1] loss: 0.343
[48,     1] loss: 0.335
[49,     1] loss: 0.342
[50,     1] loss: 0.309
[51,     1] loss: 0.275
[52,     1] loss: 0.305
[53,     1] loss: 0.256
[54,     1] loss: 0.289
[55,     1] loss: 0.304
[56,     1] loss: 0.266
[57,     1] loss: 0.481
[58,     1] loss: 0.665
[59,     1] loss: 0.328
[60,     1] loss: 0.416
[61,     1] loss: 0.418
[62,     1] loss: 0.416
[63,     1] loss: 0.379
[64,     1] loss: 0.418
Early stopping applied (best metric=0.5209208726882935)
Finished Training
Total time taken: 7.809970140457153
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.675
[4,     1] loss: 0.654
[5,     1] loss: 0.624
[6,     1] loss: 0.600
[7,     1] loss: 0.545
[8,     1] loss: 0.505
[9,     1] loss: 0.458
[10,     1] loss: 0.420
[11,     1] loss: 0.441
[12,     1] loss: 0.377
[13,     1] loss: 0.432
[14,     1] loss: 0.490
[15,     1] loss: 0.435
[16,     1] loss: 0.479
[17,     1] loss: 0.465
[18,     1] loss: 0.431
[19,     1] loss: 0.465
[20,     1] loss: 0.395
[21,     1] loss: 0.457
[22,     1] loss: 0.380
[23,     1] loss: 0.364
[24,     1] loss: 0.392
[25,     1] loss: 0.354
[26,     1] loss: 0.398
[27,     1] loss: 0.314
[28,     1] loss: 0.349
[29,     1] loss: 0.354
[30,     1] loss: 0.364
[31,     1] loss: 0.413
[32,     1] loss: 0.373
[33,     1] loss: 0.330
[34,     1] loss: 0.359
[35,     1] loss: 0.300
[36,     1] loss: 0.341
[37,     1] loss: 0.307
[38,     1] loss: 0.330
[39,     1] loss: 0.394
[40,     1] loss: 0.808
[41,     1] loss: 0.398
[42,     1] loss: 0.412
[43,     1] loss: 0.404
[44,     1] loss: 0.436
[45,     1] loss: 0.403
[46,     1] loss: 0.420
[47,     1] loss: 0.395
[48,     1] loss: 0.381
[49,     1] loss: 0.392
[50,     1] loss: 0.394
[51,     1] loss: 0.325
[52,     1] loss: 0.352
[53,     1] loss: 0.317
[54,     1] loss: 0.337
[55,     1] loss: 0.303
[56,     1] loss: 0.222
[57,     1] loss: 0.309
[58,     1] loss: 0.324
[59,     1] loss: 0.324
[60,     1] loss: 0.328
[61,     1] loss: 0.272
[62,     1] loss: 0.292
[63,     1] loss: 0.303
[64,     1] loss: 0.266
[65,     1] loss: 0.496
[66,     1] loss: 0.977
[67,     1] loss: 0.832
[68,     1] loss: 0.685
[69,     1] loss: 0.689
[70,     1] loss: 0.689
[71,     1] loss: 0.691
[72,     1] loss: 0.691
[73,     1] loss: 0.691
[74,     1] loss: 0.692
[75,     1] loss: 0.690
[76,     1] loss: 0.689
[77,     1] loss: 0.685
[78,     1] loss: 0.678
[79,     1] loss: 0.669
[80,     1] loss: 0.648
[81,     1] loss: 0.634
[82,     1] loss: 0.627
[83,     1] loss: 0.620
[84,     1] loss: 0.603
[85,     1] loss: 0.582
[86,     1] loss: 0.576
[87,     1] loss: 0.565
[88,     1] loss: 0.509
[89,     1] loss: 0.557
[90,     1] loss: 0.523
[91,     1] loss: 0.496
[92,     1] loss: 0.503
[93,     1] loss: 0.510
[94,     1] loss: 0.489
[95,     1] loss: 0.468
[96,     1] loss: 0.474
[97,     1] loss: 0.476
[98,     1] loss: 0.473
Early stopping applied (best metric=0.35957908630371094)
Finished Training
Total time taken: 11.236277341842651
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.689
[3,     1] loss: 0.686
[4,     1] loss: 0.679
[5,     1] loss: 0.650
[6,     1] loss: 0.611
[7,     1] loss: 0.556
[8,     1] loss: 0.536
[9,     1] loss: 0.475
[10,     1] loss: 0.450
[11,     1] loss: 0.477
[12,     1] loss: 0.455
[13,     1] loss: 0.388
[14,     1] loss: 0.413
[15,     1] loss: 0.370
[16,     1] loss: 0.342
[17,     1] loss: 0.410
[18,     1] loss: 0.314
[19,     1] loss: 0.345
[20,     1] loss: 0.339
[21,     1] loss: 0.387
[22,     1] loss: 0.380
[23,     1] loss: 0.362
[24,     1] loss: 0.369
[25,     1] loss: 0.341
[26,     1] loss: 0.323
[27,     1] loss: 0.313
[28,     1] loss: 0.323
[29,     1] loss: 0.345
[30,     1] loss: 0.419
[31,     1] loss: 0.333
[32,     1] loss: 0.394
[33,     1] loss: 0.371
[34,     1] loss: 0.420
[35,     1] loss: 0.319
[36,     1] loss: 0.387
[37,     1] loss: 0.326
[38,     1] loss: 0.314
[39,     1] loss: 0.275
[40,     1] loss: 0.277
[41,     1] loss: 0.267
[42,     1] loss: 0.282
[43,     1] loss: 0.239
[44,     1] loss: 0.281
[45,     1] loss: 0.296
[46,     1] loss: 0.311
[47,     1] loss: 0.295
[48,     1] loss: 0.296
[49,     1] loss: 0.313
[50,     1] loss: 0.282
[51,     1] loss: 0.280
[52,     1] loss: 0.292
[53,     1] loss: 0.570
[54,     1] loss: 0.371
[55,     1] loss: 0.320
[56,     1] loss: 0.403
[57,     1] loss: 0.372
[58,     1] loss: 0.302
[59,     1] loss: 0.298
[60,     1] loss: 0.315
[61,     1] loss: 0.269
[62,     1] loss: 0.284
[63,     1] loss: 0.288
[64,     1] loss: 0.354
[65,     1] loss: 0.370
[66,     1] loss: 0.309
[67,     1] loss: 0.436
[68,     1] loss: 0.280
[69,     1] loss: 0.412
[70,     1] loss: 0.328
[71,     1] loss: 0.323
[72,     1] loss: 0.284
[73,     1] loss: 0.280
Early stopping applied (best metric=0.466371089220047)
Finished Training
Total time taken: 8.068092346191406
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.690
[3,     1] loss: 0.692
[4,     1] loss: 0.677
[5,     1] loss: 0.645
[6,     1] loss: 0.613
[7,     1] loss: 0.564
[8,     1] loss: 0.520
[9,     1] loss: 0.459
[10,     1] loss: 0.402
[11,     1] loss: 0.562
[12,     1] loss: 0.457
[13,     1] loss: 0.403
[14,     1] loss: 0.377
[15,     1] loss: 0.389
[16,     1] loss: 0.369
[17,     1] loss: 0.369
[18,     1] loss: 0.310
[19,     1] loss: 0.402
[20,     1] loss: 0.421
[21,     1] loss: 0.378
[22,     1] loss: 0.347
[23,     1] loss: 0.383
[24,     1] loss: 0.338
[25,     1] loss: 0.319
[26,     1] loss: 0.301
[27,     1] loss: 0.339
[28,     1] loss: 0.292
[29,     1] loss: 0.289
[30,     1] loss: 0.262
[31,     1] loss: 0.258
[32,     1] loss: 0.267
[33,     1] loss: 0.302
[34,     1] loss: 0.359
[35,     1] loss: 0.320
[36,     1] loss: 0.356
[37,     1] loss: 0.333
[38,     1] loss: 0.252
[39,     1] loss: 0.276
[40,     1] loss: 0.366
[41,     1] loss: 0.374
[42,     1] loss: 0.322
[43,     1] loss: 0.306
[44,     1] loss: 0.339
[45,     1] loss: 0.324
[46,     1] loss: 0.285
[47,     1] loss: 0.366
[48,     1] loss: 0.326
[49,     1] loss: 0.291
[50,     1] loss: 0.392
[51,     1] loss: 0.331
[52,     1] loss: 0.347
[53,     1] loss: 0.320
[54,     1] loss: 0.305
[55,     1] loss: 0.373
[56,     1] loss: 0.295
[57,     1] loss: 0.319
[58,     1] loss: 0.307
[59,     1] loss: 0.306
[60,     1] loss: 0.321
[61,     1] loss: 0.348
[62,     1] loss: 0.272
[63,     1] loss: 0.339
Early stopping applied (best metric=0.4778406620025635)
Finished Training
Total time taken: 6.919625520706177
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.692
[3,     1] loss: 0.693
[4,     1] loss: 0.682
[5,     1] loss: 0.667
[6,     1] loss: 0.635
[7,     1] loss: 0.607
[8,     1] loss: 0.573
[9,     1] loss: 0.561
[10,     1] loss: 0.506
[11,     1] loss: 0.470
[12,     1] loss: 0.445
[13,     1] loss: 0.469
[14,     1] loss: 0.444
[15,     1] loss: 0.499
[16,     1] loss: 0.440
[17,     1] loss: 0.453
[18,     1] loss: 0.445
[19,     1] loss: 0.468
[20,     1] loss: 0.383
[21,     1] loss: 0.381
[22,     1] loss: 0.417
[23,     1] loss: 0.398
[24,     1] loss: 0.395
[25,     1] loss: 0.345
[26,     1] loss: 0.560
[27,     1] loss: 0.594
[28,     1] loss: 0.409
[29,     1] loss: 0.408
[30,     1] loss: 0.467
[31,     1] loss: 0.416
[32,     1] loss: 0.430
[33,     1] loss: 0.405
[34,     1] loss: 0.395
[35,     1] loss: 0.448
[36,     1] loss: 0.366
[37,     1] loss: 0.407
[38,     1] loss: 0.407
[39,     1] loss: 0.368
[40,     1] loss: 0.352
[41,     1] loss: 0.419
[42,     1] loss: 0.298
[43,     1] loss: 0.342
[44,     1] loss: 0.388
[45,     1] loss: 0.345
[46,     1] loss: 0.319
[47,     1] loss: 0.336
[48,     1] loss: 0.308
[49,     1] loss: 0.306
[50,     1] loss: 0.329
[51,     1] loss: 0.554
[52,     1] loss: 0.650
[53,     1] loss: 0.436
[54,     1] loss: 0.433
[55,     1] loss: 0.454
[56,     1] loss: 0.471
[57,     1] loss: 0.455
[58,     1] loss: 0.423
[59,     1] loss: 0.424
[60,     1] loss: 0.408
[61,     1] loss: 0.403
[62,     1] loss: 0.377
[63,     1] loss: 0.349
[64,     1] loss: 0.321
[65,     1] loss: 0.322
[66,     1] loss: 0.330
[67,     1] loss: 0.278
[68,     1] loss: 0.320
[69,     1] loss: 0.338
[70,     1] loss: 0.385
[71,     1] loss: 0.310
[72,     1] loss: 0.438
[73,     1] loss: 0.464
[74,     1] loss: 0.393
[75,     1] loss: 0.387
[76,     1] loss: 0.356
[77,     1] loss: 0.358
[78,     1] loss: 0.335
[79,     1] loss: 0.311
[80,     1] loss: 0.319
[81,     1] loss: 0.286
[82,     1] loss: 0.564
[83,     1] loss: 0.410
[84,     1] loss: 0.388
[85,     1] loss: 0.351
[86,     1] loss: 0.363
[87,     1] loss: 0.356
[88,     1] loss: 0.335
[89,     1] loss: 0.356
[90,     1] loss: 0.303
[91,     1] loss: 0.294
[92,     1] loss: 0.320
[93,     1] loss: 0.275
Early stopping applied (best metric=0.30238601565361023)
Finished Training
Total time taken: 10.233612775802612
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.695
[3,     1] loss: 0.692
[4,     1] loss: 0.686
[5,     1] loss: 0.677
[6,     1] loss: 0.653
[7,     1] loss: 0.631
[8,     1] loss: 0.619
[9,     1] loss: 0.603
[10,     1] loss: 0.552
[11,     1] loss: 0.525
[12,     1] loss: 0.502
[13,     1] loss: 0.479
[14,     1] loss: 0.442
[15,     1] loss: 0.530
[16,     1] loss: 0.418
[17,     1] loss: 0.521
[18,     1] loss: 0.438
[19,     1] loss: 0.424
[20,     1] loss: 0.407
[21,     1] loss: 0.381
[22,     1] loss: 0.382
[23,     1] loss: 0.372
[24,     1] loss: 0.338
[25,     1] loss: 0.366
[26,     1] loss: 0.341
[27,     1] loss: 0.334
[28,     1] loss: 0.324
[29,     1] loss: 0.342
[30,     1] loss: 0.441
[31,     1] loss: 0.398
[32,     1] loss: 0.391
[33,     1] loss: 0.382
[34,     1] loss: 0.294
[35,     1] loss: 0.369
[36,     1] loss: 0.352
[37,     1] loss: 0.359
[38,     1] loss: 0.345
[39,     1] loss: 0.316
[40,     1] loss: 0.321
[41,     1] loss: 0.324
[42,     1] loss: 0.342
[43,     1] loss: 0.314
[44,     1] loss: 0.289
[45,     1] loss: 0.319
[46,     1] loss: 0.450
[47,     1] loss: 0.357
[48,     1] loss: 0.389
[49,     1] loss: 0.365
[50,     1] loss: 0.344
[51,     1] loss: 0.336
[52,     1] loss: 0.335
[53,     1] loss: 0.319
[54,     1] loss: 0.303
[55,     1] loss: 0.323
[56,     1] loss: 0.373
[57,     1] loss: 0.362
[58,     1] loss: 0.383
[59,     1] loss: 0.318
[60,     1] loss: 0.325
[61,     1] loss: 0.409
[62,     1] loss: 0.374
[63,     1] loss: 0.389
[64,     1] loss: 0.293
[65,     1] loss: 0.371
[66,     1] loss: 0.311
[67,     1] loss: 0.431
[68,     1] loss: 0.407
[69,     1] loss: 0.364
[70,     1] loss: 0.363
[71,     1] loss: 0.424
[72,     1] loss: 0.383
[73,     1] loss: 0.339
[74,     1] loss: 0.314
[75,     1] loss: 0.326
[76,     1] loss: 0.294
[77,     1] loss: 0.292
[78,     1] loss: 0.291
[79,     1] loss: 0.446
[80,     1] loss: 0.434
[81,     1] loss: 0.412
[82,     1] loss: 0.467
[83,     1] loss: 0.432
[84,     1] loss: 0.436
[85,     1] loss: 0.432
[86,     1] loss: 0.478
[87,     1] loss: 0.491
[88,     1] loss: 0.454
[89,     1] loss: 0.482
[90,     1] loss: 0.543
[91,     1] loss: 0.496
[92,     1] loss: 0.464
[93,     1] loss: 0.441
[94,     1] loss: 0.407
[95,     1] loss: 0.414
[96,     1] loss: 0.418
[97,     1] loss: 0.383
[98,     1] loss: 0.379
[99,     1] loss: 0.315
[100,     1] loss: 0.350
[101,     1] loss: 0.285
[102,     1] loss: 0.306
[103,     1] loss: 0.280
[104,     1] loss: 0.289
[105,     1] loss: 0.277
[106,     1] loss: 0.459
[107,     1] loss: 0.939
[108,     1] loss: 0.492
[109,     1] loss: 0.461
[110,     1] loss: 0.488
[111,     1] loss: 0.488
[112,     1] loss: 0.475
[113,     1] loss: 0.446
[114,     1] loss: 0.415
Early stopping applied (best metric=0.3587563931941986)
Finished Training
Total time taken: 12.498620748519897
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.697
[3,     1] loss: 0.690
[4,     1] loss: 0.677
[5,     1] loss: 0.661
[6,     1] loss: 0.624
[7,     1] loss: 0.619
[8,     1] loss: 0.580
[9,     1] loss: 0.530
[10,     1] loss: 0.487
[11,     1] loss: 0.503
[12,     1] loss: 0.453
[13,     1] loss: 0.475
[14,     1] loss: 0.483
[15,     1] loss: 0.382
[16,     1] loss: 0.474
[17,     1] loss: 0.444
[18,     1] loss: 0.387
[19,     1] loss: 0.382
[20,     1] loss: 0.318
[21,     1] loss: 0.360
[22,     1] loss: 0.415
[23,     1] loss: 0.367
[24,     1] loss: 0.303
[25,     1] loss: 0.332
[26,     1] loss: 0.371
[27,     1] loss: 0.295
[28,     1] loss: 0.349
[29,     1] loss: 0.336
[30,     1] loss: 0.303
[31,     1] loss: 0.313
[32,     1] loss: 0.343
[33,     1] loss: 0.309
[34,     1] loss: 0.311
[35,     1] loss: 0.334
[36,     1] loss: 0.301
[37,     1] loss: 0.563
[38,     1] loss: 0.596
[39,     1] loss: 0.382
[40,     1] loss: 0.424
[41,     1] loss: 0.432
[42,     1] loss: 0.407
[43,     1] loss: 0.472
[44,     1] loss: 0.426
[45,     1] loss: 0.366
[46,     1] loss: 0.357
[47,     1] loss: 0.323
[48,     1] loss: 0.329
[49,     1] loss: 0.287
[50,     1] loss: 0.342
[51,     1] loss: 0.278
[52,     1] loss: 0.283
[53,     1] loss: 0.284
[54,     1] loss: 0.295
[55,     1] loss: 0.289
[56,     1] loss: 0.298
[57,     1] loss: 0.296
[58,     1] loss: 0.313
[59,     1] loss: 0.293
[60,     1] loss: 0.284
[61,     1] loss: 0.271
[62,     1] loss: 0.328
[63,     1] loss: 0.305
Early stopping applied (best metric=0.4317587912082672)
Finished Training
Total time taken: 6.925997734069824
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.691
[3,     1] loss: 0.666
[4,     1] loss: 0.611
[5,     1] loss: 0.558
[6,     1] loss: 0.488
[7,     1] loss: 0.476
[8,     1] loss: 0.484
[9,     1] loss: 0.550
[10,     1] loss: 0.443
[11,     1] loss: 0.586
[12,     1] loss: 0.459
[13,     1] loss: 0.434
[14,     1] loss: 0.426
[15,     1] loss: 0.439
[16,     1] loss: 0.403
[17,     1] loss: 0.409
[18,     1] loss: 0.378
[19,     1] loss: 0.368
[20,     1] loss: 0.327
[21,     1] loss: 0.325
[22,     1] loss: 0.316
[23,     1] loss: 0.329
[24,     1] loss: 0.348
[25,     1] loss: 0.354
[26,     1] loss: 0.325
[27,     1] loss: 0.271
[28,     1] loss: 0.290
[29,     1] loss: 0.260
[30,     1] loss: 0.259
[31,     1] loss: 0.277
[32,     1] loss: 0.334
[33,     1] loss: 0.619
[34,     1] loss: 0.463
[35,     1] loss: 0.352
[36,     1] loss: 0.357
[37,     1] loss: 0.370
[38,     1] loss: 0.330
[39,     1] loss: 0.335
[40,     1] loss: 0.344
[41,     1] loss: 0.365
[42,     1] loss: 0.312
[43,     1] loss: 0.287
[44,     1] loss: 0.294
[45,     1] loss: 0.277
[46,     1] loss: 0.240
[47,     1] loss: 0.261
[48,     1] loss: 0.254
[49,     1] loss: 0.298
[50,     1] loss: 0.346
[51,     1] loss: 0.303
[52,     1] loss: 0.334
[53,     1] loss: 0.260
[54,     1] loss: 0.292
[55,     1] loss: 0.244
[56,     1] loss: 0.269
[57,     1] loss: 0.240
[58,     1] loss: 0.297
[59,     1] loss: 0.263
[60,     1] loss: 0.298
[61,     1] loss: 0.278
[62,     1] loss: 0.263
[63,     1] loss: 0.293
[64,     1] loss: 0.262
[65,     1] loss: 0.242
[66,     1] loss: 0.251
[67,     1] loss: 0.266
[68,     1] loss: 0.202
[69,     1] loss: 0.242
[70,     1] loss: 0.176
[71,     1] loss: 0.190
[72,     1] loss: 0.208
[73,     1] loss: 0.255
[74,     1] loss: 0.518
[75,     1] loss: 0.492
[76,     1] loss: 0.346
[77,     1] loss: 0.385
[78,     1] loss: 0.373
[79,     1] loss: 0.345
[80,     1] loss: 0.327
[81,     1] loss: 0.303
[82,     1] loss: 0.332
[83,     1] loss: 0.267
[84,     1] loss: 0.274
[85,     1] loss: 0.231
[86,     1] loss: 0.232
[87,     1] loss: 0.219
[88,     1] loss: 0.220
[89,     1] loss: 0.192
[90,     1] loss: 0.234
[91,     1] loss: 0.195
[92,     1] loss: 0.286
[93,     1] loss: 0.408
[94,     1] loss: 0.420
[95,     1] loss: 0.308
[96,     1] loss: 0.302
[97,     1] loss: 0.286
[98,     1] loss: 0.281
[99,     1] loss: 0.214
[100,     1] loss: 0.256
[101,     1] loss: 0.285
[102,     1] loss: 0.293
[103,     1] loss: 0.277
[104,     1] loss: 0.304
[105,     1] loss: 0.212
[106,     1] loss: 0.284
[107,     1] loss: 0.267
[108,     1] loss: 0.262
[109,     1] loss: 0.275
[110,     1] loss: 0.214
[111,     1] loss: 0.210
[112,     1] loss: 0.214
[113,     1] loss: 0.214
[114,     1] loss: 0.519
[115,     1] loss: 0.280
[116,     1] loss: 0.356
[117,     1] loss: 0.343
[118,     1] loss: 0.264
[119,     1] loss: 0.292
[120,     1] loss: 0.319
[121,     1] loss: 0.279
[122,     1] loss: 0.240
[123,     1] loss: 0.259
[124,     1] loss: 0.226
[125,     1] loss: 0.234
Early stopping applied (best metric=0.3307408094406128)
Finished Training
Total time taken: 13.663000106811523
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.690
[3,     1] loss: 0.692
[4,     1] loss: 0.684
[5,     1] loss: 0.676
[6,     1] loss: 0.654
[7,     1] loss: 0.621
[8,     1] loss: 0.608
[9,     1] loss: 0.586
[10,     1] loss: 0.548
[11,     1] loss: 0.538
[12,     1] loss: 0.528
[13,     1] loss: 0.469
[14,     1] loss: 0.463
[15,     1] loss: 0.468
[16,     1] loss: 0.478
[17,     1] loss: 0.403
[18,     1] loss: 0.475
[19,     1] loss: 0.347
[20,     1] loss: 0.386
[21,     1] loss: 0.525
[22,     1] loss: 0.652
[23,     1] loss: 0.456
[24,     1] loss: 0.499
[25,     1] loss: 0.459
[26,     1] loss: 0.493
[27,     1] loss: 0.457
[28,     1] loss: 0.417
[29,     1] loss: 0.473
[30,     1] loss: 0.377
[31,     1] loss: 0.445
[32,     1] loss: 0.379
[33,     1] loss: 0.394
[34,     1] loss: 0.365
[35,     1] loss: 0.364
[36,     1] loss: 0.336
[37,     1] loss: 0.347
[38,     1] loss: 0.306
[39,     1] loss: 0.299
[40,     1] loss: 0.298
[41,     1] loss: 0.320
[42,     1] loss: 0.263
[43,     1] loss: 0.297
[44,     1] loss: 0.292
[45,     1] loss: 0.361
[46,     1] loss: 0.309
[47,     1] loss: 0.321
[48,     1] loss: 0.289
[49,     1] loss: 0.333
[50,     1] loss: 0.283
[51,     1] loss: 0.430
[52,     1] loss: 0.540
[53,     1] loss: 0.398
[54,     1] loss: 0.493
[55,     1] loss: 0.421
[56,     1] loss: 0.394
[57,     1] loss: 0.406
[58,     1] loss: 0.421
[59,     1] loss: 0.359
[60,     1] loss: 0.340
[61,     1] loss: 0.336
[62,     1] loss: 0.330
[63,     1] loss: 0.306
[64,     1] loss: 0.285
[65,     1] loss: 0.363
[66,     1] loss: 0.368
[67,     1] loss: 0.305
[68,     1] loss: 0.402
[69,     1] loss: 0.366
[70,     1] loss: 0.416
[71,     1] loss: 0.301
[72,     1] loss: 0.351
[73,     1] loss: 0.343
[74,     1] loss: 0.310
[75,     1] loss: 0.330
[76,     1] loss: 0.283
[77,     1] loss: 0.290
[78,     1] loss: 0.285
[79,     1] loss: 0.291
[80,     1] loss: 0.256
[81,     1] loss: 0.261
[82,     1] loss: 0.240
[83,     1] loss: 0.257
[84,     1] loss: 0.278
[85,     1] loss: 0.237
[86,     1] loss: 0.416
[87,     1] loss: 0.333
[88,     1] loss: 0.338
[89,     1] loss: 0.333
[90,     1] loss: 0.315
[91,     1] loss: 0.310
[92,     1] loss: 0.378
[93,     1] loss: 0.304
[94,     1] loss: 0.301
[95,     1] loss: 0.290
[96,     1] loss: 0.354
[97,     1] loss: 0.309
[98,     1] loss: 0.361
[99,     1] loss: 0.258
[100,     1] loss: 0.288
[101,     1] loss: 0.306
[102,     1] loss: 0.279
[103,     1] loss: 0.245
[104,     1] loss: 0.282
[105,     1] loss: 0.336
[106,     1] loss: 0.243
[107,     1] loss: 0.278
[108,     1] loss: 0.328
[109,     1] loss: 0.252
[110,     1] loss: 0.350
[111,     1] loss: 0.266
[112,     1] loss: 0.333
[113,     1] loss: 0.262
[114,     1] loss: 0.309
[115,     1] loss: 0.268
[116,     1] loss: 0.269
[117,     1] loss: 0.232
[118,     1] loss: 0.281
[119,     1] loss: 0.237
[120,     1] loss: 0.264
[121,     1] loss: 0.208
[122,     1] loss: 0.358
[123,     1] loss: 0.698
[124,     1] loss: 0.631
[125,     1] loss: 0.467
[126,     1] loss: 0.434
[127,     1] loss: 0.443
[128,     1] loss: 0.475
[129,     1] loss: 0.444
[130,     1] loss: 0.417
[131,     1] loss: 0.417
[132,     1] loss: 0.388
[133,     1] loss: 0.364
Early stopping applied (best metric=0.21038183569908142)
Finished Training
Total time taken: 14.629603147506714
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.697
[3,     1] loss: 0.689
[4,     1] loss: 0.682
[5,     1] loss: 0.666
[6,     1] loss: 0.653
[7,     1] loss: 0.626
[8,     1] loss: 0.587
[9,     1] loss: 0.569
[10,     1] loss: 0.550
[11,     1] loss: 0.482
[12,     1] loss: 0.473
[13,     1] loss: 0.463
[14,     1] loss: 0.459
[15,     1] loss: 0.462
[16,     1] loss: 0.407
[17,     1] loss: 0.394
[18,     1] loss: 0.349
[19,     1] loss: 0.371
[20,     1] loss: 0.411
[21,     1] loss: 0.590
[22,     1] loss: 0.349
[23,     1] loss: 0.368
[24,     1] loss: 0.365
[25,     1] loss: 0.411
[26,     1] loss: 0.389
[27,     1] loss: 0.345
[28,     1] loss: 0.333
[29,     1] loss: 0.297
[30,     1] loss: 0.306
[31,     1] loss: 0.299
[32,     1] loss: 0.286
[33,     1] loss: 0.282
[34,     1] loss: 0.302
[35,     1] loss: 0.255
[36,     1] loss: 0.295
[37,     1] loss: 0.313
[38,     1] loss: 0.310
[39,     1] loss: 0.362
[40,     1] loss: 0.351
[41,     1] loss: 0.297
[42,     1] loss: 0.347
[43,     1] loss: 0.321
[44,     1] loss: 0.346
[45,     1] loss: 0.294
[46,     1] loss: 0.296
[47,     1] loss: 0.350
[48,     1] loss: 0.306
[49,     1] loss: 0.328
[50,     1] loss: 0.337
[51,     1] loss: 0.343
[52,     1] loss: 0.282
[53,     1] loss: 0.293
[54,     1] loss: 0.342
[55,     1] loss: 0.278
[56,     1] loss: 0.321
[57,     1] loss: 0.260
[58,     1] loss: 0.238
[59,     1] loss: 0.289
[60,     1] loss: 0.282
[61,     1] loss: 0.322
[62,     1] loss: 0.404
[63,     1] loss: 0.317
[64,     1] loss: 0.376
[65,     1] loss: 0.317
[66,     1] loss: 0.309
[67,     1] loss: 0.311
[68,     1] loss: 0.280
[69,     1] loss: 0.313
[70,     1] loss: 0.282
[71,     1] loss: 0.344
[72,     1] loss: 0.313
[73,     1] loss: 0.301
[74,     1] loss: 0.297
[75,     1] loss: 0.283
[76,     1] loss: 0.312
[77,     1] loss: 0.255
[78,     1] loss: 0.295
[79,     1] loss: 0.470
[80,     1] loss: 0.848
[81,     1] loss: 0.443
[82,     1] loss: 0.404
[83,     1] loss: 0.473
[84,     1] loss: 0.485
[85,     1] loss: 0.471
[86,     1] loss: 0.425
[87,     1] loss: 0.420
[88,     1] loss: 0.405
[89,     1] loss: 0.388
[90,     1] loss: 0.397
[91,     1] loss: 0.371
[92,     1] loss: 0.358
[93,     1] loss: 0.295
[94,     1] loss: 0.304
[95,     1] loss: 0.302
[96,     1] loss: 0.311
[97,     1] loss: 0.326
[98,     1] loss: 0.299
[99,     1] loss: 0.334
[100,     1] loss: 0.312
[101,     1] loss: 0.320
[102,     1] loss: 0.339
[103,     1] loss: 0.319
[104,     1] loss: 0.307
[105,     1] loss: 0.282
[106,     1] loss: 0.284
[107,     1] loss: 0.315
[108,     1] loss: 0.305
[109,     1] loss: 0.321
[110,     1] loss: 0.320
[111,     1] loss: 0.408
[112,     1] loss: 0.338
[113,     1] loss: 0.336
[114,     1] loss: 0.330
[115,     1] loss: 0.300
[116,     1] loss: 0.298
[117,     1] loss: 0.340
[118,     1] loss: 0.318
[119,     1] loss: 0.347
[120,     1] loss: 0.290
[121,     1] loss: 0.403
[122,     1] loss: 0.328
[123,     1] loss: 0.343
Early stopping applied (best metric=0.3539220690727234)
Finished Training
Total time taken: 13.577000141143799
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.695
[3,     1] loss: 0.687
[4,     1] loss: 0.687
[5,     1] loss: 0.674
[6,     1] loss: 0.655
[7,     1] loss: 0.631
[8,     1] loss: 0.600
[9,     1] loss: 0.557
[10,     1] loss: 0.568
[11,     1] loss: 0.536
[12,     1] loss: 0.515
[13,     1] loss: 0.496
[14,     1] loss: 0.469
[15,     1] loss: 0.531
[16,     1] loss: 0.459
[17,     1] loss: 0.475
[18,     1] loss: 0.491
[19,     1] loss: 0.455
[20,     1] loss: 0.420
[21,     1] loss: 0.501
[22,     1] loss: 0.439
[23,     1] loss: 0.431
[24,     1] loss: 0.391
[25,     1] loss: 0.415
[26,     1] loss: 0.442
[27,     1] loss: 0.421
[28,     1] loss: 0.390
[29,     1] loss: 0.437
[30,     1] loss: 0.381
[31,     1] loss: 0.386
[32,     1] loss: 0.388
[33,     1] loss: 0.396
[34,     1] loss: 0.366
[35,     1] loss: 0.397
[36,     1] loss: 0.385
[37,     1] loss: 0.383
[38,     1] loss: 0.467
[39,     1] loss: 0.520
[40,     1] loss: 0.405
[41,     1] loss: 0.381
[42,     1] loss: 0.394
[43,     1] loss: 0.385
[44,     1] loss: 0.398
[45,     1] loss: 0.389
[46,     1] loss: 0.343
[47,     1] loss: 0.361
[48,     1] loss: 0.372
[49,     1] loss: 0.323
[50,     1] loss: 0.363
[51,     1] loss: 0.317
[52,     1] loss: 0.654
[53,     1] loss: 0.367
[54,     1] loss: 0.382
[55,     1] loss: 0.373
[56,     1] loss: 0.366
[57,     1] loss: 0.307
[58,     1] loss: 0.366
[59,     1] loss: 0.436
[60,     1] loss: 0.414
[61,     1] loss: 0.380
[62,     1] loss: 0.385
[63,     1] loss: 0.366
[64,     1] loss: 0.355
[65,     1] loss: 0.368
[66,     1] loss: 0.306
[67,     1] loss: 0.334
[68,     1] loss: 0.335
[69,     1] loss: 0.307
[70,     1] loss: 0.284
[71,     1] loss: 0.309
[72,     1] loss: 0.595
[73,     1] loss: 0.574
[74,     1] loss: 0.478
[75,     1] loss: 0.412
[76,     1] loss: 0.443
[77,     1] loss: 0.438
[78,     1] loss: 0.432
[79,     1] loss: 0.427
[80,     1] loss: 0.375
[81,     1] loss: 0.375
[82,     1] loss: 0.365
[83,     1] loss: 0.338
[84,     1] loss: 0.335
[85,     1] loss: 0.425
[86,     1] loss: 0.550
Early stopping applied (best metric=0.3235851526260376)
Finished Training
Total time taken: 9.436000108718872
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.690
[3,     1] loss: 0.685
[4,     1] loss: 0.652
[5,     1] loss: 0.626
[6,     1] loss: 0.584
[7,     1] loss: 0.545
[8,     1] loss: 0.489
[9,     1] loss: 0.465
[10,     1] loss: 0.427
[11,     1] loss: 0.444
[12,     1] loss: 0.419
[13,     1] loss: 0.381
[14,     1] loss: 0.331
[15,     1] loss: 0.435
[16,     1] loss: 0.370
[17,     1] loss: 0.377
[18,     1] loss: 0.447
[19,     1] loss: 0.400
[20,     1] loss: 0.528
[21,     1] loss: 0.536
[22,     1] loss: 0.420
[23,     1] loss: 0.421
[24,     1] loss: 0.425
[25,     1] loss: 0.398
[26,     1] loss: 0.403
[27,     1] loss: 0.351
[28,     1] loss: 0.429
[29,     1] loss: 0.387
[30,     1] loss: 0.384
[31,     1] loss: 0.324
[32,     1] loss: 0.359
[33,     1] loss: 0.368
[34,     1] loss: 0.353
[35,     1] loss: 0.296
[36,     1] loss: 0.301
[37,     1] loss: 0.250
[38,     1] loss: 0.249
[39,     1] loss: 0.269
[40,     1] loss: 0.269
[41,     1] loss: 0.294
[42,     1] loss: 0.277
[43,     1] loss: 0.246
[44,     1] loss: 0.323
[45,     1] loss: 0.304
[46,     1] loss: 0.236
[47,     1] loss: 0.240
[48,     1] loss: 0.250
[49,     1] loss: 0.223
[50,     1] loss: 0.207
[51,     1] loss: 0.199
[52,     1] loss: 0.219
[53,     1] loss: 0.405
[54,     1] loss: 0.311
[55,     1] loss: 0.235
[56,     1] loss: 0.297
[57,     1] loss: 0.286
[58,     1] loss: 0.279
[59,     1] loss: 0.302
[60,     1] loss: 0.300
[61,     1] loss: 0.268
[62,     1] loss: 0.226
[63,     1] loss: 0.256
[64,     1] loss: 0.262
[65,     1] loss: 0.258
[66,     1] loss: 0.281
[67,     1] loss: 0.425
[68,     1] loss: 0.299
[69,     1] loss: 0.439
[70,     1] loss: 0.371
Early stopping applied (best metric=0.3904504179954529)
Finished Training
Total time taken: 7.661623954772949
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.676
[4,     1] loss: 0.635
[5,     1] loss: 0.599
[6,     1] loss: 0.554
[7,     1] loss: 0.532
[8,     1] loss: 0.477
[9,     1] loss: 0.438
[10,     1] loss: 0.424
[11,     1] loss: 0.382
[12,     1] loss: 0.356
[13,     1] loss: 0.371
[14,     1] loss: 0.337
[15,     1] loss: 0.296
[16,     1] loss: 0.388
[17,     1] loss: 0.302
[18,     1] loss: 0.277
[19,     1] loss: 0.291
[20,     1] loss: 0.487
[21,     1] loss: 0.586
[22,     1] loss: 0.325
[23,     1] loss: 0.452
[24,     1] loss: 0.414
[25,     1] loss: 0.437
[26,     1] loss: 0.437
[27,     1] loss: 0.398
[28,     1] loss: 0.380
[29,     1] loss: 0.358
[30,     1] loss: 0.351
[31,     1] loss: 0.362
[32,     1] loss: 0.307
[33,     1] loss: 0.296
[34,     1] loss: 0.306
[35,     1] loss: 0.395
[36,     1] loss: 0.509
[37,     1] loss: 0.349
[38,     1] loss: 0.374
[39,     1] loss: 0.369
[40,     1] loss: 0.357
[41,     1] loss: 0.341
[42,     1] loss: 0.310
[43,     1] loss: 0.331
[44,     1] loss: 0.358
[45,     1] loss: 0.327
[46,     1] loss: 0.293
[47,     1] loss: 0.301
[48,     1] loss: 0.327
[49,     1] loss: 0.262
[50,     1] loss: 0.317
[51,     1] loss: 0.288
[52,     1] loss: 0.270
[53,     1] loss: 0.334
[54,     1] loss: 0.346
[55,     1] loss: 0.334
[56,     1] loss: 0.281
[57,     1] loss: 0.300
[58,     1] loss: 0.317
[59,     1] loss: 0.331
[60,     1] loss: 0.309
[61,     1] loss: 0.265
[62,     1] loss: 0.372
[63,     1] loss: 0.527
[64,     1] loss: 0.392
[65,     1] loss: 0.324
[66,     1] loss: 0.410
[67,     1] loss: 0.312
[68,     1] loss: 0.286
[69,     1] loss: 0.299
[70,     1] loss: 0.282
[71,     1] loss: 0.375
[72,     1] loss: 0.296
[73,     1] loss: 0.300
[74,     1] loss: 0.308
[75,     1] loss: 0.308
[76,     1] loss: 0.268
[77,     1] loss: 0.315
[78,     1] loss: 0.330
[79,     1] loss: 0.327
[80,     1] loss: 0.306
[81,     1] loss: 0.298
[82,     1] loss: 0.303
[83,     1] loss: 0.293
[84,     1] loss: 0.298
[85,     1] loss: 0.487
[86,     1] loss: 0.629
[87,     1] loss: 0.391
[88,     1] loss: 0.431
[89,     1] loss: 0.416
[90,     1] loss: 0.401
[91,     1] loss: 0.380
[92,     1] loss: 0.372
[93,     1] loss: 0.337
[94,     1] loss: 0.335
[95,     1] loss: 0.343
[96,     1] loss: 0.326
[97,     1] loss: 0.284
[98,     1] loss: 0.295
[99,     1] loss: 0.304
[100,     1] loss: 0.321
Early stopping applied (best metric=0.2924450933933258)
Finished Training
Total time taken: 10.948000192642212
{'Hydroxylation-K Validation Accuracy': 0.8208156028368795, 'Hydroxylation-K Validation Sensitivity': 0.8493333333333333, 'Hydroxylation-K Validation Specificity': 0.8136842105263158, 'Hydroxylation-K Validation Precision': 0.5520603448838743, 'Hydroxylation-K AUC ROC': 0.8428947368421053, 'Hydroxylation-K AUC PR': 0.5918387262464642, 'Hydroxylation-K MCC': 0.5798855043511112, 'Hydroxylation-K F1': 0.658964161478653, 'Validation Loss (Hydroxylation-K)': 0.3543915086984634, 'Validation Loss (total)': 0.3543915086984634, 'TimeToTrain': 10.989459743499756}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005618475463877828,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2229140222,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.126195640104207}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.687
[3,     1] loss: 0.664
[4,     1] loss: 0.626
[5,     1] loss: 0.601
[6,     1] loss: 0.570
[7,     1] loss: 0.521
[8,     1] loss: 0.514
[9,     1] loss: 0.456
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034275952237799922,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2153379784,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.11773916353533}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.691
[4,     1] loss: 0.655
[5,     1] loss: 0.627
[6,     1] loss: 0.597
[7,     1] loss: 0.569
[8,     1] loss: 0.548
[9,     1] loss: 0.515
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008222534239715296,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1481178522,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.12566086458113135}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.701
[3,     1] loss: 0.677
[4,     1] loss: 0.635
[5,     1] loss: 0.587
[6,     1] loss: 0.525
[7,     1] loss: 0.458
[8,     1] loss: 0.422
[9,     1] loss: 0.381
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037327553899036935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1053905738,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.829209804476379}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.682
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028508825760863725,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1251966838,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.194718541010899}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.697
[3,     1] loss: 0.673
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00031908595110053794,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2049495478,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.870474069939869}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004953956442534528,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4006688444,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.7595233300525255}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.667
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006403783874772181,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1696967895,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.8491051377119054}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.692
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008502612893964083,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2057306054,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.266802356628304}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.697
[3,     1] loss: 0.682
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029845412486330697,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3507316411,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.767482742239679}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.684
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00285510956882458,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1325313011,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.338072177105957}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.681
[4,     1] loss: 0.655
[5,     1] loss: 0.632
[6,     1] loss: 0.617
[7,     1] loss: 0.569
[8,     1] loss: 0.573
[9,     1] loss: 0.532
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007071188457342375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3318391311,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.010351245415396}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.694
[3,     1] loss: 0.676
[4,     1] loss: 0.654
[5,     1] loss: 0.634
[6,     1] loss: 0.608
[7,     1] loss: 0.588
[8,     1] loss: 0.572
[9,     1] loss: 0.546
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004570566775359113,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3586916477,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.918235038944307}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.652
[5,     1] loss: 0.631
[6,     1] loss: 0.608
[7,     1] loss: 0.579
[8,     1] loss: 0.580
[9,     1] loss: 0.521
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003495736167990632,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1462620703,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.595448079057167}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.679
[3,     1] loss: 0.662
[4,     1] loss: 0.623
[5,     1] loss: 0.587
[6,     1] loss: 0.578
[7,     1] loss: 0.536
[8,     1] loss: 0.513
[9,     1] loss: 0.504
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029808221420936478,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2723382172,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.411425776004118}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.683
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001040752751822198,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 624689553,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.584200643962845}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.693
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 5.1833696747604046e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2078845418,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.788058708449128}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.692
[4,     1] loss: 0.692
[5,     1] loss: 0.692
[6,     1] loss: 0.692
[7,     1] loss: 0.690
[8,     1] loss: 0.692
[9,     1] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002415095264019609,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 902110756,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.442703853440015}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.690
[3,     1] loss: 0.677
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004697101438941529,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3082849257,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.389690341860304}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.701
[3,     1] loss: 0.681
[4,     1] loss: 0.656
[5,     1] loss: 0.615
[6,     1] loss: 0.571
[7,     1] loss: 0.545
[8,     1] loss: 0.468
[9,     1] loss: 0.450
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003227843415118407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2870981162,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.319541449745422}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.679
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003836170004580715,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4080287091,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.754713674761156}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.692
[3,     1] loss: 0.678
[4,     1] loss: 0.651
[5,     1] loss: 0.627
[6,     1] loss: 0.599
[7,     1] loss: 0.555
[8,     1] loss: 0.547
[9,     1] loss: 0.483
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00497822078911151,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 698353391,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.498220003328889}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.680
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00042461569832419414,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4182815772,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.103010190059537}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.691
[4,     1] loss: 0.687
[5,     1] loss: 0.684
[6,     1] loss: 0.681
[7,     1] loss: 0.674
[8,     1] loss: 0.669
[9,     1] loss: 0.657
[10,     1] loss: 0.654
[11,     1] loss: 0.644
[12,     1] loss: 0.627
[13,     1] loss: 0.630
[14,     1] loss: 0.615
[15,     1] loss: 0.612
[16,     1] loss: 0.606
[17,     1] loss: 0.590
[18,     1] loss: 0.578
[19,     1] loss: 0.569
[20,     1] loss: 0.555
[21,     1] loss: 0.560
[22,     1] loss: 0.555
[23,     1] loss: 0.545
[24,     1] loss: 0.540
[25,     1] loss: 0.523
[26,     1] loss: 0.511
[27,     1] loss: 0.502
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004259289378080474,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 402443900,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.63811421883445}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.692
[3,     1] loss: 0.677
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00352991218975643,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 790380297,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.195837440056003}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.688
[3,     1] loss: 0.674
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003955032739566287,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 471319131,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.511346409574351}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.689
[3,     1] loss: 0.670
[4,     1] loss: 0.628
[5,     1] loss: 0.577
[6,     1] loss: 0.544
[7,     1] loss: 0.513
[8,     1] loss: 0.480
[9,     1] loss: 0.465
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024674062061251683,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3214726717,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.744807425449679}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.683
[4,     1] loss: 0.663
[5,     1] loss: 0.638
[6,     1] loss: 0.607
[7,     1] loss: 0.596
[8,     1] loss: 0.577
[9,     1] loss: 0.569
[10,     1] loss: 0.515
[11,     1] loss: 0.514
[12,     1] loss: 0.503
[13,     1] loss: 0.456
[14,     1] loss: 0.444
[15,     1] loss: 0.444
[16,     1] loss: 0.439
[17,     1] loss: 0.390
[18,     1] loss: 0.402
[19,     1] loss: 0.315
[20,     1] loss: 0.455
[21,     1] loss: 0.351
[22,     1] loss: 0.398
[23,     1] loss: 0.382
[24,     1] loss: 0.349
[25,     1] loss: 0.306
[26,     1] loss: 0.315
[27,     1] loss: 0.370
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007120584772259082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3455070748,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.085507390146393}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004863486975241628,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 671459800,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.509399723275022}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.692
[3,     1] loss: 0.683
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004354391318036519,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1628133230,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.622284386404974}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.690
[3,     1] loss: 0.674
[4,     1] loss: 0.659
[5,     1] loss: 0.621
[6,     1] loss: 0.607
[7,     1] loss: 0.584
[8,     1] loss: 0.570
[9,     1] loss: 0.519
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036204230504654628,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2500031934,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.27681157911617404}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.692
[3,     1] loss: 0.685
[4,     1] loss: 0.667
[5,     1] loss: 0.647
[6,     1] loss: 0.632
[7,     1] loss: 0.579
[8,     1] loss: 0.551
[9,     1] loss: 0.545
[10,     1] loss: 0.466
[11,     1] loss: 0.445
[12,     1] loss: 0.440
[13,     1] loss: 0.395
[14,     1] loss: 0.387
[15,     1] loss: 0.374
[16,     1] loss: 0.364
[17,     1] loss: 0.390
[18,     1] loss: 0.347
[19,     1] loss: 0.333
[20,     1] loss: 0.282
[21,     1] loss: 0.256
[22,     1] loss: 0.343
[23,     1] loss: 0.215
[24,     1] loss: 0.232
[25,     1] loss: 0.332
[26,     1] loss: 0.277
[27,     1] loss: 0.287
[28,     1] loss: 0.257
[29,     1] loss: 0.288
[30,     1] loss: 0.242
[31,     1] loss: 0.230
[32,     1] loss: 0.268
[33,     1] loss: 0.152
[34,     1] loss: 0.176
[35,     1] loss: 0.177
[36,     1] loss: 0.165
[37,     1] loss: 0.160
[38,     1] loss: 0.164
[39,     1] loss: 0.180
[40,     1] loss: 0.096
[41,     1] loss: 0.109
[42,     1] loss: 0.116
[43,     1] loss: 0.123
[44,     1] loss: 0.090
[45,     1] loss: 0.103
[46,     1] loss: 0.085
[47,     1] loss: 0.123
[48,     1] loss: 0.104
[49,     1] loss: 0.118
[50,     1] loss: 0.109
[51,     1] loss: 0.101
[52,     1] loss: 0.093
[53,     1] loss: 0.109
[54,     1] loss: 0.091
[55,     1] loss: 0.095
[56,     1] loss: 0.084
[57,     1] loss: 0.130
[58,     1] loss: 0.065
[59,     1] loss: 0.072
[60,     1] loss: 0.128
[61,     1] loss: 0.094
[62,     1] loss: 0.049
[63,     1] loss: 0.096
[64,     1] loss: 0.074
[65,     1] loss: 0.086
[66,     1] loss: 0.055
[67,     1] loss: 0.050
Early stopping applied (best metric=0.1679421067237854)
Finished Training
Total time taken: 7.344604015350342
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.674
[4,     1] loss: 0.627
[5,     1] loss: 0.592
[6,     1] loss: 0.553
[7,     1] loss: 0.498
[8,     1] loss: 0.440
[9,     1] loss: 0.423
[10,     1] loss: 0.399
[11,     1] loss: 0.360
[12,     1] loss: 0.385
[13,     1] loss: 0.417
[14,     1] loss: 0.428
[15,     1] loss: 0.342
[16,     1] loss: 0.356
[17,     1] loss: 0.361
[18,     1] loss: 0.329
[19,     1] loss: 0.265
[20,     1] loss: 0.263
[21,     1] loss: 0.301
[22,     1] loss: 0.315
[23,     1] loss: 0.275
[24,     1] loss: 0.265
[25,     1] loss: 0.198
[26,     1] loss: 0.196
[27,     1] loss: 0.225
[28,     1] loss: 0.178
[29,     1] loss: 0.213
[30,     1] loss: 0.168
[31,     1] loss: 0.254
[32,     1] loss: 0.181
[33,     1] loss: 0.176
[34,     1] loss: 0.191
[35,     1] loss: 0.201
[36,     1] loss: 0.083
[37,     1] loss: 0.149
[38,     1] loss: 0.149
[39,     1] loss: 0.177
[40,     1] loss: 0.123
[41,     1] loss: 0.121
[42,     1] loss: 0.122
[43,     1] loss: 0.125
[44,     1] loss: 0.163
[45,     1] loss: 0.085
[46,     1] loss: 0.117
[47,     1] loss: 0.090
[48,     1] loss: 0.116
[49,     1] loss: 0.117
[50,     1] loss: 0.108
[51,     1] loss: 0.121
[52,     1] loss: 0.111
[53,     1] loss: 0.137
[54,     1] loss: 0.096
[55,     1] loss: 0.105
[56,     1] loss: 0.070
[57,     1] loss: 0.116
Early stopping applied (best metric=0.3790625035762787)
Finished Training
Total time taken: 6.255001068115234
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.693
[3,     1] loss: 0.668
[4,     1] loss: 0.617
[5,     1] loss: 0.580
[6,     1] loss: 0.540
[7,     1] loss: 0.529
[8,     1] loss: 0.452
[9,     1] loss: 0.392
[10,     1] loss: 0.424
[11,     1] loss: 0.354
[12,     1] loss: 0.431
[13,     1] loss: 0.325
[14,     1] loss: 0.324
[15,     1] loss: 0.288
[16,     1] loss: 0.285
[17,     1] loss: 0.309
[18,     1] loss: 0.287
[19,     1] loss: 0.306
[20,     1] loss: 0.314
[21,     1] loss: 0.236
[22,     1] loss: 0.318
[23,     1] loss: 0.279
[24,     1] loss: 0.325
[25,     1] loss: 0.252
[26,     1] loss: 0.298
[27,     1] loss: 0.258
[28,     1] loss: 0.238
[29,     1] loss: 0.292
[30,     1] loss: 0.266
[31,     1] loss: 0.257
[32,     1] loss: 0.251
[33,     1] loss: 0.221
[34,     1] loss: 0.241
[35,     1] loss: 0.161
[36,     1] loss: 0.203
[37,     1] loss: 0.168
[38,     1] loss: 0.163
[39,     1] loss: 0.145
[40,     1] loss: 0.187
[41,     1] loss: 0.187
[42,     1] loss: 0.164
[43,     1] loss: 0.155
[44,     1] loss: 0.132
[45,     1] loss: 0.223
[46,     1] loss: 0.168
[47,     1] loss: 0.129
[48,     1] loss: 0.119
[49,     1] loss: 0.120
[50,     1] loss: 0.172
[51,     1] loss: 0.150
[52,     1] loss: 0.207
[53,     1] loss: 0.119
[54,     1] loss: 0.153
[55,     1] loss: 0.149
[56,     1] loss: 0.162
[57,     1] loss: 0.199
[58,     1] loss: 0.207
[59,     1] loss: 0.087
Early stopping applied (best metric=0.3636220097541809)
Finished Training
Total time taken: 6.462000131607056
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.694
[3,     1] loss: 0.677
[4,     1] loss: 0.658
[5,     1] loss: 0.635
[6,     1] loss: 0.582
[7,     1] loss: 0.585
[8,     1] loss: 0.512
[9,     1] loss: 0.457
[10,     1] loss: 0.446
[11,     1] loss: 0.416
[12,     1] loss: 0.400
[13,     1] loss: 0.326
[14,     1] loss: 0.345
[15,     1] loss: 0.345
[16,     1] loss: 0.322
[17,     1] loss: 0.393
[18,     1] loss: 0.316
[19,     1] loss: 0.299
[20,     1] loss: 0.281
[21,     1] loss: 0.290
[22,     1] loss: 0.249
[23,     1] loss: 0.314
[24,     1] loss: 0.220
[25,     1] loss: 0.224
[26,     1] loss: 0.345
[27,     1] loss: 0.247
[28,     1] loss: 0.240
[29,     1] loss: 0.171
[30,     1] loss: 0.185
[31,     1] loss: 0.236
[32,     1] loss: 0.196
[33,     1] loss: 0.198
[34,     1] loss: 0.187
[35,     1] loss: 0.122
[36,     1] loss: 0.178
[37,     1] loss: 0.185
[38,     1] loss: 0.131
[39,     1] loss: 0.177
[40,     1] loss: 0.203
[41,     1] loss: 0.123
[42,     1] loss: 0.101
[43,     1] loss: 0.130
[44,     1] loss: 0.153
[45,     1] loss: 0.127
[46,     1] loss: 0.112
[47,     1] loss: 0.112
[48,     1] loss: 0.119
[49,     1] loss: 0.122
[50,     1] loss: 0.062
[51,     1] loss: 0.123
[52,     1] loss: 0.104
[53,     1] loss: 0.118
[54,     1] loss: 0.100
[55,     1] loss: 0.136
[56,     1] loss: 0.114
[57,     1] loss: 0.097
[58,     1] loss: 0.053
[59,     1] loss: 0.076
[60,     1] loss: 0.100
[61,     1] loss: 0.140
[62,     1] loss: 0.093
[63,     1] loss: 0.091
[64,     1] loss: 0.139
[65,     1] loss: 0.114
[66,     1] loss: 0.100
[67,     1] loss: 0.068
[68,     1] loss: 0.091
[69,     1] loss: 0.071
Early stopping applied (best metric=0.18897125124931335)
Finished Training
Total time taken: 7.573999881744385
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.693
[3,     1] loss: 0.671
[4,     1] loss: 0.625
[5,     1] loss: 0.589
[6,     1] loss: 0.557
[7,     1] loss: 0.517
[8,     1] loss: 0.499
[9,     1] loss: 0.471
[10,     1] loss: 0.395
[11,     1] loss: 0.449
[12,     1] loss: 0.433
[13,     1] loss: 0.377
[14,     1] loss: 0.394
[15,     1] loss: 0.349
[16,     1] loss: 0.318
[17,     1] loss: 0.244
[18,     1] loss: 0.289
[19,     1] loss: 0.280
[20,     1] loss: 0.209
[21,     1] loss: 0.292
[22,     1] loss: 0.335
[23,     1] loss: 0.299
[24,     1] loss: 0.229
[25,     1] loss: 0.215
[26,     1] loss: 0.263
[27,     1] loss: 0.215
[28,     1] loss: 0.172
[29,     1] loss: 0.214
[30,     1] loss: 0.136
[31,     1] loss: 0.225
[32,     1] loss: 0.134
[33,     1] loss: 0.090
[34,     1] loss: 0.129
[35,     1] loss: 0.130
[36,     1] loss: 0.101
[37,     1] loss: 0.115
[38,     1] loss: 0.107
[39,     1] loss: 0.120
[40,     1] loss: 0.157
[41,     1] loss: 0.136
[42,     1] loss: 0.091
[43,     1] loss: 0.079
[44,     1] loss: 0.085
[45,     1] loss: 0.099
[46,     1] loss: 0.080
[47,     1] loss: 0.070
[48,     1] loss: 0.115
[49,     1] loss: 0.077
[50,     1] loss: 0.128
[51,     1] loss: 0.094
[52,     1] loss: 0.070
[53,     1] loss: 0.087
[54,     1] loss: 0.079
[55,     1] loss: 0.100
[56,     1] loss: 0.141
[57,     1] loss: 0.098
Early stopping applied (best metric=0.3920018970966339)
Finished Training
Total time taken: 6.28399920463562
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.647
[5,     1] loss: 0.606
[6,     1] loss: 0.560
[7,     1] loss: 0.521
[8,     1] loss: 0.496
[9,     1] loss: 0.484
[10,     1] loss: 0.441
[11,     1] loss: 0.437
[12,     1] loss: 0.399
[13,     1] loss: 0.348
[14,     1] loss: 0.359
[15,     1] loss: 0.335
[16,     1] loss: 0.377
[17,     1] loss: 0.312
[18,     1] loss: 0.281
[19,     1] loss: 0.228
[20,     1] loss: 0.225
[21,     1] loss: 0.180
[22,     1] loss: 0.202
[23,     1] loss: 0.202
[24,     1] loss: 0.162
[25,     1] loss: 0.245
[26,     1] loss: 0.186
[27,     1] loss: 0.142
[28,     1] loss: 0.205
[29,     1] loss: 0.189
[30,     1] loss: 0.180
[31,     1] loss: 0.167
[32,     1] loss: 0.135
[33,     1] loss: 0.156
[34,     1] loss: 0.101
[35,     1] loss: 0.165
[36,     1] loss: 0.146
[37,     1] loss: 0.155
[38,     1] loss: 0.169
[39,     1] loss: 0.127
[40,     1] loss: 0.126
[41,     1] loss: 0.119
[42,     1] loss: 0.131
[43,     1] loss: 0.133
[44,     1] loss: 0.088
[45,     1] loss: 0.098
[46,     1] loss: 0.086
[47,     1] loss: 0.096
[48,     1] loss: 0.116
[49,     1] loss: 0.110
[50,     1] loss: 0.071
[51,     1] loss: 0.099
[52,     1] loss: 0.081
[53,     1] loss: 0.072
[54,     1] loss: 0.076
[55,     1] loss: 0.079
[56,     1] loss: 0.114
Early stopping applied (best metric=0.5280673503875732)
Finished Training
Total time taken: 6.1630003452301025
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.689
[3,     1] loss: 0.673
[4,     1] loss: 0.657
[5,     1] loss: 0.621
[6,     1] loss: 0.571
[7,     1] loss: 0.533
[8,     1] loss: 0.498
[9,     1] loss: 0.454
[10,     1] loss: 0.453
[11,     1] loss: 0.441
[12,     1] loss: 0.352
[13,     1] loss: 0.386
[14,     1] loss: 0.281
[15,     1] loss: 0.275
[16,     1] loss: 0.242
[17,     1] loss: 0.309
[18,     1] loss: 0.261
[19,     1] loss: 0.228
[20,     1] loss: 0.227
[21,     1] loss: 0.187
[22,     1] loss: 0.286
[23,     1] loss: 0.241
[24,     1] loss: 0.235
[25,     1] loss: 0.195
[26,     1] loss: 0.153
[27,     1] loss: 0.160
[28,     1] loss: 0.207
[29,     1] loss: 0.218
[30,     1] loss: 0.210
[31,     1] loss: 0.126
[32,     1] loss: 0.180
[33,     1] loss: 0.136
[34,     1] loss: 0.148
[35,     1] loss: 0.176
[36,     1] loss: 0.126
[37,     1] loss: 0.132
[38,     1] loss: 0.128
[39,     1] loss: 0.108
[40,     1] loss: 0.120
[41,     1] loss: 0.120
[42,     1] loss: 0.110
[43,     1] loss: 0.089
[44,     1] loss: 0.148
[45,     1] loss: 0.129
[46,     1] loss: 0.096
[47,     1] loss: 0.131
[48,     1] loss: 0.126
[49,     1] loss: 0.091
[50,     1] loss: 0.120
[51,     1] loss: 0.105
[52,     1] loss: 0.092
[53,     1] loss: 0.085
[54,     1] loss: 0.129
[55,     1] loss: 0.086
[56,     1] loss: 0.125
[57,     1] loss: 0.147
[58,     1] loss: 0.057
[59,     1] loss: 0.138
[60,     1] loss: 0.118
Early stopping applied (best metric=0.2811771631240845)
Finished Training
Total time taken: 6.5916712284088135
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.688
[3,     1] loss: 0.660
[4,     1] loss: 0.631
[5,     1] loss: 0.598
[6,     1] loss: 0.546
[7,     1] loss: 0.564
[8,     1] loss: 0.510
[9,     1] loss: 0.448
[10,     1] loss: 0.483
[11,     1] loss: 0.449
[12,     1] loss: 0.404
[13,     1] loss: 0.368
[14,     1] loss: 0.365
[15,     1] loss: 0.351
[16,     1] loss: 0.382
[17,     1] loss: 0.343
[18,     1] loss: 0.358
[19,     1] loss: 0.295
[20,     1] loss: 0.272
[21,     1] loss: 0.283
[22,     1] loss: 0.291
[23,     1] loss: 0.245
[24,     1] loss: 0.247
[25,     1] loss: 0.257
[26,     1] loss: 0.231
[27,     1] loss: 0.193
[28,     1] loss: 0.203
[29,     1] loss: 0.192
[30,     1] loss: 0.204
[31,     1] loss: 0.265
[32,     1] loss: 0.237
[33,     1] loss: 0.230
[34,     1] loss: 0.147
[35,     1] loss: 0.171
[36,     1] loss: 0.162
[37,     1] loss: 0.234
[38,     1] loss: 0.140
[39,     1] loss: 0.173
[40,     1] loss: 0.186
[41,     1] loss: 0.152
[42,     1] loss: 0.152
[43,     1] loss: 0.162
[44,     1] loss: 0.111
[45,     1] loss: 0.158
[46,     1] loss: 0.152
[47,     1] loss: 0.160
[48,     1] loss: 0.139
[49,     1] loss: 0.130
[50,     1] loss: 0.117
[51,     1] loss: 0.188
[52,     1] loss: 0.103
[53,     1] loss: 0.097
[54,     1] loss: 0.112
[55,     1] loss: 0.080
[56,     1] loss: 0.122
[57,     1] loss: 0.080
[58,     1] loss: 0.128
[59,     1] loss: 0.142
[60,     1] loss: 0.131
[61,     1] loss: 0.094
[62,     1] loss: 0.114
[63,     1] loss: 0.108
Early stopping applied (best metric=0.3434774875640869)
Finished Training
Total time taken: 6.937002897262573
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.687
[3,     1] loss: 0.686
[4,     1] loss: 0.658
[5,     1] loss: 0.635
[6,     1] loss: 0.599
[7,     1] loss: 0.574
[8,     1] loss: 0.568
[9,     1] loss: 0.510
[10,     1] loss: 0.516
[11,     1] loss: 0.482
[12,     1] loss: 0.441
[13,     1] loss: 0.422
[14,     1] loss: 0.373
[15,     1] loss: 0.370
[16,     1] loss: 0.367
[17,     1] loss: 0.350
[18,     1] loss: 0.331
[19,     1] loss: 0.330
[20,     1] loss: 0.335
[21,     1] loss: 0.296
[22,     1] loss: 0.397
[23,     1] loss: 0.362
[24,     1] loss: 0.311
[25,     1] loss: 0.247
[26,     1] loss: 0.297
[27,     1] loss: 0.303
[28,     1] loss: 0.280
[29,     1] loss: 0.230
[30,     1] loss: 0.202
[31,     1] loss: 0.269
[32,     1] loss: 0.199
[33,     1] loss: 0.223
[34,     1] loss: 0.219
[35,     1] loss: 0.161
[36,     1] loss: 0.193
[37,     1] loss: 0.186
[38,     1] loss: 0.153
[39,     1] loss: 0.154
[40,     1] loss: 0.180
[41,     1] loss: 0.150
[42,     1] loss: 0.167
[43,     1] loss: 0.147
[44,     1] loss: 0.174
[45,     1] loss: 0.126
[46,     1] loss: 0.163
[47,     1] loss: 0.130
[48,     1] loss: 0.142
[49,     1] loss: 0.134
[50,     1] loss: 0.146
[51,     1] loss: 0.157
[52,     1] loss: 0.110
[53,     1] loss: 0.117
[54,     1] loss: 0.149
[55,     1] loss: 0.136
[56,     1] loss: 0.087
[57,     1] loss: 0.131
[58,     1] loss: 0.139
[59,     1] loss: 0.183
[60,     1] loss: 0.097
[61,     1] loss: 0.143
[62,     1] loss: 0.167
[63,     1] loss: 0.118
Early stopping applied (best metric=0.37582892179489136)
Finished Training
Total time taken: 6.926000118255615
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.684
[4,     1] loss: 0.665
[5,     1] loss: 0.627
[6,     1] loss: 0.585
[7,     1] loss: 0.558
[8,     1] loss: 0.529
[9,     1] loss: 0.467
[10,     1] loss: 0.471
[11,     1] loss: 0.400
[12,     1] loss: 0.425
[13,     1] loss: 0.333
[14,     1] loss: 0.344
[15,     1] loss: 0.363
[16,     1] loss: 0.365
[17,     1] loss: 0.252
[18,     1] loss: 0.288
[19,     1] loss: 0.300
[20,     1] loss: 0.254
[21,     1] loss: 0.293
[22,     1] loss: 0.289
[23,     1] loss: 0.273
[24,     1] loss: 0.242
[25,     1] loss: 0.248
[26,     1] loss: 0.212
[27,     1] loss: 0.194
[28,     1] loss: 0.194
[29,     1] loss: 0.228
[30,     1] loss: 0.205
[31,     1] loss: 0.161
[32,     1] loss: 0.157
[33,     1] loss: 0.185
[34,     1] loss: 0.235
[35,     1] loss: 0.229
[36,     1] loss: 0.172
[37,     1] loss: 0.128
[38,     1] loss: 0.161
[39,     1] loss: 0.167
[40,     1] loss: 0.240
[41,     1] loss: 0.129
[42,     1] loss: 0.126
[43,     1] loss: 0.150
[44,     1] loss: 0.167
[45,     1] loss: 0.145
[46,     1] loss: 0.150
[47,     1] loss: 0.127
[48,     1] loss: 0.145
[49,     1] loss: 0.116
[50,     1] loss: 0.081
[51,     1] loss: 0.153
[52,     1] loss: 0.097
[53,     1] loss: 0.106
[54,     1] loss: 0.142
[55,     1] loss: 0.102
[56,     1] loss: 0.081
[57,     1] loss: 0.105
[58,     1] loss: 0.158
Early stopping applied (best metric=0.4553699493408203)
Finished Training
Total time taken: 6.350999116897583
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.701
[3,     1] loss: 0.680
[4,     1] loss: 0.640
[5,     1] loss: 0.612
[6,     1] loss: 0.558
[7,     1] loss: 0.551
[8,     1] loss: 0.479
[9,     1] loss: 0.452
[10,     1] loss: 0.464
[11,     1] loss: 0.428
[12,     1] loss: 0.365
[13,     1] loss: 0.406
[14,     1] loss: 0.314
[15,     1] loss: 0.341
[16,     1] loss: 0.390
[17,     1] loss: 0.302
[18,     1] loss: 0.278
[19,     1] loss: 0.357
[20,     1] loss: 0.394
[21,     1] loss: 0.319
[22,     1] loss: 0.202
[23,     1] loss: 0.212
[24,     1] loss: 0.208
[25,     1] loss: 0.257
[26,     1] loss: 0.218
[27,     1] loss: 0.257
[28,     1] loss: 0.206
[29,     1] loss: 0.165
[30,     1] loss: 0.170
[31,     1] loss: 0.151
[32,     1] loss: 0.158
[33,     1] loss: 0.127
[34,     1] loss: 0.176
[35,     1] loss: 0.124
[36,     1] loss: 0.119
[37,     1] loss: 0.055
[38,     1] loss: 0.081
[39,     1] loss: 0.169
[40,     1] loss: 0.163
[41,     1] loss: 0.131
[42,     1] loss: 0.140
[43,     1] loss: 0.118
[44,     1] loss: 0.119
[45,     1] loss: 0.131
[46,     1] loss: 0.123
[47,     1] loss: 0.125
[48,     1] loss: 0.127
[49,     1] loss: 0.115
[50,     1] loss: 0.115
[51,     1] loss: 0.125
[52,     1] loss: 0.085
[53,     1] loss: 0.126
[54,     1] loss: 0.098
[55,     1] loss: 0.105
[56,     1] loss: 0.051
[57,     1] loss: 0.098
[58,     1] loss: 0.118
[59,     1] loss: 0.085
[60,     1] loss: 0.112
Early stopping applied (best metric=0.3738585114479065)
Finished Training
Total time taken: 6.62161111831665
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.688
[3,     1] loss: 0.656
[4,     1] loss: 0.609
[5,     1] loss: 0.573
[6,     1] loss: 0.514
[7,     1] loss: 0.457
[8,     1] loss: 0.420
[9,     1] loss: 0.394
[10,     1] loss: 0.304
[11,     1] loss: 0.303
[12,     1] loss: 0.233
[13,     1] loss: 0.264
[14,     1] loss: 0.284
[15,     1] loss: 0.264
[16,     1] loss: 0.286
[17,     1] loss: 0.228
[18,     1] loss: 0.251
[19,     1] loss: 0.186
[20,     1] loss: 0.204
[21,     1] loss: 0.181
[22,     1] loss: 0.195
[23,     1] loss: 0.232
[24,     1] loss: 0.158
[25,     1] loss: 0.307
[26,     1] loss: 0.280
[27,     1] loss: 0.249
[28,     1] loss: 0.260
[29,     1] loss: 0.219
[30,     1] loss: 0.246
[31,     1] loss: 0.231
[32,     1] loss: 0.150
[33,     1] loss: 0.166
[34,     1] loss: 0.280
[35,     1] loss: 0.223
[36,     1] loss: 0.242
[37,     1] loss: 0.181
[38,     1] loss: 0.215
[39,     1] loss: 0.252
[40,     1] loss: 0.149
[41,     1] loss: 0.172
[42,     1] loss: 0.138
[43,     1] loss: 0.155
[44,     1] loss: 0.191
[45,     1] loss: 0.207
[46,     1] loss: 0.221
[47,     1] loss: 0.193
[48,     1] loss: 0.156
[49,     1] loss: 0.190
[50,     1] loss: 0.201
[51,     1] loss: 0.212
[52,     1] loss: 0.240
[53,     1] loss: 0.281
[54,     1] loss: 0.215
[55,     1] loss: 0.181
Early stopping applied (best metric=0.4876861274242401)
Finished Training
Total time taken: 6.03661847114563
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.689
[3,     1] loss: 0.661
[4,     1] loss: 0.629
[5,     1] loss: 0.576
[6,     1] loss: 0.549
[7,     1] loss: 0.541
[8,     1] loss: 0.446
[9,     1] loss: 0.410
[10,     1] loss: 0.371
[11,     1] loss: 0.277
[12,     1] loss: 0.342
[13,     1] loss: 0.302
[14,     1] loss: 0.252
[15,     1] loss: 0.202
[16,     1] loss: 0.222
[17,     1] loss: 0.238
[18,     1] loss: 0.262
[19,     1] loss: 0.413
[20,     1] loss: 0.315
[21,     1] loss: 0.232
[22,     1] loss: 0.218
[23,     1] loss: 0.304
[24,     1] loss: 0.142
[25,     1] loss: 0.221
[26,     1] loss: 0.252
[27,     1] loss: 0.234
[28,     1] loss: 0.173
[29,     1] loss: 0.210
[30,     1] loss: 0.175
[31,     1] loss: 0.182
[32,     1] loss: 0.182
[33,     1] loss: 0.174
[34,     1] loss: 0.144
[35,     1] loss: 0.110
[36,     1] loss: 0.184
[37,     1] loss: 0.096
[38,     1] loss: 0.126
[39,     1] loss: 0.125
[40,     1] loss: 0.135
[41,     1] loss: 0.125
[42,     1] loss: 0.111
[43,     1] loss: 0.101
[44,     1] loss: 0.102
[45,     1] loss: 0.058
[46,     1] loss: 0.057
[47,     1] loss: 0.033
[48,     1] loss: 0.054
[49,     1] loss: 0.070
[50,     1] loss: 0.103
[51,     1] loss: 0.140
[52,     1] loss: 0.103
[53,     1] loss: 0.063
[54,     1] loss: 0.120
[55,     1] loss: 0.078
[56,     1] loss: 0.105
[57,     1] loss: 0.101
Early stopping applied (best metric=0.41147536039352417)
Finished Training
Total time taken: 6.21599817276001
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.688
[3,     1] loss: 0.676
[4,     1] loss: 0.650
[5,     1] loss: 0.614
[6,     1] loss: 0.556
[7,     1] loss: 0.515
[8,     1] loss: 0.511
[9,     1] loss: 0.444
[10,     1] loss: 0.393
[11,     1] loss: 0.382
[12,     1] loss: 0.389
[13,     1] loss: 0.317
[14,     1] loss: 0.376
[15,     1] loss: 0.284
[16,     1] loss: 0.290
[17,     1] loss: 0.228
[18,     1] loss: 0.224
[19,     1] loss: 0.226
[20,     1] loss: 0.247
[21,     1] loss: 0.176
[22,     1] loss: 0.233
[23,     1] loss: 0.189
[24,     1] loss: 0.189
[25,     1] loss: 0.164
[26,     1] loss: 0.165
[27,     1] loss: 0.173
[28,     1] loss: 0.184
[29,     1] loss: 0.169
[30,     1] loss: 0.198
[31,     1] loss: 0.134
[32,     1] loss: 0.153
[33,     1] loss: 0.158
[34,     1] loss: 0.166
[35,     1] loss: 0.099
[36,     1] loss: 0.162
[37,     1] loss: 0.155
[38,     1] loss: 0.140
[39,     1] loss: 0.143
[40,     1] loss: 0.138
[41,     1] loss: 0.151
[42,     1] loss: 0.122
[43,     1] loss: 0.093
[44,     1] loss: 0.137
[45,     1] loss: 0.141
[46,     1] loss: 0.138
[47,     1] loss: 0.157
[48,     1] loss: 0.125
[49,     1] loss: 0.146
[50,     1] loss: 0.108
[51,     1] loss: 0.080
[52,     1] loss: 0.087
[53,     1] loss: 0.142
[54,     1] loss: 0.103
[55,     1] loss: 0.078
[56,     1] loss: 0.119
[57,     1] loss: 0.102
[58,     1] loss: 0.118
[59,     1] loss: 0.073
[60,     1] loss: 0.093
[61,     1] loss: 0.078
[62,     1] loss: 0.117
[63,     1] loss: 0.089
[64,     1] loss: 0.142
Early stopping applied (best metric=0.27351582050323486)
Finished Training
Total time taken: 7.218601703643799
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.684
[3,     1] loss: 0.672
[4,     1] loss: 0.643
[5,     1] loss: 0.591
[6,     1] loss: 0.567
[7,     1] loss: 0.548
[8,     1] loss: 0.501
[9,     1] loss: 0.490
[10,     1] loss: 0.465
[11,     1] loss: 0.438
[12,     1] loss: 0.385
[13,     1] loss: 0.424
[14,     1] loss: 0.408
[15,     1] loss: 0.341
[16,     1] loss: 0.312
[17,     1] loss: 0.401
[18,     1] loss: 0.294
[19,     1] loss: 0.337
[20,     1] loss: 0.355
[21,     1] loss: 0.302
[22,     1] loss: 0.372
[23,     1] loss: 0.310
[24,     1] loss: 0.280
[25,     1] loss: 0.248
[26,     1] loss: 0.273
[27,     1] loss: 0.266
[28,     1] loss: 0.248
[29,     1] loss: 0.271
[30,     1] loss: 0.305
[31,     1] loss: 0.245
[32,     1] loss: 0.278
[33,     1] loss: 0.291
[34,     1] loss: 0.220
[35,     1] loss: 0.241
[36,     1] loss: 0.293
[37,     1] loss: 0.173
[38,     1] loss: 0.202
[39,     1] loss: 0.174
[40,     1] loss: 0.187
[41,     1] loss: 0.179
[42,     1] loss: 0.199
[43,     1] loss: 0.147
[44,     1] loss: 0.129
[45,     1] loss: 0.102
[46,     1] loss: 0.172
[47,     1] loss: 0.145
[48,     1] loss: 0.074
[49,     1] loss: 0.098
[50,     1] loss: 0.142
[51,     1] loss: 0.106
[52,     1] loss: 0.063
[53,     1] loss: 0.135
[54,     1] loss: 0.095
[55,     1] loss: 0.091
[56,     1] loss: 0.099
[57,     1] loss: 0.084
[58,     1] loss: 0.050
[59,     1] loss: 0.054
[60,     1] loss: 0.055
Early stopping applied (best metric=0.3868162930011749)
Finished Training
Total time taken: 6.610001087188721
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.668
[4,     1] loss: 0.633
[5,     1] loss: 0.597
[6,     1] loss: 0.544
[7,     1] loss: 0.509
[8,     1] loss: 0.471
[9,     1] loss: 0.493
[10,     1] loss: 0.395
[11,     1] loss: 0.397
[12,     1] loss: 0.353
[13,     1] loss: 0.345
[14,     1] loss: 0.311
[15,     1] loss: 0.399
[16,     1] loss: 0.292
[17,     1] loss: 0.249
[18,     1] loss: 0.252
[19,     1] loss: 0.241
[20,     1] loss: 0.323
[21,     1] loss: 0.248
[22,     1] loss: 0.260
[23,     1] loss: 0.187
[24,     1] loss: 0.267
[25,     1] loss: 0.248
[26,     1] loss: 0.191
[27,     1] loss: 0.186
[28,     1] loss: 0.211
[29,     1] loss: 0.173
[30,     1] loss: 0.209
[31,     1] loss: 0.201
[32,     1] loss: 0.216
[33,     1] loss: 0.141
[34,     1] loss: 0.123
[35,     1] loss: 0.181
[36,     1] loss: 0.183
[37,     1] loss: 0.134
[38,     1] loss: 0.150
[39,     1] loss: 0.183
[40,     1] loss: 0.146
[41,     1] loss: 0.135
[42,     1] loss: 0.108
[43,     1] loss: 0.121
[44,     1] loss: 0.091
[45,     1] loss: 0.099
[46,     1] loss: 0.135
[47,     1] loss: 0.106
[48,     1] loss: 0.125
[49,     1] loss: 0.146
[50,     1] loss: 0.170
[51,     1] loss: 0.096
[52,     1] loss: 0.112
[53,     1] loss: 0.080
[54,     1] loss: 0.100
[55,     1] loss: 0.086
[56,     1] loss: 0.116
Early stopping applied (best metric=0.4114980101585388)
Finished Training
Total time taken: 6.4010009765625
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.659
[4,     1] loss: 0.615
[5,     1] loss: 0.574
[6,     1] loss: 0.542
[7,     1] loss: 0.499
[8,     1] loss: 0.485
[9,     1] loss: 0.431
[10,     1] loss: 0.464
[11,     1] loss: 0.443
[12,     1] loss: 0.424
[13,     1] loss: 0.411
[14,     1] loss: 0.438
[15,     1] loss: 0.440
[16,     1] loss: 0.387
[17,     1] loss: 0.355
[18,     1] loss: 0.424
[19,     1] loss: 0.381
[20,     1] loss: 0.417
[21,     1] loss: 0.417
[22,     1] loss: 0.367
[23,     1] loss: 0.435
[24,     1] loss: 0.394
[25,     1] loss: 0.394
[26,     1] loss: 0.363
[27,     1] loss: 0.348
[28,     1] loss: 0.379
[29,     1] loss: 0.323
[30,     1] loss: 0.324
[31,     1] loss: 0.296
[32,     1] loss: 0.311
[33,     1] loss: 0.343
[34,     1] loss: 0.323
[35,     1] loss: 0.268
[36,     1] loss: 0.277
[37,     1] loss: 0.210
[38,     1] loss: 0.267
[39,     1] loss: 0.179
[40,     1] loss: 0.184
[41,     1] loss: 0.199
[42,     1] loss: 0.128
[43,     1] loss: 0.157
[44,     1] loss: 0.129
[45,     1] loss: 0.146
[46,     1] loss: 0.195
[47,     1] loss: 0.177
[48,     1] loss: 0.113
[49,     1] loss: 0.127
[50,     1] loss: 0.201
[51,     1] loss: 0.130
[52,     1] loss: 0.108
[53,     1] loss: 0.124
[54,     1] loss: 0.090
[55,     1] loss: 0.106
[56,     1] loss: 0.083
Early stopping applied (best metric=0.48649662733078003)
Finished Training
Total time taken: 6.179001092910767
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.693
[3,     1] loss: 0.676
[4,     1] loss: 0.636
[5,     1] loss: 0.597
[6,     1] loss: 0.582
[7,     1] loss: 0.513
[8,     1] loss: 0.482
[9,     1] loss: 0.434
[10,     1] loss: 0.393
[11,     1] loss: 0.377
[12,     1] loss: 0.284
[13,     1] loss: 0.302
[14,     1] loss: 0.258
[15,     1] loss: 0.234
[16,     1] loss: 0.263
[17,     1] loss: 0.303
[18,     1] loss: 0.226
[19,     1] loss: 0.248
[20,     1] loss: 0.414
[21,     1] loss: 0.234
[22,     1] loss: 0.313
[23,     1] loss: 0.264
[24,     1] loss: 0.245
[25,     1] loss: 0.232
[26,     1] loss: 0.182
[27,     1] loss: 0.163
[28,     1] loss: 0.300
[29,     1] loss: 0.169
[30,     1] loss: 0.209
[31,     1] loss: 0.150
[32,     1] loss: 0.161
[33,     1] loss: 0.162
[34,     1] loss: 0.169
[35,     1] loss: 0.185
[36,     1] loss: 0.166
[37,     1] loss: 0.155
[38,     1] loss: 0.132
[39,     1] loss: 0.124
[40,     1] loss: 0.120
[41,     1] loss: 0.149
[42,     1] loss: 0.192
[43,     1] loss: 0.151
[44,     1] loss: 0.085
[45,     1] loss: 0.145
[46,     1] loss: 0.104
[47,     1] loss: 0.084
[48,     1] loss: 0.107
[49,     1] loss: 0.088
[50,     1] loss: 0.177
[51,     1] loss: 0.090
[52,     1] loss: 0.079
[53,     1] loss: 0.138
[54,     1] loss: 0.067
[55,     1] loss: 0.148
[56,     1] loss: 0.072
[57,     1] loss: 0.120
[58,     1] loss: 0.080
Early stopping applied (best metric=0.3551062047481537)
Finished Training
Total time taken: 6.33899998664856
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.668
[4,     1] loss: 0.631
[5,     1] loss: 0.579
[6,     1] loss: 0.556
[7,     1] loss: 0.499
[8,     1] loss: 0.450
[9,     1] loss: 0.407
[10,     1] loss: 0.379
[11,     1] loss: 0.398
[12,     1] loss: 0.321
[13,     1] loss: 0.299
[14,     1] loss: 0.344
[15,     1] loss: 0.327
[16,     1] loss: 0.312
[17,     1] loss: 0.293
[18,     1] loss: 0.294
[19,     1] loss: 0.307
[20,     1] loss: 0.307
[21,     1] loss: 0.304
[22,     1] loss: 0.278
[23,     1] loss: 0.267
[24,     1] loss: 0.297
[25,     1] loss: 0.240
[26,     1] loss: 0.267
[27,     1] loss: 0.291
[28,     1] loss: 0.323
[29,     1] loss: 0.272
[30,     1] loss: 0.233
[31,     1] loss: 0.238
[32,     1] loss: 0.219
[33,     1] loss: 0.183
[34,     1] loss: 0.172
[35,     1] loss: 0.233
[36,     1] loss: 0.237
[37,     1] loss: 0.181
[38,     1] loss: 0.165
[39,     1] loss: 0.146
[40,     1] loss: 0.199
[41,     1] loss: 0.150
[42,     1] loss: 0.176
[43,     1] loss: 0.167
[44,     1] loss: 0.135
[45,     1] loss: 0.120
[46,     1] loss: 0.137
[47,     1] loss: 0.146
[48,     1] loss: 0.103
[49,     1] loss: 0.125
[50,     1] loss: 0.148
[51,     1] loss: 0.127
[52,     1] loss: 0.112
[53,     1] loss: 0.110
[54,     1] loss: 0.159
[55,     1] loss: 0.202
[56,     1] loss: 0.212
[57,     1] loss: 0.112
Early stopping applied (best metric=0.358954519033432)
Finished Training
Total time taken: 6.2270026206970215
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.682
[4,     1] loss: 0.653
[5,     1] loss: 0.628
[6,     1] loss: 0.594
[7,     1] loss: 0.540
[8,     1] loss: 0.508
[9,     1] loss: 0.456
[10,     1] loss: 0.432
[11,     1] loss: 0.324
[12,     1] loss: 0.330
[13,     1] loss: 0.329
[14,     1] loss: 0.270
[15,     1] loss: 0.318
[16,     1] loss: 0.185
[17,     1] loss: 0.274
[18,     1] loss: 0.204
[19,     1] loss: 0.209
[20,     1] loss: 0.169
[21,     1] loss: 0.181
[22,     1] loss: 0.194
[23,     1] loss: 0.155
[24,     1] loss: 0.202
[25,     1] loss: 0.145
[26,     1] loss: 0.300
[27,     1] loss: 0.143
[28,     1] loss: 0.169
[29,     1] loss: 0.163
[30,     1] loss: 0.147
[31,     1] loss: 0.130
[32,     1] loss: 0.124
[33,     1] loss: 0.158
[34,     1] loss: 0.117
[35,     1] loss: 0.074
[36,     1] loss: 0.147
[37,     1] loss: 0.142
[38,     1] loss: 0.101
[39,     1] loss: 0.104
[40,     1] loss: 0.073
[41,     1] loss: 0.090
[42,     1] loss: 0.082
[43,     1] loss: 0.092
[44,     1] loss: 0.142
[45,     1] loss: 0.150
[46,     1] loss: 0.204
[47,     1] loss: 0.145
[48,     1] loss: 0.095
[49,     1] loss: 0.184
[50,     1] loss: 0.075
[51,     1] loss: 0.083
[52,     1] loss: 0.081
[53,     1] loss: 0.118
[54,     1] loss: 0.102
[55,     1] loss: 0.070
[56,     1] loss: 0.041
[57,     1] loss: 0.042
[58,     1] loss: 0.103
[59,     1] loss: 0.108
[60,     1] loss: 0.080
Early stopping applied (best metric=0.2893607020378113)
Finished Training
Total time taken: 6.5309998989105225
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.689
[3,     1] loss: 0.671
[4,     1] loss: 0.642
[5,     1] loss: 0.609
[6,     1] loss: 0.575
[7,     1] loss: 0.525
[8,     1] loss: 0.516
[9,     1] loss: 0.489
[10,     1] loss: 0.498
[11,     1] loss: 0.438
[12,     1] loss: 0.405
[13,     1] loss: 0.323
[14,     1] loss: 0.378
[15,     1] loss: 0.315
[16,     1] loss: 0.352
[17,     1] loss: 0.350
[18,     1] loss: 0.315
[19,     1] loss: 0.284
[20,     1] loss: 0.216
[21,     1] loss: 0.358
[22,     1] loss: 0.300
[23,     1] loss: 0.340
[24,     1] loss: 0.367
[25,     1] loss: 0.302
[26,     1] loss: 0.277
[27,     1] loss: 0.326
[28,     1] loss: 0.336
[29,     1] loss: 0.215
[30,     1] loss: 0.295
[31,     1] loss: 0.307
[32,     1] loss: 0.307
[33,     1] loss: 0.216
[34,     1] loss: 0.264
[35,     1] loss: 0.271
[36,     1] loss: 0.277
[37,     1] loss: 0.232
[38,     1] loss: 0.168
[39,     1] loss: 0.228
[40,     1] loss: 0.219
[41,     1] loss: 0.214
[42,     1] loss: 0.205
[43,     1] loss: 0.195
[44,     1] loss: 0.168
[45,     1] loss: 0.209
[46,     1] loss: 0.188
[47,     1] loss: 0.203
[48,     1] loss: 0.124
[49,     1] loss: 0.196
[50,     1] loss: 0.208
[51,     1] loss: 0.146
[52,     1] loss: 0.107
[53,     1] loss: 0.165
[54,     1] loss: 0.184
[55,     1] loss: 0.126
[56,     1] loss: 0.123
[57,     1] loss: 0.146
[58,     1] loss: 0.137
[59,     1] loss: 0.082
[60,     1] loss: 0.085
Early stopping applied (best metric=0.4491799473762512)
Finished Training
Total time taken: 6.607000350952148
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.683
[3,     1] loss: 0.680
[4,     1] loss: 0.652
[5,     1] loss: 0.618
[6,     1] loss: 0.576
[7,     1] loss: 0.502
[8,     1] loss: 0.501
[9,     1] loss: 0.456
[10,     1] loss: 0.396
[11,     1] loss: 0.351
[12,     1] loss: 0.355
[13,     1] loss: 0.310
[14,     1] loss: 0.323
[15,     1] loss: 0.331
[16,     1] loss: 0.336
[17,     1] loss: 0.191
[18,     1] loss: 0.287
[19,     1] loss: 0.336
[20,     1] loss: 0.294
[21,     1] loss: 0.293
[22,     1] loss: 0.195
[23,     1] loss: 0.248
[24,     1] loss: 0.187
[25,     1] loss: 0.355
[26,     1] loss: 0.267
[27,     1] loss: 0.263
[28,     1] loss: 0.212
[29,     1] loss: 0.165
[30,     1] loss: 0.259
[31,     1] loss: 0.134
[32,     1] loss: 0.180
[33,     1] loss: 0.183
[34,     1] loss: 0.151
[35,     1] loss: 0.269
[36,     1] loss: 0.193
[37,     1] loss: 0.184
[38,     1] loss: 0.184
[39,     1] loss: 0.157
[40,     1] loss: 0.190
[41,     1] loss: 0.198
[42,     1] loss: 0.137
[43,     1] loss: 0.155
[44,     1] loss: 0.152
[45,     1] loss: 0.134
[46,     1] loss: 0.087
[47,     1] loss: 0.136
[48,     1] loss: 0.123
[49,     1] loss: 0.116
[50,     1] loss: 0.143
[51,     1] loss: 0.115
[52,     1] loss: 0.180
[53,     1] loss: 0.132
[54,     1] loss: 0.122
[55,     1] loss: 0.085
[56,     1] loss: 0.110
[57,     1] loss: 0.107
[58,     1] loss: 0.096
[59,     1] loss: 0.098
[60,     1] loss: 0.135
Early stopping applied (best metric=0.38618117570877075)
Finished Training
Total time taken: 6.538602828979492
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.670
[4,     1] loss: 0.640
[5,     1] loss: 0.589
[6,     1] loss: 0.546
[7,     1] loss: 0.549
[8,     1] loss: 0.467
[9,     1] loss: 0.390
[10,     1] loss: 0.416
[11,     1] loss: 0.384
[12,     1] loss: 0.358
[13,     1] loss: 0.344
[14,     1] loss: 0.263
[15,     1] loss: 0.290
[16,     1] loss: 0.367
[17,     1] loss: 0.314
[18,     1] loss: 0.269
[19,     1] loss: 0.187
[20,     1] loss: 0.279
[21,     1] loss: 0.242
[22,     1] loss: 0.193
[23,     1] loss: 0.194
[24,     1] loss: 0.200
[25,     1] loss: 0.162
[26,     1] loss: 0.193
[27,     1] loss: 0.143
[28,     1] loss: 0.102
[29,     1] loss: 0.182
[30,     1] loss: 0.177
[31,     1] loss: 0.140
[32,     1] loss: 0.155
[33,     1] loss: 0.136
[34,     1] loss: 0.174
[35,     1] loss: 0.135
[36,     1] loss: 0.138
[37,     1] loss: 0.143
[38,     1] loss: 0.094
[39,     1] loss: 0.159
[40,     1] loss: 0.125
[41,     1] loss: 0.152
[42,     1] loss: 0.062
[43,     1] loss: 0.088
[44,     1] loss: 0.134
[45,     1] loss: 0.151
[46,     1] loss: 0.165
[47,     1] loss: 0.130
[48,     1] loss: 0.118
[49,     1] loss: 0.101
[50,     1] loss: 0.156
[51,     1] loss: 0.083
[52,     1] loss: 0.124
[53,     1] loss: 0.146
[54,     1] loss: 0.118
[55,     1] loss: 0.081
Early stopping applied (best metric=0.47730720043182373)
Finished Training
Total time taken: 6.0510008335113525
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.678
[4,     1] loss: 0.663
[5,     1] loss: 0.629
[6,     1] loss: 0.594
[7,     1] loss: 0.577
[8,     1] loss: 0.550
[9,     1] loss: 0.488
[10,     1] loss: 0.435
[11,     1] loss: 0.427
[12,     1] loss: 0.429
[13,     1] loss: 0.326
[14,     1] loss: 0.413
[15,     1] loss: 0.263
[16,     1] loss: 0.328
[17,     1] loss: 0.363
[18,     1] loss: 0.280
[19,     1] loss: 0.365
[20,     1] loss: 0.416
[21,     1] loss: 0.334
[22,     1] loss: 0.365
[23,     1] loss: 0.422
[24,     1] loss: 0.324
[25,     1] loss: 0.375
[26,     1] loss: 0.354
[27,     1] loss: 0.346
[28,     1] loss: 0.375
[29,     1] loss: 0.244
[30,     1] loss: 0.238
[31,     1] loss: 0.268
[32,     1] loss: 0.293
[33,     1] loss: 0.266
[34,     1] loss: 0.272
[35,     1] loss: 0.216
[36,     1] loss: 0.233
[37,     1] loss: 0.195
[38,     1] loss: 0.211
[39,     1] loss: 0.167
[40,     1] loss: 0.200
[41,     1] loss: 0.205
[42,     1] loss: 0.134
[43,     1] loss: 0.170
[44,     1] loss: 0.165
[45,     1] loss: 0.182
[46,     1] loss: 0.111
[47,     1] loss: 0.125
[48,     1] loss: 0.190
[49,     1] loss: 0.234
[50,     1] loss: 0.161
[51,     1] loss: 0.157
[52,     1] loss: 0.128
[53,     1] loss: 0.153
[54,     1] loss: 0.080
[55,     1] loss: 0.140
[56,     1] loss: 0.071
[57,     1] loss: 0.110
[58,     1] loss: 0.194
[59,     1] loss: 0.092
[60,     1] loss: 0.110
[61,     1] loss: 0.102
[62,     1] loss: 0.116
[63,     1] loss: 0.103
[64,     1] loss: 0.115
[65,     1] loss: 0.123
[66,     1] loss: 0.122
[67,     1] loss: 0.128
[68,     1] loss: 0.116
[69,     1] loss: 0.081
[70,     1] loss: 0.093
[71,     1] loss: 0.137
[72,     1] loss: 0.096
[73,     1] loss: 0.077
[74,     1] loss: 0.080
[75,     1] loss: 0.119
[76,     1] loss: 0.118
[77,     1] loss: 0.134
[78,     1] loss: 0.120
[79,     1] loss: 0.068
[80,     1] loss: 0.117
[81,     1] loss: 0.073
[82,     1] loss: 0.070
[83,     1] loss: 0.055
[84,     1] loss: 0.060
[85,     1] loss: 0.068
[86,     1] loss: 0.084
[87,     1] loss: 0.056
[88,     1] loss: 0.044
[89,     1] loss: 0.086
[90,     1] loss: 0.064
[91,     1] loss: 0.050
[92,     1] loss: 0.075
[93,     1] loss: 0.075
[94,     1] loss: 0.042
[95,     1] loss: 0.054
[96,     1] loss: 0.072
[97,     1] loss: 0.071
[98,     1] loss: 0.047
Early stopping applied (best metric=0.04659034684300423)
Finished Training
Total time taken: 10.65600037574768
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.693
[3,     1] loss: 0.687
[4,     1] loss: 0.676
[5,     1] loss: 0.651
[6,     1] loss: 0.618
[7,     1] loss: 0.578
[8,     1] loss: 0.580
[9,     1] loss: 0.504
[10,     1] loss: 0.506
[11,     1] loss: 0.457
[12,     1] loss: 0.400
[13,     1] loss: 0.404
[14,     1] loss: 0.399
[15,     1] loss: 0.407
[16,     1] loss: 0.282
[17,     1] loss: 0.306
[18,     1] loss: 0.376
[19,     1] loss: 0.275
[20,     1] loss: 0.328
[21,     1] loss: 0.259
[22,     1] loss: 0.262
[23,     1] loss: 0.191
[24,     1] loss: 0.222
[25,     1] loss: 0.252
[26,     1] loss: 0.203
[27,     1] loss: 0.185
[28,     1] loss: 0.247
[29,     1] loss: 0.251
[30,     1] loss: 0.186
[31,     1] loss: 0.169
[32,     1] loss: 0.138
[33,     1] loss: 0.165
[34,     1] loss: 0.167
[35,     1] loss: 0.170
[36,     1] loss: 0.147
[37,     1] loss: 0.186
[38,     1] loss: 0.160
[39,     1] loss: 0.132
[40,     1] loss: 0.139
[41,     1] loss: 0.161
[42,     1] loss: 0.188
[43,     1] loss: 0.115
[44,     1] loss: 0.194
[45,     1] loss: 0.106
[46,     1] loss: 0.160
[47,     1] loss: 0.130
[48,     1] loss: 0.115
[49,     1] loss: 0.112
[50,     1] loss: 0.170
[51,     1] loss: 0.123
[52,     1] loss: 0.142
[53,     1] loss: 0.143
[54,     1] loss: 0.113
[55,     1] loss: 0.161
[56,     1] loss: 0.121
[57,     1] loss: 0.125
[58,     1] loss: 0.096
[59,     1] loss: 0.093
[60,     1] loss: 0.145
[61,     1] loss: 0.076
[62,     1] loss: 0.120
[63,     1] loss: 0.099
[64,     1] loss: 0.067
[65,     1] loss: 0.128
[66,     1] loss: 0.138
[67,     1] loss: 0.098
[68,     1] loss: 0.102
[69,     1] loss: 0.088
[70,     1] loss: 0.114
[71,     1] loss: 0.057
[72,     1] loss: 0.054
[73,     1] loss: 0.062
[74,     1] loss: 0.069
[75,     1] loss: 0.062
[76,     1] loss: 0.049
[77,     1] loss: 0.063
[78,     1] loss: 0.058
[79,     1] loss: 0.030
[80,     1] loss: 0.048
[81,     1] loss: 0.071
[82,     1] loss: 0.052
[83,     1] loss: 0.082
[84,     1] loss: 0.126
[85,     1] loss: 0.062
[86,     1] loss: 0.046
[87,     1] loss: 0.063
[88,     1] loss: 0.045
[89,     1] loss: 0.045
[90,     1] loss: 0.074
[91,     1] loss: 0.081
[92,     1] loss: 0.064
[93,     1] loss: 0.036
Early stopping applied (best metric=0.07638653367757797)
Finished Training
Total time taken: 10.180611371994019
{'Hydroxylation-K Validation Accuracy': 0.8169858156028369, 'Hydroxylation-K Validation Sensitivity': 0.8484444444444444, 'Hydroxylation-K Validation Specificity': 0.8094736842105263, 'Hydroxylation-K Validation Precision': 0.5418312655696557, 'Hydroxylation-K AUC ROC': 0.8314970760233918, 'Hydroxylation-K AUC PR': 0.56042445216548, 'Hydroxylation-K MCC': 0.5708670381851523, 'Hydroxylation-K F1': 0.6561625827357576, 'Validation Loss (Hydroxylation-K)': 0.34983736082911493, 'Validation Loss (total)': 0.34983736082911493, 'TimeToTrain': 6.852053155899048}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019078554480939569,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3925840685,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.27036720010919}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006343499976551833,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1116731727,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.991597473461667}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.689
[4,     1] loss: 0.679
[5,     1] loss: 0.653
[6,     1] loss: 0.626
[7,     1] loss: 0.613
[8,     1] loss: 0.568
[9,     1] loss: 0.528
[10,     1] loss: 0.510
[11,     1] loss: 0.448
[12,     1] loss: 0.453
[13,     1] loss: 0.394
[14,     1] loss: 0.382
[15,     1] loss: 0.400
[16,     1] loss: 0.440
[17,     1] loss: 0.484
[18,     1] loss: 0.360
[19,     1] loss: 0.333
[20,     1] loss: 0.316
[21,     1] loss: 0.286
[22,     1] loss: 0.289
[23,     1] loss: 0.339
[24,     1] loss: 0.340
[25,     1] loss: 0.338
[26,     1] loss: 0.311
[27,     1] loss: 0.345
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005405840256580718,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 120589206,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.259218875490102}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.692
[3,     1] loss: 0.674
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00681956097931247,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3458382793,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9463372524240998}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.690
[3,     1] loss: 0.644
[4,     1] loss: 0.587
[5,     1] loss: 0.547
[6,     1] loss: 0.498
[7,     1] loss: 0.419
[8,     1] loss: 0.411
[9,     1] loss: 0.395
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009918215164312471,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4076558084,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.694427943439073}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.694
[3,     1] loss: 0.684
[4,     1] loss: 0.665
[5,     1] loss: 0.619
[6,     1] loss: 0.572
[7,     1] loss: 0.525
[8,     1] loss: 0.500
[9,     1] loss: 0.378
[10,     1] loss: 0.403
[11,     1] loss: 0.387
[12,     1] loss: 0.317
[13,     1] loss: 0.313
[14,     1] loss: 0.241
[15,     1] loss: 0.285
[16,     1] loss: 0.276
[17,     1] loss: 0.258
[18,     1] loss: 0.213
[19,     1] loss: 0.229
[20,     1] loss: 0.223
[21,     1] loss: 0.223
[22,     1] loss: 0.222
[23,     1] loss: 0.176
[24,     1] loss: 0.143
[25,     1] loss: 0.190
[26,     1] loss: 0.112
[27,     1] loss: 0.175
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004579533525655972,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1710696171,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.434049683857962}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.692
[3,     1] loss: 0.681
[4,     1] loss: 0.649
[5,     1] loss: 0.620
[6,     1] loss: 0.577
[7,     1] loss: 0.534
[8,     1] loss: 0.501
[9,     1] loss: 0.478
[10,     1] loss: 0.419
[11,     1] loss: 0.387
[12,     1] loss: 0.359
[13,     1] loss: 0.328
[14,     1] loss: 0.289
[15,     1] loss: 0.253
[16,     1] loss: 0.330
[17,     1] loss: 0.353
[18,     1] loss: 0.247
[19,     1] loss: 0.316
[20,     1] loss: 0.309
[21,     1] loss: 0.285
[22,     1] loss: 0.303
[23,     1] loss: 0.284
[24,     1] loss: 0.259
[25,     1] loss: 0.262
[26,     1] loss: 0.301
[27,     1] loss: 0.248
[28,     1] loss: 0.229
[29,     1] loss: 0.240
[30,     1] loss: 0.241
[31,     1] loss: 0.194
[32,     1] loss: 0.214
[33,     1] loss: 0.234
[34,     1] loss: 0.219
[35,     1] loss: 0.217
[36,     1] loss: 0.234
[37,     1] loss: 0.213
[38,     1] loss: 0.244
[39,     1] loss: 0.235
[40,     1] loss: 0.217
[41,     1] loss: 0.193
[42,     1] loss: 0.208
[43,     1] loss: 0.184
[44,     1] loss: 0.186
[45,     1] loss: 0.223
[46,     1] loss: 0.199
[47,     1] loss: 0.155
[48,     1] loss: 0.176
[49,     1] loss: 0.157
[50,     1] loss: 0.172
[51,     1] loss: 0.192
[52,     1] loss: 0.118
[53,     1] loss: 0.201
[54,     1] loss: 0.152
[55,     1] loss: 0.244
[56,     1] loss: 0.178
[57,     1] loss: 0.166
[58,     1] loss: 0.140
[59,     1] loss: 0.121
[60,     1] loss: 0.130
[61,     1] loss: 0.134
[62,     1] loss: 0.159
[63,     1] loss: 0.129
Early stopping applied (best metric=0.2631833553314209)
Finished Training
Total time taken: 6.914659023284912
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.668
[4,     1] loss: 0.634
[5,     1] loss: 0.605
[6,     1] loss: 0.561
[7,     1] loss: 0.486
[8,     1] loss: 0.493
[9,     1] loss: 0.479
[10,     1] loss: 0.433
[11,     1] loss: 0.394
[12,     1] loss: 0.374
[13,     1] loss: 0.373
[14,     1] loss: 0.315
[15,     1] loss: 0.363
[16,     1] loss: 0.323
[17,     1] loss: 0.316
[18,     1] loss: 0.264
[19,     1] loss: 0.319
[20,     1] loss: 0.247
[21,     1] loss: 0.267
[22,     1] loss: 0.286
[23,     1] loss: 0.300
[24,     1] loss: 0.285
[25,     1] loss: 0.277
[26,     1] loss: 0.238
[27,     1] loss: 0.247
[28,     1] loss: 0.226
[29,     1] loss: 0.226
[30,     1] loss: 0.198
[31,     1] loss: 0.184
[32,     1] loss: 0.195
[33,     1] loss: 0.188
[34,     1] loss: 0.161
[35,     1] loss: 0.203
[36,     1] loss: 0.218
[37,     1] loss: 0.165
[38,     1] loss: 0.132
[39,     1] loss: 0.155
[40,     1] loss: 0.128
[41,     1] loss: 0.164
[42,     1] loss: 0.156
[43,     1] loss: 0.234
[44,     1] loss: 0.165
[45,     1] loss: 0.135
[46,     1] loss: 0.118
[47,     1] loss: 0.130
[48,     1] loss: 0.154
[49,     1] loss: 0.182
[50,     1] loss: 0.281
[51,     1] loss: 0.155
[52,     1] loss: 0.173
[53,     1] loss: 0.188
[54,     1] loss: 0.143
[55,     1] loss: 0.161
[56,     1] loss: 0.208
[57,     1] loss: 0.194
[58,     1] loss: 0.164
[59,     1] loss: 0.160
[60,     1] loss: 0.118
[61,     1] loss: 0.136
[62,     1] loss: 0.148
[63,     1] loss: 0.177
Early stopping applied (best metric=0.42780718207359314)
Finished Training
Total time taken: 6.890000581741333
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.686
[4,     1] loss: 0.650
[5,     1] loss: 0.623
[6,     1] loss: 0.618
[7,     1] loss: 0.570
[8,     1] loss: 0.597
[9,     1] loss: 0.532
[10,     1] loss: 0.499
[11,     1] loss: 0.498
[12,     1] loss: 0.478
[13,     1] loss: 0.446
[14,     1] loss: 0.425
[15,     1] loss: 0.470
[16,     1] loss: 0.417
[17,     1] loss: 0.385
[18,     1] loss: 0.424
[19,     1] loss: 0.378
[20,     1] loss: 0.358
[21,     1] loss: 0.383
[22,     1] loss: 0.368
[23,     1] loss: 0.376
[24,     1] loss: 0.328
[25,     1] loss: 0.296
[26,     1] loss: 0.296
[27,     1] loss: 0.315
[28,     1] loss: 0.240
[29,     1] loss: 0.242
[30,     1] loss: 0.214
[31,     1] loss: 0.241
[32,     1] loss: 0.134
[33,     1] loss: 0.164
[34,     1] loss: 0.195
[35,     1] loss: 0.172
[36,     1] loss: 0.251
[37,     1] loss: 0.152
[38,     1] loss: 0.152
[39,     1] loss: 0.195
[40,     1] loss: 0.162
[41,     1] loss: 0.153
[42,     1] loss: 0.161
[43,     1] loss: 0.148
[44,     1] loss: 0.172
[45,     1] loss: 0.147
[46,     1] loss: 0.098
[47,     1] loss: 0.126
[48,     1] loss: 0.090
[49,     1] loss: 0.114
[50,     1] loss: 0.166
[51,     1] loss: 0.147
[52,     1] loss: 0.169
[53,     1] loss: 0.130
[54,     1] loss: 0.195
[55,     1] loss: 0.110
[56,     1] loss: 0.218
[57,     1] loss: 0.149
[58,     1] loss: 0.145
[59,     1] loss: 0.220
[60,     1] loss: 0.161
[61,     1] loss: 0.307
[62,     1] loss: 0.224
[63,     1] loss: 0.176
[64,     1] loss: 0.223
[65,     1] loss: 0.176
[66,     1] loss: 0.158
[67,     1] loss: 0.181
[68,     1] loss: 0.155
[69,     1] loss: 0.162
[70,     1] loss: 0.131
[71,     1] loss: 0.155
[72,     1] loss: 0.104
[73,     1] loss: 0.118
[74,     1] loss: 0.121
[75,     1] loss: 0.107
[76,     1] loss: 0.088
[77,     1] loss: 0.177
[78,     1] loss: 0.217
[79,     1] loss: 0.304
[80,     1] loss: 0.229
[81,     1] loss: 0.263
[82,     1] loss: 0.226
[83,     1] loss: 0.143
[84,     1] loss: 0.161
[85,     1] loss: 0.221
[86,     1] loss: 0.218
[87,     1] loss: 0.185
[88,     1] loss: 0.175
Early stopping applied (best metric=0.21179187297821045)
Finished Training
Total time taken: 9.651000022888184
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.686
[3,     1] loss: 0.655
[4,     1] loss: 0.636
[5,     1] loss: 0.572
[6,     1] loss: 0.541
[7,     1] loss: 0.477
[8,     1] loss: 0.461
[9,     1] loss: 0.423
[10,     1] loss: 0.420
[11,     1] loss: 0.393
[12,     1] loss: 0.319
[13,     1] loss: 0.320
[14,     1] loss: 0.252
[15,     1] loss: 0.292
[16,     1] loss: 0.280
[17,     1] loss: 0.245
[18,     1] loss: 0.270
[19,     1] loss: 0.335
[20,     1] loss: 0.319
[21,     1] loss: 0.226
[22,     1] loss: 0.183
[23,     1] loss: 0.232
[24,     1] loss: 0.200
[25,     1] loss: 0.208
[26,     1] loss: 0.225
[27,     1] loss: 0.202
[28,     1] loss: 0.212
[29,     1] loss: 0.209
[30,     1] loss: 0.160
[31,     1] loss: 0.168
[32,     1] loss: 0.158
[33,     1] loss: 0.165
[34,     1] loss: 0.132
[35,     1] loss: 0.167
[36,     1] loss: 0.142
[37,     1] loss: 0.155
[38,     1] loss: 0.159
[39,     1] loss: 0.156
[40,     1] loss: 0.162
[41,     1] loss: 0.148
[42,     1] loss: 0.213
[43,     1] loss: 0.143
[44,     1] loss: 0.193
[45,     1] loss: 0.113
[46,     1] loss: 0.110
[47,     1] loss: 0.099
[48,     1] loss: 0.144
[49,     1] loss: 0.145
[50,     1] loss: 0.148
[51,     1] loss: 0.141
[52,     1] loss: 0.208
[53,     1] loss: 0.126
[54,     1] loss: 0.200
[55,     1] loss: 0.128
[56,     1] loss: 0.167
[57,     1] loss: 0.187
[58,     1] loss: 0.152
[59,     1] loss: 0.118
[60,     1] loss: 0.118
[61,     1] loss: 0.142
[62,     1] loss: 0.187
[63,     1] loss: 0.156
[64,     1] loss: 0.150
[65,     1] loss: 0.220
[66,     1] loss: 0.114
[67,     1] loss: 0.159
[68,     1] loss: 0.162
[69,     1] loss: 0.086
[70,     1] loss: 0.113
[71,     1] loss: 0.112
[72,     1] loss: 0.183
[73,     1] loss: 0.155
[74,     1] loss: 0.062
[75,     1] loss: 0.102
[76,     1] loss: 0.103
[77,     1] loss: 0.117
[78,     1] loss: 0.079
[79,     1] loss: 0.167
[80,     1] loss: 0.124
[81,     1] loss: 0.175
[82,     1] loss: 0.171
[83,     1] loss: 0.111
[84,     1] loss: 0.088
[85,     1] loss: 0.094
[86,     1] loss: 0.115
[87,     1] loss: 0.137
[88,     1] loss: 0.105
[89,     1] loss: 0.121
[90,     1] loss: 0.083
[91,     1] loss: 0.147
[92,     1] loss: 0.092
[93,     1] loss: 0.131
[94,     1] loss: 0.144
[95,     1] loss: 0.190
[96,     1] loss: 0.212
[97,     1] loss: 0.123
[98,     1] loss: 0.151
[99,     1] loss: 0.121
[100,     1] loss: 0.131
[101,     1] loss: 0.142
[102,     1] loss: 0.099
Early stopping applied (best metric=0.373483806848526)
Finished Training
Total time taken: 11.103003025054932
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.690
[3,     1] loss: 0.655
[4,     1] loss: 0.624
[5,     1] loss: 0.590
[6,     1] loss: 0.542
[7,     1] loss: 0.525
[8,     1] loss: 0.463
[9,     1] loss: 0.420
[10,     1] loss: 0.401
[11,     1] loss: 0.380
[12,     1] loss: 0.364
[13,     1] loss: 0.353
[14,     1] loss: 0.304
[15,     1] loss: 0.318
[16,     1] loss: 0.352
[17,     1] loss: 0.325
[18,     1] loss: 0.303
[19,     1] loss: 0.354
[20,     1] loss: 0.355
[21,     1] loss: 0.285
[22,     1] loss: 0.332
[23,     1] loss: 0.311
[24,     1] loss: 0.307
[25,     1] loss: 0.325
[26,     1] loss: 0.269
[27,     1] loss: 0.303
[28,     1] loss: 0.268
[29,     1] loss: 0.320
[30,     1] loss: 0.261
[31,     1] loss: 0.303
[32,     1] loss: 0.236
[33,     1] loss: 0.244
[34,     1] loss: 0.250
[35,     1] loss: 0.197
[36,     1] loss: 0.191
[37,     1] loss: 0.173
[38,     1] loss: 0.203
[39,     1] loss: 0.142
[40,     1] loss: 0.131
[41,     1] loss: 0.277
[42,     1] loss: 0.186
[43,     1] loss: 0.156
[44,     1] loss: 0.161
[45,     1] loss: 0.098
[46,     1] loss: 0.140
[47,     1] loss: 0.143
[48,     1] loss: 0.097
[49,     1] loss: 0.179
[50,     1] loss: 0.132
[51,     1] loss: 0.103
[52,     1] loss: 0.080
[53,     1] loss: 0.099
[54,     1] loss: 0.105
[55,     1] loss: 0.090
[56,     1] loss: 0.112
[57,     1] loss: 0.102
[58,     1] loss: 0.111
[59,     1] loss: 0.137
[60,     1] loss: 0.070
[61,     1] loss: 0.110
[62,     1] loss: 0.132
[63,     1] loss: 0.112
[64,     1] loss: 0.069
[65,     1] loss: 0.144
[66,     1] loss: 0.110
[67,     1] loss: 0.102
[68,     1] loss: 0.117
[69,     1] loss: 0.151
[70,     1] loss: 0.117
Early stopping applied (best metric=0.26753324270248413)
Finished Training
Total time taken: 7.70199990272522
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.661
[4,     1] loss: 0.648
[5,     1] loss: 0.616
[6,     1] loss: 0.585
[7,     1] loss: 0.549
[8,     1] loss: 0.503
[9,     1] loss: 0.497
[10,     1] loss: 0.439
[11,     1] loss: 0.481
[12,     1] loss: 0.466
[13,     1] loss: 0.392
[14,     1] loss: 0.375
[15,     1] loss: 0.340
[16,     1] loss: 0.473
[17,     1] loss: 0.364
[18,     1] loss: 0.497
[19,     1] loss: 0.425
[20,     1] loss: 0.362
[21,     1] loss: 0.341
[22,     1] loss: 0.312
[23,     1] loss: 0.324
[24,     1] loss: 0.294
[25,     1] loss: 0.264
[26,     1] loss: 0.331
[27,     1] loss: 0.279
[28,     1] loss: 0.268
[29,     1] loss: 0.230
[30,     1] loss: 0.211
[31,     1] loss: 0.204
[32,     1] loss: 0.181
[33,     1] loss: 0.211
[34,     1] loss: 0.136
[35,     1] loss: 0.150
[36,     1] loss: 0.125
[37,     1] loss: 0.138
[38,     1] loss: 0.158
[39,     1] loss: 0.148
[40,     1] loss: 0.092
[41,     1] loss: 0.127
[42,     1] loss: 0.217
[43,     1] loss: 0.067
[44,     1] loss: 0.221
[45,     1] loss: 0.303
[46,     1] loss: 0.268
[47,     1] loss: 0.236
[48,     1] loss: 0.212
[49,     1] loss: 0.221
[50,     1] loss: 0.200
[51,     1] loss: 0.210
[52,     1] loss: 0.195
[53,     1] loss: 0.205
[54,     1] loss: 0.178
[55,     1] loss: 0.164
[56,     1] loss: 0.160
[57,     1] loss: 0.131
[58,     1] loss: 0.123
[59,     1] loss: 0.138
[60,     1] loss: 0.162
[61,     1] loss: 0.194
[62,     1] loss: 0.124
[63,     1] loss: 0.166
[64,     1] loss: 0.118
[65,     1] loss: 0.155
[66,     1] loss: 0.187
[67,     1] loss: 0.138
[68,     1] loss: 0.113
[69,     1] loss: 0.156
[70,     1] loss: 0.112
[71,     1] loss: 0.114
[72,     1] loss: 0.132
[73,     1] loss: 0.101
[74,     1] loss: 0.095
[75,     1] loss: 0.114
[76,     1] loss: 0.135
[77,     1] loss: 0.083
[78,     1] loss: 0.072
[79,     1] loss: 0.136
[80,     1] loss: 0.322
[81,     1] loss: 0.326
[82,     1] loss: 0.240
[83,     1] loss: 0.312
[84,     1] loss: 0.159
[85,     1] loss: 0.265
[86,     1] loss: 0.226
[87,     1] loss: 0.254
[88,     1] loss: 0.254
[89,     1] loss: 0.192
[90,     1] loss: 0.161
[91,     1] loss: 0.215
[92,     1] loss: 0.197
[93,     1] loss: 0.182
[94,     1] loss: 0.192
[95,     1] loss: 0.155
[96,     1] loss: 0.177
[97,     1] loss: 0.168
[98,     1] loss: 0.135
[99,     1] loss: 0.169
[100,     1] loss: 0.100
[101,     1] loss: 0.128
[102,     1] loss: 0.103
[103,     1] loss: 0.164
[104,     1] loss: 0.149
[105,     1] loss: 0.135
[106,     1] loss: 0.124
[107,     1] loss: 0.145
[108,     1] loss: 0.179
[109,     1] loss: 0.154
[110,     1] loss: 0.294
[111,     1] loss: 0.185
[112,     1] loss: 0.167
[113,     1] loss: 0.150
[114,     1] loss: 0.181
[115,     1] loss: 0.165
[116,     1] loss: 0.175
[117,     1] loss: 0.177
[118,     1] loss: 0.161
[119,     1] loss: 0.108
[120,     1] loss: 0.152
[121,     1] loss: 0.121
[122,     1] loss: 0.160
[123,     1] loss: 0.147
[124,     1] loss: 0.161
[125,     1] loss: 0.127
[126,     1] loss: 0.129
[127,     1] loss: 0.146
[128,     1] loss: 0.122
[129,     1] loss: 0.117
[130,     1] loss: 0.149
[131,     1] loss: 0.163
[132,     1] loss: 0.140
[133,     1] loss: 0.113
[134,     1] loss: 0.141
Early stopping applied (best metric=0.30775120854377747)
Finished Training
Total time taken: 14.53361439704895
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.676
[4,     1] loss: 0.624
[5,     1] loss: 0.596
[6,     1] loss: 0.574
[7,     1] loss: 0.520
[8,     1] loss: 0.491
[9,     1] loss: 0.417
[10,     1] loss: 0.389
[11,     1] loss: 0.357
[12,     1] loss: 0.294
[13,     1] loss: 0.279
[14,     1] loss: 0.282
[15,     1] loss: 0.302
[16,     1] loss: 0.253
[17,     1] loss: 0.225
[18,     1] loss: 0.214
[19,     1] loss: 0.178
[20,     1] loss: 0.237
[21,     1] loss: 0.268
[22,     1] loss: 0.175
[23,     1] loss: 0.333
[24,     1] loss: 0.202
[25,     1] loss: 0.265
[26,     1] loss: 0.235
[27,     1] loss: 0.203
[28,     1] loss: 0.251
[29,     1] loss: 0.242
[30,     1] loss: 0.225
[31,     1] loss: 0.241
[32,     1] loss: 0.220
[33,     1] loss: 0.202
[34,     1] loss: 0.197
[35,     1] loss: 0.211
[36,     1] loss: 0.197
[37,     1] loss: 0.171
[38,     1] loss: 0.196
[39,     1] loss: 0.287
[40,     1] loss: 0.181
[41,     1] loss: 0.151
[42,     1] loss: 0.217
[43,     1] loss: 0.200
[44,     1] loss: 0.199
[45,     1] loss: 0.179
[46,     1] loss: 0.183
[47,     1] loss: 0.177
[48,     1] loss: 0.217
[49,     1] loss: 0.159
[50,     1] loss: 0.136
[51,     1] loss: 0.188
[52,     1] loss: 0.152
[53,     1] loss: 0.168
[54,     1] loss: 0.123
[55,     1] loss: 0.266
[56,     1] loss: 0.104
[57,     1] loss: 0.143
[58,     1] loss: 0.159
[59,     1] loss: 0.102
[60,     1] loss: 0.150
[61,     1] loss: 0.162
[62,     1] loss: 0.130
[63,     1] loss: 0.108
[64,     1] loss: 0.122
[65,     1] loss: 0.172
[66,     1] loss: 0.158
[67,     1] loss: 0.162
[68,     1] loss: 0.205
[69,     1] loss: 0.215
[70,     1] loss: 0.303
[71,     1] loss: 0.220
[72,     1] loss: 0.226
[73,     1] loss: 0.255
[74,     1] loss: 0.199
[75,     1] loss: 0.191
[76,     1] loss: 0.204
[77,     1] loss: 0.219
[78,     1] loss: 0.186
[79,     1] loss: 0.166
[80,     1] loss: 0.158
[81,     1] loss: 0.167
[82,     1] loss: 0.166
[83,     1] loss: 0.167
[84,     1] loss: 0.178
[85,     1] loss: 0.173
[86,     1] loss: 0.130
[87,     1] loss: 0.157
[88,     1] loss: 0.144
[89,     1] loss: 0.170
[90,     1] loss: 0.132
[91,     1] loss: 0.158
[92,     1] loss: 0.162
[93,     1] loss: 0.165
[94,     1] loss: 0.123
[95,     1] loss: 0.131
[96,     1] loss: 0.128
[97,     1] loss: 0.156
[98,     1] loss: 0.125
[99,     1] loss: 0.166
[100,     1] loss: 0.122
[101,     1] loss: 0.140
[102,     1] loss: 0.142
[103,     1] loss: 0.153
[104,     1] loss: 0.117
[105,     1] loss: 0.162
[106,     1] loss: 0.133
[107,     1] loss: 0.180
[108,     1] loss: 0.125
[109,     1] loss: 0.101
[110,     1] loss: 0.103
[111,     1] loss: 0.143
[112,     1] loss: 0.153
[113,     1] loss: 0.102
[114,     1] loss: 0.096
[115,     1] loss: 0.096
[116,     1] loss: 0.149
[117,     1] loss: 0.091
[118,     1] loss: 0.168
[119,     1] loss: 0.180
[120,     1] loss: 0.160
[121,     1] loss: 0.112
[122,     1] loss: 0.161
[123,     1] loss: 0.104
[124,     1] loss: 0.120
[125,     1] loss: 0.098
[126,     1] loss: 0.126
[127,     1] loss: 0.149
[128,     1] loss: 0.129
[129,     1] loss: 0.149
[130,     1] loss: 0.138
[131,     1] loss: 0.170
[132,     1] loss: 0.137
[133,     1] loss: 0.115
[134,     1] loss: 0.106
[135,     1] loss: 0.167
[136,     1] loss: 0.138
[137,     1] loss: 0.239
[138,     1] loss: 0.161
[139,     1] loss: 0.170
[140,     1] loss: 0.182
[141,     1] loss: 0.182
[142,     1] loss: 0.153
[143,     1] loss: 0.133
[144,     1] loss: 0.213
Early stopping applied (best metric=0.3208445906639099)
Finished Training
Total time taken: 15.638000011444092
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.691
[4,     1] loss: 0.678
[5,     1] loss: 0.658
[6,     1] loss: 0.624
[7,     1] loss: 0.596
[8,     1] loss: 0.558
[9,     1] loss: 0.545
[10,     1] loss: 0.488
[11,     1] loss: 0.495
[12,     1] loss: 0.414
[13,     1] loss: 0.393
[14,     1] loss: 0.437
[15,     1] loss: 0.426
[16,     1] loss: 0.352
[17,     1] loss: 0.341
[18,     1] loss: 0.336
[19,     1] loss: 0.349
[20,     1] loss: 0.244
[21,     1] loss: 0.226
[22,     1] loss: 0.284
[23,     1] loss: 0.260
[24,     1] loss: 0.257
[25,     1] loss: 0.245
[26,     1] loss: 0.192
[27,     1] loss: 0.251
[28,     1] loss: 0.207
[29,     1] loss: 0.180
[30,     1] loss: 0.202
[31,     1] loss: 0.170
[32,     1] loss: 0.175
[33,     1] loss: 0.129
[34,     1] loss: 0.124
[35,     1] loss: 0.142
[36,     1] loss: 0.114
[37,     1] loss: 0.161
[38,     1] loss: 0.117
[39,     1] loss: 0.134
[40,     1] loss: 0.227
[41,     1] loss: 0.198
[42,     1] loss: 0.231
[43,     1] loss: 0.276
[44,     1] loss: 0.184
[45,     1] loss: 0.218
[46,     1] loss: 0.265
[47,     1] loss: 0.197
[48,     1] loss: 0.202
[49,     1] loss: 0.185
[50,     1] loss: 0.138
[51,     1] loss: 0.181
[52,     1] loss: 0.166
[53,     1] loss: 0.200
[54,     1] loss: 0.229
[55,     1] loss: 0.160
[56,     1] loss: 0.163
[57,     1] loss: 0.137
[58,     1] loss: 0.130
[59,     1] loss: 0.151
[60,     1] loss: 0.174
[61,     1] loss: 0.130
[62,     1] loss: 0.124
[63,     1] loss: 0.103
[64,     1] loss: 0.082
[65,     1] loss: 0.092
[66,     1] loss: 0.134
Early stopping applied (best metric=0.29889434576034546)
Finished Training
Total time taken: 7.21800422668457
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.691
[3,     1] loss: 0.663
[4,     1] loss: 0.612
[5,     1] loss: 0.592
[6,     1] loss: 0.508
[7,     1] loss: 0.495
[8,     1] loss: 0.462
[9,     1] loss: 0.410
[10,     1] loss: 0.415
[11,     1] loss: 0.344
[12,     1] loss: 0.419
[13,     1] loss: 0.268
[14,     1] loss: 0.306
[15,     1] loss: 0.310
[16,     1] loss: 0.284
[17,     1] loss: 0.256
[18,     1] loss: 0.325
[19,     1] loss: 0.192
[20,     1] loss: 0.237
[21,     1] loss: 0.229
[22,     1] loss: 0.271
[23,     1] loss: 0.180
[24,     1] loss: 0.256
[25,     1] loss: 0.239
[26,     1] loss: 0.215
[27,     1] loss: 0.215
[28,     1] loss: 0.158
[29,     1] loss: 0.182
[30,     1] loss: 0.202
[31,     1] loss: 0.136
[32,     1] loss: 0.221
[33,     1] loss: 0.163
[34,     1] loss: 0.243
[35,     1] loss: 0.146
[36,     1] loss: 0.231
[37,     1] loss: 0.205
[38,     1] loss: 0.160
[39,     1] loss: 0.135
[40,     1] loss: 0.127
[41,     1] loss: 0.196
[42,     1] loss: 0.137
[43,     1] loss: 0.130
[44,     1] loss: 0.189
[45,     1] loss: 0.126
[46,     1] loss: 0.174
[47,     1] loss: 0.116
[48,     1] loss: 0.231
[49,     1] loss: 0.195
[50,     1] loss: 0.151
[51,     1] loss: 0.191
[52,     1] loss: 0.148
[53,     1] loss: 0.121
[54,     1] loss: 0.134
[55,     1] loss: 0.130
[56,     1] loss: 0.126
[57,     1] loss: 0.080
[58,     1] loss: 0.113
[59,     1] loss: 0.120
[60,     1] loss: 0.120
Early stopping applied (best metric=0.36555278301239014)
Finished Training
Total time taken: 6.621001482009888
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.692
[3,     1] loss: 0.675
[4,     1] loss: 0.642
[5,     1] loss: 0.606
[6,     1] loss: 0.567
[7,     1] loss: 0.547
[8,     1] loss: 0.486
[9,     1] loss: 0.471
[10,     1] loss: 0.434
[11,     1] loss: 0.349
[12,     1] loss: 0.363
[13,     1] loss: 0.343
[14,     1] loss: 0.324
[15,     1] loss: 0.273
[16,     1] loss: 0.321
[17,     1] loss: 0.385
[18,     1] loss: 0.338
[19,     1] loss: 0.285
[20,     1] loss: 0.240
[21,     1] loss: 0.243
[22,     1] loss: 0.304
[23,     1] loss: 0.233
[24,     1] loss: 0.275
[25,     1] loss: 0.236
[26,     1] loss: 0.199
[27,     1] loss: 0.270
[28,     1] loss: 0.213
[29,     1] loss: 0.217
[30,     1] loss: 0.226
[31,     1] loss: 0.169
[32,     1] loss: 0.146
[33,     1] loss: 0.154
[34,     1] loss: 0.150
[35,     1] loss: 0.180
[36,     1] loss: 0.167
[37,     1] loss: 0.183
[38,     1] loss: 0.161
[39,     1] loss: 0.110
[40,     1] loss: 0.176
[41,     1] loss: 0.175
[42,     1] loss: 0.166
[43,     1] loss: 0.154
[44,     1] loss: 0.179
[45,     1] loss: 0.120
[46,     1] loss: 0.106
[47,     1] loss: 0.174
[48,     1] loss: 0.147
[49,     1] loss: 0.149
[50,     1] loss: 0.101
[51,     1] loss: 0.095
[52,     1] loss: 0.141
[53,     1] loss: 0.216
[54,     1] loss: 0.317
[55,     1] loss: 0.247
[56,     1] loss: 0.294
[57,     1] loss: 0.326
[58,     1] loss: 0.238
[59,     1] loss: 0.311
[60,     1] loss: 0.238
Early stopping applied (best metric=0.23439611494541168)
Finished Training
Total time taken: 6.532999753952026
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.690
[3,     1] loss: 0.673
[4,     1] loss: 0.638
[5,     1] loss: 0.619
[6,     1] loss: 0.592
[7,     1] loss: 0.579
[8,     1] loss: 0.512
[9,     1] loss: 0.497
[10,     1] loss: 0.464
[11,     1] loss: 0.446
[12,     1] loss: 0.393
[13,     1] loss: 0.373
[14,     1] loss: 0.342
[15,     1] loss: 0.324
[16,     1] loss: 0.371
[17,     1] loss: 0.312
[18,     1] loss: 0.332
[19,     1] loss: 0.254
[20,     1] loss: 0.280
[21,     1] loss: 0.348
[22,     1] loss: 0.246
[23,     1] loss: 0.251
[24,     1] loss: 0.290
[25,     1] loss: 0.201
[26,     1] loss: 0.264
[27,     1] loss: 0.211
[28,     1] loss: 0.179
[29,     1] loss: 0.279
[30,     1] loss: 0.245
[31,     1] loss: 0.248
[32,     1] loss: 0.307
[33,     1] loss: 0.323
[34,     1] loss: 0.185
[35,     1] loss: 0.311
[36,     1] loss: 0.264
[37,     1] loss: 0.241
[38,     1] loss: 0.228
[39,     1] loss: 0.233
[40,     1] loss: 0.235
[41,     1] loss: 0.234
[42,     1] loss: 0.210
[43,     1] loss: 0.202
[44,     1] loss: 0.199
[45,     1] loss: 0.156
[46,     1] loss: 0.197
[47,     1] loss: 0.161
[48,     1] loss: 0.138
[49,     1] loss: 0.188
[50,     1] loss: 0.132
[51,     1] loss: 0.145
[52,     1] loss: 0.102
[53,     1] loss: 0.147
[54,     1] loss: 0.163
[55,     1] loss: 0.095
[56,     1] loss: 0.109
[57,     1] loss: 0.169
[58,     1] loss: 0.142
[59,     1] loss: 0.110
[60,     1] loss: 0.138
[61,     1] loss: 0.082
[62,     1] loss: 0.155
[63,     1] loss: 0.123
[64,     1] loss: 0.116
[65,     1] loss: 0.113
[66,     1] loss: 0.115
[67,     1] loss: 0.092
[68,     1] loss: 0.186
[69,     1] loss: 0.091
[70,     1] loss: 0.216
[71,     1] loss: 0.149
[72,     1] loss: 0.244
[73,     1] loss: 0.158
[74,     1] loss: 0.112
[75,     1] loss: 0.243
Early stopping applied (best metric=0.45199745893478394)
Finished Training
Total time taken: 8.196999073028564
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.695
[3,     1] loss: 0.684
[4,     1] loss: 0.666
[5,     1] loss: 0.644
[6,     1] loss: 0.623
[7,     1] loss: 0.595
[8,     1] loss: 0.573
[9,     1] loss: 0.525
[10,     1] loss: 0.466
[11,     1] loss: 0.453
[12,     1] loss: 0.421
[13,     1] loss: 0.383
[14,     1] loss: 0.379
[15,     1] loss: 0.326
[16,     1] loss: 0.338
[17,     1] loss: 0.285
[18,     1] loss: 0.311
[19,     1] loss: 0.272
[20,     1] loss: 0.277
[21,     1] loss: 0.260
[22,     1] loss: 0.254
[23,     1] loss: 0.268
[24,     1] loss: 0.176
[25,     1] loss: 0.254
[26,     1] loss: 0.242
[27,     1] loss: 0.243
[28,     1] loss: 0.266
[29,     1] loss: 0.225
[30,     1] loss: 0.303
[31,     1] loss: 0.230
[32,     1] loss: 0.205
[33,     1] loss: 0.251
[34,     1] loss: 0.267
[35,     1] loss: 0.237
[36,     1] loss: 0.250
[37,     1] loss: 0.273
[38,     1] loss: 0.227
[39,     1] loss: 0.230
[40,     1] loss: 0.201
[41,     1] loss: 0.214
[42,     1] loss: 0.188
[43,     1] loss: 0.168
[44,     1] loss: 0.192
[45,     1] loss: 0.162
[46,     1] loss: 0.187
[47,     1] loss: 0.187
[48,     1] loss: 0.136
[49,     1] loss: 0.226
[50,     1] loss: 0.202
[51,     1] loss: 0.193
[52,     1] loss: 0.161
[53,     1] loss: 0.167
[54,     1] loss: 0.180
[55,     1] loss: 0.175
[56,     1] loss: 0.179
[57,     1] loss: 0.165
[58,     1] loss: 0.152
[59,     1] loss: 0.187
[60,     1] loss: 0.163
[61,     1] loss: 0.202
[62,     1] loss: 0.145
Early stopping applied (best metric=0.4128853976726532)
Finished Training
Total time taken: 6.741612434387207
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.697
[3,     1] loss: 0.683
[4,     1] loss: 0.669
[5,     1] loss: 0.638
[6,     1] loss: 0.634
[7,     1] loss: 0.603
[8,     1] loss: 0.572
[9,     1] loss: 0.556
[10,     1] loss: 0.515
[11,     1] loss: 0.494
[12,     1] loss: 0.465
[13,     1] loss: 0.540
[14,     1] loss: 0.395
[15,     1] loss: 0.420
[16,     1] loss: 0.409
[17,     1] loss: 0.368
[18,     1] loss: 0.339
[19,     1] loss: 0.308
[20,     1] loss: 0.344
[21,     1] loss: 0.297
[22,     1] loss: 0.316
[23,     1] loss: 0.362
[24,     1] loss: 0.349
[25,     1] loss: 0.312
[26,     1] loss: 0.273
[27,     1] loss: 0.265
[28,     1] loss: 0.278
[29,     1] loss: 0.300
[30,     1] loss: 0.215
[31,     1] loss: 0.241
[32,     1] loss: 0.205
[33,     1] loss: 0.230
[34,     1] loss: 0.225
[35,     1] loss: 0.220
[36,     1] loss: 0.249
[37,     1] loss: 0.183
[38,     1] loss: 0.200
[39,     1] loss: 0.131
[40,     1] loss: 0.151
[41,     1] loss: 0.210
[42,     1] loss: 0.151
[43,     1] loss: 0.142
[44,     1] loss: 0.130
[45,     1] loss: 0.107
[46,     1] loss: 0.160
[47,     1] loss: 0.188
[48,     1] loss: 0.118
[49,     1] loss: 0.309
[50,     1] loss: 0.230
[51,     1] loss: 0.429
[52,     1] loss: 0.235
[53,     1] loss: 0.302
[54,     1] loss: 0.186
[55,     1] loss: 0.210
[56,     1] loss: 0.225
[57,     1] loss: 0.264
[58,     1] loss: 0.234
[59,     1] loss: 0.194
[60,     1] loss: 0.202
[61,     1] loss: 0.197
[62,     1] loss: 0.181
[63,     1] loss: 0.155
[64,     1] loss: 0.177
[65,     1] loss: 0.128
[66,     1] loss: 0.149
[67,     1] loss: 0.193
[68,     1] loss: 0.112
[69,     1] loss: 0.121
[70,     1] loss: 0.152
[71,     1] loss: 0.129
[72,     1] loss: 0.101
[73,     1] loss: 0.160
[74,     1] loss: 0.239
[75,     1] loss: 0.135
[76,     1] loss: 0.137
[77,     1] loss: 0.173
[78,     1] loss: 0.136
Early stopping applied (best metric=0.22701260447502136)
Finished Training
Total time taken: 8.573619604110718
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.669
[5,     1] loss: 0.647
[6,     1] loss: 0.629
[7,     1] loss: 0.586
[8,     1] loss: 0.568
[9,     1] loss: 0.526
[10,     1] loss: 0.521
[11,     1] loss: 0.460
[12,     1] loss: 0.445
[13,     1] loss: 0.395
[14,     1] loss: 0.373
[15,     1] loss: 0.350
[16,     1] loss: 0.299
[17,     1] loss: 0.324
[18,     1] loss: 0.316
[19,     1] loss: 0.213
[20,     1] loss: 0.255
[21,     1] loss: 0.360
[22,     1] loss: 0.278
[23,     1] loss: 0.243
[24,     1] loss: 0.243
[25,     1] loss: 0.212
[26,     1] loss: 0.181
[27,     1] loss: 0.240
[28,     1] loss: 0.226
[29,     1] loss: 0.217
[30,     1] loss: 0.224
[31,     1] loss: 0.179
[32,     1] loss: 0.209
[33,     1] loss: 0.199
[34,     1] loss: 0.221
[35,     1] loss: 0.203
[36,     1] loss: 0.180
[37,     1] loss: 0.178
[38,     1] loss: 0.142
[39,     1] loss: 0.258
[40,     1] loss: 0.251
[41,     1] loss: 0.285
[42,     1] loss: 0.176
[43,     1] loss: 0.203
[44,     1] loss: 0.202
[45,     1] loss: 0.194
[46,     1] loss: 0.159
[47,     1] loss: 0.135
[48,     1] loss: 0.167
[49,     1] loss: 0.150
[50,     1] loss: 0.116
[51,     1] loss: 0.170
[52,     1] loss: 0.149
[53,     1] loss: 0.154
[54,     1] loss: 0.130
[55,     1] loss: 0.254
[56,     1] loss: 0.075
[57,     1] loss: 0.332
[58,     1] loss: 0.153
[59,     1] loss: 0.181
[60,     1] loss: 0.136
[61,     1] loss: 0.174
[62,     1] loss: 0.135
[63,     1] loss: 0.132
[64,     1] loss: 0.137
[65,     1] loss: 0.109
[66,     1] loss: 0.100
[67,     1] loss: 0.093
[68,     1] loss: 0.111
[69,     1] loss: 0.179
[70,     1] loss: 0.201
Early stopping applied (best metric=0.20582403242588043)
Finished Training
Total time taken: 7.607611656188965
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.686
[4,     1] loss: 0.664
[5,     1] loss: 0.643
[6,     1] loss: 0.591
[7,     1] loss: 0.564
[8,     1] loss: 0.504
[9,     1] loss: 0.504
[10,     1] loss: 0.465
[11,     1] loss: 0.455
[12,     1] loss: 0.437
[13,     1] loss: 0.437
[14,     1] loss: 0.343
[15,     1] loss: 0.395
[16,     1] loss: 0.414
[17,     1] loss: 0.357
[18,     1] loss: 0.353
[19,     1] loss: 0.335
[20,     1] loss: 0.250
[21,     1] loss: 0.368
[22,     1] loss: 0.348
[23,     1] loss: 0.298
[24,     1] loss: 0.246
[25,     1] loss: 0.258
[26,     1] loss: 0.251
[27,     1] loss: 0.277
[28,     1] loss: 0.206
[29,     1] loss: 0.207
[30,     1] loss: 0.202
[31,     1] loss: 0.167
[32,     1] loss: 0.182
[33,     1] loss: 0.190
[34,     1] loss: 0.188
[35,     1] loss: 0.187
[36,     1] loss: 0.160
[37,     1] loss: 0.229
[38,     1] loss: 0.257
[39,     1] loss: 0.150
[40,     1] loss: 0.171
[41,     1] loss: 0.137
[42,     1] loss: 0.167
[43,     1] loss: 0.159
[44,     1] loss: 0.168
[45,     1] loss: 0.173
[46,     1] loss: 0.226
[47,     1] loss: 0.199
[48,     1] loss: 0.203
[49,     1] loss: 0.158
[50,     1] loss: 0.211
[51,     1] loss: 0.138
[52,     1] loss: 0.156
[53,     1] loss: 0.242
[54,     1] loss: 0.122
[55,     1] loss: 0.193
[56,     1] loss: 0.161
[57,     1] loss: 0.155
[58,     1] loss: 0.198
[59,     1] loss: 0.137
[60,     1] loss: 0.099
[61,     1] loss: 0.167
[62,     1] loss: 0.183
[63,     1] loss: 0.223
[64,     1] loss: 0.189
[65,     1] loss: 0.142
[66,     1] loss: 0.220
[67,     1] loss: 0.215
[68,     1] loss: 0.136
[69,     1] loss: 0.164
[70,     1] loss: 0.164
[71,     1] loss: 0.134
[72,     1] loss: 0.127
[73,     1] loss: 0.122
[74,     1] loss: 0.141
[75,     1] loss: 0.169
[76,     1] loss: 0.123
[77,     1] loss: 0.257
[78,     1] loss: 0.210
[79,     1] loss: 0.376
[80,     1] loss: 0.191
Early stopping applied (best metric=0.22050487995147705)
Finished Training
Total time taken: 8.761001586914062
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.695
[3,     1] loss: 0.684
[4,     1] loss: 0.681
[5,     1] loss: 0.655
[6,     1] loss: 0.634
[7,     1] loss: 0.611
[8,     1] loss: 0.584
[9,     1] loss: 0.552
[10,     1] loss: 0.534
[11,     1] loss: 0.482
[12,     1] loss: 0.460
[13,     1] loss: 0.488
[14,     1] loss: 0.475
[15,     1] loss: 0.424
[16,     1] loss: 0.436
[17,     1] loss: 0.394
[18,     1] loss: 0.344
[19,     1] loss: 0.456
[20,     1] loss: 0.353
[21,     1] loss: 0.360
[22,     1] loss: 0.327
[23,     1] loss: 0.304
[24,     1] loss: 0.277
[25,     1] loss: 0.335
[26,     1] loss: 0.271
[27,     1] loss: 0.259
[28,     1] loss: 0.274
[29,     1] loss: 0.233
[30,     1] loss: 0.271
[31,     1] loss: 0.217
[32,     1] loss: 0.184
[33,     1] loss: 0.540
[34,     1] loss: 0.199
[35,     1] loss: 0.253
[36,     1] loss: 0.297
[37,     1] loss: 0.314
[38,     1] loss: 0.237
[39,     1] loss: 0.195
[40,     1] loss: 0.230
[41,     1] loss: 0.265
[42,     1] loss: 0.231
[43,     1] loss: 0.213
[44,     1] loss: 0.216
[45,     1] loss: 0.208
[46,     1] loss: 0.222
[47,     1] loss: 0.208
[48,     1] loss: 0.213
[49,     1] loss: 0.191
[50,     1] loss: 0.191
[51,     1] loss: 0.186
[52,     1] loss: 0.133
[53,     1] loss: 0.184
[54,     1] loss: 0.220
[55,     1] loss: 0.117
[56,     1] loss: 0.223
[57,     1] loss: 0.190
[58,     1] loss: 0.170
[59,     1] loss: 0.206
[60,     1] loss: 0.199
[61,     1] loss: 0.266
[62,     1] loss: 0.229
[63,     1] loss: 0.198
[64,     1] loss: 0.168
[65,     1] loss: 0.163
[66,     1] loss: 0.165
[67,     1] loss: 0.164
[68,     1] loss: 0.178
[69,     1] loss: 0.210
[70,     1] loss: 0.176
[71,     1] loss: 0.133
[72,     1] loss: 0.171
[73,     1] loss: 0.147
[74,     1] loss: 0.141
[75,     1] loss: 0.132
[76,     1] loss: 0.142
[77,     1] loss: 0.163
[78,     1] loss: 0.154
[79,     1] loss: 0.185
[80,     1] loss: 0.092
[81,     1] loss: 0.104
[82,     1] loss: 0.124
[83,     1] loss: 0.177
[84,     1] loss: 0.136
[85,     1] loss: 0.166
[86,     1] loss: 0.156
[87,     1] loss: 0.111
[88,     1] loss: 0.093
[89,     1] loss: 0.089
[90,     1] loss: 0.117
[91,     1] loss: 0.090
[92,     1] loss: 0.119
[93,     1] loss: 0.077
[94,     1] loss: 0.095
[95,     1] loss: 0.075
[96,     1] loss: 0.168
[97,     1] loss: 0.454
[98,     1] loss: 0.518
[99,     1] loss: 0.399
[100,     1] loss: 0.486
[101,     1] loss: 0.423
[102,     1] loss: 0.322
[103,     1] loss: 0.338
[104,     1] loss: 0.359
[105,     1] loss: 0.340
[106,     1] loss: 0.346
[107,     1] loss: 0.367
[108,     1] loss: 0.313
[109,     1] loss: 0.341
[110,     1] loss: 0.298
[111,     1] loss: 0.337
[112,     1] loss: 0.265
[113,     1] loss: 0.297
[114,     1] loss: 0.245
[115,     1] loss: 0.264
[116,     1] loss: 0.231
[117,     1] loss: 0.195
[118,     1] loss: 0.172
[119,     1] loss: 0.204
[120,     1] loss: 0.141
[121,     1] loss: 0.203
[122,     1] loss: 0.162
[123,     1] loss: 0.189
[124,     1] loss: 0.182
[125,     1] loss: 0.175
[126,     1] loss: 0.163
[127,     1] loss: 0.123
[128,     1] loss: 0.145
[129,     1] loss: 0.101
[130,     1] loss: 0.118
[131,     1] loss: 0.136
[132,     1] loss: 0.171
[133,     1] loss: 0.094
[134,     1] loss: 0.095
[135,     1] loss: 0.154
[136,     1] loss: 0.123
[137,     1] loss: 0.152
[138,     1] loss: 0.123
[139,     1] loss: 0.127
Early stopping applied (best metric=0.21855074167251587)
Finished Training
Total time taken: 15.075610637664795
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.683
[4,     1] loss: 0.668
[5,     1] loss: 0.636
[6,     1] loss: 0.625
[7,     1] loss: 0.600
[8,     1] loss: 0.549
[9,     1] loss: 0.520
[10,     1] loss: 0.503
[11,     1] loss: 0.447
[12,     1] loss: 0.446
[13,     1] loss: 0.388
[14,     1] loss: 0.360
[15,     1] loss: 0.340
[16,     1] loss: 0.317
[17,     1] loss: 0.326
[18,     1] loss: 0.306
[19,     1] loss: 0.278
[20,     1] loss: 0.337
[21,     1] loss: 0.329
[22,     1] loss: 0.398
[23,     1] loss: 0.248
[24,     1] loss: 0.266
[25,     1] loss: 0.264
[26,     1] loss: 0.309
[27,     1] loss: 0.284
[28,     1] loss: 0.241
[29,     1] loss: 0.233
[30,     1] loss: 0.165
[31,     1] loss: 0.180
[32,     1] loss: 0.194
[33,     1] loss: 0.183
[34,     1] loss: 0.307
[35,     1] loss: 0.199
[36,     1] loss: 0.202
[37,     1] loss: 0.147
[38,     1] loss: 0.222
[39,     1] loss: 0.140
[40,     1] loss: 0.164
[41,     1] loss: 0.147
[42,     1] loss: 0.185
[43,     1] loss: 0.124
[44,     1] loss: 0.141
[45,     1] loss: 0.129
[46,     1] loss: 0.182
[47,     1] loss: 0.157
[48,     1] loss: 0.108
[49,     1] loss: 0.120
[50,     1] loss: 0.144
[51,     1] loss: 0.113
[52,     1] loss: 0.129
[53,     1] loss: 0.108
[54,     1] loss: 0.096
[55,     1] loss: 0.084
[56,     1] loss: 0.125
[57,     1] loss: 0.124
[58,     1] loss: 0.114
[59,     1] loss: 0.092
[60,     1] loss: 0.186
[61,     1] loss: 0.346
[62,     1] loss: 0.265
[63,     1] loss: 0.310
[64,     1] loss: 0.356
[65,     1] loss: 0.313
[66,     1] loss: 0.318
[67,     1] loss: 0.258
[68,     1] loss: 0.317
[69,     1] loss: 0.302
[70,     1] loss: 0.269
[71,     1] loss: 0.270
[72,     1] loss: 0.264
[73,     1] loss: 0.257
[74,     1] loss: 0.217
[75,     1] loss: 0.231
[76,     1] loss: 0.201
[77,     1] loss: 0.206
[78,     1] loss: 0.181
[79,     1] loss: 0.161
[80,     1] loss: 0.172
[81,     1] loss: 0.166
[82,     1] loss: 0.150
[83,     1] loss: 0.165
[84,     1] loss: 0.161
[85,     1] loss: 0.139
[86,     1] loss: 0.160
[87,     1] loss: 0.139
[88,     1] loss: 0.103
[89,     1] loss: 0.105
[90,     1] loss: 0.142
[91,     1] loss: 0.123
[92,     1] loss: 0.143
[93,     1] loss: 0.248
[94,     1] loss: 0.156
[95,     1] loss: 0.136
[96,     1] loss: 0.185
[97,     1] loss: 0.189
[98,     1] loss: 0.199
[99,     1] loss: 0.135
[100,     1] loss: 0.154
[101,     1] loss: 0.161
[102,     1] loss: 0.134
[103,     1] loss: 0.099
[104,     1] loss: 0.124
[105,     1] loss: 0.147
[106,     1] loss: 0.080
[107,     1] loss: 0.129
[108,     1] loss: 0.110
[109,     1] loss: 0.152
[110,     1] loss: 0.156
[111,     1] loss: 0.208
[112,     1] loss: 0.125
[113,     1] loss: 0.144
[114,     1] loss: 0.161
[115,     1] loss: 0.210
[116,     1] loss: 0.173
[117,     1] loss: 0.187
Early stopping applied (best metric=0.2709241807460785)
Finished Training
Total time taken: 12.864826917648315
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.672
[4,     1] loss: 0.636
[5,     1] loss: 0.591
[6,     1] loss: 0.558
[7,     1] loss: 0.554
[8,     1] loss: 0.536
[9,     1] loss: 0.447
[10,     1] loss: 0.419
[11,     1] loss: 0.370
[12,     1] loss: 0.426
[13,     1] loss: 0.316
[14,     1] loss: 0.398
[15,     1] loss: 0.334
[16,     1] loss: 0.313
[17,     1] loss: 0.370
[18,     1] loss: 0.303
[19,     1] loss: 0.335
[20,     1] loss: 0.360
[21,     1] loss: 0.281
[22,     1] loss: 0.246
[23,     1] loss: 0.321
[24,     1] loss: 0.323
[25,     1] loss: 0.264
[26,     1] loss: 0.284
[27,     1] loss: 0.295
[28,     1] loss: 0.318
[29,     1] loss: 0.253
[30,     1] loss: 0.252
[31,     1] loss: 0.286
[32,     1] loss: 0.212
[33,     1] loss: 0.162
[34,     1] loss: 0.181
[35,     1] loss: 0.178
[36,     1] loss: 0.214
[37,     1] loss: 0.186
[38,     1] loss: 0.180
[39,     1] loss: 0.213
[40,     1] loss: 0.187
[41,     1] loss: 0.178
[42,     1] loss: 0.263
[43,     1] loss: 0.146
[44,     1] loss: 0.248
[45,     1] loss: 0.193
[46,     1] loss: 0.184
[47,     1] loss: 0.206
[48,     1] loss: 0.173
[49,     1] loss: 0.162
[50,     1] loss: 0.156
[51,     1] loss: 0.166
[52,     1] loss: 0.114
[53,     1] loss: 0.177
[54,     1] loss: 0.097
[55,     1] loss: 0.321
[56,     1] loss: 0.160
[57,     1] loss: 0.275
[58,     1] loss: 0.403
[59,     1] loss: 0.233
[60,     1] loss: 0.287
Early stopping applied (best metric=0.3383040428161621)
Finished Training
Total time taken: 6.531999111175537
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.687
[2,     1] loss: 0.704
[3,     1] loss: 0.678
[4,     1] loss: 0.660
[5,     1] loss: 0.629
[6,     1] loss: 0.602
[7,     1] loss: 0.545
[8,     1] loss: 0.532
[9,     1] loss: 0.481
[10,     1] loss: 0.437
[11,     1] loss: 0.443
[12,     1] loss: 0.407
[13,     1] loss: 0.338
[14,     1] loss: 0.400
[15,     1] loss: 0.430
[16,     1] loss: 0.374
[17,     1] loss: 0.460
[18,     1] loss: 0.296
[19,     1] loss: 0.394
[20,     1] loss: 0.323
[21,     1] loss: 0.327
[22,     1] loss: 0.292
[23,     1] loss: 0.360
[24,     1] loss: 0.267
[25,     1] loss: 0.252
[26,     1] loss: 0.221
[27,     1] loss: 0.233
[28,     1] loss: 0.284
[29,     1] loss: 0.199
[30,     1] loss: 0.209
[31,     1] loss: 0.230
[32,     1] loss: 0.194
[33,     1] loss: 0.162
[34,     1] loss: 0.103
[35,     1] loss: 0.206
[36,     1] loss: 0.148
[37,     1] loss: 0.368
[38,     1] loss: 0.218
[39,     1] loss: 0.247
[40,     1] loss: 0.266
[41,     1] loss: 0.234
[42,     1] loss: 0.292
[43,     1] loss: 0.169
[44,     1] loss: 0.192
[45,     1] loss: 0.229
[46,     1] loss: 0.219
[47,     1] loss: 0.189
[48,     1] loss: 0.164
[49,     1] loss: 0.142
[50,     1] loss: 0.137
[51,     1] loss: 0.114
[52,     1] loss: 0.119
[53,     1] loss: 0.162
[54,     1] loss: 0.161
[55,     1] loss: 0.376
[56,     1] loss: 0.260
[57,     1] loss: 0.272
[58,     1] loss: 0.236
[59,     1] loss: 0.260
[60,     1] loss: 0.180
[61,     1] loss: 0.159
[62,     1] loss: 0.197
[63,     1] loss: 0.201
[64,     1] loss: 0.199
[65,     1] loss: 0.171
[66,     1] loss: 0.148
[67,     1] loss: 0.171
[68,     1] loss: 0.212
[69,     1] loss: 0.135
[70,     1] loss: 0.131
[71,     1] loss: 0.141
[72,     1] loss: 0.121
[73,     1] loss: 0.112
[74,     1] loss: 0.126
[75,     1] loss: 0.131
[76,     1] loss: 0.147
[77,     1] loss: 0.077
[78,     1] loss: 0.128
[79,     1] loss: 0.078
[80,     1] loss: 0.087
[81,     1] loss: 0.088
[82,     1] loss: 0.091
[83,     1] loss: 0.145
[84,     1] loss: 0.124
[85,     1] loss: 0.202
[86,     1] loss: 0.100
[87,     1] loss: 0.157
[88,     1] loss: 0.128
[89,     1] loss: 0.152
[90,     1] loss: 0.124
[91,     1] loss: 0.151
[92,     1] loss: 0.116
[93,     1] loss: 0.124
[94,     1] loss: 0.177
[95,     1] loss: 0.204
[96,     1] loss: 0.172
[97,     1] loss: 0.186
[98,     1] loss: 0.170
[99,     1] loss: 0.212
[100,     1] loss: 0.157
[101,     1] loss: 0.142
[102,     1] loss: 0.155
[103,     1] loss: 0.170
[104,     1] loss: 0.135
[105,     1] loss: 0.124
[106,     1] loss: 0.122
[107,     1] loss: 0.118
[108,     1] loss: 0.146
[109,     1] loss: 0.151
[110,     1] loss: 0.095
[111,     1] loss: 0.089
[112,     1] loss: 0.097
[113,     1] loss: 0.133
[114,     1] loss: 0.141
[115,     1] loss: 0.135
[116,     1] loss: 0.126
[117,     1] loss: 0.140
[118,     1] loss: 0.108
[119,     1] loss: 0.117
[120,     1] loss: 0.111
[121,     1] loss: 0.086
[122,     1] loss: 0.107
[123,     1] loss: 0.110
[124,     1] loss: 0.126
[125,     1] loss: 0.083
[126,     1] loss: 0.101
[127,     1] loss: 0.093
[128,     1] loss: 0.077
[129,     1] loss: 0.068
[130,     1] loss: 0.064
[131,     1] loss: 0.060
[132,     1] loss: 0.281
[133,     1] loss: 0.313
[134,     1] loss: 0.242
[135,     1] loss: 0.467
[136,     1] loss: 0.209
[137,     1] loss: 0.370
[138,     1] loss: 0.205
[139,     1] loss: 0.266
[140,     1] loss: 0.250
[141,     1] loss: 0.203
[142,     1] loss: 0.216
[143,     1] loss: 0.177
[144,     1] loss: 0.209
[145,     1] loss: 0.157
[146,     1] loss: 0.157
[147,     1] loss: 0.179
[148,     1] loss: 0.128
Early stopping applied (best metric=0.12163735926151276)
Finished Training
Total time taken: 16.151601314544678
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.691
[3,     1] loss: 0.675
[4,     1] loss: 0.658
[5,     1] loss: 0.617
[6,     1] loss: 0.578
[7,     1] loss: 0.536
[8,     1] loss: 0.516
[9,     1] loss: 0.449
[10,     1] loss: 0.408
[11,     1] loss: 0.405
[12,     1] loss: 0.392
[13,     1] loss: 0.366
[14,     1] loss: 0.345
[15,     1] loss: 0.359
[16,     1] loss: 0.297
[17,     1] loss: 0.278
[18,     1] loss: 0.268
[19,     1] loss: 0.302
[20,     1] loss: 0.249
[21,     1] loss: 0.260
[22,     1] loss: 0.296
[23,     1] loss: 0.189
[24,     1] loss: 0.211
[25,     1] loss: 0.232
[26,     1] loss: 0.210
[27,     1] loss: 0.196
[28,     1] loss: 0.214
[29,     1] loss: 0.153
[30,     1] loss: 0.125
[31,     1] loss: 0.167
[32,     1] loss: 0.242
[33,     1] loss: 0.247
[34,     1] loss: 0.146
[35,     1] loss: 0.311
[36,     1] loss: 0.220
[37,     1] loss: 0.339
[38,     1] loss: 0.326
[39,     1] loss: 0.208
[40,     1] loss: 0.249
[41,     1] loss: 0.253
[42,     1] loss: 0.223
[43,     1] loss: 0.186
[44,     1] loss: 0.200
[45,     1] loss: 0.194
[46,     1] loss: 0.147
[47,     1] loss: 0.164
[48,     1] loss: 0.147
[49,     1] loss: 0.118
[50,     1] loss: 0.105
[51,     1] loss: 0.109
[52,     1] loss: 0.096
[53,     1] loss: 0.111
[54,     1] loss: 0.129
[55,     1] loss: 0.114
[56,     1] loss: 0.096
[57,     1] loss: 0.094
[58,     1] loss: 0.051
[59,     1] loss: 0.169
[60,     1] loss: 0.086
[61,     1] loss: 0.230
Early stopping applied (best metric=0.29482904076576233)
Finished Training
Total time taken: 6.635000467300415
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.693
[3,     1] loss: 0.667
[4,     1] loss: 0.636
[5,     1] loss: 0.601
[6,     1] loss: 0.552
[7,     1] loss: 0.496
[8,     1] loss: 0.475
[9,     1] loss: 0.476
[10,     1] loss: 0.429
[11,     1] loss: 0.404
[12,     1] loss: 0.318
[13,     1] loss: 0.345
[14,     1] loss: 0.422
[15,     1] loss: 0.347
[16,     1] loss: 0.337
[17,     1] loss: 0.301
[18,     1] loss: 0.266
[19,     1] loss: 0.382
[20,     1] loss: 0.242
[21,     1] loss: 0.310
[22,     1] loss: 0.302
[23,     1] loss: 0.310
[24,     1] loss: 0.267
[25,     1] loss: 0.261
[26,     1] loss: 0.250
[27,     1] loss: 0.222
[28,     1] loss: 0.236
[29,     1] loss: 0.266
[30,     1] loss: 0.256
[31,     1] loss: 0.242
[32,     1] loss: 0.201
[33,     1] loss: 0.254
[34,     1] loss: 0.216
[35,     1] loss: 0.280
[36,     1] loss: 0.182
[37,     1] loss: 0.195
[38,     1] loss: 0.157
[39,     1] loss: 0.140
[40,     1] loss: 0.152
[41,     1] loss: 0.116
[42,     1] loss: 0.124
[43,     1] loss: 0.142
[44,     1] loss: 0.124
[45,     1] loss: 0.164
[46,     1] loss: 0.227
[47,     1] loss: 0.251
[48,     1] loss: 0.332
[49,     1] loss: 0.119
[50,     1] loss: 0.280
[51,     1] loss: 0.146
[52,     1] loss: 0.174
[53,     1] loss: 0.192
[54,     1] loss: 0.122
[55,     1] loss: 0.188
[56,     1] loss: 0.150
[57,     1] loss: 0.157
[58,     1] loss: 0.144
[59,     1] loss: 0.156
[60,     1] loss: 0.157
[61,     1] loss: 0.126
[62,     1] loss: 0.111
[63,     1] loss: 0.118
[64,     1] loss: 0.107
[65,     1] loss: 0.107
[66,     1] loss: 0.095
Early stopping applied (best metric=0.2578451335430145)
Finished Training
Total time taken: 7.193000793457031
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.648
[5,     1] loss: 0.618
[6,     1] loss: 0.595
[7,     1] loss: 0.541
[8,     1] loss: 0.526
[9,     1] loss: 0.494
[10,     1] loss: 0.467
[11,     1] loss: 0.414
[12,     1] loss: 0.408
[13,     1] loss: 0.404
[14,     1] loss: 0.478
[15,     1] loss: 0.371
[16,     1] loss: 0.436
[17,     1] loss: 0.452
[18,     1] loss: 0.400
[19,     1] loss: 0.339
[20,     1] loss: 0.350
[21,     1] loss: 0.310
[22,     1] loss: 0.345
[23,     1] loss: 0.356
[24,     1] loss: 0.280
[25,     1] loss: 0.236
[26,     1] loss: 0.295
[27,     1] loss: 0.235
[28,     1] loss: 0.264
[29,     1] loss: 0.190
[30,     1] loss: 0.254
[31,     1] loss: 0.256
[32,     1] loss: 0.220
[33,     1] loss: 0.192
[34,     1] loss: 0.276
[35,     1] loss: 0.251
[36,     1] loss: 0.315
[37,     1] loss: 0.292
[38,     1] loss: 0.253
[39,     1] loss: 0.249
[40,     1] loss: 0.210
[41,     1] loss: 0.245
[42,     1] loss: 0.190
[43,     1] loss: 0.234
[44,     1] loss: 0.188
[45,     1] loss: 0.184
[46,     1] loss: 0.179
[47,     1] loss: 0.149
[48,     1] loss: 0.162
[49,     1] loss: 0.170
[50,     1] loss: 0.113
[51,     1] loss: 0.136
[52,     1] loss: 0.115
[53,     1] loss: 0.188
[54,     1] loss: 0.194
[55,     1] loss: 0.238
[56,     1] loss: 0.248
[57,     1] loss: 0.293
[58,     1] loss: 0.199
[59,     1] loss: 0.244
[60,     1] loss: 0.240
[61,     1] loss: 0.167
[62,     1] loss: 0.182
[63,     1] loss: 0.197
[64,     1] loss: 0.219
[65,     1] loss: 0.191
[66,     1] loss: 0.183
[67,     1] loss: 0.194
[68,     1] loss: 0.160
[69,     1] loss: 0.209
[70,     1] loss: 0.173
[71,     1] loss: 0.168
[72,     1] loss: 0.140
[73,     1] loss: 0.182
[74,     1] loss: 0.119
[75,     1] loss: 0.167
[76,     1] loss: 0.145
[77,     1] loss: 0.105
[78,     1] loss: 0.144
[79,     1] loss: 0.131
[80,     1] loss: 0.114
[81,     1] loss: 0.117
Early stopping applied (best metric=0.09727315604686737)
Finished Training
Total time taken: 8.784607172012329
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.688
[3,     1] loss: 0.681
[4,     1] loss: 0.650
[5,     1] loss: 0.623
[6,     1] loss: 0.603
[7,     1] loss: 0.549
[8,     1] loss: 0.503
[9,     1] loss: 0.467
[10,     1] loss: 0.428
[11,     1] loss: 0.427
[12,     1] loss: 0.397
[13,     1] loss: 0.364
[14,     1] loss: 0.312
[15,     1] loss: 0.421
[16,     1] loss: 0.342
[17,     1] loss: 0.448
[18,     1] loss: 0.357
[19,     1] loss: 0.333
[20,     1] loss: 0.339
[21,     1] loss: 0.268
[22,     1] loss: 0.317
[23,     1] loss: 0.308
[24,     1] loss: 0.292
[25,     1] loss: 0.300
[26,     1] loss: 0.318
[27,     1] loss: 0.224
[28,     1] loss: 0.304
[29,     1] loss: 0.254
[30,     1] loss: 0.244
[31,     1] loss: 0.271
[32,     1] loss: 0.232
[33,     1] loss: 0.251
[34,     1] loss: 0.234
[35,     1] loss: 0.220
[36,     1] loss: 0.211
[37,     1] loss: 0.202
[38,     1] loss: 0.210
[39,     1] loss: 0.243
[40,     1] loss: 0.189
[41,     1] loss: 0.206
[42,     1] loss: 0.219
[43,     1] loss: 0.149
[44,     1] loss: 0.230
[45,     1] loss: 0.226
[46,     1] loss: 0.274
[47,     1] loss: 0.189
[48,     1] loss: 0.235
[49,     1] loss: 0.198
[50,     1] loss: 0.229
[51,     1] loss: 0.183
[52,     1] loss: 0.191
[53,     1] loss: 0.187
[54,     1] loss: 0.201
[55,     1] loss: 0.187
[56,     1] loss: 0.161
[57,     1] loss: 0.187
[58,     1] loss: 0.138
[59,     1] loss: 0.140
[60,     1] loss: 0.131
[61,     1] loss: 0.137
[62,     1] loss: 0.177
[63,     1] loss: 0.172
[64,     1] loss: 0.184
[65,     1] loss: 0.130
[66,     1] loss: 0.230
[67,     1] loss: 0.146
[68,     1] loss: 0.177
[69,     1] loss: 0.132
[70,     1] loss: 0.165
[71,     1] loss: 0.129
[72,     1] loss: 0.117
[73,     1] loss: 0.193
[74,     1] loss: 0.205
[75,     1] loss: 0.164
[76,     1] loss: 0.157
[77,     1] loss: 0.234
[78,     1] loss: 0.140
[79,     1] loss: 0.161
[80,     1] loss: 0.149
[81,     1] loss: 0.159
[82,     1] loss: 0.134
[83,     1] loss: 0.114
[84,     1] loss: 0.118
[85,     1] loss: 0.129
[86,     1] loss: 0.160
[87,     1] loss: 0.112
[88,     1] loss: 0.123
[89,     1] loss: 0.111
[90,     1] loss: 0.182
[91,     1] loss: 0.174
[92,     1] loss: 0.126
[93,     1] loss: 0.146
[94,     1] loss: 0.115
[95,     1] loss: 0.143
[96,     1] loss: 0.181
[97,     1] loss: 0.118
[98,     1] loss: 0.179
[99,     1] loss: 0.106
[100,     1] loss: 0.115
[101,     1] loss: 0.145
[102,     1] loss: 0.137
[103,     1] loss: 0.115
[104,     1] loss: 0.148
[105,     1] loss: 0.194
[106,     1] loss: 0.123
[107,     1] loss: 0.112
[108,     1] loss: 0.114
[109,     1] loss: 0.150
[110,     1] loss: 0.125
[111,     1] loss: 0.183
[112,     1] loss: 0.154
[113,     1] loss: 0.138
[114,     1] loss: 0.154
[115,     1] loss: 0.155
[116,     1] loss: 0.111
[117,     1] loss: 0.143
[118,     1] loss: 0.131
[119,     1] loss: 0.100
[120,     1] loss: 0.179
[121,     1] loss: 0.094
[122,     1] loss: 0.070
[123,     1] loss: 0.093
[124,     1] loss: 0.098
[125,     1] loss: 0.210
[126,     1] loss: 0.350
[127,     1] loss: 0.339
[128,     1] loss: 0.329
[129,     1] loss: 0.348
[130,     1] loss: 0.299
[131,     1] loss: 0.266
[132,     1] loss: 0.223
[133,     1] loss: 0.212
[134,     1] loss: 0.253
[135,     1] loss: 0.250
[136,     1] loss: 0.217
[137,     1] loss: 0.204
[138,     1] loss: 0.255
[139,     1] loss: 0.241
[140,     1] loss: 0.208
[141,     1] loss: 0.181
[142,     1] loss: 0.204
[143,     1] loss: 0.161
[144,     1] loss: 0.206
Early stopping applied (best metric=0.18861937522888184)
Finished Training
Total time taken: 15.707001209259033
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.698
[3,     1] loss: 0.679
[4,     1] loss: 0.638
[5,     1] loss: 0.616
[6,     1] loss: 0.561
[7,     1] loss: 0.554
[8,     1] loss: 0.506
[9,     1] loss: 0.478
[10,     1] loss: 0.452
[11,     1] loss: 0.389
[12,     1] loss: 0.409
[13,     1] loss: 0.358
[14,     1] loss: 0.355
[15,     1] loss: 0.358
[16,     1] loss: 0.269
[17,     1] loss: 0.306
[18,     1] loss: 0.253
[19,     1] loss: 0.281
[20,     1] loss: 0.282
[21,     1] loss: 0.290
[22,     1] loss: 0.233
[23,     1] loss: 0.317
[24,     1] loss: 0.193
[25,     1] loss: 0.228
[26,     1] loss: 0.230
[27,     1] loss: 0.262
[28,     1] loss: 0.161
[29,     1] loss: 0.189
[30,     1] loss: 0.215
[31,     1] loss: 0.186
[32,     1] loss: 0.138
[33,     1] loss: 0.156
[34,     1] loss: 0.212
[35,     1] loss: 0.155
[36,     1] loss: 0.165
[37,     1] loss: 0.233
[38,     1] loss: 0.146
[39,     1] loss: 0.251
[40,     1] loss: 0.233
[41,     1] loss: 0.144
[42,     1] loss: 0.203
[43,     1] loss: 0.164
[44,     1] loss: 0.163
[45,     1] loss: 0.171
[46,     1] loss: 0.160
[47,     1] loss: 0.118
[48,     1] loss: 0.100
[49,     1] loss: 0.180
[50,     1] loss: 0.148
[51,     1] loss: 0.192
[52,     1] loss: 0.170
[53,     1] loss: 0.163
[54,     1] loss: 0.119
[55,     1] loss: 0.261
[56,     1] loss: 0.154
[57,     1] loss: 0.211
[58,     1] loss: 0.145
[59,     1] loss: 0.167
[60,     1] loss: 0.169
[61,     1] loss: 0.143
[62,     1] loss: 0.153
[63,     1] loss: 0.137
[64,     1] loss: 0.140
[65,     1] loss: 0.114
[66,     1] loss: 0.136
[67,     1] loss: 0.136
[68,     1] loss: 0.131
[69,     1] loss: 0.091
[70,     1] loss: 0.135
[71,     1] loss: 0.081
[72,     1] loss: 0.162
[73,     1] loss: 0.092
[74,     1] loss: 0.141
[75,     1] loss: 0.107
[76,     1] loss: 0.107
[77,     1] loss: 0.139
[78,     1] loss: 0.166
[79,     1] loss: 0.115
[80,     1] loss: 0.099
[81,     1] loss: 0.128
[82,     1] loss: 0.175
[83,     1] loss: 0.160
[84,     1] loss: 0.129
[85,     1] loss: 0.123
[86,     1] loss: 0.102
[87,     1] loss: 0.106
[88,     1] loss: 0.114
[89,     1] loss: 0.096
[90,     1] loss: 0.105
[91,     1] loss: 0.090
[92,     1] loss: 0.133
[93,     1] loss: 0.253
[94,     1] loss: 0.184
[95,     1] loss: 0.186
[96,     1] loss: 0.160
[97,     1] loss: 0.146
[98,     1] loss: 0.151
[99,     1] loss: 0.159
[100,     1] loss: 0.151
[101,     1] loss: 0.137
[102,     1] loss: 0.132
[103,     1] loss: 0.118
[104,     1] loss: 0.109
[105,     1] loss: 0.128
[106,     1] loss: 0.104
[107,     1] loss: 0.098
[108,     1] loss: 0.113
[109,     1] loss: 0.075
[110,     1] loss: 0.124
[111,     1] loss: 0.117
[112,     1] loss: 0.181
[113,     1] loss: 0.210
[114,     1] loss: 0.220
[115,     1] loss: 0.161
[116,     1] loss: 0.166
[117,     1] loss: 0.143
[118,     1] loss: 0.114
[119,     1] loss: 0.216
[120,     1] loss: 0.165
[121,     1] loss: 0.178
[122,     1] loss: 0.176
[123,     1] loss: 0.144
[124,     1] loss: 0.121
[125,     1] loss: 0.143
[126,     1] loss: 0.145
[127,     1] loss: 0.154
[128,     1] loss: 0.176
[129,     1] loss: 0.155
[130,     1] loss: 0.117
[131,     1] loss: 0.103
[132,     1] loss: 0.115
[133,     1] loss: 0.115
[134,     1] loss: 0.080
[135,     1] loss: 0.080
[136,     1] loss: 0.094
[137,     1] loss: 0.097
[138,     1] loss: 0.090
[139,     1] loss: 0.072
[140,     1] loss: 0.134
[141,     1] loss: 0.136
[142,     1] loss: 0.280
[143,     1] loss: 0.143
[144,     1] loss: 0.264
[145,     1] loss: 0.129
Early stopping applied (best metric=0.3199557662010193)
Finished Training
Total time taken: 15.696999788284302
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.696
[3,     1] loss: 0.689
[4,     1] loss: 0.682
[5,     1] loss: 0.670
[6,     1] loss: 0.645
[7,     1] loss: 0.618
[8,     1] loss: 0.576
[9,     1] loss: 0.556
[10,     1] loss: 0.551
[11,     1] loss: 0.541
[12,     1] loss: 0.470
[13,     1] loss: 0.436
[14,     1] loss: 0.408
[15,     1] loss: 0.384
[16,     1] loss: 0.329
[17,     1] loss: 0.477
[18,     1] loss: 0.358
[19,     1] loss: 0.324
[20,     1] loss: 0.344
[21,     1] loss: 0.260
[22,     1] loss: 0.343
[23,     1] loss: 0.257
[24,     1] loss: 0.335
[25,     1] loss: 0.238
[26,     1] loss: 0.379
[27,     1] loss: 0.293
[28,     1] loss: 0.260
[29,     1] loss: 0.298
[30,     1] loss: 0.280
[31,     1] loss: 0.283
[32,     1] loss: 0.261
[33,     1] loss: 0.204
[34,     1] loss: 0.259
[35,     1] loss: 0.210
[36,     1] loss: 0.247
[37,     1] loss: 0.191
[38,     1] loss: 0.279
[39,     1] loss: 0.291
[40,     1] loss: 0.262
[41,     1] loss: 0.262
[42,     1] loss: 0.256
[43,     1] loss: 0.218
[44,     1] loss: 0.206
[45,     1] loss: 0.196
[46,     1] loss: 0.166
[47,     1] loss: 0.132
[48,     1] loss: 0.140
[49,     1] loss: 0.122
[50,     1] loss: 0.119
[51,     1] loss: 0.112
[52,     1] loss: 0.135
[53,     1] loss: 0.110
[54,     1] loss: 0.129
[55,     1] loss: 0.134
[56,     1] loss: 0.086
[57,     1] loss: 0.099
[58,     1] loss: 0.114
[59,     1] loss: 0.171
[60,     1] loss: 0.112
[61,     1] loss: 0.108
[62,     1] loss: 0.088
[63,     1] loss: 0.092
[64,     1] loss: 0.066
[65,     1] loss: 0.099
[66,     1] loss: 0.066
[67,     1] loss: 0.068
Early stopping applied (best metric=0.3946531414985657)
Finished Training
Total time taken: 7.32999849319458
{'Hydroxylation-K Validation Accuracy': 0.869645390070922, 'Hydroxylation-K Validation Sensitivity': 0.9044444444444445, 'Hydroxylation-K Validation Specificity': 0.8610526315789474, 'Hydroxylation-K Validation Precision': 0.6396042421630657, 'Hydroxylation-K AUC ROC': 0.890608187134503, 'Hydroxylation-K AUC PR': 0.7262907851857844, 'Hydroxylation-K MCC': 0.6837013668047123, 'Hydroxylation-K F1': 0.7426098948259131, 'Validation Loss (Hydroxylation-K)': 0.2836821925640106, 'Validation Loss (total)': 0.2836821925640106, 'TimeToTrain': 9.786230907440185}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005088498941479085,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2527123384,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.969333170992524}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.703
[3,     1] loss: 0.686
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007361619751869677,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2098601904,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.186616172251647}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.685
[3,     1] loss: 0.669
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004097909138930987,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2506295545,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.284406685627992}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.686
[3,     1] loss: 0.661
[4,     1] loss: 0.610
[5,     1] loss: 0.579
[6,     1] loss: 0.529
[7,     1] loss: 0.475
[8,     1] loss: 0.439
[9,     1] loss: 0.389
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005350871503417441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1429948117,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.932431561459038}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.686
[3,     1] loss: 0.691
[4,     1] loss: 0.651
[5,     1] loss: 0.607
[6,     1] loss: 0.600
[7,     1] loss: 0.546
[8,     1] loss: 0.480
[9,     1] loss: 0.449
[10,     1] loss: 0.434
[11,     1] loss: 0.388
[12,     1] loss: 0.367
[13,     1] loss: 0.319
[14,     1] loss: 0.376
[15,     1] loss: 0.243
[16,     1] loss: 0.294
[17,     1] loss: 0.294
[18,     1] loss: 0.295
[19,     1] loss: 0.248
[20,     1] loss: 0.182
[21,     1] loss: 0.194
[22,     1] loss: 0.200
[23,     1] loss: 0.224
[24,     1] loss: 0.161
[25,     1] loss: 0.170
[26,     1] loss: 0.141
[27,     1] loss: 0.186
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003672171246158753,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3398342814,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.582023478467361}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.689
[3,     1] loss: 0.673
[4,     1] loss: 0.635
[5,     1] loss: 0.601
[6,     1] loss: 0.562
[7,     1] loss: 0.545
[8,     1] loss: 0.476
[9,     1] loss: 0.411
[10,     1] loss: 0.402
[11,     1] loss: 0.378
[12,     1] loss: 0.357
[13,     1] loss: 0.334
[14,     1] loss: 0.285
[15,     1] loss: 0.347
[16,     1] loss: 0.409
[17,     1] loss: 0.326
[18,     1] loss: 0.348
[19,     1] loss: 0.268
[20,     1] loss: 0.285
[21,     1] loss: 0.284
[22,     1] loss: 0.314
[23,     1] loss: 0.316
[24,     1] loss: 0.258
[25,     1] loss: 0.252
[26,     1] loss: 0.327
[27,     1] loss: 0.256
[28,     1] loss: 0.228
[29,     1] loss: 0.228
[30,     1] loss: 0.241
[31,     1] loss: 0.197
[32,     1] loss: 0.187
[33,     1] loss: 0.167
[34,     1] loss: 0.175
[35,     1] loss: 0.169
[36,     1] loss: 0.198
[37,     1] loss: 0.131
[38,     1] loss: 0.177
[39,     1] loss: 0.118
[40,     1] loss: 0.102
[41,     1] loss: 0.205
[42,     1] loss: 0.153
[43,     1] loss: 0.289
[44,     1] loss: 0.166
[45,     1] loss: 0.165
[46,     1] loss: 0.254
[47,     1] loss: 0.164
[48,     1] loss: 0.119
[49,     1] loss: 0.285
[50,     1] loss: 0.164
[51,     1] loss: 0.200
[52,     1] loss: 0.183
[53,     1] loss: 0.187
[54,     1] loss: 0.165
[55,     1] loss: 0.168
[56,     1] loss: 0.198
[57,     1] loss: 0.149
[58,     1] loss: 0.139
[59,     1] loss: 0.172
[60,     1] loss: 0.105
[61,     1] loss: 0.122
[62,     1] loss: 0.146
[63,     1] loss: 0.134
[64,     1] loss: 0.123
[65,     1] loss: 0.101
[66,     1] loss: 0.140
[67,     1] loss: 0.117
[68,     1] loss: 0.069
[69,     1] loss: 0.070
[70,     1] loss: 0.067
[71,     1] loss: 0.093
[72,     1] loss: 0.067
[73,     1] loss: 0.066
[74,     1] loss: 0.059
[75,     1] loss: 0.113
[76,     1] loss: 0.107
[77,     1] loss: 0.054
[78,     1] loss: 0.080
[79,     1] loss: 0.057
Early stopping applied (best metric=0.30144983530044556)
Finished Training
Total time taken: 8.614999532699585
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.692
[3,     1] loss: 0.682
[4,     1] loss: 0.660
[5,     1] loss: 0.628
[6,     1] loss: 0.585
[7,     1] loss: 0.557
[8,     1] loss: 0.539
[9,     1] loss: 0.507
[10,     1] loss: 0.432
[11,     1] loss: 0.424
[12,     1] loss: 0.364
[13,     1] loss: 0.349
[14,     1] loss: 0.343
[15,     1] loss: 0.337
[16,     1] loss: 0.308
[17,     1] loss: 0.289
[18,     1] loss: 0.298
[19,     1] loss: 0.303
[20,     1] loss: 0.302
[21,     1] loss: 0.288
[22,     1] loss: 0.254
[23,     1] loss: 0.290
[24,     1] loss: 0.278
[25,     1] loss: 0.231
[26,     1] loss: 0.264
[27,     1] loss: 0.224
[28,     1] loss: 0.218
[29,     1] loss: 0.175
[30,     1] loss: 0.163
[31,     1] loss: 0.178
[32,     1] loss: 0.203
[33,     1] loss: 0.151
[34,     1] loss: 0.196
[35,     1] loss: 0.188
[36,     1] loss: 0.234
[37,     1] loss: 0.239
[38,     1] loss: 0.157
[39,     1] loss: 0.161
[40,     1] loss: 0.154
[41,     1] loss: 0.186
[42,     1] loss: 0.278
[43,     1] loss: 0.158
[44,     1] loss: 0.160
[45,     1] loss: 0.146
[46,     1] loss: 0.117
[47,     1] loss: 0.175
[48,     1] loss: 0.186
[49,     1] loss: 0.137
[50,     1] loss: 0.130
[51,     1] loss: 0.123
[52,     1] loss: 0.102
[53,     1] loss: 0.118
[54,     1] loss: 0.120
[55,     1] loss: 0.127
[56,     1] loss: 0.158
[57,     1] loss: 0.157
[58,     1] loss: 0.341
[59,     1] loss: 0.257
[60,     1] loss: 0.286
[61,     1] loss: 0.256
[62,     1] loss: 0.272
[63,     1] loss: 0.213
[64,     1] loss: 0.275
[65,     1] loss: 0.206
[66,     1] loss: 0.230
[67,     1] loss: 0.221
Early stopping applied (best metric=0.2596915662288666)
Finished Training
Total time taken: 7.280002593994141
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.685
[3,     1] loss: 0.668
[4,     1] loss: 0.652
[5,     1] loss: 0.612
[6,     1] loss: 0.595
[7,     1] loss: 0.558
[8,     1] loss: 0.527
[9,     1] loss: 0.511
[10,     1] loss: 0.475
[11,     1] loss: 0.443
[12,     1] loss: 0.385
[13,     1] loss: 0.418
[14,     1] loss: 0.420
[15,     1] loss: 0.302
[16,     1] loss: 0.338
[17,     1] loss: 0.360
[18,     1] loss: 0.259
[19,     1] loss: 0.346
[20,     1] loss: 0.442
[21,     1] loss: 0.295
[22,     1] loss: 0.313
[23,     1] loss: 0.341
[24,     1] loss: 0.253
[25,     1] loss: 0.319
[26,     1] loss: 0.258
[27,     1] loss: 0.329
[28,     1] loss: 0.276
[29,     1] loss: 0.263
[30,     1] loss: 0.272
[31,     1] loss: 0.259
[32,     1] loss: 0.241
[33,     1] loss: 0.201
[34,     1] loss: 0.254
[35,     1] loss: 0.173
[36,     1] loss: 0.230
[37,     1] loss: 0.212
[38,     1] loss: 0.211
[39,     1] loss: 0.233
[40,     1] loss: 0.209
[41,     1] loss: 0.140
[42,     1] loss: 0.144
[43,     1] loss: 0.136
[44,     1] loss: 0.123
[45,     1] loss: 0.147
[46,     1] loss: 0.148
[47,     1] loss: 0.132
[48,     1] loss: 0.112
[49,     1] loss: 0.175
[50,     1] loss: 0.100
[51,     1] loss: 0.141
[52,     1] loss: 0.111
[53,     1] loss: 0.184
[54,     1] loss: 0.104
[55,     1] loss: 0.142
[56,     1] loss: 0.072
[57,     1] loss: 0.137
[58,     1] loss: 0.194
[59,     1] loss: 0.246
[60,     1] loss: 0.151
[61,     1] loss: 0.189
[62,     1] loss: 0.181
[63,     1] loss: 0.150
[64,     1] loss: 0.175
[65,     1] loss: 0.121
[66,     1] loss: 0.134
[67,     1] loss: 0.160
[68,     1] loss: 0.112
Early stopping applied (best metric=0.3092647194862366)
Finished Training
Total time taken: 7.4719977378845215
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.691
[3,     1] loss: 0.697
[4,     1] loss: 0.687
[5,     1] loss: 0.667
[6,     1] loss: 0.652
[7,     1] loss: 0.623
[8,     1] loss: 0.601
[9,     1] loss: 0.549
[10,     1] loss: 0.550
[11,     1] loss: 0.497
[12,     1] loss: 0.465
[13,     1] loss: 0.456
[14,     1] loss: 0.420
[15,     1] loss: 0.354
[16,     1] loss: 0.330
[17,     1] loss: 0.360
[18,     1] loss: 0.460
[19,     1] loss: 0.285
[20,     1] loss: 0.311
[21,     1] loss: 0.281
[22,     1] loss: 0.294
[23,     1] loss: 0.333
[24,     1] loss: 0.253
[25,     1] loss: 0.294
[26,     1] loss: 0.215
[27,     1] loss: 0.215
[28,     1] loss: 0.241
[29,     1] loss: 0.132
[30,     1] loss: 0.147
[31,     1] loss: 0.193
[32,     1] loss: 0.148
[33,     1] loss: 0.262
[34,     1] loss: 0.178
[35,     1] loss: 0.132
[36,     1] loss: 0.145
[37,     1] loss: 0.145
[38,     1] loss: 0.150
[39,     1] loss: 0.145
[40,     1] loss: 0.150
[41,     1] loss: 0.121
[42,     1] loss: 0.179
[43,     1] loss: 0.125
[44,     1] loss: 0.153
[45,     1] loss: 0.107
[46,     1] loss: 0.126
[47,     1] loss: 0.123
[48,     1] loss: 0.083
[49,     1] loss: 0.157
[50,     1] loss: 0.123
[51,     1] loss: 0.096
[52,     1] loss: 0.166
[53,     1] loss: 0.071
[54,     1] loss: 0.113
[55,     1] loss: 0.088
[56,     1] loss: 0.072
[57,     1] loss: 0.114
[58,     1] loss: 0.083
[59,     1] loss: 0.097
[60,     1] loss: 0.065
[61,     1] loss: 0.077
Early stopping applied (best metric=0.3321492075920105)
Finished Training
Total time taken: 6.641000032424927
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.689
[3,     1] loss: 0.685
[4,     1] loss: 0.657
[5,     1] loss: 0.622
[6,     1] loss: 0.603
[7,     1] loss: 0.553
[8,     1] loss: 0.525
[9,     1] loss: 0.486
[10,     1] loss: 0.444
[11,     1] loss: 0.387
[12,     1] loss: 0.350
[13,     1] loss: 0.339
[14,     1] loss: 0.360
[15,     1] loss: 0.470
[16,     1] loss: 0.337
[17,     1] loss: 0.309
[18,     1] loss: 0.336
[19,     1] loss: 0.366
[20,     1] loss: 0.270
[21,     1] loss: 0.358
[22,     1] loss: 0.346
[23,     1] loss: 0.292
[24,     1] loss: 0.266
[25,     1] loss: 0.292
[26,     1] loss: 0.315
[27,     1] loss: 0.269
[28,     1] loss: 0.256
[29,     1] loss: 0.250
[30,     1] loss: 0.269
[31,     1] loss: 0.222
[32,     1] loss: 0.226
[33,     1] loss: 0.189
[34,     1] loss: 0.167
[35,     1] loss: 0.169
[36,     1] loss: 0.194
[37,     1] loss: 0.181
[38,     1] loss: 0.128
[39,     1] loss: 0.140
[40,     1] loss: 0.299
[41,     1] loss: 0.160
[42,     1] loss: 0.189
[43,     1] loss: 0.133
[44,     1] loss: 0.133
[45,     1] loss: 0.125
[46,     1] loss: 0.174
[47,     1] loss: 0.181
[48,     1] loss: 0.123
[49,     1] loss: 0.130
[50,     1] loss: 0.151
[51,     1] loss: 0.124
[52,     1] loss: 0.157
[53,     1] loss: 0.138
[54,     1] loss: 0.212
[55,     1] loss: 0.112
[56,     1] loss: 0.199
[57,     1] loss: 0.139
[58,     1] loss: 0.114
[59,     1] loss: 0.155
[60,     1] loss: 0.122
[61,     1] loss: 0.142
Early stopping applied (best metric=0.32821527123451233)
Finished Training
Total time taken: 6.708001375198364
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.695
[4,     1] loss: 0.671
[5,     1] loss: 0.643
[6,     1] loss: 0.620
[7,     1] loss: 0.586
[8,     1] loss: 0.531
[9,     1] loss: 0.535
[10,     1] loss: 0.499
[11,     1] loss: 0.475
[12,     1] loss: 0.400
[13,     1] loss: 0.391
[14,     1] loss: 0.347
[15,     1] loss: 0.366
[16,     1] loss: 0.358
[17,     1] loss: 0.324
[18,     1] loss: 0.341
[19,     1] loss: 0.276
[20,     1] loss: 0.352
[21,     1] loss: 0.291
[22,     1] loss: 0.433
[23,     1] loss: 0.324
[24,     1] loss: 0.323
[25,     1] loss: 0.279
[26,     1] loss: 0.246
[27,     1] loss: 0.309
[28,     1] loss: 0.290
[29,     1] loss: 0.244
[30,     1] loss: 0.250
[31,     1] loss: 0.217
[32,     1] loss: 0.235
[33,     1] loss: 0.236
[34,     1] loss: 0.234
[35,     1] loss: 0.209
[36,     1] loss: 0.185
[37,     1] loss: 0.205
[38,     1] loss: 0.261
[39,     1] loss: 0.149
[40,     1] loss: 0.213
[41,     1] loss: 0.258
[42,     1] loss: 0.182
[43,     1] loss: 0.174
[44,     1] loss: 0.207
[45,     1] loss: 0.226
[46,     1] loss: 0.167
[47,     1] loss: 0.124
[48,     1] loss: 0.152
[49,     1] loss: 0.138
[50,     1] loss: 0.148
[51,     1] loss: 0.123
[52,     1] loss: 0.096
[53,     1] loss: 0.065
[54,     1] loss: 0.101
[55,     1] loss: 0.069
[56,     1] loss: 0.126
[57,     1] loss: 0.099
[58,     1] loss: 0.160
[59,     1] loss: 0.084
[60,     1] loss: 0.136
[61,     1] loss: 0.121
[62,     1] loss: 0.056
[63,     1] loss: 0.122
[64,     1] loss: 0.096
[65,     1] loss: 0.076
[66,     1] loss: 0.206
[67,     1] loss: 0.058
[68,     1] loss: 0.118
[69,     1] loss: 0.134
[70,     1] loss: 0.102
[71,     1] loss: 0.087
[72,     1] loss: 0.081
[73,     1] loss: 0.140
[74,     1] loss: 0.096
[75,     1] loss: 0.079
Early stopping applied (best metric=0.17639005184173584)
Finished Training
Total time taken: 8.169000387191772
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.672
[4,     1] loss: 0.646
[5,     1] loss: 0.616
[6,     1] loss: 0.592
[7,     1] loss: 0.546
[8,     1] loss: 0.510
[9,     1] loss: 0.470
[10,     1] loss: 0.428
[11,     1] loss: 0.384
[12,     1] loss: 0.376
[13,     1] loss: 0.354
[14,     1] loss: 0.362
[15,     1] loss: 0.347
[16,     1] loss: 0.369
[17,     1] loss: 0.292
[18,     1] loss: 0.319
[19,     1] loss: 0.298
[20,     1] loss: 0.258
[21,     1] loss: 0.277
[22,     1] loss: 0.232
[23,     1] loss: 0.226
[24,     1] loss: 0.196
[25,     1] loss: 0.197
[26,     1] loss: 0.213
[27,     1] loss: 0.183
[28,     1] loss: 0.181
[29,     1] loss: 0.198
[30,     1] loss: 0.192
[31,     1] loss: 0.185
[32,     1] loss: 0.266
[33,     1] loss: 0.295
[34,     1] loss: 0.119
[35,     1] loss: 0.196
[36,     1] loss: 0.154
[37,     1] loss: 0.148
[38,     1] loss: 0.167
[39,     1] loss: 0.164
[40,     1] loss: 0.179
[41,     1] loss: 0.153
[42,     1] loss: 0.163
[43,     1] loss: 0.149
[44,     1] loss: 0.128
[45,     1] loss: 0.147
[46,     1] loss: 0.160
[47,     1] loss: 0.102
[48,     1] loss: 0.103
[49,     1] loss: 0.169
[50,     1] loss: 0.110
[51,     1] loss: 0.131
[52,     1] loss: 0.120
[53,     1] loss: 0.120
[54,     1] loss: 0.116
[55,     1] loss: 0.145
[56,     1] loss: 0.079
[57,     1] loss: 0.088
[58,     1] loss: 0.108
Early stopping applied (best metric=0.4445335268974304)
Finished Training
Total time taken: 6.382998466491699
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.703
[3,     1] loss: 0.684
[4,     1] loss: 0.666
[5,     1] loss: 0.643
[6,     1] loss: 0.606
[7,     1] loss: 0.565
[8,     1] loss: 0.553
[9,     1] loss: 0.511
[10,     1] loss: 0.492
[11,     1] loss: 0.458
[12,     1] loss: 0.426
[13,     1] loss: 0.429
[14,     1] loss: 0.375
[15,     1] loss: 0.341
[16,     1] loss: 0.374
[17,     1] loss: 0.298
[18,     1] loss: 0.314
[19,     1] loss: 0.237
[20,     1] loss: 0.231
[21,     1] loss: 0.234
[22,     1] loss: 0.239
[23,     1] loss: 0.176
[24,     1] loss: 0.188
[25,     1] loss: 0.196
[26,     1] loss: 0.205
[27,     1] loss: 0.146
[28,     1] loss: 0.236
[29,     1] loss: 0.137
[30,     1] loss: 0.152
[31,     1] loss: 0.201
[32,     1] loss: 0.183
[33,     1] loss: 0.203
[34,     1] loss: 0.140
[35,     1] loss: 0.143
[36,     1] loss: 0.197
[37,     1] loss: 0.137
[38,     1] loss: 0.135
[39,     1] loss: 0.159
[40,     1] loss: 0.204
[41,     1] loss: 0.179
[42,     1] loss: 0.100
[43,     1] loss: 0.206
[44,     1] loss: 0.138
[45,     1] loss: 0.192
[46,     1] loss: 0.167
[47,     1] loss: 0.116
[48,     1] loss: 0.120
[49,     1] loss: 0.166
[50,     1] loss: 0.098
[51,     1] loss: 0.133
[52,     1] loss: 0.148
[53,     1] loss: 0.126
[54,     1] loss: 0.120
[55,     1] loss: 0.084
[56,     1] loss: 0.123
[57,     1] loss: 0.110
[58,     1] loss: 0.139
[59,     1] loss: 0.102
[60,     1] loss: 0.126
[61,     1] loss: 0.130
[62,     1] loss: 0.167
[63,     1] loss: 0.162
[64,     1] loss: 0.103
Early stopping applied (best metric=0.3364355266094208)
Finished Training
Total time taken: 6.969000339508057
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.657
[5,     1] loss: 0.631
[6,     1] loss: 0.612
[7,     1] loss: 0.571
[8,     1] loss: 0.538
[9,     1] loss: 0.506
[10,     1] loss: 0.469
[11,     1] loss: 0.424
[12,     1] loss: 0.422
[13,     1] loss: 0.391
[14,     1] loss: 0.343
[15,     1] loss: 0.365
[16,     1] loss: 0.397
[17,     1] loss: 0.293
[18,     1] loss: 0.276
[19,     1] loss: 0.249
[20,     1] loss: 0.266
[21,     1] loss: 0.237
[22,     1] loss: 0.197
[23,     1] loss: 0.194
[24,     1] loss: 0.225
[25,     1] loss: 0.179
[26,     1] loss: 0.236
[27,     1] loss: 0.176
[28,     1] loss: 0.152
[29,     1] loss: 0.199
[30,     1] loss: 0.131
[31,     1] loss: 0.158
[32,     1] loss: 0.128
[33,     1] loss: 0.248
[34,     1] loss: 0.119
[35,     1] loss: 0.138
[36,     1] loss: 0.148
[37,     1] loss: 0.152
[38,     1] loss: 0.102
[39,     1] loss: 0.075
[40,     1] loss: 0.188
[41,     1] loss: 0.108
[42,     1] loss: 0.111
[43,     1] loss: 0.094
[44,     1] loss: 0.100
[45,     1] loss: 0.082
[46,     1] loss: 0.137
[47,     1] loss: 0.062
[48,     1] loss: 0.079
[49,     1] loss: 0.127
[50,     1] loss: 0.048
[51,     1] loss: 0.129
[52,     1] loss: 0.088
[53,     1] loss: 0.062
[54,     1] loss: 0.080
[55,     1] loss: 0.052
[56,     1] loss: 0.037
[57,     1] loss: 0.071
[58,     1] loss: 0.049
[59,     1] loss: 0.073
[60,     1] loss: 0.047
[61,     1] loss: 0.034
Early stopping applied (best metric=0.41236770153045654)
Finished Training
Total time taken: 6.686001300811768
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.675
[4,     1] loss: 0.642
[5,     1] loss: 0.600
[6,     1] loss: 0.576
[7,     1] loss: 0.538
[8,     1] loss: 0.518
[9,     1] loss: 0.496
[10,     1] loss: 0.448
[11,     1] loss: 0.400
[12,     1] loss: 0.433
[13,     1] loss: 0.378
[14,     1] loss: 0.305
[15,     1] loss: 0.354
[16,     1] loss: 0.315
[17,     1] loss: 0.289
[18,     1] loss: 0.307
[19,     1] loss: 0.316
[20,     1] loss: 0.256
[21,     1] loss: 0.232
[22,     1] loss: 0.258
[23,     1] loss: 0.294
[24,     1] loss: 0.210
[25,     1] loss: 0.265
[26,     1] loss: 0.218
[27,     1] loss: 0.251
[28,     1] loss: 0.291
[29,     1] loss: 0.238
[30,     1] loss: 0.266
[31,     1] loss: 0.262
[32,     1] loss: 0.256
[33,     1] loss: 0.209
[34,     1] loss: 0.213
[35,     1] loss: 0.234
[36,     1] loss: 0.214
[37,     1] loss: 0.201
[38,     1] loss: 0.244
[39,     1] loss: 0.179
[40,     1] loss: 0.169
[41,     1] loss: 0.274
[42,     1] loss: 0.199
[43,     1] loss: 0.192
[44,     1] loss: 0.169
[45,     1] loss: 0.148
[46,     1] loss: 0.155
[47,     1] loss: 0.187
[48,     1] loss: 0.184
[49,     1] loss: 0.157
[50,     1] loss: 0.177
[51,     1] loss: 0.198
[52,     1] loss: 0.146
[53,     1] loss: 0.206
[54,     1] loss: 0.135
[55,     1] loss: 0.171
Early stopping applied (best metric=0.4994516372680664)
Finished Training
Total time taken: 5.973608493804932
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.651
[5,     1] loss: 0.625
[6,     1] loss: 0.584
[7,     1] loss: 0.538
[8,     1] loss: 0.523
[9,     1] loss: 0.419
[10,     1] loss: 0.454
[11,     1] loss: 0.428
[12,     1] loss: 0.368
[13,     1] loss: 0.330
[14,     1] loss: 0.319
[15,     1] loss: 0.291
[16,     1] loss: 0.326
[17,     1] loss: 0.283
[18,     1] loss: 0.311
[19,     1] loss: 0.284
[20,     1] loss: 0.230
[21,     1] loss: 0.238
[22,     1] loss: 0.231
[23,     1] loss: 0.255
[24,     1] loss: 0.222
[25,     1] loss: 0.244
[26,     1] loss: 0.245
[27,     1] loss: 0.183
[28,     1] loss: 0.294
[29,     1] loss: 0.246
[30,     1] loss: 0.162
[31,     1] loss: 0.167
[32,     1] loss: 0.220
[33,     1] loss: 0.182
[34,     1] loss: 0.145
[35,     1] loss: 0.215
[36,     1] loss: 0.232
[37,     1] loss: 0.159
[38,     1] loss: 0.153
[39,     1] loss: 0.120
[40,     1] loss: 0.140
[41,     1] loss: 0.112
[42,     1] loss: 0.119
[43,     1] loss: 0.129
[44,     1] loss: 0.104
[45,     1] loss: 0.096
[46,     1] loss: 0.104
[47,     1] loss: 0.121
[48,     1] loss: 0.101
[49,     1] loss: 0.100
[50,     1] loss: 0.070
[51,     1] loss: 0.109
[52,     1] loss: 0.103
[53,     1] loss: 0.078
[54,     1] loss: 0.071
[55,     1] loss: 0.132
[56,     1] loss: 0.077
[57,     1] loss: 0.072
[58,     1] loss: 0.075
[59,     1] loss: 0.085
[60,     1] loss: 0.111
Early stopping applied (best metric=0.29926013946533203)
Finished Training
Total time taken: 6.568000078201294
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.684
[4,     1] loss: 0.658
[5,     1] loss: 0.645
[6,     1] loss: 0.611
[7,     1] loss: 0.574
[8,     1] loss: 0.553
[9,     1] loss: 0.517
[10,     1] loss: 0.448
[11,     1] loss: 0.456
[12,     1] loss: 0.428
[13,     1] loss: 0.376
[14,     1] loss: 0.389
[15,     1] loss: 0.375
[16,     1] loss: 0.279
[17,     1] loss: 0.347
[18,     1] loss: 0.323
[19,     1] loss: 0.369
[20,     1] loss: 0.297
[21,     1] loss: 0.362
[22,     1] loss: 0.289
[23,     1] loss: 0.358
[24,     1] loss: 0.211
[25,     1] loss: 0.237
[26,     1] loss: 0.224
[27,     1] loss: 0.258
[28,     1] loss: 0.224
[29,     1] loss: 0.179
[30,     1] loss: 0.163
[31,     1] loss: 0.215
[32,     1] loss: 0.119
[33,     1] loss: 0.142
[34,     1] loss: 0.272
[35,     1] loss: 0.220
[36,     1] loss: 0.137
[37,     1] loss: 0.146
[38,     1] loss: 0.162
[39,     1] loss: 0.163
[40,     1] loss: 0.155
[41,     1] loss: 0.130
[42,     1] loss: 0.279
[43,     1] loss: 0.145
[44,     1] loss: 0.260
[45,     1] loss: 0.186
[46,     1] loss: 0.121
[47,     1] loss: 0.196
[48,     1] loss: 0.149
[49,     1] loss: 0.164
[50,     1] loss: 0.146
[51,     1] loss: 0.135
[52,     1] loss: 0.136
[53,     1] loss: 0.112
[54,     1] loss: 0.133
[55,     1] loss: 0.129
[56,     1] loss: 0.115
[57,     1] loss: 0.144
[58,     1] loss: 0.121
[59,     1] loss: 0.116
[60,     1] loss: 0.143
[61,     1] loss: 0.104
[62,     1] loss: 0.145
[63,     1] loss: 0.133
[64,     1] loss: 0.123
[65,     1] loss: 0.130
[66,     1] loss: 0.143
[67,     1] loss: 0.265
[68,     1] loss: 0.119
[69,     1] loss: 0.236
[70,     1] loss: 0.114
[71,     1] loss: 0.108
[72,     1] loss: 0.209
[73,     1] loss: 0.166
[74,     1] loss: 0.123
[75,     1] loss: 0.141
[76,     1] loss: 0.140
[77,     1] loss: 0.123
[78,     1] loss: 0.122
[79,     1] loss: 0.136
[80,     1] loss: 0.115
[81,     1] loss: 0.097
[82,     1] loss: 0.089
[83,     1] loss: 0.103
[84,     1] loss: 0.093
[85,     1] loss: 0.092
[86,     1] loss: 0.068
[87,     1] loss: 0.130
[88,     1] loss: 0.079
[89,     1] loss: 0.068
[90,     1] loss: 0.138
[91,     1] loss: 0.086
[92,     1] loss: 0.108
[93,     1] loss: 0.119
[94,     1] loss: 0.120
[95,     1] loss: 0.085
[96,     1] loss: 0.076
[97,     1] loss: 0.066
[98,     1] loss: 0.074
[99,     1] loss: 0.080
[100,     1] loss: 0.092
[101,     1] loss: 0.065
[102,     1] loss: 0.100
[103,     1] loss: 0.102
[104,     1] loss: 0.075
[105,     1] loss: 0.081
[106,     1] loss: 0.089
[107,     1] loss: 0.085
[108,     1] loss: 0.069
[109,     1] loss: 0.100
[110,     1] loss: 0.063
[111,     1] loss: 0.105
[112,     1] loss: 0.067
[113,     1] loss: 0.069
[114,     1] loss: 0.064
[115,     1] loss: 0.084
[116,     1] loss: 0.076
[117,     1] loss: 0.112
[118,     1] loss: 0.147
[119,     1] loss: 0.199
[120,     1] loss: 0.117
[121,     1] loss: 0.174
[122,     1] loss: 0.092
[123,     1] loss: 0.064
[124,     1] loss: 0.122
[125,     1] loss: 0.126
[126,     1] loss: 0.074
[127,     1] loss: 0.066
[128,     1] loss: 0.085
[129,     1] loss: 0.113
[130,     1] loss: 0.142
[131,     1] loss: 0.087
[132,     1] loss: 0.145
[133,     1] loss: 0.069
[134,     1] loss: 0.149
[135,     1] loss: 0.073
[136,     1] loss: 0.084
[137,     1] loss: 0.094
[138,     1] loss: 0.069
[139,     1] loss: 0.115
[140,     1] loss: 0.074
[141,     1] loss: 0.055
Early stopping applied (best metric=0.2343432903289795)
Finished Training
Total time taken: 15.311221837997437
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.681
[3,     1] loss: 0.692
[4,     1] loss: 0.643
[5,     1] loss: 0.630
[6,     1] loss: 0.596
[7,     1] loss: 0.533
[8,     1] loss: 0.519
[9,     1] loss: 0.485
[10,     1] loss: 0.449
[11,     1] loss: 0.443
[12,     1] loss: 0.383
[13,     1] loss: 0.479
[14,     1] loss: 0.378
[15,     1] loss: 0.359
[16,     1] loss: 0.356
[17,     1] loss: 0.410
[18,     1] loss: 0.324
[19,     1] loss: 0.274
[20,     1] loss: 0.258
[21,     1] loss: 0.264
[22,     1] loss: 0.244
[23,     1] loss: 0.255
[24,     1] loss: 0.248
[25,     1] loss: 0.243
[26,     1] loss: 0.252
[27,     1] loss: 0.233
[28,     1] loss: 0.305
[29,     1] loss: 0.243
[30,     1] loss: 0.189
[31,     1] loss: 0.287
[32,     1] loss: 0.158
[33,     1] loss: 0.229
[34,     1] loss: 0.222
[35,     1] loss: 0.164
[36,     1] loss: 0.235
[37,     1] loss: 0.252
[38,     1] loss: 0.189
[39,     1] loss: 0.183
[40,     1] loss: 0.173
[41,     1] loss: 0.204
[42,     1] loss: 0.130
[43,     1] loss: 0.200
[44,     1] loss: 0.166
[45,     1] loss: 0.173
[46,     1] loss: 0.136
[47,     1] loss: 0.178
[48,     1] loss: 0.143
[49,     1] loss: 0.172
[50,     1] loss: 0.153
[51,     1] loss: 0.110
[52,     1] loss: 0.175
[53,     1] loss: 0.115
[54,     1] loss: 0.175
[55,     1] loss: 0.160
[56,     1] loss: 0.090
[57,     1] loss: 0.162
[58,     1] loss: 0.113
[59,     1] loss: 0.096
[60,     1] loss: 0.158
[61,     1] loss: 0.109
[62,     1] loss: 0.101
[63,     1] loss: 0.123
[64,     1] loss: 0.063
[65,     1] loss: 0.070
[66,     1] loss: 0.123
[67,     1] loss: 0.077
[68,     1] loss: 0.179
[69,     1] loss: 0.110
Early stopping applied (best metric=0.399044930934906)
Finished Training
Total time taken: 7.6156086921691895
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.666
[4,     1] loss: 0.626
[5,     1] loss: 0.603
[6,     1] loss: 0.546
[7,     1] loss: 0.511
[8,     1] loss: 0.503
[9,     1] loss: 0.488
[10,     1] loss: 0.429
[11,     1] loss: 0.458
[12,     1] loss: 0.444
[13,     1] loss: 0.431
[14,     1] loss: 0.403
[15,     1] loss: 0.351
[16,     1] loss: 0.328
[17,     1] loss: 0.287
[18,     1] loss: 0.295
[19,     1] loss: 0.293
[20,     1] loss: 0.341
[21,     1] loss: 0.252
[22,     1] loss: 0.265
[23,     1] loss: 0.235
[24,     1] loss: 0.202
[25,     1] loss: 0.244
[26,     1] loss: 0.264
[27,     1] loss: 0.196
[28,     1] loss: 0.217
[29,     1] loss: 0.224
[30,     1] loss: 0.216
[31,     1] loss: 0.241
[32,     1] loss: 0.175
[33,     1] loss: 0.201
[34,     1] loss: 0.182
[35,     1] loss: 0.170
[36,     1] loss: 0.133
[37,     1] loss: 0.129
[38,     1] loss: 0.134
[39,     1] loss: 0.179
[40,     1] loss: 0.123
[41,     1] loss: 0.165
[42,     1] loss: 0.098
[43,     1] loss: 0.255
[44,     1] loss: 0.182
[45,     1] loss: 0.155
[46,     1] loss: 0.126
[47,     1] loss: 0.131
[48,     1] loss: 0.170
[49,     1] loss: 0.099
[50,     1] loss: 0.095
[51,     1] loss: 0.144
[52,     1] loss: 0.125
[53,     1] loss: 0.111
[54,     1] loss: 0.153
[55,     1] loss: 0.116
[56,     1] loss: 0.086
[57,     1] loss: 0.072
[58,     1] loss: 0.140
[59,     1] loss: 0.128
[60,     1] loss: 0.087
[61,     1] loss: 0.147
[62,     1] loss: 0.128
[63,     1] loss: 0.173
[64,     1] loss: 0.097
[65,     1] loss: 0.137
[66,     1] loss: 0.123
[67,     1] loss: 0.111
[68,     1] loss: 0.072
[69,     1] loss: 0.096
Early stopping applied (best metric=0.45158281922340393)
Finished Training
Total time taken: 7.5090038776397705
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.668
[4,     1] loss: 0.622
[5,     1] loss: 0.591
[6,     1] loss: 0.564
[7,     1] loss: 0.518
[8,     1] loss: 0.496
[9,     1] loss: 0.485
[10,     1] loss: 0.438
[11,     1] loss: 0.400
[12,     1] loss: 0.401
[13,     1] loss: 0.334
[14,     1] loss: 0.303
[15,     1] loss: 0.328
[16,     1] loss: 0.309
[17,     1] loss: 0.305
[18,     1] loss: 0.267
[19,     1] loss: 0.212
[20,     1] loss: 0.283
[21,     1] loss: 0.179
[22,     1] loss: 0.205
[23,     1] loss: 0.169
[24,     1] loss: 0.187
[25,     1] loss: 0.159
[26,     1] loss: 0.133
[27,     1] loss: 0.183
[28,     1] loss: 0.162
[29,     1] loss: 0.129
[30,     1] loss: 0.120
[31,     1] loss: 0.158
[32,     1] loss: 0.158
[33,     1] loss: 0.111
[34,     1] loss: 0.106
[35,     1] loss: 0.134
[36,     1] loss: 0.103
[37,     1] loss: 0.141
[38,     1] loss: 0.118
[39,     1] loss: 0.131
[40,     1] loss: 0.138
[41,     1] loss: 0.099
[42,     1] loss: 0.111
[43,     1] loss: 0.112
[44,     1] loss: 0.089
[45,     1] loss: 0.079
[46,     1] loss: 0.115
[47,     1] loss: 0.085
[48,     1] loss: 0.104
[49,     1] loss: 0.100
[50,     1] loss: 0.063
[51,     1] loss: 0.060
[52,     1] loss: 0.117
[53,     1] loss: 0.120
[54,     1] loss: 0.089
[55,     1] loss: 0.112
[56,     1] loss: 0.094
[57,     1] loss: 0.187
[58,     1] loss: 0.335
[59,     1] loss: 0.391
[60,     1] loss: 0.273
[61,     1] loss: 0.267
[62,     1] loss: 0.211
[63,     1] loss: 0.224
Early stopping applied (best metric=0.34051385521888733)
Finished Training
Total time taken: 6.919000625610352
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.684
[3,     1] loss: 0.663
[4,     1] loss: 0.626
[5,     1] loss: 0.616
[6,     1] loss: 0.590
[7,     1] loss: 0.564
[8,     1] loss: 0.529
[9,     1] loss: 0.492
[10,     1] loss: 0.458
[11,     1] loss: 0.460
[12,     1] loss: 0.489
[13,     1] loss: 0.403
[14,     1] loss: 0.369
[15,     1] loss: 0.383
[16,     1] loss: 0.426
[17,     1] loss: 0.352
[18,     1] loss: 0.326
[19,     1] loss: 0.315
[20,     1] loss: 0.305
[21,     1] loss: 0.304
[22,     1] loss: 0.327
[23,     1] loss: 0.328
[24,     1] loss: 0.292
[25,     1] loss: 0.249
[26,     1] loss: 0.277
[27,     1] loss: 0.278
[28,     1] loss: 0.244
[29,     1] loss: 0.234
[30,     1] loss: 0.225
[31,     1] loss: 0.177
[32,     1] loss: 0.203
[33,     1] loss: 0.198
[34,     1] loss: 0.199
[35,     1] loss: 0.196
[36,     1] loss: 0.237
[37,     1] loss: 0.173
[38,     1] loss: 0.191
[39,     1] loss: 0.180
[40,     1] loss: 0.161
[41,     1] loss: 0.175
[42,     1] loss: 0.173
[43,     1] loss: 0.138
[44,     1] loss: 0.187
[45,     1] loss: 0.123
[46,     1] loss: 0.161
[47,     1] loss: 0.180
[48,     1] loss: 0.151
[49,     1] loss: 0.144
[50,     1] loss: 0.133
[51,     1] loss: 0.162
[52,     1] loss: 0.104
[53,     1] loss: 0.153
[54,     1] loss: 0.122
[55,     1] loss: 0.167
[56,     1] loss: 0.146
[57,     1] loss: 0.168
[58,     1] loss: 0.197
[59,     1] loss: 0.207
[60,     1] loss: 0.102
[61,     1] loss: 0.185
[62,     1] loss: 0.126
[63,     1] loss: 0.186
[64,     1] loss: 0.106
[65,     1] loss: 0.160
[66,     1] loss: 0.117
[67,     1] loss: 0.110
[68,     1] loss: 0.114
[69,     1] loss: 0.093
[70,     1] loss: 0.110
[71,     1] loss: 0.136
[72,     1] loss: 0.097
[73,     1] loss: 0.210
[74,     1] loss: 0.115
[75,     1] loss: 0.201
[76,     1] loss: 0.065
[77,     1] loss: 0.094
[78,     1] loss: 0.077
[79,     1] loss: 0.062
[80,     1] loss: 0.116
[81,     1] loss: 0.087
[82,     1] loss: 0.108
[83,     1] loss: 0.172
[84,     1] loss: 0.090
[85,     1] loss: 0.283
[86,     1] loss: 0.085
[87,     1] loss: 0.137
[88,     1] loss: 0.123
[89,     1] loss: 0.063
[90,     1] loss: 0.109
[91,     1] loss: 0.101
[92,     1] loss: 0.097
[93,     1] loss: 0.119
[94,     1] loss: 0.067
[95,     1] loss: 0.083
[96,     1] loss: 0.099
[97,     1] loss: 0.065
[98,     1] loss: 0.082
[99,     1] loss: 0.084
[100,     1] loss: 0.074
[101,     1] loss: 0.048
[102,     1] loss: 0.055
[103,     1] loss: 0.064
[104,     1] loss: 0.058
[105,     1] loss: 0.090
[106,     1] loss: 0.067
[107,     1] loss: 0.048
[108,     1] loss: 0.076
[109,     1] loss: 0.111
[110,     1] loss: 0.101
[111,     1] loss: 0.062
[112,     1] loss: 0.114
[113,     1] loss: 0.169
[114,     1] loss: 0.190
[115,     1] loss: 0.113
[116,     1] loss: 0.185
[117,     1] loss: 0.078
[118,     1] loss: 0.114
[119,     1] loss: 0.132
[120,     1] loss: 0.084
[121,     1] loss: 0.100
[122,     1] loss: 0.086
[123,     1] loss: 0.146
[124,     1] loss: 0.060
[125,     1] loss: 0.078
[126,     1] loss: 0.047
[127,     1] loss: 0.076
[128,     1] loss: 0.074
[129,     1] loss: 0.063
[130,     1] loss: 0.060
[131,     1] loss: 0.063
[132,     1] loss: 0.061
[133,     1] loss: 0.049
[134,     1] loss: 0.067
[135,     1] loss: 0.050
[136,     1] loss: 0.060
[137,     1] loss: 0.048
[138,     1] loss: 0.100
[139,     1] loss: 0.061
[140,     1] loss: 0.054
[141,     1] loss: 0.077
[142,     1] loss: 0.075
[143,     1] loss: 0.053
[144,     1] loss: 0.060
[145,     1] loss: 0.057
[146,     1] loss: 0.083
[147,     1] loss: 0.044
[148,     1] loss: 0.094
[149,     1] loss: 0.096
[150,     1] loss: 0.054
[151,     1] loss: 0.109
[152,     1] loss: 0.071
[153,     1] loss: 0.267
[154,     1] loss: 0.130
[155,     1] loss: 0.103
[156,     1] loss: 0.131
[157,     1] loss: 0.232
[158,     1] loss: 0.125
[159,     1] loss: 0.119
Early stopping applied (best metric=0.13277018070220947)
Finished Training
Total time taken: 17.271618604660034
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.672
[4,     1] loss: 0.637
[5,     1] loss: 0.605
[6,     1] loss: 0.574
[7,     1] loss: 0.528
[8,     1] loss: 0.517
[9,     1] loss: 0.443
[10,     1] loss: 0.453
[11,     1] loss: 0.394
[12,     1] loss: 0.373
[13,     1] loss: 0.319
[14,     1] loss: 0.321
[15,     1] loss: 0.270
[16,     1] loss: 0.305
[17,     1] loss: 0.287
[18,     1] loss: 0.288
[19,     1] loss: 0.231
[20,     1] loss: 0.313
[21,     1] loss: 0.328
[22,     1] loss: 0.230
[23,     1] loss: 0.304
[24,     1] loss: 0.266
[25,     1] loss: 0.192
[26,     1] loss: 0.211
[27,     1] loss: 0.248
[28,     1] loss: 0.212
[29,     1] loss: 0.165
[30,     1] loss: 0.288
[31,     1] loss: 0.213
[32,     1] loss: 0.209
[33,     1] loss: 0.179
[34,     1] loss: 0.220
[35,     1] loss: 0.166
[36,     1] loss: 0.178
[37,     1] loss: 0.219
[38,     1] loss: 0.151
[39,     1] loss: 0.126
[40,     1] loss: 0.153
[41,     1] loss: 0.103
[42,     1] loss: 0.136
[43,     1] loss: 0.111
[44,     1] loss: 0.087
[45,     1] loss: 0.099
[46,     1] loss: 0.105
[47,     1] loss: 0.129
[48,     1] loss: 0.084
[49,     1] loss: 0.132
[50,     1] loss: 0.144
[51,     1] loss: 0.147
[52,     1] loss: 0.209
[53,     1] loss: 0.125
[54,     1] loss: 0.106
[55,     1] loss: 0.089
[56,     1] loss: 0.137
[57,     1] loss: 0.096
[58,     1] loss: 0.109
[59,     1] loss: 0.076
[60,     1] loss: 0.074
[61,     1] loss: 0.086
[62,     1] loss: 0.096
Early stopping applied (best metric=0.3916875720024109)
Finished Training
Total time taken: 6.766998529434204
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.692
[3,     1] loss: 0.694
[4,     1] loss: 0.681
[5,     1] loss: 0.658
[6,     1] loss: 0.651
[7,     1] loss: 0.636
[8,     1] loss: 0.598
[9,     1] loss: 0.597
[10,     1] loss: 0.584
[11,     1] loss: 0.545
[12,     1] loss: 0.542
[13,     1] loss: 0.509
[14,     1] loss: 0.503
[15,     1] loss: 0.494
[16,     1] loss: 0.422
[17,     1] loss: 0.431
[18,     1] loss: 0.361
[19,     1] loss: 0.347
[20,     1] loss: 0.341
[21,     1] loss: 0.364
[22,     1] loss: 0.304
[23,     1] loss: 0.270
[24,     1] loss: 0.311
[25,     1] loss: 0.309
[26,     1] loss: 0.251
[27,     1] loss: 0.297
[28,     1] loss: 0.220
[29,     1] loss: 0.199
[30,     1] loss: 0.209
[31,     1] loss: 0.233
[32,     1] loss: 0.243
[33,     1] loss: 0.267
[34,     1] loss: 0.208
[35,     1] loss: 0.208
[36,     1] loss: 0.191
[37,     1] loss: 0.164
[38,     1] loss: 0.193
[39,     1] loss: 0.204
[40,     1] loss: 0.123
[41,     1] loss: 0.170
[42,     1] loss: 0.135
[43,     1] loss: 0.116
[44,     1] loss: 0.172
[45,     1] loss: 0.121
[46,     1] loss: 0.119
[47,     1] loss: 0.121
[48,     1] loss: 0.103
[49,     1] loss: 0.092
[50,     1] loss: 0.101
[51,     1] loss: 0.145
[52,     1] loss: 0.089
[53,     1] loss: 0.060
[54,     1] loss: 0.112
[55,     1] loss: 0.139
[56,     1] loss: 0.068
[57,     1] loss: 0.087
[58,     1] loss: 0.070
[59,     1] loss: 0.115
[60,     1] loss: 0.101
[61,     1] loss: 0.109
[62,     1] loss: 0.102
[63,     1] loss: 0.105
[64,     1] loss: 0.109
[65,     1] loss: 0.120
[66,     1] loss: 0.097
[67,     1] loss: 0.195
[68,     1] loss: 0.120
[69,     1] loss: 0.140
[70,     1] loss: 0.116
[71,     1] loss: 0.109
[72,     1] loss: 0.149
[73,     1] loss: 0.104
[74,     1] loss: 0.090
[75,     1] loss: 0.125
[76,     1] loss: 0.140
[77,     1] loss: 0.173
[78,     1] loss: 0.095
[79,     1] loss: 0.097
[80,     1] loss: 0.072
[81,     1] loss: 0.082
[82,     1] loss: 0.085
[83,     1] loss: 0.070
[84,     1] loss: 0.075
[85,     1] loss: 0.062
[86,     1] loss: 0.063
[87,     1] loss: 0.037
[88,     1] loss: 0.082
[89,     1] loss: 0.049
[90,     1] loss: 0.060
[91,     1] loss: 0.073
[92,     1] loss: 0.064
[93,     1] loss: 0.029
[94,     1] loss: 0.046
[95,     1] loss: 0.089
[96,     1] loss: 0.105
[97,     1] loss: 0.069
[98,     1] loss: 0.085
[99,     1] loss: 0.071
[100,     1] loss: 0.088
[101,     1] loss: 0.046
[102,     1] loss: 0.050
[103,     1] loss: 0.104
[104,     1] loss: 0.068
[105,     1] loss: 0.124
[106,     1] loss: 0.140
[107,     1] loss: 0.069
[108,     1] loss: 0.083
[109,     1] loss: 0.063
[110,     1] loss: 0.074
[111,     1] loss: 0.064
[112,     1] loss: 0.063
[113,     1] loss: 0.057
Early stopping applied (best metric=0.1629185676574707)
Finished Training
Total time taken: 12.242599725723267
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.675
[4,     1] loss: 0.666
[5,     1] loss: 0.629
[6,     1] loss: 0.628
[7,     1] loss: 0.578
[8,     1] loss: 0.532
[9,     1] loss: 0.533
[10,     1] loss: 0.494
[11,     1] loss: 0.456
[12,     1] loss: 0.420
[13,     1] loss: 0.375
[14,     1] loss: 0.405
[15,     1] loss: 0.374
[16,     1] loss: 0.390
[17,     1] loss: 0.300
[18,     1] loss: 0.403
[19,     1] loss: 0.308
[20,     1] loss: 0.334
[21,     1] loss: 0.346
[22,     1] loss: 0.302
[23,     1] loss: 0.283
[24,     1] loss: 0.249
[25,     1] loss: 0.235
[26,     1] loss: 0.264
[27,     1] loss: 0.243
[28,     1] loss: 0.234
[29,     1] loss: 0.293
[30,     1] loss: 0.267
[31,     1] loss: 0.227
[32,     1] loss: 0.183
[33,     1] loss: 0.271
[34,     1] loss: 0.249
[35,     1] loss: 0.255
[36,     1] loss: 0.229
[37,     1] loss: 0.250
[38,     1] loss: 0.228
[39,     1] loss: 0.231
[40,     1] loss: 0.208
[41,     1] loss: 0.144
[42,     1] loss: 0.183
[43,     1] loss: 0.202
[44,     1] loss: 0.193
[45,     1] loss: 0.151
[46,     1] loss: 0.137
[47,     1] loss: 0.185
[48,     1] loss: 0.128
[49,     1] loss: 0.156
[50,     1] loss: 0.155
[51,     1] loss: 0.126
[52,     1] loss: 0.141
[53,     1] loss: 0.127
[54,     1] loss: 0.135
[55,     1] loss: 0.183
[56,     1] loss: 0.191
[57,     1] loss: 0.160
[58,     1] loss: 0.134
[59,     1] loss: 0.153
[60,     1] loss: 0.196
[61,     1] loss: 0.136
[62,     1] loss: 0.125
[63,     1] loss: 0.125
Early stopping applied (best metric=0.3946591913700104)
Finished Training
Total time taken: 6.854597806930542
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.669
[4,     1] loss: 0.635
[5,     1] loss: 0.591
[6,     1] loss: 0.544
[7,     1] loss: 0.494
[8,     1] loss: 0.443
[9,     1] loss: 0.416
[10,     1] loss: 0.367
[11,     1] loss: 0.359
[12,     1] loss: 0.295
[13,     1] loss: 0.356
[14,     1] loss: 0.290
[15,     1] loss: 0.283
[16,     1] loss: 0.253
[17,     1] loss: 0.261
[18,     1] loss: 0.285
[19,     1] loss: 0.315
[20,     1] loss: 0.212
[21,     1] loss: 0.180
[22,     1] loss: 0.217
[23,     1] loss: 0.215
[24,     1] loss: 0.155
[25,     1] loss: 0.159
[26,     1] loss: 0.154
[27,     1] loss: 0.144
[28,     1] loss: 0.167
[29,     1] loss: 0.179
[30,     1] loss: 0.107
[31,     1] loss: 0.099
[32,     1] loss: 0.098
[33,     1] loss: 0.080
[34,     1] loss: 0.085
[35,     1] loss: 0.108
[36,     1] loss: 0.197
[37,     1] loss: 0.139
[38,     1] loss: 0.114
[39,     1] loss: 0.083
[40,     1] loss: 0.130
[41,     1] loss: 0.141
[42,     1] loss: 0.184
[43,     1] loss: 0.141
[44,     1] loss: 0.105
[45,     1] loss: 0.133
[46,     1] loss: 0.117
[47,     1] loss: 0.101
[48,     1] loss: 0.132
[49,     1] loss: 0.108
[50,     1] loss: 0.109
[51,     1] loss: 0.091
[52,     1] loss: 0.110
[53,     1] loss: 0.119
[54,     1] loss: 0.091
[55,     1] loss: 0.102
[56,     1] loss: 0.112
[57,     1] loss: 0.054
Early stopping applied (best metric=0.4060552716255188)
Finished Training
Total time taken: 6.195997953414917
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.686
[3,     1] loss: 0.666
[4,     1] loss: 0.640
[5,     1] loss: 0.596
[6,     1] loss: 0.567
[7,     1] loss: 0.515
[8,     1] loss: 0.518
[9,     1] loss: 0.494
[10,     1] loss: 0.458
[11,     1] loss: 0.437
[12,     1] loss: 0.383
[13,     1] loss: 0.367
[14,     1] loss: 0.370
[15,     1] loss: 0.324
[16,     1] loss: 0.317
[17,     1] loss: 0.322
[18,     1] loss: 0.242
[19,     1] loss: 0.247
[20,     1] loss: 0.257
[21,     1] loss: 0.344
[22,     1] loss: 0.291
[23,     1] loss: 0.352
[24,     1] loss: 0.342
[25,     1] loss: 0.334
[26,     1] loss: 0.288
[27,     1] loss: 0.282
[28,     1] loss: 0.251
[29,     1] loss: 0.239
[30,     1] loss: 0.260
[31,     1] loss: 0.226
[32,     1] loss: 0.261
[33,     1] loss: 0.174
[34,     1] loss: 0.202
[35,     1] loss: 0.230
[36,     1] loss: 0.136
[37,     1] loss: 0.178
[38,     1] loss: 0.168
[39,     1] loss: 0.107
[40,     1] loss: 0.120
[41,     1] loss: 0.142
[42,     1] loss: 0.137
[43,     1] loss: 0.140
[44,     1] loss: 0.082
[45,     1] loss: 0.115
[46,     1] loss: 0.106
[47,     1] loss: 0.123
[48,     1] loss: 0.090
[49,     1] loss: 0.075
[50,     1] loss: 0.114
[51,     1] loss: 0.085
[52,     1] loss: 0.066
[53,     1] loss: 0.064
[54,     1] loss: 0.103
[55,     1] loss: 0.071
[56,     1] loss: 0.072
[57,     1] loss: 0.184
[58,     1] loss: 0.182
[59,     1] loss: 0.138
[60,     1] loss: 0.226
[61,     1] loss: 0.133
[62,     1] loss: 0.123
Early stopping applied (best metric=0.3796670436859131)
Finished Training
Total time taken: 6.835000038146973
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.686
[3,     1] loss: 0.663
[4,     1] loss: 0.621
[5,     1] loss: 0.570
[6,     1] loss: 0.538
[7,     1] loss: 0.557
[8,     1] loss: 0.516
[9,     1] loss: 0.450
[10,     1] loss: 0.397
[11,     1] loss: 0.447
[12,     1] loss: 0.386
[13,     1] loss: 0.295
[14,     1] loss: 0.319
[15,     1] loss: 0.298
[16,     1] loss: 0.337
[17,     1] loss: 0.269
[18,     1] loss: 0.230
[19,     1] loss: 0.221
[20,     1] loss: 0.279
[21,     1] loss: 0.244
[22,     1] loss: 0.196
[23,     1] loss: 0.218
[24,     1] loss: 0.210
[25,     1] loss: 0.174
[26,     1] loss: 0.136
[27,     1] loss: 0.180
[28,     1] loss: 0.161
[29,     1] loss: 0.149
[30,     1] loss: 0.139
[31,     1] loss: 0.176
[32,     1] loss: 0.174
[33,     1] loss: 0.161
[34,     1] loss: 0.199
[35,     1] loss: 0.104
[36,     1] loss: 0.145
[37,     1] loss: 0.127
[38,     1] loss: 0.142
[39,     1] loss: 0.133
[40,     1] loss: 0.135
[41,     1] loss: 0.123
[42,     1] loss: 0.092
[43,     1] loss: 0.084
[44,     1] loss: 0.076
[45,     1] loss: 0.104
[46,     1] loss: 0.093
[47,     1] loss: 0.117
[48,     1] loss: 0.177
[49,     1] loss: 0.290
[50,     1] loss: 0.093
[51,     1] loss: 0.274
[52,     1] loss: 0.100
[53,     1] loss: 0.138
[54,     1] loss: 0.140
[55,     1] loss: 0.148
[56,     1] loss: 0.107
[57,     1] loss: 0.150
[58,     1] loss: 0.145
[59,     1] loss: 0.158
[60,     1] loss: 0.098
[61,     1] loss: 0.128
[62,     1] loss: 0.104
[63,     1] loss: 0.093
[64,     1] loss: 0.096
[65,     1] loss: 0.095
[66,     1] loss: 0.080
[67,     1] loss: 0.046
[68,     1] loss: 0.073
[69,     1] loss: 0.078
[70,     1] loss: 0.067
[71,     1] loss: 0.084
[72,     1] loss: 0.063
[73,     1] loss: 0.108
[74,     1] loss: 0.071
[75,     1] loss: 0.046
[76,     1] loss: 0.055
[77,     1] loss: 0.046
[78,     1] loss: 0.059
[79,     1] loss: 0.113
[80,     1] loss: 0.090
[81,     1] loss: 0.095
[82,     1] loss: 0.055
[83,     1] loss: 0.054
[84,     1] loss: 0.072
[85,     1] loss: 0.100
[86,     1] loss: 0.072
[87,     1] loss: 0.184
[88,     1] loss: 0.076
[89,     1] loss: 0.193
[90,     1] loss: 0.138
[91,     1] loss: 0.188
[92,     1] loss: 0.158
[93,     1] loss: 0.147
[94,     1] loss: 0.136
[95,     1] loss: 0.139
[96,     1] loss: 0.137
[97,     1] loss: 0.136
[98,     1] loss: 0.120
[99,     1] loss: 0.087
[100,     1] loss: 0.084
[101,     1] loss: 0.102
[102,     1] loss: 0.093
[103,     1] loss: 0.118
[104,     1] loss: 0.142
[105,     1] loss: 0.109
[106,     1] loss: 0.066
[107,     1] loss: 0.072
[108,     1] loss: 0.065
[109,     1] loss: 0.048
[110,     1] loss: 0.074
[111,     1] loss: 0.049
Early stopping applied (best metric=0.26391035318374634)
Finished Training
Total time taken: 12.036001205444336
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.690
[3,     1] loss: 0.679
[4,     1] loss: 0.631
[5,     1] loss: 0.603
[6,     1] loss: 0.573
[7,     1] loss: 0.549
[8,     1] loss: 0.483
[9,     1] loss: 0.518
[10,     1] loss: 0.481
[11,     1] loss: 0.478
[12,     1] loss: 0.413
[13,     1] loss: 0.377
[14,     1] loss: 0.451
[15,     1] loss: 0.389
[16,     1] loss: 0.399
[17,     1] loss: 0.360
[18,     1] loss: 0.416
[19,     1] loss: 0.363
[20,     1] loss: 0.356
[21,     1] loss: 0.334
[22,     1] loss: 0.318
[23,     1] loss: 0.295
[24,     1] loss: 0.332
[25,     1] loss: 0.344
[26,     1] loss: 0.307
[27,     1] loss: 0.277
[28,     1] loss: 0.269
[29,     1] loss: 0.213
[30,     1] loss: 0.171
[31,     1] loss: 0.197
[32,     1] loss: 0.201
[33,     1] loss: 0.203
[34,     1] loss: 0.152
[35,     1] loss: 0.187
[36,     1] loss: 0.138
[37,     1] loss: 0.157
[38,     1] loss: 0.131
[39,     1] loss: 0.142
[40,     1] loss: 0.147
[41,     1] loss: 0.161
[42,     1] loss: 0.091
[43,     1] loss: 0.133
[44,     1] loss: 0.155
[45,     1] loss: 0.105
[46,     1] loss: 0.117
[47,     1] loss: 0.142
[48,     1] loss: 0.159
[49,     1] loss: 0.142
[50,     1] loss: 0.077
[51,     1] loss: 0.157
[52,     1] loss: 0.184
[53,     1] loss: 0.130
[54,     1] loss: 0.103
[55,     1] loss: 0.124
[56,     1] loss: 0.066
[57,     1] loss: 0.083
[58,     1] loss: 0.093
[59,     1] loss: 0.094
[60,     1] loss: 0.063
[61,     1] loss: 0.069
[62,     1] loss: 0.064
[63,     1] loss: 0.104
[64,     1] loss: 0.071
[65,     1] loss: 0.062
[66,     1] loss: 0.052
[67,     1] loss: 0.063
[68,     1] loss: 0.055
[69,     1] loss: 0.058
[70,     1] loss: 0.054
[71,     1] loss: 0.076
Early stopping applied (best metric=0.33256855607032776)
Finished Training
Total time taken: 7.749647855758667
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.696
[3,     1] loss: 0.680
[4,     1] loss: 0.653
[5,     1] loss: 0.631
[6,     1] loss: 0.585
[7,     1] loss: 0.553
[8,     1] loss: 0.518
[9,     1] loss: 0.508
[10,     1] loss: 0.485
[11,     1] loss: 0.399
[12,     1] loss: 0.408
[13,     1] loss: 0.439
[14,     1] loss: 0.355
[15,     1] loss: 0.337
[16,     1] loss: 0.367
[17,     1] loss: 0.317
[18,     1] loss: 0.349
[19,     1] loss: 0.287
[20,     1] loss: 0.285
[21,     1] loss: 0.234
[22,     1] loss: 0.272
[23,     1] loss: 0.286
[24,     1] loss: 0.361
[25,     1] loss: 0.306
[26,     1] loss: 0.210
[27,     1] loss: 0.231
[28,     1] loss: 0.211
[29,     1] loss: 0.236
[30,     1] loss: 0.284
[31,     1] loss: 0.203
[32,     1] loss: 0.256
[33,     1] loss: 0.201
[34,     1] loss: 0.243
[35,     1] loss: 0.148
[36,     1] loss: 0.198
[37,     1] loss: 0.128
[38,     1] loss: 0.133
[39,     1] loss: 0.160
[40,     1] loss: 0.132
[41,     1] loss: 0.132
[42,     1] loss: 0.166
[43,     1] loss: 0.122
[44,     1] loss: 0.101
[45,     1] loss: 0.123
[46,     1] loss: 0.089
[47,     1] loss: 0.178
[48,     1] loss: 0.114
[49,     1] loss: 0.104
[50,     1] loss: 0.104
[51,     1] loss: 0.074
[52,     1] loss: 0.143
[53,     1] loss: 0.090
[54,     1] loss: 0.216
[55,     1] loss: 0.099
[56,     1] loss: 0.112
[57,     1] loss: 0.108
[58,     1] loss: 0.081
[59,     1] loss: 0.079
[60,     1] loss: 0.105
[61,     1] loss: 0.147
[62,     1] loss: 0.093
[63,     1] loss: 0.100
[64,     1] loss: 0.116
Early stopping applied (best metric=0.3376893997192383)
Finished Training
Total time taken: 6.984603404998779
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.680
[4,     1] loss: 0.648
[5,     1] loss: 0.612
[6,     1] loss: 0.608
[7,     1] loss: 0.556
[8,     1] loss: 0.540
[9,     1] loss: 0.516
[10,     1] loss: 0.527
[11,     1] loss: 0.463
[12,     1] loss: 0.428
[13,     1] loss: 0.415
[14,     1] loss: 0.437
[15,     1] loss: 0.355
[16,     1] loss: 0.359
[17,     1] loss: 0.316
[18,     1] loss: 0.387
[19,     1] loss: 0.415
[20,     1] loss: 0.335
[21,     1] loss: 0.293
[22,     1] loss: 0.334
[23,     1] loss: 0.301
[24,     1] loss: 0.319
[25,     1] loss: 0.335
[26,     1] loss: 0.277
[27,     1] loss: 0.271
[28,     1] loss: 0.274
[29,     1] loss: 0.268
[30,     1] loss: 0.224
[31,     1] loss: 0.242
[32,     1] loss: 0.228
[33,     1] loss: 0.238
[34,     1] loss: 0.173
[35,     1] loss: 0.154
[36,     1] loss: 0.148
[37,     1] loss: 0.215
[38,     1] loss: 0.135
[39,     1] loss: 0.189
[40,     1] loss: 0.155
[41,     1] loss: 0.199
[42,     1] loss: 0.186
[43,     1] loss: 0.145
[44,     1] loss: 0.143
[45,     1] loss: 0.232
[46,     1] loss: 0.121
[47,     1] loss: 0.200
[48,     1] loss: 0.186
[49,     1] loss: 0.183
[50,     1] loss: 0.190
[51,     1] loss: 0.134
[52,     1] loss: 0.203
[53,     1] loss: 0.137
[54,     1] loss: 0.166
[55,     1] loss: 0.120
[56,     1] loss: 0.131
[57,     1] loss: 0.172
[58,     1] loss: 0.158
[59,     1] loss: 0.122
[60,     1] loss: 0.151
[61,     1] loss: 0.124
[62,     1] loss: 0.083
[63,     1] loss: 0.087
[64,     1] loss: 0.129
[65,     1] loss: 0.108
[66,     1] loss: 0.092
[67,     1] loss: 0.109
[68,     1] loss: 0.096
[69,     1] loss: 0.147
[70,     1] loss: 0.077
[71,     1] loss: 0.082
[72,     1] loss: 0.095
[73,     1] loss: 0.077
[74,     1] loss: 0.058
[75,     1] loss: 0.099
[76,     1] loss: 0.095
[77,     1] loss: 0.077
[78,     1] loss: 0.081
[79,     1] loss: 0.106
[80,     1] loss: 0.126
[81,     1] loss: 0.138
Early stopping applied (best metric=0.2709062099456787)
Finished Training
Total time taken: 8.845594644546509
{'Hydroxylation-K Validation Accuracy': 0.84677304964539, 'Hydroxylation-K Validation Sensitivity': 0.844, 'Hydroxylation-K Validation Specificity': 0.8473684210526315, 'Hydroxylation-K Validation Precision': 0.595921059332824, 'Hydroxylation-K AUC ROC': 0.8572631578947368, 'Hydroxylation-K AUC PR': 0.6365951527678481, 'Hydroxylation-K MCC': 0.6168857617323042, 'Hydroxylation-K F1': 0.6942881234707322, 'Validation Loss (Hydroxylation-K)': 0.3279010570049286, 'Validation Loss (total)': 0.3279010570049286, 'TimeToTrain': 8.26408420562744}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028365326913329476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3572862261,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.0975329048064073}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.685
[3,     1] loss: 0.667
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017710227629702917,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 931186718,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.834899748491424}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009988915043937062,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 116208987,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.1040502846390314}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.708
[3,     1] loss: 0.683
[4,     1] loss: 0.662
[5,     1] loss: 0.624
[6,     1] loss: 0.575
[7,     1] loss: 0.514
[8,     1] loss: 0.495
[9,     1] loss: 0.436
[10,     1] loss: 0.404
[11,     1] loss: 0.333
[12,     1] loss: 0.413
[13,     1] loss: 0.394
[14,     1] loss: 0.280
[15,     1] loss: 0.414
[16,     1] loss: 0.390
[17,     1] loss: 0.361
[18,     1] loss: 0.328
[19,     1] loss: 0.347
[20,     1] loss: 0.242
[21,     1] loss: 0.223
[22,     1] loss: 0.306
[23,     1] loss: 0.234
[24,     1] loss: 0.246
[25,     1] loss: 0.242
[26,     1] loss: 0.190
[27,     1] loss: 0.204
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004483397883581645,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2925482415,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.8528957881899623}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.673
[4,     1] loss: 0.661
[5,     1] loss: 0.623
[6,     1] loss: 0.584
[7,     1] loss: 0.533
[8,     1] loss: 0.544
[9,     1] loss: 0.466
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003956725844311691,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3285416818,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.017101831539886}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005351937258657954,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1454271843,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.993805040589837}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.690
[3,     1] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028958504403248375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 585875514,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.438403392591901}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.689
[2,     1] loss: 0.693
[3,     1] loss: 0.683
[4,     1] loss: 0.658
[5,     1] loss: 0.630
[6,     1] loss: 0.597
[7,     1] loss: 0.576
[8,     1] loss: 0.545
[9,     1] loss: 0.496
[10,     1] loss: 0.466
[11,     1] loss: 0.480
[12,     1] loss: 0.451
[13,     1] loss: 0.396
[14,     1] loss: 0.383
[15,     1] loss: 0.346
[16,     1] loss: 0.381
[17,     1] loss: 0.357
[18,     1] loss: 0.319
[19,     1] loss: 0.290
[20,     1] loss: 0.272
[21,     1] loss: 0.278
[22,     1] loss: 0.288
[23,     1] loss: 0.274
[24,     1] loss: 0.256
[25,     1] loss: 0.244
[26,     1] loss: 0.206
[27,     1] loss: 0.133
[28,     1] loss: 0.173
[29,     1] loss: 0.181
[30,     1] loss: 0.184
[31,     1] loss: 0.143
[32,     1] loss: 0.179
[33,     1] loss: 0.221
[34,     1] loss: 0.131
[35,     1] loss: 0.161
[36,     1] loss: 0.143
[37,     1] loss: 0.136
[38,     1] loss: 0.149
[39,     1] loss: 0.119
[40,     1] loss: 0.157
[41,     1] loss: 0.084
[42,     1] loss: 0.108
[43,     1] loss: 0.227
[44,     1] loss: 0.147
[45,     1] loss: 0.167
[46,     1] loss: 0.166
[47,     1] loss: 0.182
[48,     1] loss: 0.127
[49,     1] loss: 0.165
[50,     1] loss: 0.125
[51,     1] loss: 0.097
[52,     1] loss: 0.092
[53,     1] loss: 0.121
[54,     1] loss: 0.119
[55,     1] loss: 0.173
[56,     1] loss: 0.112
[57,     1] loss: 0.082
[58,     1] loss: 0.178
[59,     1] loss: 0.093
[60,     1] loss: 0.084
Early stopping applied (best metric=0.308368980884552)
Finished Training
Total time taken: 6.568000793457031
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.674
[4,     1] loss: 0.650
[5,     1] loss: 0.629
[6,     1] loss: 0.577
[7,     1] loss: 0.543
[8,     1] loss: 0.520
[9,     1] loss: 0.481
[10,     1] loss: 0.437
[11,     1] loss: 0.447
[12,     1] loss: 0.388
[13,     1] loss: 0.371
[14,     1] loss: 0.348
[15,     1] loss: 0.315
[16,     1] loss: 0.343
[17,     1] loss: 0.294
[18,     1] loss: 0.289
[19,     1] loss: 0.279
[20,     1] loss: 0.270
[21,     1] loss: 0.277
[22,     1] loss: 0.307
[23,     1] loss: 0.215
[24,     1] loss: 0.236
[25,     1] loss: 0.264
[26,     1] loss: 0.257
[27,     1] loss: 0.280
[28,     1] loss: 0.315
[29,     1] loss: 0.235
[30,     1] loss: 0.276
[31,     1] loss: 0.246
[32,     1] loss: 0.262
[33,     1] loss: 0.256
[34,     1] loss: 0.243
[35,     1] loss: 0.206
[36,     1] loss: 0.224
[37,     1] loss: 0.210
[38,     1] loss: 0.161
[39,     1] loss: 0.187
[40,     1] loss: 0.203
[41,     1] loss: 0.177
[42,     1] loss: 0.192
[43,     1] loss: 0.187
[44,     1] loss: 0.185
[45,     1] loss: 0.136
[46,     1] loss: 0.177
[47,     1] loss: 0.137
[48,     1] loss: 0.170
[49,     1] loss: 0.161
[50,     1] loss: 0.168
[51,     1] loss: 0.176
[52,     1] loss: 0.122
[53,     1] loss: 0.239
[54,     1] loss: 0.165
[55,     1] loss: 0.162
[56,     1] loss: 0.106
[57,     1] loss: 0.124
[58,     1] loss: 0.123
Early stopping applied (best metric=0.4012972116470337)
Finished Training
Total time taken: 6.278605222702026
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.670
[4,     1] loss: 0.642
[5,     1] loss: 0.616
[6,     1] loss: 0.584
[7,     1] loss: 0.551
[8,     1] loss: 0.528
[9,     1] loss: 0.511
[10,     1] loss: 0.457
[11,     1] loss: 0.420
[12,     1] loss: 0.409
[13,     1] loss: 0.362
[14,     1] loss: 0.373
[15,     1] loss: 0.316
[16,     1] loss: 0.277
[17,     1] loss: 0.284
[18,     1] loss: 0.338
[19,     1] loss: 0.297
[20,     1] loss: 0.228
[21,     1] loss: 0.230
[22,     1] loss: 0.215
[23,     1] loss: 0.259
[24,     1] loss: 0.186
[25,     1] loss: 0.204
[26,     1] loss: 0.146
[27,     1] loss: 0.219
[28,     1] loss: 0.188
[29,     1] loss: 0.221
[30,     1] loss: 0.186
[31,     1] loss: 0.159
[32,     1] loss: 0.156
[33,     1] loss: 0.151
[34,     1] loss: 0.213
[35,     1] loss: 0.179
[36,     1] loss: 0.129
[37,     1] loss: 0.117
[38,     1] loss: 0.141
[39,     1] loss: 0.150
[40,     1] loss: 0.144
[41,     1] loss: 0.167
[42,     1] loss: 0.130
[43,     1] loss: 0.141
[44,     1] loss: 0.154
[45,     1] loss: 0.145
[46,     1] loss: 0.148
[47,     1] loss: 0.128
[48,     1] loss: 0.136
[49,     1] loss: 0.127
[50,     1] loss: 0.171
[51,     1] loss: 0.210
[52,     1] loss: 0.105
[53,     1] loss: 0.154
[54,     1] loss: 0.137
[55,     1] loss: 0.144
[56,     1] loss: 0.157
[57,     1] loss: 0.127
[58,     1] loss: 0.132
[59,     1] loss: 0.103
Early stopping applied (best metric=0.4387549161911011)
Finished Training
Total time taken: 6.463613986968994
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.708
[2,     1] loss: 0.693
[3,     1] loss: 0.686
[4,     1] loss: 0.675
[5,     1] loss: 0.665
[6,     1] loss: 0.645
[7,     1] loss: 0.624
[8,     1] loss: 0.616
[9,     1] loss: 0.588
[10,     1] loss: 0.544
[11,     1] loss: 0.527
[12,     1] loss: 0.511
[13,     1] loss: 0.492
[14,     1] loss: 0.467
[15,     1] loss: 0.417
[16,     1] loss: 0.424
[17,     1] loss: 0.401
[18,     1] loss: 0.354
[19,     1] loss: 0.345
[20,     1] loss: 0.322
[21,     1] loss: 0.396
[22,     1] loss: 0.278
[23,     1] loss: 0.338
[24,     1] loss: 0.336
[25,     1] loss: 0.244
[26,     1] loss: 0.280
[27,     1] loss: 0.292
[28,     1] loss: 0.348
[29,     1] loss: 0.204
[30,     1] loss: 0.274
[31,     1] loss: 0.223
[32,     1] loss: 0.352
[33,     1] loss: 0.250
[34,     1] loss: 0.246
[35,     1] loss: 0.224
[36,     1] loss: 0.226
[37,     1] loss: 0.223
[38,     1] loss: 0.207
[39,     1] loss: 0.147
[40,     1] loss: 0.221
[41,     1] loss: 0.185
[42,     1] loss: 0.135
[43,     1] loss: 0.173
[44,     1] loss: 0.150
[45,     1] loss: 0.130
[46,     1] loss: 0.163
[47,     1] loss: 0.141
[48,     1] loss: 0.111
[49,     1] loss: 0.127
[50,     1] loss: 0.110
[51,     1] loss: 0.099
[52,     1] loss: 0.086
[53,     1] loss: 0.106
[54,     1] loss: 0.101
[55,     1] loss: 0.102
[56,     1] loss: 0.111
[57,     1] loss: 0.060
[58,     1] loss: 0.087
[59,     1] loss: 0.083
[60,     1] loss: 0.084
[61,     1] loss: 0.113
[62,     1] loss: 0.064
[63,     1] loss: 0.124
[64,     1] loss: 0.084
[65,     1] loss: 0.106
Early stopping applied (best metric=0.3883623778820038)
Finished Training
Total time taken: 7.049999713897705
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.672
[4,     1] loss: 0.637
[5,     1] loss: 0.608
[6,     1] loss: 0.590
[7,     1] loss: 0.563
[8,     1] loss: 0.528
[9,     1] loss: 0.502
[10,     1] loss: 0.458
[11,     1] loss: 0.462
[12,     1] loss: 0.390
[13,     1] loss: 0.356
[14,     1] loss: 0.305
[15,     1] loss: 0.312
[16,     1] loss: 0.303
[17,     1] loss: 0.259
[18,     1] loss: 0.295
[19,     1] loss: 0.257
[20,     1] loss: 0.247
[21,     1] loss: 0.322
[22,     1] loss: 0.251
[23,     1] loss: 0.366
[24,     1] loss: 0.317
[25,     1] loss: 0.211
[26,     1] loss: 0.268
[27,     1] loss: 0.283
[28,     1] loss: 0.216
[29,     1] loss: 0.278
[30,     1] loss: 0.254
[31,     1] loss: 0.222
[32,     1] loss: 0.226
[33,     1] loss: 0.210
[34,     1] loss: 0.238
[35,     1] loss: 0.203
[36,     1] loss: 0.196
[37,     1] loss: 0.213
[38,     1] loss: 0.256
[39,     1] loss: 0.195
[40,     1] loss: 0.176
[41,     1] loss: 0.200
[42,     1] loss: 0.153
[43,     1] loss: 0.210
[44,     1] loss: 0.147
[45,     1] loss: 0.164
[46,     1] loss: 0.169
[47,     1] loss: 0.187
[48,     1] loss: 0.167
[49,     1] loss: 0.125
[50,     1] loss: 0.159
[51,     1] loss: 0.177
[52,     1] loss: 0.113
[53,     1] loss: 0.118
[54,     1] loss: 0.170
[55,     1] loss: 0.120
[56,     1] loss: 0.227
[57,     1] loss: 0.078
[58,     1] loss: 0.295
[59,     1] loss: 0.192
[60,     1] loss: 0.112
[61,     1] loss: 0.172
[62,     1] loss: 0.162
Early stopping applied (best metric=0.4611119329929352)
Finished Training
Total time taken: 6.8149988651275635
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.689
[3,     1] loss: 0.669
[4,     1] loss: 0.633
[5,     1] loss: 0.611
[6,     1] loss: 0.581
[7,     1] loss: 0.544
[8,     1] loss: 0.516
[9,     1] loss: 0.481
[10,     1] loss: 0.471
[11,     1] loss: 0.449
[12,     1] loss: 0.460
[13,     1] loss: 0.368
[14,     1] loss: 0.321
[15,     1] loss: 0.307
[16,     1] loss: 0.295
[17,     1] loss: 0.367
[18,     1] loss: 0.335
[19,     1] loss: 0.290
[20,     1] loss: 0.293
[21,     1] loss: 0.253
[22,     1] loss: 0.277
[23,     1] loss: 0.308
[24,     1] loss: 0.267
[25,     1] loss: 0.243
[26,     1] loss: 0.221
[27,     1] loss: 0.198
[28,     1] loss: 0.162
[29,     1] loss: 0.169
[30,     1] loss: 0.147
[31,     1] loss: 0.178
[32,     1] loss: 0.226
[33,     1] loss: 0.105
[34,     1] loss: 0.280
[35,     1] loss: 0.186
[36,     1] loss: 0.145
[37,     1] loss: 0.182
[38,     1] loss: 0.136
[39,     1] loss: 0.154
[40,     1] loss: 0.150
[41,     1] loss: 0.095
[42,     1] loss: 0.140
[43,     1] loss: 0.096
[44,     1] loss: 0.099
[45,     1] loss: 0.123
[46,     1] loss: 0.125
[47,     1] loss: 0.087
[48,     1] loss: 0.131
[49,     1] loss: 0.128
[50,     1] loss: 0.066
[51,     1] loss: 0.110
[52,     1] loss: 0.093
[53,     1] loss: 0.124
[54,     1] loss: 0.104
[55,     1] loss: 0.106
[56,     1] loss: 0.109
[57,     1] loss: 0.109
[58,     1] loss: 0.088
[59,     1] loss: 0.073
[60,     1] loss: 0.113
[61,     1] loss: 0.132
[62,     1] loss: 0.116
[63,     1] loss: 0.060
[64,     1] loss: 0.109
[65,     1] loss: 0.083
[66,     1] loss: 0.126
[67,     1] loss: 0.098
[68,     1] loss: 0.082
Early stopping applied (best metric=0.2871101498603821)
Finished Training
Total time taken: 7.388000249862671
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.663
[4,     1] loss: 0.617
[5,     1] loss: 0.603
[6,     1] loss: 0.553
[7,     1] loss: 0.531
[8,     1] loss: 0.479
[9,     1] loss: 0.459
[10,     1] loss: 0.409
[11,     1] loss: 0.378
[12,     1] loss: 0.410
[13,     1] loss: 0.419
[14,     1] loss: 0.372
[15,     1] loss: 0.275
[16,     1] loss: 0.356
[17,     1] loss: 0.293
[18,     1] loss: 0.294
[19,     1] loss: 0.318
[20,     1] loss: 0.309
[21,     1] loss: 0.202
[22,     1] loss: 0.193
[23,     1] loss: 0.298
[24,     1] loss: 0.272
[25,     1] loss: 0.309
[26,     1] loss: 0.305
[27,     1] loss: 0.209
[28,     1] loss: 0.250
[29,     1] loss: 0.209
[30,     1] loss: 0.268
[31,     1] loss: 0.210
[32,     1] loss: 0.191
[33,     1] loss: 0.183
[34,     1] loss: 0.183
[35,     1] loss: 0.186
[36,     1] loss: 0.162
[37,     1] loss: 0.223
[38,     1] loss: 0.160
[39,     1] loss: 0.153
[40,     1] loss: 0.184
[41,     1] loss: 0.146
[42,     1] loss: 0.120
[43,     1] loss: 0.148
[44,     1] loss: 0.145
[45,     1] loss: 0.118
[46,     1] loss: 0.103
[47,     1] loss: 0.115
[48,     1] loss: 0.086
[49,     1] loss: 0.098
[50,     1] loss: 0.132
[51,     1] loss: 0.088
[52,     1] loss: 0.073
[53,     1] loss: 0.094
[54,     1] loss: 0.079
[55,     1] loss: 0.063
[56,     1] loss: 0.075
[57,     1] loss: 0.062
[58,     1] loss: 0.098
[59,     1] loss: 0.079
[60,     1] loss: 0.089
[61,     1] loss: 0.096
[62,     1] loss: 0.085
[63,     1] loss: 0.060
[64,     1] loss: 0.108
[65,     1] loss: 0.039
[66,     1] loss: 0.110
[67,     1] loss: 0.091
[68,     1] loss: 0.047
[69,     1] loss: 0.101
[70,     1] loss: 0.115
[71,     1] loss: 0.055
[72,     1] loss: 0.065
[73,     1] loss: 0.096
[74,     1] loss: 0.070
[75,     1] loss: 0.086
[76,     1] loss: 0.074
[77,     1] loss: 0.061
[78,     1] loss: 0.061
[79,     1] loss: 0.046
[80,     1] loss: 0.064
[81,     1] loss: 0.044
[82,     1] loss: 0.047
[83,     1] loss: 0.041
[84,     1] loss: 0.074
[85,     1] loss: 0.051
[86,     1] loss: 0.069
[87,     1] loss: 0.120
[88,     1] loss: 0.041
[89,     1] loss: 0.051
[90,     1] loss: 0.115
[91,     1] loss: 0.160
[92,     1] loss: 0.194
[93,     1] loss: 0.143
[94,     1] loss: 0.060
[95,     1] loss: 0.138
[96,     1] loss: 0.090
[97,     1] loss: 0.068
[98,     1] loss: 0.089
[99,     1] loss: 0.084
[100,     1] loss: 0.081
[101,     1] loss: 0.098
[102,     1] loss: 0.074
[103,     1] loss: 0.125
[104,     1] loss: 0.140
[105,     1] loss: 0.084
[106,     1] loss: 0.048
[107,     1] loss: 0.069
[108,     1] loss: 0.147
[109,     1] loss: 0.052
[110,     1] loss: 0.119
[111,     1] loss: 0.111
[112,     1] loss: 0.093
[113,     1] loss: 0.090
[114,     1] loss: 0.082
[115,     1] loss: 0.071
[116,     1] loss: 0.072
[117,     1] loss: 0.079
[118,     1] loss: 0.105
[119,     1] loss: 0.052
[120,     1] loss: 0.054
[121,     1] loss: 0.084
[122,     1] loss: 0.052
[123,     1] loss: 0.062
[124,     1] loss: 0.058
[125,     1] loss: 0.051
[126,     1] loss: 0.082
[127,     1] loss: 0.058
[128,     1] loss: 0.070
[129,     1] loss: 0.074
[130,     1] loss: 0.085
[131,     1] loss: 0.137
[132,     1] loss: 0.073
[133,     1] loss: 0.085
[134,     1] loss: 0.102
[135,     1] loss: 0.072
[136,     1] loss: 0.053
[137,     1] loss: 0.053
[138,     1] loss: 0.045
[139,     1] loss: 0.051
[140,     1] loss: 0.072
[141,     1] loss: 0.045
[142,     1] loss: 0.043
[143,     1] loss: 0.047
[144,     1] loss: 0.041
[145,     1] loss: 0.058
[146,     1] loss: 0.089
Early stopping applied (best metric=0.2734733521938324)
Finished Training
Total time taken: 16.000614404678345
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.688
[3,     1] loss: 0.680
[4,     1] loss: 0.661
[5,     1] loss: 0.653
[6,     1] loss: 0.635
[7,     1] loss: 0.610
[8,     1] loss: 0.610
[9,     1] loss: 0.572
[10,     1] loss: 0.584
[11,     1] loss: 0.544
[12,     1] loss: 0.510
[13,     1] loss: 0.506
[14,     1] loss: 0.442
[15,     1] loss: 0.453
[16,     1] loss: 0.428
[17,     1] loss: 0.407
[18,     1] loss: 0.340
[19,     1] loss: 0.371
[20,     1] loss: 0.293
[21,     1] loss: 0.366
[22,     1] loss: 0.351
[23,     1] loss: 0.237
[24,     1] loss: 0.248
[25,     1] loss: 0.273
[26,     1] loss: 0.249
[27,     1] loss: 0.207
[28,     1] loss: 0.293
[29,     1] loss: 0.243
[30,     1] loss: 0.185
[31,     1] loss: 0.375
[32,     1] loss: 0.229
[33,     1] loss: 0.249
[34,     1] loss: 0.222
[35,     1] loss: 0.281
[36,     1] loss: 0.234
[37,     1] loss: 0.241
[38,     1] loss: 0.231
[39,     1] loss: 0.225
[40,     1] loss: 0.247
[41,     1] loss: 0.195
[42,     1] loss: 0.227
[43,     1] loss: 0.197
[44,     1] loss: 0.210
[45,     1] loss: 0.181
[46,     1] loss: 0.216
[47,     1] loss: 0.174
[48,     1] loss: 0.133
[49,     1] loss: 0.143
[50,     1] loss: 0.169
[51,     1] loss: 0.130
[52,     1] loss: 0.141
[53,     1] loss: 0.118
[54,     1] loss: 0.168
[55,     1] loss: 0.177
[56,     1] loss: 0.312
[57,     1] loss: 0.228
[58,     1] loss: 0.200
[59,     1] loss: 0.174
[60,     1] loss: 0.159
[61,     1] loss: 0.131
[62,     1] loss: 0.147
[63,     1] loss: 0.179
[64,     1] loss: 0.156
[65,     1] loss: 0.125
[66,     1] loss: 0.170
[67,     1] loss: 0.137
[68,     1] loss: 0.157
Early stopping applied (best metric=0.3664403557777405)
Finished Training
Total time taken: 7.375608444213867
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.686
[3,     1] loss: 0.653
[4,     1] loss: 0.607
[5,     1] loss: 0.595
[6,     1] loss: 0.542
[7,     1] loss: 0.527
[8,     1] loss: 0.475
[9,     1] loss: 0.485
[10,     1] loss: 0.463
[11,     1] loss: 0.422
[12,     1] loss: 0.409
[13,     1] loss: 0.394
[14,     1] loss: 0.354
[15,     1] loss: 0.350
[16,     1] loss: 0.387
[17,     1] loss: 0.368
[18,     1] loss: 0.275
[19,     1] loss: 0.306
[20,     1] loss: 0.305
[21,     1] loss: 0.325
[22,     1] loss: 0.312
[23,     1] loss: 0.267
[24,     1] loss: 0.247
[25,     1] loss: 0.305
[26,     1] loss: 0.286
[27,     1] loss: 0.220
[28,     1] loss: 0.240
[29,     1] loss: 0.277
[30,     1] loss: 0.273
[31,     1] loss: 0.251
[32,     1] loss: 0.216
[33,     1] loss: 0.227
[34,     1] loss: 0.236
[35,     1] loss: 0.207
[36,     1] loss: 0.175
[37,     1] loss: 0.259
[38,     1] loss: 0.205
[39,     1] loss: 0.218
[40,     1] loss: 0.240
[41,     1] loss: 0.184
[42,     1] loss: 0.205
[43,     1] loss: 0.250
[44,     1] loss: 0.272
[45,     1] loss: 0.242
[46,     1] loss: 0.179
[47,     1] loss: 0.180
[48,     1] loss: 0.172
[49,     1] loss: 0.210
[50,     1] loss: 0.208
[51,     1] loss: 0.135
[52,     1] loss: 0.175
[53,     1] loss: 0.162
[54,     1] loss: 0.162
[55,     1] loss: 0.209
[56,     1] loss: 0.117
[57,     1] loss: 0.148
[58,     1] loss: 0.174
[59,     1] loss: 0.122
[60,     1] loss: 0.162
[61,     1] loss: 0.169
[62,     1] loss: 0.116
[63,     1] loss: 0.116
[64,     1] loss: 0.106
[65,     1] loss: 0.098
[66,     1] loss: 0.136
[67,     1] loss: 0.135
[68,     1] loss: 0.107
[69,     1] loss: 0.102
[70,     1] loss: 0.083
[71,     1] loss: 0.130
Early stopping applied (best metric=0.4145125448703766)
Finished Training
Total time taken: 7.786999702453613
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.676
[4,     1] loss: 0.645
[5,     1] loss: 0.617
[6,     1] loss: 0.582
[7,     1] loss: 0.570
[8,     1] loss: 0.543
[9,     1] loss: 0.513
[10,     1] loss: 0.451
[11,     1] loss: 0.446
[12,     1] loss: 0.405
[13,     1] loss: 0.367
[14,     1] loss: 0.369
[15,     1] loss: 0.337
[16,     1] loss: 0.308
[17,     1] loss: 0.293
[18,     1] loss: 0.285
[19,     1] loss: 0.254
[20,     1] loss: 0.325
[21,     1] loss: 0.205
[22,     1] loss: 0.260
[23,     1] loss: 0.240
[24,     1] loss: 0.235
[25,     1] loss: 0.187
[26,     1] loss: 0.238
[27,     1] loss: 0.174
[28,     1] loss: 0.160
[29,     1] loss: 0.159
[30,     1] loss: 0.296
[31,     1] loss: 0.201
[32,     1] loss: 0.199
[33,     1] loss: 0.171
[34,     1] loss: 0.136
[35,     1] loss: 0.187
[36,     1] loss: 0.173
[37,     1] loss: 0.128
[38,     1] loss: 0.150
[39,     1] loss: 0.166
[40,     1] loss: 0.150
[41,     1] loss: 0.278
[42,     1] loss: 0.182
[43,     1] loss: 0.241
[44,     1] loss: 0.172
[45,     1] loss: 0.196
[46,     1] loss: 0.172
[47,     1] loss: 0.185
[48,     1] loss: 0.182
[49,     1] loss: 0.186
[50,     1] loss: 0.159
[51,     1] loss: 0.142
[52,     1] loss: 0.171
[53,     1] loss: 0.162
[54,     1] loss: 0.124
[55,     1] loss: 0.169
[56,     1] loss: 0.155
[57,     1] loss: 0.121
[58,     1] loss: 0.140
[59,     1] loss: 0.122
[60,     1] loss: 0.087
[61,     1] loss: 0.142
[62,     1] loss: 0.114
[63,     1] loss: 0.087
Early stopping applied (best metric=0.3162335157394409)
Finished Training
Total time taken: 6.796608924865723
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.666
[4,     1] loss: 0.640
[5,     1] loss: 0.602
[6,     1] loss: 0.564
[7,     1] loss: 0.532
[8,     1] loss: 0.520
[9,     1] loss: 0.487
[10,     1] loss: 0.441
[11,     1] loss: 0.422
[12,     1] loss: 0.423
[13,     1] loss: 0.425
[14,     1] loss: 0.376
[15,     1] loss: 0.308
[16,     1] loss: 0.280
[17,     1] loss: 0.289
[18,     1] loss: 0.320
[19,     1] loss: 0.264
[20,     1] loss: 0.296
[21,     1] loss: 0.260
[22,     1] loss: 0.213
[23,     1] loss: 0.238
[24,     1] loss: 0.282
[25,     1] loss: 0.266
[26,     1] loss: 0.215
[27,     1] loss: 0.225
[28,     1] loss: 0.177
[29,     1] loss: 0.182
[30,     1] loss: 0.162
[31,     1] loss: 0.199
[32,     1] loss: 0.300
[33,     1] loss: 0.178
[34,     1] loss: 0.163
[35,     1] loss: 0.230
[36,     1] loss: 0.134
[37,     1] loss: 0.183
[38,     1] loss: 0.199
[39,     1] loss: 0.223
[40,     1] loss: 0.215
[41,     1] loss: 0.173
[42,     1] loss: 0.167
[43,     1] loss: 0.198
[44,     1] loss: 0.142
[45,     1] loss: 0.163
[46,     1] loss: 0.176
[47,     1] loss: 0.159
[48,     1] loss: 0.173
[49,     1] loss: 0.174
[50,     1] loss: 0.130
[51,     1] loss: 0.141
[52,     1] loss: 0.133
[53,     1] loss: 0.191
[54,     1] loss: 0.177
[55,     1] loss: 0.090
[56,     1] loss: 0.154
[57,     1] loss: 0.188
[58,     1] loss: 0.142
[59,     1] loss: 0.129
[60,     1] loss: 0.138
[61,     1] loss: 0.117
[62,     1] loss: 0.132
[63,     1] loss: 0.154
[64,     1] loss: 0.106
[65,     1] loss: 0.093
Early stopping applied (best metric=0.40135759115219116)
Finished Training
Total time taken: 7.139605283737183
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.675
[4,     1] loss: 0.656
[5,     1] loss: 0.625
[6,     1] loss: 0.592
[7,     1] loss: 0.568
[8,     1] loss: 0.543
[9,     1] loss: 0.495
[10,     1] loss: 0.483
[11,     1] loss: 0.454
[12,     1] loss: 0.469
[13,     1] loss: 0.430
[14,     1] loss: 0.393
[15,     1] loss: 0.361
[16,     1] loss: 0.440
[17,     1] loss: 0.285
[18,     1] loss: 0.308
[19,     1] loss: 0.288
[20,     1] loss: 0.285
[21,     1] loss: 0.301
[22,     1] loss: 0.250
[23,     1] loss: 0.235
[24,     1] loss: 0.272
[25,     1] loss: 0.318
[26,     1] loss: 0.217
[27,     1] loss: 0.258
[28,     1] loss: 0.232
[29,     1] loss: 0.261
[30,     1] loss: 0.239
[31,     1] loss: 0.210
[32,     1] loss: 0.227
[33,     1] loss: 0.192
[34,     1] loss: 0.187
[35,     1] loss: 0.189
[36,     1] loss: 0.204
[37,     1] loss: 0.211
[38,     1] loss: 0.210
[39,     1] loss: 0.147
[40,     1] loss: 0.248
[41,     1] loss: 0.160
[42,     1] loss: 0.250
[43,     1] loss: 0.168
[44,     1] loss: 0.192
[45,     1] loss: 0.174
[46,     1] loss: 0.224
[47,     1] loss: 0.239
[48,     1] loss: 0.206
[49,     1] loss: 0.159
[50,     1] loss: 0.189
[51,     1] loss: 0.163
[52,     1] loss: 0.149
[53,     1] loss: 0.102
[54,     1] loss: 0.129
[55,     1] loss: 0.127
[56,     1] loss: 0.101
[57,     1] loss: 0.089
[58,     1] loss: 0.094
[59,     1] loss: 0.109
[60,     1] loss: 0.078
[61,     1] loss: 0.088
[62,     1] loss: 0.055
[63,     1] loss: 0.121
[64,     1] loss: 0.086
[65,     1] loss: 0.130
[66,     1] loss: 0.114
[67,     1] loss: 0.110
[68,     1] loss: 0.090
[69,     1] loss: 0.120
[70,     1] loss: 0.117
[71,     1] loss: 0.150
[72,     1] loss: 0.055
[73,     1] loss: 0.143
[74,     1] loss: 0.150
[75,     1] loss: 0.095
[76,     1] loss: 0.071
Early stopping applied (best metric=0.25873512029647827)
Finished Training
Total time taken: 8.216999292373657
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.696
[3,     1] loss: 0.685
[4,     1] loss: 0.679
[5,     1] loss: 0.657
[6,     1] loss: 0.646
[7,     1] loss: 0.623
[8,     1] loss: 0.584
[9,     1] loss: 0.551
[10,     1] loss: 0.526
[11,     1] loss: 0.488
[12,     1] loss: 0.487
[13,     1] loss: 0.439
[14,     1] loss: 0.433
[15,     1] loss: 0.411
[16,     1] loss: 0.343
[17,     1] loss: 0.374
[18,     1] loss: 0.314
[19,     1] loss: 0.325
[20,     1] loss: 0.313
[21,     1] loss: 0.348
[22,     1] loss: 0.284
[23,     1] loss: 0.272
[24,     1] loss: 0.236
[25,     1] loss: 0.271
[26,     1] loss: 0.275
[27,     1] loss: 0.210
[28,     1] loss: 0.210
[29,     1] loss: 0.193
[30,     1] loss: 0.210
[31,     1] loss: 0.217
[32,     1] loss: 0.223
[33,     1] loss: 0.238
[34,     1] loss: 0.169
[35,     1] loss: 0.325
[36,     1] loss: 0.197
[37,     1] loss: 0.271
[38,     1] loss: 0.202
[39,     1] loss: 0.173
[40,     1] loss: 0.206
[41,     1] loss: 0.169
[42,     1] loss: 0.161
[43,     1] loss: 0.151
[44,     1] loss: 0.156
[45,     1] loss: 0.150
[46,     1] loss: 0.134
[47,     1] loss: 0.140
[48,     1] loss: 0.150
[49,     1] loss: 0.124
[50,     1] loss: 0.098
[51,     1] loss: 0.162
[52,     1] loss: 0.149
[53,     1] loss: 0.078
[54,     1] loss: 0.123
[55,     1] loss: 0.101
[56,     1] loss: 0.136
[57,     1] loss: 0.102
[58,     1] loss: 0.138
[59,     1] loss: 0.103
[60,     1] loss: 0.172
[61,     1] loss: 0.192
[62,     1] loss: 0.231
[63,     1] loss: 0.210
[64,     1] loss: 0.148
[65,     1] loss: 0.184
Early stopping applied (best metric=0.42847713828086853)
Finished Training
Total time taken: 7.115000247955322
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.683
[3,     1] loss: 0.669
[4,     1] loss: 0.636
[5,     1] loss: 0.606
[6,     1] loss: 0.560
[7,     1] loss: 0.536
[8,     1] loss: 0.493
[9,     1] loss: 0.507
[10,     1] loss: 0.460
[11,     1] loss: 0.414
[12,     1] loss: 0.408
[13,     1] loss: 0.346
[14,     1] loss: 0.370
[15,     1] loss: 0.307
[16,     1] loss: 0.269
[17,     1] loss: 0.327
[18,     1] loss: 0.318
[19,     1] loss: 0.254
[20,     1] loss: 0.303
[21,     1] loss: 0.319
[22,     1] loss: 0.299
[23,     1] loss: 0.228
[24,     1] loss: 0.275
[25,     1] loss: 0.388
[26,     1] loss: 0.232
[27,     1] loss: 0.359
[28,     1] loss: 0.353
[29,     1] loss: 0.345
[30,     1] loss: 0.286
[31,     1] loss: 0.260
[32,     1] loss: 0.287
[33,     1] loss: 0.271
[34,     1] loss: 0.302
[35,     1] loss: 0.279
[36,     1] loss: 0.298
[37,     1] loss: 0.265
[38,     1] loss: 0.259
[39,     1] loss: 0.278
[40,     1] loss: 0.237
[41,     1] loss: 0.223
[42,     1] loss: 0.218
[43,     1] loss: 0.224
[44,     1] loss: 0.261
[45,     1] loss: 0.209
[46,     1] loss: 0.222
[47,     1] loss: 0.250
[48,     1] loss: 0.268
[49,     1] loss: 0.195
[50,     1] loss: 0.197
[51,     1] loss: 0.160
[52,     1] loss: 0.236
[53,     1] loss: 0.245
[54,     1] loss: 0.171
[55,     1] loss: 0.228
[56,     1] loss: 0.165
[57,     1] loss: 0.170
[58,     1] loss: 0.153
[59,     1] loss: 0.136
[60,     1] loss: 0.214
[61,     1] loss: 0.153
[62,     1] loss: 0.150
[63,     1] loss: 0.149
[64,     1] loss: 0.153
[65,     1] loss: 0.181
[66,     1] loss: 0.187
[67,     1] loss: 0.118
[68,     1] loss: 0.095
[69,     1] loss: 0.134
[70,     1] loss: 0.120
[71,     1] loss: 0.092
[72,     1] loss: 0.095
[73,     1] loss: 0.091
[74,     1] loss: 0.098
[75,     1] loss: 0.085
[76,     1] loss: 0.132
[77,     1] loss: 0.071
[78,     1] loss: 0.087
[79,     1] loss: 0.097
[80,     1] loss: 0.118
[81,     1] loss: 0.050
[82,     1] loss: 0.105
[83,     1] loss: 0.093
[84,     1] loss: 0.091
[85,     1] loss: 0.108
[86,     1] loss: 0.074
[87,     1] loss: 0.117
[88,     1] loss: 0.069
[89,     1] loss: 0.092
[90,     1] loss: 0.074
[91,     1] loss: 0.097
[92,     1] loss: 0.108
Early stopping applied (best metric=0.12768347561359406)
Finished Training
Total time taken: 10.032618999481201
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.646
[5,     1] loss: 0.608
[6,     1] loss: 0.585
[7,     1] loss: 0.546
[8,     1] loss: 0.516
[9,     1] loss: 0.503
[10,     1] loss: 0.473
[11,     1] loss: 0.437
[12,     1] loss: 0.393
[13,     1] loss: 0.379
[14,     1] loss: 0.317
[15,     1] loss: 0.323
[16,     1] loss: 0.365
[17,     1] loss: 0.304
[18,     1] loss: 0.400
[19,     1] loss: 0.312
[20,     1] loss: 0.322
[21,     1] loss: 0.295
[22,     1] loss: 0.270
[23,     1] loss: 0.295
[24,     1] loss: 0.191
[25,     1] loss: 0.305
[26,     1] loss: 0.256
[27,     1] loss: 0.208
[28,     1] loss: 0.252
[29,     1] loss: 0.212
[30,     1] loss: 0.286
[31,     1] loss: 0.246
[32,     1] loss: 0.214
[33,     1] loss: 0.364
[34,     1] loss: 0.233
[35,     1] loss: 0.235
[36,     1] loss: 0.236
[37,     1] loss: 0.150
[38,     1] loss: 0.284
[39,     1] loss: 0.214
[40,     1] loss: 0.149
[41,     1] loss: 0.182
[42,     1] loss: 0.188
[43,     1] loss: 0.112
[44,     1] loss: 0.148
[45,     1] loss: 0.160
[46,     1] loss: 0.157
[47,     1] loss: 0.173
[48,     1] loss: 0.121
[49,     1] loss: 0.111
[50,     1] loss: 0.172
[51,     1] loss: 0.105
[52,     1] loss: 0.118
[53,     1] loss: 0.139
[54,     1] loss: 0.116
[55,     1] loss: 0.118
[56,     1] loss: 0.081
[57,     1] loss: 0.134
[58,     1] loss: 0.103
[59,     1] loss: 0.099
Early stopping applied (best metric=0.4509683847427368)
Finished Training
Total time taken: 6.485611200332642
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.696
[3,     1] loss: 0.685
[4,     1] loss: 0.680
[5,     1] loss: 0.667
[6,     1] loss: 0.634
[7,     1] loss: 0.620
[8,     1] loss: 0.598
[9,     1] loss: 0.569
[10,     1] loss: 0.547
[11,     1] loss: 0.526
[12,     1] loss: 0.486
[13,     1] loss: 0.464
[14,     1] loss: 0.429
[15,     1] loss: 0.432
[16,     1] loss: 0.376
[17,     1] loss: 0.340
[18,     1] loss: 0.311
[19,     1] loss: 0.330
[20,     1] loss: 0.290
[21,     1] loss: 0.267
[22,     1] loss: 0.287
[23,     1] loss: 0.246
[24,     1] loss: 0.379
[25,     1] loss: 0.284
[26,     1] loss: 0.252
[27,     1] loss: 0.359
[28,     1] loss: 0.240
[29,     1] loss: 0.273
[30,     1] loss: 0.286
[31,     1] loss: 0.279
[32,     1] loss: 0.233
[33,     1] loss: 0.278
[34,     1] loss: 0.284
[35,     1] loss: 0.245
[36,     1] loss: 0.234
[37,     1] loss: 0.223
[38,     1] loss: 0.244
[39,     1] loss: 0.310
[40,     1] loss: 0.223
[41,     1] loss: 0.191
[42,     1] loss: 0.194
[43,     1] loss: 0.200
[44,     1] loss: 0.188
[45,     1] loss: 0.167
[46,     1] loss: 0.141
[47,     1] loss: 0.225
[48,     1] loss: 0.145
[49,     1] loss: 0.213
[50,     1] loss: 0.165
[51,     1] loss: 0.181
[52,     1] loss: 0.186
[53,     1] loss: 0.229
[54,     1] loss: 0.192
[55,     1] loss: 0.155
[56,     1] loss: 0.147
[57,     1] loss: 0.193
[58,     1] loss: 0.140
[59,     1] loss: 0.133
[60,     1] loss: 0.126
[61,     1] loss: 0.132
[62,     1] loss: 0.168
[63,     1] loss: 0.155
[64,     1] loss: 0.107
[65,     1] loss: 0.149
[66,     1] loss: 0.142
[67,     1] loss: 0.110
[68,     1] loss: 0.122
[69,     1] loss: 0.135
[70,     1] loss: 0.163
[71,     1] loss: 0.108
[72,     1] loss: 0.238
[73,     1] loss: 0.144
[74,     1] loss: 0.271
[75,     1] loss: 0.123
[76,     1] loss: 0.159
[77,     1] loss: 0.203
[78,     1] loss: 0.201
[79,     1] loss: 0.174
[80,     1] loss: 0.214
[81,     1] loss: 0.183
[82,     1] loss: 0.184
[83,     1] loss: 0.177
[84,     1] loss: 0.159
[85,     1] loss: 0.174
[86,     1] loss: 0.151
[87,     1] loss: 0.161
[88,     1] loss: 0.145
[89,     1] loss: 0.124
[90,     1] loss: 0.159
[91,     1] loss: 0.116
[92,     1] loss: 0.112
[93,     1] loss: 0.102
[94,     1] loss: 0.140
[95,     1] loss: 0.115
[96,     1] loss: 0.143
[97,     1] loss: 0.253
[98,     1] loss: 0.134
[99,     1] loss: 0.322
[100,     1] loss: 0.143
[101,     1] loss: 0.273
[102,     1] loss: 0.246
[103,     1] loss: 0.179
[104,     1] loss: 0.148
[105,     1] loss: 0.205
Early stopping applied (best metric=0.2901018559932709)
Finished Training
Total time taken: 11.510000228881836
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.687
[3,     1] loss: 0.641
[4,     1] loss: 0.612
[5,     1] loss: 0.569
[6,     1] loss: 0.521
[7,     1] loss: 0.471
[8,     1] loss: 0.412
[9,     1] loss: 0.389
[10,     1] loss: 0.359
[11,     1] loss: 0.287
[12,     1] loss: 0.297
[13,     1] loss: 0.284
[14,     1] loss: 0.227
[15,     1] loss: 0.270
[16,     1] loss: 0.220
[17,     1] loss: 0.234
[18,     1] loss: 0.246
[19,     1] loss: 0.261
[20,     1] loss: 0.191
[21,     1] loss: 0.205
[22,     1] loss: 0.213
[23,     1] loss: 0.259
[24,     1] loss: 0.229
[25,     1] loss: 0.201
[26,     1] loss: 0.196
[27,     1] loss: 0.217
[28,     1] loss: 0.186
[29,     1] loss: 0.194
[30,     1] loss: 0.229
[31,     1] loss: 0.167
[32,     1] loss: 0.200
[33,     1] loss: 0.153
[34,     1] loss: 0.157
[35,     1] loss: 0.190
[36,     1] loss: 0.124
[37,     1] loss: 0.118
[38,     1] loss: 0.150
[39,     1] loss: 0.181
[40,     1] loss: 0.149
[41,     1] loss: 0.129
[42,     1] loss: 0.273
[43,     1] loss: 0.182
[44,     1] loss: 0.105
[45,     1] loss: 0.179
[46,     1] loss: 0.162
[47,     1] loss: 0.147
[48,     1] loss: 0.147
[49,     1] loss: 0.200
[50,     1] loss: 0.086
[51,     1] loss: 0.174
[52,     1] loss: 0.151
[53,     1] loss: 0.104
[54,     1] loss: 0.181
[55,     1] loss: 0.185
Early stopping applied (best metric=0.4272967278957367)
Finished Training
Total time taken: 6.0600011348724365
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.679
[4,     1] loss: 0.651
[5,     1] loss: 0.628
[6,     1] loss: 0.596
[7,     1] loss: 0.571
[8,     1] loss: 0.513
[9,     1] loss: 0.504
[10,     1] loss: 0.460
[11,     1] loss: 0.429
[12,     1] loss: 0.471
[13,     1] loss: 0.361
[14,     1] loss: 0.426
[15,     1] loss: 0.310
[16,     1] loss: 0.308
[17,     1] loss: 0.314
[18,     1] loss: 0.371
[19,     1] loss: 0.423
[20,     1] loss: 0.290
[21,     1] loss: 0.294
[22,     1] loss: 0.281
[23,     1] loss: 0.270
[24,     1] loss: 0.277
[25,     1] loss: 0.257
[26,     1] loss: 0.238
[27,     1] loss: 0.240
[28,     1] loss: 0.219
[29,     1] loss: 0.184
[30,     1] loss: 0.190
[31,     1] loss: 0.185
[32,     1] loss: 0.198
[33,     1] loss: 0.244
[34,     1] loss: 0.195
[35,     1] loss: 0.161
[36,     1] loss: 0.131
[37,     1] loss: 0.151
[38,     1] loss: 0.181
[39,     1] loss: 0.165
[40,     1] loss: 0.149
[41,     1] loss: 0.131
[42,     1] loss: 0.234
[43,     1] loss: 0.151
[44,     1] loss: 0.221
[45,     1] loss: 0.130
[46,     1] loss: 0.139
[47,     1] loss: 0.138
[48,     1] loss: 0.137
[49,     1] loss: 0.133
[50,     1] loss: 0.198
[51,     1] loss: 0.146
[52,     1] loss: 0.154
[53,     1] loss: 0.119
[54,     1] loss: 0.107
[55,     1] loss: 0.139
[56,     1] loss: 0.106
[57,     1] loss: 0.092
[58,     1] loss: 0.111
[59,     1] loss: 0.111
[60,     1] loss: 0.087
[61,     1] loss: 0.104
[62,     1] loss: 0.107
[63,     1] loss: 0.127
[64,     1] loss: 0.143
[65,     1] loss: 0.093
[66,     1] loss: 0.156
[67,     1] loss: 0.100
[68,     1] loss: 0.197
Early stopping applied (best metric=0.3504660725593567)
Finished Training
Total time taken: 7.40500020980835
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.669
[4,     1] loss: 0.637
[5,     1] loss: 0.614
[6,     1] loss: 0.596
[7,     1] loss: 0.559
[8,     1] loss: 0.484
[9,     1] loss: 0.491
[10,     1] loss: 0.430
[11,     1] loss: 0.426
[12,     1] loss: 0.372
[13,     1] loss: 0.342
[14,     1] loss: 0.332
[15,     1] loss: 0.285
[16,     1] loss: 0.260
[17,     1] loss: 0.218
[18,     1] loss: 0.231
[19,     1] loss: 0.205
[20,     1] loss: 0.239
[21,     1] loss: 0.274
[22,     1] loss: 0.268
[23,     1] loss: 0.200
[24,     1] loss: 0.249
[25,     1] loss: 0.190
[26,     1] loss: 0.555
[27,     1] loss: 0.265
[28,     1] loss: 0.261
[29,     1] loss: 0.243
[30,     1] loss: 0.293
[31,     1] loss: 0.271
[32,     1] loss: 0.233
[33,     1] loss: 0.219
[34,     1] loss: 0.203
[35,     1] loss: 0.243
[36,     1] loss: 0.232
[37,     1] loss: 0.198
[38,     1] loss: 0.255
[39,     1] loss: 0.200
[40,     1] loss: 0.255
[41,     1] loss: 0.208
[42,     1] loss: 0.215
[43,     1] loss: 0.273
[44,     1] loss: 0.190
[45,     1] loss: 0.237
[46,     1] loss: 0.199
[47,     1] loss: 0.187
[48,     1] loss: 0.194
[49,     1] loss: 0.191
[50,     1] loss: 0.181
[51,     1] loss: 0.222
[52,     1] loss: 0.175
[53,     1] loss: 0.232
[54,     1] loss: 0.176
[55,     1] loss: 0.164
[56,     1] loss: 0.187
[57,     1] loss: 0.179
[58,     1] loss: 0.223
[59,     1] loss: 0.161
[60,     1] loss: 0.163
Early stopping applied (best metric=0.418534517288208)
Finished Training
Total time taken: 6.5929999351501465
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.656
[5,     1] loss: 0.633
[6,     1] loss: 0.616
[7,     1] loss: 0.594
[8,     1] loss: 0.571
[9,     1] loss: 0.527
[10,     1] loss: 0.506
[11,     1] loss: 0.503
[12,     1] loss: 0.483
[13,     1] loss: 0.419
[14,     1] loss: 0.433
[15,     1] loss: 0.406
[16,     1] loss: 0.381
[17,     1] loss: 0.380
[18,     1] loss: 0.342
[19,     1] loss: 0.363
[20,     1] loss: 0.364
[21,     1] loss: 0.360
[22,     1] loss: 0.286
[23,     1] loss: 0.299
[24,     1] loss: 0.329
[25,     1] loss: 0.289
[26,     1] loss: 0.268
[27,     1] loss: 0.434
[28,     1] loss: 0.286
[29,     1] loss: 0.324
[30,     1] loss: 0.365
[31,     1] loss: 0.305
[32,     1] loss: 0.297
[33,     1] loss: 0.291
[34,     1] loss: 0.276
[35,     1] loss: 0.239
[36,     1] loss: 0.323
[37,     1] loss: 0.332
[38,     1] loss: 0.276
[39,     1] loss: 0.279
[40,     1] loss: 0.236
[41,     1] loss: 0.203
[42,     1] loss: 0.208
[43,     1] loss: 0.152
[44,     1] loss: 0.159
[45,     1] loss: 0.170
[46,     1] loss: 0.207
[47,     1] loss: 0.159
[48,     1] loss: 0.138
[49,     1] loss: 0.181
[50,     1] loss: 0.158
[51,     1] loss: 0.192
[52,     1] loss: 0.107
[53,     1] loss: 0.109
[54,     1] loss: 0.139
[55,     1] loss: 0.106
[56,     1] loss: 0.152
[57,     1] loss: 0.124
[58,     1] loss: 0.169
[59,     1] loss: 0.114
[60,     1] loss: 0.112
[61,     1] loss: 0.143
[62,     1] loss: 0.106
[63,     1] loss: 0.104
[64,     1] loss: 0.123
[65,     1] loss: 0.115
[66,     1] loss: 0.088
[67,     1] loss: 0.097
[68,     1] loss: 0.129
[69,     1] loss: 0.149
[70,     1] loss: 0.167
[71,     1] loss: 0.132
[72,     1] loss: 0.134
[73,     1] loss: 0.136
[74,     1] loss: 0.097
[75,     1] loss: 0.182
[76,     1] loss: 0.098
[77,     1] loss: 0.130
[78,     1] loss: 0.135
[79,     1] loss: 0.167
[80,     1] loss: 0.195
[81,     1] loss: 0.107
[82,     1] loss: 0.165
[83,     1] loss: 0.147
[84,     1] loss: 0.097
[85,     1] loss: 0.164
[86,     1] loss: 0.113
[87,     1] loss: 0.107
[88,     1] loss: 0.095
[89,     1] loss: 0.111
[90,     1] loss: 0.095
[91,     1] loss: 0.096
[92,     1] loss: 0.138
[93,     1] loss: 0.112
[94,     1] loss: 0.125
[95,     1] loss: 0.068
[96,     1] loss: 0.098
[97,     1] loss: 0.120
[98,     1] loss: 0.078
[99,     1] loss: 0.070
[100,     1] loss: 0.063
[101,     1] loss: 0.086
[102,     1] loss: 0.094
[103,     1] loss: 0.074
[104,     1] loss: 0.085
[105,     1] loss: 0.081
[106,     1] loss: 0.064
[107,     1] loss: 0.064
[108,     1] loss: 0.088
[109,     1] loss: 0.100
[110,     1] loss: 0.062
[111,     1] loss: 0.064
[112,     1] loss: 0.197
[113,     1] loss: 0.054
[114,     1] loss: 0.158
[115,     1] loss: 0.118
[116,     1] loss: 0.084
[117,     1] loss: 0.193
[118,     1] loss: 0.073
[119,     1] loss: 0.194
[120,     1] loss: 0.116
[121,     1] loss: 0.097
[122,     1] loss: 0.069
[123,     1] loss: 0.076
[124,     1] loss: 0.099
[125,     1] loss: 0.095
[126,     1] loss: 0.090
[127,     1] loss: 0.061
[128,     1] loss: 0.110
[129,     1] loss: 0.071
[130,     1] loss: 0.069
[131,     1] loss: 0.071
[132,     1] loss: 0.084
[133,     1] loss: 0.086
[134,     1] loss: 0.070
[135,     1] loss: 0.105
[136,     1] loss: 0.097
[137,     1] loss: 0.087
[138,     1] loss: 0.062
[139,     1] loss: 0.101
[140,     1] loss: 0.094
[141,     1] loss: 0.126
[142,     1] loss: 0.069
[143,     1] loss: 0.091
[144,     1] loss: 0.079
[145,     1] loss: 0.148
[146,     1] loss: 0.099
[147,     1] loss: 0.069
[148,     1] loss: 0.086
[149,     1] loss: 0.096
[150,     1] loss: 0.070
[151,     1] loss: 0.101
[152,     1] loss: 0.069
[153,     1] loss: 0.072
[154,     1] loss: 0.104
[155,     1] loss: 0.079
[156,     1] loss: 0.072
[157,     1] loss: 0.080
[158,     1] loss: 0.046
[159,     1] loss: 0.079
[160,     1] loss: 0.041
[161,     1] loss: 0.054
[162,     1] loss: 0.082
[163,     1] loss: 0.085
[164,     1] loss: 0.060
[165,     1] loss: 0.088
[166,     1] loss: 0.065
[167,     1] loss: 0.047
[168,     1] loss: 0.050
[169,     1] loss: 0.086
[170,     1] loss: 0.090
[171,     1] loss: 0.088
[172,     1] loss: 0.069
[173,     1] loss: 0.116
[174,     1] loss: 0.056
Early stopping applied (best metric=0.324766606092453)
Finished Training
Total time taken: 18.844001531600952
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.688
[3,     1] loss: 0.673
[4,     1] loss: 0.634
[5,     1] loss: 0.606
[6,     1] loss: 0.593
[7,     1] loss: 0.558
[8,     1] loss: 0.509
[9,     1] loss: 0.479
[10,     1] loss: 0.479
[11,     1] loss: 0.413
[12,     1] loss: 0.446
[13,     1] loss: 0.377
[14,     1] loss: 0.417
[15,     1] loss: 0.329
[16,     1] loss: 0.381
[17,     1] loss: 0.371
[18,     1] loss: 0.393
[19,     1] loss: 0.359
[20,     1] loss: 0.307
[21,     1] loss: 0.354
[22,     1] loss: 0.295
[23,     1] loss: 0.303
[24,     1] loss: 0.234
[25,     1] loss: 0.305
[26,     1] loss: 0.290
[27,     1] loss: 0.241
[28,     1] loss: 0.234
[29,     1] loss: 0.221
[30,     1] loss: 0.223
[31,     1] loss: 0.220
[32,     1] loss: 0.204
[33,     1] loss: 0.219
[34,     1] loss: 0.261
[35,     1] loss: 0.292
[36,     1] loss: 0.233
[37,     1] loss: 0.211
[38,     1] loss: 0.206
[39,     1] loss: 0.275
[40,     1] loss: 0.266
[41,     1] loss: 0.156
[42,     1] loss: 0.220
[43,     1] loss: 0.168
[44,     1] loss: 0.144
[45,     1] loss: 0.168
[46,     1] loss: 0.220
[47,     1] loss: 0.184
[48,     1] loss: 0.142
[49,     1] loss: 0.194
[50,     1] loss: 0.134
[51,     1] loss: 0.140
[52,     1] loss: 0.166
[53,     1] loss: 0.145
[54,     1] loss: 0.119
[55,     1] loss: 0.138
[56,     1] loss: 0.163
[57,     1] loss: 0.251
[58,     1] loss: 0.230
[59,     1] loss: 0.242
[60,     1] loss: 0.240
[61,     1] loss: 0.227
[62,     1] loss: 0.175
[63,     1] loss: 0.217
[64,     1] loss: 0.160
[65,     1] loss: 0.189
[66,     1] loss: 0.257
[67,     1] loss: 0.178
[68,     1] loss: 0.176
[69,     1] loss: 0.134
[70,     1] loss: 0.155
[71,     1] loss: 0.168
[72,     1] loss: 0.142
[73,     1] loss: 0.112
[74,     1] loss: 0.126
[75,     1] loss: 0.164
[76,     1] loss: 0.120
[77,     1] loss: 0.135
[78,     1] loss: 0.129
[79,     1] loss: 0.089
[80,     1] loss: 0.161
[81,     1] loss: 0.133
[82,     1] loss: 0.144
[83,     1] loss: 0.083
[84,     1] loss: 0.098
[85,     1] loss: 0.097
[86,     1] loss: 0.149
[87,     1] loss: 0.114
[88,     1] loss: 0.085
[89,     1] loss: 0.103
[90,     1] loss: 0.112
[91,     1] loss: 0.106
[92,     1] loss: 0.122
[93,     1] loss: 0.067
[94,     1] loss: 0.084
[95,     1] loss: 0.132
[96,     1] loss: 0.068
[97,     1] loss: 0.182
[98,     1] loss: 0.102
Early stopping applied (best metric=0.3428875803947449)
Finished Training
Total time taken: 10.720611095428467
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.694
[3,     1] loss: 0.673
[4,     1] loss: 0.652
[5,     1] loss: 0.615
[6,     1] loss: 0.591
[7,     1] loss: 0.525
[8,     1] loss: 0.520
[9,     1] loss: 0.516
[10,     1] loss: 0.482
[11,     1] loss: 0.451
[12,     1] loss: 0.407
[13,     1] loss: 0.414
[14,     1] loss: 0.326
[15,     1] loss: 0.379
[16,     1] loss: 0.370
[17,     1] loss: 0.356
[18,     1] loss: 0.365
[19,     1] loss: 0.269
[20,     1] loss: 0.265
[21,     1] loss: 0.269
[22,     1] loss: 0.243
[23,     1] loss: 0.340
[24,     1] loss: 0.274
[25,     1] loss: 0.312
[26,     1] loss: 0.235
[27,     1] loss: 0.221
[28,     1] loss: 0.165
[29,     1] loss: 0.182
[30,     1] loss: 0.214
[31,     1] loss: 0.182
[32,     1] loss: 0.202
[33,     1] loss: 0.150
[34,     1] loss: 0.151
[35,     1] loss: 0.171
[36,     1] loss: 0.207
[37,     1] loss: 0.167
[38,     1] loss: 0.230
[39,     1] loss: 0.220
[40,     1] loss: 0.188
[41,     1] loss: 0.160
[42,     1] loss: 0.145
[43,     1] loss: 0.131
[44,     1] loss: 0.135
[45,     1] loss: 0.173
[46,     1] loss: 0.150
[47,     1] loss: 0.137
[48,     1] loss: 0.139
[49,     1] loss: 0.152
[50,     1] loss: 0.135
[51,     1] loss: 0.152
[52,     1] loss: 0.136
[53,     1] loss: 0.078
[54,     1] loss: 0.104
[55,     1] loss: 0.116
[56,     1] loss: 0.135
Early stopping applied (best metric=0.4727376401424408)
Finished Training
Total time taken: 6.140000820159912
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.678
[4,     1] loss: 0.656
[5,     1] loss: 0.634
[6,     1] loss: 0.602
[7,     1] loss: 0.579
[8,     1] loss: 0.562
[9,     1] loss: 0.534
[10,     1] loss: 0.533
[11,     1] loss: 0.478
[12,     1] loss: 0.455
[13,     1] loss: 0.424
[14,     1] loss: 0.411
[15,     1] loss: 0.384
[16,     1] loss: 0.332
[17,     1] loss: 0.338
[18,     1] loss: 0.368
[19,     1] loss: 0.301
[20,     1] loss: 0.300
[21,     1] loss: 0.254
[22,     1] loss: 0.209
[23,     1] loss: 0.251
[24,     1] loss: 0.192
[25,     1] loss: 0.238
[26,     1] loss: 0.225
[27,     1] loss: 0.253
[28,     1] loss: 0.214
[29,     1] loss: 0.161
[30,     1] loss: 0.186
[31,     1] loss: 0.141
[32,     1] loss: 0.197
[33,     1] loss: 0.194
[34,     1] loss: 0.174
[35,     1] loss: 0.147
[36,     1] loss: 0.222
[37,     1] loss: 0.180
[38,     1] loss: 0.147
[39,     1] loss: 0.226
[40,     1] loss: 0.166
[41,     1] loss: 0.140
[42,     1] loss: 0.215
[43,     1] loss: 0.160
[44,     1] loss: 0.141
[45,     1] loss: 0.143
[46,     1] loss: 0.155
[47,     1] loss: 0.162
[48,     1] loss: 0.120
[49,     1] loss: 0.155
[50,     1] loss: 0.123
[51,     1] loss: 0.153
[52,     1] loss: 0.156
[53,     1] loss: 0.095
[54,     1] loss: 0.163
[55,     1] loss: 0.130
[56,     1] loss: 0.106
[57,     1] loss: 0.081
[58,     1] loss: 0.119
[59,     1] loss: 0.119
[60,     1] loss: 0.141
[61,     1] loss: 0.120
[62,     1] loss: 0.125
[63,     1] loss: 0.107
[64,     1] loss: 0.121
[65,     1] loss: 0.159
[66,     1] loss: 0.156
Early stopping applied (best metric=0.30109667778015137)
Finished Training
Total time taken: 7.302999019622803
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.674
[4,     1] loss: 0.647
[5,     1] loss: 0.609
[6,     1] loss: 0.589
[7,     1] loss: 0.553
[8,     1] loss: 0.521
[9,     1] loss: 0.559
[10,     1] loss: 0.492
[11,     1] loss: 0.432
[12,     1] loss: 0.413
[13,     1] loss: 0.437
[14,     1] loss: 0.360
[15,     1] loss: 0.395
[16,     1] loss: 0.378
[17,     1] loss: 0.324
[18,     1] loss: 0.330
[19,     1] loss: 0.366
[20,     1] loss: 0.406
[21,     1] loss: 0.388
[22,     1] loss: 0.361
[23,     1] loss: 0.331
[24,     1] loss: 0.234
[25,     1] loss: 0.314
[26,     1] loss: 0.253
[27,     1] loss: 0.273
[28,     1] loss: 0.271
[29,     1] loss: 0.253
[30,     1] loss: 0.217
[31,     1] loss: 0.242
[32,     1] loss: 0.244
[33,     1] loss: 0.201
[34,     1] loss: 0.189
[35,     1] loss: 0.213
[36,     1] loss: 0.175
[37,     1] loss: 0.180
[38,     1] loss: 0.190
[39,     1] loss: 0.161
[40,     1] loss: 0.135
[41,     1] loss: 0.123
[42,     1] loss: 0.151
[43,     1] loss: 0.165
[44,     1] loss: 0.137
[45,     1] loss: 0.104
[46,     1] loss: 0.164
[47,     1] loss: 0.119
[48,     1] loss: 0.111
[49,     1] loss: 0.091
[50,     1] loss: 0.115
[51,     1] loss: 0.092
[52,     1] loss: 0.118
[53,     1] loss: 0.112
[54,     1] loss: 0.140
[55,     1] loss: 0.131
[56,     1] loss: 0.140
[57,     1] loss: 0.104
[58,     1] loss: 0.097
[59,     1] loss: 0.079
[60,     1] loss: 0.067
[61,     1] loss: 0.151
[62,     1] loss: 0.073
[63,     1] loss: 0.122
[64,     1] loss: 0.081
[65,     1] loss: 0.082
[66,     1] loss: 0.075
[67,     1] loss: 0.091
[68,     1] loss: 0.060
[69,     1] loss: 0.084
[70,     1] loss: 0.147
[71,     1] loss: 0.329
[72,     1] loss: 0.248
[73,     1] loss: 0.255
[74,     1] loss: 0.164
[75,     1] loss: 0.162
[76,     1] loss: 0.109
[77,     1] loss: 0.147
[78,     1] loss: 0.179
[79,     1] loss: 0.127
[80,     1] loss: 0.127
[81,     1] loss: 0.135
[82,     1] loss: 0.153
[83,     1] loss: 0.120
[84,     1] loss: 0.130
[85,     1] loss: 0.089
[86,     1] loss: 0.137
[87,     1] loss: 0.117
[88,     1] loss: 0.083
[89,     1] loss: 0.086
[90,     1] loss: 0.075
[91,     1] loss: 0.125
[92,     1] loss: 0.079
[93,     1] loss: 0.123
[94,     1] loss: 0.082
[95,     1] loss: 0.092
[96,     1] loss: 0.094
[97,     1] loss: 0.084
[98,     1] loss: 0.062
[99,     1] loss: 0.067
[100,     1] loss: 0.060
[101,     1] loss: 0.056
[102,     1] loss: 0.057
[103,     1] loss: 0.076
[104,     1] loss: 0.053
[105,     1] loss: 0.064
[106,     1] loss: 0.042
[107,     1] loss: 0.059
[108,     1] loss: 0.053
[109,     1] loss: 0.067
[110,     1] loss: 0.048
[111,     1] loss: 0.060
[112,     1] loss: 0.132
[113,     1] loss: 0.127
[114,     1] loss: 0.079
[115,     1] loss: 0.059
[116,     1] loss: 0.107
[117,     1] loss: 0.076
[118,     1] loss: 0.082
[119,     1] loss: 0.062
[120,     1] loss: 0.068
[121,     1] loss: 0.068
[122,     1] loss: 0.071
[123,     1] loss: 0.066
[124,     1] loss: 0.074
[125,     1] loss: 0.060
[126,     1] loss: 0.090
[127,     1] loss: 0.051
[128,     1] loss: 0.068
[129,     1] loss: 0.053
Early stopping applied (best metric=0.2128247767686844)
Finished Training
Total time taken: 14.136998891830444
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.652
[5,     1] loss: 0.609
[6,     1] loss: 0.583
[7,     1] loss: 0.563
[8,     1] loss: 0.567
[9,     1] loss: 0.522
[10,     1] loss: 0.457
[11,     1] loss: 0.418
[12,     1] loss: 0.378
[13,     1] loss: 0.406
[14,     1] loss: 0.363
[15,     1] loss: 0.419
[16,     1] loss: 0.333
[17,     1] loss: 0.277
[18,     1] loss: 0.382
[19,     1] loss: 0.334
[20,     1] loss: 0.358
[21,     1] loss: 0.364
[22,     1] loss: 0.364
[23,     1] loss: 0.314
[24,     1] loss: 0.330
[25,     1] loss: 0.266
[26,     1] loss: 0.279
[27,     1] loss: 0.315
[28,     1] loss: 0.297
[29,     1] loss: 0.301
[30,     1] loss: 0.270
[31,     1] loss: 0.241
[32,     1] loss: 0.261
[33,     1] loss: 0.200
[34,     1] loss: 0.259
[35,     1] loss: 0.233
[36,     1] loss: 0.235
[37,     1] loss: 0.170
[38,     1] loss: 0.236
[39,     1] loss: 0.230
[40,     1] loss: 0.214
[41,     1] loss: 0.123
[42,     1] loss: 0.208
[43,     1] loss: 0.209
[44,     1] loss: 0.159
[45,     1] loss: 0.260
[46,     1] loss: 0.124
[47,     1] loss: 0.305
[48,     1] loss: 0.219
[49,     1] loss: 0.182
[50,     1] loss: 0.171
[51,     1] loss: 0.156
[52,     1] loss: 0.148
[53,     1] loss: 0.137
[54,     1] loss: 0.117
[55,     1] loss: 0.145
[56,     1] loss: 0.141
[57,     1] loss: 0.100
[58,     1] loss: 0.100
[59,     1] loss: 0.118
[60,     1] loss: 0.078
[61,     1] loss: 0.085
[62,     1] loss: 0.092
[63,     1] loss: 0.105
[64,     1] loss: 0.098
[65,     1] loss: 0.093
[66,     1] loss: 0.157
Early stopping applied (best metric=0.3008460998535156)
Finished Training
Total time taken: 7.2789998054504395
{'Hydroxylation-K Validation Accuracy': 0.8394148936170213, 'Hydroxylation-K Validation Sensitivity': 0.8373333333333334, 'Hydroxylation-K Validation Specificity': 0.84, 'Hydroxylation-K Validation Precision': 0.5835037614604178, 'Hydroxylation-K AUC ROC': 0.8432046783625731, 'Hydroxylation-K AUC PR': 0.5762309170224476, 'Hydroxylation-K MCC': 0.6022348360868549, 'Hydroxylation-K F1': 0.6830550640985423, 'Validation Loss (Hydroxylation-K)': 0.35057782411575317, 'Validation Loss (total)': 0.35057782411575317, 'TimeToTrain': 8.540179920196532}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024265311567658455,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2825314690,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.309880278622277}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.674
[4,     1] loss: 0.640
[5,     1] loss: 0.599
[6,     1] loss: 0.590
[7,     1] loss: 0.554
[8,     1] loss: 0.550
[9,     1] loss: 0.496
[10,     1] loss: 0.456
[11,     1] loss: 0.425
[12,     1] loss: 0.409
[13,     1] loss: 0.406
[14,     1] loss: 0.418
[15,     1] loss: 0.317
[16,     1] loss: 0.374
[17,     1] loss: 0.369
[18,     1] loss: 0.377
[19,     1] loss: 0.347
[20,     1] loss: 0.341
[21,     1] loss: 0.273
[22,     1] loss: 0.276
[23,     1] loss: 0.299
[24,     1] loss: 0.272
[25,     1] loss: 0.273
[26,     1] loss: 0.259
[27,     1] loss: 0.276
[28,     1] loss: 0.190
[29,     1] loss: 0.243
[30,     1] loss: 0.250
[31,     1] loss: 0.206
[32,     1] loss: 0.168
[33,     1] loss: 0.230
[34,     1] loss: 0.251
[35,     1] loss: 0.230
[36,     1] loss: 0.162
[37,     1] loss: 0.273
[38,     1] loss: 0.205
[39,     1] loss: 0.185
[40,     1] loss: 0.167
[41,     1] loss: 0.141
[42,     1] loss: 0.177
[43,     1] loss: 0.156
[44,     1] loss: 0.111
[45,     1] loss: 0.126
[46,     1] loss: 0.134
[47,     1] loss: 0.124
[48,     1] loss: 0.092
[49,     1] loss: 0.102
[50,     1] loss: 0.092
[51,     1] loss: 0.076
[52,     1] loss: 0.129
[53,     1] loss: 0.077
[54,     1] loss: 0.070
[55,     1] loss: 0.086
[56,     1] loss: 0.102
[57,     1] loss: 0.056
[58,     1] loss: 0.069
[59,     1] loss: 0.120
[60,     1] loss: 0.126
[61,     1] loss: 0.236
[62,     1] loss: 0.183
[63,     1] loss: 0.313
[64,     1] loss: 0.062
[65,     1] loss: 0.118
[66,     1] loss: 0.118
[67,     1] loss: 0.096
[68,     1] loss: 0.100
[69,     1] loss: 0.126
[70,     1] loss: 0.135
[71,     1] loss: 0.125
[72,     1] loss: 0.106
[73,     1] loss: 0.174
[74,     1] loss: 0.079
[75,     1] loss: 0.163
[76,     1] loss: 0.111
[77,     1] loss: 0.088
[78,     1] loss: 0.073
[79,     1] loss: 0.075
[80,     1] loss: 0.086
[81,     1] loss: 0.069
[82,     1] loss: 0.058
[83,     1] loss: 0.043
[84,     1] loss: 0.104
[85,     1] loss: 0.102
[86,     1] loss: 0.079
[87,     1] loss: 0.059
[88,     1] loss: 0.102
[89,     1] loss: 0.076
[90,     1] loss: 0.143
[91,     1] loss: 0.070
Early stopping applied (best metric=0.17428064346313477)
Finished Training
Total time taken: 10.040999174118042
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.694
[3,     1] loss: 0.674
[4,     1] loss: 0.644
[5,     1] loss: 0.605
[6,     1] loss: 0.574
[7,     1] loss: 0.572
[8,     1] loss: 0.527
[9,     1] loss: 0.514
[10,     1] loss: 0.474
[11,     1] loss: 0.450
[12,     1] loss: 0.389
[13,     1] loss: 0.431
[14,     1] loss: 0.344
[15,     1] loss: 0.331
[16,     1] loss: 0.326
[17,     1] loss: 0.301
[18,     1] loss: 0.283
[19,     1] loss: 0.295
[20,     1] loss: 0.254
[21,     1] loss: 0.281
[22,     1] loss: 0.236
[23,     1] loss: 0.245
[24,     1] loss: 0.176
[25,     1] loss: 0.173
[26,     1] loss: 0.184
[27,     1] loss: 0.189
[28,     1] loss: 0.222
[29,     1] loss: 0.184
[30,     1] loss: 0.202
[31,     1] loss: 0.170
[32,     1] loss: 0.205
[33,     1] loss: 0.165
[34,     1] loss: 0.192
[35,     1] loss: 0.129
[36,     1] loss: 0.115
[37,     1] loss: 0.107
[38,     1] loss: 0.208
[39,     1] loss: 0.141
[40,     1] loss: 0.162
[41,     1] loss: 0.152
[42,     1] loss: 0.111
[43,     1] loss: 0.106
[44,     1] loss: 0.122
[45,     1] loss: 0.093
[46,     1] loss: 0.140
[47,     1] loss: 0.100
[48,     1] loss: 0.083
[49,     1] loss: 0.103
[50,     1] loss: 0.074
[51,     1] loss: 0.100
[52,     1] loss: 0.134
[53,     1] loss: 0.114
[54,     1] loss: 0.130
[55,     1] loss: 0.097
[56,     1] loss: 0.105
[57,     1] loss: 0.267
[58,     1] loss: 0.100
[59,     1] loss: 0.317
[60,     1] loss: 0.159
[61,     1] loss: 0.199
[62,     1] loss: 0.120
[63,     1] loss: 0.145
[64,     1] loss: 0.149
[65,     1] loss: 0.125
Early stopping applied (best metric=0.33292657136917114)
Finished Training
Total time taken: 7.11500096321106
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.686
[3,     1] loss: 0.657
[4,     1] loss: 0.626
[5,     1] loss: 0.589
[6,     1] loss: 0.574
[7,     1] loss: 0.572
[8,     1] loss: 0.526
[9,     1] loss: 0.481
[10,     1] loss: 0.472
[11,     1] loss: 0.448
[12,     1] loss: 0.429
[13,     1] loss: 0.390
[14,     1] loss: 0.384
[15,     1] loss: 0.371
[16,     1] loss: 0.320
[17,     1] loss: 0.336
[18,     1] loss: 0.315
[19,     1] loss: 0.280
[20,     1] loss: 0.270
[21,     1] loss: 0.275
[22,     1] loss: 0.250
[23,     1] loss: 0.340
[24,     1] loss: 0.295
[25,     1] loss: 0.325
[26,     1] loss: 0.302
[27,     1] loss: 0.308
[28,     1] loss: 0.282
[29,     1] loss: 0.324
[30,     1] loss: 0.283
[31,     1] loss: 0.286
[32,     1] loss: 0.254
[33,     1] loss: 0.333
[34,     1] loss: 0.265
[35,     1] loss: 0.264
[36,     1] loss: 0.280
[37,     1] loss: 0.288
[38,     1] loss: 0.273
[39,     1] loss: 0.235
[40,     1] loss: 0.248
[41,     1] loss: 0.252
[42,     1] loss: 0.197
[43,     1] loss: 0.261
[44,     1] loss: 0.203
[45,     1] loss: 0.185
[46,     1] loss: 0.210
[47,     1] loss: 0.164
[48,     1] loss: 0.178
[49,     1] loss: 0.283
[50,     1] loss: 0.256
[51,     1] loss: 0.169
[52,     1] loss: 0.232
[53,     1] loss: 0.122
[54,     1] loss: 0.279
[55,     1] loss: 0.234
[56,     1] loss: 0.169
[57,     1] loss: 0.163
[58,     1] loss: 0.154
[59,     1] loss: 0.124
[60,     1] loss: 0.214
[61,     1] loss: 0.146
[62,     1] loss: 0.195
[63,     1] loss: 0.110
[64,     1] loss: 0.109
[65,     1] loss: 0.122
[66,     1] loss: 0.106
[67,     1] loss: 0.085
[68,     1] loss: 0.184
[69,     1] loss: 0.111
[70,     1] loss: 0.127
[71,     1] loss: 0.110
[72,     1] loss: 0.104
[73,     1] loss: 0.108
[74,     1] loss: 0.139
[75,     1] loss: 0.083
[76,     1] loss: 0.119
[77,     1] loss: 0.137
[78,     1] loss: 0.108
[79,     1] loss: 0.098
[80,     1] loss: 0.067
[81,     1] loss: 0.068
[82,     1] loss: 0.093
[83,     1] loss: 0.076
[84,     1] loss: 0.075
[85,     1] loss: 0.065
[86,     1] loss: 0.076
[87,     1] loss: 0.071
[88,     1] loss: 0.058
[89,     1] loss: 0.064
[90,     1] loss: 0.072
[91,     1] loss: 0.122
[92,     1] loss: 0.082
[93,     1] loss: 0.184
[94,     1] loss: 0.062
[95,     1] loss: 0.208
[96,     1] loss: 0.086
[97,     1] loss: 0.108
[98,     1] loss: 0.145
[99,     1] loss: 0.105
[100,     1] loss: 0.097
[101,     1] loss: 0.141
[102,     1] loss: 0.112
[103,     1] loss: 0.118
[104,     1] loss: 0.081
[105,     1] loss: 0.088
[106,     1] loss: 0.076
[107,     1] loss: 0.077
[108,     1] loss: 0.081
[109,     1] loss: 0.036
[110,     1] loss: 0.090
Early stopping applied (best metric=0.3932816684246063)
Finished Training
Total time taken: 12.164998292922974
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.685
[4,     1] loss: 0.674
[5,     1] loss: 0.662
[6,     1] loss: 0.624
[7,     1] loss: 0.604
[8,     1] loss: 0.582
[9,     1] loss: 0.549
[10,     1] loss: 0.545
[11,     1] loss: 0.506
[12,     1] loss: 0.492
[13,     1] loss: 0.438
[14,     1] loss: 0.440
[15,     1] loss: 0.394
[16,     1] loss: 0.378
[17,     1] loss: 0.312
[18,     1] loss: 0.275
[19,     1] loss: 0.324
[20,     1] loss: 0.306
[21,     1] loss: 0.297
[22,     1] loss: 0.295
[23,     1] loss: 0.319
[24,     1] loss: 0.264
[25,     1] loss: 0.221
[26,     1] loss: 0.229
[27,     1] loss: 0.224
[28,     1] loss: 0.197
[29,     1] loss: 0.247
[30,     1] loss: 0.224
[31,     1] loss: 0.256
[32,     1] loss: 0.206
[33,     1] loss: 0.195
[34,     1] loss: 0.228
[35,     1] loss: 0.189
[36,     1] loss: 0.172
[37,     1] loss: 0.170
[38,     1] loss: 0.189
[39,     1] loss: 0.170
[40,     1] loss: 0.150
[41,     1] loss: 0.185
[42,     1] loss: 0.171
[43,     1] loss: 0.211
[44,     1] loss: 0.102
[45,     1] loss: 0.133
[46,     1] loss: 0.160
[47,     1] loss: 0.186
[48,     1] loss: 0.192
[49,     1] loss: 0.146
[50,     1] loss: 0.254
[51,     1] loss: 0.153
[52,     1] loss: 0.118
[53,     1] loss: 0.132
[54,     1] loss: 0.179
[55,     1] loss: 0.151
[56,     1] loss: 0.136
[57,     1] loss: 0.144
[58,     1] loss: 0.122
[59,     1] loss: 0.112
[60,     1] loss: 0.099
[61,     1] loss: 0.113
[62,     1] loss: 0.115
[63,     1] loss: 0.121
[64,     1] loss: 0.081
[65,     1] loss: 0.087
[66,     1] loss: 0.067
[67,     1] loss: 0.070
[68,     1] loss: 0.089
Early stopping applied (best metric=0.29522427916526794)
Finished Training
Total time taken: 7.37300181388855
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.694
[3,     1] loss: 0.679
[4,     1] loss: 0.656
[5,     1] loss: 0.629
[6,     1] loss: 0.601
[7,     1] loss: 0.574
[8,     1] loss: 0.530
[9,     1] loss: 0.511
[10,     1] loss: 0.505
[11,     1] loss: 0.444
[12,     1] loss: 0.428
[13,     1] loss: 0.383
[14,     1] loss: 0.398
[15,     1] loss: 0.371
[16,     1] loss: 0.344
[17,     1] loss: 0.340
[18,     1] loss: 0.296
[19,     1] loss: 0.313
[20,     1] loss: 0.319
[21,     1] loss: 0.292
[22,     1] loss: 0.298
[23,     1] loss: 0.307
[24,     1] loss: 0.282
[25,     1] loss: 0.218
[26,     1] loss: 0.277
[27,     1] loss: 0.253
[28,     1] loss: 0.272
[29,     1] loss: 0.323
[30,     1] loss: 0.233
[31,     1] loss: 0.234
[32,     1] loss: 0.288
[33,     1] loss: 0.226
[34,     1] loss: 0.262
[35,     1] loss: 0.248
[36,     1] loss: 0.225
[37,     1] loss: 0.186
[38,     1] loss: 0.183
[39,     1] loss: 0.192
[40,     1] loss: 0.188
[41,     1] loss: 0.152
[42,     1] loss: 0.204
[43,     1] loss: 0.232
[44,     1] loss: 0.231
[45,     1] loss: 0.162
[46,     1] loss: 0.251
[47,     1] loss: 0.217
[48,     1] loss: 0.222
[49,     1] loss: 0.233
[50,     1] loss: 0.136
[51,     1] loss: 0.180
[52,     1] loss: 0.132
[53,     1] loss: 0.147
[54,     1] loss: 0.170
[55,     1] loss: 0.139
[56,     1] loss: 0.157
[57,     1] loss: 0.149
[58,     1] loss: 0.144
[59,     1] loss: 0.116
[60,     1] loss: 0.089
[61,     1] loss: 0.110
[62,     1] loss: 0.117
[63,     1] loss: 0.184
[64,     1] loss: 0.079
[65,     1] loss: 0.158
[66,     1] loss: 0.087
[67,     1] loss: 0.159
[68,     1] loss: 0.088
[69,     1] loss: 0.133
[70,     1] loss: 0.137
[71,     1] loss: 0.107
[72,     1] loss: 0.077
[73,     1] loss: 0.092
[74,     1] loss: 0.069
[75,     1] loss: 0.065
[76,     1] loss: 0.083
[77,     1] loss: 0.067
[78,     1] loss: 0.083
[79,     1] loss: 0.068
[80,     1] loss: 0.049
[81,     1] loss: 0.088
[82,     1] loss: 0.113
[83,     1] loss: 0.070
[84,     1] loss: 0.099
[85,     1] loss: 0.053
[86,     1] loss: 0.070
[87,     1] loss: 0.058
[88,     1] loss: 0.051
[89,     1] loss: 0.096
[90,     1] loss: 0.116
[91,     1] loss: 0.064
[92,     1] loss: 0.058
[93,     1] loss: 0.113
[94,     1] loss: 0.052
[95,     1] loss: 0.122
[96,     1] loss: 0.046
[97,     1] loss: 0.066
[98,     1] loss: 0.136
[99,     1] loss: 0.091
[100,     1] loss: 0.094
[101,     1] loss: 0.077
[102,     1] loss: 0.089
[103,     1] loss: 0.058
[104,     1] loss: 0.047
[105,     1] loss: 0.075
[106,     1] loss: 0.082
[107,     1] loss: 0.078
[108,     1] loss: 0.096
[109,     1] loss: 0.061
[110,     1] loss: 0.099
[111,     1] loss: 0.070
[112,     1] loss: 0.089
[113,     1] loss: 0.053
[114,     1] loss: 0.046
[115,     1] loss: 0.064
[116,     1] loss: 0.041
[117,     1] loss: 0.086
[118,     1] loss: 0.057
[119,     1] loss: 0.048
[120,     1] loss: 0.046
Early stopping applied (best metric=0.29773852229118347)
Finished Training
Total time taken: 13.105213403701782
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.686
[3,     1] loss: 0.657
[4,     1] loss: 0.642
[5,     1] loss: 0.599
[6,     1] loss: 0.565
[7,     1] loss: 0.548
[8,     1] loss: 0.496
[9,     1] loss: 0.497
[10,     1] loss: 0.433
[11,     1] loss: 0.426
[12,     1] loss: 0.443
[13,     1] loss: 0.373
[14,     1] loss: 0.314
[15,     1] loss: 0.348
[16,     1] loss: 0.313
[17,     1] loss: 0.297
[18,     1] loss: 0.308
[19,     1] loss: 0.288
[20,     1] loss: 0.334
[21,     1] loss: 0.311
[22,     1] loss: 0.259
[23,     1] loss: 0.237
[24,     1] loss: 0.348
[25,     1] loss: 0.183
[26,     1] loss: 0.224
[27,     1] loss: 0.232
[28,     1] loss: 0.227
[29,     1] loss: 0.240
[30,     1] loss: 0.225
[31,     1] loss: 0.210
[32,     1] loss: 0.233
[33,     1] loss: 0.263
[34,     1] loss: 0.239
[35,     1] loss: 0.207
[36,     1] loss: 0.255
[37,     1] loss: 0.153
[38,     1] loss: 0.195
[39,     1] loss: 0.191
[40,     1] loss: 0.305
[41,     1] loss: 0.122
[42,     1] loss: 0.199
[43,     1] loss: 0.149
[44,     1] loss: 0.162
[45,     1] loss: 0.236
[46,     1] loss: 0.152
[47,     1] loss: 0.200
[48,     1] loss: 0.142
[49,     1] loss: 0.119
[50,     1] loss: 0.139
[51,     1] loss: 0.139
[52,     1] loss: 0.121
[53,     1] loss: 0.106
[54,     1] loss: 0.115
[55,     1] loss: 0.089
[56,     1] loss: 0.126
[57,     1] loss: 0.109
[58,     1] loss: 0.061
[59,     1] loss: 0.094
[60,     1] loss: 0.072
[61,     1] loss: 0.059
[62,     1] loss: 0.072
[63,     1] loss: 0.057
[64,     1] loss: 0.075
[65,     1] loss: 0.075
[66,     1] loss: 0.042
[67,     1] loss: 0.054
[68,     1] loss: 0.041
[69,     1] loss: 0.050
[70,     1] loss: 0.066
[71,     1] loss: 0.215
[72,     1] loss: 0.137
[73,     1] loss: 0.246
[74,     1] loss: 0.237
[75,     1] loss: 0.059
[76,     1] loss: 0.308
[77,     1] loss: 0.077
[78,     1] loss: 0.215
[79,     1] loss: 0.260
[80,     1] loss: 0.237
[81,     1] loss: 0.177
[82,     1] loss: 0.134
[83,     1] loss: 0.151
[84,     1] loss: 0.197
[85,     1] loss: 0.144
[86,     1] loss: 0.141
[87,     1] loss: 0.133
[88,     1] loss: 0.128
[89,     1] loss: 0.140
[90,     1] loss: 0.104
[91,     1] loss: 0.092
[92,     1] loss: 0.112
[93,     1] loss: 0.095
[94,     1] loss: 0.071
[95,     1] loss: 0.139
[96,     1] loss: 0.072
[97,     1] loss: 0.060
[98,     1] loss: 0.105
[99,     1] loss: 0.072
[100,     1] loss: 0.048
[101,     1] loss: 0.074
[102,     1] loss: 0.054
[103,     1] loss: 0.065
[104,     1] loss: 0.071
[105,     1] loss: 0.079
[106,     1] loss: 0.049
[107,     1] loss: 0.064
[108,     1] loss: 0.081
[109,     1] loss: 0.036
[110,     1] loss: 0.113
[111,     1] loss: 0.075
[112,     1] loss: 0.144
[113,     1] loss: 0.077
[114,     1] loss: 0.094
[115,     1] loss: 0.061
[116,     1] loss: 0.083
[117,     1] loss: 0.049
[118,     1] loss: 0.096
[119,     1] loss: 0.067
[120,     1] loss: 0.071
[121,     1] loss: 0.046
[122,     1] loss: 0.058
[123,     1] loss: 0.054
[124,     1] loss: 0.053
[125,     1] loss: 0.063
[126,     1] loss: 0.056
[127,     1] loss: 0.053
[128,     1] loss: 0.049
[129,     1] loss: 0.056
Early stopping applied (best metric=0.3381003141403198)
Finished Training
Total time taken: 13.970600366592407
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.695
[3,     1] loss: 0.682
[4,     1] loss: 0.668
[5,     1] loss: 0.640
[6,     1] loss: 0.620
[7,     1] loss: 0.615
[8,     1] loss: 0.583
[9,     1] loss: 0.562
[10,     1] loss: 0.531
[11,     1] loss: 0.472
[12,     1] loss: 0.467
[13,     1] loss: 0.472
[14,     1] loss: 0.385
[15,     1] loss: 0.385
[16,     1] loss: 0.440
[17,     1] loss: 0.404
[18,     1] loss: 0.418
[19,     1] loss: 0.387
[20,     1] loss: 0.368
[21,     1] loss: 0.391
[22,     1] loss: 0.374
[23,     1] loss: 0.450
[24,     1] loss: 0.358
[25,     1] loss: 0.395
[26,     1] loss: 0.304
[27,     1] loss: 0.266
[28,     1] loss: 0.335
[29,     1] loss: 0.307
[30,     1] loss: 0.372
[31,     1] loss: 0.338
[32,     1] loss: 0.312
[33,     1] loss: 0.292
[34,     1] loss: 0.306
[35,     1] loss: 0.303
[36,     1] loss: 0.250
[37,     1] loss: 0.332
[38,     1] loss: 0.251
[39,     1] loss: 0.273
[40,     1] loss: 0.219
[41,     1] loss: 0.232
[42,     1] loss: 0.240
[43,     1] loss: 0.217
[44,     1] loss: 0.188
[45,     1] loss: 0.227
[46,     1] loss: 0.243
[47,     1] loss: 0.174
[48,     1] loss: 0.190
[49,     1] loss: 0.141
[50,     1] loss: 0.210
[51,     1] loss: 0.179
[52,     1] loss: 0.184
[53,     1] loss: 0.151
[54,     1] loss: 0.189
[55,     1] loss: 0.199
[56,     1] loss: 0.174
[57,     1] loss: 0.149
[58,     1] loss: 0.140
[59,     1] loss: 0.136
[60,     1] loss: 0.132
[61,     1] loss: 0.124
[62,     1] loss: 0.126
[63,     1] loss: 0.113
[64,     1] loss: 0.121
[65,     1] loss: 0.137
[66,     1] loss: 0.182
[67,     1] loss: 0.179
[68,     1] loss: 0.136
[69,     1] loss: 0.138
[70,     1] loss: 0.092
[71,     1] loss: 0.129
[72,     1] loss: 0.161
[73,     1] loss: 0.115
[74,     1] loss: 0.129
[75,     1] loss: 0.137
[76,     1] loss: 0.153
[77,     1] loss: 0.155
[78,     1] loss: 0.126
[79,     1] loss: 0.265
[80,     1] loss: 0.197
[81,     1] loss: 0.271
[82,     1] loss: 0.357
[83,     1] loss: 0.222
[84,     1] loss: 0.318
[85,     1] loss: 0.260
[86,     1] loss: 0.257
[87,     1] loss: 0.206
[88,     1] loss: 0.240
Early stopping applied (best metric=0.2940162718296051)
Finished Training
Total time taken: 9.590999126434326
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.695
[3,     1] loss: 0.676
[4,     1] loss: 0.644
[5,     1] loss: 0.622
[6,     1] loss: 0.607
[7,     1] loss: 0.574
[8,     1] loss: 0.583
[9,     1] loss: 0.544
[10,     1] loss: 0.523
[11,     1] loss: 0.504
[12,     1] loss: 0.471
[13,     1] loss: 0.448
[14,     1] loss: 0.419
[15,     1] loss: 0.384
[16,     1] loss: 0.342
[17,     1] loss: 0.336
[18,     1] loss: 0.325
[19,     1] loss: 0.351
[20,     1] loss: 0.352
[21,     1] loss: 0.287
[22,     1] loss: 0.325
[23,     1] loss: 0.331
[24,     1] loss: 0.272
[25,     1] loss: 0.260
[26,     1] loss: 0.253
[27,     1] loss: 0.228
[28,     1] loss: 0.310
[29,     1] loss: 0.218
[30,     1] loss: 0.255
[31,     1] loss: 0.249
[32,     1] loss: 0.200
[33,     1] loss: 0.269
[34,     1] loss: 0.274
[35,     1] loss: 0.203
[36,     1] loss: 0.233
[37,     1] loss: 0.180
[38,     1] loss: 0.292
[39,     1] loss: 0.189
[40,     1] loss: 0.204
[41,     1] loss: 0.215
[42,     1] loss: 0.180
[43,     1] loss: 0.230
[44,     1] loss: 0.224
[45,     1] loss: 0.208
[46,     1] loss: 0.248
[47,     1] loss: 0.183
[48,     1] loss: 0.147
[49,     1] loss: 0.159
[50,     1] loss: 0.124
[51,     1] loss: 0.122
[52,     1] loss: 0.144
[53,     1] loss: 0.137
[54,     1] loss: 0.116
[55,     1] loss: 0.125
[56,     1] loss: 0.161
[57,     1] loss: 0.169
[58,     1] loss: 0.121
[59,     1] loss: 0.109
[60,     1] loss: 0.091
[61,     1] loss: 0.159
[62,     1] loss: 0.124
[63,     1] loss: 0.229
[64,     1] loss: 0.235
[65,     1] loss: 0.238
[66,     1] loss: 0.229
[67,     1] loss: 0.189
[68,     1] loss: 0.118
[69,     1] loss: 0.157
[70,     1] loss: 0.179
[71,     1] loss: 0.156
[72,     1] loss: 0.136
[73,     1] loss: 0.120
[74,     1] loss: 0.128
[75,     1] loss: 0.134
[76,     1] loss: 0.100
[77,     1] loss: 0.120
[78,     1] loss: 0.088
[79,     1] loss: 0.096
[80,     1] loss: 0.111
[81,     1] loss: 0.093
[82,     1] loss: 0.134
[83,     1] loss: 0.072
[84,     1] loss: 0.087
[85,     1] loss: 0.122
[86,     1] loss: 0.077
[87,     1] loss: 0.161
[88,     1] loss: 0.124
[89,     1] loss: 0.102
[90,     1] loss: 0.087
[91,     1] loss: 0.095
[92,     1] loss: 0.077
[93,     1] loss: 0.111
[94,     1] loss: 0.065
[95,     1] loss: 0.105
[96,     1] loss: 0.079
[97,     1] loss: 0.081
[98,     1] loss: 0.063
[99,     1] loss: 0.074
[100,     1] loss: 0.081
[101,     1] loss: 0.122
[102,     1] loss: 0.073
[103,     1] loss: 0.094
[104,     1] loss: 0.078
[105,     1] loss: 0.112
[106,     1] loss: 0.110
[107,     1] loss: 0.053
[108,     1] loss: 0.092
[109,     1] loss: 0.069
[110,     1] loss: 0.214
Early stopping applied (best metric=0.2648735046386719)
Finished Training
Total time taken: 12.000999927520752
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.704
[2,     1] loss: 0.690
[3,     1] loss: 0.675
[4,     1] loss: 0.666
[5,     1] loss: 0.633
[6,     1] loss: 0.605
[7,     1] loss: 0.608
[8,     1] loss: 0.579
[9,     1] loss: 0.548
[10,     1] loss: 0.541
[11,     1] loss: 0.525
[12,     1] loss: 0.496
[13,     1] loss: 0.491
[14,     1] loss: 0.463
[15,     1] loss: 0.443
[16,     1] loss: 0.465
[17,     1] loss: 0.446
[18,     1] loss: 0.340
[19,     1] loss: 0.317
[20,     1] loss: 0.376
[21,     1] loss: 0.370
[22,     1] loss: 0.270
[23,     1] loss: 0.371
[24,     1] loss: 0.293
[25,     1] loss: 0.285
[26,     1] loss: 0.260
[27,     1] loss: 0.272
[28,     1] loss: 0.320
[29,     1] loss: 0.210
[30,     1] loss: 0.216
[31,     1] loss: 0.294
[32,     1] loss: 0.268
[33,     1] loss: 0.242
[34,     1] loss: 0.217
[35,     1] loss: 0.260
[36,     1] loss: 0.183
[37,     1] loss: 0.249
[38,     1] loss: 0.155
[39,     1] loss: 0.178
[40,     1] loss: 0.203
[41,     1] loss: 0.133
[42,     1] loss: 0.162
[43,     1] loss: 0.172
[44,     1] loss: 0.169
[45,     1] loss: 0.100
[46,     1] loss: 0.106
[47,     1] loss: 0.174
[48,     1] loss: 0.100
[49,     1] loss: 0.175
[50,     1] loss: 0.173
[51,     1] loss: 0.123
[52,     1] loss: 0.134
[53,     1] loss: 0.115
[54,     1] loss: 0.114
[55,     1] loss: 0.136
[56,     1] loss: 0.120
[57,     1] loss: 0.119
[58,     1] loss: 0.128
[59,     1] loss: 0.103
[60,     1] loss: 0.152
[61,     1] loss: 0.138
[62,     1] loss: 0.138
[63,     1] loss: 0.135
[64,     1] loss: 0.132
[65,     1] loss: 0.122
[66,     1] loss: 0.121
[67,     1] loss: 0.124
[68,     1] loss: 0.124
[69,     1] loss: 0.106
[70,     1] loss: 0.108
[71,     1] loss: 0.144
[72,     1] loss: 0.099
[73,     1] loss: 0.106
[74,     1] loss: 0.077
[75,     1] loss: 0.148
[76,     1] loss: 0.144
[77,     1] loss: 0.104
[78,     1] loss: 0.108
[79,     1] loss: 0.143
[80,     1] loss: 0.117
[81,     1] loss: 0.175
[82,     1] loss: 0.108
[83,     1] loss: 0.210
[84,     1] loss: 0.186
[85,     1] loss: 0.116
[86,     1] loss: 0.111
[87,     1] loss: 0.137
[88,     1] loss: 0.120
[89,     1] loss: 0.112
[90,     1] loss: 0.132
[91,     1] loss: 0.157
[92,     1] loss: 0.145
[93,     1] loss: 0.076
[94,     1] loss: 0.096
[95,     1] loss: 0.102
[96,     1] loss: 0.080
[97,     1] loss: 0.113
[98,     1] loss: 0.090
[99,     1] loss: 0.065
[100,     1] loss: 0.108
[101,     1] loss: 0.132
[102,     1] loss: 0.137
[103,     1] loss: 0.065
[104,     1] loss: 0.154
[105,     1] loss: 0.074
[106,     1] loss: 0.174
[107,     1] loss: 0.119
[108,     1] loss: 0.093
[109,     1] loss: 0.112
[110,     1] loss: 0.124
[111,     1] loss: 0.125
[112,     1] loss: 0.095
[113,     1] loss: 0.102
[114,     1] loss: 0.115
[115,     1] loss: 0.087
[116,     1] loss: 0.073
[117,     1] loss: 0.075
[118,     1] loss: 0.100
[119,     1] loss: 0.095
[120,     1] loss: 0.074
[121,     1] loss: 0.059
[122,     1] loss: 0.076
[123,     1] loss: 0.103
[124,     1] loss: 0.086
[125,     1] loss: 0.085
[126,     1] loss: 0.073
[127,     1] loss: 0.071
[128,     1] loss: 0.085
[129,     1] loss: 0.077
[130,     1] loss: 0.064
[131,     1] loss: 0.085
[132,     1] loss: 0.046
[133,     1] loss: 0.071
Early stopping applied (best metric=0.23223543167114258)
Finished Training
Total time taken: 14.503013372421265
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.693
[3,     1] loss: 0.685
[4,     1] loss: 0.663
[5,     1] loss: 0.634
[6,     1] loss: 0.609
[7,     1] loss: 0.575
[8,     1] loss: 0.556
[9,     1] loss: 0.545
[10,     1] loss: 0.521
[11,     1] loss: 0.480
[12,     1] loss: 0.495
[13,     1] loss: 0.441
[14,     1] loss: 0.419
[15,     1] loss: 0.378
[16,     1] loss: 0.454
[17,     1] loss: 0.374
[18,     1] loss: 0.410
[19,     1] loss: 0.381
[20,     1] loss: 0.346
[21,     1] loss: 0.274
[22,     1] loss: 0.272
[23,     1] loss: 0.279
[24,     1] loss: 0.264
[25,     1] loss: 0.271
[26,     1] loss: 0.202
[27,     1] loss: 0.221
[28,     1] loss: 0.223
[29,     1] loss: 0.250
[30,     1] loss: 0.205
[31,     1] loss: 0.222
[32,     1] loss: 0.184
[33,     1] loss: 0.207
[34,     1] loss: 0.191
[35,     1] loss: 0.202
[36,     1] loss: 0.185
[37,     1] loss: 0.222
[38,     1] loss: 0.134
[39,     1] loss: 0.126
[40,     1] loss: 0.147
[41,     1] loss: 0.165
[42,     1] loss: 0.171
[43,     1] loss: 0.170
[44,     1] loss: 0.183
[45,     1] loss: 0.188
[46,     1] loss: 0.124
[47,     1] loss: 0.164
[48,     1] loss: 0.153
[49,     1] loss: 0.103
[50,     1] loss: 0.147
[51,     1] loss: 0.147
[52,     1] loss: 0.142
[53,     1] loss: 0.196
[54,     1] loss: 0.126
[55,     1] loss: 0.150
[56,     1] loss: 0.116
[57,     1] loss: 0.122
[58,     1] loss: 0.116
[59,     1] loss: 0.161
[60,     1] loss: 0.125
[61,     1] loss: 0.119
[62,     1] loss: 0.126
[63,     1] loss: 0.132
[64,     1] loss: 0.091
[65,     1] loss: 0.081
[66,     1] loss: 0.152
[67,     1] loss: 0.086
[68,     1] loss: 0.104
[69,     1] loss: 0.070
[70,     1] loss: 0.102
[71,     1] loss: 0.095
[72,     1] loss: 0.079
[73,     1] loss: 0.088
Early stopping applied (best metric=0.2629455327987671)
Finished Training
Total time taken: 7.937000274658203
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.676
[4,     1] loss: 0.651
[5,     1] loss: 0.633
[6,     1] loss: 0.622
[7,     1] loss: 0.584
[8,     1] loss: 0.540
[9,     1] loss: 0.517
[10,     1] loss: 0.501
[11,     1] loss: 0.446
[12,     1] loss: 0.405
[13,     1] loss: 0.416
[14,     1] loss: 0.387
[15,     1] loss: 0.357
[16,     1] loss: 0.326
[17,     1] loss: 0.312
[18,     1] loss: 0.387
[19,     1] loss: 0.272
[20,     1] loss: 0.339
[21,     1] loss: 0.335
[22,     1] loss: 0.251
[23,     1] loss: 0.224
[24,     1] loss: 0.221
[25,     1] loss: 0.256
[26,     1] loss: 0.264
[27,     1] loss: 0.229
[28,     1] loss: 0.293
[29,     1] loss: 0.302
[30,     1] loss: 0.223
[31,     1] loss: 0.254
[32,     1] loss: 0.204
[33,     1] loss: 0.260
[34,     1] loss: 0.257
[35,     1] loss: 0.262
[36,     1] loss: 0.183
[37,     1] loss: 0.235
[38,     1] loss: 0.258
[39,     1] loss: 0.258
[40,     1] loss: 0.206
[41,     1] loss: 0.188
[42,     1] loss: 0.192
[43,     1] loss: 0.171
[44,     1] loss: 0.246
[45,     1] loss: 0.203
[46,     1] loss: 0.176
[47,     1] loss: 0.306
[48,     1] loss: 0.158
[49,     1] loss: 0.232
[50,     1] loss: 0.258
[51,     1] loss: 0.202
[52,     1] loss: 0.148
[53,     1] loss: 0.150
[54,     1] loss: 0.175
[55,     1] loss: 0.159
[56,     1] loss: 0.205
[57,     1] loss: 0.110
[58,     1] loss: 0.130
[59,     1] loss: 0.175
[60,     1] loss: 0.116
[61,     1] loss: 0.085
[62,     1] loss: 0.124
[63,     1] loss: 0.133
[64,     1] loss: 0.087
[65,     1] loss: 0.102
[66,     1] loss: 0.142
[67,     1] loss: 0.092
[68,     1] loss: 0.160
[69,     1] loss: 0.131
[70,     1] loss: 0.171
[71,     1] loss: 0.190
[72,     1] loss: 0.078
[73,     1] loss: 0.213
[74,     1] loss: 0.080
[75,     1] loss: 0.143
[76,     1] loss: 0.170
[77,     1] loss: 0.116
[78,     1] loss: 0.106
[79,     1] loss: 0.100
[80,     1] loss: 0.124
[81,     1] loss: 0.073
Early stopping applied (best metric=0.30400609970092773)
Finished Training
Total time taken: 8.883996963500977
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.687
[4,     1] loss: 0.674
[5,     1] loss: 0.647
[6,     1] loss: 0.634
[7,     1] loss: 0.620
[8,     1] loss: 0.607
[9,     1] loss: 0.556
[10,     1] loss: 0.546
[11,     1] loss: 0.529
[12,     1] loss: 0.498
[13,     1] loss: 0.493
[14,     1] loss: 0.446
[15,     1] loss: 0.443
[16,     1] loss: 0.400
[17,     1] loss: 0.380
[18,     1] loss: 0.367
[19,     1] loss: 0.318
[20,     1] loss: 0.348
[21,     1] loss: 0.277
[22,     1] loss: 0.282
[23,     1] loss: 0.394
[24,     1] loss: 0.289
[25,     1] loss: 0.291
[26,     1] loss: 0.203
[27,     1] loss: 0.335
[28,     1] loss: 0.204
[29,     1] loss: 0.214
[30,     1] loss: 0.291
[31,     1] loss: 0.250
[32,     1] loss: 0.282
[33,     1] loss: 0.224
[34,     1] loss: 0.255
[35,     1] loss: 0.200
[36,     1] loss: 0.181
[37,     1] loss: 0.144
[38,     1] loss: 0.152
[39,     1] loss: 0.196
[40,     1] loss: 0.175
[41,     1] loss: 0.170
[42,     1] loss: 0.135
[43,     1] loss: 0.179
[44,     1] loss: 0.165
[45,     1] loss: 0.101
[46,     1] loss: 0.223
[47,     1] loss: 0.106
[48,     1] loss: 0.171
[49,     1] loss: 0.215
[50,     1] loss: 0.114
[51,     1] loss: 0.154
[52,     1] loss: 0.137
[53,     1] loss: 0.079
[54,     1] loss: 0.211
[55,     1] loss: 0.148
[56,     1] loss: 0.120
[57,     1] loss: 0.149
[58,     1] loss: 0.136
[59,     1] loss: 0.130
[60,     1] loss: 0.141
[61,     1] loss: 0.105
[62,     1] loss: 0.097
[63,     1] loss: 0.113
[64,     1] loss: 0.068
[65,     1] loss: 0.088
[66,     1] loss: 0.068
[67,     1] loss: 0.092
[68,     1] loss: 0.072
[69,     1] loss: 0.085
[70,     1] loss: 0.081
[71,     1] loss: 0.112
[72,     1] loss: 0.084
[73,     1] loss: 0.146
[74,     1] loss: 0.081
[75,     1] loss: 0.148
[76,     1] loss: 0.134
[77,     1] loss: 0.127
[78,     1] loss: 0.183
[79,     1] loss: 0.087
Early stopping applied (best metric=0.14716169238090515)
Finished Training
Total time taken: 8.60361909866333
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.688
[3,     1] loss: 0.681
[4,     1] loss: 0.675
[5,     1] loss: 0.645
[6,     1] loss: 0.625
[7,     1] loss: 0.605
[8,     1] loss: 0.554
[9,     1] loss: 0.552
[10,     1] loss: 0.518
[11,     1] loss: 0.470
[12,     1] loss: 0.432
[13,     1] loss: 0.439
[14,     1] loss: 0.415
[15,     1] loss: 0.382
[16,     1] loss: 0.360
[17,     1] loss: 0.353
[18,     1] loss: 0.329
[19,     1] loss: 0.314
[20,     1] loss: 0.287
[21,     1] loss: 0.259
[22,     1] loss: 0.296
[23,     1] loss: 0.259
[24,     1] loss: 0.220
[25,     1] loss: 0.269
[26,     1] loss: 0.211
[27,     1] loss: 0.276
[28,     1] loss: 0.283
[29,     1] loss: 0.295
[30,     1] loss: 0.266
[31,     1] loss: 0.201
[32,     1] loss: 0.220
[33,     1] loss: 0.211
[34,     1] loss: 0.311
[35,     1] loss: 0.212
[36,     1] loss: 0.145
[37,     1] loss: 0.181
[38,     1] loss: 0.152
[39,     1] loss: 0.189
[40,     1] loss: 0.197
[41,     1] loss: 0.160
[42,     1] loss: 0.183
[43,     1] loss: 0.184
[44,     1] loss: 0.126
[45,     1] loss: 0.142
[46,     1] loss: 0.137
[47,     1] loss: 0.192
[48,     1] loss: 0.167
[49,     1] loss: 0.123
[50,     1] loss: 0.156
[51,     1] loss: 0.182
[52,     1] loss: 0.148
[53,     1] loss: 0.139
[54,     1] loss: 0.153
[55,     1] loss: 0.143
[56,     1] loss: 0.092
[57,     1] loss: 0.131
[58,     1] loss: 0.116
[59,     1] loss: 0.153
[60,     1] loss: 0.130
[61,     1] loss: 0.114
[62,     1] loss: 0.113
[63,     1] loss: 0.137
[64,     1] loss: 0.131
[65,     1] loss: 0.102
[66,     1] loss: 0.151
[67,     1] loss: 0.079
Early stopping applied (best metric=0.26435524225234985)
Finished Training
Total time taken: 7.362997770309448
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.681
[4,     1] loss: 0.664
[5,     1] loss: 0.638
[6,     1] loss: 0.638
[7,     1] loss: 0.602
[8,     1] loss: 0.586
[9,     1] loss: 0.543
[10,     1] loss: 0.548
[11,     1] loss: 0.501
[12,     1] loss: 0.477
[13,     1] loss: 0.459
[14,     1] loss: 0.414
[15,     1] loss: 0.393
[16,     1] loss: 0.394
[17,     1] loss: 0.399
[18,     1] loss: 0.365
[19,     1] loss: 0.385
[20,     1] loss: 0.301
[21,     1] loss: 0.316
[22,     1] loss: 0.285
[23,     1] loss: 0.298
[24,     1] loss: 0.330
[25,     1] loss: 0.237
[26,     1] loss: 0.324
[27,     1] loss: 0.349
[28,     1] loss: 0.305
[29,     1] loss: 0.273
[30,     1] loss: 0.252
[31,     1] loss: 0.289
[32,     1] loss: 0.221
[33,     1] loss: 0.235
[34,     1] loss: 0.212
[35,     1] loss: 0.273
[36,     1] loss: 0.171
[37,     1] loss: 0.253
[38,     1] loss: 0.235
[39,     1] loss: 0.201
[40,     1] loss: 0.163
[41,     1] loss: 0.155
[42,     1] loss: 0.154
[43,     1] loss: 0.151
[44,     1] loss: 0.142
[45,     1] loss: 0.114
[46,     1] loss: 0.165
[47,     1] loss: 0.224
[48,     1] loss: 0.152
[49,     1] loss: 0.161
[50,     1] loss: 0.214
[51,     1] loss: 0.132
[52,     1] loss: 0.127
[53,     1] loss: 0.130
[54,     1] loss: 0.119
[55,     1] loss: 0.118
[56,     1] loss: 0.137
[57,     1] loss: 0.092
[58,     1] loss: 0.109
[59,     1] loss: 0.091
[60,     1] loss: 0.091
[61,     1] loss: 0.092
[62,     1] loss: 0.097
[63,     1] loss: 0.095
[64,     1] loss: 0.109
[65,     1] loss: 0.101
[66,     1] loss: 0.130
[67,     1] loss: 0.064
Early stopping applied (best metric=0.35181719064712524)
Finished Training
Total time taken: 7.245001554489136
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.675
[4,     1] loss: 0.652
[5,     1] loss: 0.632
[6,     1] loss: 0.609
[7,     1] loss: 0.603
[8,     1] loss: 0.568
[9,     1] loss: 0.551
[10,     1] loss: 0.524
[11,     1] loss: 0.510
[12,     1] loss: 0.483
[13,     1] loss: 0.456
[14,     1] loss: 0.447
[15,     1] loss: 0.463
[16,     1] loss: 0.437
[17,     1] loss: 0.372
[18,     1] loss: 0.364
[19,     1] loss: 0.345
[20,     1] loss: 0.351
[21,     1] loss: 0.335
[22,     1] loss: 0.293
[23,     1] loss: 0.276
[24,     1] loss: 0.294
[25,     1] loss: 0.322
[26,     1] loss: 0.303
[27,     1] loss: 0.258
[28,     1] loss: 0.211
[29,     1] loss: 0.220
[30,     1] loss: 0.201
[31,     1] loss: 0.254
[32,     1] loss: 0.243
[33,     1] loss: 0.223
[34,     1] loss: 0.175
[35,     1] loss: 0.227
[36,     1] loss: 0.229
[37,     1] loss: 0.191
[38,     1] loss: 0.249
[39,     1] loss: 0.215
[40,     1] loss: 0.227
[41,     1] loss: 0.169
[42,     1] loss: 0.166
[43,     1] loss: 0.231
[44,     1] loss: 0.211
[45,     1] loss: 0.287
[46,     1] loss: 0.168
[47,     1] loss: 0.186
[48,     1] loss: 0.185
[49,     1] loss: 0.146
[50,     1] loss: 0.188
[51,     1] loss: 0.162
[52,     1] loss: 0.169
[53,     1] loss: 0.173
[54,     1] loss: 0.145
[55,     1] loss: 0.156
[56,     1] loss: 0.147
[57,     1] loss: 0.155
[58,     1] loss: 0.123
[59,     1] loss: 0.146
[60,     1] loss: 0.174
[61,     1] loss: 0.135
[62,     1] loss: 0.110
[63,     1] loss: 0.146
[64,     1] loss: 0.101
[65,     1] loss: 0.152
[66,     1] loss: 0.098
[67,     1] loss: 0.113
[68,     1] loss: 0.128
[69,     1] loss: 0.133
[70,     1] loss: 0.123
[71,     1] loss: 0.099
[72,     1] loss: 0.143
[73,     1] loss: 0.094
[74,     1] loss: 0.121
[75,     1] loss: 0.107
[76,     1] loss: 0.153
[77,     1] loss: 0.102
[78,     1] loss: 0.081
[79,     1] loss: 0.123
[80,     1] loss: 0.085
[81,     1] loss: 0.085
[82,     1] loss: 0.136
[83,     1] loss: 0.071
[84,     1] loss: 0.159
[85,     1] loss: 0.108
[86,     1] loss: 0.119
[87,     1] loss: 0.100
[88,     1] loss: 0.151
[89,     1] loss: 0.100
[90,     1] loss: 0.106
[91,     1] loss: 0.090
[92,     1] loss: 0.088
[93,     1] loss: 0.090
[94,     1] loss: 0.102
[95,     1] loss: 0.061
[96,     1] loss: 0.109
[97,     1] loss: 0.085
[98,     1] loss: 0.084
[99,     1] loss: 0.065
[100,     1] loss: 0.076
[101,     1] loss: 0.066
[102,     1] loss: 0.071
[103,     1] loss: 0.067
[104,     1] loss: 0.068
[105,     1] loss: 0.071
[106,     1] loss: 0.056
[107,     1] loss: 0.066
[108,     1] loss: 0.062
[109,     1] loss: 0.072
[110,     1] loss: 0.062
[111,     1] loss: 0.076
[112,     1] loss: 0.122
[113,     1] loss: 0.148
[114,     1] loss: 0.286
[115,     1] loss: 0.121
[116,     1] loss: 0.152
[117,     1] loss: 0.106
[118,     1] loss: 0.114
[119,     1] loss: 0.137
[120,     1] loss: 0.101
[121,     1] loss: 0.109
[122,     1] loss: 0.099
[123,     1] loss: 0.082
[124,     1] loss: 0.088
[125,     1] loss: 0.093
[126,     1] loss: 0.088
[127,     1] loss: 0.072
[128,     1] loss: 0.085
[129,     1] loss: 0.099
[130,     1] loss: 0.072
[131,     1] loss: 0.094
[132,     1] loss: 0.103
[133,     1] loss: 0.140
[134,     1] loss: 0.086
[135,     1] loss: 0.100
[136,     1] loss: 0.095
[137,     1] loss: 0.122
[138,     1] loss: 0.066
[139,     1] loss: 0.102
[140,     1] loss: 0.113
[141,     1] loss: 0.078
[142,     1] loss: 0.054
[143,     1] loss: 0.088
[144,     1] loss: 0.075
[145,     1] loss: 0.061
[146,     1] loss: 0.084
[147,     1] loss: 0.076
[148,     1] loss: 0.076
[149,     1] loss: 0.125
[150,     1] loss: 0.168
[151,     1] loss: 0.300
[152,     1] loss: 0.265
[153,     1] loss: 0.160
[154,     1] loss: 0.153
[155,     1] loss: 0.100
[156,     1] loss: 0.159
[157,     1] loss: 0.156
[158,     1] loss: 0.241
[159,     1] loss: 0.215
[160,     1] loss: 0.180
[161,     1] loss: 0.148
[162,     1] loss: 0.160
[163,     1] loss: 0.148
[164,     1] loss: 0.163
[165,     1] loss: 0.137
[166,     1] loss: 0.094
[167,     1] loss: 0.124
[168,     1] loss: 0.120
[169,     1] loss: 0.132
[170,     1] loss: 0.152
[171,     1] loss: 0.081
[172,     1] loss: 0.088
[173,     1] loss: 0.091
Early stopping applied (best metric=0.3303075134754181)
Finished Training
Total time taken: 19.06422448158264
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.691
[3,     1] loss: 0.690
[4,     1] loss: 0.677
[5,     1] loss: 0.670
[6,     1] loss: 0.665
[7,     1] loss: 0.654
[8,     1] loss: 0.629
[9,     1] loss: 0.616
[10,     1] loss: 0.611
[11,     1] loss: 0.593
[12,     1] loss: 0.578
[13,     1] loss: 0.577
[14,     1] loss: 0.564
[15,     1] loss: 0.561
[16,     1] loss: 0.515
[17,     1] loss: 0.522
[18,     1] loss: 0.528
[19,     1] loss: 0.494
[20,     1] loss: 0.456
[21,     1] loss: 0.452
[22,     1] loss: 0.393
[23,     1] loss: 0.436
[24,     1] loss: 0.376
[25,     1] loss: 0.386
[26,     1] loss: 0.326
[27,     1] loss: 0.365
[28,     1] loss: 0.364
[29,     1] loss: 0.303
[30,     1] loss: 0.338
[31,     1] loss: 0.256
[32,     1] loss: 0.322
[33,     1] loss: 0.283
[34,     1] loss: 0.222
[35,     1] loss: 0.198
[36,     1] loss: 0.214
[37,     1] loss: 0.236
[38,     1] loss: 0.194
[39,     1] loss: 0.198
[40,     1] loss: 0.161
[41,     1] loss: 0.220
[42,     1] loss: 0.228
[43,     1] loss: 0.184
[44,     1] loss: 0.139
[45,     1] loss: 0.098
[46,     1] loss: 0.121
[47,     1] loss: 0.164
[48,     1] loss: 0.160
[49,     1] loss: 0.108
[50,     1] loss: 0.129
[51,     1] loss: 0.082
[52,     1] loss: 0.098
[53,     1] loss: 0.099
[54,     1] loss: 0.125
[55,     1] loss: 0.076
[56,     1] loss: 0.142
[57,     1] loss: 0.096
[58,     1] loss: 0.083
[59,     1] loss: 0.125
[60,     1] loss: 0.050
[61,     1] loss: 0.171
[62,     1] loss: 0.119
[63,     1] loss: 0.174
[64,     1] loss: 0.086
[65,     1] loss: 0.078
[66,     1] loss: 0.156
[67,     1] loss: 0.079
[68,     1] loss: 0.163
[69,     1] loss: 0.122
[70,     1] loss: 0.087
[71,     1] loss: 0.154
[72,     1] loss: 0.068
[73,     1] loss: 0.089
[74,     1] loss: 0.089
[75,     1] loss: 0.110
[76,     1] loss: 0.067
[77,     1] loss: 0.070
[78,     1] loss: 0.066
[79,     1] loss: 0.057
[80,     1] loss: 0.101
[81,     1] loss: 0.067
[82,     1] loss: 0.049
[83,     1] loss: 0.089
[84,     1] loss: 0.053
[85,     1] loss: 0.045
[86,     1] loss: 0.054
Early stopping applied (best metric=0.20142625272274017)
Finished Training
Total time taken: 9.402001857757568
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.687
[3,     1] loss: 0.669
[4,     1] loss: 0.633
[5,     1] loss: 0.588
[6,     1] loss: 0.575
[7,     1] loss: 0.544
[8,     1] loss: 0.500
[9,     1] loss: 0.480
[10,     1] loss: 0.447
[11,     1] loss: 0.438
[12,     1] loss: 0.388
[13,     1] loss: 0.387
[14,     1] loss: 0.366
[15,     1] loss: 0.367
[16,     1] loss: 0.369
[17,     1] loss: 0.323
[18,     1] loss: 0.290
[19,     1] loss: 0.344
[20,     1] loss: 0.270
[21,     1] loss: 0.328
[22,     1] loss: 0.328
[23,     1] loss: 0.230
[24,     1] loss: 0.281
[25,     1] loss: 0.206
[26,     1] loss: 0.226
[27,     1] loss: 0.219
[28,     1] loss: 0.250
[29,     1] loss: 0.170
[30,     1] loss: 0.171
[31,     1] loss: 0.190
[32,     1] loss: 0.190
[33,     1] loss: 0.206
[34,     1] loss: 0.178
[35,     1] loss: 0.212
[36,     1] loss: 0.202
[37,     1] loss: 0.228
[38,     1] loss: 0.193
[39,     1] loss: 0.180
[40,     1] loss: 0.202
[41,     1] loss: 0.157
[42,     1] loss: 0.149
[43,     1] loss: 0.223
[44,     1] loss: 0.200
[45,     1] loss: 0.148
[46,     1] loss: 0.263
[47,     1] loss: 0.136
[48,     1] loss: 0.107
[49,     1] loss: 0.223
[50,     1] loss: 0.131
[51,     1] loss: 0.193
[52,     1] loss: 0.225
[53,     1] loss: 0.176
[54,     1] loss: 0.172
[55,     1] loss: 0.141
[56,     1] loss: 0.146
[57,     1] loss: 0.170
[58,     1] loss: 0.125
[59,     1] loss: 0.107
[60,     1] loss: 0.088
[61,     1] loss: 0.129
[62,     1] loss: 0.091
[63,     1] loss: 0.090
[64,     1] loss: 0.081
Early stopping applied (best metric=0.21327857673168182)
Finished Training
Total time taken: 7.075613737106323
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.683
[3,     1] loss: 0.666
[4,     1] loss: 0.650
[5,     1] loss: 0.619
[6,     1] loss: 0.585
[7,     1] loss: 0.555
[8,     1] loss: 0.567
[9,     1] loss: 0.512
[10,     1] loss: 0.516
[11,     1] loss: 0.480
[12,     1] loss: 0.490
[13,     1] loss: 0.444
[14,     1] loss: 0.432
[15,     1] loss: 0.425
[16,     1] loss: 0.413
[17,     1] loss: 0.383
[18,     1] loss: 0.365
[19,     1] loss: 0.322
[20,     1] loss: 0.330
[21,     1] loss: 0.299
[22,     1] loss: 0.294
[23,     1] loss: 0.304
[24,     1] loss: 0.287
[25,     1] loss: 0.265
[26,     1] loss: 0.266
[27,     1] loss: 0.232
[28,     1] loss: 0.214
[29,     1] loss: 0.206
[30,     1] loss: 0.196
[31,     1] loss: 0.201
[32,     1] loss: 0.193
[33,     1] loss: 0.171
[34,     1] loss: 0.222
[35,     1] loss: 0.212
[36,     1] loss: 0.235
[37,     1] loss: 0.285
[38,     1] loss: 0.290
[39,     1] loss: 0.190
[40,     1] loss: 0.203
[41,     1] loss: 0.175
[42,     1] loss: 0.190
[43,     1] loss: 0.142
[44,     1] loss: 0.193
[45,     1] loss: 0.142
[46,     1] loss: 0.136
[47,     1] loss: 0.183
[48,     1] loss: 0.139
[49,     1] loss: 0.138
[50,     1] loss: 0.121
[51,     1] loss: 0.108
[52,     1] loss: 0.115
[53,     1] loss: 0.092
[54,     1] loss: 0.062
[55,     1] loss: 0.143
[56,     1] loss: 0.124
[57,     1] loss: 0.074
[58,     1] loss: 0.087
[59,     1] loss: 0.117
[60,     1] loss: 0.082
[61,     1] loss: 0.106
[62,     1] loss: 0.083
[63,     1] loss: 0.103
[64,     1] loss: 0.106
[65,     1] loss: 0.107
[66,     1] loss: 0.081
[67,     1] loss: 0.085
[68,     1] loss: 0.075
[69,     1] loss: 0.066
[70,     1] loss: 0.066
[71,     1] loss: 0.119
[72,     1] loss: 0.081
Early stopping applied (best metric=0.39842066168785095)
Finished Training
Total time taken: 7.888002157211304
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.700
[3,     1] loss: 0.691
[4,     1] loss: 0.675
[5,     1] loss: 0.650
[6,     1] loss: 0.628
[7,     1] loss: 0.605
[8,     1] loss: 0.586
[9,     1] loss: 0.564
[10,     1] loss: 0.532
[11,     1] loss: 0.498
[12,     1] loss: 0.476
[13,     1] loss: 0.417
[14,     1] loss: 0.397
[15,     1] loss: 0.388
[16,     1] loss: 0.378
[17,     1] loss: 0.356
[18,     1] loss: 0.372
[19,     1] loss: 0.406
[20,     1] loss: 0.337
[21,     1] loss: 0.288
[22,     1] loss: 0.339
[23,     1] loss: 0.251
[24,     1] loss: 0.283
[25,     1] loss: 0.330
[26,     1] loss: 0.338
[27,     1] loss: 0.321
[28,     1] loss: 0.332
[29,     1] loss: 0.314
[30,     1] loss: 0.283
[31,     1] loss: 0.290
[32,     1] loss: 0.223
[33,     1] loss: 0.246
[34,     1] loss: 0.242
[35,     1] loss: 0.246
[36,     1] loss: 0.182
[37,     1] loss: 0.214
[38,     1] loss: 0.167
[39,     1] loss: 0.256
[40,     1] loss: 0.157
[41,     1] loss: 0.135
[42,     1] loss: 0.163
[43,     1] loss: 0.150
[44,     1] loss: 0.179
[45,     1] loss: 0.118
[46,     1] loss: 0.169
[47,     1] loss: 0.122
[48,     1] loss: 0.177
[49,     1] loss: 0.133
[50,     1] loss: 0.130
[51,     1] loss: 0.174
[52,     1] loss: 0.149
[53,     1] loss: 0.189
[54,     1] loss: 0.098
[55,     1] loss: 0.108
[56,     1] loss: 0.147
[57,     1] loss: 0.083
[58,     1] loss: 0.152
[59,     1] loss: 0.153
[60,     1] loss: 0.123
[61,     1] loss: 0.184
Early stopping applied (best metric=0.3956524431705475)
Finished Training
Total time taken: 6.680001258850098
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.684
[4,     1] loss: 0.660
[5,     1] loss: 0.629
[6,     1] loss: 0.605
[7,     1] loss: 0.593
[8,     1] loss: 0.574
[9,     1] loss: 0.554
[10,     1] loss: 0.494
[11,     1] loss: 0.441
[12,     1] loss: 0.450
[13,     1] loss: 0.428
[14,     1] loss: 0.407
[15,     1] loss: 0.362
[16,     1] loss: 0.322
[17,     1] loss: 0.339
[18,     1] loss: 0.347
[19,     1] loss: 0.228
[20,     1] loss: 0.281
[21,     1] loss: 0.316
[22,     1] loss: 0.331
[23,     1] loss: 0.357
[24,     1] loss: 0.284
[25,     1] loss: 0.399
[26,     1] loss: 0.250
[27,     1] loss: 0.282
[28,     1] loss: 0.304
[29,     1] loss: 0.340
[30,     1] loss: 0.317
[31,     1] loss: 0.262
[32,     1] loss: 0.259
[33,     1] loss: 0.238
[34,     1] loss: 0.253
[35,     1] loss: 0.244
[36,     1] loss: 0.213
[37,     1] loss: 0.239
[38,     1] loss: 0.216
[39,     1] loss: 0.252
[40,     1] loss: 0.230
[41,     1] loss: 0.188
[42,     1] loss: 0.190
[43,     1] loss: 0.197
[44,     1] loss: 0.196
[45,     1] loss: 0.168
[46,     1] loss: 0.164
[47,     1] loss: 0.118
[48,     1] loss: 0.158
[49,     1] loss: 0.125
[50,     1] loss: 0.142
[51,     1] loss: 0.135
[52,     1] loss: 0.146
[53,     1] loss: 0.138
[54,     1] loss: 0.131
[55,     1] loss: 0.097
[56,     1] loss: 0.103
[57,     1] loss: 0.141
[58,     1] loss: 0.113
[59,     1] loss: 0.106
[60,     1] loss: 0.130
[61,     1] loss: 0.076
[62,     1] loss: 0.103
[63,     1] loss: 0.142
[64,     1] loss: 0.106
[65,     1] loss: 0.144
[66,     1] loss: 0.070
[67,     1] loss: 0.153
[68,     1] loss: 0.091
[69,     1] loss: 0.133
[70,     1] loss: 0.077
[71,     1] loss: 0.090
[72,     1] loss: 0.101
[73,     1] loss: 0.078
[74,     1] loss: 0.097
[75,     1] loss: 0.113
[76,     1] loss: 0.093
[77,     1] loss: 0.118
[78,     1] loss: 0.100
[79,     1] loss: 0.065
[80,     1] loss: 0.081
[81,     1] loss: 0.079
[82,     1] loss: 0.072
[83,     1] loss: 0.060
[84,     1] loss: 0.117
[85,     1] loss: 0.103
[86,     1] loss: 0.108
[87,     1] loss: 0.080
[88,     1] loss: 0.126
[89,     1] loss: 0.143
[90,     1] loss: 0.174
[91,     1] loss: 0.109
[92,     1] loss: 0.084
[93,     1] loss: 0.195
Early stopping applied (best metric=0.1176070123910904)
Finished Training
Total time taken: 10.126999616622925
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.690
[3,     1] loss: 0.688
[4,     1] loss: 0.685
[5,     1] loss: 0.676
[6,     1] loss: 0.659
[7,     1] loss: 0.650
[8,     1] loss: 0.637
[9,     1] loss: 0.618
[10,     1] loss: 0.598
[11,     1] loss: 0.586
[12,     1] loss: 0.562
[13,     1] loss: 0.559
[14,     1] loss: 0.502
[15,     1] loss: 0.478
[16,     1] loss: 0.492
[17,     1] loss: 0.458
[18,     1] loss: 0.453
[19,     1] loss: 0.450
[20,     1] loss: 0.381
[21,     1] loss: 0.415
[22,     1] loss: 0.406
[23,     1] loss: 0.365
[24,     1] loss: 0.333
[25,     1] loss: 0.372
[26,     1] loss: 0.340
[27,     1] loss: 0.373
[28,     1] loss: 0.292
[29,     1] loss: 0.261
[30,     1] loss: 0.306
[31,     1] loss: 0.305
[32,     1] loss: 0.247
[33,     1] loss: 0.271
[34,     1] loss: 0.243
[35,     1] loss: 0.273
[36,     1] loss: 0.242
[37,     1] loss: 0.263
[38,     1] loss: 0.191
[39,     1] loss: 0.289
[40,     1] loss: 0.188
[41,     1] loss: 0.199
[42,     1] loss: 0.245
[43,     1] loss: 0.264
[44,     1] loss: 0.195
[45,     1] loss: 0.226
[46,     1] loss: 0.184
[47,     1] loss: 0.231
[48,     1] loss: 0.154
[49,     1] loss: 0.206
[50,     1] loss: 0.166
[51,     1] loss: 0.153
[52,     1] loss: 0.153
[53,     1] loss: 0.163
[54,     1] loss: 0.129
[55,     1] loss: 0.159
[56,     1] loss: 0.174
[57,     1] loss: 0.159
[58,     1] loss: 0.212
[59,     1] loss: 0.147
[60,     1] loss: 0.121
[61,     1] loss: 0.124
[62,     1] loss: 0.099
[63,     1] loss: 0.107
[64,     1] loss: 0.093
[65,     1] loss: 0.110
[66,     1] loss: 0.138
[67,     1] loss: 0.115
[68,     1] loss: 0.135
[69,     1] loss: 0.123
[70,     1] loss: 0.093
[71,     1] loss: 0.109
[72,     1] loss: 0.118
[73,     1] loss: 0.156
[74,     1] loss: 0.242
[75,     1] loss: 0.090
[76,     1] loss: 0.140
[77,     1] loss: 0.109
[78,     1] loss: 0.109
[79,     1] loss: 0.129
[80,     1] loss: 0.082
[81,     1] loss: 0.097
[82,     1] loss: 0.110
[83,     1] loss: 0.090
[84,     1] loss: 0.075
[85,     1] loss: 0.088
[86,     1] loss: 0.085
[87,     1] loss: 0.075
[88,     1] loss: 0.082
[89,     1] loss: 0.056
[90,     1] loss: 0.102
[91,     1] loss: 0.085
[92,     1] loss: 0.127
[93,     1] loss: 0.149
[94,     1] loss: 0.263
[95,     1] loss: 0.203
[96,     1] loss: 0.066
[97,     1] loss: 0.347
[98,     1] loss: 0.162
[99,     1] loss: 0.219
[100,     1] loss: 0.228
[101,     1] loss: 0.154
[102,     1] loss: 0.120
[103,     1] loss: 0.168
[104,     1] loss: 0.101
[105,     1] loss: 0.113
[106,     1] loss: 0.121
[107,     1] loss: 0.129
[108,     1] loss: 0.144
[109,     1] loss: 0.107
[110,     1] loss: 0.122
[111,     1] loss: 0.140
[112,     1] loss: 0.101
[113,     1] loss: 0.137
[114,     1] loss: 0.160
[115,     1] loss: 0.122
[116,     1] loss: 0.083
[117,     1] loss: 0.095
[118,     1] loss: 0.097
[119,     1] loss: 0.096
[120,     1] loss: 0.121
[121,     1] loss: 0.156
[122,     1] loss: 0.095
[123,     1] loss: 0.097
[124,     1] loss: 0.077
[125,     1] loss: 0.097
[126,     1] loss: 0.071
[127,     1] loss: 0.078
[128,     1] loss: 0.098
Early stopping applied (best metric=0.26258957386016846)
Finished Training
Total time taken: 14.077998161315918
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.699
[3,     1] loss: 0.682
[4,     1] loss: 0.665
[5,     1] loss: 0.658
[6,     1] loss: 0.621
[7,     1] loss: 0.608
[8,     1] loss: 0.586
[9,     1] loss: 0.562
[10,     1] loss: 0.561
[11,     1] loss: 0.534
[12,     1] loss: 0.505
[13,     1] loss: 0.525
[14,     1] loss: 0.465
[15,     1] loss: 0.448
[16,     1] loss: 0.390
[17,     1] loss: 0.404
[18,     1] loss: 0.358
[19,     1] loss: 0.366
[20,     1] loss: 0.326
[21,     1] loss: 0.343
[22,     1] loss: 0.333
[23,     1] loss: 0.331
[24,     1] loss: 0.347
[25,     1] loss: 0.311
[26,     1] loss: 0.301
[27,     1] loss: 0.318
[28,     1] loss: 0.298
[29,     1] loss: 0.296
[30,     1] loss: 0.275
[31,     1] loss: 0.308
[32,     1] loss: 0.287
[33,     1] loss: 0.217
[34,     1] loss: 0.191
[35,     1] loss: 0.177
[36,     1] loss: 0.169
[37,     1] loss: 0.176
[38,     1] loss: 0.232
[39,     1] loss: 0.177
[40,     1] loss: 0.201
[41,     1] loss: 0.209
[42,     1] loss: 0.198
[43,     1] loss: 0.145
[44,     1] loss: 0.174
[45,     1] loss: 0.161
[46,     1] loss: 0.161
[47,     1] loss: 0.107
[48,     1] loss: 0.195
[49,     1] loss: 0.129
[50,     1] loss: 0.124
[51,     1] loss: 0.122
[52,     1] loss: 0.153
[53,     1] loss: 0.153
[54,     1] loss: 0.122
[55,     1] loss: 0.109
[56,     1] loss: 0.175
[57,     1] loss: 0.109
[58,     1] loss: 0.127
[59,     1] loss: 0.199
[60,     1] loss: 0.157
[61,     1] loss: 0.189
[62,     1] loss: 0.106
[63,     1] loss: 0.082
[64,     1] loss: 0.138
[65,     1] loss: 0.129
[66,     1] loss: 0.115
[67,     1] loss: 0.081
[68,     1] loss: 0.090
[69,     1] loss: 0.109
[70,     1] loss: 0.106
[71,     1] loss: 0.076
[72,     1] loss: 0.092
[73,     1] loss: 0.087
[74,     1] loss: 0.092
[75,     1] loss: 0.101
[76,     1] loss: 0.087
[77,     1] loss: 0.063
[78,     1] loss: 0.072
[79,     1] loss: 0.080
[80,     1] loss: 0.062
[81,     1] loss: 0.085
[82,     1] loss: 0.054
[83,     1] loss: 0.067
[84,     1] loss: 0.082
[85,     1] loss: 0.075
[86,     1] loss: 0.073
[87,     1] loss: 0.062
[88,     1] loss: 0.126
[89,     1] loss: 0.116
[90,     1] loss: 0.107
[91,     1] loss: 0.084
[92,     1] loss: 0.117
[93,     1] loss: 0.077
[94,     1] loss: 0.139
[95,     1] loss: 0.078
[96,     1] loss: 0.148
[97,     1] loss: 0.152
[98,     1] loss: 0.194
[99,     1] loss: 0.076
[100,     1] loss: 0.100
[101,     1] loss: 0.129
[102,     1] loss: 0.092
[103,     1] loss: 0.091
[104,     1] loss: 0.076
[105,     1] loss: 0.080
[106,     1] loss: 0.102
[107,     1] loss: 0.081
[108,     1] loss: 0.116
[109,     1] loss: 0.096
[110,     1] loss: 0.103
[111,     1] loss: 0.080
[112,     1] loss: 0.248
[113,     1] loss: 0.076
[114,     1] loss: 0.121
[115,     1] loss: 0.137
[116,     1] loss: 0.161
[117,     1] loss: 0.119
[118,     1] loss: 0.109
[119,     1] loss: 0.128
[120,     1] loss: 0.076
[121,     1] loss: 0.121
[122,     1] loss: 0.096
[123,     1] loss: 0.098
[124,     1] loss: 0.078
[125,     1] loss: 0.075
Early stopping applied (best metric=0.21681097149848938)
Finished Training
Total time taken: 13.721001148223877
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.683
[3,     1] loss: 0.682
[4,     1] loss: 0.647
[5,     1] loss: 0.638
[6,     1] loss: 0.616
[7,     1] loss: 0.594
[8,     1] loss: 0.567
[9,     1] loss: 0.538
[10,     1] loss: 0.516
[11,     1] loss: 0.486
[12,     1] loss: 0.466
[13,     1] loss: 0.446
[14,     1] loss: 0.444
[15,     1] loss: 0.432
[16,     1] loss: 0.372
[17,     1] loss: 0.421
[18,     1] loss: 0.306
[19,     1] loss: 0.291
[20,     1] loss: 0.314
[21,     1] loss: 0.283
[22,     1] loss: 0.262
[23,     1] loss: 0.275
[24,     1] loss: 0.237
[25,     1] loss: 0.217
[26,     1] loss: 0.173
[27,     1] loss: 0.229
[28,     1] loss: 0.196
[29,     1] loss: 0.180
[30,     1] loss: 0.129
[31,     1] loss: 0.191
[32,     1] loss: 0.146
[33,     1] loss: 0.164
[34,     1] loss: 0.128
[35,     1] loss: 0.151
[36,     1] loss: 0.137
[37,     1] loss: 0.149
[38,     1] loss: 0.122
[39,     1] loss: 0.116
[40,     1] loss: 0.083
[41,     1] loss: 0.105
[42,     1] loss: 0.180
[43,     1] loss: 0.089
[44,     1] loss: 0.267
[45,     1] loss: 0.227
[46,     1] loss: 0.111
[47,     1] loss: 0.139
[48,     1] loss: 0.112
[49,     1] loss: 0.107
[50,     1] loss: 0.140
[51,     1] loss: 0.123
[52,     1] loss: 0.135
[53,     1] loss: 0.138
[54,     1] loss: 0.102
[55,     1] loss: 0.098
[56,     1] loss: 0.118
[57,     1] loss: 0.134
[58,     1] loss: 0.118
[59,     1] loss: 0.086
[60,     1] loss: 0.126
Early stopping applied (best metric=0.43859630823135376)
Finished Training
Total time taken: 6.6060004234313965
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.691
[3,     1] loss: 0.683
[4,     1] loss: 0.671
[5,     1] loss: 0.643
[6,     1] loss: 0.626
[7,     1] loss: 0.604
[8,     1] loss: 0.561
[9,     1] loss: 0.567
[10,     1] loss: 0.507
[11,     1] loss: 0.496
[12,     1] loss: 0.491
[13,     1] loss: 0.454
[14,     1] loss: 0.426
[15,     1] loss: 0.418
[16,     1] loss: 0.359
[17,     1] loss: 0.344
[18,     1] loss: 0.301
[19,     1] loss: 0.263
[20,     1] loss: 0.250
[21,     1] loss: 0.265
[22,     1] loss: 0.251
[23,     1] loss: 0.219
[24,     1] loss: 0.301
[25,     1] loss: 0.226
[26,     1] loss: 0.287
[27,     1] loss: 0.253
[28,     1] loss: 0.288
[29,     1] loss: 0.218
[30,     1] loss: 0.197
[31,     1] loss: 0.208
[32,     1] loss: 0.273
[33,     1] loss: 0.192
[34,     1] loss: 0.210
[35,     1] loss: 0.232
[36,     1] loss: 0.199
[37,     1] loss: 0.143
[38,     1] loss: 0.148
[39,     1] loss: 0.206
[40,     1] loss: 0.182
[41,     1] loss: 0.162
[42,     1] loss: 0.167
[43,     1] loss: 0.180
[44,     1] loss: 0.129
[45,     1] loss: 0.162
[46,     1] loss: 0.156
[47,     1] loss: 0.101
[48,     1] loss: 0.180
[49,     1] loss: 0.102
[50,     1] loss: 0.184
[51,     1] loss: 0.110
[52,     1] loss: 0.147
[53,     1] loss: 0.127
[54,     1] loss: 0.134
[55,     1] loss: 0.091
[56,     1] loss: 0.096
[57,     1] loss: 0.117
[58,     1] loss: 0.112
[59,     1] loss: 0.117
[60,     1] loss: 0.139
[61,     1] loss: 0.099
[62,     1] loss: 0.106
[63,     1] loss: 0.157
[64,     1] loss: 0.140
[65,     1] loss: 0.168
[66,     1] loss: 0.090
Early stopping applied (best metric=0.32235410809516907)
Finished Training
Total time taken: 7.262611389160156
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.683
[4,     1] loss: 0.670
[5,     1] loss: 0.649
[6,     1] loss: 0.629
[7,     1] loss: 0.619
[8,     1] loss: 0.581
[9,     1] loss: 0.582
[10,     1] loss: 0.522
[11,     1] loss: 0.520
[12,     1] loss: 0.501
[13,     1] loss: 0.474
[14,     1] loss: 0.436
[15,     1] loss: 0.397
[16,     1] loss: 0.435
[17,     1] loss: 0.378
[18,     1] loss: 0.373
[19,     1] loss: 0.364
[20,     1] loss: 0.382
[21,     1] loss: 0.385
[22,     1] loss: 0.331
[23,     1] loss: 0.366
[24,     1] loss: 0.307
[25,     1] loss: 0.291
[26,     1] loss: 0.378
[27,     1] loss: 0.328
[28,     1] loss: 0.326
[29,     1] loss: 0.248
[30,     1] loss: 0.293
[31,     1] loss: 0.269
[32,     1] loss: 0.242
[33,     1] loss: 0.244
[34,     1] loss: 0.271
[35,     1] loss: 0.227
[36,     1] loss: 0.245
[37,     1] loss: 0.232
[38,     1] loss: 0.246
[39,     1] loss: 0.245
[40,     1] loss: 0.218
[41,     1] loss: 0.223
[42,     1] loss: 0.192
[43,     1] loss: 0.147
[44,     1] loss: 0.176
[45,     1] loss: 0.167
[46,     1] loss: 0.183
[47,     1] loss: 0.156
[48,     1] loss: 0.202
[49,     1] loss: 0.187
[50,     1] loss: 0.178
[51,     1] loss: 0.162
[52,     1] loss: 0.184
[53,     1] loss: 0.189
[54,     1] loss: 0.158
[55,     1] loss: 0.143
[56,     1] loss: 0.209
[57,     1] loss: 0.147
[58,     1] loss: 0.233
[59,     1] loss: 0.166
[60,     1] loss: 0.145
[61,     1] loss: 0.169
[62,     1] loss: 0.117
[63,     1] loss: 0.116
Early stopping applied (best metric=0.3694615066051483)
Finished Training
Total time taken: 6.920244932174683
{'Hydroxylation-K Validation Accuracy': 0.8730141843971632, 'Hydroxylation-K Validation Sensitivity': 0.8911111111111111, 'Hydroxylation-K Validation Specificity': 0.868421052631579, 'Hydroxylation-K Validation Precision': 0.6404605394605395, 'Hydroxylation-K AUC ROC': 0.897280701754386, 'Hydroxylation-K AUC PR': 0.7053202704645132, 'Hydroxylation-K MCC': 0.6802298953835652, 'Hydroxylation-K F1': 0.7411316436423988, 'Validation Loss (Hydroxylation-K)': 0.2887787157297134, 'Validation Loss (total)': 0.2887787157297134, 'TimeToTrain': 9.948885650634766}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001923161682622689,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3218225296,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.967936618884497}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.685
[3,     1] loss: 0.666
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006607916832359632,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1964087829,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.369290951819085}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.691
[3,     1] loss: 0.678
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004841061160904428,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1433570684,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.722816775412426}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.676
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008890673490065388,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1419554508,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.257017540637786}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.677
[4,     1] loss: 0.652
[5,     1] loss: 0.607
[6,     1] loss: 0.586
[7,     1] loss: 0.533
[8,     1] loss: 0.506
[9,     1] loss: 0.442
[10,     1] loss: 0.424
[11,     1] loss: 0.416
[12,     1] loss: 0.393
[13,     1] loss: 0.311
[14,     1] loss: 0.319
[15,     1] loss: 0.356
[16,     1] loss: 0.458
[17,     1] loss: 0.367
[18,     1] loss: 0.404
[19,     1] loss: 0.344
[20,     1] loss: 0.335
[21,     1] loss: 0.364
[22,     1] loss: 0.414
[23,     1] loss: 0.395
[24,     1] loss: 0.325
[25,     1] loss: 0.326
[26,     1] loss: 0.299
[27,     1] loss: 0.351
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0067276415586717065,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2213436320,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.036065161632463}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.688
[3,     1] loss: 0.676
[4,     1] loss: 0.653
[5,     1] loss: 0.601
[6,     1] loss: 0.587
[7,     1] loss: 0.523
[8,     1] loss: 0.471
[9,     1] loss: 0.386
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008595868930154063,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3791354412,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.2691644164479392}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.694
[3,     1] loss: 0.688
[4,     1] loss: 0.681
[5,     1] loss: 0.676
[6,     1] loss: 0.665
[7,     1] loss: 0.652
[8,     1] loss: 0.625
[9,     1] loss: 0.617
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002062630738720499,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3998266123,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.158062449238422}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.673
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028954238931306786,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3406037664,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.636391724532542}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.684
[3,     1] loss: 0.669
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002269589527938146,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2159914825,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.655886511666202}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.680
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006376209800158556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1274730611,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.2619703423231563}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.688
[3,     1] loss: 0.677
[4,     1] loss: 0.636
[5,     1] loss: 0.588
[6,     1] loss: 0.523
[7,     1] loss: 0.483
[8,     1] loss: 0.477
[9,     1] loss: 0.430
[10,     1] loss: 0.425
[11,     1] loss: 0.406
[12,     1] loss: 0.382
[13,     1] loss: 0.404
[14,     1] loss: 0.301
[15,     1] loss: 0.347
[16,     1] loss: 0.298
[17,     1] loss: 0.292
[18,     1] loss: 0.247
[19,     1] loss: 0.215
[20,     1] loss: 0.199
[21,     1] loss: 0.241
[22,     1] loss: 0.202
[23,     1] loss: 0.202
[24,     1] loss: 0.170
[25,     1] loss: 0.181
[26,     1] loss: 0.154
[27,     1] loss: 0.208
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006911469423697764,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1834479519,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 21.496593662391703}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.686
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017886156385778658,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3947503979,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.845577183778786}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.693
[3,     1] loss: 0.680
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009841348778567235,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3186474172,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.995820076720301}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.698
[3,     1] loss: 0.691
[4,     1] loss: 0.695
[5,     1] loss: 0.689
[6,     1] loss: 0.683
[7,     1] loss: 0.672
[8,     1] loss: 0.651
[9,     1] loss: 0.640
[10,     1] loss: 0.610
[11,     1] loss: 0.574
[12,     1] loss: 0.540
[13,     1] loss: 0.529
[14,     1] loss: 0.508
[15,     1] loss: 0.480
[16,     1] loss: 0.422
[17,     1] loss: 0.446
[18,     1] loss: 0.624
[19,     1] loss: 0.557
[20,     1] loss: 0.472
[21,     1] loss: 0.559
[22,     1] loss: 0.479
[23,     1] loss: 0.464
[24,     1] loss: 0.441
[25,     1] loss: 0.443
[26,     1] loss: 0.428
[27,     1] loss: 0.446
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003886664559620655,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 731325568,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.408402964283646}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.679
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004557370084525096,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3211124130,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.993013440948562}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.674
[4,     1] loss: 0.651
[5,     1] loss: 0.629
[6,     1] loss: 0.590
[7,     1] loss: 0.564
[8,     1] loss: 0.497
[9,     1] loss: 0.513
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007109882856560802,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 32718999,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.59893762946141}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.689
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024827339383238136,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2007671370,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.4061330516399946}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.684
[3,     1] loss: 0.659
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027268994358583534,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2940730168,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.554297761322418}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.676
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003928235973938042,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 799872563,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.084033431045652}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.716
[2,     1] loss: 0.694
[3,     1] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032392988721379784,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1138140412,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.359541026682257}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.682
[3,     1] loss: 0.654
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002826547823820429,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3967561699,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 22.30253261182213}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.674
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006617670571296217,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2069414448,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.7399020071390963}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008064613575976456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3249867130,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.965718662487358}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008668908481459006,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1725347067,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.628241436369109}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.698
[3,     1] loss: 0.672
[4,     1] loss: 0.641
[5,     1] loss: 0.591
[6,     1] loss: 0.537
[7,     1] loss: 0.470
[8,     1] loss: 0.439
[9,     1] loss: 0.389
[10,     1] loss: 0.388
[11,     1] loss: 0.353
[12,     1] loss: 0.327
[13,     1] loss: 0.263
[14,     1] loss: 0.282
[15,     1] loss: 0.306
[16,     1] loss: 0.255
[17,     1] loss: 0.210
[18,     1] loss: 0.183
[19,     1] loss: 0.316
[20,     1] loss: 0.170
[21,     1] loss: 0.189
[22,     1] loss: 0.211
[23,     1] loss: 0.153
[24,     1] loss: 0.171
[25,     1] loss: 0.167
[26,     1] loss: 0.228
[27,     1] loss: 0.180
[28,     1] loss: 0.232
[29,     1] loss: 0.183
[30,     1] loss: 0.188
[31,     1] loss: 0.177
[32,     1] loss: 0.275
[33,     1] loss: 0.176
[34,     1] loss: 0.243
[35,     1] loss: 0.214
[36,     1] loss: 0.267
[37,     1] loss: 0.181
[38,     1] loss: 0.226
[39,     1] loss: 0.181
[40,     1] loss: 0.182
[41,     1] loss: 0.151
[42,     1] loss: 0.136
[43,     1] loss: 0.128
[44,     1] loss: 0.124
[45,     1] loss: 0.337
[46,     1] loss: 0.230
[47,     1] loss: 0.348
[48,     1] loss: 0.266
[49,     1] loss: 0.213
[50,     1] loss: 0.222
[51,     1] loss: 0.230
[52,     1] loss: 0.215
[53,     1] loss: 0.215
[54,     1] loss: 0.198
[55,     1] loss: 0.212
[56,     1] loss: 0.189
[57,     1] loss: 0.199
[58,     1] loss: 0.177
[59,     1] loss: 0.149
[60,     1] loss: 0.167
[61,     1] loss: 0.120
[62,     1] loss: 0.162
[63,     1] loss: 0.166
[64,     1] loss: 0.164
[65,     1] loss: 0.148
[66,     1] loss: 0.147
[67,     1] loss: 0.132
[68,     1] loss: 0.192
[69,     1] loss: 0.209
[70,     1] loss: 0.157
[71,     1] loss: 0.142
[72,     1] loss: 0.262
[73,     1] loss: 0.111
[74,     1] loss: 0.212
[75,     1] loss: 0.158
[76,     1] loss: 0.143
[77,     1] loss: 0.172
[78,     1] loss: 0.156
[79,     1] loss: 0.192
[80,     1] loss: 0.160
[81,     1] loss: 0.178
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003028946053892055,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2575953000,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.90398471550359}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.700
[3,     1] loss: 0.695
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006175297410188136,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 307079369,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.8866416402112023}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.667
[4,     1] loss: 0.627
[5,     1] loss: 0.576
[6,     1] loss: 0.523
[7,     1] loss: 0.508
[8,     1] loss: 0.437
[9,     1] loss: 0.351
[10,     1] loss: 0.323
[11,     1] loss: 0.307
[12,     1] loss: 0.357
[13,     1] loss: 0.410
[14,     1] loss: 0.299
[15,     1] loss: 0.446
[16,     1] loss: 0.277
[17,     1] loss: 0.318
[18,     1] loss: 0.328
[19,     1] loss: 0.271
[20,     1] loss: 0.297
[21,     1] loss: 0.298
[22,     1] loss: 0.280
[23,     1] loss: 0.251
[24,     1] loss: 0.285
[25,     1] loss: 0.244
[26,     1] loss: 0.204
[27,     1] loss: 0.209
[28,     1] loss: 0.168
[29,     1] loss: 0.173
[30,     1] loss: 0.152
[31,     1] loss: 0.148
[32,     1] loss: 0.150
[33,     1] loss: 0.150
[34,     1] loss: 0.128
[35,     1] loss: 0.185
[36,     1] loss: 0.139
[37,     1] loss: 0.168
[38,     1] loss: 0.194
[39,     1] loss: 0.127
[40,     1] loss: 0.179
[41,     1] loss: 0.239
[42,     1] loss: 0.111
[43,     1] loss: 0.246
[44,     1] loss: 0.132
[45,     1] loss: 0.140
[46,     1] loss: 0.187
[47,     1] loss: 0.126
[48,     1] loss: 0.143
[49,     1] loss: 0.111
[50,     1] loss: 0.142
[51,     1] loss: 0.132
[52,     1] loss: 0.115
[53,     1] loss: 0.073
[54,     1] loss: 0.115
[55,     1] loss: 0.104
[56,     1] loss: 0.081
[57,     1] loss: 0.067
[58,     1] loss: 0.073
[59,     1] loss: 0.068
[60,     1] loss: 0.081
[61,     1] loss: 0.048
Early stopping applied (best metric=0.3128286302089691)
Finished Training
Total time taken: 6.704999685287476
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.700
[3,     1] loss: 0.682
[4,     1] loss: 0.653
[5,     1] loss: 0.622
[6,     1] loss: 0.556
[7,     1] loss: 0.498
[8,     1] loss: 0.438
[9,     1] loss: 0.439
[10,     1] loss: 0.362
[11,     1] loss: 0.295
[12,     1] loss: 0.264
[13,     1] loss: 0.283
[14,     1] loss: 0.260
[15,     1] loss: 0.280
[16,     1] loss: 0.168
[17,     1] loss: 0.258
[18,     1] loss: 0.176
[19,     1] loss: 0.303
[20,     1] loss: 0.193
[21,     1] loss: 0.210
[22,     1] loss: 0.250
[23,     1] loss: 0.278
[24,     1] loss: 0.154
[25,     1] loss: 0.181
[26,     1] loss: 0.234
[27,     1] loss: 0.181
[28,     1] loss: 0.170
[29,     1] loss: 0.229
[30,     1] loss: 0.224
[31,     1] loss: 0.203
[32,     1] loss: 0.170
[33,     1] loss: 0.276
[34,     1] loss: 0.187
[35,     1] loss: 0.192
[36,     1] loss: 0.140
[37,     1] loss: 0.180
[38,     1] loss: 0.193
[39,     1] loss: 0.197
[40,     1] loss: 0.172
[41,     1] loss: 0.150
[42,     1] loss: 0.200
[43,     1] loss: 0.183
[44,     1] loss: 0.144
[45,     1] loss: 0.161
[46,     1] loss: 0.140
[47,     1] loss: 0.102
[48,     1] loss: 0.140
[49,     1] loss: 0.119
[50,     1] loss: 0.107
[51,     1] loss: 0.109
[52,     1] loss: 0.086
[53,     1] loss: 0.108
[54,     1] loss: 0.148
[55,     1] loss: 0.118
[56,     1] loss: 0.091
[57,     1] loss: 0.154
[58,     1] loss: 0.122
[59,     1] loss: 0.069
[60,     1] loss: 0.094
Early stopping applied (best metric=0.43380650877952576)
Finished Training
Total time taken: 6.609000205993652
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.699
[3,     1] loss: 0.680
[4,     1] loss: 0.663
[5,     1] loss: 0.635
[6,     1] loss: 0.592
[7,     1] loss: 0.548
[8,     1] loss: 0.501
[9,     1] loss: 0.465
[10,     1] loss: 0.450
[11,     1] loss: 0.342
[12,     1] loss: 0.342
[13,     1] loss: 0.284
[14,     1] loss: 0.224
[15,     1] loss: 0.198
[16,     1] loss: 0.192
[17,     1] loss: 0.237
[18,     1] loss: 0.189
[19,     1] loss: 0.251
[20,     1] loss: 0.157
[21,     1] loss: 0.230
[22,     1] loss: 0.156
[23,     1] loss: 0.228
[24,     1] loss: 0.179
[25,     1] loss: 0.143
[26,     1] loss: 0.170
[27,     1] loss: 0.220
[28,     1] loss: 0.149
[29,     1] loss: 0.159
[30,     1] loss: 0.126
[31,     1] loss: 0.184
[32,     1] loss: 0.119
[33,     1] loss: 0.171
[34,     1] loss: 0.147
[35,     1] loss: 0.123
[36,     1] loss: 0.199
[37,     1] loss: 0.119
[38,     1] loss: 0.196
[39,     1] loss: 0.129
[40,     1] loss: 0.161
[41,     1] loss: 0.155
[42,     1] loss: 0.184
[43,     1] loss: 0.156
[44,     1] loss: 0.104
[45,     1] loss: 0.127
[46,     1] loss: 0.122
[47,     1] loss: 0.140
[48,     1] loss: 0.171
[49,     1] loss: 0.127
[50,     1] loss: 0.077
[51,     1] loss: 0.086
[52,     1] loss: 0.057
[53,     1] loss: 0.138
[54,     1] loss: 0.073
[55,     1] loss: 0.076
[56,     1] loss: 0.074
[57,     1] loss: 0.063
[58,     1] loss: 0.090
[59,     1] loss: 0.075
[60,     1] loss: 0.062
[61,     1] loss: 0.061
Early stopping applied (best metric=0.2593386769294739)
Finished Training
Total time taken: 6.705998659133911
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.690
[3,     1] loss: 0.676
[4,     1] loss: 0.659
[5,     1] loss: 0.631
[6,     1] loss: 0.585
[7,     1] loss: 0.555
[8,     1] loss: 0.523
[9,     1] loss: 0.455
[10,     1] loss: 0.443
[11,     1] loss: 0.457
[12,     1] loss: 0.394
[13,     1] loss: 0.355
[14,     1] loss: 0.361
[15,     1] loss: 0.367
[16,     1] loss: 0.397
[17,     1] loss: 0.364
[18,     1] loss: 0.334
[19,     1] loss: 0.318
[20,     1] loss: 0.354
[21,     1] loss: 0.287
[22,     1] loss: 0.265
[23,     1] loss: 0.283
[24,     1] loss: 0.267
[25,     1] loss: 0.234
[26,     1] loss: 0.203
[27,     1] loss: 0.187
[28,     1] loss: 0.167
[29,     1] loss: 0.230
[30,     1] loss: 0.190
[31,     1] loss: 0.213
[32,     1] loss: 0.103
[33,     1] loss: 0.126
[34,     1] loss: 0.226
[35,     1] loss: 0.168
[36,     1] loss: 0.248
[37,     1] loss: 0.149
[38,     1] loss: 0.369
[39,     1] loss: 0.174
[40,     1] loss: 0.235
[41,     1] loss: 0.201
[42,     1] loss: 0.133
[43,     1] loss: 0.216
[44,     1] loss: 0.174
[45,     1] loss: 0.130
[46,     1] loss: 0.148
[47,     1] loss: 0.169
[48,     1] loss: 0.150
[49,     1] loss: 0.121
[50,     1] loss: 0.113
[51,     1] loss: 0.157
[52,     1] loss: 0.102
[53,     1] loss: 0.121
[54,     1] loss: 0.084
[55,     1] loss: 0.071
[56,     1] loss: 0.149
[57,     1] loss: 0.094
[58,     1] loss: 0.145
[59,     1] loss: 0.085
[60,     1] loss: 0.158
[61,     1] loss: 0.063
[62,     1] loss: 0.096
[63,     1] loss: 0.099
[64,     1] loss: 0.086
[65,     1] loss: 0.145
[66,     1] loss: 0.074
Early stopping applied (best metric=0.38100922107696533)
Finished Training
Total time taken: 7.219614028930664
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.709
[3,     1] loss: 0.684
[4,     1] loss: 0.676
[5,     1] loss: 0.662
[6,     1] loss: 0.643
[7,     1] loss: 0.603
[8,     1] loss: 0.592
[9,     1] loss: 0.545
[10,     1] loss: 0.456
[11,     1] loss: 0.457
[12,     1] loss: 0.408
[13,     1] loss: 0.388
[14,     1] loss: 0.351
[15,     1] loss: 0.361
[16,     1] loss: 0.327
[17,     1] loss: 0.286
[18,     1] loss: 0.290
[19,     1] loss: 0.278
[20,     1] loss: 0.314
[21,     1] loss: 0.287
[22,     1] loss: 0.240
[23,     1] loss: 0.249
[24,     1] loss: 0.167
[25,     1] loss: 0.282
[26,     1] loss: 0.227
[27,     1] loss: 0.198
[28,     1] loss: 0.328
[29,     1] loss: 0.221
[30,     1] loss: 0.222
[31,     1] loss: 0.280
[32,     1] loss: 0.207
[33,     1] loss: 0.239
[34,     1] loss: 0.231
[35,     1] loss: 0.157
[36,     1] loss: 0.247
[37,     1] loss: 0.254
[38,     1] loss: 0.211
[39,     1] loss: 0.167
[40,     1] loss: 0.195
[41,     1] loss: 0.119
[42,     1] loss: 0.123
[43,     1] loss: 0.195
[44,     1] loss: 0.176
[45,     1] loss: 0.135
[46,     1] loss: 0.127
[47,     1] loss: 0.180
[48,     1] loss: 0.140
[49,     1] loss: 0.093
[50,     1] loss: 0.140
[51,     1] loss: 0.085
[52,     1] loss: 0.077
[53,     1] loss: 0.092
[54,     1] loss: 0.081
[55,     1] loss: 0.096
[56,     1] loss: 0.061
[57,     1] loss: 0.059
[58,     1] loss: 0.181
[59,     1] loss: 0.131
[60,     1] loss: 0.143
[61,     1] loss: 0.136
[62,     1] loss: 0.060
[63,     1] loss: 0.250
[64,     1] loss: 0.071
[65,     1] loss: 0.178
[66,     1] loss: 0.173
[67,     1] loss: 0.179
[68,     1] loss: 0.122
[69,     1] loss: 0.174
[70,     1] loss: 0.178
[71,     1] loss: 0.159
[72,     1] loss: 0.119
[73,     1] loss: 0.124
[74,     1] loss: 0.138
[75,     1] loss: 0.107
[76,     1] loss: 0.103
[77,     1] loss: 0.086
[78,     1] loss: 0.071
[79,     1] loss: 0.087
[80,     1] loss: 0.092
[81,     1] loss: 0.092
[82,     1] loss: 0.083
[83,     1] loss: 0.071
[84,     1] loss: 0.160
[85,     1] loss: 0.222
[86,     1] loss: 0.279
[87,     1] loss: 0.155
[88,     1] loss: 0.088
[89,     1] loss: 0.157
[90,     1] loss: 0.143
[91,     1] loss: 0.128
Early stopping applied (best metric=0.14110791683197021)
Finished Training
Total time taken: 9.945232629776001
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.689
[2,     1] loss: 0.700
[3,     1] loss: 0.683
[4,     1] loss: 0.678
[5,     1] loss: 0.642
[6,     1] loss: 0.613
[7,     1] loss: 0.567
[8,     1] loss: 0.536
[9,     1] loss: 0.519
[10,     1] loss: 0.435
[11,     1] loss: 0.421
[12,     1] loss: 0.417
[13,     1] loss: 0.350
[14,     1] loss: 0.336
[15,     1] loss: 0.371
[16,     1] loss: 0.297
[17,     1] loss: 0.386
[18,     1] loss: 0.396
[19,     1] loss: 0.394
[20,     1] loss: 0.259
[21,     1] loss: 0.297
[22,     1] loss: 0.293
[23,     1] loss: 0.311
[24,     1] loss: 0.268
[25,     1] loss: 0.269
[26,     1] loss: 0.351
[27,     1] loss: 0.258
[28,     1] loss: 0.258
[29,     1] loss: 0.211
[30,     1] loss: 0.267
[31,     1] loss: 0.238
[32,     1] loss: 0.171
[33,     1] loss: 0.175
[34,     1] loss: 0.202
[35,     1] loss: 0.167
[36,     1] loss: 0.145
[37,     1] loss: 0.213
[38,     1] loss: 0.216
[39,     1] loss: 0.270
[40,     1] loss: 0.255
[41,     1] loss: 0.236
[42,     1] loss: 0.133
[43,     1] loss: 0.223
[44,     1] loss: 0.191
[45,     1] loss: 0.161
[46,     1] loss: 0.162
[47,     1] loss: 0.211
[48,     1] loss: 0.173
[49,     1] loss: 0.150
[50,     1] loss: 0.161
[51,     1] loss: 0.114
[52,     1] loss: 0.135
[53,     1] loss: 0.214
[54,     1] loss: 0.126
[55,     1] loss: 0.104
[56,     1] loss: 0.137
[57,     1] loss: 0.046
[58,     1] loss: 0.143
[59,     1] loss: 0.175
[60,     1] loss: 0.221
[61,     1] loss: 0.122
[62,     1] loss: 0.133
[63,     1] loss: 0.129
[64,     1] loss: 0.071
[65,     1] loss: 0.107
[66,     1] loss: 0.132
[67,     1] loss: 0.101
[68,     1] loss: 0.075
[69,     1] loss: 0.072
[70,     1] loss: 0.109
[71,     1] loss: 0.080
[72,     1] loss: 0.069
[73,     1] loss: 0.066
[74,     1] loss: 0.070
[75,     1] loss: 0.075
[76,     1] loss: 0.063
[77,     1] loss: 0.105
Early stopping applied (best metric=0.322435200214386)
Finished Training
Total time taken: 8.389002799987793
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.696
[3,     1] loss: 0.683
[4,     1] loss: 0.667
[5,     1] loss: 0.647
[6,     1] loss: 0.614
[7,     1] loss: 0.594
[8,     1] loss: 0.537
[9,     1] loss: 0.490
[10,     1] loss: 0.452
[11,     1] loss: 0.420
[12,     1] loss: 0.366
[13,     1] loss: 0.410
[14,     1] loss: 0.335
[15,     1] loss: 0.321
[16,     1] loss: 0.259
[17,     1] loss: 0.269
[18,     1] loss: 0.309
[19,     1] loss: 0.278
[20,     1] loss: 0.256
[21,     1] loss: 0.340
[22,     1] loss: 0.257
[23,     1] loss: 0.253
[24,     1] loss: 0.265
[25,     1] loss: 0.221
[26,     1] loss: 0.211
[27,     1] loss: 0.191
[28,     1] loss: 0.216
[29,     1] loss: 0.206
[30,     1] loss: 0.170
[31,     1] loss: 0.179
[32,     1] loss: 0.171
[33,     1] loss: 0.160
[34,     1] loss: 0.150
[35,     1] loss: 0.163
[36,     1] loss: 0.166
[37,     1] loss: 0.118
[38,     1] loss: 0.144
[39,     1] loss: 0.139
[40,     1] loss: 0.133
[41,     1] loss: 0.114
[42,     1] loss: 0.130
[43,     1] loss: 0.075
[44,     1] loss: 0.055
[45,     1] loss: 0.118
[46,     1] loss: 0.117
[47,     1] loss: 0.232
[48,     1] loss: 0.090
[49,     1] loss: 0.209
[50,     1] loss: 0.107
[51,     1] loss: 0.127
[52,     1] loss: 0.142
[53,     1] loss: 0.135
[54,     1] loss: 0.132
[55,     1] loss: 0.134
[56,     1] loss: 0.103
[57,     1] loss: 0.110
[58,     1] loss: 0.096
[59,     1] loss: 0.093
[60,     1] loss: 0.099
[61,     1] loss: 0.100
[62,     1] loss: 0.062
[63,     1] loss: 0.192
[64,     1] loss: 0.156
[65,     1] loss: 0.136
[66,     1] loss: 0.079
[67,     1] loss: 0.077
[68,     1] loss: 0.150
[69,     1] loss: 0.065
[70,     1] loss: 0.130
[71,     1] loss: 0.103
[72,     1] loss: 0.083
[73,     1] loss: 0.093
[74,     1] loss: 0.084
[75,     1] loss: 0.076
[76,     1] loss: 0.078
Early stopping applied (best metric=0.1679566353559494)
Finished Training
Total time taken: 8.322001457214355
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.697
[3,     1] loss: 0.686
[4,     1] loss: 0.679
[5,     1] loss: 0.653
[6,     1] loss: 0.618
[7,     1] loss: 0.578
[8,     1] loss: 0.546
[9,     1] loss: 0.479
[10,     1] loss: 0.447
[11,     1] loss: 0.331
[12,     1] loss: 0.337
[13,     1] loss: 0.278
[14,     1] loss: 0.252
[15,     1] loss: 0.297
[16,     1] loss: 0.238
[17,     1] loss: 0.225
[18,     1] loss: 0.203
[19,     1] loss: 0.235
[20,     1] loss: 0.289
[21,     1] loss: 0.224
[22,     1] loss: 0.219
[23,     1] loss: 0.204
[24,     1] loss: 0.237
[25,     1] loss: 0.268
[26,     1] loss: 0.206
[27,     1] loss: 0.249
[28,     1] loss: 0.231
[29,     1] loss: 0.208
[30,     1] loss: 0.210
[31,     1] loss: 0.216
[32,     1] loss: 0.220
[33,     1] loss: 0.183
[34,     1] loss: 0.151
[35,     1] loss: 0.199
[36,     1] loss: 0.149
[37,     1] loss: 0.180
[38,     1] loss: 0.152
[39,     1] loss: 0.231
[40,     1] loss: 0.242
[41,     1] loss: 0.300
[42,     1] loss: 0.177
[43,     1] loss: 0.160
[44,     1] loss: 0.169
[45,     1] loss: 0.185
[46,     1] loss: 0.135
[47,     1] loss: 0.110
[48,     1] loss: 0.245
[49,     1] loss: 0.117
[50,     1] loss: 0.127
[51,     1] loss: 0.149
[52,     1] loss: 0.090
[53,     1] loss: 0.125
[54,     1] loss: 0.069
[55,     1] loss: 0.100
[56,     1] loss: 0.106
[57,     1] loss: 0.096
[58,     1] loss: 0.068
[59,     1] loss: 0.059
[60,     1] loss: 0.096
[61,     1] loss: 0.221
[62,     1] loss: 0.113
[63,     1] loss: 0.097
[64,     1] loss: 0.083
[65,     1] loss: 0.130
[66,     1] loss: 0.138
[67,     1] loss: 0.117
[68,     1] loss: 0.112
[69,     1] loss: 0.090
[70,     1] loss: 0.092
[71,     1] loss: 0.065
[72,     1] loss: 0.073
[73,     1] loss: 0.068
[74,     1] loss: 0.066
[75,     1] loss: 0.043
[76,     1] loss: 0.049
[77,     1] loss: 0.075
[78,     1] loss: 0.062
[79,     1] loss: 0.035
[80,     1] loss: 0.050
[81,     1] loss: 0.146
[82,     1] loss: 0.276
[83,     1] loss: 0.209
[84,     1] loss: 0.234
[85,     1] loss: 0.215
[86,     1] loss: 0.173
[87,     1] loss: 0.102
[88,     1] loss: 0.229
[89,     1] loss: 0.100
[90,     1] loss: 0.129
[91,     1] loss: 0.138
[92,     1] loss: 0.121
Early stopping applied (best metric=0.1918766349554062)
Finished Training
Total time taken: 10.09399938583374
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.708
[3,     1] loss: 0.679
[4,     1] loss: 0.640
[5,     1] loss: 0.596
[6,     1] loss: 0.558
[7,     1] loss: 0.479
[8,     1] loss: 0.447
[9,     1] loss: 0.379
[10,     1] loss: 0.456
[11,     1] loss: 0.358
[12,     1] loss: 0.360
[13,     1] loss: 0.282
[14,     1] loss: 0.307
[15,     1] loss: 0.327
[16,     1] loss: 0.278
[17,     1] loss: 0.284
[18,     1] loss: 0.224
[19,     1] loss: 0.291
[20,     1] loss: 0.265
[21,     1] loss: 0.260
[22,     1] loss: 0.187
[23,     1] loss: 0.222
[24,     1] loss: 0.210
[25,     1] loss: 0.169
[26,     1] loss: 0.185
[27,     1] loss: 0.193
[28,     1] loss: 0.172
[29,     1] loss: 0.184
[30,     1] loss: 0.225
[31,     1] loss: 0.123
[32,     1] loss: 0.138
[33,     1] loss: 0.180
[34,     1] loss: 0.117
[35,     1] loss: 0.191
[36,     1] loss: 0.137
[37,     1] loss: 0.209
[38,     1] loss: 0.193
[39,     1] loss: 0.156
[40,     1] loss: 0.140
[41,     1] loss: 0.183
[42,     1] loss: 0.128
[43,     1] loss: 0.157
[44,     1] loss: 0.168
[45,     1] loss: 0.125
[46,     1] loss: 0.126
[47,     1] loss: 0.104
[48,     1] loss: 0.096
[49,     1] loss: 0.093
[50,     1] loss: 0.094
[51,     1] loss: 0.164
[52,     1] loss: 0.183
[53,     1] loss: 0.076
[54,     1] loss: 0.118
[55,     1] loss: 0.073
[56,     1] loss: 0.048
Early stopping applied (best metric=0.4823962152004242)
Finished Training
Total time taken: 6.205610513687134
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.693
[3,     1] loss: 0.691
[4,     1] loss: 0.686
[5,     1] loss: 0.667
[6,     1] loss: 0.647
[7,     1] loss: 0.606
[8,     1] loss: 0.578
[9,     1] loss: 0.500
[10,     1] loss: 0.481
[11,     1] loss: 0.431
[12,     1] loss: 0.429
[13,     1] loss: 0.392
[14,     1] loss: 0.353
[15,     1] loss: 0.376
[16,     1] loss: 0.372
[17,     1] loss: 0.328
[18,     1] loss: 0.376
[19,     1] loss: 0.391
[20,     1] loss: 0.324
[21,     1] loss: 0.328
[22,     1] loss: 0.288
[23,     1] loss: 0.275
[24,     1] loss: 0.276
[25,     1] loss: 0.261
[26,     1] loss: 0.220
[27,     1] loss: 0.212
[28,     1] loss: 0.295
[29,     1] loss: 0.154
[30,     1] loss: 0.187
[31,     1] loss: 0.138
[32,     1] loss: 0.145
[33,     1] loss: 0.191
[34,     1] loss: 0.177
[35,     1] loss: 0.132
[36,     1] loss: 0.176
[37,     1] loss: 0.098
[38,     1] loss: 0.161
[39,     1] loss: 0.147
[40,     1] loss: 0.122
[41,     1] loss: 0.096
[42,     1] loss: 0.144
[43,     1] loss: 0.296
[44,     1] loss: 0.117
[45,     1] loss: 0.145
[46,     1] loss: 0.218
[47,     1] loss: 0.119
[48,     1] loss: 0.103
[49,     1] loss: 0.111
[50,     1] loss: 0.121
[51,     1] loss: 0.124
[52,     1] loss: 0.103
[53,     1] loss: 0.096
[54,     1] loss: 0.093
[55,     1] loss: 0.111
[56,     1] loss: 0.112
[57,     1] loss: 0.077
[58,     1] loss: 0.127
[59,     1] loss: 0.070
[60,     1] loss: 0.085
[61,     1] loss: 0.085
[62,     1] loss: 0.131
[63,     1] loss: 0.103
[64,     1] loss: 0.081
[65,     1] loss: 0.058
Early stopping applied (best metric=0.2383149415254593)
Finished Training
Total time taken: 7.150999307632446
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.694
[3,     1] loss: 0.682
[4,     1] loss: 0.663
[5,     1] loss: 0.616
[6,     1] loss: 0.558
[7,     1] loss: 0.506
[8,     1] loss: 0.463
[9,     1] loss: 0.437
[10,     1] loss: 0.379
[11,     1] loss: 0.302
[12,     1] loss: 0.281
[13,     1] loss: 0.244
[14,     1] loss: 0.260
[15,     1] loss: 0.237
[16,     1] loss: 0.303
[17,     1] loss: 0.253
[18,     1] loss: 0.203
[19,     1] loss: 0.213
[20,     1] loss: 0.187
[21,     1] loss: 0.145
[22,     1] loss: 0.168
[23,     1] loss: 0.170
[24,     1] loss: 0.163
[25,     1] loss: 0.159
[26,     1] loss: 0.178
[27,     1] loss: 0.198
[28,     1] loss: 0.185
[29,     1] loss: 0.172
[30,     1] loss: 0.187
[31,     1] loss: 0.160
[32,     1] loss: 0.143
[33,     1] loss: 0.152
[34,     1] loss: 0.126
[35,     1] loss: 0.147
[36,     1] loss: 0.140
[37,     1] loss: 0.161
[38,     1] loss: 0.162
[39,     1] loss: 0.175
[40,     1] loss: 0.174
[41,     1] loss: 0.114
[42,     1] loss: 0.120
[43,     1] loss: 0.167
[44,     1] loss: 0.118
[45,     1] loss: 0.103
[46,     1] loss: 0.100
[47,     1] loss: 0.163
[48,     1] loss: 0.113
[49,     1] loss: 0.085
[50,     1] loss: 0.140
[51,     1] loss: 0.119
[52,     1] loss: 0.138
[53,     1] loss: 0.143
[54,     1] loss: 0.138
[55,     1] loss: 0.185
[56,     1] loss: 0.083
[57,     1] loss: 0.115
[58,     1] loss: 0.122
[59,     1] loss: 0.105
[60,     1] loss: 0.072
[61,     1] loss: 0.124
[62,     1] loss: 0.058
[63,     1] loss: 0.092
[64,     1] loss: 0.193
[65,     1] loss: 0.068
[66,     1] loss: 0.264
[67,     1] loss: 0.109
[68,     1] loss: 0.220
[69,     1] loss: 0.131
[70,     1] loss: 0.143
[71,     1] loss: 0.155
[72,     1] loss: 0.150
[73,     1] loss: 0.137
[74,     1] loss: 0.194
[75,     1] loss: 0.129
[76,     1] loss: 0.095
[77,     1] loss: 0.092
[78,     1] loss: 0.080
[79,     1] loss: 0.094
[80,     1] loss: 0.066
[81,     1] loss: 0.083
[82,     1] loss: 0.100
[83,     1] loss: 0.059
[84,     1] loss: 0.060
[85,     1] loss: 0.070
[86,     1] loss: 0.084
[87,     1] loss: 0.106
[88,     1] loss: 0.101
Early stopping applied (best metric=0.21124260127544403)
Finished Training
Total time taken: 9.69100022315979
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.699
[3,     1] loss: 0.692
[4,     1] loss: 0.679
[5,     1] loss: 0.667
[6,     1] loss: 0.641
[7,     1] loss: 0.614
[8,     1] loss: 0.577
[9,     1] loss: 0.518
[10,     1] loss: 0.490
[11,     1] loss: 0.418
[12,     1] loss: 0.401
[13,     1] loss: 0.349
[14,     1] loss: 0.327
[15,     1] loss: 0.247
[16,     1] loss: 0.260
[17,     1] loss: 0.247
[18,     1] loss: 0.223
[19,     1] loss: 0.202
[20,     1] loss: 0.216
[21,     1] loss: 0.231
[22,     1] loss: 0.176
[23,     1] loss: 0.205
[24,     1] loss: 0.157
[25,     1] loss: 0.167
[26,     1] loss: 0.168
[27,     1] loss: 0.131
[28,     1] loss: 0.176
[29,     1] loss: 0.152
[30,     1] loss: 0.134
[31,     1] loss: 0.135
[32,     1] loss: 0.151
[33,     1] loss: 0.128
[34,     1] loss: 0.198
[35,     1] loss: 0.106
[36,     1] loss: 0.163
[37,     1] loss: 0.125
[38,     1] loss: 0.233
[39,     1] loss: 0.092
[40,     1] loss: 0.127
[41,     1] loss: 0.136
[42,     1] loss: 0.180
[43,     1] loss: 0.151
[44,     1] loss: 0.157
[45,     1] loss: 0.173
[46,     1] loss: 0.136
[47,     1] loss: 0.241
[48,     1] loss: 0.170
[49,     1] loss: 0.118
[50,     1] loss: 0.165
[51,     1] loss: 0.099
[52,     1] loss: 0.094
[53,     1] loss: 0.147
[54,     1] loss: 0.133
[55,     1] loss: 0.089
[56,     1] loss: 0.156
[57,     1] loss: 0.082
[58,     1] loss: 0.044
[59,     1] loss: 0.102
[60,     1] loss: 0.076
[61,     1] loss: 0.101
Early stopping applied (best metric=0.36530041694641113)
Finished Training
Total time taken: 6.752001762390137
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.702
[3,     1] loss: 0.683
[4,     1] loss: 0.677
[5,     1] loss: 0.664
[6,     1] loss: 0.643
[7,     1] loss: 0.612
[8,     1] loss: 0.567
[9,     1] loss: 0.559
[10,     1] loss: 0.540
[11,     1] loss: 0.479
[12,     1] loss: 0.480
[13,     1] loss: 0.482
[14,     1] loss: 0.450
[15,     1] loss: 0.437
[16,     1] loss: 0.403
[17,     1] loss: 0.357
[18,     1] loss: 0.375
[19,     1] loss: 0.390
[20,     1] loss: 0.332
[21,     1] loss: 0.433
[22,     1] loss: 0.384
[23,     1] loss: 0.280
[24,     1] loss: 0.304
[25,     1] loss: 0.277
[26,     1] loss: 0.300
[27,     1] loss: 0.293
[28,     1] loss: 0.278
[29,     1] loss: 0.205
[30,     1] loss: 0.271
[31,     1] loss: 0.257
[32,     1] loss: 0.267
[33,     1] loss: 0.216
[34,     1] loss: 0.166
[35,     1] loss: 0.140
[36,     1] loss: 0.151
[37,     1] loss: 0.103
[38,     1] loss: 0.175
[39,     1] loss: 0.141
[40,     1] loss: 0.166
[41,     1] loss: 0.154
[42,     1] loss: 0.101
[43,     1] loss: 0.109
[44,     1] loss: 0.131
[45,     1] loss: 0.101
[46,     1] loss: 0.120
[47,     1] loss: 0.065
[48,     1] loss: 0.129
[49,     1] loss: 0.093
[50,     1] loss: 0.091
[51,     1] loss: 0.115
[52,     1] loss: 0.094
[53,     1] loss: 0.084
[54,     1] loss: 0.141
[55,     1] loss: 0.070
[56,     1] loss: 0.069
[57,     1] loss: 0.069
[58,     1] loss: 0.072
Early stopping applied (best metric=0.4990963935852051)
Finished Training
Total time taken: 6.425000190734863
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.692
[3,     1] loss: 0.668
[4,     1] loss: 0.625
[5,     1] loss: 0.565
[6,     1] loss: 0.545
[7,     1] loss: 0.515
[8,     1] loss: 0.423
[9,     1] loss: 0.381
[10,     1] loss: 0.375
[11,     1] loss: 0.365
[12,     1] loss: 0.350
[13,     1] loss: 0.258
[14,     1] loss: 0.335
[15,     1] loss: 0.317
[16,     1] loss: 0.247
[17,     1] loss: 0.245
[18,     1] loss: 0.340
[19,     1] loss: 0.243
[20,     1] loss: 0.257
[21,     1] loss: 0.223
[22,     1] loss: 0.281
[23,     1] loss: 0.310
[24,     1] loss: 0.236
[25,     1] loss: 0.215
[26,     1] loss: 0.231
[27,     1] loss: 0.189
[28,     1] loss: 0.219
[29,     1] loss: 0.180
[30,     1] loss: 0.201
[31,     1] loss: 0.209
[32,     1] loss: 0.228
[33,     1] loss: 0.221
[34,     1] loss: 0.169
[35,     1] loss: 0.209
[36,     1] loss: 0.230
[37,     1] loss: 0.193
[38,     1] loss: 0.151
[39,     1] loss: 0.131
[40,     1] loss: 0.225
[41,     1] loss: 0.160
[42,     1] loss: 0.132
[43,     1] loss: 0.162
[44,     1] loss: 0.126
[45,     1] loss: 0.127
[46,     1] loss: 0.157
[47,     1] loss: 0.142
[48,     1] loss: 0.112
[49,     1] loss: 0.102
[50,     1] loss: 0.170
[51,     1] loss: 0.134
[52,     1] loss: 0.178
[53,     1] loss: 0.141
[54,     1] loss: 0.070
[55,     1] loss: 0.134
[56,     1] loss: 0.084
[57,     1] loss: 0.069
[58,     1] loss: 0.095
Early stopping applied (best metric=0.42040154337882996)
Finished Training
Total time taken: 6.366999626159668
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.659
[4,     1] loss: 0.604
[5,     1] loss: 0.540
[6,     1] loss: 0.499
[7,     1] loss: 0.430
[8,     1] loss: 0.390
[9,     1] loss: 0.318
[10,     1] loss: 0.307
[11,     1] loss: 0.323
[12,     1] loss: 0.267
[13,     1] loss: 0.261
[14,     1] loss: 0.193
[15,     1] loss: 0.199
[16,     1] loss: 0.222
[17,     1] loss: 0.269
[18,     1] loss: 0.263
[19,     1] loss: 0.180
[20,     1] loss: 0.306
[21,     1] loss: 0.215
[22,     1] loss: 0.194
[23,     1] loss: 0.205
[24,     1] loss: 0.216
[25,     1] loss: 0.294
[26,     1] loss: 0.163
[27,     1] loss: 0.205
[28,     1] loss: 0.150
[29,     1] loss: 0.185
[30,     1] loss: 0.147
[31,     1] loss: 0.116
[32,     1] loss: 0.129
[33,     1] loss: 0.152
[34,     1] loss: 0.079
[35,     1] loss: 0.123
[36,     1] loss: 0.097
[37,     1] loss: 0.143
[38,     1] loss: 0.088
[39,     1] loss: 0.130
[40,     1] loss: 0.091
[41,     1] loss: 0.071
[42,     1] loss: 0.173
[43,     1] loss: 0.182
[44,     1] loss: 0.115
[45,     1] loss: 0.168
[46,     1] loss: 0.156
[47,     1] loss: 0.170
[48,     1] loss: 0.192
[49,     1] loss: 0.200
[50,     1] loss: 0.134
[51,     1] loss: 0.125
[52,     1] loss: 0.139
[53,     1] loss: 0.169
[54,     1] loss: 0.127
[55,     1] loss: 0.105
[56,     1] loss: 0.089
[57,     1] loss: 0.123
[58,     1] loss: 0.076
[59,     1] loss: 0.082
[60,     1] loss: 0.227
Early stopping applied (best metric=0.4303019046783447)
Finished Training
Total time taken: 6.5856032371521
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.672
[4,     1] loss: 0.637
[5,     1] loss: 0.583
[6,     1] loss: 0.555
[7,     1] loss: 0.478
[8,     1] loss: 0.424
[9,     1] loss: 0.450
[10,     1] loss: 0.434
[11,     1] loss: 0.410
[12,     1] loss: 0.338
[13,     1] loss: 0.327
[14,     1] loss: 0.415
[15,     1] loss: 0.345
[16,     1] loss: 0.325
[17,     1] loss: 0.348
[18,     1] loss: 0.238
[19,     1] loss: 0.271
[20,     1] loss: 0.227
[21,     1] loss: 0.278
[22,     1] loss: 0.238
[23,     1] loss: 0.242
[24,     1] loss: 0.237
[25,     1] loss: 0.271
[26,     1] loss: 0.239
[27,     1] loss: 0.145
[28,     1] loss: 0.220
[29,     1] loss: 0.174
[30,     1] loss: 0.209
[31,     1] loss: 0.183
[32,     1] loss: 0.146
[33,     1] loss: 0.192
[34,     1] loss: 0.146
[35,     1] loss: 0.140
[36,     1] loss: 0.173
[37,     1] loss: 0.182
[38,     1] loss: 0.193
[39,     1] loss: 0.108
[40,     1] loss: 0.208
[41,     1] loss: 0.196
[42,     1] loss: 0.135
[43,     1] loss: 0.113
[44,     1] loss: 0.118
[45,     1] loss: 0.140
[46,     1] loss: 0.134
[47,     1] loss: 0.130
[48,     1] loss: 0.177
[49,     1] loss: 0.188
[50,     1] loss: 0.112
[51,     1] loss: 0.121
[52,     1] loss: 0.213
[53,     1] loss: 0.120
[54,     1] loss: 0.139
[55,     1] loss: 0.169
[56,     1] loss: 0.136
[57,     1] loss: 0.189
[58,     1] loss: 0.150
[59,     1] loss: 0.120
[60,     1] loss: 0.101
[61,     1] loss: 0.100
[62,     1] loss: 0.112
Early stopping applied (best metric=0.34086304903030396)
Finished Training
Total time taken: 6.802002906799316
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.692
[3,     1] loss: 0.667
[4,     1] loss: 0.654
[5,     1] loss: 0.607
[6,     1] loss: 0.545
[7,     1] loss: 0.499
[8,     1] loss: 0.462
[9,     1] loss: 0.425
[10,     1] loss: 0.391
[11,     1] loss: 0.377
[12,     1] loss: 0.378
[13,     1] loss: 0.330
[14,     1] loss: 0.323
[15,     1] loss: 0.372
[16,     1] loss: 0.232
[17,     1] loss: 0.234
[18,     1] loss: 0.240
[19,     1] loss: 0.276
[20,     1] loss: 0.285
[21,     1] loss: 0.250
[22,     1] loss: 0.268
[23,     1] loss: 0.218
[24,     1] loss: 0.186
[25,     1] loss: 0.219
[26,     1] loss: 0.205
[27,     1] loss: 0.168
[28,     1] loss: 0.186
[29,     1] loss: 0.197
[30,     1] loss: 0.183
[31,     1] loss: 0.159
[32,     1] loss: 0.184
[33,     1] loss: 0.138
[34,     1] loss: 0.130
[35,     1] loss: 0.087
[36,     1] loss: 0.120
[37,     1] loss: 0.096
[38,     1] loss: 0.107
[39,     1] loss: 0.102
[40,     1] loss: 0.106
[41,     1] loss: 0.095
[42,     1] loss: 0.128
[43,     1] loss: 0.100
[44,     1] loss: 0.079
[45,     1] loss: 0.215
[46,     1] loss: 0.158
[47,     1] loss: 0.171
[48,     1] loss: 0.166
[49,     1] loss: 0.098
[50,     1] loss: 0.129
[51,     1] loss: 0.142
[52,     1] loss: 0.108
[53,     1] loss: 0.119
[54,     1] loss: 0.101
[55,     1] loss: 0.132
[56,     1] loss: 0.107
[57,     1] loss: 0.081
[58,     1] loss: 0.082
[59,     1] loss: 0.058
[60,     1] loss: 0.047
[61,     1] loss: 0.066
[62,     1] loss: 0.075
[63,     1] loss: 0.058
[64,     1] loss: 0.089
[65,     1] loss: 0.147
[66,     1] loss: 0.088
[67,     1] loss: 0.173
[68,     1] loss: 0.066
[69,     1] loss: 0.198
[70,     1] loss: 0.154
[71,     1] loss: 0.067
[72,     1] loss: 0.180
[73,     1] loss: 0.095
[74,     1] loss: 0.123
[75,     1] loss: 0.119
[76,     1] loss: 0.150
[77,     1] loss: 0.088
[78,     1] loss: 0.077
[79,     1] loss: 0.078
[80,     1] loss: 0.114
[81,     1] loss: 0.107
[82,     1] loss: 0.079
[83,     1] loss: 0.143
Early stopping applied (best metric=0.12419748306274414)
Finished Training
Total time taken: 9.095999240875244
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.661
[4,     1] loss: 0.632
[5,     1] loss: 0.573
[6,     1] loss: 0.571
[7,     1] loss: 0.503
[8,     1] loss: 0.495
[9,     1] loss: 0.421
[10,     1] loss: 0.407
[11,     1] loss: 0.392
[12,     1] loss: 0.346
[13,     1] loss: 0.307
[14,     1] loss: 0.375
[15,     1] loss: 0.312
[16,     1] loss: 0.381
[17,     1] loss: 0.319
[18,     1] loss: 0.285
[19,     1] loss: 0.322
[20,     1] loss: 0.293
[21,     1] loss: 0.295
[22,     1] loss: 0.333
[23,     1] loss: 0.272
[24,     1] loss: 0.334
[25,     1] loss: 0.271
[26,     1] loss: 0.221
[27,     1] loss: 0.206
[28,     1] loss: 0.230
[29,     1] loss: 0.221
[30,     1] loss: 0.184
[31,     1] loss: 0.188
[32,     1] loss: 0.205
[33,     1] loss: 0.248
[34,     1] loss: 0.161
[35,     1] loss: 0.208
[36,     1] loss: 0.215
[37,     1] loss: 0.204
[38,     1] loss: 0.178
[39,     1] loss: 0.184
[40,     1] loss: 0.188
[41,     1] loss: 0.213
[42,     1] loss: 0.166
[43,     1] loss: 0.192
[44,     1] loss: 0.172
[45,     1] loss: 0.149
[46,     1] loss: 0.206
[47,     1] loss: 0.272
[48,     1] loss: 0.253
[49,     1] loss: 0.180
[50,     1] loss: 0.204
[51,     1] loss: 0.223
[52,     1] loss: 0.188
[53,     1] loss: 0.139
[54,     1] loss: 0.155
[55,     1] loss: 0.128
[56,     1] loss: 0.191
[57,     1] loss: 0.145
Early stopping applied (best metric=0.48985999822616577)
Finished Training
Total time taken: 6.26200008392334
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.670
[4,     1] loss: 0.621
[5,     1] loss: 0.597
[6,     1] loss: 0.534
[7,     1] loss: 0.483
[8,     1] loss: 0.404
[9,     1] loss: 0.401
[10,     1] loss: 0.396
[11,     1] loss: 0.327
[12,     1] loss: 0.375
[13,     1] loss: 0.388
[14,     1] loss: 0.335
[15,     1] loss: 0.362
[16,     1] loss: 0.255
[17,     1] loss: 0.261
[18,     1] loss: 0.264
[19,     1] loss: 0.234
[20,     1] loss: 0.278
[21,     1] loss: 0.222
[22,     1] loss: 0.230
[23,     1] loss: 0.225
[24,     1] loss: 0.208
[25,     1] loss: 0.221
[26,     1] loss: 0.186
[27,     1] loss: 0.200
[28,     1] loss: 0.242
[29,     1] loss: 0.144
[30,     1] loss: 0.160
[31,     1] loss: 0.150
[32,     1] loss: 0.114
[33,     1] loss: 0.119
[34,     1] loss: 0.157
[35,     1] loss: 0.174
[36,     1] loss: 0.113
[37,     1] loss: 0.119
[38,     1] loss: 0.130
[39,     1] loss: 0.079
[40,     1] loss: 0.147
[41,     1] loss: 0.114
[42,     1] loss: 0.223
[43,     1] loss: 0.102
[44,     1] loss: 0.182
[45,     1] loss: 0.147
[46,     1] loss: 0.127
[47,     1] loss: 0.138
[48,     1] loss: 0.112
[49,     1] loss: 0.128
[50,     1] loss: 0.110
[51,     1] loss: 0.124
[52,     1] loss: 0.143
[53,     1] loss: 0.083
[54,     1] loss: 0.174
[55,     1] loss: 0.089
[56,     1] loss: 0.105
[57,     1] loss: 0.094
[58,     1] loss: 0.109
[59,     1] loss: 0.080
[60,     1] loss: 0.074
[61,     1] loss: 0.056
[62,     1] loss: 0.161
[63,     1] loss: 0.086
[64,     1] loss: 0.062
Early stopping applied (best metric=0.2732899785041809)
Finished Training
Total time taken: 7.0299999713897705
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.700
[3,     1] loss: 0.688
[4,     1] loss: 0.680
[5,     1] loss: 0.662
[6,     1] loss: 0.637
[7,     1] loss: 0.619
[8,     1] loss: 0.557
[9,     1] loss: 0.564
[10,     1] loss: 0.528
[11,     1] loss: 0.500
[12,     1] loss: 0.490
[13,     1] loss: 0.444
[14,     1] loss: 0.393
[15,     1] loss: 0.342
[16,     1] loss: 0.320
[17,     1] loss: 0.376
[18,     1] loss: 0.392
[19,     1] loss: 0.298
[20,     1] loss: 0.314
[21,     1] loss: 0.291
[22,     1] loss: 0.373
[23,     1] loss: 0.269
[24,     1] loss: 0.322
[25,     1] loss: 0.305
[26,     1] loss: 0.272
[27,     1] loss: 0.304
[28,     1] loss: 0.219
[29,     1] loss: 0.244
[30,     1] loss: 0.258
[31,     1] loss: 0.191
[32,     1] loss: 0.229
[33,     1] loss: 0.246
[34,     1] loss: 0.165
[35,     1] loss: 0.197
[36,     1] loss: 0.137
[37,     1] loss: 0.189
[38,     1] loss: 0.176
[39,     1] loss: 0.155
[40,     1] loss: 0.230
[41,     1] loss: 0.140
[42,     1] loss: 0.232
[43,     1] loss: 0.122
[44,     1] loss: 0.132
[45,     1] loss: 0.164
[46,     1] loss: 0.103
[47,     1] loss: 0.108
[48,     1] loss: 0.204
[49,     1] loss: 0.118
[50,     1] loss: 0.166
[51,     1] loss: 0.107
[52,     1] loss: 0.092
[53,     1] loss: 0.120
[54,     1] loss: 0.067
[55,     1] loss: 0.072
[56,     1] loss: 0.088
[57,     1] loss: 0.126
[58,     1] loss: 0.082
[59,     1] loss: 0.070
[60,     1] loss: 0.070
[61,     1] loss: 0.082
[62,     1] loss: 0.081
[63,     1] loss: 0.117
[64,     1] loss: 0.049
[65,     1] loss: 0.158
[66,     1] loss: 0.145
[67,     1] loss: 0.134
[68,     1] loss: 0.166
[69,     1] loss: 0.078
[70,     1] loss: 0.208
[71,     1] loss: 0.115
[72,     1] loss: 0.082
[73,     1] loss: 0.137
[74,     1] loss: 0.150
[75,     1] loss: 0.145
[76,     1] loss: 0.163
[77,     1] loss: 0.077
[78,     1] loss: 0.131
[79,     1] loss: 0.079
[80,     1] loss: 0.075
[81,     1] loss: 0.071
[82,     1] loss: 0.149
[83,     1] loss: 0.067
[84,     1] loss: 0.086
[85,     1] loss: 0.059
[86,     1] loss: 0.057
[87,     1] loss: 0.112
[88,     1] loss: 0.052
[89,     1] loss: 0.074
[90,     1] loss: 0.067
[91,     1] loss: 0.118
[92,     1] loss: 0.073
[93,     1] loss: 0.063
[94,     1] loss: 0.094
[95,     1] loss: 0.040
[96,     1] loss: 0.104
[97,     1] loss: 0.092
[98,     1] loss: 0.058
[99,     1] loss: 0.061
[100,     1] loss: 0.074
[101,     1] loss: 0.084
[102,     1] loss: 0.062
[103,     1] loss: 0.050
[104,     1] loss: 0.056
[105,     1] loss: 0.059
[106,     1] loss: 0.120
[107,     1] loss: 0.056
[108,     1] loss: 0.112
[109,     1] loss: 0.057
[110,     1] loss: 0.052
[111,     1] loss: 0.100
[112,     1] loss: 0.087
[113,     1] loss: 0.067
[114,     1] loss: 0.133
[115,     1] loss: 0.074
[116,     1] loss: 0.108
[117,     1] loss: 0.065
[118,     1] loss: 0.148
[119,     1] loss: 0.062
[120,     1] loss: 0.090
[121,     1] loss: 0.063
[122,     1] loss: 0.093
[123,     1] loss: 0.045
[124,     1] loss: 0.099
[125,     1] loss: 0.055
[126,     1] loss: 0.090
Early stopping applied (best metric=0.1423603594303131)
Finished Training
Total time taken: 13.768610000610352
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.679
[4,     1] loss: 0.652
[5,     1] loss: 0.598
[6,     1] loss: 0.561
[7,     1] loss: 0.494
[8,     1] loss: 0.489
[9,     1] loss: 0.409
[10,     1] loss: 0.371
[11,     1] loss: 0.309
[12,     1] loss: 0.371
[13,     1] loss: 0.275
[14,     1] loss: 0.315
[15,     1] loss: 0.250
[16,     1] loss: 0.264
[17,     1] loss: 0.251
[18,     1] loss: 0.262
[19,     1] loss: 0.176
[20,     1] loss: 0.188
[21,     1] loss: 0.187
[22,     1] loss: 0.243
[23,     1] loss: 0.178
[24,     1] loss: 0.187
[25,     1] loss: 0.156
[26,     1] loss: 0.178
[27,     1] loss: 0.164
[28,     1] loss: 0.135
[29,     1] loss: 0.148
[30,     1] loss: 0.119
[31,     1] loss: 0.110
[32,     1] loss: 0.134
[33,     1] loss: 0.119
[34,     1] loss: 0.234
[35,     1] loss: 0.239
[36,     1] loss: 0.100
[37,     1] loss: 0.179
[38,     1] loss: 0.117
[39,     1] loss: 0.159
[40,     1] loss: 0.179
[41,     1] loss: 0.109
[42,     1] loss: 0.116
[43,     1] loss: 0.115
[44,     1] loss: 0.111
[45,     1] loss: 0.093
[46,     1] loss: 0.091
[47,     1] loss: 0.096
[48,     1] loss: 0.105
[49,     1] loss: 0.079
[50,     1] loss: 0.062
[51,     1] loss: 0.077
[52,     1] loss: 0.180
[53,     1] loss: 0.060
[54,     1] loss: 0.111
Early stopping applied (best metric=0.5384747982025146)
Finished Training
Total time taken: 5.958000421524048
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.703
[3,     1] loss: 0.694
[4,     1] loss: 0.673
[5,     1] loss: 0.641
[6,     1] loss: 0.608
[7,     1] loss: 0.570
[8,     1] loss: 0.527
[9,     1] loss: 0.479
[10,     1] loss: 0.403
[11,     1] loss: 0.422
[12,     1] loss: 0.286
[13,     1] loss: 0.303
[14,     1] loss: 0.258
[15,     1] loss: 0.229
[16,     1] loss: 0.421
[17,     1] loss: 0.266
[18,     1] loss: 0.279
[19,     1] loss: 0.365
[20,     1] loss: 0.330
[21,     1] loss: 0.249
[22,     1] loss: 0.287
[23,     1] loss: 0.265
[24,     1] loss: 0.213
[25,     1] loss: 0.254
[26,     1] loss: 0.246
[27,     1] loss: 0.259
[28,     1] loss: 0.216
[29,     1] loss: 0.204
[30,     1] loss: 0.181
[31,     1] loss: 0.195
[32,     1] loss: 0.196
[33,     1] loss: 0.164
[34,     1] loss: 0.258
[35,     1] loss: 0.162
[36,     1] loss: 0.132
[37,     1] loss: 0.125
[38,     1] loss: 0.201
[39,     1] loss: 0.132
[40,     1] loss: 0.177
[41,     1] loss: 0.139
[42,     1] loss: 0.113
[43,     1] loss: 0.142
[44,     1] loss: 0.107
[45,     1] loss: 0.095
[46,     1] loss: 0.142
[47,     1] loss: 0.141
[48,     1] loss: 0.114
[49,     1] loss: 0.096
[50,     1] loss: 0.123
[51,     1] loss: 0.135
[52,     1] loss: 0.086
[53,     1] loss: 0.130
[54,     1] loss: 0.098
[55,     1] loss: 0.202
[56,     1] loss: 0.160
[57,     1] loss: 0.107
[58,     1] loss: 0.150
[59,     1] loss: 0.106
[60,     1] loss: 0.128
[61,     1] loss: 0.122
[62,     1] loss: 0.115
[63,     1] loss: 0.109
[64,     1] loss: 0.060
[65,     1] loss: 0.119
[66,     1] loss: 0.068
[67,     1] loss: 0.037
[68,     1] loss: 0.055
[69,     1] loss: 0.086
[70,     1] loss: 0.080
Early stopping applied (best metric=0.3090759217739105)
Finished Training
Total time taken: 7.7039947509765625
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.697
[3,     1] loss: 0.662
[4,     1] loss: 0.623
[5,     1] loss: 0.596
[6,     1] loss: 0.554
[7,     1] loss: 0.503
[8,     1] loss: 0.430
[9,     1] loss: 0.430
[10,     1] loss: 0.389
[11,     1] loss: 0.406
[12,     1] loss: 0.452
[13,     1] loss: 0.305
[14,     1] loss: 0.381
[15,     1] loss: 0.328
[16,     1] loss: 0.348
[17,     1] loss: 0.299
[18,     1] loss: 0.311
[19,     1] loss: 0.322
[20,     1] loss: 0.319
[21,     1] loss: 0.311
[22,     1] loss: 0.300
[23,     1] loss: 0.271
[24,     1] loss: 0.285
[25,     1] loss: 0.289
[26,     1] loss: 0.226
[27,     1] loss: 0.232
[28,     1] loss: 0.211
[29,     1] loss: 0.205
[30,     1] loss: 0.216
[31,     1] loss: 0.151
[32,     1] loss: 0.149
[33,     1] loss: 0.160
[34,     1] loss: 0.152
[35,     1] loss: 0.118
[36,     1] loss: 0.184
[37,     1] loss: 0.096
[38,     1] loss: 0.132
[39,     1] loss: 0.100
[40,     1] loss: 0.060
[41,     1] loss: 0.077
[42,     1] loss: 0.132
[43,     1] loss: 0.081
[44,     1] loss: 0.120
[45,     1] loss: 0.087
[46,     1] loss: 0.052
[47,     1] loss: 0.102
[48,     1] loss: 0.133
[49,     1] loss: 0.100
[50,     1] loss: 0.083
[51,     1] loss: 0.091
[52,     1] loss: 0.061
[53,     1] loss: 0.089
[54,     1] loss: 0.161
[55,     1] loss: 0.158
[56,     1] loss: 0.151
[57,     1] loss: 0.145
Early stopping applied (best metric=0.3447256088256836)
Finished Training
Total time taken: 6.302013874053955
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.702
[3,     1] loss: 0.676
[4,     1] loss: 0.669
[5,     1] loss: 0.624
[6,     1] loss: 0.593
[7,     1] loss: 0.556
[8,     1] loss: 0.528
[9,     1] loss: 0.494
[10,     1] loss: 0.420
[11,     1] loss: 0.387
[12,     1] loss: 0.389
[13,     1] loss: 0.288
[14,     1] loss: 0.295
[15,     1] loss: 0.290
[16,     1] loss: 0.225
[17,     1] loss: 0.250
[18,     1] loss: 0.272
[19,     1] loss: 0.210
[20,     1] loss: 0.203
[21,     1] loss: 0.226
[22,     1] loss: 0.253
[23,     1] loss: 0.248
[24,     1] loss: 0.184
[25,     1] loss: 0.178
[26,     1] loss: 0.276
[27,     1] loss: 0.224
[28,     1] loss: 0.181
[29,     1] loss: 0.176
[30,     1] loss: 0.181
[31,     1] loss: 0.130
[32,     1] loss: 0.170
[33,     1] loss: 0.212
[34,     1] loss: 0.122
[35,     1] loss: 0.158
[36,     1] loss: 0.154
[37,     1] loss: 0.124
[38,     1] loss: 0.156
[39,     1] loss: 0.120
[40,     1] loss: 0.120
[41,     1] loss: 0.102
[42,     1] loss: 0.092
[43,     1] loss: 0.087
[44,     1] loss: 0.061
[45,     1] loss: 0.077
[46,     1] loss: 0.099
[47,     1] loss: 0.100
[48,     1] loss: 0.099
[49,     1] loss: 0.071
[50,     1] loss: 0.158
[51,     1] loss: 0.097
[52,     1] loss: 0.126
[53,     1] loss: 0.111
[54,     1] loss: 0.168
[55,     1] loss: 0.099
[56,     1] loss: 0.121
[57,     1] loss: 0.115
[58,     1] loss: 0.077
[59,     1] loss: 0.113
Early stopping applied (best metric=0.31423529982566833)
Finished Training
Total time taken: 6.516999244689941
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.694
[3,     1] loss: 0.674
[4,     1] loss: 0.633
[5,     1] loss: 0.583
[6,     1] loss: 0.542
[7,     1] loss: 0.495
[8,     1] loss: 0.486
[9,     1] loss: 0.407
[10,     1] loss: 0.395
[11,     1] loss: 0.416
[12,     1] loss: 0.306
[13,     1] loss: 0.283
[14,     1] loss: 0.419
[15,     1] loss: 0.316
[16,     1] loss: 0.324
[17,     1] loss: 0.271
[18,     1] loss: 0.319
[19,     1] loss: 0.273
[20,     1] loss: 0.358
[21,     1] loss: 0.276
[22,     1] loss: 0.273
[23,     1] loss: 0.233
[24,     1] loss: 0.269
[25,     1] loss: 0.265
[26,     1] loss: 0.272
[27,     1] loss: 0.244
[28,     1] loss: 0.258
[29,     1] loss: 0.269
[30,     1] loss: 0.208
[31,     1] loss: 0.185
[32,     1] loss: 0.197
[33,     1] loss: 0.195
[34,     1] loss: 0.193
[35,     1] loss: 0.206
[36,     1] loss: 0.187
[37,     1] loss: 0.144
[38,     1] loss: 0.207
[39,     1] loss: 0.195
[40,     1] loss: 0.148
[41,     1] loss: 0.169
[42,     1] loss: 0.170
[43,     1] loss: 0.109
[44,     1] loss: 0.183
[45,     1] loss: 0.166
[46,     1] loss: 0.177
[47,     1] loss: 0.133
[48,     1] loss: 0.175
[49,     1] loss: 0.164
[50,     1] loss: 0.116
[51,     1] loss: 0.137
[52,     1] loss: 0.167
[53,     1] loss: 0.124
[54,     1] loss: 0.087
[55,     1] loss: 0.100
[56,     1] loss: 0.120
Early stopping applied (best metric=0.42513713240623474)
Finished Training
Total time taken: 6.206997394561768
{'Hydroxylation-K Validation Accuracy': 0.8386170212765958, 'Hydroxylation-K Validation Sensitivity': 0.8466666666666667, 'Hydroxylation-K Validation Specificity': 0.8368421052631579, 'Hydroxylation-K Validation Precision': 0.5794513329807447, 'Hydroxylation-K AUC ROC': 0.8507485380116959, 'Hydroxylation-K AUC PR': 0.6403907233137159, 'Hydroxylation-K MCC': 0.6039666961934343, 'Hydroxylation-K F1': 0.6826962921644844, 'Validation Loss (Hydroxylation-K)': 0.32638532280921934, 'Validation Loss (total)': 0.32638532280921934, 'TimeToTrain': 7.552547264099121}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007917313405012,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2534012407,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.453515277782287}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.692
[4,     1] loss: 0.683
[5,     1] loss: 0.676
[6,     1] loss: 0.666
[7,     1] loss: 0.657
[8,     1] loss: 0.649
[9,     1] loss: 0.629
[10,     1] loss: 0.629
[11,     1] loss: 0.616
[12,     1] loss: 0.606
[13,     1] loss: 0.608
[14,     1] loss: 0.592
[15,     1] loss: 0.585
[16,     1] loss: 0.564
[17,     1] loss: 0.559
[18,     1] loss: 0.549
[19,     1] loss: 0.522
[20,     1] loss: 0.530
[21,     1] loss: 0.505
[22,     1] loss: 0.513
[23,     1] loss: 0.490
[24,     1] loss: 0.484
[25,     1] loss: 0.465
[26,     1] loss: 0.489
[27,     1] loss: 0.450
[28,     1] loss: 0.418
[29,     1] loss: 0.418
[30,     1] loss: 0.441
[31,     1] loss: 0.414
[32,     1] loss: 0.392
[33,     1] loss: 0.408
[34,     1] loss: 0.390
[35,     1] loss: 0.419
[36,     1] loss: 0.376
[37,     1] loss: 0.370
[38,     1] loss: 0.348
[39,     1] loss: 0.340
[40,     1] loss: 0.354
[41,     1] loss: 0.343
[42,     1] loss: 0.329
[43,     1] loss: 0.351
[44,     1] loss: 0.329
[45,     1] loss: 0.338
[46,     1] loss: 0.318
[47,     1] loss: 0.300
[48,     1] loss: 0.294
[49,     1] loss: 0.306
[50,     1] loss: 0.295
[51,     1] loss: 0.314
[52,     1] loss: 0.264
[53,     1] loss: 0.284
[54,     1] loss: 0.295
[55,     1] loss: 0.286
[56,     1] loss: 0.252
[57,     1] loss: 0.285
[58,     1] loss: 0.295
[59,     1] loss: 0.291
[60,     1] loss: 0.298
[61,     1] loss: 0.263
[62,     1] loss: 0.260
[63,     1] loss: 0.265
[64,     1] loss: 0.244
[65,     1] loss: 0.262
[66,     1] loss: 0.255
[67,     1] loss: 0.247
[68,     1] loss: 0.262
[69,     1] loss: 0.234
[70,     1] loss: 0.277
[71,     1] loss: 0.317
[72,     1] loss: 0.236
[73,     1] loss: 0.243
[74,     1] loss: 0.266
[75,     1] loss: 0.233
[76,     1] loss: 0.243
[77,     1] loss: 0.257
[78,     1] loss: 0.218
[79,     1] loss: 0.216
[80,     1] loss: 0.272
[81,     1] loss: 0.256
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035140941153452994,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 165156027,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.1795344672740455}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.687
[3,     1] loss: 0.653
[4,     1] loss: 0.607
[5,     1] loss: 0.574
[6,     1] loss: 0.526
[7,     1] loss: 0.471
[8,     1] loss: 0.451
[9,     1] loss: 0.398
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003951209164188816,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4079991516,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.5521294698632009}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.686
[3,     1] loss: 0.676
[4,     1] loss: 0.645
[5,     1] loss: 0.624
[6,     1] loss: 0.584
[7,     1] loss: 0.546
[8,     1] loss: 0.517
[9,     1] loss: 0.497
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028676053048233706,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1487539480,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.0645410430465385}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.681
[3,     1] loss: 0.689
[4,     1] loss: 0.647
[5,     1] loss: 0.626
[6,     1] loss: 0.589
[7,     1] loss: 0.575
[8,     1] loss: 0.558
[9,     1] loss: 0.525
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009976540034804466,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1287331733,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.160256054114626}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.696
[3,     1] loss: 0.685
[4,     1] loss: 0.664
[5,     1] loss: 0.631
[6,     1] loss: 0.587
[7,     1] loss: 0.520
[8,     1] loss: 0.502
[9,     1] loss: 0.432
[10,     1] loss: 0.440
[11,     1] loss: 0.387
[12,     1] loss: 0.372
[13,     1] loss: 0.324
[14,     1] loss: 0.315
[15,     1] loss: 0.269
[16,     1] loss: 0.335
[17,     1] loss: 0.374
[18,     1] loss: 0.258
[19,     1] loss: 0.274
[20,     1] loss: 0.262
[21,     1] loss: 0.276
[22,     1] loss: 0.203
[23,     1] loss: 0.208
[24,     1] loss: 0.188
[25,     1] loss: 0.198
[26,     1] loss: 0.207
[27,     1] loss: 0.163
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006965790303271476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2614648894,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.517105004203236}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.695
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017903902216322648,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1987285662,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.143935429669643}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.693
[3,     1] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006591556863111771,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2717687779,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.945578466633404}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.700
[3,     1] loss: 0.673
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003285073786462161,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2047751446,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.508053262551153}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.698
[3,     1] loss: 0.671
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005693389366497185,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 581830809,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.4952601865882906}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.684
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00844656031999748,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 554104298,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.478675652662771}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.727
[3,     1] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007122543938802695,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 109563171,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.6903045278324831}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.692
[3,     1] loss: 0.688
[4,     1] loss: 0.671
[5,     1] loss: 0.638
[6,     1] loss: 0.594
[7,     1] loss: 0.555
[8,     1] loss: 0.462
[9,     1] loss: 0.415
[10,     1] loss: 0.450
[11,     1] loss: 0.421
[12,     1] loss: 0.341
[13,     1] loss: 0.383
[14,     1] loss: 0.350
[15,     1] loss: 0.307
[16,     1] loss: 0.375
[17,     1] loss: 0.271
[18,     1] loss: 0.279
[19,     1] loss: 0.265
[20,     1] loss: 0.285
[21,     1] loss: 0.229
[22,     1] loss: 0.243
[23,     1] loss: 0.318
[24,     1] loss: 0.233
[25,     1] loss: 0.196
[26,     1] loss: 0.237
[27,     1] loss: 0.207
[28,     1] loss: 0.224
[29,     1] loss: 0.255
[30,     1] loss: 0.232
[31,     1] loss: 0.193
[32,     1] loss: 0.186
[33,     1] loss: 0.220
[34,     1] loss: 0.163
[35,     1] loss: 0.200
[36,     1] loss: 0.162
[37,     1] loss: 0.170
[38,     1] loss: 0.157
[39,     1] loss: 0.184
[40,     1] loss: 0.199
[41,     1] loss: 0.181
[42,     1] loss: 0.188
[43,     1] loss: 0.158
[44,     1] loss: 0.146
[45,     1] loss: 0.152
[46,     1] loss: 0.156
[47,     1] loss: 0.197
[48,     1] loss: 0.122
[49,     1] loss: 0.148
[50,     1] loss: 0.137
[51,     1] loss: 0.141
[52,     1] loss: 0.157
[53,     1] loss: 0.193
[54,     1] loss: 0.167
[55,     1] loss: 0.119
[56,     1] loss: 0.146
[57,     1] loss: 0.169
[58,     1] loss: 0.146
[59,     1] loss: 0.158
[60,     1] loss: 0.155
[61,     1] loss: 0.162
[62,     1] loss: 0.110
[63,     1] loss: 0.087
[64,     1] loss: 0.139
[65,     1] loss: 0.121
[66,     1] loss: 0.167
[67,     1] loss: 0.126
[68,     1] loss: 0.141
[69,     1] loss: 0.102
[70,     1] loss: 0.097
[71,     1] loss: 0.130
[72,     1] loss: 0.076
[73,     1] loss: 0.125
[74,     1] loss: 0.112
[75,     1] loss: 0.127
[76,     1] loss: 0.117
[77,     1] loss: 0.151
[78,     1] loss: 0.140
[79,     1] loss: 0.129
[80,     1] loss: 0.119
[81,     1] loss: 0.120
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008234995090757818,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2630879271,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.7071507079847054}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.690
[3,     1] loss: 0.682
[4,     1] loss: 0.654
[5,     1] loss: 0.617
[6,     1] loss: 0.608
[7,     1] loss: 0.512
[8,     1] loss: 0.476
[9,     1] loss: 0.357
[10,     1] loss: 0.429
[11,     1] loss: 0.446
[12,     1] loss: 0.328
[13,     1] loss: 0.331
[14,     1] loss: 0.304
[15,     1] loss: 0.269
[16,     1] loss: 0.282
[17,     1] loss: 0.239
[18,     1] loss: 0.281
[19,     1] loss: 0.226
[20,     1] loss: 0.231
[21,     1] loss: 0.188
[22,     1] loss: 0.185
[23,     1] loss: 0.166
[24,     1] loss: 0.242
[25,     1] loss: 0.190
[26,     1] loss: 0.152
[27,     1] loss: 0.221
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0047757574320389596,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2414734942,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.7067868089789457}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.655
[5,     1] loss: 0.614
[6,     1] loss: 0.572
[7,     1] loss: 0.533
[8,     1] loss: 0.512
[9,     1] loss: 0.432
[10,     1] loss: 0.416
[11,     1] loss: 0.380
[12,     1] loss: 0.364
[13,     1] loss: 0.395
[14,     1] loss: 0.350
[15,     1] loss: 0.367
[16,     1] loss: 0.310
[17,     1] loss: 0.313
[18,     1] loss: 0.255
[19,     1] loss: 0.287
[20,     1] loss: 0.275
[21,     1] loss: 0.244
[22,     1] loss: 0.291
[23,     1] loss: 0.228
[24,     1] loss: 0.230
[25,     1] loss: 0.216
[26,     1] loss: 0.272
[27,     1] loss: 0.244
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006144842145372106,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2980178416,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.4193911044931204}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.705
[3,     1] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007757657119690493,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 28384299,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.8123851854021003}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.696
[3,     1] loss: 0.691
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0043051373416170356,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2298981103,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.6693654468917245}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.693
[3,     1] loss: 0.673
[4,     1] loss: 0.652
[5,     1] loss: 0.611
[6,     1] loss: 0.569
[7,     1] loss: 0.529
[8,     1] loss: 0.490
[9,     1] loss: 0.418
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005723431067806933,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2698464221,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.12123113668723}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.677
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035996299385986814,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2155905395,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.46096886783299}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.687
[3,     1] loss: 0.664
[4,     1] loss: 0.635
[5,     1] loss: 0.616
[6,     1] loss: 0.584
[7,     1] loss: 0.535
[8,     1] loss: 0.510
[9,     1] loss: 0.502
[10,     1] loss: 0.466
[11,     1] loss: 0.420
[12,     1] loss: 0.440
[13,     1] loss: 0.423
[14,     1] loss: 0.407
[15,     1] loss: 0.405
[16,     1] loss: 0.411
[17,     1] loss: 0.360
[18,     1] loss: 0.373
[19,     1] loss: 0.387
[20,     1] loss: 0.410
[21,     1] loss: 0.325
[22,     1] loss: 0.337
[23,     1] loss: 0.290
[24,     1] loss: 0.332
[25,     1] loss: 0.419
[26,     1] loss: 0.325
[27,     1] loss: 0.315
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033311783154528567,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3770567,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.840928023249601}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.696
[3,     1] loss: 0.672
[4,     1] loss: 0.652
[5,     1] loss: 0.605
[6,     1] loss: 0.565
[7,     1] loss: 0.525
[8,     1] loss: 0.496
[9,     1] loss: 0.483
[10,     1] loss: 0.444
[11,     1] loss: 0.392
[12,     1] loss: 0.419
[13,     1] loss: 0.399
[14,     1] loss: 0.393
[15,     1] loss: 0.397
[16,     1] loss: 0.440
[17,     1] loss: 0.343
[18,     1] loss: 0.322
[19,     1] loss: 0.383
[20,     1] loss: 0.297
[21,     1] loss: 0.394
[22,     1] loss: 0.310
[23,     1] loss: 0.340
[24,     1] loss: 0.258
[25,     1] loss: 0.301
[26,     1] loss: 0.281
[27,     1] loss: 0.266
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025791645199763243,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 690301916,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.906467745220038}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.668
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003334033516198556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1576198825,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.339657467391085}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.690
[3,     1] loss: 0.666
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008541075957357074,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1334417175,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.86714418337476}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.676
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037737741477905963,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2586207665,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.678212122101506}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.685
[3,     1] loss: 0.651
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005939813918540174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4066824917,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.543554572462252}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.687
[3,     1] loss: 0.663
[4,     1] loss: 0.649
[5,     1] loss: 0.620
[6,     1] loss: 0.576
[7,     1] loss: 0.552
[8,     1] loss: 0.527
[9,     1] loss: 0.511
[10,     1] loss: 0.504
[11,     1] loss: 0.493
[12,     1] loss: 0.484
[13,     1] loss: 0.453
[14,     1] loss: 0.460
[15,     1] loss: 0.435
[16,     1] loss: 0.442
[17,     1] loss: 0.510
[18,     1] loss: 0.537
[19,     1] loss: 0.454
[20,     1] loss: 0.431
[21,     1] loss: 0.458
[22,     1] loss: 0.431
[23,     1] loss: 0.444
[24,     1] loss: 0.407
[25,     1] loss: 0.367
[26,     1] loss: 0.418
[27,     1] loss: 0.408
[28,     1] loss: 0.378
[29,     1] loss: 0.412
[30,     1] loss: 0.394
[31,     1] loss: 0.407
[32,     1] loss: 0.390
[33,     1] loss: 0.641
[34,     1] loss: 0.536
[35,     1] loss: 0.417
[36,     1] loss: 0.453
[37,     1] loss: 0.443
[38,     1] loss: 0.422
[39,     1] loss: 0.438
[40,     1] loss: 0.445
[41,     1] loss: 0.418
[42,     1] loss: 0.392
[43,     1] loss: 0.365
[44,     1] loss: 0.370
[45,     1] loss: 0.384
[46,     1] loss: 0.413
[47,     1] loss: 0.408
[48,     1] loss: 0.388
[49,     1] loss: 0.396
[50,     1] loss: 0.420
[51,     1] loss: 0.387
[52,     1] loss: 0.401
[53,     1] loss: 0.376
[54,     1] loss: 0.390
[55,     1] loss: 0.340
[56,     1] loss: 0.429
[57,     1] loss: 0.608
[58,     1] loss: 0.380
[59,     1] loss: 0.483
[60,     1] loss: 0.395
[61,     1] loss: 0.411
[62,     1] loss: 0.423
[63,     1] loss: 0.426
[64,     1] loss: 0.383
[65,     1] loss: 0.399
[66,     1] loss: 0.406
[67,     1] loss: 0.368
[68,     1] loss: 0.404
[69,     1] loss: 0.406
[70,     1] loss: 0.378
[71,     1] loss: 0.401
[72,     1] loss: 0.369
[73,     1] loss: 0.397
[74,     1] loss: 0.405
[75,     1] loss: 0.382
[76,     1] loss: 0.388
[77,     1] loss: 0.376
[78,     1] loss: 0.502
[79,     1] loss: 0.586
[80,     1] loss: 0.426
[81,     1] loss: 0.457
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021285978156597293,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1299750058,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.977733406119654}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.676
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002705084176463423,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1585706171,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.372754206774818}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.688
[4,     1] loss: 0.681
[5,     1] loss: 0.651
[6,     1] loss: 0.625
[7,     1] loss: 0.609
[8,     1] loss: 0.588
[9,     1] loss: 0.539
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003438843320241836,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3841938597,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.133559902748484}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.691
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004707055152463634,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1575749947,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.08334975845152667}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.667
[4,     1] loss: 0.630
[5,     1] loss: 0.579
[6,     1] loss: 0.526
[7,     1] loss: 0.484
[8,     1] loss: 0.422
[9,     1] loss: 0.408
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006384481449538215,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3685818778,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.1478621108694729}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.671
[4,     1] loss: 0.632
[5,     1] loss: 0.579
[6,     1] loss: 0.535
[7,     1] loss: 0.458
[8,     1] loss: 0.421
[9,     1] loss: 0.403
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004743575354665677,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1756457428,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.64276633398844}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.697
[3,     1] loss: 0.674
[4,     1] loss: 0.628
[5,     1] loss: 0.587
[6,     1] loss: 0.554
[7,     1] loss: 0.495
[8,     1] loss: 0.458
[9,     1] loss: 0.445
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006442463111723765,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1938831061,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.2948681262662864}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.695
[3,     1] loss: 0.674
[4,     1] loss: 0.632
[5,     1] loss: 0.607
[6,     1] loss: 0.536
[7,     1] loss: 0.495
[8,     1] loss: 0.390
[9,     1] loss: 0.409
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00044495546676069555,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3949098399,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.725881772174795}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.701
[2,     1] loss: 0.694
[3,     1] loss: 0.689
[4,     1] loss: 0.687
[5,     1] loss: 0.686
[6,     1] loss: 0.683
[7,     1] loss: 0.680
[8,     1] loss: 0.676
[9,     1] loss: 0.672
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030566574874613924,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2660324701,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.797137202183539}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.688
[3,     1] loss: 0.658
[4,     1] loss: 0.628
[5,     1] loss: 0.598
[6,     1] loss: 0.567
[7,     1] loss: 0.519
[8,     1] loss: 0.492
[9,     1] loss: 0.456
[10,     1] loss: 0.456
[11,     1] loss: 0.422
[12,     1] loss: 0.386
[13,     1] loss: 0.321
[14,     1] loss: 0.336
[15,     1] loss: 0.315
[16,     1] loss: 0.370
[17,     1] loss: 0.284
[18,     1] loss: 0.301
[19,     1] loss: 0.284
[20,     1] loss: 0.272
[21,     1] loss: 0.293
[22,     1] loss: 0.295
[23,     1] loss: 0.265
[24,     1] loss: 0.248
[25,     1] loss: 0.328
[26,     1] loss: 0.216
[27,     1] loss: 0.198
[28,     1] loss: 0.276
[29,     1] loss: 0.288
[30,     1] loss: 0.246
[31,     1] loss: 0.222
[32,     1] loss: 0.224
[33,     1] loss: 0.178
[34,     1] loss: 0.250
[35,     1] loss: 0.172
[36,     1] loss: 0.212
[37,     1] loss: 0.181
[38,     1] loss: 0.275
[39,     1] loss: 0.248
[40,     1] loss: 0.221
[41,     1] loss: 0.318
[42,     1] loss: 0.198
[43,     1] loss: 0.255
[44,     1] loss: 0.227
[45,     1] loss: 0.198
[46,     1] loss: 0.235
[47,     1] loss: 0.251
[48,     1] loss: 0.193
[49,     1] loss: 0.214
[50,     1] loss: 0.177
[51,     1] loss: 0.177
[52,     1] loss: 0.202
[53,     1] loss: 0.157
[54,     1] loss: 0.131
[55,     1] loss: 0.148
[56,     1] loss: 0.180
[57,     1] loss: 0.086
[58,     1] loss: 0.141
[59,     1] loss: 0.109
[60,     1] loss: 0.198
[61,     1] loss: 0.152
[62,     1] loss: 0.116
[63,     1] loss: 0.118
[64,     1] loss: 0.094
[65,     1] loss: 0.114
[66,     1] loss: 0.111
[67,     1] loss: 0.101
[68,     1] loss: 0.076
[69,     1] loss: 0.127
[70,     1] loss: 0.139
[71,     1] loss: 0.057
[72,     1] loss: 0.125
[73,     1] loss: 0.179
[74,     1] loss: 0.080
[75,     1] loss: 0.115
[76,     1] loss: 0.140
[77,     1] loss: 0.084
[78,     1] loss: 0.198
[79,     1] loss: 0.117
[80,     1] loss: 0.118
[81,     1] loss: 0.103
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034734888841670363,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 726297166,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 17.48531961176802}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.685
[3,     1] loss: 0.674
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019719101609940214,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3524916663,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.640809889610441}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.689
[3,     1] loss: 0.674
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007036222849269944,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1289738377,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.764607359971105}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.689
[3,     1] loss: 0.699
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008588881034073174,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 328558589,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.03733164676271}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.686
[2,     1] loss: 0.695
[3,     1] loss: 0.670
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003914897065432676,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1250376933,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.585145580105788}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.698
[3,     1] loss: 0.690
[4,     1] loss: 0.684
[5,     1] loss: 0.675
[6,     1] loss: 0.659
[7,     1] loss: 0.645
[8,     1] loss: 0.626
[9,     1] loss: 0.606
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003268527986684526,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2916632702,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.20759391682876}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.690
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006866577282972443,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 585995033,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.118079113000082}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.686
[4,     1] loss: 0.666
[5,     1] loss: 0.642
[6,     1] loss: 0.612
[7,     1] loss: 0.554
[8,     1] loss: 0.527
[9,     1] loss: 0.505
[10,     1] loss: 0.443
[11,     1] loss: 0.449
[12,     1] loss: 0.442
[13,     1] loss: 0.457
[14,     1] loss: 0.419
[15,     1] loss: 0.414
[16,     1] loss: 0.431
[17,     1] loss: 0.388
[18,     1] loss: 0.421
[19,     1] loss: 0.379
[20,     1] loss: 0.409
[21,     1] loss: 0.369
[22,     1] loss: 0.374
[23,     1] loss: 0.374
[24,     1] loss: 0.378
[25,     1] loss: 0.377
[26,     1] loss: 0.376
[27,     1] loss: 0.352
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026432194921384863,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4163614041,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.346348218121809}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.689
[3,     1] loss: 0.674
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014691638816230776,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2602282837,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.825616556191303}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.692
[3,     1] loss: 0.684
[4,     1] loss: 0.671
[5,     1] loss: 0.656
[6,     1] loss: 0.644
[7,     1] loss: 0.632
[8,     1] loss: 0.586
[9,     1] loss: 0.582
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005246416599157099,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 203410550,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.05255111890534}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.691
[3,     1] loss: 0.680
[4,     1] loss: 0.629
[5,     1] loss: 0.596
[6,     1] loss: 0.548
[7,     1] loss: 0.518
[8,     1] loss: 0.451
[9,     1] loss: 0.394
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00838139265153788,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1645410035,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 24.19828829951864}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.675
[4,     1] loss: 0.653
[5,     1] loss: 0.620
[6,     1] loss: 0.580
[7,     1] loss: 0.579
[8,     1] loss: 0.563
[9,     1] loss: 0.517
[10,     1] loss: 0.516
[11,     1] loss: 0.537
[12,     1] loss: 0.501
[13,     1] loss: 0.479
[14,     1] loss: 0.439
[15,     1] loss: 0.440
[16,     1] loss: 0.450
[17,     1] loss: 0.434
[18,     1] loss: 0.440
[19,     1] loss: 0.460
[20,     1] loss: 0.437
[21,     1] loss: 0.405
[22,     1] loss: 0.418
[23,     1] loss: 0.380
[24,     1] loss: 0.394
[25,     1] loss: 0.369
[26,     1] loss: 0.432
[27,     1] loss: 0.804
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009234683724201161,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1397374613,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.579788054130212}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.691
[3,     1] loss: 0.688
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002964693871233424,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 766793131,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.342581963937077}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.693
[3,     1] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009762977827105388,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2073012828,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.039258970349783}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.693
[3,     1] loss: 0.691
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020593488209003034,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2556852173,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.021563818359022}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.693
[3,     1] loss: 0.686
[4,     1] loss: 0.680
[5,     1] loss: 0.665
[6,     1] loss: 0.641
[7,     1] loss: 0.620
[8,     1] loss: 0.602
[9,     1] loss: 0.576
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002091150519227677,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 692049873,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.770045971301389}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009908646829144547,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2407726333,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 15.267786949109404}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.717
[3,     1] loss: 0.692
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004508459106689449,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2909660022,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.962258548623618}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.693
[3,     1] loss: 0.677
[4,     1] loss: 0.648
[5,     1] loss: 0.611
[6,     1] loss: 0.579
[7,     1] loss: 0.532
[8,     1] loss: 0.539
[9,     1] loss: 0.492
[10,     1] loss: 0.447
[11,     1] loss: 0.438
[12,     1] loss: 0.369
[13,     1] loss: 0.320
[14,     1] loss: 0.331
[15,     1] loss: 0.301
[16,     1] loss: 0.328
[17,     1] loss: 0.262
[18,     1] loss: 0.191
[19,     1] loss: 0.209
[20,     1] loss: 0.257
[21,     1] loss: 0.270
[22,     1] loss: 0.244
[23,     1] loss: 0.183
[24,     1] loss: 0.177
[25,     1] loss: 0.213
[26,     1] loss: 0.183
[27,     1] loss: 0.194
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008213504628769207,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2304287064,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 12.762758657158319}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.700
[3,     1] loss: 0.686
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004038391947125271,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4129997378,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.731142531715074}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.692
[3,     1] loss: 0.676
[4,     1] loss: 0.653
[5,     1] loss: 0.637
[6,     1] loss: 0.630
[7,     1] loss: 0.611
[8,     1] loss: 0.582
[9,     1] loss: 0.571
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003601084061499072,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 549930840,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.323295830997816}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.696
[3,     1] loss: 0.670
[4,     1] loss: 0.648
[5,     1] loss: 0.629
[6,     1] loss: 0.581
[7,     1] loss: 0.549
[8,     1] loss: 0.488
[9,     1] loss: 0.466
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009873856004306993,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4271677205,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.295234972028044}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.700
[3,     1] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002782424190390118,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2972365751,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.307505419061648}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.690
[3,     1] loss: 0.680
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005516581906750082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2271172152,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 8.11242281234126}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.705
[2,     1] loss: 0.692
[3,     1] loss: 0.691
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 3.8494816544672184e-05,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1144156111,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.550005397243561}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.695
[3,     1] loss: 0.694
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004666482364632612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2278088521,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.634399147989176}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.682
[3,     1] loss: 0.637
[4,     1] loss: 0.598
[5,     1] loss: 0.574
[6,     1] loss: 0.525
[7,     1] loss: 0.522
[8,     1] loss: 0.508
[9,     1] loss: 0.507
[10,     1] loss: 0.384
[11,     1] loss: 0.486
[12,     1] loss: 0.433
[13,     1] loss: 0.387
[14,     1] loss: 0.419
[15,     1] loss: 0.374
[16,     1] loss: 0.333
[17,     1] loss: 0.281
[18,     1] loss: 0.266
[19,     1] loss: 0.279
[20,     1] loss: 0.287
[21,     1] loss: 0.259
[22,     1] loss: 0.186
[23,     1] loss: 0.205
[24,     1] loss: 0.160
[25,     1] loss: 0.196
[26,     1] loss: 0.219
[27,     1] loss: 0.165
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019088642524124064,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 518026992,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.6310226518850041}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.679
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019594309649630555,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3791941695,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.214215066772802}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.682
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009987852264896537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4166518976,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.872711309972622}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.699
[3,     1] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009710209331202441,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4282602485,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 1.2571182445359326}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.710
[3,     1] loss: 0.691
[4,     1] loss: 0.695
[5,     1] loss: 0.680
[6,     1] loss: 0.664
[7,     1] loss: 0.632
[8,     1] loss: 0.590
[9,     1] loss: 0.541
[10,     1] loss: 0.511
[11,     1] loss: 0.426
[12,     1] loss: 0.367
[13,     1] loss: 0.370
[14,     1] loss: 0.289
[15,     1] loss: 0.363
[16,     1] loss: 0.434
[17,     1] loss: 0.346
[18,     1] loss: 0.291
[19,     1] loss: 0.307
[20,     1] loss: 0.284
[21,     1] loss: 0.226
[22,     1] loss: 0.252
[23,     1] loss: 0.233
[24,     1] loss: 0.269
[25,     1] loss: 0.256
[26,     1] loss: 0.255
[27,     1] loss: 0.186
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00796325936790582,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 770211686,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.352068326914458}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.689
[3,     1] loss: 0.672
[4,     1] loss: 0.624
[5,     1] loss: 0.607
[6,     1] loss: 0.563
[7,     1] loss: 0.507
[8,     1] loss: 0.467
[9,     1] loss: 0.436
[10,     1] loss: 0.395
[11,     1] loss: 0.346
[12,     1] loss: 0.401
[13,     1] loss: 0.424
[14,     1] loss: 0.359
[15,     1] loss: 0.303
[16,     1] loss: 0.332
[17,     1] loss: 0.294
[18,     1] loss: 0.244
[19,     1] loss: 0.302
[20,     1] loss: 0.279
[21,     1] loss: 0.325
[22,     1] loss: 0.255
[23,     1] loss: 0.304
[24,     1] loss: 0.275
[25,     1] loss: 0.211
[26,     1] loss: 0.270
[27,     1] loss: 0.310
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004361288810110006,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 822028592,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9693832347502127}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.685
[3,     1] loss: 0.681
[4,     1] loss: 0.653
[5,     1] loss: 0.613
[6,     1] loss: 0.559
[7,     1] loss: 0.514
[8,     1] loss: 0.468
[9,     1] loss: 0.440
[10,     1] loss: 0.390
[11,     1] loss: 0.337
[12,     1] loss: 0.352
[13,     1] loss: 0.284
[14,     1] loss: 0.249
[15,     1] loss: 0.266
[16,     1] loss: 0.290
[17,     1] loss: 0.259
[18,     1] loss: 0.276
[19,     1] loss: 0.265
[20,     1] loss: 0.174
[21,     1] loss: 0.258
[22,     1] loss: 0.273
[23,     1] loss: 0.199
[24,     1] loss: 0.281
[25,     1] loss: 0.197
[26,     1] loss: 0.197
[27,     1] loss: 0.267
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004993265698147604,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 921075050,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.825060912479159}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.685
[2,     1] loss: 0.685
[3,     1] loss: 0.659
[4,     1] loss: 0.622
[5,     1] loss: 0.562
[6,     1] loss: 0.540
[7,     1] loss: 0.504
[8,     1] loss: 0.398
[9,     1] loss: 0.368
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004538978110778111,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1532433729,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.916032856715378}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.694
[3,     1] loss: 0.670
[4,     1] loss: 0.638
[5,     1] loss: 0.591
[6,     1] loss: 0.564
[7,     1] loss: 0.535
[8,     1] loss: 0.475
[9,     1] loss: 0.487
[10,     1] loss: 0.461
[11,     1] loss: 0.430
[12,     1] loss: 0.418
[13,     1] loss: 0.331
[14,     1] loss: 0.286
[15,     1] loss: 0.363
[16,     1] loss: 0.287
[17,     1] loss: 0.358
[18,     1] loss: 0.246
[19,     1] loss: 0.291
[20,     1] loss: 0.216
[21,     1] loss: 0.214
[22,     1] loss: 0.233
[23,     1] loss: 0.198
[24,     1] loss: 0.166
[25,     1] loss: 0.176
[26,     1] loss: 0.214
[27,     1] loss: 0.148
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007062721792233275,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2355697094,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.1462402602354524}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.707
[3,     1] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007996768337859592,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2163044135,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 23.86839490849723}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.683
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037078167722491627,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3764816014,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.163735007708419}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.685
[3,     1] loss: 0.641
[4,     1] loss: 0.631
[5,     1] loss: 0.585
[6,     1] loss: 0.544
[7,     1] loss: 0.547
[8,     1] loss: 0.533
[9,     1] loss: 0.446
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005784778781435169,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 37368225,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 20.348915679530737}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.675
[4,     1] loss: 0.643
[5,     1] loss: 0.609
[6,     1] loss: 0.564
[7,     1] loss: 0.538
[8,     1] loss: 0.486
[9,     1] loss: 0.475
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008107744571326395,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1494247308,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.858517148787964}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.694
[3,     1] loss: 0.694
[4,     1] loss: 0.677
[5,     1] loss: 0.671
[6,     1] loss: 0.656
[7,     1] loss: 0.628
[8,     1] loss: 0.617
[9,     1] loss: 0.608
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009508617326134533,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1375357311,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.223122715960838}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.688
[2,     1] loss: 0.712
[3,     1] loss: 0.692
[4,     1] loss: 0.679
[5,     1] loss: 0.665
[6,     1] loss: 0.627
[7,     1] loss: 0.592
[8,     1] loss: 0.508
[9,     1] loss: 0.493
[10,     1] loss: 0.455
[11,     1] loss: 0.436
[12,     1] loss: 0.382
[13,     1] loss: 0.386
[14,     1] loss: 0.318
[15,     1] loss: 0.359
[16,     1] loss: 0.276
[17,     1] loss: 0.284
[18,     1] loss: 0.265
[19,     1] loss: 0.197
[20,     1] loss: 0.227
[21,     1] loss: 0.195
[22,     1] loss: 0.287
[23,     1] loss: 0.215
[24,     1] loss: 0.282
[25,     1] loss: 0.218
[26,     1] loss: 0.219
[27,     1] loss: 0.284
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014989321364739495,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4249520727,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 6.046187909292607}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.695
[3,     1] loss: 0.686
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004581164399684095,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 719493171,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.197875495231205}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.696
[3,     1] loss: 0.687
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003403687432577242,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3574911337,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 7.37282499986031}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.691
[3,     1] loss: 0.677
[4,     1] loss: 0.653
[5,     1] loss: 0.629
[6,     1] loss: 0.607
[7,     1] loss: 0.574
[8,     1] loss: 0.535
[9,     1] loss: 0.483
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006994809656806548,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1550230137,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 3.1505357300274763}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.669
[4,     1] loss: 0.638
[5,     1] loss: 0.593
[6,     1] loss: 0.562
[7,     1] loss: 0.509
[8,     1] loss: 0.496
[9,     1] loss: 0.448
[10,     1] loss: 0.428
[11,     1] loss: 0.397
[12,     1] loss: 0.309
[13,     1] loss: 0.337
[14,     1] loss: 0.286
[15,     1] loss: 0.313
[16,     1] loss: 0.367
[17,     1] loss: 0.282
[18,     1] loss: 0.370
[19,     1] loss: 0.291
[20,     1] loss: 0.315
[21,     1] loss: 0.257
[22,     1] loss: 0.297
[23,     1] loss: 0.241
[24,     1] loss: 0.283
[25,     1] loss: 0.257
[26,     1] loss: 0.208
[27,     1] loss: 0.237
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020785852128410737,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2231468060,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 14.652929889064003}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.679
[4,     1] loss: 0.657
[5,     1] loss: 0.645
[6,     1] loss: 0.619
[7,     1] loss: 0.602
[8,     1] loss: 0.568
[9,     1] loss: 0.563
[10,     1] loss: 0.514
[11,     1] loss: 0.520
[12,     1] loss: 0.514
[13,     1] loss: 0.476
[14,     1] loss: 0.479
[15,     1] loss: 0.438
[16,     1] loss: 0.409
[17,     1] loss: 0.414
[18,     1] loss: 0.433
[19,     1] loss: 0.361
[20,     1] loss: 0.348
[21,     1] loss: 0.395
[22,     1] loss: 0.336
[23,     1] loss: 0.290
[24,     1] loss: 0.307
[25,     1] loss: 0.342
[26,     1] loss: 0.287
[27,     1] loss: 0.298
[28,     1] loss: 0.285
[29,     1] loss: 0.282
[30,     1] loss: 0.256
[31,     1] loss: 0.238
[32,     1] loss: 0.261
[33,     1] loss: 0.284
[34,     1] loss: 0.242
[35,     1] loss: 0.278
[36,     1] loss: 0.235
[37,     1] loss: 0.277
[38,     1] loss: 0.210
[39,     1] loss: 0.332
[40,     1] loss: 0.246
[41,     1] loss: 0.243
[42,     1] loss: 0.217
[43,     1] loss: 0.231
[44,     1] loss: 0.198
[45,     1] loss: 0.193
[46,     1] loss: 0.167
[47,     1] loss: 0.219
[48,     1] loss: 0.177
[49,     1] loss: 0.169
[50,     1] loss: 0.208
[51,     1] loss: 0.190
[52,     1] loss: 0.127
[53,     1] loss: 0.163
[54,     1] loss: 0.166
[55,     1] loss: 0.144
[56,     1] loss: 0.185
[57,     1] loss: 0.153
[58,     1] loss: 0.152
[59,     1] loss: 0.166
[60,     1] loss: 0.120
[61,     1] loss: 0.291
[62,     1] loss: 0.164
[63,     1] loss: 0.250
[64,     1] loss: 0.292
[65,     1] loss: 0.207
[66,     1] loss: 0.159
[67,     1] loss: 0.277
[68,     1] loss: 0.129
[69,     1] loss: 0.234
[70,     1] loss: 0.205
[71,     1] loss: 0.172
[72,     1] loss: 0.171
[73,     1] loss: 0.163
[74,     1] loss: 0.171
[75,     1] loss: 0.191
[76,     1] loss: 0.195
[77,     1] loss: 0.146
[78,     1] loss: 0.143
[79,     1] loss: 0.155
[80,     1] loss: 0.134
[81,     1] loss: 0.159
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006824637092307982,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3007966218,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.224939958776347}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.694
[3,     1] loss: 0.679
[4,     1] loss: 0.645
[5,     1] loss: 0.599
[6,     1] loss: 0.532
[7,     1] loss: 0.496
[8,     1] loss: 0.485
[9,     1] loss: 0.450
[10,     1] loss: 0.439
[11,     1] loss: 0.401
[12,     1] loss: 0.417
[13,     1] loss: 0.367
[14,     1] loss: 0.302
[15,     1] loss: 0.324
[16,     1] loss: 0.329
[17,     1] loss: 0.290
[18,     1] loss: 0.273
[19,     1] loss: 0.366
[20,     1] loss: 0.303
[21,     1] loss: 0.291
[22,     1] loss: 0.262
[23,     1] loss: 0.299
[24,     1] loss: 0.218
[25,     1] loss: 0.311
[26,     1] loss: 0.276
[27,     1] loss: 0.203
[28,     1] loss: 0.223
[29,     1] loss: 0.223
[30,     1] loss: 0.204
[31,     1] loss: 0.149
[32,     1] loss: 0.143
[33,     1] loss: 0.172
[34,     1] loss: 0.143
[35,     1] loss: 0.188
[36,     1] loss: 0.176
[37,     1] loss: 0.130
[38,     1] loss: 0.374
[39,     1] loss: 0.114
[40,     1] loss: 0.241
[41,     1] loss: 0.143
[42,     1] loss: 0.208
[43,     1] loss: 0.170
[44,     1] loss: 0.146
[45,     1] loss: 0.123
[46,     1] loss: 0.181
[47,     1] loss: 0.166
[48,     1] loss: 0.145
[49,     1] loss: 0.133
[50,     1] loss: 0.107
[51,     1] loss: 0.106
[52,     1] loss: 0.133
[53,     1] loss: 0.116
[54,     1] loss: 0.098
[55,     1] loss: 0.107
[56,     1] loss: 0.407
[57,     1] loss: 0.237
[58,     1] loss: 0.266
[59,     1] loss: 0.135
[60,     1] loss: 0.142
[61,     1] loss: 0.178
[62,     1] loss: 0.130
[63,     1] loss: 0.143
[64,     1] loss: 0.147
[65,     1] loss: 0.136
[66,     1] loss: 0.114
[67,     1] loss: 0.152
[68,     1] loss: 0.105
[69,     1] loss: 0.086
[70,     1] loss: 0.128
[71,     1] loss: 0.116
[72,     1] loss: 0.162
[73,     1] loss: 0.077
[74,     1] loss: 0.111
[75,     1] loss: 0.124
Early stopping applied (best metric=0.3059980571269989)
Finished Training
Total time taken: 8.297614336013794
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.687
[3,     1] loss: 0.686
[4,     1] loss: 0.649
[5,     1] loss: 0.600
[6,     1] loss: 0.544
[7,     1] loss: 0.564
[8,     1] loss: 0.511
[9,     1] loss: 0.467
[10,     1] loss: 0.491
[11,     1] loss: 0.402
[12,     1] loss: 0.358
[13,     1] loss: 0.441
[14,     1] loss: 0.356
[15,     1] loss: 0.389
[16,     1] loss: 0.439
[17,     1] loss: 0.320
[18,     1] loss: 0.307
[19,     1] loss: 0.325
[20,     1] loss: 0.273
[21,     1] loss: 0.310
[22,     1] loss: 0.201
[23,     1] loss: 0.231
[24,     1] loss: 0.264
[25,     1] loss: 0.238
[26,     1] loss: 0.227
[27,     1] loss: 0.203
[28,     1] loss: 0.169
[29,     1] loss: 0.164
[30,     1] loss: 0.175
[31,     1] loss: 0.136
[32,     1] loss: 0.090
[33,     1] loss: 0.253
[34,     1] loss: 0.137
[35,     1] loss: 0.193
[36,     1] loss: 0.262
[37,     1] loss: 0.188
[38,     1] loss: 0.114
[39,     1] loss: 0.141
[40,     1] loss: 0.147
[41,     1] loss: 0.125
[42,     1] loss: 0.138
[43,     1] loss: 0.099
[44,     1] loss: 0.093
[45,     1] loss: 0.167
[46,     1] loss: 0.111
[47,     1] loss: 0.116
[48,     1] loss: 0.077
[49,     1] loss: 0.113
[50,     1] loss: 0.156
[51,     1] loss: 0.123
[52,     1] loss: 0.234
[53,     1] loss: 0.182
[54,     1] loss: 0.312
[55,     1] loss: 0.196
[56,     1] loss: 0.188
[57,     1] loss: 0.258
[58,     1] loss: 0.174
[59,     1] loss: 0.198
[60,     1] loss: 0.228
[61,     1] loss: 0.179
[62,     1] loss: 0.165
[63,     1] loss: 0.157
[64,     1] loss: 0.142
[65,     1] loss: 0.100
[66,     1] loss: 0.100
Early stopping applied (best metric=0.16948136687278748)
Finished Training
Total time taken: 7.28800106048584
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.685
[3,     1] loss: 0.664
[4,     1] loss: 0.616
[5,     1] loss: 0.577
[6,     1] loss: 0.522
[7,     1] loss: 0.469
[8,     1] loss: 0.394
[9,     1] loss: 0.335
[10,     1] loss: 0.247
[11,     1] loss: 0.334
[12,     1] loss: 0.318
[13,     1] loss: 0.290
[14,     1] loss: 0.225
[15,     1] loss: 0.267
[16,     1] loss: 0.242
[17,     1] loss: 0.254
[18,     1] loss: 0.271
[19,     1] loss: 0.200
[20,     1] loss: 0.281
[21,     1] loss: 0.160
[22,     1] loss: 0.234
[23,     1] loss: 0.222
[24,     1] loss: 0.199
[25,     1] loss: 0.190
[26,     1] loss: 0.222
[27,     1] loss: 0.188
[28,     1] loss: 0.256
[29,     1] loss: 0.202
[30,     1] loss: 0.154
[31,     1] loss: 0.156
[32,     1] loss: 0.153
[33,     1] loss: 0.173
[34,     1] loss: 0.116
[35,     1] loss: 0.173
[36,     1] loss: 0.115
[37,     1] loss: 0.141
[38,     1] loss: 0.149
[39,     1] loss: 0.191
[40,     1] loss: 0.173
[41,     1] loss: 0.186
[42,     1] loss: 0.189
[43,     1] loss: 0.124
[44,     1] loss: 0.189
[45,     1] loss: 0.128
[46,     1] loss: 0.202
[47,     1] loss: 0.151
[48,     1] loss: 0.135
[49,     1] loss: 0.169
[50,     1] loss: 0.142
[51,     1] loss: 0.099
[52,     1] loss: 0.154
[53,     1] loss: 0.103
[54,     1] loss: 0.200
[55,     1] loss: 0.110
[56,     1] loss: 0.139
[57,     1] loss: 0.076
[58,     1] loss: 0.075
[59,     1] loss: 0.064
[60,     1] loss: 0.060
[61,     1] loss: 0.119
[62,     1] loss: 0.123
[63,     1] loss: 0.184
[64,     1] loss: 0.120
[65,     1] loss: 0.248
[66,     1] loss: 0.151
[67,     1] loss: 0.311
[68,     1] loss: 0.150
[69,     1] loss: 0.088
[70,     1] loss: 0.158
[71,     1] loss: 0.103
[72,     1] loss: 0.139
[73,     1] loss: 0.137
[74,     1] loss: 0.114
[75,     1] loss: 0.126
[76,     1] loss: 0.088
[77,     1] loss: 0.095
[78,     1] loss: 0.070
[79,     1] loss: 0.068
[80,     1] loss: 0.048
[81,     1] loss: 0.077
[82,     1] loss: 0.073
[83,     1] loss: 0.057
[84,     1] loss: 0.135
[85,     1] loss: 0.203
[86,     1] loss: 0.332
[87,     1] loss: 0.280
[88,     1] loss: 0.262
[89,     1] loss: 0.271
[90,     1] loss: 0.181
[91,     1] loss: 0.219
[92,     1] loss: 0.216
[93,     1] loss: 0.230
[94,     1] loss: 0.235
[95,     1] loss: 0.226
[96,     1] loss: 0.209
[97,     1] loss: 0.224
[98,     1] loss: 0.184
[99,     1] loss: 0.182
[100,     1] loss: 0.193
[101,     1] loss: 0.130
[102,     1] loss: 0.159
[103,     1] loss: 0.182
[104,     1] loss: 0.165
[105,     1] loss: 0.200
[106,     1] loss: 0.148
[107,     1] loss: 0.188
[108,     1] loss: 0.142
[109,     1] loss: 0.119
[110,     1] loss: 0.131
[111,     1] loss: 0.093
[112,     1] loss: 0.103
[113,     1] loss: 0.093
[114,     1] loss: 0.081
[115,     1] loss: 0.063
[116,     1] loss: 0.086
[117,     1] loss: 0.121
[118,     1] loss: 0.090
[119,     1] loss: 0.140
[120,     1] loss: 0.114
[121,     1] loss: 0.086
[122,     1] loss: 0.122
[123,     1] loss: 0.121
[124,     1] loss: 0.064
[125,     1] loss: 0.122
[126,     1] loss: 0.109
[127,     1] loss: 0.215
[128,     1] loss: 0.085
[129,     1] loss: 0.087
[130,     1] loss: 0.130
[131,     1] loss: 0.104
[132,     1] loss: 0.114
[133,     1] loss: 0.085
[134,     1] loss: 0.112
[135,     1] loss: 0.068
[136,     1] loss: 0.082
[137,     1] loss: 0.112
[138,     1] loss: 0.102
[139,     1] loss: 0.125
[140,     1] loss: 0.072
[141,     1] loss: 0.168
[142,     1] loss: 0.125
[143,     1] loss: 0.298
[144,     1] loss: 0.109
[145,     1] loss: 0.148
[146,     1] loss: 0.167
[147,     1] loss: 0.126
[148,     1] loss: 0.100
[149,     1] loss: 0.177
[150,     1] loss: 0.100
[151,     1] loss: 0.114
[152,     1] loss: 0.125
[153,     1] loss: 0.107
[154,     1] loss: 0.082
[155,     1] loss: 0.091
[156,     1] loss: 0.134
[157,     1] loss: 0.080
[158,     1] loss: 0.100
[159,     1] loss: 0.103
[160,     1] loss: 0.091
Early stopping applied (best metric=0.13592344522476196)
Finished Training
Total time taken: 17.560240030288696
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.682
[4,     1] loss: 0.664
[5,     1] loss: 0.636
[6,     1] loss: 0.580
[7,     1] loss: 0.524
[8,     1] loss: 0.471
[9,     1] loss: 0.490
[10,     1] loss: 0.395
[11,     1] loss: 0.407
[12,     1] loss: 0.424
[13,     1] loss: 0.460
[14,     1] loss: 0.322
[15,     1] loss: 0.437
[16,     1] loss: 0.400
[17,     1] loss: 0.315
[18,     1] loss: 0.293
[19,     1] loss: 0.364
[20,     1] loss: 0.292
[21,     1] loss: 0.286
[22,     1] loss: 0.292
[23,     1] loss: 0.333
[24,     1] loss: 0.248
[25,     1] loss: 0.216
[26,     1] loss: 0.226
[27,     1] loss: 0.245
[28,     1] loss: 0.176
[29,     1] loss: 0.239
[30,     1] loss: 0.296
[31,     1] loss: 0.210
[32,     1] loss: 0.236
[33,     1] loss: 0.245
[34,     1] loss: 0.249
[35,     1] loss: 0.165
[36,     1] loss: 0.201
[37,     1] loss: 0.122
[38,     1] loss: 0.148
[39,     1] loss: 0.236
[40,     1] loss: 0.288
[41,     1] loss: 0.133
[42,     1] loss: 0.182
[43,     1] loss: 0.154
[44,     1] loss: 0.126
[45,     1] loss: 0.169
[46,     1] loss: 0.200
[47,     1] loss: 0.091
[48,     1] loss: 0.121
[49,     1] loss: 0.152
[50,     1] loss: 0.122
[51,     1] loss: 0.134
[52,     1] loss: 0.120
[53,     1] loss: 0.090
[54,     1] loss: 0.079
[55,     1] loss: 0.079
[56,     1] loss: 0.082
[57,     1] loss: 0.159
[58,     1] loss: 0.102
[59,     1] loss: 0.083
[60,     1] loss: 0.073
[61,     1] loss: 0.098
[62,     1] loss: 0.192
[63,     1] loss: 0.084
[64,     1] loss: 0.126
[65,     1] loss: 0.099
[66,     1] loss: 0.102
[67,     1] loss: 0.089
[68,     1] loss: 0.091
[69,     1] loss: 0.072
[70,     1] loss: 0.156
[71,     1] loss: 0.096
[72,     1] loss: 0.089
[73,     1] loss: 0.091
[74,     1] loss: 0.076
[75,     1] loss: 0.102
Early stopping applied (best metric=0.209026038646698)
Finished Training
Total time taken: 8.22800064086914
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.697
[3,     1] loss: 0.683
[4,     1] loss: 0.667
[5,     1] loss: 0.631
[6,     1] loss: 0.610
[7,     1] loss: 0.579
[8,     1] loss: 0.559
[9,     1] loss: 0.501
[10,     1] loss: 0.447
[11,     1] loss: 0.407
[12,     1] loss: 0.374
[13,     1] loss: 0.397
[14,     1] loss: 0.378
[15,     1] loss: 0.302
[16,     1] loss: 0.327
[17,     1] loss: 0.370
[18,     1] loss: 0.252
[19,     1] loss: 0.307
[20,     1] loss: 0.326
[21,     1] loss: 0.299
[22,     1] loss: 0.300
[23,     1] loss: 0.232
[24,     1] loss: 0.213
[25,     1] loss: 0.211
[26,     1] loss: 0.192
[27,     1] loss: 0.192
[28,     1] loss: 0.185
[29,     1] loss: 0.190
[30,     1] loss: 0.152
[31,     1] loss: 0.211
[32,     1] loss: 0.115
[33,     1] loss: 0.191
[34,     1] loss: 0.309
[35,     1] loss: 0.195
[36,     1] loss: 0.203
[37,     1] loss: 0.312
[38,     1] loss: 0.287
[39,     1] loss: 0.222
[40,     1] loss: 0.278
[41,     1] loss: 0.234
[42,     1] loss: 0.215
[43,     1] loss: 0.255
[44,     1] loss: 0.210
[45,     1] loss: 0.237
[46,     1] loss: 0.197
[47,     1] loss: 0.175
[48,     1] loss: 0.200
[49,     1] loss: 0.144
[50,     1] loss: 0.215
[51,     1] loss: 0.176
[52,     1] loss: 0.175
[53,     1] loss: 0.181
[54,     1] loss: 0.219
[55,     1] loss: 0.207
[56,     1] loss: 0.236
[57,     1] loss: 0.177
[58,     1] loss: 0.195
[59,     1] loss: 0.208
Early stopping applied (best metric=0.4166775345802307)
Finished Training
Total time taken: 6.5116050243377686
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.699
[2,     1] loss: 0.693
[3,     1] loss: 0.684
[4,     1] loss: 0.665
[5,     1] loss: 0.627
[6,     1] loss: 0.610
[7,     1] loss: 0.550
[8,     1] loss: 0.506
[9,     1] loss: 0.423
[10,     1] loss: 0.374
[11,     1] loss: 0.369
[12,     1] loss: 0.331
[13,     1] loss: 0.294
[14,     1] loss: 0.280
[15,     1] loss: 0.223
[16,     1] loss: 0.355
[17,     1] loss: 0.241
[18,     1] loss: 0.325
[19,     1] loss: 0.238
[20,     1] loss: 0.321
[21,     1] loss: 0.218
[22,     1] loss: 0.308
[23,     1] loss: 0.285
[24,     1] loss: 0.255
[25,     1] loss: 0.332
[26,     1] loss: 0.214
[27,     1] loss: 0.212
[28,     1] loss: 0.225
[29,     1] loss: 0.236
[30,     1] loss: 0.232
[31,     1] loss: 0.260
[32,     1] loss: 0.274
[33,     1] loss: 0.201
[34,     1] loss: 0.248
[35,     1] loss: 0.244
[36,     1] loss: 0.166
[37,     1] loss: 0.163
[38,     1] loss: 0.240
[39,     1] loss: 0.246
[40,     1] loss: 0.204
[41,     1] loss: 0.194
[42,     1] loss: 0.183
[43,     1] loss: 0.135
[44,     1] loss: 0.234
[45,     1] loss: 0.174
[46,     1] loss: 0.174
[47,     1] loss: 0.151
[48,     1] loss: 0.256
[49,     1] loss: 0.184
[50,     1] loss: 0.214
[51,     1] loss: 0.150
[52,     1] loss: 0.173
[53,     1] loss: 0.158
[54,     1] loss: 0.145
[55,     1] loss: 0.158
[56,     1] loss: 0.088
[57,     1] loss: 0.092
[58,     1] loss: 0.093
[59,     1] loss: 0.085
Early stopping applied (best metric=0.299017995595932)
Finished Training
Total time taken: 6.509613037109375
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.685
[3,     1] loss: 0.660
[4,     1] loss: 0.631
[5,     1] loss: 0.602
[6,     1] loss: 0.552
[7,     1] loss: 0.507
[8,     1] loss: 0.455
[9,     1] loss: 0.451
[10,     1] loss: 0.442
[11,     1] loss: 0.422
[12,     1] loss: 0.383
[13,     1] loss: 0.539
[14,     1] loss: 0.378
[15,     1] loss: 0.361
[16,     1] loss: 0.356
[17,     1] loss: 0.367
[18,     1] loss: 0.358
[19,     1] loss: 0.287
[20,     1] loss: 0.315
[21,     1] loss: 0.306
[22,     1] loss: 0.258
[23,     1] loss: 0.279
[24,     1] loss: 0.245
[25,     1] loss: 0.213
[26,     1] loss: 0.231
[27,     1] loss: 0.274
[28,     1] loss: 0.191
[29,     1] loss: 0.195
[30,     1] loss: 0.174
[31,     1] loss: 0.191
[32,     1] loss: 0.184
[33,     1] loss: 0.173
[34,     1] loss: 0.171
[35,     1] loss: 0.203
[36,     1] loss: 0.160
[37,     1] loss: 0.180
[38,     1] loss: 0.183
[39,     1] loss: 0.172
[40,     1] loss: 0.158
[41,     1] loss: 0.175
[42,     1] loss: 0.196
[43,     1] loss: 0.192
[44,     1] loss: 0.158
[45,     1] loss: 0.154
[46,     1] loss: 0.138
[47,     1] loss: 0.116
[48,     1] loss: 0.199
[49,     1] loss: 0.230
[50,     1] loss: 0.142
[51,     1] loss: 0.213
[52,     1] loss: 0.150
[53,     1] loss: 0.104
[54,     1] loss: 0.242
[55,     1] loss: 0.120
[56,     1] loss: 0.148
[57,     1] loss: 0.149
[58,     1] loss: 0.138
[59,     1] loss: 0.136
[60,     1] loss: 0.123
[61,     1] loss: 0.373
[62,     1] loss: 0.388
[63,     1] loss: 0.258
Early stopping applied (best metric=0.26642629504203796)
Finished Training
Total time taken: 6.931999444961548
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.693
[3,     1] loss: 0.681
[4,     1] loss: 0.656
[5,     1] loss: 0.633
[6,     1] loss: 0.599
[7,     1] loss: 0.538
[8,     1] loss: 0.532
[9,     1] loss: 0.461
[10,     1] loss: 0.417
[11,     1] loss: 0.393
[12,     1] loss: 0.404
[13,     1] loss: 0.335
[14,     1] loss: 0.376
[15,     1] loss: 0.317
[16,     1] loss: 0.313
[17,     1] loss: 0.290
[18,     1] loss: 0.236
[19,     1] loss: 0.299
[20,     1] loss: 0.266
[21,     1] loss: 0.276
[22,     1] loss: 0.223
[23,     1] loss: 0.227
[24,     1] loss: 0.289
[25,     1] loss: 0.258
[26,     1] loss: 0.187
[27,     1] loss: 0.272
[28,     1] loss: 0.163
[29,     1] loss: 0.214
[30,     1] loss: 0.224
[31,     1] loss: 0.191
[32,     1] loss: 0.238
[33,     1] loss: 0.246
[34,     1] loss: 0.277
[35,     1] loss: 0.222
[36,     1] loss: 0.210
[37,     1] loss: 0.257
[38,     1] loss: 0.195
[39,     1] loss: 0.210
[40,     1] loss: 0.217
[41,     1] loss: 0.185
[42,     1] loss: 0.195
[43,     1] loss: 0.229
[44,     1] loss: 0.184
[45,     1] loss: 0.216
[46,     1] loss: 0.201
[47,     1] loss: 0.190
[48,     1] loss: 0.162
[49,     1] loss: 0.175
[50,     1] loss: 0.142
[51,     1] loss: 0.228
[52,     1] loss: 0.157
[53,     1] loss: 0.151
[54,     1] loss: 0.214
[55,     1] loss: 0.216
[56,     1] loss: 0.238
[57,     1] loss: 0.147
[58,     1] loss: 0.188
[59,     1] loss: 0.210
[60,     1] loss: 0.190
[61,     1] loss: 0.161
Early stopping applied (best metric=0.35696277022361755)
Finished Training
Total time taken: 6.728999614715576
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.706
[2,     1] loss: 0.698
[3,     1] loss: 0.694
[4,     1] loss: 0.686
[5,     1] loss: 0.682
[6,     1] loss: 0.678
[7,     1] loss: 0.663
[8,     1] loss: 0.646
[9,     1] loss: 0.615
[10,     1] loss: 0.585
[11,     1] loss: 0.558
[12,     1] loss: 0.532
[13,     1] loss: 0.526
[14,     1] loss: 0.446
[15,     1] loss: 0.500
[16,     1] loss: 0.457
[17,     1] loss: 0.427
[18,     1] loss: 0.419
[19,     1] loss: 0.361
[20,     1] loss: 0.386
[21,     1] loss: 0.345
[22,     1] loss: 0.334
[23,     1] loss: 0.410
[24,     1] loss: 0.392
[25,     1] loss: 0.260
[26,     1] loss: 0.318
[27,     1] loss: 0.256
[28,     1] loss: 0.351
[29,     1] loss: 0.283
[30,     1] loss: 0.275
[31,     1] loss: 0.227
[32,     1] loss: 0.248
[33,     1] loss: 0.244
[34,     1] loss: 0.259
[35,     1] loss: 0.203
[36,     1] loss: 0.194
[37,     1] loss: 0.163
[38,     1] loss: 0.200
[39,     1] loss: 0.180
[40,     1] loss: 0.150
[41,     1] loss: 0.137
[42,     1] loss: 0.153
[43,     1] loss: 0.124
[44,     1] loss: 0.127
[45,     1] loss: 0.150
[46,     1] loss: 0.462
[47,     1] loss: 0.252
[48,     1] loss: 0.365
[49,     1] loss: 0.210
[50,     1] loss: 0.244
[51,     1] loss: 0.285
[52,     1] loss: 0.226
[53,     1] loss: 0.226
[54,     1] loss: 0.218
[55,     1] loss: 0.183
[56,     1] loss: 0.215
[57,     1] loss: 0.208
[58,     1] loss: 0.182
[59,     1] loss: 0.168
[60,     1] loss: 0.135
[61,     1] loss: 0.181
[62,     1] loss: 0.174
[63,     1] loss: 0.185
[64,     1] loss: 0.133
[65,     1] loss: 0.121
[66,     1] loss: 0.170
[67,     1] loss: 0.098
[68,     1] loss: 0.154
[69,     1] loss: 0.126
[70,     1] loss: 0.115
Early stopping applied (best metric=0.33993643522262573)
Finished Training
Total time taken: 7.701000690460205
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.694
[3,     1] loss: 0.678
[4,     1] loss: 0.659
[5,     1] loss: 0.620
[6,     1] loss: 0.583
[7,     1] loss: 0.548
[8,     1] loss: 0.510
[9,     1] loss: 0.517
[10,     1] loss: 0.456
[11,     1] loss: 0.523
[12,     1] loss: 0.460
[13,     1] loss: 0.457
[14,     1] loss: 0.390
[15,     1] loss: 0.409
[16,     1] loss: 0.400
[17,     1] loss: 0.365
[18,     1] loss: 0.326
[19,     1] loss: 0.377
[20,     1] loss: 0.376
[21,     1] loss: 0.317
[22,     1] loss: 0.289
[23,     1] loss: 0.255
[24,     1] loss: 0.269
[25,     1] loss: 0.301
[26,     1] loss: 0.271
[27,     1] loss: 0.279
[28,     1] loss: 0.170
[29,     1] loss: 0.181
[30,     1] loss: 0.179
[31,     1] loss: 0.193
[32,     1] loss: 0.166
[33,     1] loss: 0.147
[34,     1] loss: 0.171
[35,     1] loss: 0.182
[36,     1] loss: 0.157
[37,     1] loss: 0.207
[38,     1] loss: 0.237
[39,     1] loss: 0.160
[40,     1] loss: 0.226
[41,     1] loss: 0.144
[42,     1] loss: 0.177
[43,     1] loss: 0.202
[44,     1] loss: 0.163
[45,     1] loss: 0.183
[46,     1] loss: 0.113
[47,     1] loss: 0.155
[48,     1] loss: 0.135
[49,     1] loss: 0.143
[50,     1] loss: 0.172
[51,     1] loss: 0.108
[52,     1] loss: 0.143
[53,     1] loss: 0.141
[54,     1] loss: 0.151
[55,     1] loss: 0.183
[56,     1] loss: 0.203
[57,     1] loss: 0.171
[58,     1] loss: 0.147
[59,     1] loss: 0.142
[60,     1] loss: 0.180
[61,     1] loss: 0.142
[62,     1] loss: 0.121
[63,     1] loss: 0.134
[64,     1] loss: 0.102
[65,     1] loss: 0.067
[66,     1] loss: 0.115
[67,     1] loss: 0.130
[68,     1] loss: 0.181
[69,     1] loss: 0.094
[70,     1] loss: 0.084
[71,     1] loss: 0.110
[72,     1] loss: 0.087
[73,     1] loss: 0.113
[74,     1] loss: 0.177
[75,     1] loss: 0.183
[76,     1] loss: 0.120
[77,     1] loss: 0.152
[78,     1] loss: 0.086
[79,     1] loss: 0.150
[80,     1] loss: 0.124
[81,     1] loss: 0.109
[82,     1] loss: 0.117
[83,     1] loss: 0.129
[84,     1] loss: 0.148
[85,     1] loss: 0.122
[86,     1] loss: 0.098
[87,     1] loss: 0.101
[88,     1] loss: 0.081
[89,     1] loss: 0.088
[90,     1] loss: 0.073
[91,     1] loss: 0.154
[92,     1] loss: 0.271
[93,     1] loss: 0.125
[94,     1] loss: 0.096
[95,     1] loss: 0.131
[96,     1] loss: 0.089
[97,     1] loss: 0.132
[98,     1] loss: 0.133
[99,     1] loss: 0.109
[100,     1] loss: 0.105
[101,     1] loss: 0.125
[102,     1] loss: 0.206
[103,     1] loss: 0.196
[104,     1] loss: 0.136
[105,     1] loss: 0.128
[106,     1] loss: 0.095
[107,     1] loss: 0.111
[108,     1] loss: 0.129
[109,     1] loss: 0.111
Early stopping applied (best metric=0.25003132224082947)
Finished Training
Total time taken: 11.978598356246948
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.692
[3,     1] loss: 0.672
[4,     1] loss: 0.635
[5,     1] loss: 0.579
[6,     1] loss: 0.536
[7,     1] loss: 0.506
[8,     1] loss: 0.452
[9,     1] loss: 0.402
[10,     1] loss: 0.388
[11,     1] loss: 0.386
[12,     1] loss: 0.330
[13,     1] loss: 0.425
[14,     1] loss: 0.354
[15,     1] loss: 0.246
[16,     1] loss: 0.277
[17,     1] loss: 0.284
[18,     1] loss: 0.249
[19,     1] loss: 0.196
[20,     1] loss: 0.197
[21,     1] loss: 0.194
[22,     1] loss: 0.236
[23,     1] loss: 0.205
[24,     1] loss: 0.214
[25,     1] loss: 0.134
[26,     1] loss: 0.200
[27,     1] loss: 0.179
[28,     1] loss: 0.095
[29,     1] loss: 0.147
[30,     1] loss: 0.173
[31,     1] loss: 0.207
[32,     1] loss: 0.137
[33,     1] loss: 0.203
[34,     1] loss: 0.169
[35,     1] loss: 0.112
[36,     1] loss: 0.168
[37,     1] loss: 0.140
[38,     1] loss: 0.162
[39,     1] loss: 0.110
[40,     1] loss: 0.152
[41,     1] loss: 0.146
[42,     1] loss: 0.104
[43,     1] loss: 0.126
[44,     1] loss: 0.118
[45,     1] loss: 0.162
[46,     1] loss: 0.130
[47,     1] loss: 0.297
[48,     1] loss: 0.143
[49,     1] loss: 0.237
[50,     1] loss: 0.227
[51,     1] loss: 0.168
[52,     1] loss: 0.277
[53,     1] loss: 0.123
[54,     1] loss: 0.169
[55,     1] loss: 0.216
[56,     1] loss: 0.139
[57,     1] loss: 0.145
[58,     1] loss: 0.163
[59,     1] loss: 0.121
[60,     1] loss: 0.114
[61,     1] loss: 0.129
[62,     1] loss: 0.184
Early stopping applied (best metric=0.35898643732070923)
Finished Training
Total time taken: 6.845000267028809
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.694
[3,     1] loss: 0.681
[4,     1] loss: 0.657
[5,     1] loss: 0.619
[6,     1] loss: 0.579
[7,     1] loss: 0.544
[8,     1] loss: 0.476
[9,     1] loss: 0.468
[10,     1] loss: 0.369
[11,     1] loss: 0.343
[12,     1] loss: 0.324
[13,     1] loss: 0.327
[14,     1] loss: 0.398
[15,     1] loss: 0.296
[16,     1] loss: 0.360
[17,     1] loss: 0.314
[18,     1] loss: 0.352
[19,     1] loss: 0.290
[20,     1] loss: 0.292
[21,     1] loss: 0.288
[22,     1] loss: 0.259
[23,     1] loss: 0.238
[24,     1] loss: 0.359
[25,     1] loss: 0.325
[26,     1] loss: 0.285
[27,     1] loss: 0.309
[28,     1] loss: 0.267
[29,     1] loss: 0.310
[30,     1] loss: 0.208
[31,     1] loss: 0.308
[32,     1] loss: 0.274
[33,     1] loss: 0.240
[34,     1] loss: 0.193
[35,     1] loss: 0.215
[36,     1] loss: 0.209
[37,     1] loss: 0.232
[38,     1] loss: 0.215
[39,     1] loss: 0.197
[40,     1] loss: 0.220
[41,     1] loss: 0.212
[42,     1] loss: 0.195
[43,     1] loss: 0.188
[44,     1] loss: 0.209
[45,     1] loss: 0.212
[46,     1] loss: 0.145
[47,     1] loss: 0.171
[48,     1] loss: 0.154
[49,     1] loss: 0.162
[50,     1] loss: 0.143
[51,     1] loss: 0.154
[52,     1] loss: 0.184
[53,     1] loss: 0.177
[54,     1] loss: 0.229
[55,     1] loss: 0.157
[56,     1] loss: 0.220
[57,     1] loss: 0.150
[58,     1] loss: 0.152
[59,     1] loss: 0.164
[60,     1] loss: 0.179
[61,     1] loss: 0.150
[62,     1] loss: 0.148
[63,     1] loss: 0.235
[64,     1] loss: 0.321
[65,     1] loss: 0.421
[66,     1] loss: 0.234
[67,     1] loss: 0.374
[68,     1] loss: 0.243
[69,     1] loss: 0.288
[70,     1] loss: 0.238
[71,     1] loss: 0.266
[72,     1] loss: 0.236
[73,     1] loss: 0.219
[74,     1] loss: 0.233
[75,     1] loss: 0.191
[76,     1] loss: 0.179
[77,     1] loss: 0.168
[78,     1] loss: 0.152
[79,     1] loss: 0.153
[80,     1] loss: 0.153
[81,     1] loss: 0.089
[82,     1] loss: 0.125
[83,     1] loss: 0.103
[84,     1] loss: 0.128
[85,     1] loss: 0.089
[86,     1] loss: 0.158
[87,     1] loss: 0.123
[88,     1] loss: 0.125
[89,     1] loss: 0.196
[90,     1] loss: 0.078
[91,     1] loss: 0.197
[92,     1] loss: 0.111
[93,     1] loss: 0.188
[94,     1] loss: 0.098
[95,     1] loss: 0.155
[96,     1] loss: 0.143
[97,     1] loss: 0.128
[98,     1] loss: 0.126
[99,     1] loss: 0.167
[100,     1] loss: 0.130
[101,     1] loss: 0.158
[102,     1] loss: 0.104
[103,     1] loss: 0.108
[104,     1] loss: 0.111
[105,     1] loss: 0.097
[106,     1] loss: 0.078
[107,     1] loss: 0.101
[108,     1] loss: 0.075
[109,     1] loss: 0.072
Early stopping applied (best metric=0.36675500869750977)
Finished Training
Total time taken: 11.940593719482422
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.701
[3,     1] loss: 0.695
[4,     1] loss: 0.686
[5,     1] loss: 0.676
[6,     1] loss: 0.660
[7,     1] loss: 0.634
[8,     1] loss: 0.589
[9,     1] loss: 0.543
[10,     1] loss: 0.528
[11,     1] loss: 0.457
[12,     1] loss: 0.403
[13,     1] loss: 0.387
[14,     1] loss: 0.356
[15,     1] loss: 0.380
[16,     1] loss: 0.312
[17,     1] loss: 0.324
[18,     1] loss: 0.347
[19,     1] loss: 0.340
[20,     1] loss: 0.310
[21,     1] loss: 0.274
[22,     1] loss: 0.334
[23,     1] loss: 0.251
[24,     1] loss: 0.354
[25,     1] loss: 0.285
[26,     1] loss: 0.274
[27,     1] loss: 0.276
[28,     1] loss: 0.273
[29,     1] loss: 0.330
[30,     1] loss: 0.278
[31,     1] loss: 0.300
[32,     1] loss: 0.254
[33,     1] loss: 0.243
[34,     1] loss: 0.220
[35,     1] loss: 0.206
[36,     1] loss: 0.215
[37,     1] loss: 0.176
[38,     1] loss: 0.156
[39,     1] loss: 0.206
[40,     1] loss: 0.201
[41,     1] loss: 0.173
[42,     1] loss: 0.151
[43,     1] loss: 0.173
[44,     1] loss: 0.180
[45,     1] loss: 0.158
[46,     1] loss: 0.291
[47,     1] loss: 0.166
[48,     1] loss: 0.246
[49,     1] loss: 0.176
[50,     1] loss: 0.191
[51,     1] loss: 0.218
[52,     1] loss: 0.167
[53,     1] loss: 0.144
[54,     1] loss: 0.191
[55,     1] loss: 0.149
[56,     1] loss: 0.105
[57,     1] loss: 0.159
[58,     1] loss: 0.146
[59,     1] loss: 0.119
[60,     1] loss: 0.252
[61,     1] loss: 0.254
[62,     1] loss: 0.160
[63,     1] loss: 0.236
[64,     1] loss: 0.157
[65,     1] loss: 0.185
[66,     1] loss: 0.175
[67,     1] loss: 0.188
[68,     1] loss: 0.204
[69,     1] loss: 0.138
[70,     1] loss: 0.116
[71,     1] loss: 0.136
[72,     1] loss: 0.128
[73,     1] loss: 0.123
[74,     1] loss: 0.100
[75,     1] loss: 0.194
[76,     1] loss: 0.157
[77,     1] loss: 0.231
[78,     1] loss: 0.107
[79,     1] loss: 0.244
[80,     1] loss: 0.129
[81,     1] loss: 0.154
[82,     1] loss: 0.195
[83,     1] loss: 0.129
[84,     1] loss: 0.123
[85,     1] loss: 0.130
[86,     1] loss: 0.138
[87,     1] loss: 0.121
[88,     1] loss: 0.107
[89,     1] loss: 0.122
[90,     1] loss: 0.077
[91,     1] loss: 0.098
[92,     1] loss: 0.114
[93,     1] loss: 0.089
[94,     1] loss: 0.149
[95,     1] loss: 0.096
[96,     1] loss: 0.116
[97,     1] loss: 0.068
[98,     1] loss: 0.107
[99,     1] loss: 0.077
[100,     1] loss: 0.145
[101,     1] loss: 0.068
[102,     1] loss: 0.073
[103,     1] loss: 0.075
[104,     1] loss: 0.084
[105,     1] loss: 0.104
[106,     1] loss: 0.172
[107,     1] loss: 0.238
[108,     1] loss: 0.205
[109,     1] loss: 0.167
[110,     1] loss: 0.132
[111,     1] loss: 0.188
[112,     1] loss: 0.118
[113,     1] loss: 0.139
[114,     1] loss: 0.180
[115,     1] loss: 0.177
[116,     1] loss: 0.127
[117,     1] loss: 0.178
[118,     1] loss: 0.113
[119,     1] loss: 0.177
[120,     1] loss: 0.089
[121,     1] loss: 0.086
[122,     1] loss: 0.085
[123,     1] loss: 0.102
[124,     1] loss: 0.064
[125,     1] loss: 0.100
[126,     1] loss: 0.062
[127,     1] loss: 0.108
[128,     1] loss: 0.053
[129,     1] loss: 0.086
[130,     1] loss: 0.068
[131,     1] loss: 0.063
[132,     1] loss: 0.119
[133,     1] loss: 0.183
[134,     1] loss: 0.078
[135,     1] loss: 0.112
[136,     1] loss: 0.071
[137,     1] loss: 0.074
[138,     1] loss: 0.116
[139,     1] loss: 0.080
[140,     1] loss: 0.092
[141,     1] loss: 0.111
[142,     1] loss: 0.103
[143,     1] loss: 0.104
[144,     1] loss: 0.095
[145,     1] loss: 0.088
[146,     1] loss: 0.169
[147,     1] loss: 0.151
[148,     1] loss: 0.069
[149,     1] loss: 0.114
[150,     1] loss: 0.097
[151,     1] loss: 0.119
[152,     1] loss: 0.080
[153,     1] loss: 0.156
[154,     1] loss: 0.098
[155,     1] loss: 0.099
[156,     1] loss: 0.063
[157,     1] loss: 0.121
[158,     1] loss: 0.068
[159,     1] loss: 0.095
[160,     1] loss: 0.061
[161,     1] loss: 0.069
[162,     1] loss: 0.053
[163,     1] loss: 0.062
[164,     1] loss: 0.071
[165,     1] loss: 0.128
[166,     1] loss: 0.125
[167,     1] loss: 0.229
[168,     1] loss: 0.169
[169,     1] loss: 0.101
[170,     1] loss: 0.216
[171,     1] loss: 0.107
[172,     1] loss: 0.217
[173,     1] loss: 0.138
[174,     1] loss: 0.190
[175,     1] loss: 0.099
[176,     1] loss: 0.100
[177,     1] loss: 0.156
[178,     1] loss: 0.108
[179,     1] loss: 0.124
[180,     1] loss: 0.082
Early stopping applied (best metric=0.08899933099746704)
Finished Training
Total time taken: 19.742817163467407
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.694
[3,     1] loss: 0.679
[4,     1] loss: 0.653
[5,     1] loss: 0.620
[6,     1] loss: 0.559
[7,     1] loss: 0.550
[8,     1] loss: 0.490
[9,     1] loss: 0.378
[10,     1] loss: 0.380
[11,     1] loss: 0.368
[12,     1] loss: 0.354
[13,     1] loss: 0.323
[14,     1] loss: 0.273
[15,     1] loss: 0.254
[16,     1] loss: 0.365
[17,     1] loss: 0.295
[18,     1] loss: 0.250
[19,     1] loss: 0.294
[20,     1] loss: 0.272
[21,     1] loss: 0.274
[22,     1] loss: 0.260
[23,     1] loss: 0.279
[24,     1] loss: 0.294
[25,     1] loss: 0.251
[26,     1] loss: 0.333
[27,     1] loss: 0.208
[28,     1] loss: 0.222
[29,     1] loss: 0.202
[30,     1] loss: 0.186
[31,     1] loss: 0.194
[32,     1] loss: 0.176
[33,     1] loss: 0.149
[34,     1] loss: 0.179
[35,     1] loss: 0.184
[36,     1] loss: 0.171
[37,     1] loss: 0.202
[38,     1] loss: 0.166
[39,     1] loss: 0.126
[40,     1] loss: 0.172
[41,     1] loss: 0.179
[42,     1] loss: 0.119
[43,     1] loss: 0.165
[44,     1] loss: 0.116
[45,     1] loss: 0.149
[46,     1] loss: 0.109
[47,     1] loss: 0.168
[48,     1] loss: 0.067
[49,     1] loss: 0.091
[50,     1] loss: 0.111
[51,     1] loss: 0.075
[52,     1] loss: 0.133
[53,     1] loss: 0.079
[54,     1] loss: 0.067
[55,     1] loss: 0.092
[56,     1] loss: 0.103
[57,     1] loss: 0.054
[58,     1] loss: 0.083
[59,     1] loss: 0.070
[60,     1] loss: 0.055
[61,     1] loss: 0.052
[62,     1] loss: 0.112
[63,     1] loss: 0.081
[64,     1] loss: 0.222
[65,     1] loss: 0.254
[66,     1] loss: 0.257
[67,     1] loss: 0.136
[68,     1] loss: 0.151
[69,     1] loss: 0.120
[70,     1] loss: 0.123
[71,     1] loss: 0.112
[72,     1] loss: 0.095
[73,     1] loss: 0.115
[74,     1] loss: 0.066
[75,     1] loss: 0.078
[76,     1] loss: 0.061
[77,     1] loss: 0.112
[78,     1] loss: 0.100
[79,     1] loss: 0.091
[80,     1] loss: 0.104
[81,     1] loss: 0.068
[82,     1] loss: 0.093
[83,     1] loss: 0.113
[84,     1] loss: 0.142
[85,     1] loss: 0.077
[86,     1] loss: 0.084
[87,     1] loss: 0.083
[88,     1] loss: 0.089
[89,     1] loss: 0.103
[90,     1] loss: 0.057
[91,     1] loss: 0.081
[92,     1] loss: 0.111
[93,     1] loss: 0.055
[94,     1] loss: 0.161
[95,     1] loss: 0.104
[96,     1] loss: 0.075
[97,     1] loss: 0.076
[98,     1] loss: 0.137
[99,     1] loss: 0.085
[100,     1] loss: 0.127
[101,     1] loss: 0.054
[102,     1] loss: 0.150
[103,     1] loss: 0.086
[104,     1] loss: 0.056
[105,     1] loss: 0.080
[106,     1] loss: 0.123
[107,     1] loss: 0.065
[108,     1] loss: 0.084
[109,     1] loss: 0.127
[110,     1] loss: 0.091
[111,     1] loss: 0.099
[112,     1] loss: 0.112
[113,     1] loss: 0.087
[114,     1] loss: 0.072
[115,     1] loss: 0.048
[116,     1] loss: 0.065
[117,     1] loss: 0.086
[118,     1] loss: 0.152
[119,     1] loss: 0.045
[120,     1] loss: 0.161
[121,     1] loss: 0.142
[122,     1] loss: 0.123
[123,     1] loss: 0.080
[124,     1] loss: 0.091
[125,     1] loss: 0.089
[126,     1] loss: 0.058
[127,     1] loss: 0.093
[128,     1] loss: 0.045
[129,     1] loss: 0.060
[130,     1] loss: 0.097
[131,     1] loss: 0.135
[132,     1] loss: 0.058
[133,     1] loss: 0.263
[134,     1] loss: 0.232
Early stopping applied (best metric=0.14661265909671783)
Finished Training
Total time taken: 14.705000638961792
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.696
[3,     1] loss: 0.692
[4,     1] loss: 0.681
[5,     1] loss: 0.671
[6,     1] loss: 0.641
[7,     1] loss: 0.617
[8,     1] loss: 0.580
[9,     1] loss: 0.545
[10,     1] loss: 0.528
[11,     1] loss: 0.485
[12,     1] loss: 0.453
[13,     1] loss: 0.411
[14,     1] loss: 0.444
[15,     1] loss: 0.425
[16,     1] loss: 0.296
[17,     1] loss: 0.353
[18,     1] loss: 0.351
[19,     1] loss: 0.267
[20,     1] loss: 0.378
[21,     1] loss: 0.316
[22,     1] loss: 0.368
[23,     1] loss: 0.342
[24,     1] loss: 0.318
[25,     1] loss: 0.335
[26,     1] loss: 0.391
[27,     1] loss: 0.268
[28,     1] loss: 0.330
[29,     1] loss: 0.254
[30,     1] loss: 0.289
[31,     1] loss: 0.276
[32,     1] loss: 0.263
[33,     1] loss: 0.224
[34,     1] loss: 0.267
[35,     1] loss: 0.210
[36,     1] loss: 0.252
[37,     1] loss: 0.290
[38,     1] loss: 0.173
[39,     1] loss: 0.222
[40,     1] loss: 0.212
[41,     1] loss: 0.237
[42,     1] loss: 0.269
[43,     1] loss: 0.242
[44,     1] loss: 0.166
[45,     1] loss: 0.127
[46,     1] loss: 0.157
[47,     1] loss: 0.176
[48,     1] loss: 0.162
[49,     1] loss: 0.132
[50,     1] loss: 0.112
[51,     1] loss: 0.133
[52,     1] loss: 0.118
[53,     1] loss: 0.155
[54,     1] loss: 0.122
[55,     1] loss: 0.248
[56,     1] loss: 0.160
[57,     1] loss: 0.148
[58,     1] loss: 0.096
[59,     1] loss: 0.114
[60,     1] loss: 0.125
[61,     1] loss: 0.158
[62,     1] loss: 0.152
[63,     1] loss: 0.121
[64,     1] loss: 0.140
[65,     1] loss: 0.100
[66,     1] loss: 0.101
[67,     1] loss: 0.085
[68,     1] loss: 0.128
[69,     1] loss: 0.139
[70,     1] loss: 0.161
[71,     1] loss: 0.116
[72,     1] loss: 0.108
[73,     1] loss: 0.123
[74,     1] loss: 0.072
[75,     1] loss: 0.118
[76,     1] loss: 0.078
[77,     1] loss: 0.110
[78,     1] loss: 0.086
[79,     1] loss: 0.101
[80,     1] loss: 0.063
Early stopping applied (best metric=0.2731649875640869)
Finished Training
Total time taken: 8.835000276565552
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.696
[3,     1] loss: 0.678
[4,     1] loss: 0.638
[5,     1] loss: 0.600
[6,     1] loss: 0.573
[7,     1] loss: 0.509
[8,     1] loss: 0.461
[9,     1] loss: 0.451
[10,     1] loss: 0.415
[11,     1] loss: 0.376
[12,     1] loss: 0.315
[13,     1] loss: 0.336
[14,     1] loss: 0.317
[15,     1] loss: 0.271
[16,     1] loss: 0.365
[17,     1] loss: 0.403
[18,     1] loss: 0.313
[19,     1] loss: 0.244
[20,     1] loss: 0.310
[21,     1] loss: 0.226
[22,     1] loss: 0.238
[23,     1] loss: 0.213
[24,     1] loss: 0.224
[25,     1] loss: 0.158
[26,     1] loss: 0.139
[27,     1] loss: 0.169
[28,     1] loss: 0.156
[29,     1] loss: 0.141
[30,     1] loss: 0.124
[31,     1] loss: 0.193
[32,     1] loss: 0.117
[33,     1] loss: 0.129
[34,     1] loss: 0.139
[35,     1] loss: 0.196
[36,     1] loss: 0.176
[37,     1] loss: 0.101
[38,     1] loss: 0.102
[39,     1] loss: 0.162
[40,     1] loss: 0.098
[41,     1] loss: 0.116
[42,     1] loss: 0.113
[43,     1] loss: 0.191
[44,     1] loss: 0.123
[45,     1] loss: 0.114
[46,     1] loss: 0.133
[47,     1] loss: 0.100
[48,     1] loss: 0.089
[49,     1] loss: 0.140
[50,     1] loss: 0.113
[51,     1] loss: 0.093
[52,     1] loss: 0.107
[53,     1] loss: 0.103
[54,     1] loss: 0.145
[55,     1] loss: 0.112
[56,     1] loss: 0.176
[57,     1] loss: 0.175
[58,     1] loss: 0.233
[59,     1] loss: 0.150
[60,     1] loss: 0.150
[61,     1] loss: 0.147
[62,     1] loss: 0.123
[63,     1] loss: 0.164
[64,     1] loss: 0.189
[65,     1] loss: 0.158
[66,     1] loss: 0.116
[67,     1] loss: 0.130
[68,     1] loss: 0.127
[69,     1] loss: 0.172
[70,     1] loss: 0.187
[71,     1] loss: 0.197
[72,     1] loss: 0.103
[73,     1] loss: 0.136
[74,     1] loss: 0.149
[75,     1] loss: 0.149
[76,     1] loss: 0.169
[77,     1] loss: 0.157
[78,     1] loss: 0.114
[79,     1] loss: 0.125
[80,     1] loss: 0.116
[81,     1] loss: 0.102
[82,     1] loss: 0.092
[83,     1] loss: 0.073
[84,     1] loss: 0.093
[85,     1] loss: 0.086
[86,     1] loss: 0.060
[87,     1] loss: 0.067
[88,     1] loss: 0.041
[89,     1] loss: 0.057
[90,     1] loss: 0.096
[91,     1] loss: 0.077
[92,     1] loss: 0.081
Early stopping applied (best metric=0.3893432319164276)
Finished Training
Total time taken: 10.117002010345459
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.668
[4,     1] loss: 0.632
[5,     1] loss: 0.567
[6,     1] loss: 0.514
[7,     1] loss: 0.495
[8,     1] loss: 0.422
[9,     1] loss: 0.357
[10,     1] loss: 0.372
[11,     1] loss: 0.327
[12,     1] loss: 0.260
[13,     1] loss: 0.331
[14,     1] loss: 0.389
[15,     1] loss: 0.350
[16,     1] loss: 0.309
[17,     1] loss: 0.339
[18,     1] loss: 0.253
[19,     1] loss: 0.244
[20,     1] loss: 0.273
[21,     1] loss: 0.228
[22,     1] loss: 0.230
[23,     1] loss: 0.172
[24,     1] loss: 0.197
[25,     1] loss: 0.305
[26,     1] loss: 0.218
[27,     1] loss: 0.162
[28,     1] loss: 0.227
[29,     1] loss: 0.149
[30,     1] loss: 0.218
[31,     1] loss: 0.143
[32,     1] loss: 0.184
[33,     1] loss: 0.154
[34,     1] loss: 0.188
[35,     1] loss: 0.141
[36,     1] loss: 0.143
[37,     1] loss: 0.146
[38,     1] loss: 0.117
[39,     1] loss: 0.185
[40,     1] loss: 0.173
[41,     1] loss: 0.144
[42,     1] loss: 0.085
[43,     1] loss: 0.139
[44,     1] loss: 0.101
[45,     1] loss: 0.113
[46,     1] loss: 0.106
[47,     1] loss: 0.128
[48,     1] loss: 0.055
[49,     1] loss: 0.086
[50,     1] loss: 0.089
[51,     1] loss: 0.069
[52,     1] loss: 0.065
[53,     1] loss: 0.075
[54,     1] loss: 0.164
[55,     1] loss: 0.120
[56,     1] loss: 0.071
[57,     1] loss: 0.088
[58,     1] loss: 0.114
Early stopping applied (best metric=0.3148912191390991)
Finished Training
Total time taken: 6.409604549407959
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.689
[4,     1] loss: 0.678
[5,     1] loss: 0.653
[6,     1] loss: 0.632
[7,     1] loss: 0.567
[8,     1] loss: 0.535
[9,     1] loss: 0.483
[10,     1] loss: 0.415
[11,     1] loss: 0.420
[12,     1] loss: 0.347
[13,     1] loss: 0.338
[14,     1] loss: 0.243
[15,     1] loss: 0.269
[16,     1] loss: 0.241
[17,     1] loss: 0.230
[18,     1] loss: 0.224
[19,     1] loss: 0.306
[20,     1] loss: 0.215
[21,     1] loss: 0.260
[22,     1] loss: 0.222
[23,     1] loss: 0.223
[24,     1] loss: 0.227
[25,     1] loss: 0.246
[26,     1] loss: 0.176
[27,     1] loss: 0.226
[28,     1] loss: 0.212
[29,     1] loss: 0.210
[30,     1] loss: 0.215
[31,     1] loss: 0.143
[32,     1] loss: 0.171
[33,     1] loss: 0.162
[34,     1] loss: 0.165
[35,     1] loss: 0.116
[36,     1] loss: 0.138
[37,     1] loss: 0.118
[38,     1] loss: 0.120
[39,     1] loss: 0.154
[40,     1] loss: 0.176
[41,     1] loss: 0.184
[42,     1] loss: 0.166
[43,     1] loss: 0.135
[44,     1] loss: 0.170
[45,     1] loss: 0.188
[46,     1] loss: 0.156
[47,     1] loss: 0.135
[48,     1] loss: 0.117
[49,     1] loss: 0.139
[50,     1] loss: 0.124
[51,     1] loss: 0.143
[52,     1] loss: 0.162
[53,     1] loss: 0.102
[54,     1] loss: 0.093
[55,     1] loss: 0.095
[56,     1] loss: 0.125
[57,     1] loss: 0.114
[58,     1] loss: 0.083
[59,     1] loss: 0.109
[60,     1] loss: 0.199
Early stopping applied (best metric=0.3228081464767456)
Finished Training
Total time taken: 6.60961651802063
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.690
[3,     1] loss: 0.662
[4,     1] loss: 0.602
[5,     1] loss: 0.577
[6,     1] loss: 0.526
[7,     1] loss: 0.423
[8,     1] loss: 0.384
[9,     1] loss: 0.295
[10,     1] loss: 0.392
[11,     1] loss: 0.323
[12,     1] loss: 0.355
[13,     1] loss: 0.306
[14,     1] loss: 0.276
[15,     1] loss: 0.213
[16,     1] loss: 0.233
[17,     1] loss: 0.246
[18,     1] loss: 0.248
[19,     1] loss: 0.223
[20,     1] loss: 0.231
[21,     1] loss: 0.194
[22,     1] loss: 0.256
[23,     1] loss: 0.205
[24,     1] loss: 0.175
[25,     1] loss: 0.201
[26,     1] loss: 0.199
[27,     1] loss: 0.243
[28,     1] loss: 0.231
[29,     1] loss: 0.144
[30,     1] loss: 0.142
[31,     1] loss: 0.243
[32,     1] loss: 0.160
[33,     1] loss: 0.150
[34,     1] loss: 0.167
[35,     1] loss: 0.160
[36,     1] loss: 0.144
[37,     1] loss: 0.127
[38,     1] loss: 0.117
[39,     1] loss: 0.119
[40,     1] loss: 0.159
[41,     1] loss: 0.148
[42,     1] loss: 0.142
[43,     1] loss: 0.148
[44,     1] loss: 0.139
[45,     1] loss: 0.121
[46,     1] loss: 0.164
[47,     1] loss: 0.187
[48,     1] loss: 0.172
[49,     1] loss: 0.151
[50,     1] loss: 0.243
[51,     1] loss: 0.106
[52,     1] loss: 0.210
[53,     1] loss: 0.168
[54,     1] loss: 0.107
[55,     1] loss: 0.236
[56,     1] loss: 0.111
[57,     1] loss: 0.160
[58,     1] loss: 0.171
[59,     1] loss: 0.137
[60,     1] loss: 0.143
[61,     1] loss: 0.164
Early stopping applied (best metric=0.3333849310874939)
Finished Training
Total time taken: 6.746001720428467
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.680
[3,     1] loss: 0.637
[4,     1] loss: 0.573
[5,     1] loss: 0.508
[6,     1] loss: 0.450
[7,     1] loss: 0.365
[8,     1] loss: 0.352
[9,     1] loss: 0.250
[10,     1] loss: 0.331
[11,     1] loss: 0.270
[12,     1] loss: 0.311
[13,     1] loss: 0.303
[14,     1] loss: 0.236
[15,     1] loss: 0.211
[16,     1] loss: 0.209
[17,     1] loss: 0.179
[18,     1] loss: 0.160
[19,     1] loss: 0.185
[20,     1] loss: 0.186
[21,     1] loss: 0.144
[22,     1] loss: 0.166
[23,     1] loss: 0.161
[24,     1] loss: 0.190
[25,     1] loss: 0.140
[26,     1] loss: 0.227
[27,     1] loss: 0.155
[28,     1] loss: 0.140
[29,     1] loss: 0.132
[30,     1] loss: 0.271
[31,     1] loss: 0.100
[32,     1] loss: 0.154
[33,     1] loss: 0.177
[34,     1] loss: 0.136
[35,     1] loss: 0.167
[36,     1] loss: 0.132
[37,     1] loss: 0.123
[38,     1] loss: 0.140
[39,     1] loss: 0.166
[40,     1] loss: 0.109
[41,     1] loss: 0.083
[42,     1] loss: 0.119
[43,     1] loss: 0.094
[44,     1] loss: 0.118
[45,     1] loss: 0.078
[46,     1] loss: 0.166
[47,     1] loss: 0.136
[48,     1] loss: 0.164
[49,     1] loss: 0.101
[50,     1] loss: 0.188
[51,     1] loss: 0.123
[52,     1] loss: 0.152
[53,     1] loss: 0.147
[54,     1] loss: 0.108
[55,     1] loss: 0.102
Early stopping applied (best metric=0.4668810963630676)
Finished Training
Total time taken: 6.084997653961182
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.682
[4,     1] loss: 0.648
[5,     1] loss: 0.617
[6,     1] loss: 0.567
[7,     1] loss: 0.546
[8,     1] loss: 0.549
[9,     1] loss: 0.486
[10,     1] loss: 0.415
[11,     1] loss: 0.345
[12,     1] loss: 0.371
[13,     1] loss: 0.392
[14,     1] loss: 0.320
[15,     1] loss: 0.311
[16,     1] loss: 0.234
[17,     1] loss: 0.256
[18,     1] loss: 0.291
[19,     1] loss: 0.334
[20,     1] loss: 0.272
[21,     1] loss: 0.328
[22,     1] loss: 0.300
[23,     1] loss: 0.236
[24,     1] loss: 0.179
[25,     1] loss: 0.178
[26,     1] loss: 0.209
[27,     1] loss: 0.175
[28,     1] loss: 0.202
[29,     1] loss: 0.158
[30,     1] loss: 0.164
[31,     1] loss: 0.175
[32,     1] loss: 0.100
[33,     1] loss: 0.108
[34,     1] loss: 0.181
[35,     1] loss: 0.114
[36,     1] loss: 0.113
[37,     1] loss: 0.158
[38,     1] loss: 0.172
[39,     1] loss: 0.105
[40,     1] loss: 0.147
[41,     1] loss: 0.155
[42,     1] loss: 0.129
[43,     1] loss: 0.155
[44,     1] loss: 0.139
[45,     1] loss: 0.086
[46,     1] loss: 0.105
[47,     1] loss: 0.092
[48,     1] loss: 0.161
[49,     1] loss: 0.101
[50,     1] loss: 0.093
[51,     1] loss: 0.076
[52,     1] loss: 0.071
[53,     1] loss: 0.087
[54,     1] loss: 0.232
[55,     1] loss: 0.374
[56,     1] loss: 0.301
[57,     1] loss: 0.250
[58,     1] loss: 0.242
[59,     1] loss: 0.221
[60,     1] loss: 0.189
[61,     1] loss: 0.234
[62,     1] loss: 0.235
[63,     1] loss: 0.175
[64,     1] loss: 0.203
[65,     1] loss: 0.181
[66,     1] loss: 0.134
[67,     1] loss: 0.129
[68,     1] loss: 0.144
[69,     1] loss: 0.112
[70,     1] loss: 0.174
[71,     1] loss: 0.099
[72,     1] loss: 0.193
[73,     1] loss: 0.197
[74,     1] loss: 0.100
[75,     1] loss: 0.118
[76,     1] loss: 0.169
[77,     1] loss: 0.091
[78,     1] loss: 0.105
[79,     1] loss: 0.142
[80,     1] loss: 0.091
[81,     1] loss: 0.104
[82,     1] loss: 0.128
[83,     1] loss: 0.065
Early stopping applied (best metric=0.22452449798583984)
Finished Training
Total time taken: 9.157000064849854
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.689
[3,     1] loss: 0.670
[4,     1] loss: 0.640
[5,     1] loss: 0.571
[6,     1] loss: 0.542
[7,     1] loss: 0.474
[8,     1] loss: 0.462
[9,     1] loss: 0.386
[10,     1] loss: 0.425
[11,     1] loss: 0.314
[12,     1] loss: 0.329
[13,     1] loss: 0.298
[14,     1] loss: 0.348
[15,     1] loss: 0.272
[16,     1] loss: 0.343
[17,     1] loss: 0.309
[18,     1] loss: 0.275
[19,     1] loss: 0.307
[20,     1] loss: 0.321
[21,     1] loss: 0.261
[22,     1] loss: 0.261
[23,     1] loss: 0.317
[24,     1] loss: 0.260
[25,     1] loss: 0.226
[26,     1] loss: 0.293
[27,     1] loss: 0.257
[28,     1] loss: 0.229
[29,     1] loss: 0.167
[30,     1] loss: 0.240
[31,     1] loss: 0.142
[32,     1] loss: 0.173
[33,     1] loss: 0.122
[34,     1] loss: 0.140
[35,     1] loss: 0.117
[36,     1] loss: 0.128
[37,     1] loss: 0.277
[38,     1] loss: 0.180
[39,     1] loss: 0.281
[40,     1] loss: 0.188
[41,     1] loss: 0.286
[42,     1] loss: 0.140
[43,     1] loss: 0.218
[44,     1] loss: 0.211
[45,     1] loss: 0.149
[46,     1] loss: 0.172
[47,     1] loss: 0.159
[48,     1] loss: 0.177
[49,     1] loss: 0.180
[50,     1] loss: 0.163
[51,     1] loss: 0.106
[52,     1] loss: 0.147
[53,     1] loss: 0.086
[54,     1] loss: 0.095
[55,     1] loss: 0.136
[56,     1] loss: 0.137
Early stopping applied (best metric=0.39293432235717773)
Finished Training
Total time taken: 6.1156065464019775
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.700
[3,     1] loss: 0.668
[4,     1] loss: 0.641
[5,     1] loss: 0.611
[6,     1] loss: 0.552
[7,     1] loss: 0.510
[8,     1] loss: 0.514
[9,     1] loss: 0.415
[10,     1] loss: 0.436
[11,     1] loss: 0.410
[12,     1] loss: 0.341
[13,     1] loss: 0.375
[14,     1] loss: 0.291
[15,     1] loss: 0.315
[16,     1] loss: 0.275
[17,     1] loss: 0.306
[18,     1] loss: 0.211
[19,     1] loss: 0.200
[20,     1] loss: 0.297
[21,     1] loss: 0.211
[22,     1] loss: 0.266
[23,     1] loss: 0.172
[24,     1] loss: 0.164
[25,     1] loss: 0.170
[26,     1] loss: 0.191
[27,     1] loss: 0.149
[28,     1] loss: 0.144
[29,     1] loss: 0.130
[30,     1] loss: 0.097
[31,     1] loss: 0.085
[32,     1] loss: 0.110
[33,     1] loss: 0.164
[34,     1] loss: 0.121
[35,     1] loss: 0.113
[36,     1] loss: 0.091
[37,     1] loss: 0.116
[38,     1] loss: 0.117
[39,     1] loss: 0.081
[40,     1] loss: 0.055
[41,     1] loss: 0.058
[42,     1] loss: 0.094
[43,     1] loss: 0.088
[44,     1] loss: 0.140
[45,     1] loss: 0.095
[46,     1] loss: 0.154
[47,     1] loss: 0.093
[48,     1] loss: 0.196
[49,     1] loss: 0.098
[50,     1] loss: 0.082
[51,     1] loss: 0.129
[52,     1] loss: 0.127
[53,     1] loss: 0.153
[54,     1] loss: 0.128
[55,     1] loss: 0.199
[56,     1] loss: 0.075
[57,     1] loss: 0.142
[58,     1] loss: 0.088
[59,     1] loss: 0.076
[60,     1] loss: 0.119
[61,     1] loss: 0.065
[62,     1] loss: 0.091
[63,     1] loss: 0.048
[64,     1] loss: 0.051
[65,     1] loss: 0.059
[66,     1] loss: 0.068
[67,     1] loss: 0.040
[68,     1] loss: 0.038
[69,     1] loss: 0.051
[70,     1] loss: 0.050
[71,     1] loss: 0.062
[72,     1] loss: 0.084
[73,     1] loss: 0.144
[74,     1] loss: 0.417
Early stopping applied (best metric=0.2425369918346405)
Finished Training
Total time taken: 8.209999322891235
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.684
[4,     1] loss: 0.663
[5,     1] loss: 0.611
[6,     1] loss: 0.590
[7,     1] loss: 0.542
[8,     1] loss: 0.490
[9,     1] loss: 0.458
[10,     1] loss: 0.430
[11,     1] loss: 0.404
[12,     1] loss: 0.387
[13,     1] loss: 0.380
[14,     1] loss: 0.381
[15,     1] loss: 0.392
[16,     1] loss: 0.313
[17,     1] loss: 0.313
[18,     1] loss: 0.337
[19,     1] loss: 0.301
[20,     1] loss: 0.256
[21,     1] loss: 0.315
[22,     1] loss: 0.309
[23,     1] loss: 0.263
[24,     1] loss: 0.228
[25,     1] loss: 0.260
[26,     1] loss: 0.209
[27,     1] loss: 0.237
[28,     1] loss: 0.206
[29,     1] loss: 0.183
[30,     1] loss: 0.263
[31,     1] loss: 0.354
[32,     1] loss: 0.214
[33,     1] loss: 0.205
[34,     1] loss: 0.212
[35,     1] loss: 0.226
[36,     1] loss: 0.193
[37,     1] loss: 0.194
[38,     1] loss: 0.196
[39,     1] loss: 0.213
[40,     1] loss: 0.292
[41,     1] loss: 0.146
[42,     1] loss: 0.184
[43,     1] loss: 0.172
[44,     1] loss: 0.227
[45,     1] loss: 0.196
[46,     1] loss: 0.250
[47,     1] loss: 0.208
[48,     1] loss: 0.183
[49,     1] loss: 0.211
[50,     1] loss: 0.158
[51,     1] loss: 0.227
[52,     1] loss: 0.160
[53,     1] loss: 0.154
[54,     1] loss: 0.155
[55,     1] loss: 0.263
[56,     1] loss: 0.114
[57,     1] loss: 0.198
[58,     1] loss: 0.264
[59,     1] loss: 0.213
[60,     1] loss: 0.242
[61,     1] loss: 0.196
[62,     1] loss: 0.167
[63,     1] loss: 0.218
[64,     1] loss: 0.186
[65,     1] loss: 0.196
[66,     1] loss: 0.151
[67,     1] loss: 0.175
[68,     1] loss: 0.147
[69,     1] loss: 0.170
[70,     1] loss: 0.133
[71,     1] loss: 0.183
[72,     1] loss: 0.175
[73,     1] loss: 0.158
[74,     1] loss: 0.230
[75,     1] loss: 0.208
[76,     1] loss: 0.144
[77,     1] loss: 0.143
[78,     1] loss: 0.214
[79,     1] loss: 0.172
[80,     1] loss: 0.120
[81,     1] loss: 0.119
[82,     1] loss: 0.209
[83,     1] loss: 0.093
[84,     1] loss: 0.121
[85,     1] loss: 0.153
[86,     1] loss: 0.100
[87,     1] loss: 0.113
[88,     1] loss: 0.125
[89,     1] loss: 0.171
[90,     1] loss: 0.184
[91,     1] loss: 0.143
[92,     1] loss: 0.135
[93,     1] loss: 0.114
[94,     1] loss: 0.165
[95,     1] loss: 0.110
[96,     1] loss: 0.139
[97,     1] loss: 0.126
[98,     1] loss: 0.107
[99,     1] loss: 0.089
[100,     1] loss: 0.074
[101,     1] loss: 0.087
[102,     1] loss: 0.266
[103,     1] loss: 0.330
[104,     1] loss: 0.352
[105,     1] loss: 0.231
[106,     1] loss: 0.351
[107,     1] loss: 0.257
[108,     1] loss: 0.260
[109,     1] loss: 0.254
[110,     1] loss: 0.297
[111,     1] loss: 0.247
[112,     1] loss: 0.219
[113,     1] loss: 0.224
[114,     1] loss: 0.218
[115,     1] loss: 0.174
[116,     1] loss: 0.208
Early stopping applied (best metric=0.1384592354297638)
Finished Training
Total time taken: 12.806999683380127
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.692
[3,     1] loss: 0.679
[4,     1] loss: 0.644
[5,     1] loss: 0.600
[6,     1] loss: 0.560
[7,     1] loss: 0.519
[8,     1] loss: 0.480
[9,     1] loss: 0.396
[10,     1] loss: 0.455
[11,     1] loss: 0.401
[12,     1] loss: 0.330
[13,     1] loss: 0.386
[14,     1] loss: 0.435
[15,     1] loss: 0.347
[16,     1] loss: 0.340
[17,     1] loss: 0.377
[18,     1] loss: 0.381
[19,     1] loss: 0.299
[20,     1] loss: 0.348
[21,     1] loss: 0.340
[22,     1] loss: 0.262
[23,     1] loss: 0.328
[24,     1] loss: 0.287
[25,     1] loss: 0.237
[26,     1] loss: 0.341
[27,     1] loss: 0.331
[28,     1] loss: 0.292
[29,     1] loss: 0.240
[30,     1] loss: 0.259
[31,     1] loss: 0.241
[32,     1] loss: 0.260
[33,     1] loss: 0.225
[34,     1] loss: 0.239
[35,     1] loss: 0.286
[36,     1] loss: 0.268
[37,     1] loss: 0.200
[38,     1] loss: 0.232
[39,     1] loss: 0.281
[40,     1] loss: 0.254
[41,     1] loss: 0.314
[42,     1] loss: 0.255
[43,     1] loss: 0.280
[44,     1] loss: 0.231
[45,     1] loss: 0.204
[46,     1] loss: 0.180
[47,     1] loss: 0.244
[48,     1] loss: 0.195
[49,     1] loss: 0.212
[50,     1] loss: 0.137
[51,     1] loss: 0.115
[52,     1] loss: 0.130
[53,     1] loss: 0.135
[54,     1] loss: 0.094
[55,     1] loss: 0.100
[56,     1] loss: 0.112
[57,     1] loss: 0.102
[58,     1] loss: 0.150
[59,     1] loss: 0.119
[60,     1] loss: 0.150
[61,     1] loss: 0.091
[62,     1] loss: 0.144
[63,     1] loss: 0.095
[64,     1] loss: 0.141
[65,     1] loss: 0.153
[66,     1] loss: 0.103
[67,     1] loss: 0.143
[68,     1] loss: 0.142
[69,     1] loss: 0.087
[70,     1] loss: 0.101
[71,     1] loss: 0.116
[72,     1] loss: 0.135
[73,     1] loss: 0.097
[74,     1] loss: 0.074
[75,     1] loss: 0.065
[76,     1] loss: 0.070
[77,     1] loss: 0.058
[78,     1] loss: 0.085
[79,     1] loss: 0.131
[80,     1] loss: 0.315
[81,     1] loss: 0.411
[82,     1] loss: 0.270
[83,     1] loss: 0.167
[84,     1] loss: 0.132
[85,     1] loss: 0.169
[86,     1] loss: 0.180
[87,     1] loss: 0.178
[88,     1] loss: 0.198
[89,     1] loss: 0.149
[90,     1] loss: 0.144
[91,     1] loss: 0.139
[92,     1] loss: 0.134
Early stopping applied (best metric=0.233902707695961)
Finished Training
Total time taken: 10.1576087474823
{'Hydroxylation-K Validation Accuracy': 0.8663475177304965, 'Hydroxylation-K Validation Sensitivity': 0.8862222222222222, 'Hydroxylation-K Validation Specificity': 0.8610526315789474, 'Hydroxylation-K Validation Precision': 0.6263141249874996, 'Hydroxylation-K AUC ROC': 0.9001988304093568, 'Hydroxylation-K AUC PR': 0.7054867381184308, 'Hydroxylation-K MCC': 0.6660642562936477, 'Hydroxylation-K F1': 0.7300895583802236, 'Validation Loss (Hydroxylation-K)': 0.2817466425895691, 'Validation Loss (total)': 0.2817466425895691, 'TimeToTrain': 9.288740844726563}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0067999845403928835,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1683855939,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.859336250482469}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.709
[3,     1] loss: 0.680
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007001645565603621,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1798914225,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 19.663774898965155}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.692
[3,     1] loss: 0.689
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007124888328582882,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 848382649,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.8062119279338873}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.688
[3,     1] loss: 0.660
[4,     1] loss: 0.620
[5,     1] loss: 0.546
[6,     1] loss: 0.509
[7,     1] loss: 0.448
[8,     1] loss: 0.395
[9,     1] loss: 0.420
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0077181719289450956,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1504888680,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.85146802397005}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.688
[3,     1] loss: 0.667
[4,     1] loss: 0.635
[5,     1] loss: 0.592
[6,     1] loss: 0.531
[7,     1] loss: 0.517
[8,     1] loss: 0.461
[9,     1] loss: 0.423
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007043662604000888,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3102374982,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.8362613280834053}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.689
[2,     1] loss: 0.702
[3,     1] loss: 0.691
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00748356391977231,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1504236546,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.218263539378683}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.701
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0060919998376412505,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 446259059,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.710225907945268}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.689
[3,     1] loss: 0.675
[4,     1] loss: 0.634
[5,     1] loss: 0.589
[6,     1] loss: 0.544
[7,     1] loss: 0.503
[8,     1] loss: 0.421
[9,     1] loss: 0.409
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007303511284669728,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2448463430,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 0.9412163272354281}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.685
[3,     1] loss: 0.662
[4,     1] loss: 0.626
[5,     1] loss: 0.557
[6,     1] loss: 0.472
[7,     1] loss: 0.482
[8,     1] loss: 0.397
[9,     1] loss: 0.364
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007190845156386794,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1327896388,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 16.257763572612976}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.687
[2,     1] loss: 0.693
[3,     1] loss: 0.668
[4,     1] loss: 0.628
[5,     1] loss: 0.601
[6,     1] loss: 0.528
[7,     1] loss: 0.474
[8,     1] loss: 0.447
[9,     1] loss: 0.421
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007223793822635754,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3474651909,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 9.88564320496998}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.697
[3,     1] loss: 0.690
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006603863775331219,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3445178032,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.771689886531066}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.696
[2,     1] loss: 0.695
[3,     1] loss: 0.681
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008744303908569813,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3445383960,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 10.260906061356383}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.705
[3,     1] loss: 0.693
[4,     1] loss: 0.687
[5,     1] loss: 0.682
[6,     1] loss: 0.664
[7,     1] loss: 0.644
[8,     1] loss: 0.615
[9,     1] loss: 0.595
[10,     1] loss: 0.532
[11,     1] loss: 0.501
[12,     1] loss: 0.462
[13,     1] loss: 0.439
[14,     1] loss: 0.409
[15,     1] loss: 0.382
[16,     1] loss: 0.408
[17,     1] loss: 0.430
[18,     1] loss: 0.372
[19,     1] loss: 0.359
[20,     1] loss: 0.432
[21,     1] loss: 0.334
[22,     1] loss: 0.361
[23,     1] loss: 0.347
[24,     1] loss: 0.355
[25,     1] loss: 0.351
[26,     1] loss: 0.307
[27,     1] loss: 0.283
[28,     1] loss: 0.317
[29,     1] loss: 0.280
[30,     1] loss: 0.249
[31,     1] loss: 0.281
[32,     1] loss: 0.240
[33,     1] loss: 0.206
[34,     1] loss: 0.239
[35,     1] loss: 0.224
[36,     1] loss: 0.251
[37,     1] loss: 0.209
[38,     1] loss: 0.205
[39,     1] loss: 0.211
[40,     1] loss: 0.291
[41,     1] loss: 0.327
[42,     1] loss: 0.624
[43,     1] loss: 0.264
[44,     1] loss: 0.332
[45,     1] loss: 0.269
[46,     1] loss: 0.266
[47,     1] loss: 0.309
[48,     1] loss: 0.349
[49,     1] loss: 0.336
[50,     1] loss: 0.280
[51,     1] loss: 0.306
[52,     1] loss: 0.262
[53,     1] loss: 0.226
[54,     1] loss: 0.271
[55,     1] loss: 0.279
[56,     1] loss: 0.222
[57,     1] loss: 0.208
[58,     1] loss: 0.189
[59,     1] loss: 0.231
[60,     1] loss: 0.224
[61,     1] loss: 0.432
[62,     1] loss: 0.322
[63,     1] loss: 0.304
[64,     1] loss: 0.272
[65,     1] loss: 0.378
[66,     1] loss: 0.274
[67,     1] loss: 0.291
[68,     1] loss: 0.269
[69,     1] loss: 0.256
[70,     1] loss: 0.265
[71,     1] loss: 0.241
[72,     1] loss: 0.250
[73,     1] loss: 0.266
[74,     1] loss: 0.254
[75,     1] loss: 0.281
[76,     1] loss: 0.234
[77,     1] loss: 0.230
[78,     1] loss: 0.281
[79,     1] loss: 0.201
[80,     1] loss: 0.218
[81,     1] loss: 0.231
[82,     1] loss: 0.220
[83,     1] loss: 0.260
[84,     1] loss: 0.203
[85,     1] loss: 0.256
[86,     1] loss: 0.410
[87,     1] loss: 0.314
[88,     1] loss: 0.366
[89,     1] loss: 0.268
[90,     1] loss: 0.337
[91,     1] loss: 0.276
[92,     1] loss: 0.294
[93,     1] loss: 0.252
[94,     1] loss: 0.279
[95,     1] loss: 0.219
[96,     1] loss: 0.252
[97,     1] loss: 0.261
[98,     1] loss: 0.250
[99,     1] loss: 0.243
[100,     1] loss: 0.243
[101,     1] loss: 0.264
[102,     1] loss: 0.255
[103,     1] loss: 0.209
[104,     1] loss: 0.306
[105,     1] loss: 0.463
[106,     1] loss: 0.381
[107,     1] loss: 0.320
[108,     1] loss: 0.231
[109,     1] loss: 0.261
Early stopping applied (best metric=0.19128632545471191)
Finished Training
Total time taken: 12.038609027862549
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.695
[3,     1] loss: 0.681
[4,     1] loss: 0.669
[5,     1] loss: 0.628
[6,     1] loss: 0.606
[7,     1] loss: 0.552
[8,     1] loss: 0.494
[9,     1] loss: 0.447
[10,     1] loss: 0.431
[11,     1] loss: 0.377
[12,     1] loss: 0.324
[13,     1] loss: 0.338
[14,     1] loss: 0.605
[15,     1] loss: 0.423
[16,     1] loss: 0.398
[17,     1] loss: 0.333
[18,     1] loss: 0.364
[19,     1] loss: 0.363
[20,     1] loss: 0.320
[21,     1] loss: 0.339
[22,     1] loss: 0.335
[23,     1] loss: 0.317
[24,     1] loss: 0.296
[25,     1] loss: 0.299
[26,     1] loss: 0.294
[27,     1] loss: 0.326
[28,     1] loss: 0.315
[29,     1] loss: 0.243
[30,     1] loss: 0.279
[31,     1] loss: 0.302
[32,     1] loss: 0.310
[33,     1] loss: 0.266
[34,     1] loss: 0.250
[35,     1] loss: 0.256
[36,     1] loss: 0.286
[37,     1] loss: 0.223
[38,     1] loss: 0.192
[39,     1] loss: 0.261
[40,     1] loss: 0.229
[41,     1] loss: 0.194
[42,     1] loss: 0.213
[43,     1] loss: 0.395
[44,     1] loss: 0.485
[45,     1] loss: 0.291
[46,     1] loss: 0.413
[47,     1] loss: 0.272
[48,     1] loss: 0.313
[49,     1] loss: 0.294
[50,     1] loss: 0.288
[51,     1] loss: 0.293
[52,     1] loss: 0.263
[53,     1] loss: 0.271
[54,     1] loss: 0.277
[55,     1] loss: 0.248
[56,     1] loss: 0.240
[57,     1] loss: 0.212
[58,     1] loss: 0.205
[59,     1] loss: 0.195
[60,     1] loss: 0.164
[61,     1] loss: 0.186
[62,     1] loss: 0.227
[63,     1] loss: 0.167
[64,     1] loss: 0.155
[65,     1] loss: 0.153
[66,     1] loss: 0.203
[67,     1] loss: 0.179
[68,     1] loss: 0.255
[69,     1] loss: 0.403
[70,     1] loss: 0.223
[71,     1] loss: 0.260
[72,     1] loss: 0.219
[73,     1] loss: 0.264
[74,     1] loss: 0.203
[75,     1] loss: 0.196
[76,     1] loss: 0.209
[77,     1] loss: 0.196
[78,     1] loss: 0.192
[79,     1] loss: 0.163
[80,     1] loss: 0.161
[81,     1] loss: 0.188
[82,     1] loss: 0.200
[83,     1] loss: 0.243
[84,     1] loss: 0.166
[85,     1] loss: 0.208
[86,     1] loss: 0.283
[87,     1] loss: 0.212
[88,     1] loss: 0.302
[89,     1] loss: 0.233
[90,     1] loss: 0.229
[91,     1] loss: 0.270
[92,     1] loss: 0.242
[93,     1] loss: 0.202
[94,     1] loss: 0.216
[95,     1] loss: 0.231
[96,     1] loss: 0.192
[97,     1] loss: 0.215
Early stopping applied (best metric=0.4099527597427368)
Finished Training
Total time taken: 10.738999366760254
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.684
[3,     1] loss: 0.671
[4,     1] loss: 0.639
[5,     1] loss: 0.595
[6,     1] loss: 0.572
[7,     1] loss: 0.514
[8,     1] loss: 0.493
[9,     1] loss: 0.435
[10,     1] loss: 0.422
[11,     1] loss: 0.360
[12,     1] loss: 0.380
[13,     1] loss: 0.358
[14,     1] loss: 0.311
[15,     1] loss: 0.316
[16,     1] loss: 0.380
[17,     1] loss: 0.297
[18,     1] loss: 0.352
[19,     1] loss: 0.321
[20,     1] loss: 0.344
[21,     1] loss: 0.255
[22,     1] loss: 0.305
[23,     1] loss: 0.242
[24,     1] loss: 0.287
[25,     1] loss: 0.266
[26,     1] loss: 0.299
[27,     1] loss: 0.273
[28,     1] loss: 0.283
[29,     1] loss: 0.232
[30,     1] loss: 0.313
[31,     1] loss: 0.253
[32,     1] loss: 0.321
[33,     1] loss: 0.334
[34,     1] loss: 0.351
[35,     1] loss: 0.418
[36,     1] loss: 0.348
[37,     1] loss: 0.372
[38,     1] loss: 0.399
[39,     1] loss: 0.322
[40,     1] loss: 0.335
[41,     1] loss: 0.256
[42,     1] loss: 0.259
[43,     1] loss: 0.292
[44,     1] loss: 0.412
[45,     1] loss: 0.308
[46,     1] loss: 0.244
[47,     1] loss: 0.247
[48,     1] loss: 0.260
[49,     1] loss: 0.260
[50,     1] loss: 0.251
[51,     1] loss: 0.298
[52,     1] loss: 0.273
[53,     1] loss: 0.302
[54,     1] loss: 0.278
[55,     1] loss: 0.244
[56,     1] loss: 0.303
[57,     1] loss: 0.208
[58,     1] loss: 0.196
[59,     1] loss: 0.177
[60,     1] loss: 0.223
[61,     1] loss: 0.193
[62,     1] loss: 0.291
[63,     1] loss: 0.289
[64,     1] loss: 0.287
[65,     1] loss: 0.263
[66,     1] loss: 0.194
[67,     1] loss: 0.214
[68,     1] loss: 0.225
[69,     1] loss: 0.201
[70,     1] loss: 0.231
[71,     1] loss: 0.192
[72,     1] loss: 0.198
[73,     1] loss: 0.187
[74,     1] loss: 0.165
[75,     1] loss: 0.224
[76,     1] loss: 0.150
[77,     1] loss: 0.557
[78,     1] loss: 0.192
[79,     1] loss: 0.334
[80,     1] loss: 0.195
[81,     1] loss: 0.397
[82,     1] loss: 0.264
[83,     1] loss: 0.262
[84,     1] loss: 0.227
[85,     1] loss: 0.256
[86,     1] loss: 0.273
[87,     1] loss: 0.196
[88,     1] loss: 0.284
[89,     1] loss: 0.250
[90,     1] loss: 0.237
[91,     1] loss: 0.210
[92,     1] loss: 0.213
[93,     1] loss: 0.208
[94,     1] loss: 0.196
[95,     1] loss: 0.163
[96,     1] loss: 0.177
[97,     1] loss: 0.173
[98,     1] loss: 0.194
[99,     1] loss: 0.324
[100,     1] loss: 0.280
[101,     1] loss: 0.298
[102,     1] loss: 0.210
[103,     1] loss: 0.212
[104,     1] loss: 0.252
[105,     1] loss: 0.238
[106,     1] loss: 0.192
[107,     1] loss: 0.150
[108,     1] loss: 0.188
[109,     1] loss: 0.202
[110,     1] loss: 0.190
[111,     1] loss: 0.205
[112,     1] loss: 0.306
[113,     1] loss: 0.435
[114,     1] loss: 0.376
[115,     1] loss: 0.383
[116,     1] loss: 0.321
[117,     1] loss: 0.339
[118,     1] loss: 0.312
[119,     1] loss: 0.312
[120,     1] loss: 0.292
[121,     1] loss: 0.282
[122,     1] loss: 0.261
[123,     1] loss: 0.254
[124,     1] loss: 0.242
[125,     1] loss: 0.217
[126,     1] loss: 0.245
[127,     1] loss: 0.247
[128,     1] loss: 0.267
[129,     1] loss: 0.207
[130,     1] loss: 0.207
[131,     1] loss: 0.178
[132,     1] loss: 0.207
[133,     1] loss: 0.185
[134,     1] loss: 0.154
[135,     1] loss: 0.137
[136,     1] loss: 0.171
[137,     1] loss: 0.147
[138,     1] loss: 0.214
[139,     1] loss: 0.179
[140,     1] loss: 0.204
[141,     1] loss: 0.212
[142,     1] loss: 0.203
[143,     1] loss: 0.217
[144,     1] loss: 0.220
[145,     1] loss: 0.210
[146,     1] loss: 0.176
[147,     1] loss: 0.236
[148,     1] loss: 0.228
[149,     1] loss: 0.183
[150,     1] loss: 0.223
[151,     1] loss: 0.187
[152,     1] loss: 0.171
[153,     1] loss: 0.182
[154,     1] loss: 0.258
[155,     1] loss: 0.495
[156,     1] loss: 0.281
[157,     1] loss: 0.311
[158,     1] loss: 0.241
[159,     1] loss: 0.283
[160,     1] loss: 0.275
[161,     1] loss: 0.263
[162,     1] loss: 0.252
[163,     1] loss: 0.246
[164,     1] loss: 0.225
[165,     1] loss: 0.256
[166,     1] loss: 0.159
[167,     1] loss: 0.222
[168,     1] loss: 0.227
[169,     1] loss: 0.188
[170,     1] loss: 0.183
[171,     1] loss: 0.189
[172,     1] loss: 0.199
[173,     1] loss: 0.217
[174,     1] loss: 0.148
[175,     1] loss: 0.175
[176,     1] loss: 0.223
[177,     1] loss: 0.145
[178,     1] loss: 0.211
[179,     1] loss: 0.222
[180,     1] loss: 0.201
[181,     1] loss: 0.164
[182,     1] loss: 0.208
[183,     1] loss: 0.173
[184,     1] loss: 0.181
[185,     1] loss: 0.168
[186,     1] loss: 0.133
[187,     1] loss: 0.202
[188,     1] loss: 0.179
[189,     1] loss: 0.272
[190,     1] loss: 0.781
[191,     1] loss: 0.239
[192,     1] loss: 0.399
[193,     1] loss: 0.271
[194,     1] loss: 0.301
[195,     1] loss: 0.284
[196,     1] loss: 0.268
[197,     1] loss: 0.314
[198,     1] loss: 0.239
[199,     1] loss: 0.243
[200,     1] loss: 0.235
Finished Training
Total time taken: 21.852603673934937
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.694
[3,     1] loss: 0.682
[4,     1] loss: 0.668
[5,     1] loss: 0.630
[6,     1] loss: 0.612
[7,     1] loss: 0.594
[8,     1] loss: 0.530
[9,     1] loss: 0.471
[10,     1] loss: 0.424
[11,     1] loss: 0.443
[12,     1] loss: 0.360
[13,     1] loss: 0.479
[14,     1] loss: 0.393
[15,     1] loss: 0.370
[16,     1] loss: 0.400
[17,     1] loss: 0.333
[18,     1] loss: 0.397
[19,     1] loss: 0.335
[20,     1] loss: 0.325
[21,     1] loss: 0.318
[22,     1] loss: 0.295
[23,     1] loss: 0.310
[24,     1] loss: 0.283
[25,     1] loss: 0.278
[26,     1] loss: 0.310
[27,     1] loss: 0.223
[28,     1] loss: 0.223
[29,     1] loss: 0.241
[30,     1] loss: 0.423
[31,     1] loss: 0.432
[32,     1] loss: 0.440
[33,     1] loss: 0.448
[34,     1] loss: 0.369
[35,     1] loss: 0.417
[36,     1] loss: 0.377
[37,     1] loss: 0.402
[38,     1] loss: 0.410
[39,     1] loss: 0.360
[40,     1] loss: 0.379
[41,     1] loss: 0.350
[42,     1] loss: 0.298
[43,     1] loss: 0.322
[44,     1] loss: 0.321
[45,     1] loss: 0.297
[46,     1] loss: 0.287
[47,     1] loss: 0.295
[48,     1] loss: 0.253
[49,     1] loss: 0.285
[50,     1] loss: 0.238
[51,     1] loss: 0.237
[52,     1] loss: 0.239
[53,     1] loss: 0.247
[54,     1] loss: 0.240
[55,     1] loss: 0.276
[56,     1] loss: 0.433
[57,     1] loss: 0.272
[58,     1] loss: 0.307
[59,     1] loss: 0.325
[60,     1] loss: 0.277
[61,     1] loss: 0.266
[62,     1] loss: 0.349
[63,     1] loss: 0.227
[64,     1] loss: 0.277
[65,     1] loss: 0.314
[66,     1] loss: 0.238
[67,     1] loss: 0.231
[68,     1] loss: 0.266
[69,     1] loss: 0.220
[70,     1] loss: 0.250
[71,     1] loss: 0.186
[72,     1] loss: 0.206
[73,     1] loss: 0.233
[74,     1] loss: 0.211
[75,     1] loss: 0.179
[76,     1] loss: 0.222
[77,     1] loss: 0.274
[78,     1] loss: 0.256
[79,     1] loss: 0.211
[80,     1] loss: 0.253
[81,     1] loss: 0.221
[82,     1] loss: 0.231
[83,     1] loss: 0.210
[84,     1] loss: 0.183
[85,     1] loss: 0.253
[86,     1] loss: 0.188
[87,     1] loss: 0.246
[88,     1] loss: 0.379
[89,     1] loss: 0.428
[90,     1] loss: 0.207
[91,     1] loss: 0.254
[92,     1] loss: 0.293
[93,     1] loss: 0.263
[94,     1] loss: 0.253
[95,     1] loss: 0.290
[96,     1] loss: 0.262
[97,     1] loss: 0.230
[98,     1] loss: 0.201
[99,     1] loss: 0.222
[100,     1] loss: 0.166
[101,     1] loss: 0.221
[102,     1] loss: 0.223
[103,     1] loss: 0.210
[104,     1] loss: 0.206
[105,     1] loss: 0.287
[106,     1] loss: 0.194
[107,     1] loss: 0.271
[108,     1] loss: 0.222
[109,     1] loss: 0.255
[110,     1] loss: 0.265
[111,     1] loss: 0.223
[112,     1] loss: 0.268
[113,     1] loss: 0.222
[114,     1] loss: 0.294
[115,     1] loss: 0.183
[116,     1] loss: 0.302
[117,     1] loss: 0.252
[118,     1] loss: 0.315
[119,     1] loss: 0.183
[120,     1] loss: 0.351
[121,     1] loss: 0.303
[122,     1] loss: 0.257
[123,     1] loss: 0.287
[124,     1] loss: 0.262
[125,     1] loss: 0.215
[126,     1] loss: 0.244
[127,     1] loss: 0.202
[128,     1] loss: 0.227
Early stopping applied (best metric=0.1524418145418167)
Finished Training
Total time taken: 14.102618932723999
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.694
[3,     1] loss: 0.689
[4,     1] loss: 0.682
[5,     1] loss: 0.663
[6,     1] loss: 0.640
[7,     1] loss: 0.602
[8,     1] loss: 0.573
[9,     1] loss: 0.570
[10,     1] loss: 0.488
[11,     1] loss: 0.469
[12,     1] loss: 0.490
[13,     1] loss: 0.425
[14,     1] loss: 0.375
[15,     1] loss: 0.365
[16,     1] loss: 0.377
[17,     1] loss: 0.350
[18,     1] loss: 0.297
[19,     1] loss: 0.319
[20,     1] loss: 0.411
[21,     1] loss: 0.291
[22,     1] loss: 0.320
[23,     1] loss: 0.245
[24,     1] loss: 0.269
[25,     1] loss: 0.322
[26,     1] loss: 0.275
[27,     1] loss: 0.260
[28,     1] loss: 0.232
[29,     1] loss: 0.210
[30,     1] loss: 0.214
[31,     1] loss: 0.538
[32,     1] loss: 0.429
[33,     1] loss: 0.288
[34,     1] loss: 0.346
[35,     1] loss: 0.300
[36,     1] loss: 0.320
[37,     1] loss: 0.295
[38,     1] loss: 0.302
[39,     1] loss: 0.303
[40,     1] loss: 0.253
[41,     1] loss: 0.236
[42,     1] loss: 0.256
[43,     1] loss: 0.225
[44,     1] loss: 0.211
[45,     1] loss: 0.222
[46,     1] loss: 0.170
[47,     1] loss: 0.189
[48,     1] loss: 0.232
[49,     1] loss: 0.312
[50,     1] loss: 0.276
[51,     1] loss: 0.276
[52,     1] loss: 0.278
[53,     1] loss: 0.258
[54,     1] loss: 0.222
[55,     1] loss: 0.283
[56,     1] loss: 0.258
[57,     1] loss: 0.254
[58,     1] loss: 0.249
[59,     1] loss: 0.245
[60,     1] loss: 0.291
[61,     1] loss: 0.230
[62,     1] loss: 0.217
[63,     1] loss: 0.195
[64,     1] loss: 0.220
[65,     1] loss: 0.209
[66,     1] loss: 0.161
[67,     1] loss: 0.156
[68,     1] loss: 0.288
[69,     1] loss: 0.376
[70,     1] loss: 0.247
[71,     1] loss: 0.219
[72,     1] loss: 0.308
[73,     1] loss: 0.228
[74,     1] loss: 0.211
[75,     1] loss: 0.272
[76,     1] loss: 0.184
[77,     1] loss: 0.199
[78,     1] loss: 0.164
[79,     1] loss: 0.178
[80,     1] loss: 0.171
[81,     1] loss: 0.218
[82,     1] loss: 0.239
[83,     1] loss: 0.227
[84,     1] loss: 0.255
[85,     1] loss: 0.156
[86,     1] loss: 0.217
[87,     1] loss: 0.189
[88,     1] loss: 0.204
[89,     1] loss: 0.193
[90,     1] loss: 0.144
[91,     1] loss: 0.220
[92,     1] loss: 0.136
[93,     1] loss: 0.363
[94,     1] loss: 0.240
[95,     1] loss: 0.259
[96,     1] loss: 0.289
[97,     1] loss: 0.279
[98,     1] loss: 0.237
[99,     1] loss: 0.252
[100,     1] loss: 0.260
[101,     1] loss: 0.237
[102,     1] loss: 0.239
[103,     1] loss: 0.199
[104,     1] loss: 0.235
[105,     1] loss: 0.193
[106,     1] loss: 0.184
[107,     1] loss: 0.155
[108,     1] loss: 0.192
[109,     1] loss: 0.201
[110,     1] loss: 0.167
[111,     1] loss: 0.149
[112,     1] loss: 0.165
[113,     1] loss: 0.175
[114,     1] loss: 0.144
[115,     1] loss: 0.201
[116,     1] loss: 0.202
[117,     1] loss: 0.288
[118,     1] loss: 0.251
[119,     1] loss: 0.206
[120,     1] loss: 0.193
Early stopping applied (best metric=0.32851365208625793)
Finished Training
Total time taken: 13.203227758407593
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.703
[2,     1] loss: 0.693
[3,     1] loss: 0.691
[4,     1] loss: 0.686
[5,     1] loss: 0.688
[6,     1] loss: 0.677
[7,     1] loss: 0.646
[8,     1] loss: 0.636
[9,     1] loss: 0.601
[10,     1] loss: 0.563
[11,     1] loss: 0.512
[12,     1] loss: 0.485
[13,     1] loss: 0.432
[14,     1] loss: 0.436
[15,     1] loss: 0.403
[16,     1] loss: 0.380
[17,     1] loss: 0.353
[18,     1] loss: 0.324
[19,     1] loss: 0.348
[20,     1] loss: 0.348
[21,     1] loss: 0.357
[22,     1] loss: 0.340
[23,     1] loss: 0.319
[24,     1] loss: 0.302
[25,     1] loss: 0.387
[26,     1] loss: 0.338
[27,     1] loss: 0.390
[28,     1] loss: 0.328
[29,     1] loss: 0.369
[30,     1] loss: 0.341
[31,     1] loss: 0.325
[32,     1] loss: 0.350
[33,     1] loss: 0.330
[34,     1] loss: 0.305
[35,     1] loss: 0.364
[36,     1] loss: 0.258
[37,     1] loss: 0.260
[38,     1] loss: 0.394
[39,     1] loss: 0.311
[40,     1] loss: 0.449
[41,     1] loss: 0.257
[42,     1] loss: 0.345
[43,     1] loss: 0.350
[44,     1] loss: 0.323
[45,     1] loss: 0.337
[46,     1] loss: 0.281
[47,     1] loss: 0.393
[48,     1] loss: 0.345
[49,     1] loss: 0.338
[50,     1] loss: 0.316
[51,     1] loss: 0.297
[52,     1] loss: 0.300
[53,     1] loss: 0.318
[54,     1] loss: 0.304
[55,     1] loss: 0.274
[56,     1] loss: 0.305
[57,     1] loss: 0.232
[58,     1] loss: 0.254
[59,     1] loss: 0.244
[60,     1] loss: 0.224
[61,     1] loss: 0.292
[62,     1] loss: 0.280
[63,     1] loss: 0.265
[64,     1] loss: 0.255
[65,     1] loss: 0.267
[66,     1] loss: 0.251
[67,     1] loss: 0.308
[68,     1] loss: 0.268
[69,     1] loss: 0.334
[70,     1] loss: 0.252
[71,     1] loss: 0.256
[72,     1] loss: 0.315
[73,     1] loss: 0.342
[74,     1] loss: 0.298
[75,     1] loss: 0.296
[76,     1] loss: 0.258
[77,     1] loss: 0.282
[78,     1] loss: 0.251
[79,     1] loss: 0.249
[80,     1] loss: 0.294
[81,     1] loss: 0.340
[82,     1] loss: 0.352
[83,     1] loss: 0.226
[84,     1] loss: 0.371
[85,     1] loss: 0.329
[86,     1] loss: 0.455
[87,     1] loss: 0.283
[88,     1] loss: 0.270
[89,     1] loss: 0.320
[90,     1] loss: 0.293
[91,     1] loss: 0.334
[92,     1] loss: 0.268
[93,     1] loss: 0.250
[94,     1] loss: 0.266
[95,     1] loss: 0.210
[96,     1] loss: 0.217
[97,     1] loss: 0.216
[98,     1] loss: 0.221
[99,     1] loss: 0.326
[100,     1] loss: 0.286
[101,     1] loss: 0.256
[102,     1] loss: 0.278
[103,     1] loss: 0.237
[104,     1] loss: 0.265
[105,     1] loss: 0.217
[106,     1] loss: 0.239
[107,     1] loss: 0.265
[108,     1] loss: 0.298
[109,     1] loss: 0.250
[110,     1] loss: 0.273
[111,     1] loss: 0.285
[112,     1] loss: 0.273
[113,     1] loss: 0.241
[114,     1] loss: 0.274
[115,     1] loss: 0.205
[116,     1] loss: 0.249
[117,     1] loss: 0.199
[118,     1] loss: 0.185
[119,     1] loss: 0.187
[120,     1] loss: 0.171
[121,     1] loss: 0.203
[122,     1] loss: 0.199
[123,     1] loss: 0.208
[124,     1] loss: 0.594
[125,     1] loss: 0.563
[126,     1] loss: 0.287
[127,     1] loss: 0.435
[128,     1] loss: 0.387
[129,     1] loss: 0.348
[130,     1] loss: 0.373
[131,     1] loss: 0.328
[132,     1] loss: 0.304
[133,     1] loss: 0.363
[134,     1] loss: 0.302
[135,     1] loss: 0.297
[136,     1] loss: 0.256
[137,     1] loss: 0.248
[138,     1] loss: 0.223
[139,     1] loss: 0.274
[140,     1] loss: 0.226
[141,     1] loss: 0.261
[142,     1] loss: 0.237
[143,     1] loss: 0.271
[144,     1] loss: 0.191
[145,     1] loss: 0.223
[146,     1] loss: 0.233
[147,     1] loss: 0.197
[148,     1] loss: 0.241
[149,     1] loss: 0.253
[150,     1] loss: 0.199
[151,     1] loss: 0.220
[152,     1] loss: 0.197
[153,     1] loss: 0.222
[154,     1] loss: 0.195
[155,     1] loss: 0.178
[156,     1] loss: 0.322
[157,     1] loss: 0.576
[158,     1] loss: 0.296
[159,     1] loss: 0.388
[160,     1] loss: 0.323
[161,     1] loss: 0.296
[162,     1] loss: 0.348
[163,     1] loss: 0.331
[164,     1] loss: 0.290
[165,     1] loss: 0.270
[166,     1] loss: 0.265
[167,     1] loss: 0.220
[168,     1] loss: 0.212
[169,     1] loss: 0.203
[170,     1] loss: 0.222
[171,     1] loss: 0.198
[172,     1] loss: 0.188
[173,     1] loss: 0.190
[174,     1] loss: 0.277
[175,     1] loss: 0.165
[176,     1] loss: 0.212
[177,     1] loss: 0.190
[178,     1] loss: 0.217
[179,     1] loss: 0.203
[180,     1] loss: 0.250
[181,     1] loss: 0.433
[182,     1] loss: 0.357
[183,     1] loss: 0.459
[184,     1] loss: 0.342
[185,     1] loss: 0.395
[186,     1] loss: 0.354
[187,     1] loss: 0.336
[188,     1] loss: 0.343
[189,     1] loss: 0.298
[190,     1] loss: 0.293
[191,     1] loss: 0.339
[192,     1] loss: 0.281
[193,     1] loss: 0.325
[194,     1] loss: 0.314
[195,     1] loss: 0.233
[196,     1] loss: 0.264
Early stopping applied (best metric=0.1618795096874237)
Finished Training
Total time taken: 21.512843370437622
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.694
[2,     1] loss: 0.691
[3,     1] loss: 0.673
[4,     1] loss: 0.648
[5,     1] loss: 0.597
[6,     1] loss: 0.522
[7,     1] loss: 0.493
[8,     1] loss: 0.458
[9,     1] loss: 0.425
[10,     1] loss: 0.366
[11,     1] loss: 0.407
[12,     1] loss: 0.391
[13,     1] loss: 0.352
[14,     1] loss: 0.401
[15,     1] loss: 0.331
[16,     1] loss: 0.333
[17,     1] loss: 0.298
[18,     1] loss: 0.288
[19,     1] loss: 0.302
[20,     1] loss: 0.274
[21,     1] loss: 0.299
[22,     1] loss: 0.270
[23,     1] loss: 0.365
[24,     1] loss: 0.262
[25,     1] loss: 0.278
[26,     1] loss: 0.281
[27,     1] loss: 0.369
[28,     1] loss: 0.259
[29,     1] loss: 0.329
[30,     1] loss: 0.316
[31,     1] loss: 0.233
[32,     1] loss: 0.275
[33,     1] loss: 0.235
[34,     1] loss: 0.246
[35,     1] loss: 0.197
[36,     1] loss: 0.169
[37,     1] loss: 0.216
[38,     1] loss: 0.224
[39,     1] loss: 0.537
[40,     1] loss: 0.389
[41,     1] loss: 0.353
[42,     1] loss: 0.269
[43,     1] loss: 0.295
[44,     1] loss: 0.324
[45,     1] loss: 0.285
[46,     1] loss: 0.315
[47,     1] loss: 0.241
[48,     1] loss: 0.276
[49,     1] loss: 0.229
[50,     1] loss: 0.232
[51,     1] loss: 0.250
[52,     1] loss: 0.202
[53,     1] loss: 0.211
[54,     1] loss: 0.240
[55,     1] loss: 0.202
[56,     1] loss: 0.252
[57,     1] loss: 0.204
[58,     1] loss: 0.183
[59,     1] loss: 0.205
[60,     1] loss: 0.175
[61,     1] loss: 0.184
[62,     1] loss: 0.176
[63,     1] loss: 0.165
[64,     1] loss: 0.174
[65,     1] loss: 0.161
[66,     1] loss: 0.186
[67,     1] loss: 0.224
[68,     1] loss: 0.241
[69,     1] loss: 0.358
[70,     1] loss: 0.408
[71,     1] loss: 0.429
[72,     1] loss: 0.291
[73,     1] loss: 0.363
[74,     1] loss: 0.338
[75,     1] loss: 0.268
[76,     1] loss: 0.284
[77,     1] loss: 0.283
[78,     1] loss: 0.267
[79,     1] loss: 0.220
[80,     1] loss: 0.220
[81,     1] loss: 0.176
[82,     1] loss: 0.173
[83,     1] loss: 0.216
[84,     1] loss: 0.187
[85,     1] loss: 0.201
[86,     1] loss: 0.149
[87,     1] loss: 0.196
[88,     1] loss: 0.222
[89,     1] loss: 0.165
[90,     1] loss: 0.295
[91,     1] loss: 0.178
[92,     1] loss: 0.309
[93,     1] loss: 0.213
[94,     1] loss: 0.179
[95,     1] loss: 0.228
[96,     1] loss: 0.165
[97,     1] loss: 0.182
[98,     1] loss: 0.174
[99,     1] loss: 0.161
[100,     1] loss: 0.167
[101,     1] loss: 0.149
Early stopping applied (best metric=0.21673110127449036)
Finished Training
Total time taken: 11.128000497817993
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.698
[3,     1] loss: 0.678
[4,     1] loss: 0.645
[5,     1] loss: 0.585
[6,     1] loss: 0.560
[7,     1] loss: 0.509
[8,     1] loss: 0.516
[9,     1] loss: 0.408
[10,     1] loss: 0.373
[11,     1] loss: 0.328
[12,     1] loss: 0.385
[13,     1] loss: 0.303
[14,     1] loss: 0.322
[15,     1] loss: 0.367
[16,     1] loss: 0.324
[17,     1] loss: 0.288
[18,     1] loss: 0.334
[19,     1] loss: 0.333
[20,     1] loss: 0.291
[21,     1] loss: 0.342
[22,     1] loss: 0.326
[23,     1] loss: 0.297
[24,     1] loss: 0.277
[25,     1] loss: 0.306
[26,     1] loss: 0.244
[27,     1] loss: 0.268
[28,     1] loss: 0.239
[29,     1] loss: 0.215
[30,     1] loss: 0.280
[31,     1] loss: 0.264
[32,     1] loss: 0.286
[33,     1] loss: 0.251
[34,     1] loss: 0.227
[35,     1] loss: 0.270
[36,     1] loss: 0.259
[37,     1] loss: 0.239
[38,     1] loss: 0.272
[39,     1] loss: 0.230
[40,     1] loss: 0.220
[41,     1] loss: 0.279
[42,     1] loss: 0.228
[43,     1] loss: 0.238
[44,     1] loss: 0.210
[45,     1] loss: 0.180
[46,     1] loss: 0.196
[47,     1] loss: 0.195
[48,     1] loss: 0.150
[49,     1] loss: 0.155
[50,     1] loss: 0.167
[51,     1] loss: 0.166
[52,     1] loss: 0.147
[53,     1] loss: 0.160
[54,     1] loss: 0.154
[55,     1] loss: 0.142
[56,     1] loss: 0.140
[57,     1] loss: 0.215
[58,     1] loss: 0.125
[59,     1] loss: 0.136
[60,     1] loss: 0.166
[61,     1] loss: 0.166
[62,     1] loss: 0.134
[63,     1] loss: 0.152
[64,     1] loss: 0.212
[65,     1] loss: 0.177
[66,     1] loss: 0.344
[67,     1] loss: 0.645
[68,     1] loss: 0.219
[69,     1] loss: 0.358
[70,     1] loss: 0.270
[71,     1] loss: 0.278
[72,     1] loss: 0.271
[73,     1] loss: 0.294
[74,     1] loss: 0.229
[75,     1] loss: 0.214
[76,     1] loss: 0.214
[77,     1] loss: 0.213
[78,     1] loss: 0.142
[79,     1] loss: 0.172
[80,     1] loss: 0.196
[81,     1] loss: 0.225
[82,     1] loss: 0.187
[83,     1] loss: 0.224
[84,     1] loss: 0.169
[85,     1] loss: 0.213
[86,     1] loss: 0.164
Early stopping applied (best metric=0.38435280323028564)
Finished Training
Total time taken: 9.483001232147217
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.690
[2,     1] loss: 0.695
[3,     1] loss: 0.680
[4,     1] loss: 0.668
[5,     1] loss: 0.626
[6,     1] loss: 0.615
[7,     1] loss: 0.548
[8,     1] loss: 0.512
[9,     1] loss: 0.443
[10,     1] loss: 0.398
[11,     1] loss: 0.517
[12,     1] loss: 0.392
[13,     1] loss: 0.345
[14,     1] loss: 0.337
[15,     1] loss: 0.344
[16,     1] loss: 0.322
[17,     1] loss: 0.266
[18,     1] loss: 0.289
[19,     1] loss: 0.364
[20,     1] loss: 0.353
[21,     1] loss: 0.310
[22,     1] loss: 0.351
[23,     1] loss: 0.346
[24,     1] loss: 0.312
[25,     1] loss: 0.280
[26,     1] loss: 0.310
[27,     1] loss: 0.259
[28,     1] loss: 0.254
[29,     1] loss: 0.249
[30,     1] loss: 0.281
[31,     1] loss: 0.250
[32,     1] loss: 0.241
[33,     1] loss: 0.227
[34,     1] loss: 0.232
[35,     1] loss: 0.246
[36,     1] loss: 0.241
[37,     1] loss: 0.305
[38,     1] loss: 0.231
[39,     1] loss: 0.354
[40,     1] loss: 0.197
[41,     1] loss: 0.224
[42,     1] loss: 0.260
[43,     1] loss: 0.245
[44,     1] loss: 0.169
[45,     1] loss: 0.214
[46,     1] loss: 0.304
[47,     1] loss: 0.276
[48,     1] loss: 0.254
[49,     1] loss: 0.220
[50,     1] loss: 0.214
[51,     1] loss: 0.223
[52,     1] loss: 0.207
[53,     1] loss: 0.191
[54,     1] loss: 0.181
[55,     1] loss: 0.167
[56,     1] loss: 0.186
[57,     1] loss: 0.189
[58,     1] loss: 0.153
[59,     1] loss: 0.206
[60,     1] loss: 0.154
[61,     1] loss: 0.164
[62,     1] loss: 0.165
[63,     1] loss: 0.219
[64,     1] loss: 0.293
[65,     1] loss: 0.253
[66,     1] loss: 0.242
[67,     1] loss: 0.252
[68,     1] loss: 0.220
[69,     1] loss: 0.244
[70,     1] loss: 0.243
[71,     1] loss: 0.208
[72,     1] loss: 0.166
[73,     1] loss: 0.153
[74,     1] loss: 0.169
[75,     1] loss: 0.223
[76,     1] loss: 0.371
[77,     1] loss: 0.393
[78,     1] loss: 0.303
[79,     1] loss: 0.286
[80,     1] loss: 0.259
[81,     1] loss: 0.297
[82,     1] loss: 0.297
[83,     1] loss: 0.266
[84,     1] loss: 0.273
[85,     1] loss: 0.249
[86,     1] loss: 0.231
[87,     1] loss: 0.235
[88,     1] loss: 0.206
[89,     1] loss: 0.238
[90,     1] loss: 0.188
[91,     1] loss: 0.153
[92,     1] loss: 0.219
[93,     1] loss: 0.181
[94,     1] loss: 0.207
[95,     1] loss: 0.262
[96,     1] loss: 0.242
[97,     1] loss: 0.197
[98,     1] loss: 0.240
[99,     1] loss: 0.171
[100,     1] loss: 0.149
[101,     1] loss: 0.196
[102,     1] loss: 0.192
[103,     1] loss: 0.320
[104,     1] loss: 0.272
[105,     1] loss: 0.174
[106,     1] loss: 0.176
[107,     1] loss: 0.207
[108,     1] loss: 0.205
[109,     1] loss: 0.144
[110,     1] loss: 0.188
[111,     1] loss: 0.139
[112,     1] loss: 0.185
[113,     1] loss: 0.145
[114,     1] loss: 0.376
[115,     1] loss: 0.661
[116,     1] loss: 0.283
[117,     1] loss: 0.444
[118,     1] loss: 0.259
[119,     1] loss: 0.273
[120,     1] loss: 0.277
[121,     1] loss: 0.277
[122,     1] loss: 0.289
[123,     1] loss: 0.277
[124,     1] loss: 0.253
[125,     1] loss: 0.213
[126,     1] loss: 0.215
[127,     1] loss: 0.217
[128,     1] loss: 0.202
[129,     1] loss: 0.169
[130,     1] loss: 0.212
[131,     1] loss: 0.175
[132,     1] loss: 0.144
[133,     1] loss: 0.148
[134,     1] loss: 0.168
[135,     1] loss: 0.132
[136,     1] loss: 0.130
[137,     1] loss: 0.182
[138,     1] loss: 0.179
[139,     1] loss: 0.172
[140,     1] loss: 0.161
[141,     1] loss: 0.212
[142,     1] loss: 0.173
[143,     1] loss: 0.143
[144,     1] loss: 0.172
[145,     1] loss: 0.195
[146,     1] loss: 0.167
[147,     1] loss: 0.190
[148,     1] loss: 0.136
[149,     1] loss: 0.185
[150,     1] loss: 0.145
[151,     1] loss: 0.150
[152,     1] loss: 0.236
Early stopping applied (best metric=0.3236710727214813)
Finished Training
Total time taken: 16.68459916114807
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.691
[3,     1] loss: 0.661
[4,     1] loss: 0.601
[5,     1] loss: 0.550
[6,     1] loss: 0.473
[7,     1] loss: 0.406
[8,     1] loss: 0.348
[9,     1] loss: 0.418
[10,     1] loss: 0.453
[11,     1] loss: 0.396
[12,     1] loss: 0.335
[13,     1] loss: 0.345
[14,     1] loss: 0.325
[15,     1] loss: 0.273
[16,     1] loss: 0.338
[17,     1] loss: 0.273
[18,     1] loss: 0.282
[19,     1] loss: 0.230
[20,     1] loss: 0.266
[21,     1] loss: 0.217
[22,     1] loss: 0.193
[23,     1] loss: 0.198
[24,     1] loss: 0.313
[25,     1] loss: 0.332
[26,     1] loss: 0.268
[27,     1] loss: 0.344
[28,     1] loss: 0.287
[29,     1] loss: 0.304
[30,     1] loss: 0.278
[31,     1] loss: 0.270
[32,     1] loss: 0.287
[33,     1] loss: 0.254
[34,     1] loss: 0.235
[35,     1] loss: 0.241
[36,     1] loss: 0.219
[37,     1] loss: 0.229
[38,     1] loss: 0.284
[39,     1] loss: 0.260
[40,     1] loss: 0.300
[41,     1] loss: 0.302
[42,     1] loss: 0.242
[43,     1] loss: 0.278
[44,     1] loss: 0.257
[45,     1] loss: 0.211
[46,     1] loss: 0.243
[47,     1] loss: 0.217
[48,     1] loss: 0.176
[49,     1] loss: 0.190
[50,     1] loss: 0.209
[51,     1] loss: 0.287
[52,     1] loss: 0.368
[53,     1] loss: 0.263
[54,     1] loss: 0.390
[55,     1] loss: 0.265
[56,     1] loss: 0.298
[57,     1] loss: 0.318
[58,     1] loss: 0.273
[59,     1] loss: 0.293
[60,     1] loss: 0.291
[61,     1] loss: 0.235
[62,     1] loss: 0.273
[63,     1] loss: 0.287
[64,     1] loss: 0.228
[65,     1] loss: 0.236
[66,     1] loss: 0.228
[67,     1] loss: 0.210
[68,     1] loss: 0.193
[69,     1] loss: 0.234
[70,     1] loss: 0.201
[71,     1] loss: 0.234
[72,     1] loss: 0.243
[73,     1] loss: 0.191
[74,     1] loss: 0.196
[75,     1] loss: 0.202
[76,     1] loss: 0.216
[77,     1] loss: 0.203
[78,     1] loss: 0.218
[79,     1] loss: 0.214
[80,     1] loss: 0.154
[81,     1] loss: 0.163
[82,     1] loss: 0.180
[83,     1] loss: 0.144
[84,     1] loss: 0.149
[85,     1] loss: 0.188
[86,     1] loss: 0.233
[87,     1] loss: 0.739
[88,     1] loss: 0.516
[89,     1] loss: 0.409
[90,     1] loss: 0.248
[91,     1] loss: 0.378
[92,     1] loss: 0.377
[93,     1] loss: 0.337
[94,     1] loss: 0.316
[95,     1] loss: 0.308
[96,     1] loss: 0.293
[97,     1] loss: 0.274
[98,     1] loss: 0.256
[99,     1] loss: 0.235
[100,     1] loss: 0.223
[101,     1] loss: 0.207
[102,     1] loss: 0.192
[103,     1] loss: 0.185
[104,     1] loss: 0.202
[105,     1] loss: 0.202
[106,     1] loss: 0.219
[107,     1] loss: 0.155
[108,     1] loss: 0.191
[109,     1] loss: 0.191
[110,     1] loss: 0.179
[111,     1] loss: 0.178
[112,     1] loss: 0.175
[113,     1] loss: 0.155
[114,     1] loss: 0.197
[115,     1] loss: 0.153
[116,     1] loss: 0.188
[117,     1] loss: 0.153
[118,     1] loss: 0.157
[119,     1] loss: 0.139
[120,     1] loss: 0.145
[121,     1] loss: 0.202
[122,     1] loss: 0.462
[123,     1] loss: 0.355
[124,     1] loss: 0.354
[125,     1] loss: 0.389
[126,     1] loss: 0.286
[127,     1] loss: 0.278
[128,     1] loss: 0.263
[129,     1] loss: 0.252
[130,     1] loss: 0.272
[131,     1] loss: 0.274
[132,     1] loss: 0.245
[133,     1] loss: 0.231
Early stopping applied (best metric=0.3452607989311218)
Finished Training
Total time taken: 14.634619235992432
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.697
[2,     1] loss: 0.690
[3,     1] loss: 0.676
[4,     1] loss: 0.634
[5,     1] loss: 0.581
[6,     1] loss: 0.496
[7,     1] loss: 0.463
[8,     1] loss: 0.403
[9,     1] loss: 0.396
[10,     1] loss: 0.324
[11,     1] loss: 0.319
[12,     1] loss: 0.325
[13,     1] loss: 0.285
[14,     1] loss: 0.314
[15,     1] loss: 0.257
[16,     1] loss: 0.361
[17,     1] loss: 0.264
[18,     1] loss: 0.412
[19,     1] loss: 0.217
[20,     1] loss: 0.349
[21,     1] loss: 0.325
[22,     1] loss: 0.273
[23,     1] loss: 0.230
[24,     1] loss: 0.304
[25,     1] loss: 0.274
[26,     1] loss: 0.202
[27,     1] loss: 0.211
[28,     1] loss: 0.231
[29,     1] loss: 0.234
[30,     1] loss: 0.157
[31,     1] loss: 0.247
[32,     1] loss: 0.181
[33,     1] loss: 0.236
[34,     1] loss: 0.163
[35,     1] loss: 0.189
[36,     1] loss: 0.133
[37,     1] loss: 0.179
[38,     1] loss: 0.209
[39,     1] loss: 0.163
[40,     1] loss: 0.137
[41,     1] loss: 0.183
[42,     1] loss: 0.231
[43,     1] loss: 0.461
[44,     1] loss: 0.253
[45,     1] loss: 0.290
[46,     1] loss: 0.315
[47,     1] loss: 0.268
[48,     1] loss: 0.304
[49,     1] loss: 0.269
[50,     1] loss: 0.264
[51,     1] loss: 0.235
[52,     1] loss: 0.216
[53,     1] loss: 0.247
[54,     1] loss: 0.237
[55,     1] loss: 0.190
[56,     1] loss: 0.218
[57,     1] loss: 0.249
[58,     1] loss: 0.241
[59,     1] loss: 0.285
[60,     1] loss: 0.258
[61,     1] loss: 0.456
[62,     1] loss: 0.254
[63,     1] loss: 0.406
[64,     1] loss: 0.263
[65,     1] loss: 0.280
[66,     1] loss: 0.266
[67,     1] loss: 0.282
[68,     1] loss: 0.250
[69,     1] loss: 0.248
[70,     1] loss: 0.256
[71,     1] loss: 0.213
[72,     1] loss: 0.222
[73,     1] loss: 0.188
Early stopping applied (best metric=0.35165131092071533)
Finished Training
Total time taken: 8.040000915527344
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.702
[2,     1] loss: 0.693
[3,     1] loss: 0.688
[4,     1] loss: 0.676
[5,     1] loss: 0.646
[6,     1] loss: 0.624
[7,     1] loss: 0.559
[8,     1] loss: 0.511
[9,     1] loss: 0.492
[10,     1] loss: 0.458
[11,     1] loss: 0.412
[12,     1] loss: 0.405
[13,     1] loss: 0.362
[14,     1] loss: 0.342
[15,     1] loss: 0.331
[16,     1] loss: 0.484
[17,     1] loss: 0.391
[18,     1] loss: 0.347
[19,     1] loss: 0.344
[20,     1] loss: 0.384
[21,     1] loss: 0.349
[22,     1] loss: 0.380
[23,     1] loss: 0.350
[24,     1] loss: 0.305
[25,     1] loss: 0.318
[26,     1] loss: 0.294
[27,     1] loss: 0.274
[28,     1] loss: 0.221
[29,     1] loss: 0.264
[30,     1] loss: 0.234
[31,     1] loss: 0.271
[32,     1] loss: 0.217
[33,     1] loss: 0.205
[34,     1] loss: 0.228
[35,     1] loss: 0.214
[36,     1] loss: 0.267
[37,     1] loss: 0.248
[38,     1] loss: 0.249
[39,     1] loss: 0.196
[40,     1] loss: 0.190
[41,     1] loss: 0.212
[42,     1] loss: 0.197
[43,     1] loss: 0.145
[44,     1] loss: 0.252
[45,     1] loss: 0.164
[46,     1] loss: 0.886
[47,     1] loss: 0.307
[48,     1] loss: 0.288
[49,     1] loss: 0.282
[50,     1] loss: 0.362
[51,     1] loss: 0.349
[52,     1] loss: 0.342
[53,     1] loss: 0.344
[54,     1] loss: 0.355
[55,     1] loss: 0.350
[56,     1] loss: 0.311
[57,     1] loss: 0.292
[58,     1] loss: 0.257
[59,     1] loss: 0.214
[60,     1] loss: 0.253
[61,     1] loss: 0.181
[62,     1] loss: 0.200
[63,     1] loss: 0.185
[64,     1] loss: 0.171
[65,     1] loss: 0.204
[66,     1] loss: 0.196
[67,     1] loss: 0.102
[68,     1] loss: 0.169
[69,     1] loss: 0.201
[70,     1] loss: 0.147
[71,     1] loss: 0.220
[72,     1] loss: 0.159
[73,     1] loss: 0.115
[74,     1] loss: 0.196
[75,     1] loss: 0.382
[76,     1] loss: 0.847
[77,     1] loss: 0.254
[78,     1] loss: 0.343
[79,     1] loss: 0.327
[80,     1] loss: 0.310
[81,     1] loss: 0.329
[82,     1] loss: 0.368
[83,     1] loss: 0.308
[84,     1] loss: 0.279
[85,     1] loss: 0.291
[86,     1] loss: 0.256
[87,     1] loss: 0.218
[88,     1] loss: 0.248
[89,     1] loss: 0.212
[90,     1] loss: 0.180
[91,     1] loss: 0.198
[92,     1] loss: 0.172
[93,     1] loss: 0.217
[94,     1] loss: 0.158
[95,     1] loss: 0.186
[96,     1] loss: 0.229
[97,     1] loss: 0.187
[98,     1] loss: 0.190
[99,     1] loss: 0.174
[100,     1] loss: 0.186
[101,     1] loss: 0.212
[102,     1] loss: 0.228
[103,     1] loss: 0.164
[104,     1] loss: 0.187
[105,     1] loss: 0.261
[106,     1] loss: 0.247
[107,     1] loss: 0.160
[108,     1] loss: 0.196
[109,     1] loss: 0.215
[110,     1] loss: 0.215
[111,     1] loss: 0.209
[112,     1] loss: 0.213
[113,     1] loss: 0.222
[114,     1] loss: 0.188
[115,     1] loss: 0.188
[116,     1] loss: 0.165
[117,     1] loss: 0.148
[118,     1] loss: 0.215
[119,     1] loss: 0.257
[120,     1] loss: 0.194
[121,     1] loss: 0.178
[122,     1] loss: 0.176
[123,     1] loss: 0.159
[124,     1] loss: 0.223
[125,     1] loss: 0.223
[126,     1] loss: 0.165
[127,     1] loss: 0.191
[128,     1] loss: 0.176
[129,     1] loss: 0.154
[130,     1] loss: 0.150
[131,     1] loss: 0.188
[132,     1] loss: 0.165
[133,     1] loss: 0.133
[134,     1] loss: 0.177
[135,     1] loss: 0.170
[136,     1] loss: 0.239
[137,     1] loss: 0.183
[138,     1] loss: 0.169
[139,     1] loss: 0.211
[140,     1] loss: 0.203
[141,     1] loss: 0.173
[142,     1] loss: 0.172
[143,     1] loss: 0.196
[144,     1] loss: 0.274
[145,     1] loss: 0.718
[146,     1] loss: 0.314
[147,     1] loss: 0.356
[148,     1] loss: 0.301
[149,     1] loss: 0.352
Early stopping applied (best metric=0.17225955426692963)
Finished Training
Total time taken: 16.36560893058777
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.689
[3,     1] loss: 0.676
[4,     1] loss: 0.654
[5,     1] loss: 0.623
[6,     1] loss: 0.562
[7,     1] loss: 0.518
[8,     1] loss: 0.474
[9,     1] loss: 0.414
[10,     1] loss: 0.401
[11,     1] loss: 0.385
[12,     1] loss: 0.349
[13,     1] loss: 0.355
[14,     1] loss: 0.380
[15,     1] loss: 0.326
[16,     1] loss: 0.331
[17,     1] loss: 0.302
[18,     1] loss: 0.276
[19,     1] loss: 0.248
[20,     1] loss: 0.341
[21,     1] loss: 0.244
[22,     1] loss: 0.252
[23,     1] loss: 0.219
[24,     1] loss: 0.407
[25,     1] loss: 0.283
[26,     1] loss: 0.406
[27,     1] loss: 0.320
[28,     1] loss: 0.358
[29,     1] loss: 0.362
[30,     1] loss: 0.315
[31,     1] loss: 0.330
[32,     1] loss: 0.340
[33,     1] loss: 0.286
[34,     1] loss: 0.291
[35,     1] loss: 0.239
[36,     1] loss: 0.251
[37,     1] loss: 0.244
[38,     1] loss: 0.330
[39,     1] loss: 0.228
[40,     1] loss: 0.247
[41,     1] loss: 0.225
[42,     1] loss: 0.271
[43,     1] loss: 0.222
[44,     1] loss: 0.248
[45,     1] loss: 0.229
[46,     1] loss: 0.338
[47,     1] loss: 0.298
[48,     1] loss: 0.237
[49,     1] loss: 0.289
[50,     1] loss: 0.246
[51,     1] loss: 0.231
[52,     1] loss: 0.178
[53,     1] loss: 0.208
[54,     1] loss: 0.206
[55,     1] loss: 0.208
[56,     1] loss: 0.303
[57,     1] loss: 0.274
[58,     1] loss: 0.241
[59,     1] loss: 0.238
[60,     1] loss: 0.260
[61,     1] loss: 0.270
[62,     1] loss: 0.228
[63,     1] loss: 0.243
Early stopping applied (best metric=0.2584366500377655)
Finished Training
Total time taken: 6.9516284465789795
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.693
[3,     1] loss: 0.682
[4,     1] loss: 0.640
[5,     1] loss: 0.606
[6,     1] loss: 0.577
[7,     1] loss: 0.502
[8,     1] loss: 0.425
[9,     1] loss: 0.500
[10,     1] loss: 0.434
[11,     1] loss: 0.440
[12,     1] loss: 0.404
[13,     1] loss: 0.372
[14,     1] loss: 0.427
[15,     1] loss: 0.412
[16,     1] loss: 0.403
[17,     1] loss: 0.428
[18,     1] loss: 0.372
[19,     1] loss: 0.407
[20,     1] loss: 0.321
[21,     1] loss: 0.337
[22,     1] loss: 0.313
[23,     1] loss: 0.327
[24,     1] loss: 0.349
[25,     1] loss: 0.229
[26,     1] loss: 0.282
[27,     1] loss: 0.693
[28,     1] loss: 0.294
[29,     1] loss: 0.317
[30,     1] loss: 0.287
[31,     1] loss: 0.324
[32,     1] loss: 0.357
[33,     1] loss: 0.293
[34,     1] loss: 0.326
[35,     1] loss: 0.311
[36,     1] loss: 0.295
[37,     1] loss: 0.272
[38,     1] loss: 0.258
[39,     1] loss: 0.207
[40,     1] loss: 0.204
[41,     1] loss: 0.190
[42,     1] loss: 0.382
[43,     1] loss: 0.252
[44,     1] loss: 0.371
[45,     1] loss: 0.281
[46,     1] loss: 0.340
[47,     1] loss: 0.268
[48,     1] loss: 0.256
[49,     1] loss: 0.287
[50,     1] loss: 0.272
[51,     1] loss: 0.252
[52,     1] loss: 0.206
[53,     1] loss: 0.200
[54,     1] loss: 0.191
[55,     1] loss: 0.155
[56,     1] loss: 0.170
[57,     1] loss: 0.149
[58,     1] loss: 0.218
[59,     1] loss: 0.256
[60,     1] loss: 0.297
[61,     1] loss: 0.205
[62,     1] loss: 0.231
[63,     1] loss: 0.221
[64,     1] loss: 0.224
[65,     1] loss: 0.215
[66,     1] loss: 0.217
[67,     1] loss: 0.200
[68,     1] loss: 0.172
Early stopping applied (best metric=0.31777769327163696)
Finished Training
Total time taken: 7.483607053756714
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.698
[2,     1] loss: 0.699
[3,     1] loss: 0.685
[4,     1] loss: 0.683
[5,     1] loss: 0.674
[6,     1] loss: 0.650
[7,     1] loss: 0.624
[8,     1] loss: 0.605
[9,     1] loss: 0.567
[10,     1] loss: 0.552
[11,     1] loss: 0.492
[12,     1] loss: 0.473
[13,     1] loss: 0.415
[14,     1] loss: 0.405
[15,     1] loss: 0.383
[16,     1] loss: 0.386
[17,     1] loss: 0.341
[18,     1] loss: 0.350
[19,     1] loss: 0.299
[20,     1] loss: 0.322
[21,     1] loss: 0.311
[22,     1] loss: 0.278
[23,     1] loss: 0.279
[24,     1] loss: 0.271
[25,     1] loss: 0.298
[26,     1] loss: 0.251
[27,     1] loss: 0.394
[28,     1] loss: 0.345
[29,     1] loss: 0.371
[30,     1] loss: 0.322
[31,     1] loss: 0.299
[32,     1] loss: 0.355
[33,     1] loss: 0.293
[34,     1] loss: 0.297
[35,     1] loss: 0.305
[36,     1] loss: 0.281
[37,     1] loss: 0.340
[38,     1] loss: 0.259
[39,     1] loss: 0.308
[40,     1] loss: 0.269
[41,     1] loss: 0.315
[42,     1] loss: 0.286
[43,     1] loss: 0.256
[44,     1] loss: 0.239
[45,     1] loss: 0.215
[46,     1] loss: 0.231
[47,     1] loss: 0.202
[48,     1] loss: 0.160
[49,     1] loss: 0.178
[50,     1] loss: 0.168
[51,     1] loss: 0.266
[52,     1] loss: 0.209
[53,     1] loss: 0.225
[54,     1] loss: 0.274
[55,     1] loss: 0.219
[56,     1] loss: 0.337
[57,     1] loss: 0.225
[58,     1] loss: 0.306
[59,     1] loss: 0.188
[60,     1] loss: 0.252
[61,     1] loss: 0.209
[62,     1] loss: 0.164
[63,     1] loss: 0.186
Early stopping applied (best metric=0.38136374950408936)
Finished Training
Total time taken: 6.998999357223511
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.692
[2,     1] loss: 0.691
[3,     1] loss: 0.685
[4,     1] loss: 0.664
[5,     1] loss: 0.633
[6,     1] loss: 0.578
[7,     1] loss: 0.537
[8,     1] loss: 0.512
[9,     1] loss: 0.468
[10,     1] loss: 0.410
[11,     1] loss: 0.392
[12,     1] loss: 0.385
[13,     1] loss: 0.351
[14,     1] loss: 0.417
[15,     1] loss: 0.297
[16,     1] loss: 0.295
[17,     1] loss: 0.327
[18,     1] loss: 0.383
[19,     1] loss: 0.305
[20,     1] loss: 0.224
[21,     1] loss: 0.266
[22,     1] loss: 0.222
[23,     1] loss: 0.236
[24,     1] loss: 0.222
[25,     1] loss: 0.252
[26,     1] loss: 0.301
[27,     1] loss: 0.285
[28,     1] loss: 0.284
[29,     1] loss: 0.268
[30,     1] loss: 0.230
[31,     1] loss: 0.268
[32,     1] loss: 0.258
[33,     1] loss: 0.248
[34,     1] loss: 0.221
[35,     1] loss: 0.251
[36,     1] loss: 0.226
[37,     1] loss: 0.207
[38,     1] loss: 0.212
[39,     1] loss: 0.153
[40,     1] loss: 0.161
[41,     1] loss: 0.181
[42,     1] loss: 0.184
[43,     1] loss: 0.268
[44,     1] loss: 0.402
[45,     1] loss: 0.190
[46,     1] loss: 0.285
[47,     1] loss: 0.223
[48,     1] loss: 0.207
[49,     1] loss: 0.217
[50,     1] loss: 0.193
[51,     1] loss: 0.168
[52,     1] loss: 0.174
[53,     1] loss: 0.167
[54,     1] loss: 0.169
[55,     1] loss: 0.247
[56,     1] loss: 0.167
[57,     1] loss: 0.270
[58,     1] loss: 0.212
[59,     1] loss: 0.223
[60,     1] loss: 0.261
[61,     1] loss: 0.239
[62,     1] loss: 0.257
[63,     1] loss: 0.203
[64,     1] loss: 0.195
[65,     1] loss: 0.222
[66,     1] loss: 0.186
[67,     1] loss: 0.190
[68,     1] loss: 0.170
[69,     1] loss: 0.170
[70,     1] loss: 0.211
[71,     1] loss: 0.132
[72,     1] loss: 0.159
Early stopping applied (best metric=0.39679765701293945)
Finished Training
Total time taken: 7.963999271392822
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.691
[3,     1] loss: 0.684
[4,     1] loss: 0.659
[5,     1] loss: 0.633
[6,     1] loss: 0.596
[7,     1] loss: 0.529
[8,     1] loss: 0.499
[9,     1] loss: 0.463
[10,     1] loss: 0.426
[11,     1] loss: 0.431
[12,     1] loss: 0.382
[13,     1] loss: 0.406
[14,     1] loss: 0.443
[15,     1] loss: 0.402
[16,     1] loss: 0.341
[17,     1] loss: 0.394
[18,     1] loss: 0.380
[19,     1] loss: 0.327
[20,     1] loss: 0.339
[21,     1] loss: 0.291
[22,     1] loss: 0.339
[23,     1] loss: 0.456
[24,     1] loss: 0.370
[25,     1] loss: 0.418
[26,     1] loss: 0.386
[27,     1] loss: 0.346
[28,     1] loss: 0.388
[29,     1] loss: 0.370
[30,     1] loss: 0.345
[31,     1] loss: 0.320
[32,     1] loss: 0.288
[33,     1] loss: 0.305
[34,     1] loss: 0.242
[35,     1] loss: 0.595
[36,     1] loss: 0.321
[37,     1] loss: 0.284
[38,     1] loss: 0.322
[39,     1] loss: 0.343
[40,     1] loss: 0.337
[41,     1] loss: 0.311
[42,     1] loss: 0.297
[43,     1] loss: 0.249
[44,     1] loss: 0.229
[45,     1] loss: 0.254
[46,     1] loss: 0.210
[47,     1] loss: 0.215
[48,     1] loss: 0.187
[49,     1] loss: 0.231
[50,     1] loss: 0.173
[51,     1] loss: 0.161
[52,     1] loss: 0.161
[53,     1] loss: 0.157
[54,     1] loss: 0.143
[55,     1] loss: 0.185
[56,     1] loss: 0.116
[57,     1] loss: 0.147
[58,     1] loss: 0.239
[59,     1] loss: 0.183
[60,     1] loss: 0.155
[61,     1] loss: 0.177
[62,     1] loss: 0.201
[63,     1] loss: 0.316
[64,     1] loss: 0.243
[65,     1] loss: 0.269
[66,     1] loss: 0.158
[67,     1] loss: 0.178
[68,     1] loss: 0.197
[69,     1] loss: 0.177
[70,     1] loss: 0.292
[71,     1] loss: 0.229
[72,     1] loss: 0.187
[73,     1] loss: 0.169
[74,     1] loss: 0.165
[75,     1] loss: 0.207
[76,     1] loss: 0.167
[77,     1] loss: 0.166
[78,     1] loss: 0.164
[79,     1] loss: 0.152
[80,     1] loss: 0.116
[81,     1] loss: 0.149
[82,     1] loss: 0.118
[83,     1] loss: 0.160
[84,     1] loss: 0.126
[85,     1] loss: 0.148
Early stopping applied (best metric=0.22353558242321014)
Finished Training
Total time taken: 9.435206413269043
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.692
[3,     1] loss: 0.675
[4,     1] loss: 0.623
[5,     1] loss: 0.593
[6,     1] loss: 0.550
[7,     1] loss: 0.492
[8,     1] loss: 0.457
[9,     1] loss: 0.414
[10,     1] loss: 0.397
[11,     1] loss: 0.382
[12,     1] loss: 0.358
[13,     1] loss: 0.293
[14,     1] loss: 0.373
[15,     1] loss: 0.233
[16,     1] loss: 0.322
[17,     1] loss: 0.345
[18,     1] loss: 0.287
[19,     1] loss: 0.301
[20,     1] loss: 0.246
[21,     1] loss: 0.315
[22,     1] loss: 0.236
[23,     1] loss: 0.323
[24,     1] loss: 0.315
[25,     1] loss: 0.245
[26,     1] loss: 0.302
[27,     1] loss: 0.266
[28,     1] loss: 0.283
[29,     1] loss: 0.537
[30,     1] loss: 0.254
[31,     1] loss: 0.365
[32,     1] loss: 0.313
[33,     1] loss: 0.286
[34,     1] loss: 0.284
[35,     1] loss: 0.245
[36,     1] loss: 0.243
[37,     1] loss: 0.277
[38,     1] loss: 0.322
[39,     1] loss: 0.230
[40,     1] loss: 0.248
[41,     1] loss: 0.257
[42,     1] loss: 0.308
[43,     1] loss: 0.215
[44,     1] loss: 0.236
[45,     1] loss: 0.247
[46,     1] loss: 0.214
[47,     1] loss: 0.228
[48,     1] loss: 0.183
[49,     1] loss: 0.193
[50,     1] loss: 0.144
[51,     1] loss: 0.135
[52,     1] loss: 0.226
[53,     1] loss: 0.349
[54,     1] loss: 0.428
[55,     1] loss: 0.195
[56,     1] loss: 0.235
[57,     1] loss: 0.223
[58,     1] loss: 0.217
[59,     1] loss: 0.222
[60,     1] loss: 0.212
[61,     1] loss: 0.201
[62,     1] loss: 0.276
[63,     1] loss: 0.204
[64,     1] loss: 0.215
[65,     1] loss: 0.184
[66,     1] loss: 0.195
[67,     1] loss: 0.178
[68,     1] loss: 0.184
[69,     1] loss: 0.231
[70,     1] loss: 0.163
[71,     1] loss: 0.203
[72,     1] loss: 0.182
[73,     1] loss: 0.218
[74,     1] loss: 0.163
[75,     1] loss: 0.150
[76,     1] loss: 0.141
[77,     1] loss: 0.182
[78,     1] loss: 0.252
[79,     1] loss: 0.133
[80,     1] loss: 0.212
[81,     1] loss: 0.146
[82,     1] loss: 0.173
[83,     1] loss: 0.125
[84,     1] loss: 0.123
[85,     1] loss: 0.123
[86,     1] loss: 0.124
[87,     1] loss: 0.336
[88,     1] loss: 0.594
[89,     1] loss: 0.432
[90,     1] loss: 0.377
[91,     1] loss: 0.294
[92,     1] loss: 0.309
[93,     1] loss: 0.354
[94,     1] loss: 0.312
[95,     1] loss: 0.297
[96,     1] loss: 0.266
[97,     1] loss: 0.276
[98,     1] loss: 0.241
[99,     1] loss: 0.194
[100,     1] loss: 0.211
[101,     1] loss: 0.188
[102,     1] loss: 0.210
[103,     1] loss: 0.249
[104,     1] loss: 0.142
[105,     1] loss: 0.182
[106,     1] loss: 0.173
[107,     1] loss: 0.188
[108,     1] loss: 0.177
[109,     1] loss: 0.185
[110,     1] loss: 0.162
[111,     1] loss: 0.197
[112,     1] loss: 0.172
[113,     1] loss: 0.266
[114,     1] loss: 0.151
[115,     1] loss: 0.217
[116,     1] loss: 0.207
[117,     1] loss: 0.301
[118,     1] loss: 0.221
[119,     1] loss: 0.210
[120,     1] loss: 0.192
[121,     1] loss: 0.171
[122,     1] loss: 0.215
[123,     1] loss: 0.225
[124,     1] loss: 0.170
Early stopping applied (best metric=0.3250914514064789)
Finished Training
Total time taken: 13.622602701187134
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.707
[2,     1] loss: 0.686
[3,     1] loss: 0.717
[4,     1] loss: 0.683
[5,     1] loss: 0.688
[6,     1] loss: 0.679
[7,     1] loss: 0.669
[8,     1] loss: 0.659
[9,     1] loss: 0.639
[10,     1] loss: 0.602
[11,     1] loss: 0.578
[12,     1] loss: 0.535
[13,     1] loss: 0.515
[14,     1] loss: 0.552
[15,     1] loss: 0.479
[16,     1] loss: 0.441
[17,     1] loss: 0.411
[18,     1] loss: 0.450
[19,     1] loss: 0.486
[20,     1] loss: 0.427
[21,     1] loss: 0.408
[22,     1] loss: 0.444
[23,     1] loss: 0.420
[24,     1] loss: 0.400
[25,     1] loss: 0.346
[26,     1] loss: 0.345
[27,     1] loss: 0.347
[28,     1] loss: 0.333
[29,     1] loss: 0.299
[30,     1] loss: 0.308
[31,     1] loss: 0.289
[32,     1] loss: 0.250
[33,     1] loss: 0.283
[34,     1] loss: 0.294
[35,     1] loss: 0.245
[36,     1] loss: 0.353
[37,     1] loss: 0.280
[38,     1] loss: 0.227
[39,     1] loss: 0.223
[40,     1] loss: 0.255
[41,     1] loss: 0.227
[42,     1] loss: 0.221
[43,     1] loss: 0.201
[44,     1] loss: 0.234
[45,     1] loss: 0.404
[46,     1] loss: 0.179
[47,     1] loss: 0.291
[48,     1] loss: 0.219
[49,     1] loss: 0.199
[50,     1] loss: 0.251
[51,     1] loss: 0.243
[52,     1] loss: 0.172
[53,     1] loss: 0.212
[54,     1] loss: 0.177
[55,     1] loss: 0.203
[56,     1] loss: 0.350
[57,     1] loss: 0.473
[58,     1] loss: 0.271
[59,     1] loss: 0.350
[60,     1] loss: 0.327
[61,     1] loss: 0.254
[62,     1] loss: 0.335
[63,     1] loss: 0.324
[64,     1] loss: 0.307
[65,     1] loss: 0.254
[66,     1] loss: 0.303
[67,     1] loss: 0.240
[68,     1] loss: 0.232
[69,     1] loss: 0.207
[70,     1] loss: 0.268
[71,     1] loss: 0.195
[72,     1] loss: 0.261
[73,     1] loss: 0.199
[74,     1] loss: 0.166
[75,     1] loss: 0.199
[76,     1] loss: 0.203
[77,     1] loss: 0.223
[78,     1] loss: 0.168
[79,     1] loss: 0.147
[80,     1] loss: 0.178
[81,     1] loss: 0.218
[82,     1] loss: 0.177
[83,     1] loss: 0.136
[84,     1] loss: 0.171
[85,     1] loss: 0.179
[86,     1] loss: 0.161
[87,     1] loss: 0.161
[88,     1] loss: 0.198
[89,     1] loss: 0.654
[90,     1] loss: 1.176
[91,     1] loss: 0.892
[92,     1] loss: 0.608
[93,     1] loss: 0.638
[94,     1] loss: 0.635
[95,     1] loss: 0.628
[96,     1] loss: 0.594
[97,     1] loss: 0.564
[98,     1] loss: 0.558
[99,     1] loss: 0.554
[100,     1] loss: 0.529
[101,     1] loss: 0.536
[102,     1] loss: 0.518
[103,     1] loss: 0.502
[104,     1] loss: 0.453
[105,     1] loss: 0.462
[106,     1] loss: 0.396
[107,     1] loss: 0.400
[108,     1] loss: 0.345
[109,     1] loss: 0.355
[110,     1] loss: 0.383
[111,     1] loss: 0.327
[112,     1] loss: 0.396
[113,     1] loss: 0.290
[114,     1] loss: 0.294
[115,     1] loss: 0.286
[116,     1] loss: 0.285
[117,     1] loss: 0.258
[118,     1] loss: 0.251
[119,     1] loss: 0.268
[120,     1] loss: 0.244
[121,     1] loss: 0.243
[122,     1] loss: 0.211
[123,     1] loss: 0.217
Early stopping applied (best metric=0.3060583770275116)
Finished Training
Total time taken: 13.505001783370972
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.695
[3,     1] loss: 0.683
[4,     1] loss: 0.664
[5,     1] loss: 0.626
[6,     1] loss: 0.576
[7,     1] loss: 0.549
[8,     1] loss: 0.526
[9,     1] loss: 0.472
[10,     1] loss: 0.408
[11,     1] loss: 0.436
[12,     1] loss: 0.337
[13,     1] loss: 0.525
[14,     1] loss: 0.453
[15,     1] loss: 0.441
[16,     1] loss: 0.394
[17,     1] loss: 0.389
[18,     1] loss: 0.386
[19,     1] loss: 0.411
[20,     1] loss: 0.393
[21,     1] loss: 0.361
[22,     1] loss: 0.390
[23,     1] loss: 0.381
[24,     1] loss: 0.337
[25,     1] loss: 0.345
[26,     1] loss: 0.334
[27,     1] loss: 0.372
[28,     1] loss: 0.372
[29,     1] loss: 0.295
[30,     1] loss: 0.303
[31,     1] loss: 0.273
[32,     1] loss: 0.274
[33,     1] loss: 0.328
[34,     1] loss: 0.207
[35,     1] loss: 0.340
[36,     1] loss: 0.361
[37,     1] loss: 0.319
[38,     1] loss: 0.281
[39,     1] loss: 0.292
[40,     1] loss: 0.279
[41,     1] loss: 0.312
[42,     1] loss: 0.276
[43,     1] loss: 0.279
[44,     1] loss: 0.240
[45,     1] loss: 0.217
[46,     1] loss: 0.244
[47,     1] loss: 0.256
[48,     1] loss: 0.228
[49,     1] loss: 0.227
[50,     1] loss: 0.183
[51,     1] loss: 0.184
[52,     1] loss: 0.155
[53,     1] loss: 0.198
[54,     1] loss: 0.229
[55,     1] loss: 0.305
[56,     1] loss: 0.159
[57,     1] loss: 0.220
[58,     1] loss: 0.263
[59,     1] loss: 0.299
[60,     1] loss: 0.244
[61,     1] loss: 0.284
[62,     1] loss: 0.226
[63,     1] loss: 0.234
[64,     1] loss: 0.210
[65,     1] loss: 0.219
[66,     1] loss: 0.197
[67,     1] loss: 0.214
[68,     1] loss: 0.168
[69,     1] loss: 0.215
[70,     1] loss: 0.218
[71,     1] loss: 0.257
[72,     1] loss: 0.318
[73,     1] loss: 0.222
[74,     1] loss: 0.212
[75,     1] loss: 0.206
[76,     1] loss: 0.269
[77,     1] loss: 0.237
[78,     1] loss: 0.212
[79,     1] loss: 0.215
[80,     1] loss: 0.201
[81,     1] loss: 0.267
[82,     1] loss: 0.198
[83,     1] loss: 0.145
[84,     1] loss: 0.172
[85,     1] loss: 0.163
[86,     1] loss: 0.213
[87,     1] loss: 0.233
[88,     1] loss: 0.220
[89,     1] loss: 0.185
[90,     1] loss: 0.175
[91,     1] loss: 0.192
[92,     1] loss: 0.145
[93,     1] loss: 0.211
[94,     1] loss: 0.188
[95,     1] loss: 0.148
[96,     1] loss: 0.182
[97,     1] loss: 0.145
[98,     1] loss: 0.233
[99,     1] loss: 0.211
[100,     1] loss: 0.297
[101,     1] loss: 0.321
[102,     1] loss: 0.200
[103,     1] loss: 0.217
[104,     1] loss: 0.251
[105,     1] loss: 0.274
[106,     1] loss: 0.229
[107,     1] loss: 0.223
[108,     1] loss: 0.229
[109,     1] loss: 0.213
[110,     1] loss: 0.212
[111,     1] loss: 0.205
[112,     1] loss: 0.184
[113,     1] loss: 0.168
[114,     1] loss: 0.167
[115,     1] loss: 0.162
[116,     1] loss: 0.155
[117,     1] loss: 0.123
[118,     1] loss: 0.141
[119,     1] loss: 0.236
[120,     1] loss: 0.284
[121,     1] loss: 0.182
[122,     1] loss: 0.140
[123,     1] loss: 0.135
[124,     1] loss: 0.234
[125,     1] loss: 0.150
[126,     1] loss: 0.149
[127,     1] loss: 0.147
[128,     1] loss: 0.128
[129,     1] loss: 0.226
[130,     1] loss: 0.199
[131,     1] loss: 0.138
[132,     1] loss: 0.137
[133,     1] loss: 0.165
[134,     1] loss: 0.147
[135,     1] loss: 0.180
[136,     1] loss: 0.163
[137,     1] loss: 0.143
[138,     1] loss: 0.188
Early stopping applied (best metric=0.18177762627601624)
Finished Training
Total time taken: 15.168999433517456
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.684
[3,     1] loss: 0.678
[4,     1] loss: 0.652
[5,     1] loss: 0.604
[6,     1] loss: 0.573
[7,     1] loss: 0.548
[8,     1] loss: 0.494
[9,     1] loss: 0.512
[10,     1] loss: 0.467
[11,     1] loss: 0.382
[12,     1] loss: 0.411
[13,     1] loss: 0.535
[14,     1] loss: 0.343
[15,     1] loss: 0.363
[16,     1] loss: 0.334
[17,     1] loss: 0.332
[18,     1] loss: 0.326
[19,     1] loss: 0.317
[20,     1] loss: 0.309
[21,     1] loss: 0.263
[22,     1] loss: 0.290
[23,     1] loss: 0.245
[24,     1] loss: 0.217
[25,     1] loss: 0.238
[26,     1] loss: 0.237
[27,     1] loss: 0.307
[28,     1] loss: 0.243
[29,     1] loss: 0.318
[30,     1] loss: 0.231
[31,     1] loss: 0.237
[32,     1] loss: 0.225
[33,     1] loss: 0.211
[34,     1] loss: 0.218
[35,     1] loss: 0.211
[36,     1] loss: 0.213
[37,     1] loss: 0.281
[38,     1] loss: 0.160
[39,     1] loss: 0.175
[40,     1] loss: 0.161
[41,     1] loss: 0.171
[42,     1] loss: 0.191
[43,     1] loss: 0.194
[44,     1] loss: 0.139
[45,     1] loss: 0.154
[46,     1] loss: 0.181
[47,     1] loss: 0.246
[48,     1] loss: 0.208
[49,     1] loss: 0.170
[50,     1] loss: 0.183
[51,     1] loss: 0.202
[52,     1] loss: 0.240
[53,     1] loss: 0.254
[54,     1] loss: 0.311
[55,     1] loss: 0.281
[56,     1] loss: 0.209
[57,     1] loss: 0.239
[58,     1] loss: 0.221
[59,     1] loss: 0.198
[60,     1] loss: 0.228
[61,     1] loss: 0.180
[62,     1] loss: 0.174
[63,     1] loss: 0.147
[64,     1] loss: 0.181
Early stopping applied (best metric=0.4695677161216736)
Finished Training
Total time taken: 7.054000616073608
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.687
[3,     1] loss: 0.665
[4,     1] loss: 0.621
[5,     1] loss: 0.550
[6,     1] loss: 0.474
[7,     1] loss: 0.454
[8,     1] loss: 0.413
[9,     1] loss: 0.368
[10,     1] loss: 0.370
[11,     1] loss: 0.338
[12,     1] loss: 0.276
[13,     1] loss: 0.263
[14,     1] loss: 0.290
[15,     1] loss: 0.312
[16,     1] loss: 0.376
[17,     1] loss: 0.272
[18,     1] loss: 0.256
[19,     1] loss: 0.306
[20,     1] loss: 0.325
[21,     1] loss: 0.331
[22,     1] loss: 0.282
[23,     1] loss: 0.233
[24,     1] loss: 0.298
[25,     1] loss: 0.703
[26,     1] loss: 0.265
[27,     1] loss: 0.399
[28,     1] loss: 0.319
[29,     1] loss: 0.347
[30,     1] loss: 0.354
[31,     1] loss: 0.325
[32,     1] loss: 0.330
[33,     1] loss: 0.319
[34,     1] loss: 0.262
[35,     1] loss: 0.235
[36,     1] loss: 0.216
[37,     1] loss: 0.243
[38,     1] loss: 0.226
[39,     1] loss: 0.300
[40,     1] loss: 0.293
[41,     1] loss: 0.272
[42,     1] loss: 0.255
[43,     1] loss: 0.224
[44,     1] loss: 0.246
[45,     1] loss: 0.276
[46,     1] loss: 0.234
[47,     1] loss: 0.181
[48,     1] loss: 0.177
[49,     1] loss: 0.239
[50,     1] loss: 0.163
[51,     1] loss: 0.177
[52,     1] loss: 0.186
[53,     1] loss: 0.153
[54,     1] loss: 0.178
[55,     1] loss: 0.165
[56,     1] loss: 0.343
[57,     1] loss: 0.832
[58,     1] loss: 0.330
[59,     1] loss: 0.327
[60,     1] loss: 0.431
[61,     1] loss: 0.354
[62,     1] loss: 0.378
[63,     1] loss: 0.375
[64,     1] loss: 0.374
[65,     1] loss: 0.354
[66,     1] loss: 0.310
[67,     1] loss: 0.288
[68,     1] loss: 0.236
[69,     1] loss: 0.254
[70,     1] loss: 0.246
[71,     1] loss: 0.308
[72,     1] loss: 0.290
[73,     1] loss: 0.214
[74,     1] loss: 0.224
[75,     1] loss: 0.200
[76,     1] loss: 0.246
[77,     1] loss: 0.179
[78,     1] loss: 0.227
[79,     1] loss: 0.198
[80,     1] loss: 0.173
[81,     1] loss: 0.185
[82,     1] loss: 0.148
[83,     1] loss: 0.165
[84,     1] loss: 0.150
[85,     1] loss: 0.249
[86,     1] loss: 0.216
[87,     1] loss: 0.224
Early stopping applied (best metric=0.26967692375183105)
Finished Training
Total time taken: 9.70900011062622
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.688
[3,     1] loss: 0.685
[4,     1] loss: 0.658
[5,     1] loss: 0.616
[6,     1] loss: 0.582
[7,     1] loss: 0.495
[8,     1] loss: 0.451
[9,     1] loss: 0.454
[10,     1] loss: 0.419
[11,     1] loss: 0.432
[12,     1] loss: 0.395
[13,     1] loss: 0.355
[14,     1] loss: 0.343
[15,     1] loss: 0.310
[16,     1] loss: 0.279
[17,     1] loss: 0.362
[18,     1] loss: 0.328
[19,     1] loss: 0.248
[20,     1] loss: 0.281
[21,     1] loss: 0.286
[22,     1] loss: 0.268
[23,     1] loss: 0.281
[24,     1] loss: 0.295
[25,     1] loss: 0.248
[26,     1] loss: 0.259
[27,     1] loss: 0.247
[28,     1] loss: 0.240
[29,     1] loss: 0.230
[30,     1] loss: 0.229
[31,     1] loss: 0.242
[32,     1] loss: 0.256
[33,     1] loss: 0.262
[34,     1] loss: 0.273
[35,     1] loss: 0.231
[36,     1] loss: 0.291
[37,     1] loss: 0.371
[38,     1] loss: 0.241
[39,     1] loss: 0.251
[40,     1] loss: 0.287
[41,     1] loss: 0.266
[42,     1] loss: 0.235
[43,     1] loss: 0.265
[44,     1] loss: 0.187
[45,     1] loss: 0.204
[46,     1] loss: 0.180
[47,     1] loss: 0.228
[48,     1] loss: 0.229
[49,     1] loss: 0.158
[50,     1] loss: 0.168
[51,     1] loss: 0.260
[52,     1] loss: 0.167
[53,     1] loss: 0.281
[54,     1] loss: 0.255
[55,     1] loss: 0.329
[56,     1] loss: 0.214
[57,     1] loss: 0.318
Early stopping applied (best metric=0.4501577615737915)
Finished Training
Total time taken: 6.290999174118042
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.689
[3,     1] loss: 0.657
[4,     1] loss: 0.625
[5,     1] loss: 0.562
[6,     1] loss: 0.529
[7,     1] loss: 0.484
[8,     1] loss: 0.413
[9,     1] loss: 0.355
[10,     1] loss: 0.341
[11,     1] loss: 0.343
[12,     1] loss: 0.302
[13,     1] loss: 0.441
[14,     1] loss: 0.339
[15,     1] loss: 0.441
[16,     1] loss: 0.336
[17,     1] loss: 0.324
[18,     1] loss: 0.319
[19,     1] loss: 0.407
[20,     1] loss: 0.344
[21,     1] loss: 0.309
[22,     1] loss: 0.315
[23,     1] loss: 0.268
[24,     1] loss: 0.289
[25,     1] loss: 0.396
[26,     1] loss: 0.244
[27,     1] loss: 0.387
[28,     1] loss: 0.289
[29,     1] loss: 0.288
[30,     1] loss: 0.299
[31,     1] loss: 0.288
[32,     1] loss: 0.274
[33,     1] loss: 0.257
[34,     1] loss: 0.277
[35,     1] loss: 0.254
[36,     1] loss: 0.203
[37,     1] loss: 0.211
[38,     1] loss: 0.271
[39,     1] loss: 0.312
[40,     1] loss: 0.266
[41,     1] loss: 0.322
[42,     1] loss: 0.253
[43,     1] loss: 0.294
[44,     1] loss: 0.231
[45,     1] loss: 0.286
[46,     1] loss: 0.277
[47,     1] loss: 0.272
[48,     1] loss: 0.256
[49,     1] loss: 0.263
[50,     1] loss: 0.257
[51,     1] loss: 0.254
[52,     1] loss: 0.243
[53,     1] loss: 0.286
[54,     1] loss: 0.252
[55,     1] loss: 0.236
[56,     1] loss: 0.302
[57,     1] loss: 0.242
[58,     1] loss: 0.273
[59,     1] loss: 0.213
[60,     1] loss: 0.268
[61,     1] loss: 0.223
[62,     1] loss: 0.216
[63,     1] loss: 0.185
[64,     1] loss: 0.175
[65,     1] loss: 0.560
[66,     1] loss: 0.934
[67,     1] loss: 0.304
[68,     1] loss: 0.397
Early stopping applied (best metric=0.4502951204776764)
Finished Training
Total time taken: 7.553999900817871
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.690
[3,     1] loss: 0.676
[4,     1] loss: 0.647
[5,     1] loss: 0.617
[6,     1] loss: 0.587
[7,     1] loss: 0.540
[8,     1] loss: 0.497
[9,     1] loss: 0.432
[10,     1] loss: 0.382
[11,     1] loss: 0.399
[12,     1] loss: 0.392
[13,     1] loss: 0.372
[14,     1] loss: 0.325
[15,     1] loss: 0.349
[16,     1] loss: 0.290
[17,     1] loss: 0.293
[18,     1] loss: 0.295
[19,     1] loss: 0.226
[20,     1] loss: 0.281
[21,     1] loss: 0.239
[22,     1] loss: 0.223
[23,     1] loss: 0.268
[24,     1] loss: 0.263
[25,     1] loss: 0.184
[26,     1] loss: 0.223
[27,     1] loss: 0.251
[28,     1] loss: 0.244
[29,     1] loss: 0.158
[30,     1] loss: 0.227
[31,     1] loss: 0.227
[32,     1] loss: 0.157
[33,     1] loss: 0.182
[34,     1] loss: 0.146
[35,     1] loss: 0.183
[36,     1] loss: 0.206
[37,     1] loss: 0.228
[38,     1] loss: 0.321
[39,     1] loss: 0.218
[40,     1] loss: 0.202
[41,     1] loss: 0.195
[42,     1] loss: 0.194
[43,     1] loss: 0.231
[44,     1] loss: 0.188
[45,     1] loss: 0.207
[46,     1] loss: 0.175
[47,     1] loss: 0.179
[48,     1] loss: 0.186
[49,     1] loss: 0.187
[50,     1] loss: 0.177
[51,     1] loss: 0.187
[52,     1] loss: 0.159
[53,     1] loss: 0.152
[54,     1] loss: 0.227
[55,     1] loss: 0.235
[56,     1] loss: 0.277
[57,     1] loss: 0.217
[58,     1] loss: 0.218
[59,     1] loss: 0.185
[60,     1] loss: 0.210
Early stopping applied (best metric=0.4466846287250519)
Finished Training
Total time taken: 6.64900016784668
{'Hydroxylation-K Validation Accuracy': 0.8563120567375887, 'Hydroxylation-K Validation Sensitivity': 0.8573333333333334, 'Hydroxylation-K Validation Specificity': 0.8557894736842105, 'Hydroxylation-K Validation Precision': 0.6277912087912088, 'Hydroxylation-K AUC ROC': 0.8836140350877193, 'Hydroxylation-K AUC PR': 0.7170736372590816, 'Hydroxylation-K MCC': 0.6453941396401448, 'Hydroxylation-K F1': 0.7143526837365178, 'Validation Loss (Hydroxylation-K)': 0.30855615258216856, 'Validation Loss (total)': 0.30855615258216856, 'TimeToTrain': 11.526871061325073}
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 5,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006900385925923837,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3373640947,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 2.9012750964993153}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.683
[3,     1] loss: 0.648
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005280718748486366,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 670248503,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 4.001304167012179}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.693
[2,     1] loss: 0.691
[3,     1] loss: 0.695
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009670573255235581,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1532310850,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 13.942147612249308}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.695
[2,     1] loss: 0.692
[3,     1] loss: 0.686
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003327971600548032,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3475879113,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 5.947831020536071}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.700
[2,     1] loss: 0.692
[3,     1] loss: 0.685
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Prottrans embeddings - local',
 'FCType': 'Adapt',
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 32,
 'LSTM_layers': 1,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (total)',
 'aminoAcid': ['Hydroxylation-K'],
 'batch_size': 512,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample'],
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'protBert',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004100019425655584,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 417612592,
 'sample_weights': [1.0],
 'test_data_ratio': 0.2,
 'useLrWeight': False,
 'weight_decay': 11.237033479417537}
(48, 33, 1024)
(190, 33, 1024)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/embeddings (238 samples)
[1,     1] loss: 0.691
[2,     1] loss: 0.703
[3,     1] loss: 0.672
