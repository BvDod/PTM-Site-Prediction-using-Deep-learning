{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00882196360454416,
 'learning_rate_Hydroxylation-K': 0.008602549995306208,
 'learning_rate_Hydroxylation-P': 0.007262360782706008,
 'log_base': 1.3244488696893262,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3685596277,
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.45853057795959,
 'weight_decay_Hydroxylation-K': 4.835236525266713,
 'weight_decay_Hydroxylation-P': 6.484855037697584}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2162.381
[2,     1] loss: 2162.845
[3,     1] loss: 2191.060
[4,     1] loss: 2158.327
[5,     1] loss: 2152.611
[6,     1] loss: 2159.267
[7,     1] loss: 2159.189
[8,     1] loss: 2148.551
[9,     1] loss: 2155.365
[10,     1] loss: 2150.902
[11,     1] loss: 2143.741
[12,     1] loss: 2134.752
[13,     1] loss: 2125.755
[14,     1] loss: 2111.936
[15,     1] loss: 2068.209
[16,     1] loss: 2025.782
[17,     1] loss: 1966.945
[18,     1] loss: 1884.248
[19,     1] loss: 1871.696
[20,     1] loss: 1853.995
[21,     1] loss: 1821.645
[22,     1] loss: 1931.549
[23,     1] loss: 1860.667
[24,     1] loss: 1819.100
[25,     1] loss: 1846.826
[26,     1] loss: 1784.624
[27,     1] loss: 1824.669
[28,     1] loss: 1844.585
[29,     1] loss: 1804.733
[30,     1] loss: 1698.679
[31,     1] loss: 1722.182
[32,     1] loss: 1769.365
[33,     1] loss: 1658.369
[34,     1] loss: 1679.627
[35,     1] loss: 1705.568
[36,     1] loss: 1644.040
[37,     1] loss: 1758.947
[38,     1] loss: 1696.392
[39,     1] loss: 1702.985
[40,     1] loss: 1569.629
[41,     1] loss: 1662.044
[42,     1] loss: 1662.346
[43,     1] loss: 1679.130
[44,     1] loss: 1615.592
[45,     1] loss: 1535.193
[46,     1] loss: 1429.268
[47,     1] loss: 1591.122
[48,     1] loss: 1487.152
[49,     1] loss: 1457.192
[50,     1] loss: 1569.155
[51,     1] loss: 1525.935
[52,     1] loss: 1416.543
[53,     1] loss: 1422.444
[54,     1] loss: 1382.809
[55,     1] loss: 1443.180
[56,     1] loss: 1459.071
[57,     1] loss: 1427.134
[58,     1] loss: 1310.664
[59,     1] loss: 1360.986
[60,     1] loss: 1347.234
[61,     1] loss: 1370.266
[62,     1] loss: 1298.679
[63,     1] loss: 1199.735
[64,     1] loss: 1218.542
[65,     1] loss: 1068.912
[66,     1] loss: 1162.476
[67,     1] loss: 1191.288
[68,     1] loss: 1382.609
[69,     1] loss: 1375.120
[70,     1] loss: 1258.244
[71,     1] loss: 1228.930
[72,     1] loss: 1225.406
[73,     1] loss: 1231.596
[74,     1] loss: 1334.895
[75,     1] loss: 1181.863
[76,     1] loss: 1215.019
[77,     1] loss: 1085.257
[78,     1] loss: 1156.097
[79,     1] loss: 1105.739
[80,     1] loss: 1152.788
[81,     1] loss: 962.674
[82,     1] loss: 1057.529
[83,     1] loss: 946.829
[84,     1] loss: 871.054
Early stopping applied (best metric=0.37062370777130127)
Finished Training
Total time taken: 18.431411743164062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2165.358
[2,     1] loss: 2176.619
[3,     1] loss: 2158.513
[4,     1] loss: 2157.320
[5,     1] loss: 2155.460
[6,     1] loss: 2156.030
[7,     1] loss: 2155.407
[8,     1] loss: 2176.861
[9,     1] loss: 2149.418
[10,     1] loss: 2160.209
[11,     1] loss: 2154.328
[12,     1] loss: 2158.929
[13,     1] loss: 2152.460
[14,     1] loss: 2170.303
[15,     1] loss: 2167.633
[16,     1] loss: 2156.608
[17,     1] loss: 2157.524
[18,     1] loss: 2150.529
[19,     1] loss: 2147.524
[20,     1] loss: 2133.998
[21,     1] loss: 2124.982
[22,     1] loss: 2110.507
[23,     1] loss: 2093.649
[24,     1] loss: 2064.231
[25,     1] loss: 2022.821
[26,     1] loss: 1963.541
[27,     1] loss: 1937.502
[28,     1] loss: 1938.081
[29,     1] loss: 1834.431
[30,     1] loss: 1826.252
[31,     1] loss: 1801.913
[32,     1] loss: 1743.283
[33,     1] loss: 1775.132
[34,     1] loss: 1782.291
[35,     1] loss: 1782.635
[36,     1] loss: 1650.303
[37,     1] loss: 1832.512
[38,     1] loss: 1648.014
[39,     1] loss: 1763.440
[40,     1] loss: 1535.166
[41,     1] loss: 1633.548
[42,     1] loss: 1623.193
[43,     1] loss: 1673.618
[44,     1] loss: 1661.156
[45,     1] loss: 1535.608
[46,     1] loss: 1625.578
[47,     1] loss: 1422.957
[48,     1] loss: 1425.241
[49,     1] loss: 1467.112
[50,     1] loss: 1451.639
[51,     1] loss: 1564.530
[52,     1] loss: 1588.645
[53,     1] loss: 1524.750
[54,     1] loss: 1349.374
[55,     1] loss: 1464.122
[56,     1] loss: 1388.308
[57,     1] loss: 1371.255
[58,     1] loss: 1301.169
[59,     1] loss: 1255.605
[60,     1] loss: 1235.633
[61,     1] loss: 1370.864
[62,     1] loss: 1343.574
[63,     1] loss: 1224.064
[64,     1] loss: 1143.127
[65,     1] loss: 1217.394
[66,     1] loss: 1114.428
[67,     1] loss: 1179.836
[68,     1] loss: 1147.085
[69,     1] loss: 1347.459
[70,     1] loss: 1328.323
[71,     1] loss: 1428.023
[72,     1] loss: 1045.579
[73,     1] loss: 1176.467
[74,     1] loss: 1130.870
[75,     1] loss: 1106.948
[76,     1] loss: 996.184
[77,     1] loss: 1057.721
[78,     1] loss: 1057.411
[79,     1] loss: 1107.032
[80,     1] loss: 971.878
[81,     1] loss: 969.170
[82,     1] loss: 1036.309
[83,     1] loss: 1204.393
[84,     1] loss: 1030.732
[85,     1] loss: 948.658
[86,     1] loss: 981.495
[87,     1] loss: 908.227
[88,     1] loss: 893.206
[89,     1] loss: 948.282
[90,     1] loss: 1052.765
[91,     1] loss: 1013.136
[92,     1] loss: 888.522
[93,     1] loss: 897.212
[94,     1] loss: 1184.803
[95,     1] loss: 1002.066
Early stopping applied (best metric=0.4350946247577667)
Finished Training
Total time taken: 15.041613817214966
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2160.366
[2,     1] loss: 2161.292
[3,     1] loss: 2178.612
[4,     1] loss: 2162.837
[5,     1] loss: 2157.448
[6,     1] loss: 2169.294
[7,     1] loss: 2158.828
[8,     1] loss: 2167.382
[9,     1] loss: 2155.502
[10,     1] loss: 2163.967
[11,     1] loss: 2155.753
[12,     1] loss: 2152.862
[13,     1] loss: 2151.330
[14,     1] loss: 2150.527
[15,     1] loss: 2166.525
[16,     1] loss: 2157.972
[17,     1] loss: 2145.585
[18,     1] loss: 2138.333
[19,     1] loss: 2149.332
[20,     1] loss: 2141.012
[21,     1] loss: 2129.468
[22,     1] loss: 2126.875
[23,     1] loss: 2084.685
[24,     1] loss: 2052.912
[25,     1] loss: 2024.716
[26,     1] loss: 1896.379
[27,     1] loss: 1807.545
[28,     1] loss: 1791.348
[29,     1] loss: 1788.675
[30,     1] loss: 1930.527
[31,     1] loss: 1795.551
[32,     1] loss: 1949.200
[33,     1] loss: 1674.807
[34,     1] loss: 1905.447
[35,     1] loss: 1795.542
[36,     1] loss: 1709.272
[37,     1] loss: 1709.535
[38,     1] loss: 1760.528
[39,     1] loss: 1662.911
[40,     1] loss: 1642.063
[41,     1] loss: 1725.081
[42,     1] loss: 1607.161
[43,     1] loss: 1765.430
[44,     1] loss: 1595.335
[45,     1] loss: 1499.626
[46,     1] loss: 1509.347
[47,     1] loss: 1487.395
[48,     1] loss: 1474.236
[49,     1] loss: 1407.354
[50,     1] loss: 1396.333
[51,     1] loss: 1307.571
[52,     1] loss: 1371.069
[53,     1] loss: 1589.301
[54,     1] loss: 1316.423
[55,     1] loss: 1226.908
[56,     1] loss: 1261.313
[57,     1] loss: 1326.602
[58,     1] loss: 1452.878
[59,     1] loss: 1227.183
[60,     1] loss: 1330.704
[61,     1] loss: 1126.109
[62,     1] loss: 1158.631
[63,     1] loss: 1251.364
[64,     1] loss: 1070.835
[65,     1] loss: 1234.240
[66,     1] loss: 1239.295
[67,     1] loss: 1221.221
[68,     1] loss: 1497.481
[69,     1] loss: 1033.705
[70,     1] loss: 1205.920
[71,     1] loss: 1103.066
[72,     1] loss: 1032.225
[73,     1] loss: 1125.506
[74,     1] loss: 1050.911
[75,     1] loss: 950.540
Early stopping applied (best metric=0.5038654804229736)
Finished Training
Total time taken: 13.050017833709717
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2155.728
[2,     1] loss: 2179.087
[3,     1] loss: 2157.450
[4,     1] loss: 2160.930
[5,     1] loss: 2166.828
[6,     1] loss: 2171.890
[7,     1] loss: 2165.782
[8,     1] loss: 2163.540
[9,     1] loss: 2157.877
[10,     1] loss: 2159.627
[11,     1] loss: 2162.249
[12,     1] loss: 2158.855
[13,     1] loss: 2160.283
[14,     1] loss: 2159.706
[15,     1] loss: 2160.238
[16,     1] loss: 2160.139
[17,     1] loss: 2158.143
[18,     1] loss: 2158.698
[19,     1] loss: 2157.709
[20,     1] loss: 2155.658
[21,     1] loss: 2160.700
[22,     1] loss: 2155.676
[23,     1] loss: 2153.416
[24,     1] loss: 2160.446
[25,     1] loss: 2153.146
[26,     1] loss: 2157.937
[27,     1] loss: 2153.410
[28,     1] loss: 2144.640
[29,     1] loss: 2136.521
[30,     1] loss: 2126.200
[31,     1] loss: 2100.553
[32,     1] loss: 2082.624
[33,     1] loss: 2011.937
[34,     1] loss: 1918.984
[35,     1] loss: 1824.657
[36,     1] loss: 1764.737
[37,     1] loss: 1697.128
[38,     1] loss: 1739.356
[39,     1] loss: 1743.719
[40,     1] loss: 1786.599
[41,     1] loss: 1705.848
[42,     1] loss: 1685.344
[43,     1] loss: 1574.590
[44,     1] loss: 1756.517
[45,     1] loss: 1525.077
[46,     1] loss: 1586.775
[47,     1] loss: 1586.662
[48,     1] loss: 1508.830
[49,     1] loss: 1611.955
[50,     1] loss: 1548.112
[51,     1] loss: 1692.409
[52,     1] loss: 1469.197
[53,     1] loss: 1582.330
[54,     1] loss: 1507.471
[55,     1] loss: 1452.628
[56,     1] loss: 1416.072
[57,     1] loss: 1488.272
[58,     1] loss: 1317.744
[59,     1] loss: 1433.712
[60,     1] loss: 1280.102
[61,     1] loss: 1314.058
[62,     1] loss: 1203.823
[63,     1] loss: 1275.384
[64,     1] loss: 1184.864
[65,     1] loss: 1306.819
[66,     1] loss: 1216.591
[67,     1] loss: 1048.244
[68,     1] loss: 1127.020
[69,     1] loss: 1154.016
[70,     1] loss: 1252.018
[71,     1] loss: 1123.934
[72,     1] loss: 1004.863
[73,     1] loss: 1146.811
[74,     1] loss: 1017.046
[75,     1] loss: 1062.148
[76,     1] loss: 1089.522
[77,     1] loss: 1009.370
[78,     1] loss: 1304.506
[79,     1] loss: 1165.921
[80,     1] loss: 1140.250
[81,     1] loss: 1047.332
[82,     1] loss: 1013.407
[83,     1] loss: 1064.016
[84,     1] loss: 918.742
[85,     1] loss: 1063.986
[86,     1] loss: 915.885
[87,     1] loss: 1090.176
[88,     1] loss: 876.085
[89,     1] loss: 877.658
[90,     1] loss: 999.036
[91,     1] loss: 878.659
Early stopping applied (best metric=0.4204186201095581)
Finished Training
Total time taken: 15.164891242980957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2159.627
[2,     1] loss: 2175.434
[3,     1] loss: 2158.435
[4,     1] loss: 2169.304
[5,     1] loss: 2165.864
[6,     1] loss: 2160.582
[7,     1] loss: 2176.329
[8,     1] loss: 2167.746
[9,     1] loss: 2161.470
[10,     1] loss: 2165.844
[11,     1] loss: 2164.903
[12,     1] loss: 2171.697
[13,     1] loss: 2162.604
[14,     1] loss: 2159.890
[15,     1] loss: 2162.449
[16,     1] loss: 2159.501
[17,     1] loss: 2158.298
[18,     1] loss: 2164.264
[19,     1] loss: 2162.092
[20,     1] loss: 2152.185
[21,     1] loss: 2154.966
[22,     1] loss: 2161.539
[23,     1] loss: 2158.827
[24,     1] loss: 2161.474
[25,     1] loss: 2141.118
[26,     1] loss: 2146.816
[27,     1] loss: 2135.958
[28,     1] loss: 2100.644
[29,     1] loss: 2094.993
[30,     1] loss: 2054.619
[31,     1] loss: 2001.249
[32,     1] loss: 1973.413
[33,     1] loss: 1968.569
[34,     1] loss: 1879.498
[35,     1] loss: 1925.210
[36,     1] loss: 1905.307
[37,     1] loss: 1774.869
[38,     1] loss: 1804.743
[39,     1] loss: 1788.930
[40,     1] loss: 1831.549
[41,     1] loss: 1926.221
[42,     1] loss: 1820.990
[43,     1] loss: 1678.355
[44,     1] loss: 1787.246
[45,     1] loss: 1868.779
[46,     1] loss: 1720.604
[47,     1] loss: 1812.569
[48,     1] loss: 1675.173
[49,     1] loss: 1644.106
[50,     1] loss: 1744.793
[51,     1] loss: 1575.082
[52,     1] loss: 1642.244
[53,     1] loss: 1561.877
[54,     1] loss: 1505.345
[55,     1] loss: 1640.290
[56,     1] loss: 1577.878
[57,     1] loss: 1588.096
[58,     1] loss: 1566.655
[59,     1] loss: 1564.472
[60,     1] loss: 1522.603
[61,     1] loss: 1628.611
[62,     1] loss: 1578.990
[63,     1] loss: 1518.764
[64,     1] loss: 1508.794
[65,     1] loss: 1495.362
[66,     1] loss: 1393.592
[67,     1] loss: 1390.545
[68,     1] loss: 1310.893
[69,     1] loss: 1482.046
[70,     1] loss: 1552.359
[71,     1] loss: 1314.018
[72,     1] loss: 1271.257
[73,     1] loss: 1230.929
[74,     1] loss: 1249.578
[75,     1] loss: 1281.820
[76,     1] loss: 1285.988
[77,     1] loss: 1243.821
[78,     1] loss: 1361.610
[79,     1] loss: 1363.316
[80,     1] loss: 1158.307
[81,     1] loss: 1328.360
[82,     1] loss: 1281.225
[83,     1] loss: 1006.741
[84,     1] loss: 1029.798
[85,     1] loss: 1093.287
[86,     1] loss: 1145.289
[87,     1] loss: 1218.746
[88,     1] loss: 1099.341
[89,     1] loss: 1108.729
[90,     1] loss: 1029.577
[91,     1] loss: 992.810
[92,     1] loss: 1086.071
[93,     1] loss: 1000.625
[94,     1] loss: 1011.372
[95,     1] loss: 1046.394
[96,     1] loss: 913.860
[97,     1] loss: 905.893
[98,     1] loss: 820.497
[99,     1] loss: 892.762
[100,     1] loss: 810.620
[101,     1] loss: 776.700
[102,     1] loss: 999.923
[103,     1] loss: 2630.055
[104,     1] loss: 1515.316
[105,     1] loss: 1125.698
[106,     1] loss: 1149.435
[107,     1] loss: 1332.963
[108,     1] loss: 1261.385
[109,     1] loss: 1125.689
Early stopping applied (best metric=0.41269010305404663)
Finished Training
Total time taken: 16.630025386810303
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2161.619
[2,     1] loss: 2165.427
[3,     1] loss: 2154.037
[4,     1] loss: 2169.597
[5,     1] loss: 2154.466
[6,     1] loss: 2154.724
[7,     1] loss: 2159.923
[8,     1] loss: 2172.067
[9,     1] loss: 2159.020
[10,     1] loss: 2143.999
[11,     1] loss: 2161.858
[12,     1] loss: 2160.347
[13,     1] loss: 2154.845
[14,     1] loss: 2141.959
[15,     1] loss: 2128.485
[16,     1] loss: 2124.752
[17,     1] loss: 2109.164
[18,     1] loss: 2076.982
[19,     1] loss: 2035.786
[20,     1] loss: 1967.102
[21,     1] loss: 1865.865
[22,     1] loss: 1841.750
[23,     1] loss: 1826.867
[24,     1] loss: 1827.145
[25,     1] loss: 1761.527
[26,     1] loss: 1764.978
[27,     1] loss: 1812.068
[28,     1] loss: 1763.305
[29,     1] loss: 1657.422
[30,     1] loss: 1665.218
[31,     1] loss: 1721.324
[32,     1] loss: 1539.177
[33,     1] loss: 1896.493
[34,     1] loss: 1629.358
[35,     1] loss: 1717.863
[36,     1] loss: 1586.234
[37,     1] loss: 1711.838
[38,     1] loss: 1525.578
[39,     1] loss: 1579.934
[40,     1] loss: 1641.877
[41,     1] loss: 1406.684
[42,     1] loss: 1455.926
[43,     1] loss: 1492.950
[44,     1] loss: 1394.261
[45,     1] loss: 1342.062
[46,     1] loss: 1440.926
[47,     1] loss: 1451.606
[48,     1] loss: 1365.046
[49,     1] loss: 1241.329
[50,     1] loss: 1294.278
[51,     1] loss: 1176.875
[52,     1] loss: 1429.641
[53,     1] loss: 1269.343
[54,     1] loss: 1365.082
[55,     1] loss: 1217.513
[56,     1] loss: 1338.455
[57,     1] loss: 1162.700
[58,     1] loss: 1235.718
[59,     1] loss: 1171.214
[60,     1] loss: 1135.321
[61,     1] loss: 1186.863
[62,     1] loss: 1150.346
[63,     1] loss: 1212.259
[64,     1] loss: 1242.450
[65,     1] loss: 1219.470
[66,     1] loss: 1142.123
[67,     1] loss: 1016.003
[68,     1] loss: 1056.430
[69,     1] loss: 1272.165
[70,     1] loss: 1211.553
Early stopping applied (best metric=0.4689665138721466)
Finished Training
Total time taken: 10.737537622451782
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2181.086
[2,     1] loss: 2180.653
[3,     1] loss: 2165.488
[4,     1] loss: 2165.178
[5,     1] loss: 2168.792
[6,     1] loss: 2168.717
[7,     1] loss: 2172.059
[8,     1] loss: 2164.577
[9,     1] loss: 2175.740
[10,     1] loss: 2157.824
[11,     1] loss: 2158.857
[12,     1] loss: 2162.033
[13,     1] loss: 2163.734
[14,     1] loss: 2160.680
[15,     1] loss: 2159.209
[16,     1] loss: 2160.411
[17,     1] loss: 2160.158
[18,     1] loss: 2160.883
[19,     1] loss: 2159.906
[20,     1] loss: 2157.449
[21,     1] loss: 2160.891
[22,     1] loss: 2162.312
[23,     1] loss: 2160.096
[24,     1] loss: 2162.300
[25,     1] loss: 2159.229
[26,     1] loss: 2160.351
[27,     1] loss: 2160.151
[28,     1] loss: 2159.451
[29,     1] loss: 2161.482
[30,     1] loss: 2161.049
[31,     1] loss: 2161.506
[32,     1] loss: 2161.310
[33,     1] loss: 2160.621
[34,     1] loss: 2160.275
[35,     1] loss: 2159.805
[36,     1] loss: 2160.043
[37,     1] loss: 2159.329
[38,     1] loss: 2160.064
[39,     1] loss: 2158.214
[40,     1] loss: 2160.815
[41,     1] loss: 2161.156
[42,     1] loss: 2157.987
[43,     1] loss: 2159.893
[44,     1] loss: 2159.434
[45,     1] loss: 2160.345
[46,     1] loss: 2159.063
[47,     1] loss: 2156.796
[48,     1] loss: 2156.645
[49,     1] loss: 2150.498
[50,     1] loss: 2142.517
[51,     1] loss: 2133.282
[52,     1] loss: 2105.526
Early stopping applied (best metric=0.5273645520210266)
Finished Training
Total time taken: 8.017529487609863
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2161.827
[2,     1] loss: 2155.867
[3,     1] loss: 2162.448
[4,     1] loss: 2171.306
[5,     1] loss: 2166.324
[6,     1] loss: 2153.208
[7,     1] loss: 2163.188
[8,     1] loss: 2164.872
[9,     1] loss: 2155.972
[10,     1] loss: 2154.670
[11,     1] loss: 2146.532
[12,     1] loss: 2157.710
[13,     1] loss: 2152.824
[14,     1] loss: 2149.678
[15,     1] loss: 2153.022
[16,     1] loss: 2134.683
[17,     1] loss: 2126.177
[18,     1] loss: 2119.985
[19,     1] loss: 2114.513
[20,     1] loss: 2116.642
[21,     1] loss: 2097.643
[22,     1] loss: 2056.995
[23,     1] loss: 2056.497
[24,     1] loss: 2008.315
[25,     1] loss: 1926.247
[26,     1] loss: 1889.907
[27,     1] loss: 1913.635
[28,     1] loss: 1821.938
[29,     1] loss: 1769.167
[30,     1] loss: 1822.203
[31,     1] loss: 1777.855
[32,     1] loss: 1853.341
[33,     1] loss: 1693.738
[34,     1] loss: 1772.830
[35,     1] loss: 1652.569
[36,     1] loss: 1702.894
[37,     1] loss: 1552.213
[38,     1] loss: 1585.416
[39,     1] loss: 1506.444
[40,     1] loss: 1482.090
[41,     1] loss: 1478.723
[42,     1] loss: 1472.618
[43,     1] loss: 1496.116
[44,     1] loss: 1634.916
[45,     1] loss: 1606.259
[46,     1] loss: 1428.592
[47,     1] loss: 1514.854
[48,     1] loss: 1486.536
[49,     1] loss: 1439.084
[50,     1] loss: 1494.009
[51,     1] loss: 1446.730
[52,     1] loss: 1405.207
[53,     1] loss: 1572.965
[54,     1] loss: 1319.010
[55,     1] loss: 1446.112
[56,     1] loss: 1384.770
[57,     1] loss: 1276.657
[58,     1] loss: 1244.190
[59,     1] loss: 1392.592
[60,     1] loss: 1271.292
[61,     1] loss: 1219.987
[62,     1] loss: 1267.916
[63,     1] loss: 1287.216
[64,     1] loss: 1225.637
[65,     1] loss: 1171.469
[66,     1] loss: 1035.485
[67,     1] loss: 1230.919
[68,     1] loss: 1154.097
[69,     1] loss: 1141.992
[70,     1] loss: 1094.933
[71,     1] loss: 1196.932
[72,     1] loss: 1384.702
[73,     1] loss: 1591.967
[74,     1] loss: 1209.250
[75,     1] loss: 1357.274
[76,     1] loss: 1154.967
[77,     1] loss: 1367.998
[78,     1] loss: 1154.274
[79,     1] loss: 1300.567
[80,     1] loss: 1031.960
[81,     1] loss: 1130.135
[82,     1] loss: 1185.299
[83,     1] loss: 1315.279
[84,     1] loss: 1050.933
[85,     1] loss: 1400.326
Early stopping applied (best metric=0.450990229845047)
Finished Training
Total time taken: 13.7100191116333
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2157.542
[2,     1] loss: 2187.018
[3,     1] loss: 2208.156
[4,     1] loss: 2163.350
[5,     1] loss: 2159.048
[6,     1] loss: 2165.523
[7,     1] loss: 2148.617
[8,     1] loss: 2160.736
[9,     1] loss: 2168.441
[10,     1] loss: 2159.510
[11,     1] loss: 2157.669
[12,     1] loss: 2153.832
[13,     1] loss: 2158.862
[14,     1] loss: 2158.231
[15,     1] loss: 2157.566
[16,     1] loss: 2161.061
[17,     1] loss: 2154.816
[18,     1] loss: 2164.115
[19,     1] loss: 2172.938
[20,     1] loss: 2144.922
[21,     1] loss: 2159.725
[22,     1] loss: 2167.185
[23,     1] loss: 2158.622
[24,     1] loss: 2159.788
[25,     1] loss: 2163.591
[26,     1] loss: 2158.517
[27,     1] loss: 2160.578
[28,     1] loss: 2157.212
[29,     1] loss: 2160.412
[30,     1] loss: 2160.095
[31,     1] loss: 2157.365
[32,     1] loss: 2157.532
[33,     1] loss: 2159.876
[34,     1] loss: 2155.649
[35,     1] loss: 2161.045
[36,     1] loss: 2162.358
[37,     1] loss: 2159.910
[38,     1] loss: 2160.262
[39,     1] loss: 2159.660
[40,     1] loss: 2155.534
[41,     1] loss: 2156.528
[42,     1] loss: 2157.564
[43,     1] loss: 2151.923
[44,     1] loss: 2151.547
[45,     1] loss: 2148.138
[46,     1] loss: 2146.717
[47,     1] loss: 2139.836
[48,     1] loss: 2112.371
[49,     1] loss: 2067.266
[50,     1] loss: 2022.081
[51,     1] loss: 1957.373
[52,     1] loss: 1952.559
[53,     1] loss: 1858.101
[54,     1] loss: 1863.182
[55,     1] loss: 1799.182
[56,     1] loss: 1733.376
[57,     1] loss: 1728.355
[58,     1] loss: 1777.627
[59,     1] loss: 1966.851
[60,     1] loss: 1759.391
[61,     1] loss: 1818.442
[62,     1] loss: 1745.762
[63,     1] loss: 1764.328
[64,     1] loss: 1782.250
[65,     1] loss: 1706.923
[66,     1] loss: 1654.381
[67,     1] loss: 1611.562
[68,     1] loss: 1606.418
[69,     1] loss: 1567.866
[70,     1] loss: 1719.687
[71,     1] loss: 1662.202
[72,     1] loss: 1753.923
[73,     1] loss: 1572.482
[74,     1] loss: 1707.743
[75,     1] loss: 1591.489
[76,     1] loss: 1522.129
[77,     1] loss: 1557.871
[78,     1] loss: 1604.318
[79,     1] loss: 1510.476
[80,     1] loss: 1629.418
[81,     1] loss: 1445.654
[82,     1] loss: 1537.962
[83,     1] loss: 1349.553
[84,     1] loss: 1384.455
[85,     1] loss: 1465.760
[86,     1] loss: 1368.294
[87,     1] loss: 1416.914
[88,     1] loss: 1359.311
[89,     1] loss: 1257.316
[90,     1] loss: 1299.356
[91,     1] loss: 1149.441
[92,     1] loss: 1300.099
[93,     1] loss: 1474.319
[94,     1] loss: 1446.290
[95,     1] loss: 1250.265
[96,     1] loss: 1584.104
[97,     1] loss: 1433.130
[98,     1] loss: 1368.038
[99,     1] loss: 1290.106
[100,     1] loss: 1444.852
[101,     1] loss: 1152.907
[102,     1] loss: 1248.548
[103,     1] loss: 1154.646
[104,     1] loss: 1050.568
[105,     1] loss: 1023.708
[106,     1] loss: 1029.203
[107,     1] loss: 990.729
[108,     1] loss: 1046.458
[109,     1] loss: 992.459
[110,     1] loss: 893.217
[111,     1] loss: 1097.844
[112,     1] loss: 987.099
[113,     1] loss: 924.662
[114,     1] loss: 1224.237
[115,     1] loss: 1763.630
[116,     1] loss: 1014.210
[117,     1] loss: 1668.786
[118,     1] loss: 1228.115
[119,     1] loss: 1571.508
[120,     1] loss: 1207.197
[121,     1] loss: 1236.439
[122,     1] loss: 1424.025
[123,     1] loss: 1206.489
[124,     1] loss: 1134.902
[125,     1] loss: 1171.350
[126,     1] loss: 1013.823
[127,     1] loss: 979.976
[128,     1] loss: 1197.889
[129,     1] loss: 915.318
[130,     1] loss: 1155.989
[131,     1] loss: 995.505
[132,     1] loss: 1060.920
[133,     1] loss: 1021.001
[134,     1] loss: 1137.068
[135,     1] loss: 908.945
[136,     1] loss: 1081.674
[137,     1] loss: 835.025
[138,     1] loss: 960.568
[139,     1] loss: 777.909
[140,     1] loss: 922.644
[141,     1] loss: 838.374
[142,     1] loss: 884.930
[143,     1] loss: 834.006
[144,     1] loss: 762.707
[145,     1] loss: 778.708
[146,     1] loss: 847.206
[147,     1] loss: 967.154
[148,     1] loss: 878.782
[149,     1] loss: 817.348
[150,     1] loss: 1649.409
[151,     1] loss: 1559.289
Early stopping applied (best metric=0.3780449330806732)
Finished Training
Total time taken: 23.326838731765747
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2165.978
[2,     1] loss: 2165.067
[3,     1] loss: 2170.568
[4,     1] loss: 2170.939
[5,     1] loss: 2162.190
[6,     1] loss: 2164.840
[7,     1] loss: 2162.062
[8,     1] loss: 2159.083
[9,     1] loss: 2157.666
[10,     1] loss: 2166.048
[11,     1] loss: 2164.229
[12,     1] loss: 2164.908
[13,     1] loss: 2160.683
[14,     1] loss: 2159.197
[15,     1] loss: 2165.929
[16,     1] loss: 2158.355
[17,     1] loss: 2156.866
[18,     1] loss: 2154.346
[19,     1] loss: 2159.355
[20,     1] loss: 2164.109
[21,     1] loss: 2153.582
[22,     1] loss: 2156.020
[23,     1] loss: 2159.353
[24,     1] loss: 2167.901
[25,     1] loss: 2151.661
[26,     1] loss: 2158.484
[27,     1] loss: 2146.005
[28,     1] loss: 2145.092
[29,     1] loss: 2137.343
[30,     1] loss: 2129.910
[31,     1] loss: 2098.562
[32,     1] loss: 2063.297
[33,     1] loss: 1997.745
[34,     1] loss: 1951.451
[35,     1] loss: 1893.315
[36,     1] loss: 1923.440
[37,     1] loss: 1899.836
[38,     1] loss: 1797.957
[39,     1] loss: 1798.899
[40,     1] loss: 1745.042
[41,     1] loss: 1680.443
[42,     1] loss: 1869.780
[43,     1] loss: 1714.118
[44,     1] loss: 1809.248
[45,     1] loss: 1679.494
[46,     1] loss: 1751.433
[47,     1] loss: 1761.826
[48,     1] loss: 1638.512
[49,     1] loss: 1654.553
[50,     1] loss: 1683.957
[51,     1] loss: 1615.754
[52,     1] loss: 1566.800
[53,     1] loss: 1490.392
[54,     1] loss: 1645.423
[55,     1] loss: 1603.585
[56,     1] loss: 1501.000
[57,     1] loss: 1456.612
[58,     1] loss: 1401.512
[59,     1] loss: 1415.317
[60,     1] loss: 1443.759
[61,     1] loss: 1395.774
[62,     1] loss: 1416.321
[63,     1] loss: 1378.792
[64,     1] loss: 1354.214
[65,     1] loss: 1350.379
[66,     1] loss: 1366.802
[67,     1] loss: 1284.688
[68,     1] loss: 1254.651
[69,     1] loss: 1324.115
[70,     1] loss: 1405.700
[71,     1] loss: 1274.058
[72,     1] loss: 1536.685
[73,     1] loss: 1286.022
[74,     1] loss: 1410.209
[75,     1] loss: 1216.774
[76,     1] loss: 1254.461
[77,     1] loss: 1358.172
[78,     1] loss: 1202.843
[79,     1] loss: 1279.700
[80,     1] loss: 1018.620
[81,     1] loss: 1329.530
[82,     1] loss: 1104.923
[83,     1] loss: 1159.853
[84,     1] loss: 1153.217
[85,     1] loss: 1095.092
[86,     1] loss: 1330.278
[87,     1] loss: 1050.373
[88,     1] loss: 1231.239
[89,     1] loss: 1095.403
[90,     1] loss: 1364.011
[91,     1] loss: 1092.778
[92,     1] loss: 1299.988
[93,     1] loss: 989.884
[94,     1] loss: 1189.903
[95,     1] loss: 1157.501
[96,     1] loss: 1042.108
[97,     1] loss: 1401.875
[98,     1] loss: 1035.943
[99,     1] loss: 1294.883
Early stopping applied (best metric=0.4172571301460266)
Finished Training
Total time taken: 17.84491539001465
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2153.679
[2,     1] loss: 2156.244
[3,     1] loss: 2161.526
[4,     1] loss: 2162.495
[5,     1] loss: 2171.407
[6,     1] loss: 2151.407
[7,     1] loss: 2172.554
[8,     1] loss: 2152.232
[9,     1] loss: 2153.327
[10,     1] loss: 2152.417
[11,     1] loss: 2153.112
[12,     1] loss: 2146.145
[13,     1] loss: 2154.031
[14,     1] loss: 2160.168
[15,     1] loss: 2147.004
[16,     1] loss: 2149.641
[17,     1] loss: 2132.029
[18,     1] loss: 2139.073
[19,     1] loss: 2113.792
[20,     1] loss: 2085.815
[21,     1] loss: 2039.119
[22,     1] loss: 1983.454
[23,     1] loss: 1944.065
[24,     1] loss: 1921.773
[25,     1] loss: 1837.757
[26,     1] loss: 1875.265
[27,     1] loss: 1864.615
[28,     1] loss: 1870.960
[29,     1] loss: 1905.762
[30,     1] loss: 1787.189
[31,     1] loss: 1856.226
[32,     1] loss: 1720.686
[33,     1] loss: 1809.826
[34,     1] loss: 1744.385
[35,     1] loss: 1838.268
[36,     1] loss: 1716.655
[37,     1] loss: 1654.979
[38,     1] loss: 1681.249
[39,     1] loss: 1594.641
[40,     1] loss: 1620.667
[41,     1] loss: 1677.616
[42,     1] loss: 1640.544
[43,     1] loss: 1642.267
[44,     1] loss: 1565.011
[45,     1] loss: 1730.297
[46,     1] loss: 1584.965
[47,     1] loss: 1702.102
[48,     1] loss: 1409.479
[49,     1] loss: 1628.567
[50,     1] loss: 1464.840
[51,     1] loss: 1649.644
[52,     1] loss: 1330.956
[53,     1] loss: 1287.556
[54,     1] loss: 1329.654
[55,     1] loss: 1524.602
[56,     1] loss: 1265.804
[57,     1] loss: 1289.867
[58,     1] loss: 1263.751
[59,     1] loss: 1107.273
[60,     1] loss: 1238.188
[61,     1] loss: 1194.018
[62,     1] loss: 1209.280
[63,     1] loss: 1236.112
[64,     1] loss: 1138.023
[65,     1] loss: 1340.634
[66,     1] loss: 1370.678
[67,     1] loss: 1162.875
[68,     1] loss: 1423.110
[69,     1] loss: 1122.969
[70,     1] loss: 1199.340
[71,     1] loss: 1240.197
[72,     1] loss: 1294.978
[73,     1] loss: 1167.499
[74,     1] loss: 1180.770
[75,     1] loss: 1076.420
[76,     1] loss: 1185.348
[77,     1] loss: 1075.034
[78,     1] loss: 1133.427
[79,     1] loss: 1253.815
[80,     1] loss: 1035.820
[81,     1] loss: 1161.260
[82,     1] loss: 1138.522
[83,     1] loss: 1029.727
[84,     1] loss: 1038.258
[85,     1] loss: 1353.906
[86,     1] loss: 1204.603
[87,     1] loss: 961.982
[88,     1] loss: 1146.923
[89,     1] loss: 870.562
[90,     1] loss: 1164.310
[91,     1] loss: 921.421
[92,     1] loss: 1078.442
[93,     1] loss: 999.632
Early stopping applied (best metric=0.3779830038547516)
Finished Training
Total time taken: 16.689289331436157
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2163.780
[2,     1] loss: 2163.359
[3,     1] loss: 2156.672
[4,     1] loss: 2146.200
[5,     1] loss: 2158.587
[6,     1] loss: 2175.443
[7,     1] loss: 2170.022
[8,     1] loss: 2160.266
[9,     1] loss: 2155.814
[10,     1] loss: 2172.397
[11,     1] loss: 2170.364
[12,     1] loss: 2160.809
[13,     1] loss: 2165.149
[14,     1] loss: 2159.755
[15,     1] loss: 2157.453
[16,     1] loss: 2152.619
[17,     1] loss: 2157.865
[18,     1] loss: 2155.500
[19,     1] loss: 2156.142
[20,     1] loss: 2155.706
[21,     1] loss: 2176.147
[22,     1] loss: 2152.245
[23,     1] loss: 2155.296
[24,     1] loss: 2153.382
[25,     1] loss: 2143.395
[26,     1] loss: 2140.744
[27,     1] loss: 2141.890
[28,     1] loss: 2122.858
[29,     1] loss: 2101.287
[30,     1] loss: 2065.506
[31,     1] loss: 2038.444
[32,     1] loss: 2024.870
[33,     1] loss: 1909.781
[34,     1] loss: 1839.734
[35,     1] loss: 1853.182
[36,     1] loss: 1805.050
[37,     1] loss: 1788.677
[38,     1] loss: 1784.660
[39,     1] loss: 1703.417
[40,     1] loss: 1727.141
[41,     1] loss: 1739.866
[42,     1] loss: 1799.675
[43,     1] loss: 1625.224
[44,     1] loss: 1776.431
[45,     1] loss: 1736.508
[46,     1] loss: 1709.593
[47,     1] loss: 1633.604
[48,     1] loss: 1587.909
[49,     1] loss: 1645.143
[50,     1] loss: 1553.235
[51,     1] loss: 1515.116
[52,     1] loss: 1573.908
[53,     1] loss: 1461.354
[54,     1] loss: 1495.256
[55,     1] loss: 1373.893
[56,     1] loss: 1571.782
[57,     1] loss: 1361.506
[58,     1] loss: 1533.483
[59,     1] loss: 1266.838
[60,     1] loss: 1538.265
[61,     1] loss: 1454.045
[62,     1] loss: 1365.861
[63,     1] loss: 1594.772
[64,     1] loss: 1370.321
[65,     1] loss: 1581.419
[66,     1] loss: 1285.677
[67,     1] loss: 1412.497
[68,     1] loss: 1213.891
[69,     1] loss: 1267.165
[70,     1] loss: 1180.795
[71,     1] loss: 1179.748
[72,     1] loss: 963.943
[73,     1] loss: 1426.578
[74,     1] loss: 1465.285
[75,     1] loss: 1034.467
[76,     1] loss: 1262.366
[77,     1] loss: 1072.031
[78,     1] loss: 1085.158
[79,     1] loss: 1170.409
[80,     1] loss: 1124.701
[81,     1] loss: 1035.976
[82,     1] loss: 1073.284
[83,     1] loss: 1015.801
[84,     1] loss: 1000.603
[85,     1] loss: 923.603
[86,     1] loss: 906.977
[87,     1] loss: 901.615
[88,     1] loss: 945.418
[89,     1] loss: 875.186
[90,     1] loss: 922.520
[91,     1] loss: 1044.381
[92,     1] loss: 935.380
[93,     1] loss: 885.139
Early stopping applied (best metric=0.41586026549339294)
Finished Training
Total time taken: 16.234540700912476
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2158.921
[2,     1] loss: 2191.641
[3,     1] loss: 2176.479
[4,     1] loss: 2165.834
[5,     1] loss: 2164.608
[6,     1] loss: 2159.065
[7,     1] loss: 2163.239
[8,     1] loss: 2163.470
[9,     1] loss: 2166.108
[10,     1] loss: 2164.648
[11,     1] loss: 2161.589
[12,     1] loss: 2158.400
[13,     1] loss: 2158.528
[14,     1] loss: 2159.450
[15,     1] loss: 2161.658
[16,     1] loss: 2157.937
[17,     1] loss: 2161.821
[18,     1] loss: 2160.179
[19,     1] loss: 2157.790
[20,     1] loss: 2157.361
[21,     1] loss: 2160.503
[22,     1] loss: 2159.024
[23,     1] loss: 2156.148
[24,     1] loss: 2153.996
[25,     1] loss: 2157.916
[26,     1] loss: 2155.192
[27,     1] loss: 2150.514
[28,     1] loss: 2145.792
[29,     1] loss: 2140.367
[30,     1] loss: 2124.647
[31,     1] loss: 2112.895
[32,     1] loss: 2086.798
[33,     1] loss: 2044.841
[34,     1] loss: 1992.957
[35,     1] loss: 1940.214
[36,     1] loss: 1931.791
[37,     1] loss: 1931.299
[38,     1] loss: 1953.149
[39,     1] loss: 1891.137
[40,     1] loss: 1806.566
[41,     1] loss: 1895.711
[42,     1] loss: 1862.833
[43,     1] loss: 1844.073
[44,     1] loss: 1779.521
[45,     1] loss: 1814.504
[46,     1] loss: 1872.932
[47,     1] loss: 1703.328
[48,     1] loss: 1813.641
[49,     1] loss: 1646.965
[50,     1] loss: 1700.838
[51,     1] loss: 1648.258
[52,     1] loss: 1679.826
[53,     1] loss: 1561.370
[54,     1] loss: 1597.375
[55,     1] loss: 1515.000
[56,     1] loss: 1478.403
[57,     1] loss: 1507.245
[58,     1] loss: 1436.430
[59,     1] loss: 1457.675
[60,     1] loss: 1454.513
[61,     1] loss: 1521.628
[62,     1] loss: 1574.652
[63,     1] loss: 1410.778
[64,     1] loss: 1518.089
[65,     1] loss: 1464.219
[66,     1] loss: 1412.855
[67,     1] loss: 1247.732
[68,     1] loss: 1308.698
[69,     1] loss: 1143.615
[70,     1] loss: 1267.664
[71,     1] loss: 1189.616
[72,     1] loss: 1282.764
[73,     1] loss: 1472.942
[74,     1] loss: 1292.560
[75,     1] loss: 1117.799
[76,     1] loss: 1209.029
[77,     1] loss: 1214.060
[78,     1] loss: 1241.189
[79,     1] loss: 1104.177
[80,     1] loss: 1164.396
[81,     1] loss: 1085.396
[82,     1] loss: 1032.971
[83,     1] loss: 1095.966
[84,     1] loss: 1116.772
[85,     1] loss: 1317.110
[86,     1] loss: 1373.845
[87,     1] loss: 1112.607
[88,     1] loss: 1319.198
[89,     1] loss: 1132.715
[90,     1] loss: 1136.678
[91,     1] loss: 1111.385
[92,     1] loss: 1135.527
[93,     1] loss: 1177.201
[94,     1] loss: 959.613
[95,     1] loss: 1260.679
[96,     1] loss: 950.037
Early stopping applied (best metric=0.4408614933490753)
Finished Training
Total time taken: 16.35610318183899
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2156.382
[2,     1] loss: 2185.762
[3,     1] loss: 2166.246
[4,     1] loss: 2164.342
[5,     1] loss: 2163.240
[6,     1] loss: 2159.853
[7,     1] loss: 2157.370
[8,     1] loss: 2161.129
[9,     1] loss: 2151.611
[10,     1] loss: 2162.553
[11,     1] loss: 2154.600
[12,     1] loss: 2147.114
[13,     1] loss: 2149.701
[14,     1] loss: 2140.017
[15,     1] loss: 2130.538
[16,     1] loss: 2109.453
[17,     1] loss: 2085.770
[18,     1] loss: 2037.392
[19,     1] loss: 1979.561
[20,     1] loss: 1949.150
[21,     1] loss: 1895.010
[22,     1] loss: 1800.017
[23,     1] loss: 1814.952
[24,     1] loss: 1780.752
[25,     1] loss: 1799.025
[26,     1] loss: 1793.397
[27,     1] loss: 1743.489
[28,     1] loss: 1701.083
[29,     1] loss: 1696.430
[30,     1] loss: 1588.123
[31,     1] loss: 1664.056
[32,     1] loss: 1637.504
[33,     1] loss: 1728.885
[34,     1] loss: 1653.622
[35,     1] loss: 1601.552
[36,     1] loss: 1579.741
[37,     1] loss: 1539.462
[38,     1] loss: 1600.075
[39,     1] loss: 1583.267
[40,     1] loss: 1540.507
[41,     1] loss: 1418.254
[42,     1] loss: 1470.899
[43,     1] loss: 1460.141
[44,     1] loss: 1497.580
[45,     1] loss: 1438.885
[46,     1] loss: 1385.938
[47,     1] loss: 1433.345
[48,     1] loss: 1357.577
[49,     1] loss: 1293.870
[50,     1] loss: 1348.033
[51,     1] loss: 1366.147
[52,     1] loss: 1196.062
[53,     1] loss: 1299.971
[54,     1] loss: 1216.651
[55,     1] loss: 1232.672
[56,     1] loss: 1242.250
[57,     1] loss: 1237.078
[58,     1] loss: 1256.180
[59,     1] loss: 1156.709
[60,     1] loss: 1118.823
[61,     1] loss: 1463.198
[62,     1] loss: 1050.654
[63,     1] loss: 1367.667
[64,     1] loss: 1076.696
[65,     1] loss: 1310.045
[66,     1] loss: 1133.865
[67,     1] loss: 1199.448
[68,     1] loss: 1363.063
[69,     1] loss: 1075.030
[70,     1] loss: 1429.142
[71,     1] loss: 1058.025
[72,     1] loss: 1538.191
[73,     1] loss: 1153.858
[74,     1] loss: 1046.660
Early stopping applied (best metric=0.391962468624115)
Finished Training
Total time taken: 13.066240787506104
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2168.251
[2,     1] loss: 2166.203
[3,     1] loss: 2150.194
[4,     1] loss: 2184.743
[5,     1] loss: 2185.414
[6,     1] loss: 2157.289
[7,     1] loss: 2166.107
[8,     1] loss: 2152.959
[9,     1] loss: 2169.917
[10,     1] loss: 2163.879
[11,     1] loss: 2168.012
[12,     1] loss: 2164.440
[13,     1] loss: 2155.451
[14,     1] loss: 2157.665
[15,     1] loss: 2164.865
[16,     1] loss: 2160.568
[17,     1] loss: 2154.172
[18,     1] loss: 2151.844
[19,     1] loss: 2153.157
[20,     1] loss: 2140.755
[21,     1] loss: 2127.421
[22,     1] loss: 2112.820
[23,     1] loss: 2083.853
[24,     1] loss: 2042.565
[25,     1] loss: 1991.221
[26,     1] loss: 1900.146
[27,     1] loss: 1907.294
[28,     1] loss: 1808.073
[29,     1] loss: 1781.601
[30,     1] loss: 1778.636
[31,     1] loss: 1780.775
[32,     1] loss: 1802.385
[33,     1] loss: 1719.245
[34,     1] loss: 1823.311
[35,     1] loss: 1701.067
[36,     1] loss: 1699.174
[37,     1] loss: 1691.048
[38,     1] loss: 1680.617
[39,     1] loss: 1645.568
[40,     1] loss: 1732.076
[41,     1] loss: 1645.492
[42,     1] loss: 1602.880
[43,     1] loss: 1568.838
[44,     1] loss: 1586.327
[45,     1] loss: 1648.391
[46,     1] loss: 1526.021
[47,     1] loss: 1561.626
[48,     1] loss: 1431.531
[49,     1] loss: 1409.174
[50,     1] loss: 1540.863
[51,     1] loss: 1562.537
[52,     1] loss: 1379.936
[53,     1] loss: 1456.444
[54,     1] loss: 1413.058
[55,     1] loss: 1285.341
[56,     1] loss: 1238.523
[57,     1] loss: 1365.621
[58,     1] loss: 1374.888
[59,     1] loss: 1614.716
[60,     1] loss: 1275.980
[61,     1] loss: 1420.727
[62,     1] loss: 1305.438
[63,     1] loss: 1363.276
[64,     1] loss: 1382.222
[65,     1] loss: 1428.236
[66,     1] loss: 1142.546
[67,     1] loss: 1303.059
[68,     1] loss: 1089.164
[69,     1] loss: 1254.853
[70,     1] loss: 1157.446
[71,     1] loss: 1131.098
[72,     1] loss: 1083.572
[73,     1] loss: 1126.831
[74,     1] loss: 1037.262
[75,     1] loss: 1007.761
[76,     1] loss: 1048.197
[77,     1] loss: 1025.025
[78,     1] loss: 947.445
[79,     1] loss: 1074.049
Early stopping applied (best metric=0.37426701188087463)
Finished Training
Total time taken: 13.718442440032959
{'Hydroxylation-K Validation Accuracy': 0.638563829787234, 'Hydroxylation-K Validation Sensitivity': 0.6955555555555556, 'Hydroxylation-K Validation Specificity': 0.624561403508772, 'Hydroxylation-K Validation Precision': 0.3447594635995049, 'Hydroxylation-K AUC ROC': 0.7146198830409356, 'Hydroxylation-K AUC PR': 0.4914137386840919, 'Hydroxylation-K MCC': 0.2703215103797315, 'Hydroxylation-K F1': 0.44905139910178565, 'Validation Loss (Hydroxylation-K)': 0.519760153690974, 'Hydroxylation-P Validation Accuracy': 0.6813958682300391, 'Hydroxylation-P Validation Sensitivity': 0.7785714285714286, 'Hydroxylation-P Validation Specificity': 0.6606164896004788, 'Hydroxylation-P Validation Precision': 0.35005576283887896, 'Hydroxylation-P AUC ROC': 0.7792765383540484, 'Hydroxylation-P AUC PR': 0.47376962918635945, 'Hydroxylation-P MCC': 0.3485214334315388, 'Hydroxylation-P F1': 0.4766604254331563, 'Validation Loss (Hydroxylation-P)': 0.4257500092188517, 'Validation Loss (total)': 0.9455101648966472, 'TimeToTrain': 15.201294453938802}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001656595979804872,
 'learning_rate_Hydroxylation-K': 0.00669727340577165,
 'learning_rate_Hydroxylation-P': 0.0036540641097919007,
 'log_base': 1.1955785143941493,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3066036214,
 'sample_weights': [5.9455615304697425, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.698011410978523,
 'weight_decay_Hydroxylation-K': 9.648268217521538,
 'weight_decay_Hydroxylation-P': 3.521233786790603}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3030.459
[2,     1] loss: 3022.652
[3,     1] loss: 3042.936
[4,     1] loss: 3037.874
[5,     1] loss: 3024.365
[6,     1] loss: 3037.614
[7,     1] loss: 3028.167
[8,     1] loss: 3029.633
[9,     1] loss: 3019.264
[10,     1] loss: 3021.934
[11,     1] loss: 3017.273
[12,     1] loss: 3011.140
[13,     1] loss: 3016.605
[14,     1] loss: 3000.264
[15,     1] loss: 2995.723
[16,     1] loss: 2981.764
[17,     1] loss: 2946.414
[18,     1] loss: 2923.812
[19,     1] loss: 2873.499
[20,     1] loss: 2858.622
[21,     1] loss: 2829.516
[22,     1] loss: 2763.299
[23,     1] loss: 2761.026
[24,     1] loss: 2715.698
[25,     1] loss: 2678.375
[26,     1] loss: 2519.760
[27,     1] loss: 2564.029
[28,     1] loss: 2494.659
[29,     1] loss: 2525.902
[30,     1] loss: 2459.098
[31,     1] loss: 2451.335
[32,     1] loss: 2449.555
[33,     1] loss: 2626.458
[34,     1] loss: 2633.780
[35,     1] loss: 2436.308
[36,     1] loss: 2203.498
[37,     1] loss: 2700.986
[38,     1] loss: 2435.234
[39,     1] loss: 2342.028
[40,     1] loss: 2400.069
[41,     1] loss: 2249.959
[42,     1] loss: 2448.617
[43,     1] loss: 2405.151
[44,     1] loss: 2202.079
[45,     1] loss: 2280.139
[46,     1] loss: 2187.788
[47,     1] loss: 2173.919
[48,     1] loss: 2220.152
[49,     1] loss: 2089.870
[50,     1] loss: 2101.991
[51,     1] loss: 2124.117
[52,     1] loss: 1924.386
[53,     1] loss: 2026.566
[54,     1] loss: 1885.617
[55,     1] loss: 1949.063
[56,     1] loss: 1992.512
[57,     1] loss: 1874.335
[58,     1] loss: 1998.937
[59,     1] loss: 2020.875
[60,     1] loss: 1966.268
[61,     1] loss: 1848.147
[62,     1] loss: 1783.470
[63,     1] loss: 1846.921
[64,     1] loss: 1741.351
[65,     1] loss: 1884.385
[66,     1] loss: 1747.062
[67,     1] loss: 1724.848
[68,     1] loss: 2118.506
[69,     1] loss: 1678.789
[70,     1] loss: 2113.758
[71,     1] loss: 1764.671
[72,     1] loss: 2049.074
[73,     1] loss: 1668.097
[74,     1] loss: 1755.121
[75,     1] loss: 1670.440
[76,     1] loss: 1637.931
[77,     1] loss: 1787.794
[78,     1] loss: 1622.652
[79,     1] loss: 1622.997
[80,     1] loss: 1575.654
[81,     1] loss: 1583.842
[82,     1] loss: 1456.641
[83,     1] loss: 1521.931
[84,     1] loss: 1480.531
[85,     1] loss: 1454.421
[86,     1] loss: 1429.904
[87,     1] loss: 1263.535
[88,     1] loss: 1470.963
[89,     1] loss: 1308.956
[90,     1] loss: 1288.304
[91,     1] loss: 1494.356
[92,     1] loss: 1347.457
[93,     1] loss: 1527.063
[94,     1] loss: 1318.048
[95,     1] loss: 1353.070
[96,     1] loss: 1316.222
[97,     1] loss: 1697.360
[98,     1] loss: 1235.366
[99,     1] loss: 1580.176
[100,     1] loss: 1301.141
[101,     1] loss: 1872.209
[102,     1] loss: 1423.691
[103,     1] loss: 1377.114
[104,     1] loss: 1802.901
[105,     1] loss: 1408.107
[106,     1] loss: 1463.444
[107,     1] loss: 1492.798
[108,     1] loss: 1260.953
[109,     1] loss: 1501.928
[110,     1] loss: 1294.067
[111,     1] loss: 1458.879
[112,     1] loss: 1356.197
[113,     1] loss: 1292.787
[114,     1] loss: 1406.215
[115,     1] loss: 1363.094
[116,     1] loss: 1420.766
[117,     1] loss: 1388.505
[118,     1] loss: 1252.107
[119,     1] loss: 1207.313
[120,     1] loss: 1221.233
[121,     1] loss: 1219.469
[122,     1] loss: 1082.261
[123,     1] loss: 1274.743
[124,     1] loss: 1093.634
[125,     1] loss: 1283.036
[126,     1] loss: 1173.676
Early stopping applied (best metric=0.3916977643966675)
Finished Training
Total time taken: 22.05786371231079
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3029.204
[2,     1] loss: 3034.883
[3,     1] loss: 3030.247
[4,     1] loss: 3025.545
[5,     1] loss: 3032.675
[6,     1] loss: 3023.314
[7,     1] loss: 3024.191
[8,     1] loss: 3033.271
[9,     1] loss: 3027.902
[10,     1] loss: 3026.937
[11,     1] loss: 3013.885
[12,     1] loss: 3005.334
[13,     1] loss: 2978.662
[14,     1] loss: 3004.228
[15,     1] loss: 2989.550
[16,     1] loss: 2949.139
[17,     1] loss: 2904.461
[18,     1] loss: 2864.673
[19,     1] loss: 2840.650
[20,     1] loss: 2793.026
[21,     1] loss: 2735.101
[22,     1] loss: 2732.833
[23,     1] loss: 2617.196
[24,     1] loss: 2633.930
[25,     1] loss: 2559.565
[26,     1] loss: 2452.349
[27,     1] loss: 2445.748
[28,     1] loss: 2402.464
[29,     1] loss: 2519.082
[30,     1] loss: 2539.839
[31,     1] loss: 2245.979
[32,     1] loss: 2372.009
[33,     1] loss: 2291.736
[34,     1] loss: 2312.883
[35,     1] loss: 2277.848
[36,     1] loss: 2161.333
[37,     1] loss: 2174.253
[38,     1] loss: 2315.010
[39,     1] loss: 2112.863
[40,     1] loss: 2327.626
[41,     1] loss: 2184.202
[42,     1] loss: 2190.498
[43,     1] loss: 2121.595
[44,     1] loss: 2079.926
[45,     1] loss: 2117.895
[46,     1] loss: 2022.342
[47,     1] loss: 2123.536
[48,     1] loss: 2066.936
[49,     1] loss: 1925.091
[50,     1] loss: 1974.988
[51,     1] loss: 1927.694
[52,     1] loss: 2088.931
[53,     1] loss: 2002.227
[54,     1] loss: 1766.167
[55,     1] loss: 1871.893
[56,     1] loss: 1969.030
[57,     1] loss: 1684.338
[58,     1] loss: 1630.312
[59,     1] loss: 1812.075
[60,     1] loss: 1713.401
[61,     1] loss: 1721.326
[62,     1] loss: 1761.084
[63,     1] loss: 1676.051
[64,     1] loss: 1600.983
[65,     1] loss: 1562.109
[66,     1] loss: 1559.987
[67,     1] loss: 1757.107
[68,     1] loss: 1764.050
[69,     1] loss: 1515.410
[70,     1] loss: 1453.591
[71,     1] loss: 1522.611
[72,     1] loss: 1427.748
[73,     1] loss: 1425.664
[74,     1] loss: 1573.180
[75,     1] loss: 1470.585
[76,     1] loss: 1437.916
[77,     1] loss: 1476.886
[78,     1] loss: 1295.596
[79,     1] loss: 1456.404
[80,     1] loss: 1345.959
[81,     1] loss: 1620.107
[82,     1] loss: 1642.982
[83,     1] loss: 1401.305
[84,     1] loss: 1528.487
[85,     1] loss: 1482.131
[86,     1] loss: 1485.844
[87,     1] loss: 1548.174
[88,     1] loss: 1349.927
[89,     1] loss: 1632.420
[90,     1] loss: 1508.948
[91,     1] loss: 1675.766
[92,     1] loss: 1517.336
[93,     1] loss: 1627.650
[94,     1] loss: 1431.875
[95,     1] loss: 1440.330
[96,     1] loss: 1623.134
[97,     1] loss: 1378.593
[98,     1] loss: 1423.737
[99,     1] loss: 1868.984
[100,     1] loss: 1386.249
[101,     1] loss: 1984.580
[102,     1] loss: 1457.489
[103,     1] loss: 1569.315
[104,     1] loss: 1563.296
[105,     1] loss: 1349.319
[106,     1] loss: 1423.831
[107,     1] loss: 1622.707
[108,     1] loss: 1336.065
[109,     1] loss: 1262.807
[110,     1] loss: 1479.832
[111,     1] loss: 1356.547
[112,     1] loss: 1333.614
[113,     1] loss: 1171.989
[114,     1] loss: 1222.082
[115,     1] loss: 1261.659
[116,     1] loss: 1193.919
[117,     1] loss: 1295.870
[118,     1] loss: 1322.647
[119,     1] loss: 1210.425
[120,     1] loss: 1122.825
[121,     1] loss: 1200.631
[122,     1] loss: 1131.457
[123,     1] loss: 1273.016
[124,     1] loss: 1238.566
[125,     1] loss: 1151.141
[126,     1] loss: 1188.118
[127,     1] loss: 1090.766
[128,     1] loss: 1286.818
[129,     1] loss: 1201.893
[130,     1] loss: 1292.724
[131,     1] loss: 1242.493
[132,     1] loss: 1341.088
[133,     1] loss: 1159.153
[134,     1] loss: 1077.636
[135,     1] loss: 1064.297
[136,     1] loss: 1149.664
[137,     1] loss: 1363.551
[138,     1] loss: 1098.361
[139,     1] loss: 1187.250
[140,     1] loss: 1153.943
[141,     1] loss: 1146.796
[142,     1] loss: 1337.359
[143,     1] loss: 1090.860
[144,     1] loss: 1368.237
[145,     1] loss: 1129.160
[146,     1] loss: 1708.557
[147,     1] loss: 1234.742
[148,     1] loss: 1418.070
[149,     1] loss: 1145.884
[150,     1] loss: 1340.224
[151,     1] loss: 1387.583
[152,     1] loss: 1163.616
[153,     1] loss: 1523.352
[154,     1] loss: 1325.383
[155,     1] loss: 1135.180
[156,     1] loss: 1376.575
Early stopping applied (best metric=0.48858126997947693)
Finished Training
Total time taken: 25.164790868759155
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3040.851
[2,     1] loss: 3030.097
[3,     1] loss: 3047.289
[4,     1] loss: 3023.598
[5,     1] loss: 3029.504
[6,     1] loss: 3050.717
[7,     1] loss: 3040.922
[8,     1] loss: 3020.295
[9,     1] loss: 3043.623
[10,     1] loss: 3026.716
[11,     1] loss: 3018.995
[12,     1] loss: 3027.745
[13,     1] loss: 3022.618
[14,     1] loss: 3012.950
[15,     1] loss: 3031.979
[16,     1] loss: 3018.551
[17,     1] loss: 3019.998
[18,     1] loss: 3011.277
[19,     1] loss: 3004.819
[20,     1] loss: 3006.492
[21,     1] loss: 2998.698
[22,     1] loss: 2974.770
[23,     1] loss: 2980.680
[24,     1] loss: 2927.562
[25,     1] loss: 2902.752
[26,     1] loss: 2869.169
[27,     1] loss: 2865.295
[28,     1] loss: 2833.977
[29,     1] loss: 2843.197
[30,     1] loss: 2739.134
[31,     1] loss: 2800.600
[32,     1] loss: 2619.140
[33,     1] loss: 2603.202
[34,     1] loss: 2638.143
[35,     1] loss: 2487.423
[36,     1] loss: 2555.765
[37,     1] loss: 2506.726
[38,     1] loss: 2586.581
[39,     1] loss: 2682.780
[40,     1] loss: 2557.932
[41,     1] loss: 2538.837
[42,     1] loss: 2520.121
[43,     1] loss: 2355.562
[44,     1] loss: 2290.101
[45,     1] loss: 2475.911
[46,     1] loss: 2459.991
[47,     1] loss: 2406.710
[48,     1] loss: 2301.042
[49,     1] loss: 2387.268
[50,     1] loss: 2204.108
[51,     1] loss: 2260.057
[52,     1] loss: 2204.095
[53,     1] loss: 2164.970
[54,     1] loss: 2253.101
[55,     1] loss: 2078.850
[56,     1] loss: 2065.050
[57,     1] loss: 2143.307
[58,     1] loss: 2089.923
[59,     1] loss: 1993.612
[60,     1] loss: 2026.240
[61,     1] loss: 1925.431
[62,     1] loss: 1971.442
[63,     1] loss: 1986.335
[64,     1] loss: 2086.692
[65,     1] loss: 1887.828
[66,     1] loss: 2213.746
[67,     1] loss: 1711.158
[68,     1] loss: 1974.053
[69,     1] loss: 1768.656
[70,     1] loss: 1692.379
[71,     1] loss: 1720.197
[72,     1] loss: 1859.773
[73,     1] loss: 1766.633
[74,     1] loss: 1960.238
[75,     1] loss: 1866.065
[76,     1] loss: 2146.648
[77,     1] loss: 1841.892
[78,     1] loss: 1725.467
[79,     1] loss: 1771.343
[80,     1] loss: 1797.475
[81,     1] loss: 1854.405
[82,     1] loss: 1717.255
[83,     1] loss: 1667.589
[84,     1] loss: 1820.543
[85,     1] loss: 1633.611
[86,     1] loss: 1701.167
Early stopping applied (best metric=0.4846818447113037)
Finished Training
Total time taken: 13.741618394851685
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3022.129
[2,     1] loss: 3038.794
[3,     1] loss: 3024.577
[4,     1] loss: 3026.332
[5,     1] loss: 3025.632
[6,     1] loss: 3039.540
[7,     1] loss: 3015.503
[8,     1] loss: 3033.911
[9,     1] loss: 3019.596
[10,     1] loss: 3015.448
[11,     1] loss: 2997.997
[12,     1] loss: 2983.748
[13,     1] loss: 2977.824
[14,     1] loss: 2961.967
[15,     1] loss: 2945.076
[16,     1] loss: 2917.281
[17,     1] loss: 2851.661
[18,     1] loss: 2820.543
[19,     1] loss: 2794.759
[20,     1] loss: 2775.200
[21,     1] loss: 2690.343
[22,     1] loss: 2646.423
[23,     1] loss: 2509.416
[24,     1] loss: 2659.029
[25,     1] loss: 2545.946
[26,     1] loss: 2455.687
[27,     1] loss: 2611.632
[28,     1] loss: 2303.876
[29,     1] loss: 2350.307
[30,     1] loss: 2449.864
[31,     1] loss: 2365.226
[32,     1] loss: 2318.358
[33,     1] loss: 2336.688
[34,     1] loss: 2318.436
[35,     1] loss: 2115.792
[36,     1] loss: 2258.557
[37,     1] loss: 2055.618
[38,     1] loss: 2219.516
[39,     1] loss: 2153.301
[40,     1] loss: 2354.818
[41,     1] loss: 2038.179
[42,     1] loss: 2033.494
[43,     1] loss: 2099.080
[44,     1] loss: 1802.519
[45,     1] loss: 2130.657
[46,     1] loss: 2054.780
[47,     1] loss: 1797.840
[48,     1] loss: 1920.181
[49,     1] loss: 1943.933
[50,     1] loss: 1925.905
[51,     1] loss: 1904.947
[52,     1] loss: 1718.591
[53,     1] loss: 1638.573
[54,     1] loss: 1697.652
[55,     1] loss: 1770.503
[56,     1] loss: 1618.174
[57,     1] loss: 1758.351
[58,     1] loss: 1492.104
[59,     1] loss: 1614.947
[60,     1] loss: 1588.880
[61,     1] loss: 1993.106
[62,     1] loss: 1491.568
[63,     1] loss: 1641.842
[64,     1] loss: 1639.389
[65,     1] loss: 1563.795
[66,     1] loss: 1711.178
[67,     1] loss: 1537.320
[68,     1] loss: 1498.719
[69,     1] loss: 1530.825
[70,     1] loss: 1525.318
[71,     1] loss: 1576.461
[72,     1] loss: 1500.107
[73,     1] loss: 1698.965
[74,     1] loss: 1510.063
[75,     1] loss: 1596.840
[76,     1] loss: 1527.273
Early stopping applied (best metric=0.44639110565185547)
Finished Training
Total time taken: 12.922686100006104
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3025.517
[2,     1] loss: 3042.339
[3,     1] loss: 3029.538
[4,     1] loss: 3033.045
[5,     1] loss: 3042.381
[6,     1] loss: 3033.745
[7,     1] loss: 3026.735
[8,     1] loss: 3041.436
[9,     1] loss: 3030.754
[10,     1] loss: 3024.362
[11,     1] loss: 3013.830
[12,     1] loss: 2999.616
[13,     1] loss: 2999.415
[14,     1] loss: 2968.753
[15,     1] loss: 2942.941
[16,     1] loss: 2918.443
[17,     1] loss: 2874.672
[18,     1] loss: 2828.913
[19,     1] loss: 2777.583
[20,     1] loss: 2764.682
[21,     1] loss: 2706.051
[22,     1] loss: 2733.721
[23,     1] loss: 2603.817
[24,     1] loss: 2594.474
[25,     1] loss: 2614.208
[26,     1] loss: 2447.727
[27,     1] loss: 2599.619
[28,     1] loss: 2448.590
[29,     1] loss: 2412.444
[30,     1] loss: 2455.566
[31,     1] loss: 2536.829
[32,     1] loss: 2376.404
[33,     1] loss: 2522.033
[34,     1] loss: 2317.288
[35,     1] loss: 2368.832
[36,     1] loss: 2330.990
[37,     1] loss: 2499.817
[38,     1] loss: 2301.536
[39,     1] loss: 2244.396
[40,     1] loss: 2201.360
[41,     1] loss: 2270.621
[42,     1] loss: 2162.961
[43,     1] loss: 2192.072
[44,     1] loss: 2123.059
[45,     1] loss: 2104.868
[46,     1] loss: 2137.247
[47,     1] loss: 1857.441
[48,     1] loss: 2005.329
[49,     1] loss: 2011.189
[50,     1] loss: 1938.857
[51,     1] loss: 1837.648
[52,     1] loss: 1824.064
[53,     1] loss: 1768.074
[54,     1] loss: 1741.668
[55,     1] loss: 1694.240
[56,     1] loss: 1687.797
[57,     1] loss: 1787.672
[58,     1] loss: 1727.863
[59,     1] loss: 1622.687
[60,     1] loss: 1561.141
[61,     1] loss: 1719.064
[62,     1] loss: 1887.672
[63,     1] loss: 1727.874
[64,     1] loss: 1796.609
[65,     1] loss: 1729.468
[66,     1] loss: 1567.246
[67,     1] loss: 1571.272
[68,     1] loss: 1583.885
[69,     1] loss: 1396.286
[70,     1] loss: 1674.680
[71,     1] loss: 1638.154
[72,     1] loss: 1654.996
[73,     1] loss: 1803.400
[74,     1] loss: 1563.841
[75,     1] loss: 1482.926
[76,     1] loss: 1581.327
[77,     1] loss: 1466.102
[78,     1] loss: 1647.591
[79,     1] loss: 1449.256
[80,     1] loss: 1553.666
[81,     1] loss: 1382.725
[82,     1] loss: 1669.884
[83,     1] loss: 1391.307
[84,     1] loss: 1465.919
[85,     1] loss: 1312.785
[86,     1] loss: 1268.237
[87,     1] loss: 1510.639
Early stopping applied (best metric=0.4158684015274048)
Finished Training
Total time taken: 14.56116795539856
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3022.958
[2,     1] loss: 3036.900
[3,     1] loss: 3018.456
[4,     1] loss: 3021.189
[5,     1] loss: 3023.429
[6,     1] loss: 3026.314
[7,     1] loss: 3018.215
[8,     1] loss: 3019.607
[9,     1] loss: 3016.073
[10,     1] loss: 3008.067
[11,     1] loss: 2988.232
[12,     1] loss: 2969.297
[13,     1] loss: 2933.288
[14,     1] loss: 2893.190
[15,     1] loss: 2829.083
[16,     1] loss: 2778.256
[17,     1] loss: 2675.440
[18,     1] loss: 2661.509
[19,     1] loss: 2609.646
[20,     1] loss: 2526.434
[21,     1] loss: 2437.699
[22,     1] loss: 2467.843
[23,     1] loss: 2421.301
[24,     1] loss: 2401.935
[25,     1] loss: 2600.566
[26,     1] loss: 2565.187
[27,     1] loss: 2483.602
[28,     1] loss: 2265.455
[29,     1] loss: 2338.528
[30,     1] loss: 2526.460
[31,     1] loss: 2365.798
[32,     1] loss: 2565.330
[33,     1] loss: 2438.765
[34,     1] loss: 2336.815
[35,     1] loss: 2216.695
[36,     1] loss: 2352.378
[37,     1] loss: 2242.476
[38,     1] loss: 2213.187
[39,     1] loss: 2294.861
[40,     1] loss: 2248.777
[41,     1] loss: 2196.533
[42,     1] loss: 2153.731
[43,     1] loss: 2242.235
[44,     1] loss: 2090.390
[45,     1] loss: 2192.050
[46,     1] loss: 1976.367
[47,     1] loss: 2352.504
[48,     1] loss: 1959.263
[49,     1] loss: 1836.926
[50,     1] loss: 2009.469
[51,     1] loss: 1877.329
[52,     1] loss: 1887.636
[53,     1] loss: 1797.041
[54,     1] loss: 1898.568
[55,     1] loss: 1837.863
[56,     1] loss: 1852.072
[57,     1] loss: 1890.426
[58,     1] loss: 1714.718
[59,     1] loss: 1715.882
[60,     1] loss: 1752.935
[61,     1] loss: 1876.529
[62,     1] loss: 1593.454
[63,     1] loss: 1759.250
[64,     1] loss: 1754.457
[65,     1] loss: 1863.239
[66,     1] loss: 1519.262
[67,     1] loss: 1710.573
[68,     1] loss: 1499.524
[69,     1] loss: 1885.569
[70,     1] loss: 1588.121
[71,     1] loss: 1612.533
[72,     1] loss: 1507.719
[73,     1] loss: 1611.085
[74,     1] loss: 1466.567
[75,     1] loss: 1473.767
[76,     1] loss: 1495.740
[77,     1] loss: 1593.851
[78,     1] loss: 1502.754
[79,     1] loss: 1329.214
[80,     1] loss: 1351.357
[81,     1] loss: 1332.186
[82,     1] loss: 1632.790
Early stopping applied (best metric=0.43201085925102234)
Finished Training
Total time taken: 12.724624872207642
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3030.449
[2,     1] loss: 3037.992
[3,     1] loss: 3022.768
[4,     1] loss: 3037.467
[5,     1] loss: 3020.068
[6,     1] loss: 3023.698
[7,     1] loss: 3019.410
[8,     1] loss: 3016.301
[9,     1] loss: 3014.894
[10,     1] loss: 3014.442
[11,     1] loss: 2993.772
[12,     1] loss: 2999.465
[13,     1] loss: 2970.870
[14,     1] loss: 2941.034
[15,     1] loss: 2893.163
[16,     1] loss: 2880.943
[17,     1] loss: 2863.447
[18,     1] loss: 2799.569
[19,     1] loss: 2710.064
[20,     1] loss: 2813.737
[21,     1] loss: 2674.618
[22,     1] loss: 2644.626
[23,     1] loss: 2646.910
[24,     1] loss: 2601.687
[25,     1] loss: 2540.115
[26,     1] loss: 2521.827
[27,     1] loss: 2551.575
[28,     1] loss: 2412.042
[29,     1] loss: 2436.474
[30,     1] loss: 2495.582
[31,     1] loss: 2488.593
[32,     1] loss: 2487.622
[33,     1] loss: 2351.722
[34,     1] loss: 2333.331
[35,     1] loss: 2397.613
[36,     1] loss: 2360.507
[37,     1] loss: 2251.720
[38,     1] loss: 2396.668
[39,     1] loss: 2168.289
[40,     1] loss: 2259.062
[41,     1] loss: 2311.702
[42,     1] loss: 2207.933
[43,     1] loss: 2413.061
[44,     1] loss: 2138.857
[45,     1] loss: 2117.813
[46,     1] loss: 2158.538
[47,     1] loss: 2226.813
[48,     1] loss: 1932.455
[49,     1] loss: 2051.602
[50,     1] loss: 2087.722
[51,     1] loss: 2042.029
[52,     1] loss: 2052.719
[53,     1] loss: 1790.159
[54,     1] loss: 1835.722
[55,     1] loss: 2018.357
[56,     1] loss: 2004.679
[57,     1] loss: 1998.007
[58,     1] loss: 1819.889
[59,     1] loss: 2045.474
[60,     1] loss: 1713.299
[61,     1] loss: 1864.300
[62,     1] loss: 1762.462
[63,     1] loss: 1752.968
[64,     1] loss: 1744.209
[65,     1] loss: 1678.055
[66,     1] loss: 1815.929
[67,     1] loss: 1573.317
[68,     1] loss: 1551.369
[69,     1] loss: 1723.468
[70,     1] loss: 1709.090
[71,     1] loss: 1606.105
[72,     1] loss: 1653.667
[73,     1] loss: 1465.557
Early stopping applied (best metric=0.47511953115463257)
Finished Training
Total time taken: 11.16709589958191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3027.045
[2,     1] loss: 3034.137
[3,     1] loss: 3031.060
[4,     1] loss: 3029.522
[5,     1] loss: 3023.127
[6,     1] loss: 3027.177
[7,     1] loss: 3032.848
[8,     1] loss: 3009.661
[9,     1] loss: 3027.295
[10,     1] loss: 3030.690
[11,     1] loss: 3017.304
[12,     1] loss: 3009.241
[13,     1] loss: 3014.062
[14,     1] loss: 3001.441
[15,     1] loss: 2984.979
[16,     1] loss: 2972.783
[17,     1] loss: 2953.146
[18,     1] loss: 2890.401
[19,     1] loss: 2866.191
[20,     1] loss: 2852.761
[21,     1] loss: 2818.360
[22,     1] loss: 2855.863
[23,     1] loss: 2728.617
[24,     1] loss: 2772.798
[25,     1] loss: 2675.383
[26,     1] loss: 2656.904
[27,     1] loss: 2629.459
[28,     1] loss: 2526.753
[29,     1] loss: 2483.607
[30,     1] loss: 2505.153
[31,     1] loss: 2403.395
[32,     1] loss: 2377.832
[33,     1] loss: 2617.206
[34,     1] loss: 2454.159
[35,     1] loss: 2477.858
[36,     1] loss: 2456.085
[37,     1] loss: 2377.671
[38,     1] loss: 2424.817
[39,     1] loss: 2220.097
[40,     1] loss: 2373.534
[41,     1] loss: 2137.101
[42,     1] loss: 2241.450
[43,     1] loss: 2209.520
[44,     1] loss: 2204.990
[45,     1] loss: 2463.434
[46,     1] loss: 2168.546
[47,     1] loss: 2148.688
[48,     1] loss: 2124.171
[49,     1] loss: 1930.136
[50,     1] loss: 2042.555
[51,     1] loss: 1994.383
[52,     1] loss: 1945.242
[53,     1] loss: 2082.142
[54,     1] loss: 1996.101
[55,     1] loss: 2018.525
[56,     1] loss: 1860.893
[57,     1] loss: 2026.728
[58,     1] loss: 1857.519
[59,     1] loss: 1762.409
[60,     1] loss: 1949.590
[61,     1] loss: 1656.889
[62,     1] loss: 1935.077
[63,     1] loss: 1777.809
[64,     1] loss: 1917.312
[65,     1] loss: 1765.708
[66,     1] loss: 1960.398
[67,     1] loss: 1754.669
[68,     1] loss: 1880.764
[69,     1] loss: 1752.563
[70,     1] loss: 1666.879
[71,     1] loss: 1642.236
[72,     1] loss: 1674.837
[73,     1] loss: 1471.868
[74,     1] loss: 1694.140
[75,     1] loss: 1382.962
[76,     1] loss: 1525.330
[77,     1] loss: 1554.721
[78,     1] loss: 1448.674
[79,     1] loss: 1513.025
[80,     1] loss: 1590.434
[81,     1] loss: 1315.852
[82,     1] loss: 1542.247
[83,     1] loss: 1498.840
[84,     1] loss: 1470.966
[85,     1] loss: 1343.910
[86,     1] loss: 1548.603
[87,     1] loss: 1353.580
[88,     1] loss: 1572.255
[89,     1] loss: 1411.831
Early stopping applied (best metric=0.4618511199951172)
Finished Training
Total time taken: 13.832691669464111
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3033.314
[2,     1] loss: 3025.309
[3,     1] loss: 3026.858
[4,     1] loss: 3033.999
[5,     1] loss: 3025.799
[6,     1] loss: 3034.447
[7,     1] loss: 3031.152
[8,     1] loss: 3034.980
[9,     1] loss: 3014.064
[10,     1] loss: 3020.346
[11,     1] loss: 3016.737
[12,     1] loss: 2995.992
[13,     1] loss: 2982.666
[14,     1] loss: 2962.647
[15,     1] loss: 2934.126
[16,     1] loss: 2901.666
[17,     1] loss: 2886.374
[18,     1] loss: 2798.018
[19,     1] loss: 2722.869
[20,     1] loss: 2656.353
[21,     1] loss: 2618.159
[22,     1] loss: 2752.457
[23,     1] loss: 2631.752
[24,     1] loss: 2639.535
[25,     1] loss: 2572.072
[26,     1] loss: 2609.478
[27,     1] loss: 2386.878
[28,     1] loss: 2424.907
[29,     1] loss: 2435.880
[30,     1] loss: 2456.932
[31,     1] loss: 2381.803
[32,     1] loss: 2406.342
[33,     1] loss: 2312.304
[34,     1] loss: 2379.344
[35,     1] loss: 2370.375
[36,     1] loss: 2292.583
[37,     1] loss: 2238.537
[38,     1] loss: 2395.925
[39,     1] loss: 2206.294
[40,     1] loss: 2320.607
[41,     1] loss: 2125.219
[42,     1] loss: 2209.172
[43,     1] loss: 2313.100
[44,     1] loss: 2142.376
[45,     1] loss: 1985.111
[46,     1] loss: 1979.163
[47,     1] loss: 1998.251
[48,     1] loss: 2168.192
[49,     1] loss: 2040.781
[50,     1] loss: 1906.325
[51,     1] loss: 2199.018
[52,     1] loss: 1958.735
[53,     1] loss: 2020.698
[54,     1] loss: 2092.063
[55,     1] loss: 1922.518
[56,     1] loss: 1852.892
[57,     1] loss: 2218.501
[58,     1] loss: 1971.289
[59,     1] loss: 2261.765
[60,     1] loss: 1835.565
[61,     1] loss: 1982.403
[62,     1] loss: 1936.090
[63,     1] loss: 1978.701
[64,     1] loss: 1904.037
[65,     1] loss: 1835.527
[66,     1] loss: 1892.491
[67,     1] loss: 1610.156
[68,     1] loss: 1763.076
[69,     1] loss: 1575.046
[70,     1] loss: 2019.772
[71,     1] loss: 1501.396
[72,     1] loss: 1805.426
[73,     1] loss: 1494.825
[74,     1] loss: 1737.121
[75,     1] loss: 1542.753
[76,     1] loss: 1652.085
[77,     1] loss: 1387.319
[78,     1] loss: 1981.273
[79,     1] loss: 1412.540
[80,     1] loss: 1828.464
[81,     1] loss: 1418.097
[82,     1] loss: 1684.573
[83,     1] loss: 1508.609
[84,     1] loss: 1435.191
[85,     1] loss: 1529.464
[86,     1] loss: 1493.085
[87,     1] loss: 1429.603
[88,     1] loss: 1555.248
[89,     1] loss: 1317.195
[90,     1] loss: 1548.681
[91,     1] loss: 1571.173
[92,     1] loss: 1632.780
[93,     1] loss: 1552.852
[94,     1] loss: 1303.225
[95,     1] loss: 1627.099
[96,     1] loss: 1379.643
[97,     1] loss: 1406.719
[98,     1] loss: 1331.056
[99,     1] loss: 1323.408
[100,     1] loss: 1548.611
[101,     1] loss: 1276.938
[102,     1] loss: 1393.259
[103,     1] loss: 1179.006
[104,     1] loss: 1551.179
[105,     1] loss: 1303.727
[106,     1] loss: 1293.112
[107,     1] loss: 1245.207
[108,     1] loss: 1179.977
Early stopping applied (best metric=0.4499679505825043)
Finished Training
Total time taken: 17.09825587272644
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3036.357
[2,     1] loss: 3036.389
[3,     1] loss: 3044.612
[4,     1] loss: 3032.892
[5,     1] loss: 3037.631
[6,     1] loss: 3024.265
[7,     1] loss: 3031.003
[8,     1] loss: 3029.814
[9,     1] loss: 3019.847
[10,     1] loss: 3023.778
[11,     1] loss: 3028.882
[12,     1] loss: 3034.079
[13,     1] loss: 3033.813
[14,     1] loss: 3019.490
[15,     1] loss: 3009.918
[16,     1] loss: 3006.715
[17,     1] loss: 3003.895
[18,     1] loss: 2983.940
[19,     1] loss: 2975.316
[20,     1] loss: 2959.326
[21,     1] loss: 2897.572
[22,     1] loss: 2874.867
[23,     1] loss: 2838.167
[24,     1] loss: 2772.123
[25,     1] loss: 2807.203
[26,     1] loss: 2743.826
[27,     1] loss: 2665.348
[28,     1] loss: 2675.710
[29,     1] loss: 2613.511
[30,     1] loss: 2561.656
[31,     1] loss: 2495.093
[32,     1] loss: 2441.316
[33,     1] loss: 2431.728
[34,     1] loss: 2480.009
[35,     1] loss: 2736.999
[36,     1] loss: 2517.950
[37,     1] loss: 2478.966
[38,     1] loss: 2475.309
[39,     1] loss: 2430.060
[40,     1] loss: 2417.968
[41,     1] loss: 2353.259
[42,     1] loss: 2392.221
[43,     1] loss: 2335.154
[44,     1] loss: 2261.871
[45,     1] loss: 2300.299
[46,     1] loss: 2213.221
[47,     1] loss: 2301.201
[48,     1] loss: 2041.560
[49,     1] loss: 2225.632
[50,     1] loss: 2113.229
[51,     1] loss: 2117.196
[52,     1] loss: 2210.083
[53,     1] loss: 2050.392
[54,     1] loss: 1872.112
[55,     1] loss: 1920.244
[56,     1] loss: 2076.042
[57,     1] loss: 1859.871
[58,     1] loss: 2126.546
[59,     1] loss: 1808.806
[60,     1] loss: 1943.383
[61,     1] loss: 1760.062
[62,     1] loss: 1808.421
[63,     1] loss: 1664.832
[64,     1] loss: 1750.850
[65,     1] loss: 1800.488
[66,     1] loss: 1823.962
[67,     1] loss: 1580.447
[68,     1] loss: 1670.496
[69,     1] loss: 1686.914
[70,     1] loss: 1624.968
[71,     1] loss: 1697.698
[72,     1] loss: 1528.346
[73,     1] loss: 1459.704
[74,     1] loss: 1787.835
[75,     1] loss: 1473.299
[76,     1] loss: 1609.402
[77,     1] loss: 1509.354
[78,     1] loss: 1488.926
[79,     1] loss: 1493.888
[80,     1] loss: 1443.730
[81,     1] loss: 1476.438
[82,     1] loss: 1396.636
[83,     1] loss: 1424.356
[84,     1] loss: 1458.754
[85,     1] loss: 1422.572
[86,     1] loss: 1541.838
[87,     1] loss: 1614.377
[88,     1] loss: 1465.111
[89,     1] loss: 1348.024
Early stopping applied (best metric=0.39831653237342834)
Finished Training
Total time taken: 14.985486268997192
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3057.274
[2,     1] loss: 3033.450
[3,     1] loss: 3028.008
[4,     1] loss: 3026.119
[5,     1] loss: 3038.461
[6,     1] loss: 3026.951
[7,     1] loss: 3030.791
[8,     1] loss: 3040.631
[9,     1] loss: 3010.607
[10,     1] loss: 3045.081
[11,     1] loss: 3026.873
[12,     1] loss: 3024.856
[13,     1] loss: 3005.027
[14,     1] loss: 2996.479
[15,     1] loss: 3005.953
[16,     1] loss: 2987.142
[17,     1] loss: 2972.181
[18,     1] loss: 2922.422
[19,     1] loss: 2898.484
[20,     1] loss: 2854.782
[21,     1] loss: 2840.435
[22,     1] loss: 2831.312
[23,     1] loss: 2766.533
[24,     1] loss: 2716.373
[25,     1] loss: 2723.986
[26,     1] loss: 2573.852
[27,     1] loss: 2570.600
[28,     1] loss: 2691.699
[29,     1] loss: 2538.545
[30,     1] loss: 2441.688
[31,     1] loss: 2552.371
[32,     1] loss: 2432.363
[33,     1] loss: 2418.042
[34,     1] loss: 2548.629
[35,     1] loss: 2283.206
[36,     1] loss: 2397.542
[37,     1] loss: 2474.980
[38,     1] loss: 2363.779
[39,     1] loss: 2409.136
[40,     1] loss: 2351.527
[41,     1] loss: 2349.990
[42,     1] loss: 2269.188
[43,     1] loss: 2396.717
[44,     1] loss: 2163.167
[45,     1] loss: 2087.610
[46,     1] loss: 2054.880
[47,     1] loss: 1969.711
[48,     1] loss: 2121.956
[49,     1] loss: 2045.898
[50,     1] loss: 2180.881
[51,     1] loss: 2166.726
[52,     1] loss: 1922.633
[53,     1] loss: 2174.324
[54,     1] loss: 1871.693
[55,     1] loss: 2168.074
[56,     1] loss: 1827.619
[57,     1] loss: 1910.463
[58,     1] loss: 2146.100
[59,     1] loss: 1998.183
[60,     1] loss: 1850.509
[61,     1] loss: 1791.469
[62,     1] loss: 1782.624
[63,     1] loss: 1887.697
[64,     1] loss: 1817.532
[65,     1] loss: 1769.695
[66,     1] loss: 1711.341
[67,     1] loss: 1730.595
[68,     1] loss: 1772.003
[69,     1] loss: 1724.864
[70,     1] loss: 2150.163
[71,     1] loss: 1570.129
[72,     1] loss: 1707.666
[73,     1] loss: 1725.228
[74,     1] loss: 1455.006
[75,     1] loss: 1786.374
[76,     1] loss: 1755.311
Early stopping applied (best metric=0.4790305495262146)
Finished Training
Total time taken: 13.082823276519775
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3040.942
[2,     1] loss: 3040.480
[3,     1] loss: 3043.547
[4,     1] loss: 3031.153
[5,     1] loss: 3019.752
[6,     1] loss: 3030.644
[7,     1] loss: 3026.432
[8,     1] loss: 3022.364
[9,     1] loss: 3023.408
[10,     1] loss: 3026.078
[11,     1] loss: 3016.528
[12,     1] loss: 3022.162
[13,     1] loss: 3012.526
[14,     1] loss: 2996.588
[15,     1] loss: 3001.636
[16,     1] loss: 2982.850
[17,     1] loss: 2954.750
[18,     1] loss: 2896.868
[19,     1] loss: 2944.125
[20,     1] loss: 2833.633
[21,     1] loss: 2820.238
[22,     1] loss: 2762.776
[23,     1] loss: 2747.412
[24,     1] loss: 2695.501
[25,     1] loss: 2693.797
[26,     1] loss: 2598.335
[27,     1] loss: 2593.251
[28,     1] loss: 2641.502
[29,     1] loss: 2621.719
[30,     1] loss: 2520.313
[31,     1] loss: 2484.282
[32,     1] loss: 2588.987
[33,     1] loss: 2391.582
[34,     1] loss: 2491.056
[35,     1] loss: 2387.071
[36,     1] loss: 2469.720
[37,     1] loss: 2417.398
[38,     1] loss: 2327.279
[39,     1] loss: 2469.123
[40,     1] loss: 2321.189
[41,     1] loss: 2289.324
[42,     1] loss: 2215.495
[43,     1] loss: 2280.969
[44,     1] loss: 2210.772
[45,     1] loss: 2246.044
[46,     1] loss: 2059.792
[47,     1] loss: 2342.604
[48,     1] loss: 2103.096
[49,     1] loss: 2160.834
[50,     1] loss: 2069.387
[51,     1] loss: 1985.800
[52,     1] loss: 2166.361
[53,     1] loss: 2030.191
[54,     1] loss: 2130.038
[55,     1] loss: 1994.257
[56,     1] loss: 1843.772
[57,     1] loss: 1884.930
[58,     1] loss: 1765.786
[59,     1] loss: 1833.743
[60,     1] loss: 1770.012
[61,     1] loss: 1886.023
[62,     1] loss: 1772.865
[63,     1] loss: 1847.423
[64,     1] loss: 1774.555
[65,     1] loss: 1758.343
[66,     1] loss: 1601.719
[67,     1] loss: 1730.383
[68,     1] loss: 1744.632
[69,     1] loss: 1784.288
[70,     1] loss: 2045.854
[71,     1] loss: 2054.485
[72,     1] loss: 1737.895
[73,     1] loss: 1759.545
[74,     1] loss: 1724.168
[75,     1] loss: 1716.736
[76,     1] loss: 1774.182
[77,     1] loss: 1600.969
[78,     1] loss: 1546.961
[79,     1] loss: 1620.227
[80,     1] loss: 1538.068
[81,     1] loss: 1518.015
[82,     1] loss: 1653.168
[83,     1] loss: 1566.915
[84,     1] loss: 1518.700
[85,     1] loss: 1349.449
[86,     1] loss: 1517.397
[87,     1] loss: 1474.535
[88,     1] loss: 1444.228
[89,     1] loss: 1541.344
[90,     1] loss: 1430.046
[91,     1] loss: 1466.379
[92,     1] loss: 1417.609
[93,     1] loss: 1453.638
[94,     1] loss: 1309.577
[95,     1] loss: 1400.864
[96,     1] loss: 1270.515
[97,     1] loss: 1320.127
Early stopping applied (best metric=0.4170292913913727)
Finished Training
Total time taken: 15.555768966674805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3016.377
[2,     1] loss: 3045.901
[3,     1] loss: 3040.117
[4,     1] loss: 3043.190
[5,     1] loss: 3023.421
[6,     1] loss: 3028.787
[7,     1] loss: 3033.777
[8,     1] loss: 3027.345
[9,     1] loss: 3025.652
[10,     1] loss: 3012.139
[11,     1] loss: 3031.149
[12,     1] loss: 3005.549
[13,     1] loss: 2996.687
[14,     1] loss: 2993.003
[15,     1] loss: 2961.998
[16,     1] loss: 2949.601
[17,     1] loss: 2955.942
[18,     1] loss: 2897.740
[19,     1] loss: 2880.425
[20,     1] loss: 2855.099
[21,     1] loss: 2755.756
[22,     1] loss: 2730.245
[23,     1] loss: 2727.643
[24,     1] loss: 2704.444
[25,     1] loss: 2698.388
[26,     1] loss: 2517.404
[27,     1] loss: 2447.966
[28,     1] loss: 2595.301
[29,     1] loss: 2395.283
[30,     1] loss: 2403.736
[31,     1] loss: 2547.058
[32,     1] loss: 2487.326
[33,     1] loss: 2536.972
[34,     1] loss: 2571.506
[35,     1] loss: 2365.499
[36,     1] loss: 2489.059
[37,     1] loss: 2264.174
[38,     1] loss: 2382.950
[39,     1] loss: 2467.248
[40,     1] loss: 2258.144
[41,     1] loss: 2417.314
[42,     1] loss: 2159.930
[43,     1] loss: 2261.097
[44,     1] loss: 2276.325
[45,     1] loss: 2238.969
[46,     1] loss: 2104.042
[47,     1] loss: 2133.918
[48,     1] loss: 1961.526
[49,     1] loss: 2178.164
[50,     1] loss: 2019.739
[51,     1] loss: 1953.823
[52,     1] loss: 2122.145
[53,     1] loss: 1973.471
[54,     1] loss: 2065.732
[55,     1] loss: 2190.641
[56,     1] loss: 1815.133
[57,     1] loss: 1799.298
[58,     1] loss: 1894.526
[59,     1] loss: 1913.659
[60,     1] loss: 1870.779
[61,     1] loss: 1772.402
[62,     1] loss: 1926.098
[63,     1] loss: 1762.087
[64,     1] loss: 1883.254
[65,     1] loss: 1853.615
[66,     1] loss: 1759.354
[67,     1] loss: 1804.420
[68,     1] loss: 1757.053
[69,     1] loss: 1618.278
[70,     1] loss: 1545.882
[71,     1] loss: 1813.407
[72,     1] loss: 1548.111
[73,     1] loss: 1561.527
[74,     1] loss: 1732.443
[75,     1] loss: 1618.879
[76,     1] loss: 1626.943
[77,     1] loss: 1857.377
[78,     1] loss: 1832.081
[79,     1] loss: 1839.000
[80,     1] loss: 1664.029
[81,     1] loss: 1591.494
[82,     1] loss: 1526.523
[83,     1] loss: 1550.868
[84,     1] loss: 1654.969
[85,     1] loss: 1559.054
[86,     1] loss: 1408.371
[87,     1] loss: 1391.335
[88,     1] loss: 1560.158
[89,     1] loss: 1396.630
[90,     1] loss: 1546.813
[91,     1] loss: 1334.385
[92,     1] loss: 1565.749
[93,     1] loss: 1297.358
[94,     1] loss: 1385.393
[95,     1] loss: 1403.648
[96,     1] loss: 1352.112
[97,     1] loss: 1386.854
[98,     1] loss: 1789.509
[99,     1] loss: 1365.092
[100,     1] loss: 1339.180
[101,     1] loss: 1433.005
[102,     1] loss: 1197.702
[103,     1] loss: 1556.001
[104,     1] loss: 1341.058
[105,     1] loss: 1617.865
[106,     1] loss: 1294.370
[107,     1] loss: 1428.623
[108,     1] loss: 1328.664
[109,     1] loss: 1359.708
[110,     1] loss: 1410.062
[111,     1] loss: 1270.295
[112,     1] loss: 1415.356
[113,     1] loss: 1380.262
[114,     1] loss: 1416.763
[115,     1] loss: 1268.063
[116,     1] loss: 1233.989
[117,     1] loss: 1295.808
[118,     1] loss: 1310.683
[119,     1] loss: 1426.589
[120,     1] loss: 1267.996
[121,     1] loss: 1444.242
[122,     1] loss: 1193.663
[123,     1] loss: 1614.876
[124,     1] loss: 1267.821
[125,     1] loss: 1355.248
[126,     1] loss: 1309.946
[127,     1] loss: 1165.301
[128,     1] loss: 1256.932
[129,     1] loss: 1249.624
[130,     1] loss: 1485.985
[131,     1] loss: 1387.223
[132,     1] loss: 1153.349
[133,     1] loss: 1261.108
[134,     1] loss: 1201.195
[135,     1] loss: 1267.473
[136,     1] loss: 1185.705
[137,     1] loss: 1233.488
[138,     1] loss: 1272.427
[139,     1] loss: 1195.104
[140,     1] loss: 1265.467
[141,     1] loss: 1199.766
[142,     1] loss: 1302.376
[143,     1] loss: 1168.295
[144,     1] loss: 1149.764
[145,     1] loss: 1111.765
[146,     1] loss: 1055.007
[147,     1] loss: 1115.633
Early stopping applied (best metric=0.44011449813842773)
Finished Training
Total time taken: 23.612231016159058
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3060.390
[2,     1] loss: 3028.470
[3,     1] loss: 3042.482
[4,     1] loss: 3029.479
[5,     1] loss: 3030.360
[6,     1] loss: 3036.642
[7,     1] loss: 3030.402
[8,     1] loss: 3025.538
[9,     1] loss: 3029.592
[10,     1] loss: 3017.184
[11,     1] loss: 3024.551
[12,     1] loss: 3011.109
[13,     1] loss: 3005.517
[14,     1] loss: 2991.001
[15,     1] loss: 2966.873
[16,     1] loss: 2952.592
[17,     1] loss: 2924.382
[18,     1] loss: 2900.623
[19,     1] loss: 2875.402
[20,     1] loss: 2821.925
[21,     1] loss: 2815.434
[22,     1] loss: 2754.355
[23,     1] loss: 2682.334
[24,     1] loss: 2629.681
[25,     1] loss: 2640.207
[26,     1] loss: 2600.760
[27,     1] loss: 2575.086
[28,     1] loss: 2551.650
[29,     1] loss: 2561.714
[30,     1] loss: 2479.099
[31,     1] loss: 2626.313
[32,     1] loss: 2442.529
[33,     1] loss: 2460.123
[34,     1] loss: 2473.858
[35,     1] loss: 2452.876
[36,     1] loss: 2461.127
[37,     1] loss: 2428.361
[38,     1] loss: 2325.545
[39,     1] loss: 2335.529
[40,     1] loss: 2332.121
[41,     1] loss: 2321.188
[42,     1] loss: 2279.870
[43,     1] loss: 2147.553
[44,     1] loss: 2144.691
[45,     1] loss: 2114.587
[46,     1] loss: 2160.972
[47,     1] loss: 2193.925
[48,     1] loss: 2023.168
[49,     1] loss: 2196.748
[50,     1] loss: 1992.484
[51,     1] loss: 1911.424
[52,     1] loss: 1940.111
[53,     1] loss: 1997.078
[54,     1] loss: 1762.548
[55,     1] loss: 1815.074
[56,     1] loss: 1808.151
[57,     1] loss: 1867.593
[58,     1] loss: 1833.393
[59,     1] loss: 1878.370
[60,     1] loss: 1832.717
[61,     1] loss: 1805.640
[62,     1] loss: 1822.190
[63,     1] loss: 1712.022
[64,     1] loss: 1681.755
[65,     1] loss: 1608.229
[66,     1] loss: 1683.839
[67,     1] loss: 1530.845
[68,     1] loss: 1683.618
[69,     1] loss: 1616.875
[70,     1] loss: 1535.787
[71,     1] loss: 1454.971
[72,     1] loss: 1557.289
[73,     1] loss: 1595.906
[74,     1] loss: 1529.652
[75,     1] loss: 1478.741
[76,     1] loss: 1433.071
[77,     1] loss: 1485.627
[78,     1] loss: 1622.518
[79,     1] loss: 1413.252
[80,     1] loss: 1462.159
[81,     1] loss: 1520.118
[82,     1] loss: 1540.104
[83,     1] loss: 1465.475
[84,     1] loss: 1413.908
[85,     1] loss: 1332.311
Early stopping applied (best metric=0.3950211703777313)
Finished Training
Total time taken: 13.728156805038452
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3032.781
[2,     1] loss: 3035.605
[3,     1] loss: 3035.629
[4,     1] loss: 3034.277
[5,     1] loss: 3029.437
[6,     1] loss: 3033.416
[7,     1] loss: 3035.629
[8,     1] loss: 3030.980
[9,     1] loss: 3033.478
[10,     1] loss: 3031.999
[11,     1] loss: 3018.512
[12,     1] loss: 3014.399
[13,     1] loss: 3014.569
[14,     1] loss: 3004.212
[15,     1] loss: 2985.932
[16,     1] loss: 2970.202
[17,     1] loss: 2950.178
[18,     1] loss: 2918.893
[19,     1] loss: 2903.130
[20,     1] loss: 2859.144
[21,     1] loss: 2859.734
[22,     1] loss: 2844.085
[23,     1] loss: 2796.950
[24,     1] loss: 2796.837
[25,     1] loss: 2693.602
[26,     1] loss: 2646.942
[27,     1] loss: 2700.448
[28,     1] loss: 2590.825
[29,     1] loss: 2490.883
[30,     1] loss: 2455.482
[31,     1] loss: 2478.820
[32,     1] loss: 2528.069
[33,     1] loss: 2210.313
[34,     1] loss: 2488.758
[35,     1] loss: 2431.413
[36,     1] loss: 2309.784
[37,     1] loss: 2288.327
[38,     1] loss: 2313.778
[39,     1] loss: 2260.477
[40,     1] loss: 2126.628
[41,     1] loss: 2144.524
[42,     1] loss: 2376.770
[43,     1] loss: 2072.518
[44,     1] loss: 2265.769
[45,     1] loss: 2294.919
[46,     1] loss: 1979.994
[47,     1] loss: 2080.974
[48,     1] loss: 1950.605
[49,     1] loss: 1999.936
[50,     1] loss: 1969.849
[51,     1] loss: 1988.806
[52,     1] loss: 2071.080
[53,     1] loss: 1785.833
[54,     1] loss: 1964.581
[55,     1] loss: 1854.231
[56,     1] loss: 1934.217
[57,     1] loss: 1951.652
[58,     1] loss: 1971.420
[59,     1] loss: 1912.714
[60,     1] loss: 1839.990
[61,     1] loss: 1610.446
[62,     1] loss: 1635.775
[63,     1] loss: 1829.231
[64,     1] loss: 1520.028
[65,     1] loss: 1504.831
[66,     1] loss: 1562.512
[67,     1] loss: 1706.953
[68,     1] loss: 1657.912
[69,     1] loss: 1496.899
[70,     1] loss: 1549.286
[71,     1] loss: 1623.624
[72,     1] loss: 1716.514
[73,     1] loss: 1735.283
[74,     1] loss: 1683.425
[75,     1] loss: 1894.254
[76,     1] loss: 1740.847
[77,     1] loss: 2019.963
[78,     1] loss: 1659.743
[79,     1] loss: 1732.672
[80,     1] loss: 1778.699
[81,     1] loss: 1513.919
[82,     1] loss: 1802.262
[83,     1] loss: 1504.070
[84,     1] loss: 1727.593
[85,     1] loss: 1437.507
[86,     1] loss: 1633.175
[87,     1] loss: 1415.187
[88,     1] loss: 1442.141
[89,     1] loss: 1497.014
[90,     1] loss: 1483.465
[91,     1] loss: 1356.270
[92,     1] loss: 1496.217
[93,     1] loss: 1427.607
[94,     1] loss: 1422.725
[95,     1] loss: 1365.142
[96,     1] loss: 1472.482
[97,     1] loss: 1327.039
[98,     1] loss: 1421.971
[99,     1] loss: 1331.455
[100,     1] loss: 1516.848
[101,     1] loss: 1366.908
[102,     1] loss: 1364.937
[103,     1] loss: 1340.464
[104,     1] loss: 1276.143
[105,     1] loss: 1254.456
[106,     1] loss: 1201.832
[107,     1] loss: 1422.772
[108,     1] loss: 1434.128
[109,     1] loss: 1184.074
[110,     1] loss: 1193.927
[111,     1] loss: 1250.103
[112,     1] loss: 1367.576
[113,     1] loss: 1379.538
[114,     1] loss: 1371.876
[115,     1] loss: 1202.212
[116,     1] loss: 1239.277
[117,     1] loss: 1138.539
[118,     1] loss: 1295.907
[119,     1] loss: 1153.591
[120,     1] loss: 1190.856
[121,     1] loss: 1124.781
[122,     1] loss: 1302.350
[123,     1] loss: 1115.162
[124,     1] loss: 1264.774
[125,     1] loss: 1220.194
[126,     1] loss: 1192.434
[127,     1] loss: 1128.828
[128,     1] loss: 1271.056
[129,     1] loss: 1166.626
[130,     1] loss: 1119.805
[131,     1] loss: 1228.332
[132,     1] loss: 1379.536
[133,     1] loss: 1103.911
Early stopping applied (best metric=0.41920652985572815)
Finished Training
Total time taken: 21.441680431365967
{'Hydroxylation-K Validation Accuracy': 0.7102836879432624, 'Hydroxylation-K Validation Sensitivity': 0.7622222222222222, 'Hydroxylation-K Validation Specificity': 0.6964912280701754, 'Hydroxylation-K Validation Precision': 0.40640937155767876, 'Hydroxylation-K AUC ROC': 0.7918518518518519, 'Hydroxylation-K AUC PR': 0.5611846797319378, 'Hydroxylation-K MCC': 0.3897382066872851, 'Hydroxylation-K F1': 0.519745234475483, 'Validation Loss (Hydroxylation-K)': 0.46737996737162274, 'Hydroxylation-P Validation Accuracy': 0.7162908481802954, 'Hydroxylation-P Validation Sensitivity': 0.7555555555555555, 'Hydroxylation-P Validation Specificity': 0.7078582472941294, 'Hydroxylation-P Validation Precision': 0.36640753075567967, 'Hydroxylation-P AUC ROC': 0.7787593570355803, 'Hydroxylation-P AUC PR': 0.5002010123308226, 'Hydroxylation-P MCC': 0.37067913822092935, 'Hydroxylation-P F1': 0.49059713072903705, 'Validation Loss (Hydroxylation-P)': 0.4396592279275258, 'Validation Loss (total)': 0.9070391933123271, 'TimeToTrain': 16.378462807337442}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036523893912003766,
 'learning_rate_Hydroxylation-K': 0.0017832851038724542,
 'learning_rate_Hydroxylation-P': 0.005091057535140256,
 'log_base': 2.6669672756516682,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 571870291,
 'sample_weights': [9.3527394480816, 1.1666632634941656],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.9370471016370425,
 'weight_decay_Hydroxylation-K': 8.9475472866005,
 'weight_decay_Hydroxylation-P': 2.358952038951342}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.950
[2,     1] loss: 1275.371
[3,     1] loss: 1266.709
[4,     1] loss: 1267.203
[5,     1] loss: 1264.043
[6,     1] loss: 1267.261
[7,     1] loss: 1263.811
[8,     1] loss: 1264.191
[9,     1] loss: 1264.427
[10,     1] loss: 1263.985
[11,     1] loss: 1262.672
[12,     1] loss: 1258.532
[13,     1] loss: 1250.529
[14,     1] loss: 1233.977
[15,     1] loss: 1212.708
[16,     1] loss: 1209.852
[17,     1] loss: 1179.079
[18,     1] loss: 1147.147
[19,     1] loss: 1135.220
[20,     1] loss: 1098.464
[21,     1] loss: 1054.374
[22,     1] loss: 1070.161
[23,     1] loss: 1019.932
[24,     1] loss: 1004.828
[25,     1] loss: 1002.031
[26,     1] loss: 1013.372
[27,     1] loss: 990.002
[28,     1] loss: 1041.095
[29,     1] loss: 991.550
[30,     1] loss: 970.340
[31,     1] loss: 966.196
[32,     1] loss: 951.153
[33,     1] loss: 973.444
[34,     1] loss: 942.635
[35,     1] loss: 885.756
[36,     1] loss: 942.867
[37,     1] loss: 880.339
[38,     1] loss: 880.975
[39,     1] loss: 917.321
[40,     1] loss: 891.577
[41,     1] loss: 848.586
[42,     1] loss: 820.414
[43,     1] loss: 847.978
[44,     1] loss: 839.014
[45,     1] loss: 915.592
[46,     1] loss: 879.001
[47,     1] loss: 832.446
[48,     1] loss: 845.447
[49,     1] loss: 835.503
[50,     1] loss: 862.918
[51,     1] loss: 812.461
[52,     1] loss: 792.616
[53,     1] loss: 963.899
[54,     1] loss: 884.344
[55,     1] loss: 780.977
[56,     1] loss: 797.969
[57,     1] loss: 746.567
[58,     1] loss: 783.029
[59,     1] loss: 757.896
[60,     1] loss: 757.616
[61,     1] loss: 725.839
[62,     1] loss: 708.400
[63,     1] loss: 807.711
[64,     1] loss: 777.415
[65,     1] loss: 708.642
[66,     1] loss: 713.242
[67,     1] loss: 719.580
[68,     1] loss: 662.531
[69,     1] loss: 683.889
[70,     1] loss: 769.660
[71,     1] loss: 654.855
[72,     1] loss: 658.599
[73,     1] loss: 724.531
[74,     1] loss: 734.628
[75,     1] loss: 657.637
[76,     1] loss: 619.302
[77,     1] loss: 631.846
[78,     1] loss: 600.021
[79,     1] loss: 602.220
[80,     1] loss: 556.645
[81,     1] loss: 535.956
[82,     1] loss: 622.983
[83,     1] loss: 1471.371
[84,     1] loss: 730.527
[85,     1] loss: 655.117
[86,     1] loss: 702.316
[87,     1] loss: 772.694
[88,     1] loss: 744.299
[89,     1] loss: 699.389
[90,     1] loss: 724.307
[91,     1] loss: 663.042
[92,     1] loss: 725.083
[93,     1] loss: 658.128
[94,     1] loss: 633.581
[95,     1] loss: 619.381
[96,     1] loss: 633.969
[97,     1] loss: 591.462
[98,     1] loss: 592.846
[99,     1] loss: 579.239
[100,     1] loss: 558.444
[101,     1] loss: 574.035
[102,     1] loss: 708.305
[103,     1] loss: 506.030
[104,     1] loss: 713.710
[105,     1] loss: 771.844
[106,     1] loss: 667.678
[107,     1] loss: 685.612
[108,     1] loss: 623.270
[109,     1] loss: 594.072
[110,     1] loss: 650.378
[111,     1] loss: 589.162
[112,     1] loss: 633.336
[113,     1] loss: 527.060
[114,     1] loss: 588.017
[115,     1] loss: 524.802
[116,     1] loss: 482.442
[117,     1] loss: 579.564
[118,     1] loss: 458.312
[119,     1] loss: 715.855
[120,     1] loss: 915.848
[121,     1] loss: 617.116
[122,     1] loss: 794.349
Early stopping applied (best metric=0.4117797613143921)
Finished Training
Total time taken: 19.78248906135559
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.812
[2,     1] loss: 1264.991
[3,     1] loss: 1268.599
[4,     1] loss: 1273.810
[5,     1] loss: 1265.364
[6,     1] loss: 1259.515
[7,     1] loss: 1255.417
[8,     1] loss: 1243.960
[9,     1] loss: 1219.615
[10,     1] loss: 1190.401
[11,     1] loss: 1143.763
[12,     1] loss: 1118.443
[13,     1] loss: 1090.337
[14,     1] loss: 1054.513
[15,     1] loss: 1058.432
[16,     1] loss: 1011.175
[17,     1] loss: 1045.561
[18,     1] loss: 989.554
[19,     1] loss: 1000.463
[20,     1] loss: 958.922
[21,     1] loss: 976.220
[22,     1] loss: 970.755
[23,     1] loss: 918.446
[24,     1] loss: 937.132
[25,     1] loss: 924.400
[26,     1] loss: 881.933
[27,     1] loss: 943.531
[28,     1] loss: 896.369
[29,     1] loss: 949.962
[30,     1] loss: 874.767
[31,     1] loss: 906.318
[32,     1] loss: 855.887
[33,     1] loss: 830.258
[34,     1] loss: 885.392
[35,     1] loss: 842.797
[36,     1] loss: 878.048
[37,     1] loss: 893.636
[38,     1] loss: 800.463
[39,     1] loss: 867.763
[40,     1] loss: 797.426
[41,     1] loss: 836.081
[42,     1] loss: 836.340
[43,     1] loss: 781.735
[44,     1] loss: 842.376
[45,     1] loss: 751.550
[46,     1] loss: 752.238
[47,     1] loss: 801.919
[48,     1] loss: 772.457
[49,     1] loss: 731.505
[50,     1] loss: 710.223
[51,     1] loss: 696.445
[52,     1] loss: 744.526
[53,     1] loss: 904.729
[54,     1] loss: 931.727
[55,     1] loss: 764.054
[56,     1] loss: 796.801
[57,     1] loss: 816.064
[58,     1] loss: 756.428
[59,     1] loss: 828.396
[60,     1] loss: 842.401
[61,     1] loss: 724.968
[62,     1] loss: 870.546
[63,     1] loss: 755.921
[64,     1] loss: 762.036
[65,     1] loss: 755.658
[66,     1] loss: 672.238
[67,     1] loss: 755.730
[68,     1] loss: 641.658
[69,     1] loss: 678.219
[70,     1] loss: 625.849
[71,     1] loss: 717.799
[72,     1] loss: 598.559
[73,     1] loss: 590.521
[74,     1] loss: 607.551
[75,     1] loss: 598.369
[76,     1] loss: 692.367
[77,     1] loss: 570.973
[78,     1] loss: 541.761
[79,     1] loss: 578.756
[80,     1] loss: 548.974
[81,     1] loss: 549.877
[82,     1] loss: 716.183
[83,     1] loss: 770.482
[84,     1] loss: 613.229
[85,     1] loss: 805.926
[86,     1] loss: 621.405
[87,     1] loss: 677.126
[88,     1] loss: 652.421
[89,     1] loss: 579.630
[90,     1] loss: 691.981
[91,     1] loss: 553.460
[92,     1] loss: 584.604
[93,     1] loss: 521.033
[94,     1] loss: 558.100
[95,     1] loss: 590.891
[96,     1] loss: 515.881
[97,     1] loss: 604.882
[98,     1] loss: 549.664
[99,     1] loss: 540.964
[100,     1] loss: 572.399
[101,     1] loss: 496.043
Early stopping applied (best metric=0.3872169554233551)
Finished Training
Total time taken: 16.40170669555664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.036
[2,     1] loss: 1268.337
[3,     1] loss: 1269.630
[4,     1] loss: 1264.248
[5,     1] loss: 1265.423
[6,     1] loss: 1265.061
[7,     1] loss: 1263.333
[8,     1] loss: 1265.444
[9,     1] loss: 1262.352
[10,     1] loss: 1258.306
[11,     1] loss: 1252.506
[12,     1] loss: 1244.325
[13,     1] loss: 1228.150
[14,     1] loss: 1197.648
[15,     1] loss: 1178.802
[16,     1] loss: 1141.019
[17,     1] loss: 1125.056
[18,     1] loss: 1099.211
[19,     1] loss: 1083.676
[20,     1] loss: 1061.578
[21,     1] loss: 1055.517
[22,     1] loss: 1065.677
[23,     1] loss: 981.312
[24,     1] loss: 1046.708
[25,     1] loss: 1036.641
[26,     1] loss: 959.326
[27,     1] loss: 1007.085
[28,     1] loss: 1009.644
[29,     1] loss: 977.984
[30,     1] loss: 977.008
[31,     1] loss: 963.591
[32,     1] loss: 993.143
[33,     1] loss: 935.102
[34,     1] loss: 971.774
[35,     1] loss: 900.621
[36,     1] loss: 987.778
[37,     1] loss: 898.590
[38,     1] loss: 907.273
[39,     1] loss: 871.236
[40,     1] loss: 889.479
[41,     1] loss: 863.122
[42,     1] loss: 902.375
[43,     1] loss: 804.068
[44,     1] loss: 809.494
[45,     1] loss: 794.073
[46,     1] loss: 888.592
[47,     1] loss: 920.758
[48,     1] loss: 831.019
[49,     1] loss: 745.226
[50,     1] loss: 819.568
[51,     1] loss: 754.016
[52,     1] loss: 786.099
[53,     1] loss: 838.590
[54,     1] loss: 692.809
[55,     1] loss: 753.502
[56,     1] loss: 836.452
[57,     1] loss: 836.900
[58,     1] loss: 775.747
[59,     1] loss: 704.347
[60,     1] loss: 763.100
[61,     1] loss: 713.291
[62,     1] loss: 669.736
[63,     1] loss: 779.783
[64,     1] loss: 812.589
[65,     1] loss: 685.932
[66,     1] loss: 682.724
[67,     1] loss: 780.229
[68,     1] loss: 677.282
[69,     1] loss: 833.615
[70,     1] loss: 642.642
[71,     1] loss: 791.161
[72,     1] loss: 645.308
[73,     1] loss: 758.050
[74,     1] loss: 662.708
[75,     1] loss: 677.398
[76,     1] loss: 709.120
[77,     1] loss: 653.861
[78,     1] loss: 769.703
[79,     1] loss: 757.626
[80,     1] loss: 810.692
[81,     1] loss: 655.071
[82,     1] loss: 790.325
[83,     1] loss: 613.126
[84,     1] loss: 773.071
[85,     1] loss: 639.947
[86,     1] loss: 636.449
[87,     1] loss: 667.844
[88,     1] loss: 586.344
[89,     1] loss: 644.102
[90,     1] loss: 545.437
[91,     1] loss: 579.164
[92,     1] loss: 567.406
[93,     1] loss: 530.542
Early stopping applied (best metric=0.3614441454410553)
Finished Training
Total time taken: 14.609628200531006
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.859
[2,     1] loss: 1266.504
[3,     1] loss: 1269.383
[4,     1] loss: 1272.098
[5,     1] loss: 1258.650
[6,     1] loss: 1263.124
[7,     1] loss: 1261.844
[8,     1] loss: 1262.792
[9,     1] loss: 1247.366
[10,     1] loss: 1225.885
[11,     1] loss: 1194.536
[12,     1] loss: 1158.630
[13,     1] loss: 1125.402
[14,     1] loss: 1059.977
[15,     1] loss: 1046.461
[16,     1] loss: 984.495
[17,     1] loss: 994.928
[18,     1] loss: 1021.074
[19,     1] loss: 1012.673
[20,     1] loss: 965.534
[21,     1] loss: 995.857
[22,     1] loss: 1003.535
[23,     1] loss: 922.881
[24,     1] loss: 963.974
[25,     1] loss: 952.351
[26,     1] loss: 971.920
[27,     1] loss: 987.681
[28,     1] loss: 973.768
[29,     1] loss: 943.673
[30,     1] loss: 970.171
[31,     1] loss: 912.888
[32,     1] loss: 953.740
[33,     1] loss: 880.050
[34,     1] loss: 946.724
[35,     1] loss: 877.620
[36,     1] loss: 879.764
[37,     1] loss: 889.394
[38,     1] loss: 836.831
[39,     1] loss: 860.301
[40,     1] loss: 837.528
[41,     1] loss: 877.808
[42,     1] loss: 826.941
[43,     1] loss: 866.995
[44,     1] loss: 784.652
[45,     1] loss: 857.272
[46,     1] loss: 829.604
[47,     1] loss: 834.528
[48,     1] loss: 823.646
[49,     1] loss: 759.991
[50,     1] loss: 785.379
[51,     1] loss: 770.438
[52,     1] loss: 741.686
[53,     1] loss: 740.455
[54,     1] loss: 734.406
[55,     1] loss: 769.149
[56,     1] loss: 949.167
[57,     1] loss: 930.695
[58,     1] loss: 745.641
[59,     1] loss: 855.467
[60,     1] loss: 762.680
[61,     1] loss: 794.814
[62,     1] loss: 777.244
[63,     1] loss: 730.393
[64,     1] loss: 728.741
[65,     1] loss: 739.570
[66,     1] loss: 691.805
[67,     1] loss: 695.398
[68,     1] loss: 710.818
[69,     1] loss: 702.035
[70,     1] loss: 707.088
[71,     1] loss: 701.354
[72,     1] loss: 686.513
[73,     1] loss: 710.072
[74,     1] loss: 628.995
[75,     1] loss: 659.955
[76,     1] loss: 648.827
[77,     1] loss: 681.646
[78,     1] loss: 658.758
[79,     1] loss: 598.859
[80,     1] loss: 652.211
[81,     1] loss: 591.238
[82,     1] loss: 653.284
[83,     1] loss: 758.175
[84,     1] loss: 918.177
[85,     1] loss: 622.105
[86,     1] loss: 683.201
[87,     1] loss: 662.041
[88,     1] loss: 714.674
[89,     1] loss: 628.140
[90,     1] loss: 631.300
[91,     1] loss: 625.872
[92,     1] loss: 614.085
[93,     1] loss: 575.349
[94,     1] loss: 641.062
[95,     1] loss: 544.325
[96,     1] loss: 594.544
[97,     1] loss: 664.182
[98,     1] loss: 652.234
[99,     1] loss: 576.170
[100,     1] loss: 634.456
[101,     1] loss: 533.136
[102,     1] loss: 580.967
[103,     1] loss: 526.806
[104,     1] loss: 632.766
[105,     1] loss: 555.540
[106,     1] loss: 520.565
[107,     1] loss: 642.845
[108,     1] loss: 485.287
[109,     1] loss: 510.650
[110,     1] loss: 528.450
[111,     1] loss: 456.966
[112,     1] loss: 501.957
[113,     1] loss: 448.432
[114,     1] loss: 522.571
[115,     1] loss: 988.696
[116,     1] loss: 511.085
[117,     1] loss: 537.372
[118,     1] loss: 712.371
[119,     1] loss: 540.754
[120,     1] loss: 757.102
[121,     1] loss: 520.308
[122,     1] loss: 597.234
[123,     1] loss: 482.599
[124,     1] loss: 658.127
[125,     1] loss: 498.212
[126,     1] loss: 447.452
[127,     1] loss: 489.356
[128,     1] loss: 462.424
[129,     1] loss: 455.630
[130,     1] loss: 467.179
[131,     1] loss: 419.835
[132,     1] loss: 486.375
[133,     1] loss: 559.017
[134,     1] loss: 682.872
[135,     1] loss: 433.235
[136,     1] loss: 475.331
[137,     1] loss: 557.352
[138,     1] loss: 441.088
[139,     1] loss: 479.086
[140,     1] loss: 555.985
[141,     1] loss: 462.115
[142,     1] loss: 423.749
[143,     1] loss: 413.825
[144,     1] loss: 397.389
[145,     1] loss: 426.102
[146,     1] loss: 449.350
[147,     1] loss: 641.283
[148,     1] loss: 777.433
[149,     1] loss: 1102.901
[150,     1] loss: 756.049
[151,     1] loss: 765.064
[152,     1] loss: 959.023
[153,     1] loss: 701.446
[154,     1] loss: 747.125
[155,     1] loss: 805.375
[156,     1] loss: 664.042
[157,     1] loss: 743.562
[158,     1] loss: 569.019
[159,     1] loss: 692.247
[160,     1] loss: 597.041
[161,     1] loss: 677.271
[162,     1] loss: 572.943
[163,     1] loss: 565.698
[164,     1] loss: 552.906
[165,     1] loss: 507.582
[166,     1] loss: 498.677
[167,     1] loss: 475.079
[168,     1] loss: 505.436
[169,     1] loss: 640.762
[170,     1] loss: 504.473
[171,     1] loss: 417.538
Early stopping applied (best metric=0.4204992949962616)
Finished Training
Total time taken: 26.807721853256226
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1268.601
[2,     1] loss: 1269.878
[3,     1] loss: 1273.206
[4,     1] loss: 1268.676
[5,     1] loss: 1265.700
[6,     1] loss: 1263.511
[7,     1] loss: 1256.370
[8,     1] loss: 1241.588
[9,     1] loss: 1222.050
[10,     1] loss: 1182.553
[11,     1] loss: 1156.031
[12,     1] loss: 1108.195
[13,     1] loss: 1120.926
[14,     1] loss: 1067.770
[15,     1] loss: 1055.682
[16,     1] loss: 1056.608
[17,     1] loss: 1022.298
[18,     1] loss: 1004.669
[19,     1] loss: 1019.931
[20,     1] loss: 1009.976
[21,     1] loss: 1003.916
[22,     1] loss: 970.475
[23,     1] loss: 1057.021
[24,     1] loss: 958.348
[25,     1] loss: 938.188
[26,     1] loss: 936.335
[27,     1] loss: 899.789
[28,     1] loss: 905.042
[29,     1] loss: 936.295
[30,     1] loss: 1004.973
[31,     1] loss: 992.166
[32,     1] loss: 923.871
[33,     1] loss: 899.473
[34,     1] loss: 927.238
[35,     1] loss: 906.820
[36,     1] loss: 894.727
[37,     1] loss: 870.647
[38,     1] loss: 823.547
[39,     1] loss: 862.100
[40,     1] loss: 850.621
[41,     1] loss: 828.658
[42,     1] loss: 794.892
[43,     1] loss: 804.443
[44,     1] loss: 827.679
[45,     1] loss: 1034.033
[46,     1] loss: 875.241
[47,     1] loss: 832.285
[48,     1] loss: 868.423
[49,     1] loss: 860.461
[50,     1] loss: 789.794
[51,     1] loss: 807.833
[52,     1] loss: 801.581
[53,     1] loss: 762.974
[54,     1] loss: 795.575
[55,     1] loss: 743.942
[56,     1] loss: 776.814
[57,     1] loss: 737.801
[58,     1] loss: 673.243
[59,     1] loss: 758.274
[60,     1] loss: 730.106
[61,     1] loss: 728.430
[62,     1] loss: 904.853
[63,     1] loss: 691.389
[64,     1] loss: 802.381
[65,     1] loss: 715.965
[66,     1] loss: 785.816
[67,     1] loss: 713.783
[68,     1] loss: 701.415
[69,     1] loss: 627.941
[70,     1] loss: 707.097
[71,     1] loss: 680.377
[72,     1] loss: 634.618
[73,     1] loss: 609.974
[74,     1] loss: 577.127
[75,     1] loss: 591.637
[76,     1] loss: 540.702
[77,     1] loss: 564.218
[78,     1] loss: 639.140
[79,     1] loss: 699.518
[80,     1] loss: 1481.362
[81,     1] loss: 742.083
[82,     1] loss: 979.737
[83,     1] loss: 964.552
[84,     1] loss: 887.201
[85,     1] loss: 909.975
[86,     1] loss: 948.684
[87,     1] loss: 953.186
[88,     1] loss: 900.975
[89,     1] loss: 803.480
[90,     1] loss: 820.942
[91,     1] loss: 895.168
[92,     1] loss: 842.843
[93,     1] loss: 816.481
[94,     1] loss: 829.251
[95,     1] loss: 814.495
[96,     1] loss: 782.383
[97,     1] loss: 782.728
[98,     1] loss: 784.281
[99,     1] loss: 776.291
[100,     1] loss: 728.735
[101,     1] loss: 763.287
[102,     1] loss: 701.931
[103,     1] loss: 692.327
[104,     1] loss: 661.004
[105,     1] loss: 615.703
Early stopping applied (best metric=0.4128284752368927)
Finished Training
Total time taken: 16.881964683532715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.642
[2,     1] loss: 1267.087
[3,     1] loss: 1269.186
[4,     1] loss: 1266.237
[5,     1] loss: 1267.340
[6,     1] loss: 1264.270
[7,     1] loss: 1264.872
[8,     1] loss: 1267.323
[9,     1] loss: 1261.949
[10,     1] loss: 1253.648
[11,     1] loss: 1249.066
[12,     1] loss: 1236.810
[13,     1] loss: 1211.905
[14,     1] loss: 1181.785
[15,     1] loss: 1156.600
[16,     1] loss: 1122.883
[17,     1] loss: 1097.613
[18,     1] loss: 1103.788
[19,     1] loss: 1092.700
[20,     1] loss: 1036.811
[21,     1] loss: 998.761
[22,     1] loss: 1031.225
[23,     1] loss: 997.099
[24,     1] loss: 1034.044
[25,     1] loss: 947.324
[26,     1] loss: 956.035
[27,     1] loss: 919.830
[28,     1] loss: 968.441
[29,     1] loss: 914.202
[30,     1] loss: 910.448
[31,     1] loss: 954.301
[32,     1] loss: 894.504
[33,     1] loss: 954.005
[34,     1] loss: 903.210
[35,     1] loss: 864.689
[36,     1] loss: 872.467
[37,     1] loss: 853.093
[38,     1] loss: 828.828
[39,     1] loss: 846.325
[40,     1] loss: 803.214
[41,     1] loss: 848.469
[42,     1] loss: 823.233
[43,     1] loss: 735.950
[44,     1] loss: 775.120
[45,     1] loss: 815.446
[46,     1] loss: 790.016
[47,     1] loss: 772.094
[48,     1] loss: 740.673
[49,     1] loss: 849.825
[50,     1] loss: 758.079
[51,     1] loss: 743.386
[52,     1] loss: 762.662
[53,     1] loss: 700.376
[54,     1] loss: 729.319
[55,     1] loss: 765.746
[56,     1] loss: 726.427
[57,     1] loss: 674.290
[58,     1] loss: 712.833
[59,     1] loss: 680.644
[60,     1] loss: 671.843
[61,     1] loss: 727.605
[62,     1] loss: 873.597
[63,     1] loss: 741.763
[64,     1] loss: 695.952
[65,     1] loss: 744.835
[66,     1] loss: 691.160
[67,     1] loss: 699.314
[68,     1] loss: 613.793
[69,     1] loss: 639.263
[70,     1] loss: 617.856
[71,     1] loss: 600.028
[72,     1] loss: 581.112
[73,     1] loss: 584.634
[74,     1] loss: 663.840
[75,     1] loss: 773.406
[76,     1] loss: 729.414
[77,     1] loss: 599.955
[78,     1] loss: 633.561
[79,     1] loss: 605.754
[80,     1] loss: 633.209
[81,     1] loss: 574.446
[82,     1] loss: 540.336
[83,     1] loss: 601.317
[84,     1] loss: 573.209
[85,     1] loss: 515.914
[86,     1] loss: 565.777
Early stopping applied (best metric=0.3872852623462677)
Finished Training
Total time taken: 13.798145294189453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.483
[2,     1] loss: 1266.461
[3,     1] loss: 1265.915
[4,     1] loss: 1268.789
[5,     1] loss: 1266.879
[6,     1] loss: 1257.543
[7,     1] loss: 1250.475
[8,     1] loss: 1238.358
[9,     1] loss: 1220.186
[10,     1] loss: 1194.986
[11,     1] loss: 1147.604
[12,     1] loss: 1138.690
[13,     1] loss: 1086.770
[14,     1] loss: 1065.758
[15,     1] loss: 1052.488
[16,     1] loss: 1073.525
[17,     1] loss: 1062.132
[18,     1] loss: 1031.063
[19,     1] loss: 1061.130
[20,     1] loss: 1025.819
[21,     1] loss: 1000.007
[22,     1] loss: 989.085
[23,     1] loss: 1007.652
[24,     1] loss: 1000.271
[25,     1] loss: 949.307
[26,     1] loss: 924.610
[27,     1] loss: 971.465
[28,     1] loss: 935.911
[29,     1] loss: 895.097
[30,     1] loss: 918.130
[31,     1] loss: 968.627
[32,     1] loss: 896.008
[33,     1] loss: 934.831
[34,     1] loss: 886.029
[35,     1] loss: 897.555
[36,     1] loss: 903.927
[37,     1] loss: 861.746
[38,     1] loss: 937.186
[39,     1] loss: 839.416
[40,     1] loss: 911.845
[41,     1] loss: 844.722
[42,     1] loss: 847.992
[43,     1] loss: 813.471
[44,     1] loss: 822.540
[45,     1] loss: 859.083
[46,     1] loss: 841.280
[47,     1] loss: 889.477
[48,     1] loss: 830.005
[49,     1] loss: 792.446
[50,     1] loss: 801.824
[51,     1] loss: 785.455
[52,     1] loss: 757.413
[53,     1] loss: 791.511
[54,     1] loss: 789.730
[55,     1] loss: 769.714
[56,     1] loss: 722.185
[57,     1] loss: 748.359
[58,     1] loss: 857.786
[59,     1] loss: 902.236
[60,     1] loss: 702.279
[61,     1] loss: 806.043
[62,     1] loss: 703.983
[63,     1] loss: 801.000
[64,     1] loss: 687.131
[65,     1] loss: 713.155
[66,     1] loss: 690.017
[67,     1] loss: 661.491
[68,     1] loss: 684.065
[69,     1] loss: 707.098
[70,     1] loss: 625.985
[71,     1] loss: 687.057
[72,     1] loss: 683.374
[73,     1] loss: 765.352
[74,     1] loss: 716.619
[75,     1] loss: 646.379
[76,     1] loss: 687.054
[77,     1] loss: 594.075
[78,     1] loss: 687.676
[79,     1] loss: 638.113
[80,     1] loss: 741.070
[81,     1] loss: 613.582
[82,     1] loss: 552.018
[83,     1] loss: 734.704
[84,     1] loss: 799.122
[85,     1] loss: 568.218
[86,     1] loss: 677.985
[87,     1] loss: 572.954
[88,     1] loss: 606.332
[89,     1] loss: 595.759
[90,     1] loss: 543.865
[91,     1] loss: 562.555
[92,     1] loss: 497.553
[93,     1] loss: 584.044
[94,     1] loss: 541.683
[95,     1] loss: 478.914
[96,     1] loss: 530.929
[97,     1] loss: 509.665
[98,     1] loss: 527.502
[99,     1] loss: 548.801
[100,     1] loss: 537.099
[101,     1] loss: 578.927
[102,     1] loss: 495.913
[103,     1] loss: 469.268
[104,     1] loss: 501.623
[105,     1] loss: 630.874
[106,     1] loss: 1086.179
[107,     1] loss: 475.249
[108,     1] loss: 785.331
[109,     1] loss: 638.649
[110,     1] loss: 725.889
[111,     1] loss: 578.106
[112,     1] loss: 709.563
[113,     1] loss: 543.237
[114,     1] loss: 597.995
[115,     1] loss: 571.643
[116,     1] loss: 558.429
[117,     1] loss: 557.166
[118,     1] loss: 512.854
[119,     1] loss: 475.433
[120,     1] loss: 537.237
[121,     1] loss: 456.893
[122,     1] loss: 557.771
[123,     1] loss: 444.228
[124,     1] loss: 617.030
[125,     1] loss: 574.294
[126,     1] loss: 607.580
[127,     1] loss: 660.352
Early stopping applied (best metric=0.36881354451179504)
Finished Training
Total time taken: 20.484291791915894
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.265
[2,     1] loss: 1267.467
[3,     1] loss: 1264.581
[4,     1] loss: 1265.276
[5,     1] loss: 1264.878
[6,     1] loss: 1258.551
[7,     1] loss: 1257.146
[8,     1] loss: 1244.918
[9,     1] loss: 1220.956
[10,     1] loss: 1186.870
[11,     1] loss: 1144.269
[12,     1] loss: 1101.050
[13,     1] loss: 1089.190
[14,     1] loss: 1073.142
[15,     1] loss: 1054.374
[16,     1] loss: 1069.531
[17,     1] loss: 1112.087
[18,     1] loss: 1056.141
[19,     1] loss: 1138.236
[20,     1] loss: 1039.932
[21,     1] loss: 1057.781
[22,     1] loss: 1073.333
[23,     1] loss: 1013.277
[24,     1] loss: 1005.537
[25,     1] loss: 1038.546
[26,     1] loss: 1005.742
[27,     1] loss: 970.764
[28,     1] loss: 964.112
[29,     1] loss: 941.282
[30,     1] loss: 975.043
[31,     1] loss: 971.702
[32,     1] loss: 919.880
[33,     1] loss: 915.960
[34,     1] loss: 899.831
[35,     1] loss: 942.500
[36,     1] loss: 919.765
[37,     1] loss: 925.238
[38,     1] loss: 897.874
[39,     1] loss: 888.088
[40,     1] loss: 862.083
[41,     1] loss: 853.156
[42,     1] loss: 854.157
[43,     1] loss: 835.113
[44,     1] loss: 856.858
[45,     1] loss: 814.086
[46,     1] loss: 785.295
[47,     1] loss: 830.746
[48,     1] loss: 896.990
[49,     1] loss: 968.984
[50,     1] loss: 790.192
[51,     1] loss: 839.988
[52,     1] loss: 799.532
[53,     1] loss: 851.670
[54,     1] loss: 801.227
[55,     1] loss: 804.281
[56,     1] loss: 740.786
[57,     1] loss: 788.445
[58,     1] loss: 744.848
[59,     1] loss: 793.689
[60,     1] loss: 774.209
[61,     1] loss: 707.907
[62,     1] loss: 749.388
[63,     1] loss: 747.870
[64,     1] loss: 675.893
[65,     1] loss: 673.382
[66,     1] loss: 696.332
[67,     1] loss: 697.090
[68,     1] loss: 1060.839
[69,     1] loss: 911.913
[70,     1] loss: 698.723
[71,     1] loss: 830.714
[72,     1] loss: 732.765
[73,     1] loss: 772.133
[74,     1] loss: 737.846
[75,     1] loss: 735.305
[76,     1] loss: 789.916
[77,     1] loss: 703.015
[78,     1] loss: 710.545
[79,     1] loss: 654.908
[80,     1] loss: 676.807
[81,     1] loss: 566.304
[82,     1] loss: 684.624
[83,     1] loss: 595.582
[84,     1] loss: 632.285
[85,     1] loss: 626.500
[86,     1] loss: 548.274
[87,     1] loss: 576.979
[88,     1] loss: 662.473
[89,     1] loss: 762.478
[90,     1] loss: 536.114
[91,     1] loss: 721.048
[92,     1] loss: 685.854
[93,     1] loss: 704.914
[94,     1] loss: 622.109
[95,     1] loss: 631.198
[96,     1] loss: 575.291
[97,     1] loss: 598.703
[98,     1] loss: 608.438
[99,     1] loss: 507.785
[100,     1] loss: 575.174
[101,     1] loss: 587.396
[102,     1] loss: 580.764
[103,     1] loss: 485.201
[104,     1] loss: 548.700
[105,     1] loss: 482.583
Early stopping applied (best metric=0.3563889265060425)
Finished Training
Total time taken: 16.970682621002197
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.187
[2,     1] loss: 1268.834
[3,     1] loss: 1268.788
[4,     1] loss: 1265.501
[5,     1] loss: 1261.291
[6,     1] loss: 1261.265
[7,     1] loss: 1262.171
[8,     1] loss: 1248.837
[9,     1] loss: 1231.692
[10,     1] loss: 1205.360
[11,     1] loss: 1177.523
[12,     1] loss: 1162.029
[13,     1] loss: 1101.066
[14,     1] loss: 1139.395
[15,     1] loss: 1109.813
[16,     1] loss: 1053.535
[17,     1] loss: 1034.817
[18,     1] loss: 1070.811
[19,     1] loss: 1035.123
[20,     1] loss: 1002.629
[21,     1] loss: 1066.182
[22,     1] loss: 980.384
[23,     1] loss: 1028.915
[24,     1] loss: 1023.102
[25,     1] loss: 996.632
[26,     1] loss: 966.418
[27,     1] loss: 918.065
[28,     1] loss: 956.914
[29,     1] loss: 941.507
[30,     1] loss: 914.745
[31,     1] loss: 915.021
[32,     1] loss: 952.939
[33,     1] loss: 866.836
[34,     1] loss: 885.858
[35,     1] loss: 966.510
[36,     1] loss: 1021.835
[37,     1] loss: 890.900
[38,     1] loss: 1040.760
[39,     1] loss: 895.292
[40,     1] loss: 999.380
[41,     1] loss: 977.640
[42,     1] loss: 888.612
[43,     1] loss: 929.282
[44,     1] loss: 952.597
[45,     1] loss: 854.910
[46,     1] loss: 873.019
[47,     1] loss: 929.503
[48,     1] loss: 819.191
[49,     1] loss: 875.844
[50,     1] loss: 870.838
[51,     1] loss: 804.572
[52,     1] loss: 892.927
[53,     1] loss: 793.492
[54,     1] loss: 821.874
[55,     1] loss: 771.394
[56,     1] loss: 801.591
[57,     1] loss: 753.754
[58,     1] loss: 787.878
[59,     1] loss: 789.960
[60,     1] loss: 754.516
[61,     1] loss: 775.121
[62,     1] loss: 795.277
[63,     1] loss: 710.774
[64,     1] loss: 915.056
[65,     1] loss: 728.396
[66,     1] loss: 776.502
[67,     1] loss: 713.350
[68,     1] loss: 759.916
[69,     1] loss: 733.346
[70,     1] loss: 727.126
[71,     1] loss: 722.348
[72,     1] loss: 724.499
[73,     1] loss: 680.629
[74,     1] loss: 714.486
[75,     1] loss: 649.802
[76,     1] loss: 600.498
[77,     1] loss: 637.714
[78,     1] loss: 644.389
[79,     1] loss: 720.308
[80,     1] loss: 843.875
[81,     1] loss: 739.560
[82,     1] loss: 708.521
[83,     1] loss: 816.277
[84,     1] loss: 673.456
[85,     1] loss: 690.718
[86,     1] loss: 696.516
[87,     1] loss: 608.199
[88,     1] loss: 672.781
[89,     1] loss: 606.599
[90,     1] loss: 708.724
[91,     1] loss: 612.000
[92,     1] loss: 623.961
[93,     1] loss: 725.793
[94,     1] loss: 570.900
[95,     1] loss: 827.117
[96,     1] loss: 644.025
[97,     1] loss: 687.247
[98,     1] loss: 690.704
[99,     1] loss: 681.554
[100,     1] loss: 611.077
[101,     1] loss: 611.582
[102,     1] loss: 576.599
[103,     1] loss: 577.062
[104,     1] loss: 566.324
[105,     1] loss: 571.248
[106,     1] loss: 641.290
[107,     1] loss: 720.377
[108,     1] loss: 501.448
[109,     1] loss: 822.583
[110,     1] loss: 652.015
[111,     1] loss: 698.207
[112,     1] loss: 627.663
[113,     1] loss: 668.595
[114,     1] loss: 571.632
[115,     1] loss: 667.380
[116,     1] loss: 526.089
[117,     1] loss: 644.708
[118,     1] loss: 649.881
[119,     1] loss: 567.199
[120,     1] loss: 591.070
[121,     1] loss: 499.153
[122,     1] loss: 585.914
[123,     1] loss: 504.248
[124,     1] loss: 614.939
[125,     1] loss: 488.125
[126,     1] loss: 539.342
[127,     1] loss: 584.326
[128,     1] loss: 444.461
[129,     1] loss: 536.688
[130,     1] loss: 504.438
[131,     1] loss: 405.713
[132,     1] loss: 508.322
[133,     1] loss: 503.155
[134,     1] loss: 489.382
[135,     1] loss: 421.681
[136,     1] loss: 433.976
[137,     1] loss: 681.622
[138,     1] loss: 766.108
[139,     1] loss: 671.907
Early stopping applied (best metric=0.3076116144657135)
Finished Training
Total time taken: 23.92124366760254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1268.086
[2,     1] loss: 1272.947
[3,     1] loss: 1264.989
[4,     1] loss: 1266.143
[5,     1] loss: 1259.549
[6,     1] loss: 1255.253
[7,     1] loss: 1229.591
[8,     1] loss: 1190.352
[9,     1] loss: 1168.887
[10,     1] loss: 1126.530
[11,     1] loss: 1091.608
[12,     1] loss: 1084.083
[13,     1] loss: 1056.308
[14,     1] loss: 1084.288
[15,     1] loss: 1022.418
[16,     1] loss: 1043.028
[17,     1] loss: 1010.882
[18,     1] loss: 1019.782
[19,     1] loss: 996.272
[20,     1] loss: 969.284
[21,     1] loss: 1028.442
[22,     1] loss: 997.718
[23,     1] loss: 1008.222
[24,     1] loss: 941.490
[25,     1] loss: 984.646
[26,     1] loss: 947.773
[27,     1] loss: 924.076
[28,     1] loss: 917.960
[29,     1] loss: 933.370
[30,     1] loss: 929.913
[31,     1] loss: 877.487
[32,     1] loss: 895.096
[33,     1] loss: 880.796
[34,     1] loss: 899.445
[35,     1] loss: 902.496
[36,     1] loss: 841.737
[37,     1] loss: 816.989
[38,     1] loss: 811.256
[39,     1] loss: 844.974
[40,     1] loss: 798.885
[41,     1] loss: 832.251
[42,     1] loss: 840.320
[43,     1] loss: 793.218
[44,     1] loss: 810.516
[45,     1] loss: 756.198
[46,     1] loss: 795.407
[47,     1] loss: 740.581
[48,     1] loss: 782.332
[49,     1] loss: 973.199
[50,     1] loss: 900.977
[51,     1] loss: 785.884
[52,     1] loss: 831.997
[53,     1] loss: 868.794
[54,     1] loss: 764.285
[55,     1] loss: 826.240
[56,     1] loss: 733.976
[57,     1] loss: 785.027
[58,     1] loss: 745.483
[59,     1] loss: 712.964
[60,     1] loss: 714.416
[61,     1] loss: 719.874
[62,     1] loss: 694.871
[63,     1] loss: 647.858
[64,     1] loss: 655.164
[65,     1] loss: 679.501
[66,     1] loss: 646.925
[67,     1] loss: 660.046
[68,     1] loss: 596.763
[69,     1] loss: 719.027
[70,     1] loss: 861.425
[71,     1] loss: 713.010
[72,     1] loss: 656.055
[73,     1] loss: 716.016
[74,     1] loss: 637.921
[75,     1] loss: 661.959
[76,     1] loss: 612.710
[77,     1] loss: 626.997
[78,     1] loss: 567.720
[79,     1] loss: 580.701
[80,     1] loss: 674.654
[81,     1] loss: 981.632
[82,     1] loss: 758.276
[83,     1] loss: 579.323
[84,     1] loss: 667.868
[85,     1] loss: 642.183
[86,     1] loss: 665.234
[87,     1] loss: 674.625
[88,     1] loss: 636.500
[89,     1] loss: 637.671
[90,     1] loss: 596.216
[91,     1] loss: 617.185
[92,     1] loss: 573.030
[93,     1] loss: 549.575
[94,     1] loss: 575.293
[95,     1] loss: 574.625
Early stopping applied (best metric=0.35321861505508423)
Finished Training
Total time taken: 16.328070402145386
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.550
[2,     1] loss: 1270.417
[3,     1] loss: 1268.953
[4,     1] loss: 1265.591
[5,     1] loss: 1264.113
[6,     1] loss: 1268.427
[7,     1] loss: 1254.833
[8,     1] loss: 1243.680
[9,     1] loss: 1237.782
[10,     1] loss: 1210.151
[11,     1] loss: 1169.617
[12,     1] loss: 1142.701
[13,     1] loss: 1092.177
[14,     1] loss: 1063.468
[15,     1] loss: 1022.247
[16,     1] loss: 1077.417
[17,     1] loss: 1050.244
[18,     1] loss: 1000.190
[19,     1] loss: 986.523
[20,     1] loss: 1003.632
[21,     1] loss: 951.823
[22,     1] loss: 1011.839
[23,     1] loss: 911.402
[24,     1] loss: 985.069
[25,     1] loss: 972.258
[26,     1] loss: 960.730
[27,     1] loss: 930.579
[28,     1] loss: 907.583
[29,     1] loss: 909.304
[30,     1] loss: 914.250
[31,     1] loss: 907.887
[32,     1] loss: 926.264
[33,     1] loss: 855.585
[34,     1] loss: 884.711
[35,     1] loss: 883.106
[36,     1] loss: 894.298
[37,     1] loss: 873.233
[38,     1] loss: 860.150
[39,     1] loss: 831.371
[40,     1] loss: 864.501
[41,     1] loss: 832.651
[42,     1] loss: 853.644
[43,     1] loss: 833.945
[44,     1] loss: 856.356
[45,     1] loss: 824.495
[46,     1] loss: 800.265
[47,     1] loss: 816.091
[48,     1] loss: 727.825
[49,     1] loss: 814.485
[50,     1] loss: 826.299
[51,     1] loss: 761.541
[52,     1] loss: 762.733
[53,     1] loss: 734.215
[54,     1] loss: 716.868
[55,     1] loss: 781.413
[56,     1] loss: 746.461
[57,     1] loss: 674.768
[58,     1] loss: 744.722
[59,     1] loss: 664.996
[60,     1] loss: 668.388
[61,     1] loss: 630.789
[62,     1] loss: 661.380
[63,     1] loss: 681.930
[64,     1] loss: 1008.055
[65,     1] loss: 1401.249
[66,     1] loss: 942.378
[67,     1] loss: 824.361
[68,     1] loss: 989.843
[69,     1] loss: 1030.951
[70,     1] loss: 1034.611
[71,     1] loss: 1003.876
[72,     1] loss: 1022.512
[73,     1] loss: 1022.587
[74,     1] loss: 996.581
[75,     1] loss: 931.831
[76,     1] loss: 953.603
[77,     1] loss: 911.339
[78,     1] loss: 890.074
[79,     1] loss: 894.763
[80,     1] loss: 893.703
[81,     1] loss: 922.101
[82,     1] loss: 854.808
[83,     1] loss: 846.689
[84,     1] loss: 788.380
[85,     1] loss: 841.491
[86,     1] loss: 788.814
[87,     1] loss: 757.675
[88,     1] loss: 770.322
[89,     1] loss: 801.129
[90,     1] loss: 733.989
[91,     1] loss: 735.347
[92,     1] loss: 683.193
[93,     1] loss: 712.913
[94,     1] loss: 700.463
Early stopping applied (best metric=0.3704227805137634)
Finished Training
Total time taken: 14.737671613693237
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.380
[2,     1] loss: 1268.587
[3,     1] loss: 1269.377
[4,     1] loss: 1266.005
[5,     1] loss: 1263.797
[6,     1] loss: 1265.084
[7,     1] loss: 1254.291
[8,     1] loss: 1248.811
[9,     1] loss: 1227.970
[10,     1] loss: 1209.117
[11,     1] loss: 1180.802
[12,     1] loss: 1146.177
[13,     1] loss: 1109.596
[14,     1] loss: 1087.233
[15,     1] loss: 1077.692
[16,     1] loss: 1087.710
[17,     1] loss: 1061.691
[18,     1] loss: 1043.640
[19,     1] loss: 1079.268
[20,     1] loss: 1011.627
[21,     1] loss: 1027.693
[22,     1] loss: 1009.782
[23,     1] loss: 1002.422
[24,     1] loss: 1009.900
[25,     1] loss: 1000.796
[26,     1] loss: 938.542
[27,     1] loss: 988.387
[28,     1] loss: 922.520
[29,     1] loss: 938.355
[30,     1] loss: 946.480
[31,     1] loss: 943.086
[32,     1] loss: 989.192
[33,     1] loss: 861.266
[34,     1] loss: 936.022
[35,     1] loss: 896.469
[36,     1] loss: 929.044
[37,     1] loss: 885.756
[38,     1] loss: 908.764
[39,     1] loss: 824.194
[40,     1] loss: 876.524
[41,     1] loss: 977.205
[42,     1] loss: 856.614
[43,     1] loss: 895.860
[44,     1] loss: 884.902
[45,     1] loss: 862.796
[46,     1] loss: 928.636
[47,     1] loss: 822.400
[48,     1] loss: 879.301
[49,     1] loss: 818.668
[50,     1] loss: 881.016
[51,     1] loss: 833.950
[52,     1] loss: 841.111
[53,     1] loss: 852.768
[54,     1] loss: 756.542
[55,     1] loss: 800.286
[56,     1] loss: 760.383
[57,     1] loss: 803.079
[58,     1] loss: 775.721
[59,     1] loss: 751.294
[60,     1] loss: 775.412
[61,     1] loss: 701.439
[62,     1] loss: 776.486
[63,     1] loss: 901.244
[64,     1] loss: 689.224
[65,     1] loss: 843.734
[66,     1] loss: 727.008
[67,     1] loss: 820.354
[68,     1] loss: 717.537
[69,     1] loss: 751.934
[70,     1] loss: 691.095
[71,     1] loss: 687.424
[72,     1] loss: 655.069
[73,     1] loss: 705.143
[74,     1] loss: 776.528
[75,     1] loss: 633.379
[76,     1] loss: 683.416
[77,     1] loss: 667.903
[78,     1] loss: 604.348
[79,     1] loss: 658.022
[80,     1] loss: 623.423
[81,     1] loss: 609.773
[82,     1] loss: 602.527
[83,     1] loss: 645.557
[84,     1] loss: 775.789
[85,     1] loss: 965.070
[86,     1] loss: 600.598
[87,     1] loss: 847.283
[88,     1] loss: 677.941
[89,     1] loss: 723.691
[90,     1] loss: 724.914
[91,     1] loss: 620.210
[92,     1] loss: 695.200
[93,     1] loss: 589.024
[94,     1] loss: 670.639
[95,     1] loss: 570.076
[96,     1] loss: 619.574
[97,     1] loss: 685.401
[98,     1] loss: 546.263
[99,     1] loss: 756.798
[100,     1] loss: 588.708
[101,     1] loss: 626.840
[102,     1] loss: 570.233
[103,     1] loss: 549.269
[104,     1] loss: 499.302
[105,     1] loss: 509.090
[106,     1] loss: 566.390
[107,     1] loss: 524.615
[108,     1] loss: 402.193
[109,     1] loss: 428.019
[110,     1] loss: 503.936
[111,     1] loss: 489.231
[112,     1] loss: 652.521
[113,     1] loss: 630.244
[114,     1] loss: 603.526
[115,     1] loss: 469.819
[116,     1] loss: 552.595
[117,     1] loss: 529.790
[118,     1] loss: 511.290
[119,     1] loss: 613.009
[120,     1] loss: 590.281
[121,     1] loss: 494.262
[122,     1] loss: 690.008
[123,     1] loss: 615.177
[124,     1] loss: 601.167
[125,     1] loss: 594.586
[126,     1] loss: 543.143
[127,     1] loss: 553.969
[128,     1] loss: 451.294
[129,     1] loss: 521.869
[130,     1] loss: 400.017
[131,     1] loss: 536.095
[132,     1] loss: 526.250
[133,     1] loss: 496.281
[134,     1] loss: 496.700
[135,     1] loss: 585.894
Early stopping applied (best metric=0.29331836104393005)
Finished Training
Total time taken: 20.885928869247437
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.886
[2,     1] loss: 1264.213
[3,     1] loss: 1263.247
[4,     1] loss: 1264.656
[5,     1] loss: 1260.802
[6,     1] loss: 1254.592
[7,     1] loss: 1243.308
[8,     1] loss: 1230.518
[9,     1] loss: 1196.345
[10,     1] loss: 1155.251
[11,     1] loss: 1114.931
[12,     1] loss: 1084.615
[13,     1] loss: 1050.544
[14,     1] loss: 1014.156
[15,     1] loss: 1036.283
[16,     1] loss: 991.709
[17,     1] loss: 1057.707
[18,     1] loss: 986.312
[19,     1] loss: 1033.342
[20,     1] loss: 1026.247
[21,     1] loss: 1045.711
[22,     1] loss: 992.870
[23,     1] loss: 995.281
[24,     1] loss: 974.619
[25,     1] loss: 977.658
[26,     1] loss: 921.438
[27,     1] loss: 948.998
[28,     1] loss: 941.508
[29,     1] loss: 930.472
[30,     1] loss: 918.857
[31,     1] loss: 863.300
[32,     1] loss: 884.042
[33,     1] loss: 881.966
[34,     1] loss: 871.539
[35,     1] loss: 872.627
[36,     1] loss: 836.860
[37,     1] loss: 796.011
[38,     1] loss: 823.810
[39,     1] loss: 848.990
[40,     1] loss: 1036.065
[41,     1] loss: 894.760
[42,     1] loss: 875.848
[43,     1] loss: 833.818
[44,     1] loss: 843.760
[45,     1] loss: 838.535
[46,     1] loss: 810.442
[47,     1] loss: 788.306
[48,     1] loss: 814.957
[49,     1] loss: 819.515
[50,     1] loss: 784.689
[51,     1] loss: 766.686
[52,     1] loss: 763.749
[53,     1] loss: 754.276
[54,     1] loss: 680.247
[55,     1] loss: 699.808
[56,     1] loss: 661.716
[57,     1] loss: 676.770
[58,     1] loss: 752.416
[59,     1] loss: 842.778
[60,     1] loss: 620.061
[61,     1] loss: 737.338
[62,     1] loss: 688.145
[63,     1] loss: 644.110
[64,     1] loss: 675.919
[65,     1] loss: 619.728
[66,     1] loss: 613.299
[67,     1] loss: 644.785
[68,     1] loss: 624.064
[69,     1] loss: 614.337
[70,     1] loss: 713.802
[71,     1] loss: 652.266
[72,     1] loss: 605.921
[73,     1] loss: 676.510
[74,     1] loss: 608.166
[75,     1] loss: 595.298
[76,     1] loss: 686.054
[77,     1] loss: 588.597
[78,     1] loss: 543.992
[79,     1] loss: 574.735
[80,     1] loss: 499.115
[81,     1] loss: 512.293
[82,     1] loss: 559.445
[83,     1] loss: 621.580
[84,     1] loss: 703.060
[85,     1] loss: 545.065
[86,     1] loss: 547.204
[87,     1] loss: 601.455
[88,     1] loss: 508.638
[89,     1] loss: 525.292
[90,     1] loss: 570.370
[91,     1] loss: 492.863
[92,     1] loss: 541.954
[93,     1] loss: 889.572
[94,     1] loss: 618.511
[95,     1] loss: 444.469
Early stopping applied (best metric=0.3692591190338135)
Finished Training
Total time taken: 15.534575462341309
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.406
[2,     1] loss: 1265.353
[3,     1] loss: 1264.379
[4,     1] loss: 1267.515
[5,     1] loss: 1263.873
[6,     1] loss: 1253.622
[7,     1] loss: 1248.437
[8,     1] loss: 1228.568
[9,     1] loss: 1202.359
[10,     1] loss: 1169.330
[11,     1] loss: 1134.211
[12,     1] loss: 1101.454
[13,     1] loss: 1058.669
[14,     1] loss: 1051.727
[15,     1] loss: 1036.489
[16,     1] loss: 1039.532
[17,     1] loss: 1040.165
[18,     1] loss: 1005.278
[19,     1] loss: 1062.203
[20,     1] loss: 1008.090
[21,     1] loss: 998.158
[22,     1] loss: 1030.262
[23,     1] loss: 992.206
[24,     1] loss: 995.778
[25,     1] loss: 1009.907
[26,     1] loss: 942.298
[27,     1] loss: 957.415
[28,     1] loss: 949.787
[29,     1] loss: 947.109
[30,     1] loss: 902.621
[31,     1] loss: 923.045
[32,     1] loss: 906.307
[33,     1] loss: 899.146
[34,     1] loss: 907.518
[35,     1] loss: 942.226
[36,     1] loss: 973.516
[37,     1] loss: 861.248
[38,     1] loss: 920.653
[39,     1] loss: 895.812
[40,     1] loss: 873.316
[41,     1] loss: 917.450
[42,     1] loss: 843.280
[43,     1] loss: 850.077
[44,     1] loss: 840.343
[45,     1] loss: 895.293
[46,     1] loss: 837.179
[47,     1] loss: 832.119
[48,     1] loss: 765.416
[49,     1] loss: 793.639
[50,     1] loss: 794.344
[51,     1] loss: 689.674
[52,     1] loss: 811.737
[53,     1] loss: 803.886
[54,     1] loss: 801.201
[55,     1] loss: 776.333
[56,     1] loss: 750.859
[57,     1] loss: 747.582
[58,     1] loss: 747.771
[59,     1] loss: 695.878
[60,     1] loss: 753.569
[61,     1] loss: 715.845
[62,     1] loss: 656.459
[63,     1] loss: 775.315
[64,     1] loss: 871.844
[65,     1] loss: 693.717
[66,     1] loss: 685.487
[67,     1] loss: 694.550
[68,     1] loss: 639.592
[69,     1] loss: 719.453
[70,     1] loss: 614.815
[71,     1] loss: 708.030
[72,     1] loss: 714.469
[73,     1] loss: 604.704
[74,     1] loss: 602.961
[75,     1] loss: 657.512
[76,     1] loss: 573.961
[77,     1] loss: 604.462
[78,     1] loss: 585.266
[79,     1] loss: 642.659
[80,     1] loss: 541.915
[81,     1] loss: 706.931
[82,     1] loss: 903.847
[83,     1] loss: 604.802
[84,     1] loss: 815.029
[85,     1] loss: 602.253
[86,     1] loss: 726.348
[87,     1] loss: 598.098
[88,     1] loss: 613.083
[89,     1] loss: 613.703
[90,     1] loss: 536.700
[91,     1] loss: 576.988
[92,     1] loss: 663.197
[93,     1] loss: 550.979
[94,     1] loss: 509.288
[95,     1] loss: 513.842
[96,     1] loss: 488.162
[97,     1] loss: 510.669
[98,     1] loss: 528.400
[99,     1] loss: 547.769
[100,     1] loss: 596.695
[101,     1] loss: 722.811
[102,     1] loss: 496.904
[103,     1] loss: 511.673
[104,     1] loss: 524.323
[105,     1] loss: 492.233
[106,     1] loss: 479.440
[107,     1] loss: 518.519
[108,     1] loss: 526.961
[109,     1] loss: 656.162
[110,     1] loss: 720.682
[111,     1] loss: 559.681
[112,     1] loss: 560.879
[113,     1] loss: 559.268
[114,     1] loss: 550.330
[115,     1] loss: 588.971
[116,     1] loss: 509.436
[117,     1] loss: 745.533
[118,     1] loss: 558.708
[119,     1] loss: 568.911
[120,     1] loss: 574.073
[121,     1] loss: 504.970
[122,     1] loss: 564.657
[123,     1] loss: 465.321
[124,     1] loss: 484.808
[125,     1] loss: 493.666
[126,     1] loss: 486.985
[127,     1] loss: 586.556
[128,     1] loss: 609.087
[129,     1] loss: 466.449
[130,     1] loss: 667.693
[131,     1] loss: 534.845
[132,     1] loss: 534.595
[133,     1] loss: 529.504
[134,     1] loss: 451.341
[135,     1] loss: 507.868
[136,     1] loss: 399.200
[137,     1] loss: 468.835
[138,     1] loss: 426.323
[139,     1] loss: 401.081
[140,     1] loss: 409.217
[141,     1] loss: 416.809
[142,     1] loss: 595.501
[143,     1] loss: 1014.004
[144,     1] loss: 472.642
[145,     1] loss: 787.907
[146,     1] loss: 616.058
[147,     1] loss: 713.510
[148,     1] loss: 513.891
[149,     1] loss: 671.914
[150,     1] loss: 652.710
[151,     1] loss: 560.485
[152,     1] loss: 605.657
[153,     1] loss: 491.551
[154,     1] loss: 523.440
[155,     1] loss: 446.843
[156,     1] loss: 529.727
[157,     1] loss: 440.292
[158,     1] loss: 473.778
[159,     1] loss: 442.215
[160,     1] loss: 507.771
[161,     1] loss: 554.316
[162,     1] loss: 526.244
[163,     1] loss: 433.788
[164,     1] loss: 499.766
[165,     1] loss: 543.600
[166,     1] loss: 416.960
[167,     1] loss: 524.766
[168,     1] loss: 523.158
[169,     1] loss: 394.058
[170,     1] loss: 439.760
[171,     1] loss: 456.902
Early stopping applied (best metric=0.32957887649536133)
Finished Training
Total time taken: 28.163400173187256
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1270.070
[2,     1] loss: 1271.140
[3,     1] loss: 1267.825
[4,     1] loss: 1267.495
[5,     1] loss: 1266.701
[6,     1] loss: 1267.759
[7,     1] loss: 1261.041
[8,     1] loss: 1254.515
[9,     1] loss: 1246.391
[10,     1] loss: 1236.523
[11,     1] loss: 1212.536
[12,     1] loss: 1167.878
[13,     1] loss: 1152.165
[14,     1] loss: 1116.978
[15,     1] loss: 1083.620
[16,     1] loss: 1085.075
[17,     1] loss: 1046.444
[18,     1] loss: 1055.359
[19,     1] loss: 1027.433
[20,     1] loss: 1128.273
[21,     1] loss: 970.122
[22,     1] loss: 1010.506
[23,     1] loss: 958.576
[24,     1] loss: 993.040
[25,     1] loss: 958.136
[26,     1] loss: 980.168
[27,     1] loss: 923.528
[28,     1] loss: 998.802
[29,     1] loss: 955.734
[30,     1] loss: 934.243
[31,     1] loss: 918.078
[32,     1] loss: 940.145
[33,     1] loss: 890.522
[34,     1] loss: 891.573
[35,     1] loss: 890.303
[36,     1] loss: 881.128
[37,     1] loss: 910.906
[38,     1] loss: 851.186
[39,     1] loss: 843.117
[40,     1] loss: 883.470
[41,     1] loss: 846.078
[42,     1] loss: 801.345
[43,     1] loss: 810.632
[44,     1] loss: 781.386
[45,     1] loss: 720.323
[46,     1] loss: 842.126
[47,     1] loss: 1192.410
[48,     1] loss: 896.842
[49,     1] loss: 904.010
[50,     1] loss: 818.487
[51,     1] loss: 851.389
[52,     1] loss: 913.946
[53,     1] loss: 868.729
[54,     1] loss: 855.064
[55,     1] loss: 854.011
[56,     1] loss: 863.473
[57,     1] loss: 814.608
[58,     1] loss: 807.997
[59,     1] loss: 757.339
[60,     1] loss: 807.190
[61,     1] loss: 771.548
[62,     1] loss: 747.082
[63,     1] loss: 765.493
[64,     1] loss: 715.990
[65,     1] loss: 716.952
[66,     1] loss: 741.718
[67,     1] loss: 723.751
[68,     1] loss: 692.368
[69,     1] loss: 657.211
[70,     1] loss: 713.435
[71,     1] loss: 649.645
[72,     1] loss: 670.613
[73,     1] loss: 753.104
[74,     1] loss: 1083.343
[75,     1] loss: 931.980
[76,     1] loss: 742.583
[77,     1] loss: 810.121
[78,     1] loss: 865.995
[79,     1] loss: 747.888
[80,     1] loss: 776.729
[81,     1] loss: 830.628
[82,     1] loss: 772.964
[83,     1] loss: 796.976
[84,     1] loss: 798.007
[85,     1] loss: 738.613
[86,     1] loss: 705.211
[87,     1] loss: 694.424
[88,     1] loss: 688.514
[89,     1] loss: 739.541
[90,     1] loss: 645.970
[91,     1] loss: 722.732
[92,     1] loss: 677.459
[93,     1] loss: 645.530
[94,     1] loss: 606.992
[95,     1] loss: 612.676
[96,     1] loss: 582.829
[97,     1] loss: 550.607
[98,     1] loss: 589.773
[99,     1] loss: 656.113
[100,     1] loss: 623.221
[101,     1] loss: 540.013
[102,     1] loss: 571.368
[103,     1] loss: 675.266
[104,     1] loss: 868.723
[105,     1] loss: 543.316
[106,     1] loss: 947.264
[107,     1] loss: 732.843
[108,     1] loss: 943.312
[109,     1] loss: 667.717
[110,     1] loss: 797.670
[111,     1] loss: 750.896
Early stopping applied (best metric=0.3856494724750519)
Finished Training
Total time taken: 18.36306071281433
{'Hydroxylation-K Validation Accuracy': 0.7325059101654846, 'Hydroxylation-K Validation Sensitivity': 0.6711111111111111, 'Hydroxylation-K Validation Specificity': 0.7491228070175439, 'Hydroxylation-K Validation Precision': 0.40779803593304736, 'Hydroxylation-K AUC ROC': 0.7837621832358674, 'Hydroxylation-K AUC PR': 0.5818539913919825, 'Hydroxylation-K MCC': 0.3603483819941269, 'Hydroxylation-K F1': 0.49993914490446867, 'Validation Loss (Hydroxylation-K)': 0.4689147392908732, 'Hydroxylation-P Validation Accuracy': 0.7762121719709659, 'Hydroxylation-P Validation Sensitivity': 0.7862433862433863, 'Hydroxylation-P Validation Specificity': 0.7741308793456033, 'Hydroxylation-P Validation Precision': 0.4372223041493416, 'Hydroxylation-P AUC ROC': 0.8479831832644941, 'Hydroxylation-P AUC PR': 0.5935933972167582, 'Hydroxylation-P MCC': 0.46187606993195296, 'Hydroxylation-P F1': 0.5585685297439272, 'Validation Loss (Hydroxylation-P)': 0.3676876803239187, 'Validation Loss (total)': 0.8366024136543274, 'TimeToTrain': 18.911372073491414}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001974447879886873,
 'learning_rate_Hydroxylation-K': 0.0034927649095954113,
 'learning_rate_Hydroxylation-P': 0.00842277846160217,
 'log_base': 2.0001584434612107,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2435327195,
 'sample_weights': [1.7031400227598372, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.707216634885922,
 'weight_decay_Hydroxylation-K': 3.2818397440491545,
 'weight_decay_Hydroxylation-P': 0.30518498141183237}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1426.839
[2,     1] loss: 1418.727
[3,     1] loss: 1410.945
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061416180362847345,
 'learning_rate_Hydroxylation-K': 0.0009958354860042369,
 'learning_rate_Hydroxylation-P': 0.008348341094305791,
 'log_base': 1.8035107268455417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1700319337,
 'sample_weights': [2.408222118292406, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.024901015732892162,
 'weight_decay_Hydroxylation-K': 8.083544137814393,
 'weight_decay_Hydroxylation-P': 0.35311681150349994}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1513.226
[2,     1] loss: 1508.344
[3,     1] loss: 1508.894
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010827610117058006,
 'learning_rate_Hydroxylation-K': 0.0028267033121555417,
 'learning_rate_Hydroxylation-P': 0.0036943894386419636,
 'log_base': 2.0523705846469404,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2329007606,
 'sample_weights': [2.830835322007298, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.030042915538173,
 'weight_decay_Hydroxylation-K': 2.345848216959854,
 'weight_decay_Hydroxylation-P': 1.2758157929919078}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.360
[2,     1] loss: 1399.515
[3,     1] loss: 1400.047
[4,     1] loss: 1398.465
[5,     1] loss: 1395.075
[6,     1] loss: 1396.812
[7,     1] loss: 1394.676
[8,     1] loss: 1390.491
[9,     1] loss: 1393.438
[10,     1] loss: 1388.576
[11,     1] loss: 1386.700
[12,     1] loss: 1383.333
[13,     1] loss: 1375.796
[14,     1] loss: 1374.339
[15,     1] loss: 1364.015
[16,     1] loss: 1341.561
[17,     1] loss: 1337.996
[18,     1] loss: 1318.889
[19,     1] loss: 1302.939
[20,     1] loss: 1283.433
[21,     1] loss: 1263.857
[22,     1] loss: 1240.812
[23,     1] loss: 1256.350
[24,     1] loss: 1229.328
[25,     1] loss: 1217.861
[26,     1] loss: 1199.285
[27,     1] loss: 1219.826
[28,     1] loss: 1191.619
[29,     1] loss: 1146.836
[30,     1] loss: 1188.364
[31,     1] loss: 1220.183
[32,     1] loss: 1167.140
[33,     1] loss: 1129.520
[34,     1] loss: 1138.122
[35,     1] loss: 1189.523
[36,     1] loss: 1139.600
[37,     1] loss: 1141.173
[38,     1] loss: 1134.854
[39,     1] loss: 1106.040
[40,     1] loss: 1151.442
[41,     1] loss: 1120.257
[42,     1] loss: 1125.953
[43,     1] loss: 1083.314
[44,     1] loss: 1098.679
[45,     1] loss: 1107.551
[46,     1] loss: 1086.263
[47,     1] loss: 1124.352
[48,     1] loss: 1063.554
[49,     1] loss: 1112.411
[50,     1] loss: 1103.116
[51,     1] loss: 1103.483
[52,     1] loss: 1078.758
[53,     1] loss: 1029.306
[54,     1] loss: 1065.186
[55,     1] loss: 974.132
[56,     1] loss: 1029.401
[57,     1] loss: 1072.640
[58,     1] loss: 1024.963
[59,     1] loss: 1028.821
[60,     1] loss: 1038.662
[61,     1] loss: 1014.513
[62,     1] loss: 1023.151
[63,     1] loss: 1004.086
[64,     1] loss: 978.081
[65,     1] loss: 975.631
[66,     1] loss: 946.155
[67,     1] loss: 992.917
[68,     1] loss: 926.998
[69,     1] loss: 963.606
[70,     1] loss: 908.017
[71,     1] loss: 948.267
[72,     1] loss: 883.528
[73,     1] loss: 899.192
[74,     1] loss: 893.647
[75,     1] loss: 871.688
[76,     1] loss: 839.817
[77,     1] loss: 906.259
[78,     1] loss: 852.551
[79,     1] loss: 836.514
[80,     1] loss: 856.228
[81,     1] loss: 807.429
[82,     1] loss: 789.005
[83,     1] loss: 850.828
[84,     1] loss: 872.220
[85,     1] loss: 766.915
[86,     1] loss: 783.350
[87,     1] loss: 781.882
[88,     1] loss: 755.158
[89,     1] loss: 736.385
[90,     1] loss: 766.607
[91,     1] loss: 736.704
[92,     1] loss: 774.660
[93,     1] loss: 678.010
[94,     1] loss: 708.783
[95,     1] loss: 749.146
[96,     1] loss: 771.847
[97,     1] loss: 793.565
[98,     1] loss: 708.373
[99,     1] loss: 717.106
[100,     1] loss: 751.321
[101,     1] loss: 688.384
[102,     1] loss: 766.127
[103,     1] loss: 673.558
[104,     1] loss: 746.667
[105,     1] loss: 632.579
[106,     1] loss: 720.153
[107,     1] loss: 733.002
[108,     1] loss: 704.206
[109,     1] loss: 640.809
[110,     1] loss: 652.894
[111,     1] loss: 664.430
[112,     1] loss: 646.921
[113,     1] loss: 627.244
[114,     1] loss: 601.292
[115,     1] loss: 649.204
[116,     1] loss: 600.323
[117,     1] loss: 641.824
[118,     1] loss: 600.646
[119,     1] loss: 562.439
[120,     1] loss: 584.662
[121,     1] loss: 522.398
Early stopping applied (best metric=0.3537658751010895)
Finished Training
Total time taken: 19.390435457229614
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1400.819
[2,     1] loss: 1396.748
[3,     1] loss: 1397.077
[4,     1] loss: 1398.096
[5,     1] loss: 1396.729
[6,     1] loss: 1394.595
[7,     1] loss: 1394.643
[8,     1] loss: 1396.789
[9,     1] loss: 1388.145
[10,     1] loss: 1384.691
[11,     1] loss: 1388.518
[12,     1] loss: 1382.001
[13,     1] loss: 1374.381
[14,     1] loss: 1362.430
[15,     1] loss: 1339.620
[16,     1] loss: 1333.916
[17,     1] loss: 1314.251
[18,     1] loss: 1298.407
[19,     1] loss: 1276.378
[20,     1] loss: 1259.979
[21,     1] loss: 1228.801
[22,     1] loss: 1224.498
[23,     1] loss: 1197.751
[24,     1] loss: 1199.322
[25,     1] loss: 1180.812
[26,     1] loss: 1181.972
[27,     1] loss: 1147.767
[28,     1] loss: 1139.688
[29,     1] loss: 1174.659
[30,     1] loss: 1105.638
[31,     1] loss: 1134.655
[32,     1] loss: 1117.549
[33,     1] loss: 1132.262
[34,     1] loss: 1109.651
[35,     1] loss: 1109.933
[36,     1] loss: 1104.822
[37,     1] loss: 1159.417
[38,     1] loss: 1096.785
[39,     1] loss: 1070.191
[40,     1] loss: 1074.075
[41,     1] loss: 1094.347
[42,     1] loss: 1110.586
[43,     1] loss: 1066.376
[44,     1] loss: 1060.670
[45,     1] loss: 1015.241
[46,     1] loss: 1042.711
[47,     1] loss: 992.617
[48,     1] loss: 993.862
[49,     1] loss: 999.538
[50,     1] loss: 1017.718
[51,     1] loss: 1002.605
[52,     1] loss: 956.807
[53,     1] loss: 962.127
[54,     1] loss: 956.300
[55,     1] loss: 976.104
[56,     1] loss: 912.603
[57,     1] loss: 938.507
[58,     1] loss: 911.755
[59,     1] loss: 907.449
[60,     1] loss: 859.041
[61,     1] loss: 883.843
[62,     1] loss: 885.076
[63,     1] loss: 895.504
[64,     1] loss: 980.144
[65,     1] loss: 856.094
[66,     1] loss: 917.577
[67,     1] loss: 822.035
[68,     1] loss: 870.194
[69,     1] loss: 877.312
[70,     1] loss: 842.691
[71,     1] loss: 854.369
[72,     1] loss: 782.486
[73,     1] loss: 857.388
[74,     1] loss: 831.071
[75,     1] loss: 820.527
[76,     1] loss: 830.433
[77,     1] loss: 781.380
[78,     1] loss: 855.004
[79,     1] loss: 816.836
[80,     1] loss: 858.395
[81,     1] loss: 768.682
[82,     1] loss: 777.442
[83,     1] loss: 722.744
[84,     1] loss: 707.955
[85,     1] loss: 752.215
[86,     1] loss: 768.452
[87,     1] loss: 738.723
[88,     1] loss: 793.791
[89,     1] loss: 692.473
[90,     1] loss: 681.823
[91,     1] loss: 719.000
Early stopping applied (best metric=0.4034726023674011)
Finished Training
Total time taken: 14.261198282241821
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.797
[2,     1] loss: 1402.557
[3,     1] loss: 1400.020
[4,     1] loss: 1400.040
[5,     1] loss: 1396.207
[6,     1] loss: 1394.284
[7,     1] loss: 1399.810
[8,     1] loss: 1396.006
[9,     1] loss: 1390.588
[10,     1] loss: 1393.256
[11,     1] loss: 1391.853
[12,     1] loss: 1380.121
[13,     1] loss: 1373.043
[14,     1] loss: 1369.174
[15,     1] loss: 1354.611
[16,     1] loss: 1346.128
[17,     1] loss: 1325.926
[18,     1] loss: 1305.690
[19,     1] loss: 1290.019
[20,     1] loss: 1282.268
[21,     1] loss: 1253.079
[22,     1] loss: 1207.957
[23,     1] loss: 1218.321
[24,     1] loss: 1206.813
[25,     1] loss: 1197.253
[26,     1] loss: 1199.348
[27,     1] loss: 1197.699
[28,     1] loss: 1172.093
[29,     1] loss: 1176.834
[30,     1] loss: 1124.196
[31,     1] loss: 1160.977
[32,     1] loss: 1165.601
[33,     1] loss: 1162.994
[34,     1] loss: 1140.610
[35,     1] loss: 1142.083
[36,     1] loss: 1139.155
[37,     1] loss: 1117.971
[38,     1] loss: 1104.970
[39,     1] loss: 1121.640
[40,     1] loss: 1095.959
[41,     1] loss: 1127.229
[42,     1] loss: 1086.984
[43,     1] loss: 1090.086
[44,     1] loss: 1130.449
[45,     1] loss: 1058.026
[46,     1] loss: 1007.636
[47,     1] loss: 1064.661
[48,     1] loss: 1052.140
[49,     1] loss: 1069.361
[50,     1] loss: 1062.823
[51,     1] loss: 1024.284
[52,     1] loss: 1052.910
[53,     1] loss: 1018.086
[54,     1] loss: 1065.975
[55,     1] loss: 1048.504
[56,     1] loss: 1014.308
[57,     1] loss: 1013.942
[58,     1] loss: 1008.178
[59,     1] loss: 1037.970
[60,     1] loss: 993.167
[61,     1] loss: 1014.605
[62,     1] loss: 1009.698
[63,     1] loss: 952.426
[64,     1] loss: 959.708
[65,     1] loss: 944.656
[66,     1] loss: 1020.812
[67,     1] loss: 916.593
[68,     1] loss: 966.093
[69,     1] loss: 915.979
[70,     1] loss: 927.903
[71,     1] loss: 883.473
[72,     1] loss: 987.993
[73,     1] loss: 884.726
[74,     1] loss: 937.590
[75,     1] loss: 884.328
[76,     1] loss: 896.962
[77,     1] loss: 958.333
[78,     1] loss: 836.912
[79,     1] loss: 859.318
[80,     1] loss: 878.536
[81,     1] loss: 865.697
[82,     1] loss: 814.149
[83,     1] loss: 811.653
[84,     1] loss: 781.335
[85,     1] loss: 800.606
[86,     1] loss: 778.719
[87,     1] loss: 803.003
[88,     1] loss: 758.023
[89,     1] loss: 806.716
[90,     1] loss: 686.882
[91,     1] loss: 733.168
[92,     1] loss: 726.085
[93,     1] loss: 695.908
[94,     1] loss: 714.731
[95,     1] loss: 737.062
[96,     1] loss: 717.132
[97,     1] loss: 690.195
[98,     1] loss: 745.341
[99,     1] loss: 646.677
[100,     1] loss: 689.409
[101,     1] loss: 664.925
[102,     1] loss: 698.900
Early stopping applied (best metric=0.38840535283088684)
Finished Training
Total time taken: 16.54876160621643
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1394.471
[2,     1] loss: 1404.388
[3,     1] loss: 1398.053
[4,     1] loss: 1395.115
[5,     1] loss: 1393.533
[6,     1] loss: 1392.849
[7,     1] loss: 1397.232
[8,     1] loss: 1401.037
[9,     1] loss: 1393.186
[10,     1] loss: 1395.488
[11,     1] loss: 1390.614
[12,     1] loss: 1388.657
[13,     1] loss: 1386.915
[14,     1] loss: 1382.034
[15,     1] loss: 1374.790
[16,     1] loss: 1365.631
[17,     1] loss: 1355.821
[18,     1] loss: 1332.203
[19,     1] loss: 1318.942
[20,     1] loss: 1298.162
[21,     1] loss: 1273.823
[22,     1] loss: 1247.418
[23,     1] loss: 1237.023
[24,     1] loss: 1231.977
[25,     1] loss: 1226.850
[26,     1] loss: 1191.774
[27,     1] loss: 1154.493
[28,     1] loss: 1135.675
[29,     1] loss: 1117.894
[30,     1] loss: 1156.676
[31,     1] loss: 1153.318
[32,     1] loss: 1116.504
[33,     1] loss: 1137.860
[34,     1] loss: 1075.997
[35,     1] loss: 1093.912
[36,     1] loss: 1117.731
[37,     1] loss: 1101.369
[38,     1] loss: 1125.544
[39,     1] loss: 1095.571
[40,     1] loss: 1115.095
[41,     1] loss: 1090.509
[42,     1] loss: 1102.199
[43,     1] loss: 1092.759
[44,     1] loss: 1072.437
[45,     1] loss: 1059.937
[46,     1] loss: 1080.103
[47,     1] loss: 1040.561
[48,     1] loss: 1016.413
[49,     1] loss: 1076.012
[50,     1] loss: 1046.459
[51,     1] loss: 1059.074
[52,     1] loss: 1012.556
[53,     1] loss: 1041.555
[54,     1] loss: 997.933
[55,     1] loss: 982.816
[56,     1] loss: 967.519
[57,     1] loss: 983.442
[58,     1] loss: 964.615
[59,     1] loss: 1005.613
[60,     1] loss: 944.317
[61,     1] loss: 918.854
[62,     1] loss: 910.079
[63,     1] loss: 970.198
[64,     1] loss: 935.010
[65,     1] loss: 969.081
[66,     1] loss: 901.757
[67,     1] loss: 957.462
[68,     1] loss: 937.250
[69,     1] loss: 885.317
[70,     1] loss: 871.409
[71,     1] loss: 937.150
[72,     1] loss: 836.984
[73,     1] loss: 829.038
[74,     1] loss: 880.501
[75,     1] loss: 869.675
[76,     1] loss: 837.337
[77,     1] loss: 927.188
[78,     1] loss: 813.544
[79,     1] loss: 841.053
[80,     1] loss: 865.523
[81,     1] loss: 803.284
[82,     1] loss: 800.736
[83,     1] loss: 899.144
[84,     1] loss: 841.347
[85,     1] loss: 884.737
[86,     1] loss: 818.068
[87,     1] loss: 784.715
[88,     1] loss: 858.947
[89,     1] loss: 756.947
[90,     1] loss: 711.827
[91,     1] loss: 836.824
[92,     1] loss: 806.228
[93,     1] loss: 733.215
[94,     1] loss: 773.608
[95,     1] loss: 711.445
[96,     1] loss: 703.942
[97,     1] loss: 677.610
[98,     1] loss: 837.797
[99,     1] loss: 743.164
[100,     1] loss: 713.405
Early stopping applied (best metric=0.42550644278526306)
Finished Training
Total time taken: 16.22588086128235
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1401.257
[2,     1] loss: 1401.793
[3,     1] loss: 1397.646
[4,     1] loss: 1395.769
[5,     1] loss: 1396.319
[6,     1] loss: 1403.978
[7,     1] loss: 1394.209
[8,     1] loss: 1390.783
[9,     1] loss: 1394.227
[10,     1] loss: 1392.534
[11,     1] loss: 1384.666
[12,     1] loss: 1383.728
[13,     1] loss: 1371.657
[14,     1] loss: 1360.381
[15,     1] loss: 1344.203
[16,     1] loss: 1322.574
[17,     1] loss: 1316.384
[18,     1] loss: 1283.428
[19,     1] loss: 1268.534
[20,     1] loss: 1260.662
[21,     1] loss: 1231.987
[22,     1] loss: 1214.370
[23,     1] loss: 1190.308
[24,     1] loss: 1177.044
[25,     1] loss: 1139.157
[26,     1] loss: 1121.373
[27,     1] loss: 1139.780
[28,     1] loss: 1162.043
[29,     1] loss: 1166.733
[30,     1] loss: 1149.296
[31,     1] loss: 1121.134
[32,     1] loss: 1072.850
[33,     1] loss: 1101.554
[34,     1] loss: 1157.347
[35,     1] loss: 1060.350
[36,     1] loss: 1101.145
[37,     1] loss: 1102.485
[38,     1] loss: 1136.534
[39,     1] loss: 1093.488
[40,     1] loss: 1050.801
[41,     1] loss: 1074.336
[42,     1] loss: 1112.358
[43,     1] loss: 1057.327
[44,     1] loss: 1052.927
[45,     1] loss: 1051.329
[46,     1] loss: 1017.305
[47,     1] loss: 994.947
[48,     1] loss: 1031.948
[49,     1] loss: 995.907
[50,     1] loss: 1041.429
[51,     1] loss: 1036.063
[52,     1] loss: 1017.142
[53,     1] loss: 1009.277
[54,     1] loss: 944.798
[55,     1] loss: 982.242
[56,     1] loss: 965.950
[57,     1] loss: 985.456
[58,     1] loss: 962.942
[59,     1] loss: 935.936
[60,     1] loss: 954.567
[61,     1] loss: 980.439
[62,     1] loss: 928.268
[63,     1] loss: 913.595
[64,     1] loss: 930.801
[65,     1] loss: 890.114
[66,     1] loss: 866.155
[67,     1] loss: 855.160
[68,     1] loss: 894.772
[69,     1] loss: 885.940
[70,     1] loss: 841.124
[71,     1] loss: 846.549
[72,     1] loss: 885.482
[73,     1] loss: 864.083
[74,     1] loss: 820.286
[75,     1] loss: 837.893
[76,     1] loss: 892.866
[77,     1] loss: 770.516
[78,     1] loss: 806.767
[79,     1] loss: 791.043
[80,     1] loss: 822.279
[81,     1] loss: 762.747
[82,     1] loss: 795.040
Early stopping applied (best metric=0.39776429533958435)
Finished Training
Total time taken: 13.014074802398682
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.320
[2,     1] loss: 1405.489
[3,     1] loss: 1395.890
[4,     1] loss: 1399.984
[5,     1] loss: 1398.374
[6,     1] loss: 1399.595
[7,     1] loss: 1397.370
[8,     1] loss: 1398.191
[9,     1] loss: 1395.289
[10,     1] loss: 1394.178
[11,     1] loss: 1396.948
[12,     1] loss: 1391.909
[13,     1] loss: 1392.320
[14,     1] loss: 1390.715
[15,     1] loss: 1382.661
[16,     1] loss: 1379.912
[17,     1] loss: 1365.862
[18,     1] loss: 1371.599
[19,     1] loss: 1359.864
[20,     1] loss: 1346.082
[21,     1] loss: 1331.157
[22,     1] loss: 1314.504
[23,     1] loss: 1299.831
[24,     1] loss: 1276.220
[25,     1] loss: 1260.280
[26,     1] loss: 1227.768
[27,     1] loss: 1245.401
[28,     1] loss: 1231.969
[29,     1] loss: 1210.381
[30,     1] loss: 1191.850
[31,     1] loss: 1170.602
[32,     1] loss: 1193.859
[33,     1] loss: 1143.313
[34,     1] loss: 1172.215
[35,     1] loss: 1157.802
[36,     1] loss: 1132.497
[37,     1] loss: 1166.489
[38,     1] loss: 1084.670
[39,     1] loss: 1084.755
[40,     1] loss: 1106.256
[41,     1] loss: 1125.011
[42,     1] loss: 1108.422
[43,     1] loss: 1110.853
[44,     1] loss: 1107.827
[45,     1] loss: 1090.199
[46,     1] loss: 1087.851
[47,     1] loss: 1051.604
[48,     1] loss: 1066.553
[49,     1] loss: 1026.339
[50,     1] loss: 1045.763
[51,     1] loss: 1009.874
[52,     1] loss: 1052.236
[53,     1] loss: 977.278
[54,     1] loss: 1015.846
[55,     1] loss: 998.929
[56,     1] loss: 1008.192
[57,     1] loss: 946.094
[58,     1] loss: 910.083
[59,     1] loss: 921.613
[60,     1] loss: 896.404
[61,     1] loss: 938.466
[62,     1] loss: 979.745
[63,     1] loss: 921.472
[64,     1] loss: 929.280
[65,     1] loss: 921.030
[66,     1] loss: 890.615
[67,     1] loss: 912.599
[68,     1] loss: 811.156
[69,     1] loss: 882.805
[70,     1] loss: 852.792
[71,     1] loss: 870.183
[72,     1] loss: 781.606
[73,     1] loss: 787.161
[74,     1] loss: 801.680
[75,     1] loss: 809.279
[76,     1] loss: 852.736
[77,     1] loss: 795.125
[78,     1] loss: 893.986
[79,     1] loss: 822.507
[80,     1] loss: 799.080
[81,     1] loss: 776.389
[82,     1] loss: 808.672
[83,     1] loss: 774.374
[84,     1] loss: 763.418
[85,     1] loss: 748.366
[86,     1] loss: 750.371
[87,     1] loss: 723.072
[88,     1] loss: 736.422
[89,     1] loss: 812.361
[90,     1] loss: 709.893
[91,     1] loss: 725.646
[92,     1] loss: 709.078
[93,     1] loss: 652.755
[94,     1] loss: 702.260
[95,     1] loss: 714.823
[96,     1] loss: 687.576
[97,     1] loss: 657.561
[98,     1] loss: 798.616
[99,     1] loss: 666.014
[100,     1] loss: 720.074
[101,     1] loss: 627.362
[102,     1] loss: 700.540
[103,     1] loss: 652.949
[104,     1] loss: 653.529
[105,     1] loss: 619.376
[106,     1] loss: 671.300
[107,     1] loss: 623.795
[108,     1] loss: 601.418
Early stopping applied (best metric=0.40086501836776733)
Finished Training
Total time taken: 16.935692071914673
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1405.680
[2,     1] loss: 1402.316
[3,     1] loss: 1394.773
[4,     1] loss: 1399.744
[5,     1] loss: 1398.269
[6,     1] loss: 1396.229
[7,     1] loss: 1395.177
[8,     1] loss: 1393.311
[9,     1] loss: 1391.301
[10,     1] loss: 1390.881
[11,     1] loss: 1386.303
[12,     1] loss: 1379.527
[13,     1] loss: 1370.458
[14,     1] loss: 1367.656
[15,     1] loss: 1348.726
[16,     1] loss: 1341.256
[17,     1] loss: 1325.197
[18,     1] loss: 1314.422
[19,     1] loss: 1292.357
[20,     1] loss: 1277.528
[21,     1] loss: 1266.545
[22,     1] loss: 1222.787
[23,     1] loss: 1242.426
[24,     1] loss: 1240.875
[25,     1] loss: 1187.043
[26,     1] loss: 1184.515
[27,     1] loss: 1193.689
[28,     1] loss: 1173.399
[29,     1] loss: 1171.744
[30,     1] loss: 1159.140
[31,     1] loss: 1145.690
[32,     1] loss: 1118.732
[33,     1] loss: 1162.581
[34,     1] loss: 1107.390
[35,     1] loss: 1100.891
[36,     1] loss: 1130.618
[37,     1] loss: 1108.834
[38,     1] loss: 1127.192
[39,     1] loss: 1131.080
[40,     1] loss: 1117.261
[41,     1] loss: 1111.138
[42,     1] loss: 1098.900
[43,     1] loss: 1080.694
[44,     1] loss: 1117.332
[45,     1] loss: 1075.220
[46,     1] loss: 1047.561
[47,     1] loss: 1113.242
[48,     1] loss: 1074.529
[49,     1] loss: 1082.719
[50,     1] loss: 1053.303
[51,     1] loss: 1043.741
[52,     1] loss: 1045.548
[53,     1] loss: 1011.763
[54,     1] loss: 1088.913
[55,     1] loss: 1009.339
[56,     1] loss: 996.557
[57,     1] loss: 1049.840
[58,     1] loss: 967.760
[59,     1] loss: 1045.465
[60,     1] loss: 1044.875
[61,     1] loss: 1020.293
[62,     1] loss: 1036.627
[63,     1] loss: 992.686
[64,     1] loss: 993.561
[65,     1] loss: 933.160
[66,     1] loss: 981.735
[67,     1] loss: 997.611
[68,     1] loss: 933.015
[69,     1] loss: 952.572
[70,     1] loss: 898.550
[71,     1] loss: 913.177
[72,     1] loss: 923.796
[73,     1] loss: 916.364
[74,     1] loss: 900.012
[75,     1] loss: 997.341
[76,     1] loss: 882.479
[77,     1] loss: 871.830
[78,     1] loss: 852.635
[79,     1] loss: 912.012
[80,     1] loss: 810.352
[81,     1] loss: 853.113
[82,     1] loss: 838.833
[83,     1] loss: 860.933
[84,     1] loss: 828.177
[85,     1] loss: 841.647
[86,     1] loss: 782.946
[87,     1] loss: 762.462
[88,     1] loss: 795.717
[89,     1] loss: 726.911
[90,     1] loss: 837.049
[91,     1] loss: 763.536
[92,     1] loss: 755.727
[93,     1] loss: 765.752
[94,     1] loss: 721.248
[95,     1] loss: 755.332
[96,     1] loss: 719.678
[97,     1] loss: 713.573
[98,     1] loss: 721.821
[99,     1] loss: 751.296
[100,     1] loss: 804.997
[101,     1] loss: 737.252
[102,     1] loss: 743.515
[103,     1] loss: 681.037
[104,     1] loss: 708.639
[105,     1] loss: 703.813
[106,     1] loss: 622.562
[107,     1] loss: 711.603
[108,     1] loss: 677.856
[109,     1] loss: 678.466
[110,     1] loss: 643.196
[111,     1] loss: 684.281
[112,     1] loss: 654.488
[113,     1] loss: 664.877
[114,     1] loss: 680.016
[115,     1] loss: 589.222
[116,     1] loss: 667.753
[117,     1] loss: 581.100
[118,     1] loss: 624.422
[119,     1] loss: 609.750
[120,     1] loss: 618.971
[121,     1] loss: 605.113
[122,     1] loss: 595.119
[123,     1] loss: 599.486
[124,     1] loss: 526.537
[125,     1] loss: 581.389
[126,     1] loss: 565.047
[127,     1] loss: 602.949
[128,     1] loss: 562.838
[129,     1] loss: 655.526
[130,     1] loss: 564.084
[131,     1] loss: 516.809
[132,     1] loss: 582.509
Early stopping applied (best metric=0.3262026309967041)
Finished Training
Total time taken: 23.889735221862793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.300
[2,     1] loss: 1399.704
[3,     1] loss: 1398.876
[4,     1] loss: 1396.942
[5,     1] loss: 1393.842
[6,     1] loss: 1393.730
[7,     1] loss: 1399.117
[8,     1] loss: 1394.428
[9,     1] loss: 1390.615
[10,     1] loss: 1386.297
[11,     1] loss: 1383.957
[12,     1] loss: 1381.044
[13,     1] loss: 1371.041
[14,     1] loss: 1363.026
[15,     1] loss: 1357.708
[16,     1] loss: 1339.427
[17,     1] loss: 1319.330
[18,     1] loss: 1316.195
[19,     1] loss: 1290.775
[20,     1] loss: 1280.040
[21,     1] loss: 1259.158
[22,     1] loss: 1262.770
[23,     1] loss: 1231.558
[24,     1] loss: 1228.047
[25,     1] loss: 1203.401
[26,     1] loss: 1175.640
[27,     1] loss: 1206.858
[28,     1] loss: 1203.085
[29,     1] loss: 1195.791
[30,     1] loss: 1153.875
[31,     1] loss: 1146.079
[32,     1] loss: 1166.372
[33,     1] loss: 1140.857
[34,     1] loss: 1159.261
[35,     1] loss: 1142.621
[36,     1] loss: 1149.344
[37,     1] loss: 1138.617
[38,     1] loss: 1088.772
[39,     1] loss: 1150.798
[40,     1] loss: 1141.634
[41,     1] loss: 1164.182
[42,     1] loss: 1139.638
[43,     1] loss: 1088.627
[44,     1] loss: 1150.535
[45,     1] loss: 1114.949
[46,     1] loss: 1101.092
[47,     1] loss: 1105.505
[48,     1] loss: 1084.968
[49,     1] loss: 1054.210
[50,     1] loss: 1050.710
[51,     1] loss: 1042.808
[52,     1] loss: 1057.648
[53,     1] loss: 1032.006
[54,     1] loss: 1027.292
[55,     1] loss: 1052.151
[56,     1] loss: 1013.379
[57,     1] loss: 1012.565
[58,     1] loss: 999.949
[59,     1] loss: 1000.244
[60,     1] loss: 973.325
[61,     1] loss: 897.668
[62,     1] loss: 1001.419
[63,     1] loss: 968.285
[64,     1] loss: 954.614
[65,     1] loss: 906.008
[66,     1] loss: 934.654
[67,     1] loss: 874.785
[68,     1] loss: 902.364
[69,     1] loss: 891.093
[70,     1] loss: 847.141
[71,     1] loss: 884.389
[72,     1] loss: 836.599
[73,     1] loss: 805.859
[74,     1] loss: 865.312
[75,     1] loss: 881.043
[76,     1] loss: 851.707
[77,     1] loss: 781.335
[78,     1] loss: 816.748
[79,     1] loss: 823.548
[80,     1] loss: 798.157
[81,     1] loss: 830.488
[82,     1] loss: 741.518
[83,     1] loss: 815.974
[84,     1] loss: 779.349
[85,     1] loss: 762.584
[86,     1] loss: 697.362
[87,     1] loss: 703.594
[88,     1] loss: 709.897
[89,     1] loss: 700.831
[90,     1] loss: 720.712
[91,     1] loss: 755.049
[92,     1] loss: 734.473
[93,     1] loss: 726.457
[94,     1] loss: 643.188
[95,     1] loss: 697.443
[96,     1] loss: 691.040
[97,     1] loss: 702.721
[98,     1] loss: 636.218
[99,     1] loss: 659.570
[100,     1] loss: 721.095
[101,     1] loss: 604.097
[102,     1] loss: 649.652
[103,     1] loss: 617.412
[104,     1] loss: 661.007
[105,     1] loss: 594.040
[106,     1] loss: 592.179
[107,     1] loss: 578.289
[108,     1] loss: 554.735
[109,     1] loss: 593.141
[110,     1] loss: 578.421
Early stopping applied (best metric=0.3579147458076477)
Finished Training
Total time taken: 18.73384976387024
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1404.142
[2,     1] loss: 1395.424
[3,     1] loss: 1398.525
[4,     1] loss: 1392.981
[5,     1] loss: 1394.453
[6,     1] loss: 1393.632
[7,     1] loss: 1393.650
[8,     1] loss: 1392.994
[9,     1] loss: 1391.123
[10,     1] loss: 1386.634
[11,     1] loss: 1377.858
[12,     1] loss: 1367.672
[13,     1] loss: 1351.667
[14,     1] loss: 1344.270
[15,     1] loss: 1325.056
[16,     1] loss: 1310.578
[17,     1] loss: 1291.365
[18,     1] loss: 1275.312
[19,     1] loss: 1239.875
[20,     1] loss: 1206.893
[21,     1] loss: 1252.192
[22,     1] loss: 1177.054
[23,     1] loss: 1174.040
[24,     1] loss: 1212.478
[25,     1] loss: 1183.943
[26,     1] loss: 1154.591
[27,     1] loss: 1181.340
[28,     1] loss: 1172.381
[29,     1] loss: 1188.461
[30,     1] loss: 1106.659
[31,     1] loss: 1134.049
[32,     1] loss: 1171.904
[33,     1] loss: 1163.702
[34,     1] loss: 1153.653
[35,     1] loss: 1117.280
[36,     1] loss: 1150.640
[37,     1] loss: 1149.848
[38,     1] loss: 1101.847
[39,     1] loss: 1093.768
[40,     1] loss: 1132.658
[41,     1] loss: 1103.030
[42,     1] loss: 1169.760
[43,     1] loss: 1121.222
[44,     1] loss: 1107.069
[45,     1] loss: 1091.374
[46,     1] loss: 1071.618
[47,     1] loss: 1090.420
[48,     1] loss: 1078.892
[49,     1] loss: 1058.922
[50,     1] loss: 1093.492
[51,     1] loss: 1062.449
[52,     1] loss: 1090.273
[53,     1] loss: 998.955
[54,     1] loss: 1020.519
[55,     1] loss: 1057.425
[56,     1] loss: 1055.858
[57,     1] loss: 970.088
[58,     1] loss: 1002.545
[59,     1] loss: 1054.990
[60,     1] loss: 986.726
[61,     1] loss: 976.098
[62,     1] loss: 953.611
[63,     1] loss: 1086.036
[64,     1] loss: 963.137
[65,     1] loss: 963.811
[66,     1] loss: 961.403
[67,     1] loss: 933.067
[68,     1] loss: 910.924
[69,     1] loss: 980.495
[70,     1] loss: 899.534
[71,     1] loss: 917.231
[72,     1] loss: 988.507
[73,     1] loss: 900.653
[74,     1] loss: 917.168
[75,     1] loss: 905.910
[76,     1] loss: 919.000
[77,     1] loss: 903.577
[78,     1] loss: 885.360
[79,     1] loss: 937.665
[80,     1] loss: 812.313
[81,     1] loss: 925.578
[82,     1] loss: 859.013
[83,     1] loss: 973.244
[84,     1] loss: 858.565
[85,     1] loss: 924.374
[86,     1] loss: 843.374
[87,     1] loss: 822.931
[88,     1] loss: 868.190
[89,     1] loss: 799.369
[90,     1] loss: 845.724
[91,     1] loss: 817.565
[92,     1] loss: 868.926
[93,     1] loss: 774.083
[94,     1] loss: 734.248
[95,     1] loss: 906.190
[96,     1] loss: 762.679
[97,     1] loss: 829.491
[98,     1] loss: 803.123
[99,     1] loss: 788.463
[100,     1] loss: 794.361
[101,     1] loss: 729.432
[102,     1] loss: 739.966
[103,     1] loss: 736.704
[104,     1] loss: 759.289
[105,     1] loss: 706.921
[106,     1] loss: 732.166
[107,     1] loss: 694.296
[108,     1] loss: 667.299
[109,     1] loss: 660.302
[110,     1] loss: 716.801
[111,     1] loss: 685.215
[112,     1] loss: 660.244
[113,     1] loss: 652.383
[114,     1] loss: 649.089
[115,     1] loss: 712.448
[116,     1] loss: 671.076
[117,     1] loss: 579.669
[118,     1] loss: 576.916
[119,     1] loss: 667.305
[120,     1] loss: 600.308
[121,     1] loss: 654.543
[122,     1] loss: 581.958
[123,     1] loss: 609.953
[124,     1] loss: 639.028
[125,     1] loss: 558.232
[126,     1] loss: 683.504
[127,     1] loss: 585.316
[128,     1] loss: 560.083
[129,     1] loss: 545.345
[130,     1] loss: 640.568
[131,     1] loss: 546.227
[132,     1] loss: 567.387
[133,     1] loss: 539.894
[134,     1] loss: 577.187
[135,     1] loss: 582.237
[136,     1] loss: 610.482
[137,     1] loss: 532.007
[138,     1] loss: 522.914
[139,     1] loss: 554.259
[140,     1] loss: 520.314
[141,     1] loss: 519.894
[142,     1] loss: 553.330
[143,     1] loss: 477.629
[144,     1] loss: 528.824
[145,     1] loss: 612.385
[146,     1] loss: 526.494
[147,     1] loss: 586.474
[148,     1] loss: 555.947
[149,     1] loss: 531.956
[150,     1] loss: 509.609
[151,     1] loss: 553.068
[152,     1] loss: 494.848
[153,     1] loss: 520.278
[154,     1] loss: 470.664
[155,     1] loss: 515.517
[156,     1] loss: 485.139
[157,     1] loss: 487.732
Early stopping applied (best metric=0.28112345933914185)
Finished Training
Total time taken: 27.260271310806274
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1402.034
[2,     1] loss: 1398.995
[3,     1] loss: 1398.366
[4,     1] loss: 1401.265
[5,     1] loss: 1401.577
[6,     1] loss: 1397.878
[7,     1] loss: 1395.256
[8,     1] loss: 1396.387
[9,     1] loss: 1397.531
[10,     1] loss: 1393.834
[11,     1] loss: 1398.912
[12,     1] loss: 1394.768
[13,     1] loss: 1392.617
[14,     1] loss: 1391.711
[15,     1] loss: 1385.337
[16,     1] loss: 1382.312
[17,     1] loss: 1374.587
[18,     1] loss: 1366.283
[19,     1] loss: 1357.482
[20,     1] loss: 1332.788
[21,     1] loss: 1328.712
[22,     1] loss: 1309.333
[23,     1] loss: 1292.201
[24,     1] loss: 1277.816
[25,     1] loss: 1274.051
[26,     1] loss: 1222.744
[27,     1] loss: 1193.878
[28,     1] loss: 1231.177
[29,     1] loss: 1212.773
[30,     1] loss: 1155.434
[31,     1] loss: 1173.193
[32,     1] loss: 1154.245
[33,     1] loss: 1208.740
[34,     1] loss: 1112.382
[35,     1] loss: 1189.611
[36,     1] loss: 1178.029
[37,     1] loss: 1138.149
[38,     1] loss: 1127.617
[39,     1] loss: 1155.981
[40,     1] loss: 1140.143
[41,     1] loss: 1192.187
[42,     1] loss: 1092.092
[43,     1] loss: 1140.007
[44,     1] loss: 1142.625
[45,     1] loss: 1131.330
[46,     1] loss: 1090.141
[47,     1] loss: 1093.855
[48,     1] loss: 1093.718
[49,     1] loss: 1070.398
[50,     1] loss: 1108.871
[51,     1] loss: 1059.677
[52,     1] loss: 1041.664
[53,     1] loss: 1076.836
[54,     1] loss: 1036.551
[55,     1] loss: 1066.041
[56,     1] loss: 1019.976
[57,     1] loss: 1053.309
[58,     1] loss: 969.087
[59,     1] loss: 1039.692
[60,     1] loss: 974.701
[61,     1] loss: 989.165
[62,     1] loss: 983.676
[63,     1] loss: 995.564
[64,     1] loss: 952.578
[65,     1] loss: 977.903
[66,     1] loss: 895.807
[67,     1] loss: 953.747
[68,     1] loss: 936.402
[69,     1] loss: 922.743
[70,     1] loss: 927.443
[71,     1] loss: 885.702
[72,     1] loss: 940.367
[73,     1] loss: 900.808
[74,     1] loss: 822.456
[75,     1] loss: 872.402
[76,     1] loss: 856.648
[77,     1] loss: 838.263
[78,     1] loss: 867.816
[79,     1] loss: 811.898
[80,     1] loss: 833.341
[81,     1] loss: 847.813
[82,     1] loss: 791.529
[83,     1] loss: 804.987
[84,     1] loss: 818.732
[85,     1] loss: 856.017
[86,     1] loss: 820.367
[87,     1] loss: 798.906
[88,     1] loss: 810.486
[89,     1] loss: 734.278
[90,     1] loss: 759.117
[91,     1] loss: 757.709
[92,     1] loss: 783.396
[93,     1] loss: 709.065
[94,     1] loss: 725.499
[95,     1] loss: 716.218
[96,     1] loss: 735.224
[97,     1] loss: 680.376
[98,     1] loss: 726.840
[99,     1] loss: 686.557
[100,     1] loss: 659.825
[101,     1] loss: 631.226
[102,     1] loss: 672.701
[103,     1] loss: 663.226
[104,     1] loss: 659.368
[105,     1] loss: 680.587
[106,     1] loss: 584.640
[107,     1] loss: 647.601
[108,     1] loss: 619.680
[109,     1] loss: 600.548
[110,     1] loss: 597.990
[111,     1] loss: 634.925
[112,     1] loss: 591.077
[113,     1] loss: 583.351
[114,     1] loss: 569.862
[115,     1] loss: 657.102
[116,     1] loss: 580.532
[117,     1] loss: 595.556
[118,     1] loss: 583.911
Early stopping applied (best metric=0.3600872755050659)
Finished Training
Total time taken: 17.990092277526855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1398.612
[2,     1] loss: 1400.605
[3,     1] loss: 1399.372
[4,     1] loss: 1395.704
[5,     1] loss: 1395.334
[6,     1] loss: 1395.676
[7,     1] loss: 1393.609
[8,     1] loss: 1393.003
[9,     1] loss: 1394.009
[10,     1] loss: 1390.971
[11,     1] loss: 1382.655
[12,     1] loss: 1386.129
[13,     1] loss: 1381.205
[14,     1] loss: 1375.552
[15,     1] loss: 1359.273
[16,     1] loss: 1345.542
[17,     1] loss: 1330.827
[18,     1] loss: 1312.591
[19,     1] loss: 1289.160
[20,     1] loss: 1260.739
[21,     1] loss: 1248.177
[22,     1] loss: 1220.598
[23,     1] loss: 1201.771
[24,     1] loss: 1186.681
[25,     1] loss: 1167.183
[26,     1] loss: 1155.773
[27,     1] loss: 1141.394
[28,     1] loss: 1150.642
[29,     1] loss: 1160.646
[30,     1] loss: 1137.686
[31,     1] loss: 1114.822
[32,     1] loss: 1096.841
[33,     1] loss: 1116.206
[34,     1] loss: 1136.837
[35,     1] loss: 1133.300
[36,     1] loss: 1074.368
[37,     1] loss: 1110.083
[38,     1] loss: 1059.393
[39,     1] loss: 1027.176
[40,     1] loss: 1055.577
[41,     1] loss: 1058.193
[42,     1] loss: 1050.253
[43,     1] loss: 1053.361
[44,     1] loss: 1060.470
[45,     1] loss: 1024.574
[46,     1] loss: 1009.602
[47,     1] loss: 1041.498
[48,     1] loss: 996.558
[49,     1] loss: 1007.437
[50,     1] loss: 977.540
[51,     1] loss: 994.641
[52,     1] loss: 952.382
[53,     1] loss: 993.205
[54,     1] loss: 950.056
[55,     1] loss: 946.787
[56,     1] loss: 947.574
[57,     1] loss: 918.084
[58,     1] loss: 893.085
[59,     1] loss: 954.893
[60,     1] loss: 891.002
[61,     1] loss: 891.635
[62,     1] loss: 903.444
[63,     1] loss: 913.642
[64,     1] loss: 904.746
[65,     1] loss: 926.736
[66,     1] loss: 841.370
[67,     1] loss: 872.949
[68,     1] loss: 860.110
[69,     1] loss: 821.484
[70,     1] loss: 790.091
[71,     1] loss: 790.749
[72,     1] loss: 821.576
[73,     1] loss: 792.877
[74,     1] loss: 795.548
[75,     1] loss: 798.914
[76,     1] loss: 776.782
[77,     1] loss: 755.966
[78,     1] loss: 776.887
[79,     1] loss: 704.270
[80,     1] loss: 710.022
[81,     1] loss: 779.184
[82,     1] loss: 708.396
[83,     1] loss: 723.958
Early stopping applied (best metric=0.43610644340515137)
Finished Training
Total time taken: 12.939841985702515
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1401.206
[2,     1] loss: 1401.807
[3,     1] loss: 1398.801
[4,     1] loss: 1395.918
[5,     1] loss: 1397.218
[6,     1] loss: 1394.564
[7,     1] loss: 1392.866
[8,     1] loss: 1392.829
[9,     1] loss: 1381.502
[10,     1] loss: 1375.962
[11,     1] loss: 1364.706
[12,     1] loss: 1350.355
[13,     1] loss: 1344.359
[14,     1] loss: 1327.354
[15,     1] loss: 1313.435
[16,     1] loss: 1288.953
[17,     1] loss: 1257.814
[18,     1] loss: 1254.717
[19,     1] loss: 1239.540
[20,     1] loss: 1222.975
[21,     1] loss: 1231.060
[22,     1] loss: 1207.289
[23,     1] loss: 1219.669
[24,     1] loss: 1156.668
[25,     1] loss: 1153.893
[26,     1] loss: 1193.083
[27,     1] loss: 1154.948
[28,     1] loss: 1130.339
[29,     1] loss: 1115.668
[30,     1] loss: 1167.317
[31,     1] loss: 1127.838
[32,     1] loss: 1133.219
[33,     1] loss: 1115.601
[34,     1] loss: 1127.943
[35,     1] loss: 1121.732
[36,     1] loss: 1076.745
[37,     1] loss: 1112.509
[38,     1] loss: 1068.887
[39,     1] loss: 1100.472
[40,     1] loss: 1076.025
[41,     1] loss: 1093.518
[42,     1] loss: 1070.307
[43,     1] loss: 1095.639
[44,     1] loss: 1064.955
[45,     1] loss: 1059.880
[46,     1] loss: 1070.263
[47,     1] loss: 1071.297
[48,     1] loss: 1046.569
[49,     1] loss: 1034.340
[50,     1] loss: 1023.435
[51,     1] loss: 1007.211
[52,     1] loss: 1010.393
[53,     1] loss: 988.510
[54,     1] loss: 942.528
[55,     1] loss: 994.250
[56,     1] loss: 993.668
[57,     1] loss: 953.619
[58,     1] loss: 969.043
[59,     1] loss: 947.431
[60,     1] loss: 962.688
[61,     1] loss: 909.200
[62,     1] loss: 967.680
[63,     1] loss: 943.650
[64,     1] loss: 877.392
[65,     1] loss: 928.871
[66,     1] loss: 934.293
[67,     1] loss: 892.514
[68,     1] loss: 905.105
[69,     1] loss: 914.244
[70,     1] loss: 900.060
[71,     1] loss: 913.331
[72,     1] loss: 845.955
[73,     1] loss: 821.921
[74,     1] loss: 871.294
[75,     1] loss: 889.798
[76,     1] loss: 894.855
[77,     1] loss: 888.401
[78,     1] loss: 847.908
[79,     1] loss: 818.210
[80,     1] loss: 748.097
[81,     1] loss: 793.901
[82,     1] loss: 787.693
[83,     1] loss: 772.789
[84,     1] loss: 805.900
[85,     1] loss: 796.136
[86,     1] loss: 800.014
[87,     1] loss: 795.877
[88,     1] loss: 738.845
[89,     1] loss: 709.022
[90,     1] loss: 755.727
[91,     1] loss: 754.692
[92,     1] loss: 654.866
Early stopping applied (best metric=0.35207971930503845)
Finished Training
Total time taken: 13.951018333435059
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1403.907
[2,     1] loss: 1396.729
[3,     1] loss: 1400.221
[4,     1] loss: 1398.527
[5,     1] loss: 1397.607
[6,     1] loss: 1398.824
[7,     1] loss: 1395.766
[8,     1] loss: 1393.511
[9,     1] loss: 1394.908
[10,     1] loss: 1395.415
[11,     1] loss: 1390.954
[12,     1] loss: 1387.656
[13,     1] loss: 1382.031
[14,     1] loss: 1377.872
[15,     1] loss: 1367.356
[16,     1] loss: 1361.278
[17,     1] loss: 1344.695
[18,     1] loss: 1331.531
[19,     1] loss: 1313.883
[20,     1] loss: 1300.916
[21,     1] loss: 1278.386
[22,     1] loss: 1278.460
[23,     1] loss: 1252.547
[24,     1] loss: 1231.661
[25,     1] loss: 1196.713
[26,     1] loss: 1210.054
[27,     1] loss: 1198.056
[28,     1] loss: 1145.429
[29,     1] loss: 1161.106
[30,     1] loss: 1161.345
[31,     1] loss: 1213.206
[32,     1] loss: 1171.644
[33,     1] loss: 1171.286
[34,     1] loss: 1128.407
[35,     1] loss: 1114.015
[36,     1] loss: 1132.532
[37,     1] loss: 1136.471
[38,     1] loss: 1163.593
[39,     1] loss: 1128.479
[40,     1] loss: 1093.898
[41,     1] loss: 1075.591
[42,     1] loss: 1133.619
[43,     1] loss: 1093.243
[44,     1] loss: 1116.324
[45,     1] loss: 1077.906
[46,     1] loss: 1055.045
[47,     1] loss: 1017.300
[48,     1] loss: 1059.179
[49,     1] loss: 1069.149
[50,     1] loss: 1051.964
[51,     1] loss: 1037.026
[52,     1] loss: 999.466
[53,     1] loss: 1033.930
[54,     1] loss: 963.488
[55,     1] loss: 974.152
[56,     1] loss: 1033.882
[57,     1] loss: 1011.266
[58,     1] loss: 1059.181
[59,     1] loss: 994.685
[60,     1] loss: 973.414
[61,     1] loss: 989.810
[62,     1] loss: 1007.833
[63,     1] loss: 924.852
[64,     1] loss: 907.168
[65,     1] loss: 931.263
[66,     1] loss: 960.727
[67,     1] loss: 898.231
[68,     1] loss: 856.282
[69,     1] loss: 878.936
[70,     1] loss: 898.425
[71,     1] loss: 857.316
[72,     1] loss: 840.385
[73,     1] loss: 810.081
[74,     1] loss: 822.249
[75,     1] loss: 835.265
[76,     1] loss: 868.527
[77,     1] loss: 767.223
[78,     1] loss: 796.549
[79,     1] loss: 803.957
[80,     1] loss: 803.210
[81,     1] loss: 790.501
[82,     1] loss: 774.006
[83,     1] loss: 791.332
[84,     1] loss: 786.095
[85,     1] loss: 800.093
[86,     1] loss: 765.845
[87,     1] loss: 752.191
[88,     1] loss: 730.283
[89,     1] loss: 674.406
[90,     1] loss: 758.220
[91,     1] loss: 724.240
[92,     1] loss: 710.732
[93,     1] loss: 622.184
[94,     1] loss: 696.164
[95,     1] loss: 666.119
[96,     1] loss: 729.723
[97,     1] loss: 643.154
[98,     1] loss: 676.163
[99,     1] loss: 660.626
[100,     1] loss: 685.456
[101,     1] loss: 621.505
Early stopping applied (best metric=0.3439146876335144)
Finished Training
Total time taken: 15.227020502090454
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1411.827
[2,     1] loss: 1404.734
[3,     1] loss: 1395.923
[4,     1] loss: 1396.773
[5,     1] loss: 1395.457
[6,     1] loss: 1398.097
[7,     1] loss: 1397.796
[8,     1] loss: 1398.576
[9,     1] loss: 1396.812
[10,     1] loss: 1393.727
[11,     1] loss: 1393.039
[12,     1] loss: 1394.537
[13,     1] loss: 1391.733
[14,     1] loss: 1387.591
[15,     1] loss: 1385.533
[16,     1] loss: 1382.635
[17,     1] loss: 1380.162
[18,     1] loss: 1368.460
[19,     1] loss: 1363.063
[20,     1] loss: 1362.467
[21,     1] loss: 1343.826
[22,     1] loss: 1317.576
[23,     1] loss: 1314.668
[24,     1] loss: 1289.360
[25,     1] loss: 1283.076
[26,     1] loss: 1268.953
[27,     1] loss: 1243.759
[28,     1] loss: 1239.960
[29,     1] loss: 1218.771
[30,     1] loss: 1229.663
[31,     1] loss: 1208.121
[32,     1] loss: 1154.301
[33,     1] loss: 1200.606
[34,     1] loss: 1164.260
[35,     1] loss: 1165.363
[36,     1] loss: 1168.547
[37,     1] loss: 1202.736
[38,     1] loss: 1204.248
[39,     1] loss: 1144.028
[40,     1] loss: 1116.872
[41,     1] loss: 1128.776
[42,     1] loss: 1144.001
[43,     1] loss: 1136.906
[44,     1] loss: 1127.181
[45,     1] loss: 1117.260
[46,     1] loss: 1132.388
[47,     1] loss: 1149.263
[48,     1] loss: 1112.301
[49,     1] loss: 1087.513
[50,     1] loss: 1125.764
[51,     1] loss: 1078.822
[52,     1] loss: 1110.708
[53,     1] loss: 1185.686
[54,     1] loss: 1042.429
[55,     1] loss: 1138.009
[56,     1] loss: 1078.693
[57,     1] loss: 1069.224
[58,     1] loss: 1048.906
[59,     1] loss: 1064.799
[60,     1] loss: 1045.501
[61,     1] loss: 1002.346
[62,     1] loss: 998.261
[63,     1] loss: 1027.333
[64,     1] loss: 955.850
[65,     1] loss: 990.879
[66,     1] loss: 971.061
[67,     1] loss: 1029.911
[68,     1] loss: 963.719
[69,     1] loss: 952.803
[70,     1] loss: 988.247
[71,     1] loss: 951.134
[72,     1] loss: 931.560
[73,     1] loss: 924.119
[74,     1] loss: 872.629
[75,     1] loss: 923.349
[76,     1] loss: 928.110
[77,     1] loss: 948.225
[78,     1] loss: 920.410
[79,     1] loss: 958.688
[80,     1] loss: 839.682
[81,     1] loss: 872.608
[82,     1] loss: 871.867
[83,     1] loss: 855.425
[84,     1] loss: 905.713
[85,     1] loss: 905.263
[86,     1] loss: 784.596
[87,     1] loss: 795.453
[88,     1] loss: 826.453
[89,     1] loss: 787.493
[90,     1] loss: 796.844
[91,     1] loss: 754.140
[92,     1] loss: 706.730
[93,     1] loss: 792.972
[94,     1] loss: 744.837
[95,     1] loss: 831.164
[96,     1] loss: 783.317
[97,     1] loss: 750.194
[98,     1] loss: 759.915
[99,     1] loss: 746.066
[100,     1] loss: 712.947
[101,     1] loss: 711.655
[102,     1] loss: 736.489
[103,     1] loss: 739.142
[104,     1] loss: 727.586
[105,     1] loss: 740.283
[106,     1] loss: 712.227
[107,     1] loss: 665.287
[108,     1] loss: 664.779
[109,     1] loss: 678.367
[110,     1] loss: 706.573
[111,     1] loss: 712.824
[112,     1] loss: 636.424
[113,     1] loss: 697.429
[114,     1] loss: 593.805
[115,     1] loss: 635.070
[116,     1] loss: 649.298
[117,     1] loss: 708.217
[118,     1] loss: 578.781
[119,     1] loss: 740.626
[120,     1] loss: 633.397
[121,     1] loss: 736.257
[122,     1] loss: 648.042
Early stopping applied (best metric=0.3016546666622162)
Finished Training
Total time taken: 18.89413356781006
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1399.903
[2,     1] loss: 1403.969
[3,     1] loss: 1400.153
[4,     1] loss: 1400.124
[5,     1] loss: 1397.919
[6,     1] loss: 1401.625
[7,     1] loss: 1398.437
[8,     1] loss: 1398.924
[9,     1] loss: 1397.065
[10,     1] loss: 1397.985
[11,     1] loss: 1396.241
[12,     1] loss: 1395.896
[13,     1] loss: 1396.580
[14,     1] loss: 1391.156
[15,     1] loss: 1388.755
[16,     1] loss: 1381.041
[17,     1] loss: 1378.885
[18,     1] loss: 1372.173
[19,     1] loss: 1351.486
[20,     1] loss: 1348.772
[21,     1] loss: 1335.442
[22,     1] loss: 1308.737
[23,     1] loss: 1292.068
[24,     1] loss: 1278.260
[25,     1] loss: 1253.239
[26,     1] loss: 1241.693
[27,     1] loss: 1232.759
[28,     1] loss: 1202.065
[29,     1] loss: 1177.617
[30,     1] loss: 1170.188
[31,     1] loss: 1164.470
[32,     1] loss: 1124.715
[33,     1] loss: 1127.511
[34,     1] loss: 1147.754
[35,     1] loss: 1128.953
[36,     1] loss: 1130.766
[37,     1] loss: 1152.330
[38,     1] loss: 1088.878
[39,     1] loss: 1063.018
[40,     1] loss: 1107.068
[41,     1] loss: 1115.229
[42,     1] loss: 1081.571
[43,     1] loss: 1090.305
[44,     1] loss: 1094.683
[45,     1] loss: 1033.840
[46,     1] loss: 1036.964
[47,     1] loss: 1042.180
[48,     1] loss: 1046.794
[49,     1] loss: 1010.062
[50,     1] loss: 999.717
[51,     1] loss: 958.432
[52,     1] loss: 976.975
[53,     1] loss: 996.099
[54,     1] loss: 957.545
[55,     1] loss: 1059.857
[56,     1] loss: 994.114
[57,     1] loss: 1071.053
[58,     1] loss: 969.092
[59,     1] loss: 994.191
[60,     1] loss: 945.918
[61,     1] loss: 963.651
[62,     1] loss: 951.990
[63,     1] loss: 971.248
[64,     1] loss: 900.825
[65,     1] loss: 875.991
[66,     1] loss: 896.707
[67,     1] loss: 911.366
[68,     1] loss: 907.233
[69,     1] loss: 890.316
[70,     1] loss: 856.959
[71,     1] loss: 882.728
[72,     1] loss: 838.988
[73,     1] loss: 836.029
[74,     1] loss: 894.180
[75,     1] loss: 815.023
[76,     1] loss: 877.987
[77,     1] loss: 855.413
[78,     1] loss: 803.075
[79,     1] loss: 797.587
[80,     1] loss: 766.881
[81,     1] loss: 756.917
[82,     1] loss: 809.447
[83,     1] loss: 789.474
[84,     1] loss: 760.426
[85,     1] loss: 780.563
[86,     1] loss: 792.988
[87,     1] loss: 784.818
[88,     1] loss: 713.562
[89,     1] loss: 746.468
[90,     1] loss: 738.661
[91,     1] loss: 674.320
[92,     1] loss: 696.153
[93,     1] loss: 673.884
[94,     1] loss: 678.034
[95,     1] loss: 766.188
[96,     1] loss: 723.185
[97,     1] loss: 746.649
[98,     1] loss: 672.982
[99,     1] loss: 758.835
[100,     1] loss: 691.635
[101,     1] loss: 727.075
[102,     1] loss: 693.303
[103,     1] loss: 736.896
[104,     1] loss: 713.507
[105,     1] loss: 733.505
[106,     1] loss: 695.739
[107,     1] loss: 639.937
[108,     1] loss: 688.516
[109,     1] loss: 659.792
[110,     1] loss: 628.047
[111,     1] loss: 675.675
[112,     1] loss: 625.916
[113,     1] loss: 611.890
[114,     1] loss: 617.515
[115,     1] loss: 656.982
[116,     1] loss: 637.658
[117,     1] loss: 601.368
[118,     1] loss: 617.533
[119,     1] loss: 562.957
[120,     1] loss: 604.795
[121,     1] loss: 577.219
Early stopping applied (best metric=0.40719103813171387)
Finished Training
Total time taken: 17.86608862876892
{'Hydroxylation-K Validation Accuracy': 0.7674054373522459, 'Hydroxylation-K Validation Sensitivity': 0.6570370370370371, 'Hydroxylation-K Validation Specificity': 0.7947368421052632, 'Hydroxylation-K Validation Precision': 0.447556735203794, 'Hydroxylation-K AUC ROC': 0.7785964912280702, 'Hydroxylation-K AUC PR': 0.5659242312913707, 'Hydroxylation-K MCC': 0.39802635087518345, 'Hydroxylation-K F1': 0.5282132521262957, 'Validation Loss (Hydroxylation-K)': 0.47053855657577515, 'Hydroxylation-P Validation Accuracy': 0.7732120535336615, 'Hydroxylation-P Validation Sensitivity': 0.7935449735449736, 'Hydroxylation-P Validation Specificity': 0.7688213876003791, 'Hydroxylation-P Validation Precision': 0.4266125489186253, 'Hydroxylation-P AUC ROC': 0.8545854897564459, 'Hydroxylation-P AUC PR': 0.6063895852035911, 'Hydroxylation-P MCC': 0.4573562446802124, 'Hydroxylation-P F1': 0.5541014698887627, 'Validation Loss (Hydroxylation-P)': 0.36907028357187904, 'Validation Loss (total)': 0.8396088441212972, 'TimeToTrain': 17.54187297821045}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0043721624466743015,
 'learning_rate_Hydroxylation-K': 0.0016273105638696167,
 'learning_rate_Hydroxylation-P': 0.009752710633550588,
 'log_base': 1.4747777305489274,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4006222517,
 'sample_weights': [2.323632789813953, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.5433540232969873,
 'weight_decay_Hydroxylation-K': 4.208228758329964,
 'weight_decay_Hydroxylation-P': 7.359025308838042}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1821.374
[2,     1] loss: 1818.947
[3,     1] loss: 1811.579
[4,     1] loss: 1815.965
[5,     1] loss: 1815.958
[6,     1] loss: 1812.156
[7,     1] loss: 1813.258
[8,     1] loss: 1816.178
[9,     1] loss: 1814.776
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00926456414855386,
 'learning_rate_Hydroxylation-K': 0.006512438992619833,
 'learning_rate_Hydroxylation-P': 0.0022039081088132914,
 'log_base': 1.195038393275012,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1002693697,
 'sample_weights': [4.2970703563807815, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.595945899889509,
 'weight_decay_Hydroxylation-K': 3.0312142420914014,
 'weight_decay_Hydroxylation-P': 7.042312029848656}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3045.684
[2,     1] loss: 3025.623
[3,     1] loss: 3054.937
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0028519073254327054,
 'learning_rate_Hydroxylation-K': 0.0022084057608050566,
 'learning_rate_Hydroxylation-P': 0.008805167237835792,
 'log_base': 2.2750352705984973,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 711068433,
 'sample_weights': [9.369508096063564, 1.1712336308924864],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.737096949580486,
 'weight_decay_Hydroxylation-K': 0.755223374234002,
 'weight_decay_Hydroxylation-P': 5.543583787695444}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1341.611
[2,     1] loss: 1335.602
[3,     1] loss: 1332.893
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004999247661373119,
 'learning_rate_Hydroxylation-K': 0.0015179523370140995,
 'learning_rate_Hydroxylation-P': 0.0009418686664313965,
 'log_base': 2.297470119949855,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3913520266,
 'sample_weights': [2.0309637145331734, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.645084396523316,
 'weight_decay_Hydroxylation-K': 9.377057149912623,
 'weight_decay_Hydroxylation-P': 4.155168061165842}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1330.963
[2,     1] loss: 1338.658
[3,     1] loss: 1329.592
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005741657156548699,
 'learning_rate_Hydroxylation-K': 0.007845144707199244,
 'learning_rate_Hydroxylation-P': 0.0029255003706417067,
 'log_base': 1.5298536270400227,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2907799529,
 'sample_weights': [2.0070040250060748, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.3191553785843775,
 'weight_decay_Hydroxylation-K': 1.2476737369275537,
 'weight_decay_Hydroxylation-P': 5.186055964935664}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1734.294
[2,     1] loss: 1744.350
[3,     1] loss: 1740.679
[4,     1] loss: 1735.565
[5,     1] loss: 1736.052
[6,     1] loss: 1739.354
[7,     1] loss: 1732.460
[8,     1] loss: 1737.818
[9,     1] loss: 1732.447
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005557476296884385,
 'learning_rate_Hydroxylation-K': 0.003827433765734273,
 'learning_rate_Hydroxylation-P': 0.008487197822817611,
 'log_base': 1.6655377411362073,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2892788608,
 'sample_weights': [3.9265118659989517, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.780919327609894,
 'weight_decay_Hydroxylation-K': 8.63755059985164,
 'weight_decay_Hydroxylation-P': 3.0434869374503104}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1603.155
[2,     1] loss: 1598.182
[3,     1] loss: 1596.900
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007989803850910835,
 'learning_rate_Hydroxylation-K': 0.008256945374780654,
 'learning_rate_Hydroxylation-P': 0.004900156567247456,
 'log_base': 2.2647868738522874,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1241050173,
 'sample_weights': [3.272468028880213, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.316220362227809,
 'weight_decay_Hydroxylation-K': 7.834308084851042,
 'weight_decay_Hydroxylation-P': 8.43381350836477}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1337.925
[2,     1] loss: 1345.037
[3,     1] loss: 1344.358
[4,     1] loss: 1335.063
[5,     1] loss: 1336.344
[6,     1] loss: 1336.171
[7,     1] loss: 1337.292
[8,     1] loss: 1340.451
[9,     1] loss: 1333.771
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005957165829010401,
 'learning_rate_Hydroxylation-K': 0.0006160264124999262,
 'learning_rate_Hydroxylation-P': 0.00866299563375148,
 'log_base': 2.426186447537214,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2085350954,
 'sample_weights': [2.0421806056010774, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.288253028155901,
 'weight_decay_Hydroxylation-K': 3.2938594852460445,
 'weight_decay_Hydroxylation-P': 5.025048432627704}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1308.692
[2,     1] loss: 1302.550
[3,     1] loss: 1307.587
[4,     1] loss: 1305.028
[5,     1] loss: 1307.586
[6,     1] loss: 1303.528
[7,     1] loss: 1304.748
[8,     1] loss: 1301.391
[9,     1] loss: 1302.042
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003885620103900079,
 'learning_rate_Hydroxylation-K': 0.0083919209272993,
 'learning_rate_Hydroxylation-P': 0.0065276074187117245,
 'log_base': 2.2921733431376827,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1251625272,
 'sample_weights': [1.8835656439239097, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8718160329592592,
 'weight_decay_Hydroxylation-K': 8.964135998531313,
 'weight_decay_Hydroxylation-P': 6.321348021097858}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.161
[2,     1] loss: 1331.316
[3,     1] loss: 1331.409
[4,     1] loss: 1331.361
[5,     1] loss: 1330.453
[6,     1] loss: 1332.387
[7,     1] loss: 1334.525
[8,     1] loss: 1329.322
[9,     1] loss: 1330.091
[10,     1] loss: 1327.203
[11,     1] loss: 1326.078
[12,     1] loss: 1321.386
[13,     1] loss: 1319.828
[14,     1] loss: 1308.167
[15,     1] loss: 1294.188
[16,     1] loss: 1280.618
[17,     1] loss: 1254.086
[18,     1] loss: 1240.875
[19,     1] loss: 1209.011
[20,     1] loss: 1192.805
[21,     1] loss: 1148.667
[22,     1] loss: 1115.668
[23,     1] loss: 1112.895
[24,     1] loss: 1152.361
[25,     1] loss: 1155.497
[26,     1] loss: 1104.808
[27,     1] loss: 1107.443
[28,     1] loss: 1123.410
[29,     1] loss: 1073.488
[30,     1] loss: 1056.569
[31,     1] loss: 1058.207
[32,     1] loss: 1069.174
[33,     1] loss: 1008.484
[34,     1] loss: 1088.869
[35,     1] loss: 997.972
[36,     1] loss: 1025.131
[37,     1] loss: 1003.151
[38,     1] loss: 978.150
[39,     1] loss: 972.494
[40,     1] loss: 992.018
[41,     1] loss: 961.785
[42,     1] loss: 917.774
[43,     1] loss: 944.548
[44,     1] loss: 923.491
[45,     1] loss: 916.202
[46,     1] loss: 941.335
[47,     1] loss: 954.494
[48,     1] loss: 906.852
[49,     1] loss: 848.404
[50,     1] loss: 862.410
[51,     1] loss: 872.086
[52,     1] loss: 849.861
[53,     1] loss: 813.549
[54,     1] loss: 848.850
[55,     1] loss: 824.788
[56,     1] loss: 803.760
[57,     1] loss: 771.073
[58,     1] loss: 836.392
[59,     1] loss: 790.553
[60,     1] loss: 742.754
[61,     1] loss: 760.433
[62,     1] loss: 769.535
[63,     1] loss: 753.482
[64,     1] loss: 724.785
[65,     1] loss: 762.156
[66,     1] loss: 811.209
[67,     1] loss: 687.985
[68,     1] loss: 680.908
[69,     1] loss: 760.534
[70,     1] loss: 708.460
[71,     1] loss: 694.329
[72,     1] loss: 646.444
[73,     1] loss: 729.888
[74,     1] loss: 675.601
[75,     1] loss: 614.916
[76,     1] loss: 700.678
[77,     1] loss: 642.151
[78,     1] loss: 680.158
[79,     1] loss: 648.261
[80,     1] loss: 583.036
[81,     1] loss: 622.555
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007626166729418039,
 'learning_rate_Hydroxylation-K': 0.009235290166148012,
 'learning_rate_Hydroxylation-P': 0.0026978105230237555,
 'log_base': 2.284031903994342,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2570260018,
 'sample_weights': [2.0125886567783424, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6002489298208522,
 'weight_decay_Hydroxylation-K': 8.883127408611017,
 'weight_decay_Hydroxylation-P': 8.88035448051569}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1335.988
[2,     1] loss: 1340.346
[3,     1] loss: 1337.003
[4,     1] loss: 1338.695
[5,     1] loss: 1335.144
[6,     1] loss: 1337.327
[7,     1] loss: 1334.669
[8,     1] loss: 1333.665
[9,     1] loss: 1336.223
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004191183859278821,
 'learning_rate_Hydroxylation-K': 0.007829174082328779,
 'learning_rate_Hydroxylation-P': 0.003219827755731505,
 'log_base': 1.8979176154470958,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3225665728,
 'sample_weights': [2.021258903836553, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9408384469533759,
 'weight_decay_Hydroxylation-K': 9.5027442540174,
 'weight_decay_Hydroxylation-P': 2.919979093933444}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1453.231
[2,     1] loss: 1466.333
[3,     1] loss: 1461.229
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007885246536490316,
 'learning_rate_Hydroxylation-K': 0.004836747135053933,
 'learning_rate_Hydroxylation-P': 0.007859756982278287,
 'log_base': 1.5723367190508415,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2135563607,
 'sample_weights': [2.6054219982720097, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.157269911830431,
 'weight_decay_Hydroxylation-K': 4.457759338732318,
 'weight_decay_Hydroxylation-P': 6.541510967472454}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1696.658
[2,     1] loss: 1704.620
[3,     1] loss: 1692.086
[4,     1] loss: 1695.542
[5,     1] loss: 1689.846
[6,     1] loss: 1686.145
[7,     1] loss: 1684.014
[8,     1] loss: 1686.506
[9,     1] loss: 1686.890
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004070968596987376,
 'learning_rate_Hydroxylation-K': 0.0029853507553988345,
 'learning_rate_Hydroxylation-P': 0.009852954523521312,
 'log_base': 2.790687853871654,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2136456875,
 'sample_weights': [3.688864602029799, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.712326421478885,
 'weight_decay_Hydroxylation-K': 0.7385814780977373,
 'weight_decay_Hydroxylation-P': 4.386953844135123}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.134
[2,     1] loss: 1251.880
[3,     1] loss: 1248.932
[4,     1] loss: 1250.669
[5,     1] loss: 1246.411
[6,     1] loss: 1240.554
[7,     1] loss: 1221.040
[8,     1] loss: 1194.190
[9,     1] loss: 1149.822
[10,     1] loss: 1101.699
[11,     1] loss: 1054.459
[12,     1] loss: 1037.265
[13,     1] loss: 1026.070
[14,     1] loss: 1004.390
[15,     1] loss: 993.686
[16,     1] loss: 960.348
[17,     1] loss: 1054.104
[18,     1] loss: 1003.523
[19,     1] loss: 1001.150
[20,     1] loss: 994.087
[21,     1] loss: 957.135
[22,     1] loss: 983.302
[23,     1] loss: 920.163
[24,     1] loss: 920.021
[25,     1] loss: 931.353
[26,     1] loss: 913.725
[27,     1] loss: 933.963
[28,     1] loss: 903.083
[29,     1] loss: 850.332
[30,     1] loss: 887.586
[31,     1] loss: 893.832
[32,     1] loss: 824.004
[33,     1] loss: 784.829
[34,     1] loss: 821.915
[35,     1] loss: 871.090
[36,     1] loss: 979.130
[37,     1] loss: 879.907
[38,     1] loss: 833.201
[39,     1] loss: 872.965
[40,     1] loss: 826.740
[41,     1] loss: 823.025
[42,     1] loss: 862.121
[43,     1] loss: 800.811
[44,     1] loss: 804.793
[45,     1] loss: 758.101
[46,     1] loss: 760.517
[47,     1] loss: 787.590
[48,     1] loss: 722.536
[49,     1] loss: 708.314
[50,     1] loss: 746.468
[51,     1] loss: 757.813
[52,     1] loss: 690.424
[53,     1] loss: 657.342
[54,     1] loss: 702.456
[55,     1] loss: 681.951
[56,     1] loss: 656.627
[57,     1] loss: 664.543
[58,     1] loss: 627.956
[59,     1] loss: 659.852
[60,     1] loss: 900.114
[61,     1] loss: 757.767
[62,     1] loss: 696.381
[63,     1] loss: 728.996
[64,     1] loss: 683.871
[65,     1] loss: 699.786
[66,     1] loss: 723.340
[67,     1] loss: 653.485
[68,     1] loss: 682.434
[69,     1] loss: 583.634
[70,     1] loss: 675.172
[71,     1] loss: 596.971
[72,     1] loss: 724.473
[73,     1] loss: 581.558
[74,     1] loss: 557.568
[75,     1] loss: 543.226
[76,     1] loss: 515.892
[77,     1] loss: 469.512
[78,     1] loss: 539.097
[79,     1] loss: 531.575
[80,     1] loss: 500.858
[81,     1] loss: 585.236
[82,     1] loss: 807.115
[83,     1] loss: 834.688
[84,     1] loss: 521.391
[85,     1] loss: 676.602
[86,     1] loss: 612.866
[87,     1] loss: 622.858
[88,     1] loss: 682.256
[89,     1] loss: 519.759
[90,     1] loss: 667.659
[91,     1] loss: 553.023
[92,     1] loss: 591.766
[93,     1] loss: 505.578
[94,     1] loss: 552.891
[95,     1] loss: 460.958
[96,     1] loss: 537.052
[97,     1] loss: 449.002
[98,     1] loss: 435.371
[99,     1] loss: 367.669
[100,     1] loss: 418.423
[101,     1] loss: 495.897
[102,     1] loss: 411.197
[103,     1] loss: 455.617
[104,     1] loss: 519.082
[105,     1] loss: 476.262
[106,     1] loss: 396.679
[107,     1] loss: 546.459
[108,     1] loss: 429.431
[109,     1] loss: 470.899
[110,     1] loss: 402.886
[111,     1] loss: 444.175
[112,     1] loss: 421.120
[113,     1] loss: 397.891
[114,     1] loss: 520.922
[115,     1] loss: 584.641
[116,     1] loss: 348.312
[117,     1] loss: 386.622
Early stopping applied (best metric=0.41085565090179443)
Finished Training
Total time taken: 18.08354091644287
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.152
[2,     1] loss: 1248.941
[3,     1] loss: 1251.456
[4,     1] loss: 1248.622
[5,     1] loss: 1249.061
[6,     1] loss: 1243.491
[7,     1] loss: 1244.816
[8,     1] loss: 1237.886
[9,     1] loss: 1212.960
[10,     1] loss: 1196.063
[11,     1] loss: 1149.408
[12,     1] loss: 1116.438
[13,     1] loss: 1094.409
[14,     1] loss: 1083.190
[15,     1] loss: 1059.702
[16,     1] loss: 1046.887
[17,     1] loss: 1037.834
[18,     1] loss: 1031.312
[19,     1] loss: 1099.758
[20,     1] loss: 993.826
[21,     1] loss: 1031.628
[22,     1] loss: 973.737
[23,     1] loss: 988.374
[24,     1] loss: 1009.262
[25,     1] loss: 969.204
[26,     1] loss: 946.034
[27,     1] loss: 913.384
[28,     1] loss: 955.964
[29,     1] loss: 966.050
[30,     1] loss: 899.852
[31,     1] loss: 983.790
[32,     1] loss: 978.500
[33,     1] loss: 903.842
[34,     1] loss: 959.461
[35,     1] loss: 930.414
[36,     1] loss: 1013.962
[37,     1] loss: 927.534
[38,     1] loss: 927.013
[39,     1] loss: 888.937
[40,     1] loss: 860.748
[41,     1] loss: 908.645
[42,     1] loss: 822.346
[43,     1] loss: 869.053
[44,     1] loss: 816.004
[45,     1] loss: 934.839
[46,     1] loss: 829.308
[47,     1] loss: 833.275
[48,     1] loss: 788.720
[49,     1] loss: 830.789
[50,     1] loss: 780.831
[51,     1] loss: 767.171
[52,     1] loss: 761.591
[53,     1] loss: 777.916
[54,     1] loss: 773.770
[55,     1] loss: 907.953
[56,     1] loss: 870.002
[57,     1] loss: 735.811
[58,     1] loss: 805.152
[59,     1] loss: 772.055
[60,     1] loss: 770.466
[61,     1] loss: 746.186
[62,     1] loss: 748.147
[63,     1] loss: 773.365
[64,     1] loss: 694.482
[65,     1] loss: 687.258
[66,     1] loss: 758.453
[67,     1] loss: 703.705
[68,     1] loss: 664.624
[69,     1] loss: 616.191
[70,     1] loss: 654.558
[71,     1] loss: 654.669
[72,     1] loss: 654.055
[73,     1] loss: 661.801
[74,     1] loss: 822.939
[75,     1] loss: 767.969
[76,     1] loss: 638.341
[77,     1] loss: 708.613
[78,     1] loss: 657.190
[79,     1] loss: 689.394
[80,     1] loss: 594.702
[81,     1] loss: 671.555
[82,     1] loss: 601.135
[83,     1] loss: 618.544
[84,     1] loss: 597.264
[85,     1] loss: 584.201
[86,     1] loss: 636.843
[87,     1] loss: 524.166
[88,     1] loss: 562.516
[89,     1] loss: 548.033
[90,     1] loss: 512.779
[91,     1] loss: 547.486
[92,     1] loss: 510.174
[93,     1] loss: 486.534
[94,     1] loss: 443.371
[95,     1] loss: 462.238
[96,     1] loss: 401.742
[97,     1] loss: 458.788
[98,     1] loss: 479.813
[99,     1] loss: 611.107
[100,     1] loss: 1003.588
[101,     1] loss: 1056.141
[102,     1] loss: 688.607
[103,     1] loss: 672.338
[104,     1] loss: 841.042
[105,     1] loss: 836.565
[106,     1] loss: 784.798
[107,     1] loss: 746.012
[108,     1] loss: 781.917
[109,     1] loss: 762.631
[110,     1] loss: 642.585
[111,     1] loss: 664.684
[112,     1] loss: 710.755
Early stopping applied (best metric=0.3551360070705414)
Finished Training
Total time taken: 17.173616409301758
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.566
[2,     1] loss: 1256.503
[3,     1] loss: 1253.195
[4,     1] loss: 1252.827
[5,     1] loss: 1250.682
[6,     1] loss: 1251.724
[7,     1] loss: 1249.351
[8,     1] loss: 1247.833
[9,     1] loss: 1250.586
[10,     1] loss: 1247.913
[11,     1] loss: 1247.019
[12,     1] loss: 1241.732
[13,     1] loss: 1237.243
[14,     1] loss: 1224.404
[15,     1] loss: 1210.604
[16,     1] loss: 1194.213
[17,     1] loss: 1163.973
[18,     1] loss: 1144.794
[19,     1] loss: 1126.693
[20,     1] loss: 1086.210
[21,     1] loss: 1086.309
[22,     1] loss: 1068.341
[23,     1] loss: 1026.107
[24,     1] loss: 1034.237
[25,     1] loss: 1067.207
[26,     1] loss: 979.527
[27,     1] loss: 1031.141
[28,     1] loss: 1004.288
[29,     1] loss: 1019.161
[30,     1] loss: 985.600
[31,     1] loss: 975.865
[32,     1] loss: 974.724
[33,     1] loss: 960.118
[34,     1] loss: 961.898
[35,     1] loss: 922.375
[36,     1] loss: 876.667
[37,     1] loss: 895.886
[38,     1] loss: 885.448
[39,     1] loss: 906.851
[40,     1] loss: 881.339
[41,     1] loss: 860.909
[42,     1] loss: 857.797
[43,     1] loss: 840.037
[44,     1] loss: 831.882
[45,     1] loss: 838.255
[46,     1] loss: 743.601
[47,     1] loss: 831.535
[48,     1] loss: 872.120
[49,     1] loss: 912.823
[50,     1] loss: 785.885
[51,     1] loss: 883.593
[52,     1] loss: 782.058
[53,     1] loss: 861.985
[54,     1] loss: 750.872
[55,     1] loss: 802.719
[56,     1] loss: 730.514
[57,     1] loss: 775.860
[58,     1] loss: 732.437
[59,     1] loss: 712.079
[60,     1] loss: 732.975
[61,     1] loss: 611.504
[62,     1] loss: 733.028
[63,     1] loss: 643.142
[64,     1] loss: 697.522
[65,     1] loss: 634.008
[66,     1] loss: 652.403
[67,     1] loss: 632.840
[68,     1] loss: 721.691
[69,     1] loss: 619.498
[70,     1] loss: 579.652
[71,     1] loss: 634.805
[72,     1] loss: 627.689
[73,     1] loss: 601.946
[74,     1] loss: 596.160
[75,     1] loss: 551.036
[76,     1] loss: 613.770
[77,     1] loss: 724.032
[78,     1] loss: 597.828
[79,     1] loss: 518.872
[80,     1] loss: 562.917
[81,     1] loss: 532.677
[82,     1] loss: 494.318
[83,     1] loss: 522.383
[84,     1] loss: 524.770
[85,     1] loss: 496.044
[86,     1] loss: 529.923
[87,     1] loss: 699.911
[88,     1] loss: 1044.525
[89,     1] loss: 475.483
[90,     1] loss: 812.493
[91,     1] loss: 559.396
[92,     1] loss: 695.661
[93,     1] loss: 727.596
[94,     1] loss: 557.373
[95,     1] loss: 571.030
[96,     1] loss: 576.321
[97,     1] loss: 583.828
[98,     1] loss: 571.527
[99,     1] loss: 504.344
[100,     1] loss: 477.835
[101,     1] loss: 469.109
[102,     1] loss: 507.511
[103,     1] loss: 459.988
[104,     1] loss: 484.585
[105,     1] loss: 447.068
[106,     1] loss: 429.259
[107,     1] loss: 481.298
[108,     1] loss: 422.794
[109,     1] loss: 409.292
[110,     1] loss: 402.743
[111,     1] loss: 388.995
[112,     1] loss: 385.595
[113,     1] loss: 388.659
[114,     1] loss: 401.707
[115,     1] loss: 371.953
[116,     1] loss: 402.123
[117,     1] loss: 385.062
Early stopping applied (best metric=0.37644025683403015)
Finished Training
Total time taken: 17.525020837783813
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.177
[2,     1] loss: 1253.874
[3,     1] loss: 1248.021
[4,     1] loss: 1252.285
[5,     1] loss: 1249.811
[6,     1] loss: 1246.559
[7,     1] loss: 1246.505
[8,     1] loss: 1234.163
[9,     1] loss: 1227.823
[10,     1] loss: 1197.825
[11,     1] loss: 1182.080
[12,     1] loss: 1153.646
[13,     1] loss: 1116.203
[14,     1] loss: 1119.531
[15,     1] loss: 1065.646
[16,     1] loss: 1064.375
[17,     1] loss: 1048.675
[18,     1] loss: 1033.198
[19,     1] loss: 1043.759
[20,     1] loss: 995.793
[21,     1] loss: 1040.032
[22,     1] loss: 966.652
[23,     1] loss: 1009.924
[24,     1] loss: 965.565
[25,     1] loss: 999.688
[26,     1] loss: 934.996
[27,     1] loss: 1019.621
[28,     1] loss: 923.759
[29,     1] loss: 1013.476
[30,     1] loss: 951.185
[31,     1] loss: 985.927
[32,     1] loss: 928.585
[33,     1] loss: 911.358
[34,     1] loss: 936.400
[35,     1] loss: 919.415
[36,     1] loss: 914.274
[37,     1] loss: 959.748
[38,     1] loss: 880.518
[39,     1] loss: 932.472
[40,     1] loss: 892.243
[41,     1] loss: 901.702
[42,     1] loss: 858.142
[43,     1] loss: 843.551
[44,     1] loss: 869.635
[45,     1] loss: 848.975
[46,     1] loss: 835.522
[47,     1] loss: 821.990
[48,     1] loss: 790.472
[49,     1] loss: 803.989
[50,     1] loss: 784.428
[51,     1] loss: 895.238
[52,     1] loss: 808.675
[53,     1] loss: 745.175
[54,     1] loss: 768.245
[55,     1] loss: 707.320
[56,     1] loss: 794.520
[57,     1] loss: 757.600
[58,     1] loss: 758.662
[59,     1] loss: 740.564
[60,     1] loss: 682.407
[61,     1] loss: 759.000
[62,     1] loss: 703.209
[63,     1] loss: 761.131
[64,     1] loss: 778.003
[65,     1] loss: 655.508
[66,     1] loss: 801.888
[67,     1] loss: 680.838
[68,     1] loss: 758.937
[69,     1] loss: 647.961
[70,     1] loss: 728.127
[71,     1] loss: 614.039
[72,     1] loss: 628.627
[73,     1] loss: 628.397
[74,     1] loss: 575.922
[75,     1] loss: 682.456
[76,     1] loss: 651.831
[77,     1] loss: 556.118
[78,     1] loss: 620.305
[79,     1] loss: 627.446
[80,     1] loss: 524.337
[81,     1] loss: 658.375
[82,     1] loss: 533.985
[83,     1] loss: 591.480
[84,     1] loss: 564.551
[85,     1] loss: 550.628
[86,     1] loss: 532.370
[87,     1] loss: 574.733
[88,     1] loss: 627.332
[89,     1] loss: 564.098
[90,     1] loss: 500.789
[91,     1] loss: 481.477
[92,     1] loss: 571.300
[93,     1] loss: 554.577
[94,     1] loss: 471.710
[95,     1] loss: 484.570
[96,     1] loss: 536.852
[97,     1] loss: 570.137
[98,     1] loss: 477.270
[99,     1] loss: 450.052
[100,     1] loss: 525.453
[101,     1] loss: 552.999
[102,     1] loss: 545.048
[103,     1] loss: 478.273
[104,     1] loss: 462.384
[105,     1] loss: 568.581
[106,     1] loss: 497.040
[107,     1] loss: 487.694
[108,     1] loss: 579.020
Early stopping applied (best metric=0.35291382670402527)
Finished Training
Total time taken: 16.154544591903687
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1247.444
[2,     1] loss: 1255.297
[3,     1] loss: 1258.763
[4,     1] loss: 1252.854
[5,     1] loss: 1247.327
[6,     1] loss: 1249.047
[7,     1] loss: 1243.319
[8,     1] loss: 1236.038
[9,     1] loss: 1219.048
[10,     1] loss: 1206.068
[11,     1] loss: 1144.527
[12,     1] loss: 1136.243
[13,     1] loss: 1098.460
[14,     1] loss: 1080.257
[15,     1] loss: 1060.388
[16,     1] loss: 1081.501
[17,     1] loss: 1028.709
[18,     1] loss: 1022.565
[19,     1] loss: 1015.285
[20,     1] loss: 1023.663
[21,     1] loss: 1020.659
[22,     1] loss: 1002.680
[23,     1] loss: 971.179
[24,     1] loss: 990.177
[25,     1] loss: 964.363
[26,     1] loss: 983.698
[27,     1] loss: 971.731
[28,     1] loss: 947.163
[29,     1] loss: 950.030
[30,     1] loss: 887.903
[31,     1] loss: 924.765
[32,     1] loss: 891.569
[33,     1] loss: 917.819
[34,     1] loss: 869.833
[35,     1] loss: 861.205
[36,     1] loss: 830.875
[37,     1] loss: 877.994
[38,     1] loss: 797.113
[39,     1] loss: 826.131
[40,     1] loss: 781.442
[41,     1] loss: 788.797
[42,     1] loss: 846.517
[43,     1] loss: 961.906
[44,     1] loss: 805.595
[45,     1] loss: 808.656
[46,     1] loss: 782.652
[47,     1] loss: 816.073
[48,     1] loss: 773.508
[49,     1] loss: 817.397
[50,     1] loss: 737.157
[51,     1] loss: 712.677
[52,     1] loss: 722.249
[53,     1] loss: 727.362
[54,     1] loss: 705.009
[55,     1] loss: 630.768
[56,     1] loss: 663.415
[57,     1] loss: 671.974
[58,     1] loss: 628.220
[59,     1] loss: 580.546
[60,     1] loss: 652.150
[61,     1] loss: 786.877
[62,     1] loss: 707.285
[63,     1] loss: 624.533
[64,     1] loss: 686.112
[65,     1] loss: 638.195
[66,     1] loss: 664.804
[67,     1] loss: 687.292
[68,     1] loss: 578.310
[69,     1] loss: 608.569
[70,     1] loss: 543.570
[71,     1] loss: 598.595
[72,     1] loss: 553.083
[73,     1] loss: 523.724
[74,     1] loss: 510.111
[75,     1] loss: 618.175
[76,     1] loss: 684.882
Early stopping applied (best metric=0.3480609953403473)
Finished Training
Total time taken: 11.332064628601074
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.395
[2,     1] loss: 1249.691
[3,     1] loss: 1252.388
[4,     1] loss: 1248.559
[5,     1] loss: 1245.029
[6,     1] loss: 1232.328
[7,     1] loss: 1220.482
[8,     1] loss: 1196.672
[9,     1] loss: 1161.798
[10,     1] loss: 1118.242
[11,     1] loss: 1125.370
[12,     1] loss: 1072.807
[13,     1] loss: 1137.184
[14,     1] loss: 1056.074
[15,     1] loss: 1101.718
[16,     1] loss: 1042.923
[17,     1] loss: 1056.076
[18,     1] loss: 1030.944
[19,     1] loss: 1031.586
[20,     1] loss: 1011.239
[21,     1] loss: 1003.943
[22,     1] loss: 981.613
[23,     1] loss: 995.407
[24,     1] loss: 953.743
[25,     1] loss: 948.152
[26,     1] loss: 1004.684
[27,     1] loss: 913.161
[28,     1] loss: 942.457
[29,     1] loss: 924.842
[30,     1] loss: 888.464
[31,     1] loss: 917.918
[32,     1] loss: 913.547
[33,     1] loss: 876.280
[34,     1] loss: 871.897
[35,     1] loss: 853.813
[36,     1] loss: 822.050
[37,     1] loss: 846.740
[38,     1] loss: 853.967
[39,     1] loss: 833.109
[40,     1] loss: 804.743
[41,     1] loss: 802.133
[42,     1] loss: 768.537
[43,     1] loss: 753.575
[44,     1] loss: 814.315
[45,     1] loss: 994.403
[46,     1] loss: 829.895
[47,     1] loss: 840.084
[48,     1] loss: 777.100
[49,     1] loss: 839.020
[50,     1] loss: 826.494
[51,     1] loss: 791.019
[52,     1] loss: 777.357
[53,     1] loss: 749.551
[54,     1] loss: 751.817
[55,     1] loss: 695.536
[56,     1] loss: 776.168
[57,     1] loss: 681.285
[58,     1] loss: 784.802
[59,     1] loss: 660.200
[60,     1] loss: 693.303
[61,     1] loss: 636.417
[62,     1] loss: 665.001
[63,     1] loss: 625.432
[64,     1] loss: 620.468
[65,     1] loss: 662.256
[66,     1] loss: 836.564
[67,     1] loss: 865.302
[68,     1] loss: 636.293
[69,     1] loss: 755.737
[70,     1] loss: 683.193
[71,     1] loss: 679.230
[72,     1] loss: 708.414
[73,     1] loss: 608.497
[74,     1] loss: 657.848
[75,     1] loss: 592.717
[76,     1] loss: 583.893
[77,     1] loss: 576.424
[78,     1] loss: 666.675
[79,     1] loss: 907.882
[80,     1] loss: 539.512
[81,     1] loss: 876.774
[82,     1] loss: 548.192
[83,     1] loss: 709.207
[84,     1] loss: 690.954
[85,     1] loss: 612.920
[86,     1] loss: 674.893
[87,     1] loss: 603.972
[88,     1] loss: 573.463
[89,     1] loss: 623.730
[90,     1] loss: 523.997
[91,     1] loss: 591.900
[92,     1] loss: 471.080
[93,     1] loss: 550.902
[94,     1] loss: 503.088
[95,     1] loss: 552.577
[96,     1] loss: 479.370
[97,     1] loss: 454.993
[98,     1] loss: 440.534
[99,     1] loss: 454.534
[100,     1] loss: 455.596
[101,     1] loss: 380.731
[102,     1] loss: 506.834
[103,     1] loss: 597.002
[104,     1] loss: 388.381
Early stopping applied (best metric=0.3819468319416046)
Finished Training
Total time taken: 16.815563201904297
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1253.727
[2,     1] loss: 1249.348
[3,     1] loss: 1246.871
[4,     1] loss: 1249.252
[5,     1] loss: 1250.675
[6,     1] loss: 1241.685
[7,     1] loss: 1238.423
[8,     1] loss: 1237.325
[9,     1] loss: 1219.555
[10,     1] loss: 1205.893
[11,     1] loss: 1156.423
[12,     1] loss: 1119.285
[13,     1] loss: 1137.634
[14,     1] loss: 1055.261
[15,     1] loss: 1079.787
[16,     1] loss: 1043.390
[17,     1] loss: 1020.427
[18,     1] loss: 979.312
[19,     1] loss: 1026.897
[20,     1] loss: 996.148
[21,     1] loss: 982.321
[22,     1] loss: 983.110
[23,     1] loss: 925.413
[24,     1] loss: 947.924
[25,     1] loss: 911.029
[26,     1] loss: 891.564
[27,     1] loss: 955.657
[28,     1] loss: 922.642
[29,     1] loss: 842.269
[30,     1] loss: 891.416
[31,     1] loss: 840.508
[32,     1] loss: 913.168
[33,     1] loss: 851.696
[34,     1] loss: 922.639
[35,     1] loss: 881.135
[36,     1] loss: 856.331
[37,     1] loss: 842.150
[38,     1] loss: 826.956
[39,     1] loss: 819.593
[40,     1] loss: 818.705
[41,     1] loss: 828.334
[42,     1] loss: 795.410
[43,     1] loss: 795.078
[44,     1] loss: 750.237
[45,     1] loss: 746.069
[46,     1] loss: 808.619
[47,     1] loss: 752.037
[48,     1] loss: 713.684
[49,     1] loss: 706.818
[50,     1] loss: 680.878
[51,     1] loss: 699.150
[52,     1] loss: 723.711
[53,     1] loss: 735.103
[54,     1] loss: 769.891
[55,     1] loss: 740.595
[56,     1] loss: 652.608
[57,     1] loss: 723.359
[58,     1] loss: 726.519
[59,     1] loss: 643.196
[60,     1] loss: 657.975
[61,     1] loss: 713.100
[62,     1] loss: 593.922
[63,     1] loss: 593.648
[64,     1] loss: 662.138
[65,     1] loss: 543.076
[66,     1] loss: 559.201
[67,     1] loss: 650.637
[68,     1] loss: 558.024
[69,     1] loss: 539.835
[70,     1] loss: 628.231
[71,     1] loss: 605.727
[72,     1] loss: 561.798
Early stopping applied (best metric=0.4379139840602875)
Finished Training
Total time taken: 11.818784952163696
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.900
[2,     1] loss: 1251.404
[3,     1] loss: 1247.317
[4,     1] loss: 1247.948
[5,     1] loss: 1252.668
[6,     1] loss: 1249.829
[7,     1] loss: 1250.141
[8,     1] loss: 1249.578
[9,     1] loss: 1243.671
[10,     1] loss: 1238.530
[11,     1] loss: 1232.261
[12,     1] loss: 1214.051
[13,     1] loss: 1194.259
[14,     1] loss: 1166.520
[15,     1] loss: 1131.638
[16,     1] loss: 1106.728
[17,     1] loss: 1088.573
[18,     1] loss: 1054.246
[19,     1] loss: 1073.011
[20,     1] loss: 1067.253
[21,     1] loss: 1066.583
[22,     1] loss: 1078.144
[23,     1] loss: 1059.806
[24,     1] loss: 1038.373
[25,     1] loss: 1009.944
[26,     1] loss: 1038.580
[27,     1] loss: 1009.631
[28,     1] loss: 1005.490
[29,     1] loss: 980.648
[30,     1] loss: 940.540
[31,     1] loss: 959.405
[32,     1] loss: 931.280
[33,     1] loss: 956.997
[34,     1] loss: 944.870
[35,     1] loss: 922.254
[36,     1] loss: 924.815
[37,     1] loss: 907.924
[38,     1] loss: 957.078
[39,     1] loss: 895.650
[40,     1] loss: 855.295
[41,     1] loss: 853.959
[42,     1] loss: 859.326
[43,     1] loss: 847.879
[44,     1] loss: 835.209
[45,     1] loss: 819.270
[46,     1] loss: 781.695
[47,     1] loss: 823.496
[48,     1] loss: 844.258
[49,     1] loss: 835.039
[50,     1] loss: 775.176
[51,     1] loss: 777.344
[52,     1] loss: 776.914
[53,     1] loss: 732.546
[54,     1] loss: 732.378
[55,     1] loss: 688.314
[56,     1] loss: 661.508
[57,     1] loss: 710.251
[58,     1] loss: 691.581
[59,     1] loss: 636.775
[60,     1] loss: 712.097
[61,     1] loss: 647.957
[62,     1] loss: 633.341
[63,     1] loss: 617.010
[64,     1] loss: 632.402
[65,     1] loss: 600.267
[66,     1] loss: 625.637
[67,     1] loss: 891.962
[68,     1] loss: 1340.955
[69,     1] loss: 790.251
[70,     1] loss: 798.566
[71,     1] loss: 883.213
[72,     1] loss: 821.039
[73,     1] loss: 846.096
[74,     1] loss: 862.994
[75,     1] loss: 877.381
[76,     1] loss: 837.826
[77,     1] loss: 772.558
[78,     1] loss: 710.742
[79,     1] loss: 827.683
[80,     1] loss: 793.769
[81,     1] loss: 707.074
[82,     1] loss: 705.814
[83,     1] loss: 680.617
[84,     1] loss: 658.056
[85,     1] loss: 660.764
[86,     1] loss: 645.918
[87,     1] loss: 647.529
[88,     1] loss: 593.155
[89,     1] loss: 572.461
[90,     1] loss: 553.274
[91,     1] loss: 553.188
[92,     1] loss: 523.125
[93,     1] loss: 494.683
[94,     1] loss: 507.435
Early stopping applied (best metric=0.3133830726146698)
Finished Training
Total time taken: 16.05073118209839
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.831
[2,     1] loss: 1263.004
[3,     1] loss: 1251.060
[4,     1] loss: 1251.638
[5,     1] loss: 1252.659
[6,     1] loss: 1249.990
[7,     1] loss: 1247.789
[8,     1] loss: 1250.369
[9,     1] loss: 1250.955
[10,     1] loss: 1251.440
[11,     1] loss: 1253.661
[12,     1] loss: 1248.724
[13,     1] loss: 1247.514
[14,     1] loss: 1247.683
[15,     1] loss: 1241.974
[16,     1] loss: 1237.487
[17,     1] loss: 1231.530
[18,     1] loss: 1228.013
[19,     1] loss: 1204.479
[20,     1] loss: 1190.453
[21,     1] loss: 1156.736
[22,     1] loss: 1144.984
[23,     1] loss: 1118.779
[24,     1] loss: 1089.130
[25,     1] loss: 1063.026
[26,     1] loss: 1069.443
[27,     1] loss: 1073.548
[28,     1] loss: 1012.343
[29,     1] loss: 1053.993
[30,     1] loss: 1021.448
[31,     1] loss: 1089.057
[32,     1] loss: 1066.900
[33,     1] loss: 1023.207
[34,     1] loss: 1044.290
[35,     1] loss: 995.350
[36,     1] loss: 1000.153
[37,     1] loss: 997.490
[38,     1] loss: 998.087
[39,     1] loss: 985.668
[40,     1] loss: 972.210
[41,     1] loss: 993.341
[42,     1] loss: 936.728
[43,     1] loss: 890.179
[44,     1] loss: 923.324
[45,     1] loss: 915.943
[46,     1] loss: 926.808
[47,     1] loss: 927.801
[48,     1] loss: 930.985
[49,     1] loss: 888.707
[50,     1] loss: 885.133
[51,     1] loss: 865.547
[52,     1] loss: 910.834
[53,     1] loss: 834.813
[54,     1] loss: 851.561
[55,     1] loss: 822.839
[56,     1] loss: 804.347
[57,     1] loss: 811.243
[58,     1] loss: 796.971
[59,     1] loss: 760.191
[60,     1] loss: 780.513
[61,     1] loss: 1055.329
[62,     1] loss: 1216.707
[63,     1] loss: 842.965
[64,     1] loss: 904.729
[65,     1] loss: 979.365
[66,     1] loss: 913.794
[67,     1] loss: 883.975
[68,     1] loss: 876.336
[69,     1] loss: 910.163
[70,     1] loss: 937.194
[71,     1] loss: 842.054
[72,     1] loss: 862.580
[73,     1] loss: 885.310
[74,     1] loss: 810.739
[75,     1] loss: 787.666
[76,     1] loss: 797.678
[77,     1] loss: 822.454
[78,     1] loss: 736.056
[79,     1] loss: 812.683
[80,     1] loss: 758.851
[81,     1] loss: 747.473
[82,     1] loss: 769.973
[83,     1] loss: 742.835
[84,     1] loss: 732.906
[85,     1] loss: 682.288
[86,     1] loss: 707.175
[87,     1] loss: 664.629
[88,     1] loss: 638.189
[89,     1] loss: 649.861
[90,     1] loss: 655.613
[91,     1] loss: 746.775
[92,     1] loss: 717.598
[93,     1] loss: 638.879
[94,     1] loss: 677.192
[95,     1] loss: 683.433
[96,     1] loss: 586.050
[97,     1] loss: 678.958
[98,     1] loss: 583.059
[99,     1] loss: 668.861
[100,     1] loss: 643.184
[101,     1] loss: 534.212
[102,     1] loss: 694.010
[103,     1] loss: 602.325
[104,     1] loss: 515.584
[105,     1] loss: 556.063
[106,     1] loss: 505.247
[107,     1] loss: 479.795
[108,     1] loss: 543.419
[109,     1] loss: 565.979
[110,     1] loss: 667.429
[111,     1] loss: 665.844
[112,     1] loss: 611.730
[113,     1] loss: 546.762
[114,     1] loss: 634.626
[115,     1] loss: 554.737
[116,     1] loss: 591.765
[117,     1] loss: 512.252
[118,     1] loss: 552.671
[119,     1] loss: 490.790
[120,     1] loss: 589.859
[121,     1] loss: 491.845
[122,     1] loss: 426.625
[123,     1] loss: 544.155
[124,     1] loss: 429.199
[125,     1] loss: 419.866
[126,     1] loss: 465.106
[127,     1] loss: 407.904
[128,     1] loss: 467.834
[129,     1] loss: 403.676
[130,     1] loss: 416.726
[131,     1] loss: 472.036
[132,     1] loss: 470.276
[133,     1] loss: 355.158
[134,     1] loss: 584.667
[135,     1] loss: 1079.956
[136,     1] loss: 394.418
[137,     1] loss: 738.771
[138,     1] loss: 492.631
[139,     1] loss: 703.044
Early stopping applied (best metric=0.3471614718437195)
Finished Training
Total time taken: 21.096179962158203
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1253.818
[2,     1] loss: 1252.898
[3,     1] loss: 1249.623
[4,     1] loss: 1248.028
[5,     1] loss: 1246.023
[6,     1] loss: 1237.566
[7,     1] loss: 1215.505
[8,     1] loss: 1172.665
[9,     1] loss: 1138.723
[10,     1] loss: 1088.799
[11,     1] loss: 1070.508
[12,     1] loss: 1113.728
[13,     1] loss: 1027.228
[14,     1] loss: 1048.475
[15,     1] loss: 1041.278
[16,     1] loss: 981.318
[17,     1] loss: 985.472
[18,     1] loss: 1040.431
[19,     1] loss: 990.667
[20,     1] loss: 1015.904
[21,     1] loss: 941.641
[22,     1] loss: 977.819
[23,     1] loss: 953.196
[24,     1] loss: 941.832
[25,     1] loss: 922.541
[26,     1] loss: 975.129
[27,     1] loss: 919.327
[28,     1] loss: 919.714
[29,     1] loss: 938.644
[30,     1] loss: 920.971
[31,     1] loss: 910.200
[32,     1] loss: 868.497
[33,     1] loss: 862.675
[34,     1] loss: 877.646
[35,     1] loss: 881.136
[36,     1] loss: 804.693
[37,     1] loss: 849.921
[38,     1] loss: 868.071
[39,     1] loss: 805.578
[40,     1] loss: 820.087
[41,     1] loss: 795.154
[42,     1] loss: 894.702
[43,     1] loss: 987.296
[44,     1] loss: 786.240
[45,     1] loss: 880.377
[46,     1] loss: 818.842
[47,     1] loss: 855.344
[48,     1] loss: 803.186
[49,     1] loss: 787.453
[50,     1] loss: 780.075
[51,     1] loss: 756.016
[52,     1] loss: 772.574
[53,     1] loss: 752.660
[54,     1] loss: 786.883
[55,     1] loss: 723.002
[56,     1] loss: 699.495
[57,     1] loss: 729.023
[58,     1] loss: 684.419
[59,     1] loss: 741.941
[60,     1] loss: 687.141
[61,     1] loss: 652.352
[62,     1] loss: 652.556
[63,     1] loss: 658.644
[64,     1] loss: 666.830
[65,     1] loss: 735.159
[66,     1] loss: 892.973
[67,     1] loss: 622.956
[68,     1] loss: 757.715
[69,     1] loss: 681.272
[70,     1] loss: 677.634
[71,     1] loss: 607.894
[72,     1] loss: 730.532
[73,     1] loss: 606.438
[74,     1] loss: 673.380
[75,     1] loss: 571.566
[76,     1] loss: 633.561
[77,     1] loss: 610.098
[78,     1] loss: 514.991
[79,     1] loss: 626.886
[80,     1] loss: 559.353
[81,     1] loss: 595.582
[82,     1] loss: 517.973
[83,     1] loss: 546.952
[84,     1] loss: 533.661
[85,     1] loss: 531.661
[86,     1] loss: 660.897
[87,     1] loss: 641.186
[88,     1] loss: 469.216
[89,     1] loss: 576.578
[90,     1] loss: 474.568
[91,     1] loss: 592.267
[92,     1] loss: 500.007
[93,     1] loss: 608.364
[94,     1] loss: 490.397
[95,     1] loss: 558.845
[96,     1] loss: 465.595
[97,     1] loss: 477.391
[98,     1] loss: 520.701
[99,     1] loss: 479.973
[100,     1] loss: 599.825
[101,     1] loss: 516.288
[102,     1] loss: 432.223
[103,     1] loss: 504.820
[104,     1] loss: 409.785
[105,     1] loss: 512.715
[106,     1] loss: 373.501
[107,     1] loss: 472.639
[108,     1] loss: 443.208
[109,     1] loss: 388.152
[110,     1] loss: 499.491
[111,     1] loss: 432.667
[112,     1] loss: 377.261
[113,     1] loss: 477.943
[114,     1] loss: 401.167
[115,     1] loss: 399.637
[116,     1] loss: 400.742
[117,     1] loss: 383.181
[118,     1] loss: 333.364
[119,     1] loss: 361.333
[120,     1] loss: 374.329
[121,     1] loss: 494.619
[122,     1] loss: 830.262
[123,     1] loss: 901.901
[124,     1] loss: 518.881
[125,     1] loss: 650.045
[126,     1] loss: 725.782
[127,     1] loss: 525.362
[128,     1] loss: 586.838
[129,     1] loss: 554.942
[130,     1] loss: 518.551
[131,     1] loss: 515.252
[132,     1] loss: 439.614
[133,     1] loss: 487.645
[134,     1] loss: 453.567
[135,     1] loss: 433.106
[136,     1] loss: 422.780
[137,     1] loss: 415.891
[138,     1] loss: 442.646
[139,     1] loss: 386.184
[140,     1] loss: 377.328
[141,     1] loss: 347.221
[142,     1] loss: 372.606
[143,     1] loss: 340.680
[144,     1] loss: 334.957
[145,     1] loss: 389.369
[146,     1] loss: 373.116
[147,     1] loss: 302.404
[148,     1] loss: 305.736
[149,     1] loss: 285.982
[150,     1] loss: 279.275
[151,     1] loss: 338.079
[152,     1] loss: 451.211
[153,     1] loss: 842.998
[154,     1] loss: 1451.483
[155,     1] loss: 523.913
[156,     1] loss: 801.136
[157,     1] loss: 896.289
[158,     1] loss: 830.903
[159,     1] loss: 790.038
[160,     1] loss: 734.320
[161,     1] loss: 696.433
[162,     1] loss: 720.557
[163,     1] loss: 728.454
[164,     1] loss: 599.052
[165,     1] loss: 596.024
[166,     1] loss: 556.231
[167,     1] loss: 647.696
[168,     1] loss: 573.749
[169,     1] loss: 575.994
[170,     1] loss: 556.090
[171,     1] loss: 471.572
[172,     1] loss: 491.451
[173,     1] loss: 458.709
[174,     1] loss: 455.182
[175,     1] loss: 438.846
[176,     1] loss: 435.264
[177,     1] loss: 437.893
[178,     1] loss: 404.374
Early stopping applied (best metric=0.36804068088531494)
Finished Training
Total time taken: 26.17447257041931
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.539
[2,     1] loss: 1252.217
[3,     1] loss: 1250.899
[4,     1] loss: 1248.298
[5,     1] loss: 1247.665
[6,     1] loss: 1244.821
[7,     1] loss: 1236.701
[8,     1] loss: 1222.934
[9,     1] loss: 1202.760
[10,     1] loss: 1163.874
[11,     1] loss: 1144.361
[12,     1] loss: 1119.878
[13,     1] loss: 1075.081
[14,     1] loss: 1078.454
[15,     1] loss: 1043.602
[16,     1] loss: 1033.664
[17,     1] loss: 1046.271
[18,     1] loss: 1055.917
[19,     1] loss: 966.768
[20,     1] loss: 1007.154
[21,     1] loss: 974.836
[22,     1] loss: 952.396
[23,     1] loss: 962.106
[24,     1] loss: 930.540
[25,     1] loss: 898.359
[26,     1] loss: 969.510
[27,     1] loss: 963.980
[28,     1] loss: 902.906
[29,     1] loss: 941.638
[30,     1] loss: 871.650
[31,     1] loss: 889.044
[32,     1] loss: 885.277
[33,     1] loss: 843.090
[34,     1] loss: 873.900
[35,     1] loss: 807.403
[36,     1] loss: 825.978
[37,     1] loss: 809.355
[38,     1] loss: 840.775
[39,     1] loss: 791.621
[40,     1] loss: 865.650
[41,     1] loss: 857.107
[42,     1] loss: 784.793
[43,     1] loss: 749.506
[44,     1] loss: 783.810
[45,     1] loss: 717.702
[46,     1] loss: 750.359
[47,     1] loss: 718.874
[48,     1] loss: 664.167
[49,     1] loss: 712.288
[50,     1] loss: 775.030
[51,     1] loss: 919.724
[52,     1] loss: 672.779
[53,     1] loss: 916.231
[54,     1] loss: 770.411
[55,     1] loss: 861.579
[56,     1] loss: 736.909
[57,     1] loss: 735.439
[58,     1] loss: 864.810
[59,     1] loss: 675.916
[60,     1] loss: 777.466
[61,     1] loss: 692.188
[62,     1] loss: 661.299
[63,     1] loss: 807.487
[64,     1] loss: 670.933
[65,     1] loss: 703.939
[66,     1] loss: 610.668
[67,     1] loss: 674.397
Early stopping applied (best metric=0.3460187017917633)
Finished Training
Total time taken: 10.350144147872925
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.583
[2,     1] loss: 1251.290
[3,     1] loss: 1252.483
[4,     1] loss: 1251.076
[5,     1] loss: 1249.738
[6,     1] loss: 1244.141
[7,     1] loss: 1238.721
[8,     1] loss: 1223.056
[9,     1] loss: 1203.307
[10,     1] loss: 1179.050
[11,     1] loss: 1143.564
[12,     1] loss: 1085.662
[13,     1] loss: 1053.861
[14,     1] loss: 1045.225
[15,     1] loss: 975.075
[16,     1] loss: 1022.844
[17,     1] loss: 1031.008
[18,     1] loss: 955.138
[19,     1] loss: 1019.596
[20,     1] loss: 980.168
[21,     1] loss: 966.510
[22,     1] loss: 934.079
[23,     1] loss: 950.374
[24,     1] loss: 933.910
[25,     1] loss: 920.923
[26,     1] loss: 891.849
[27,     1] loss: 945.214
[28,     1] loss: 890.469
[29,     1] loss: 907.501
[30,     1] loss: 902.494
[31,     1] loss: 842.079
[32,     1] loss: 884.961
[33,     1] loss: 867.089
[34,     1] loss: 869.774
[35,     1] loss: 834.630
[36,     1] loss: 866.825
[37,     1] loss: 861.045
[38,     1] loss: 850.390
[39,     1] loss: 801.086
[40,     1] loss: 882.816
[41,     1] loss: 846.342
[42,     1] loss: 749.227
[43,     1] loss: 777.919
[44,     1] loss: 748.573
[45,     1] loss: 766.531
[46,     1] loss: 756.981
[47,     1] loss: 689.805
[48,     1] loss: 793.730
[49,     1] loss: 1013.455
[50,     1] loss: 775.961
[51,     1] loss: 767.976
[52,     1] loss: 782.179
[53,     1] loss: 795.430
[54,     1] loss: 730.648
[55,     1] loss: 780.253
[56,     1] loss: 779.344
[57,     1] loss: 755.318
[58,     1] loss: 750.650
[59,     1] loss: 718.234
[60,     1] loss: 733.948
[61,     1] loss: 647.582
[62,     1] loss: 699.177
[63,     1] loss: 645.603
[64,     1] loss: 737.560
[65,     1] loss: 713.948
[66,     1] loss: 702.324
[67,     1] loss: 723.938
[68,     1] loss: 622.211
[69,     1] loss: 708.328
[70,     1] loss: 617.148
[71,     1] loss: 686.262
[72,     1] loss: 671.902
[73,     1] loss: 723.046
[74,     1] loss: 602.675
[75,     1] loss: 626.165
[76,     1] loss: 694.131
[77,     1] loss: 568.003
[78,     1] loss: 682.828
[79,     1] loss: 597.032
[80,     1] loss: 596.616
[81,     1] loss: 567.353
[82,     1] loss: 583.792
[83,     1] loss: 538.773
[84,     1] loss: 515.736
[85,     1] loss: 586.989
Early stopping applied (best metric=0.3642279803752899)
Finished Training
Total time taken: 14.075777530670166
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.180
[2,     1] loss: 1250.378
[3,     1] loss: 1252.757
[4,     1] loss: 1253.488
[5,     1] loss: 1253.968
[6,     1] loss: 1248.298
[7,     1] loss: 1244.538
[8,     1] loss: 1243.351
[9,     1] loss: 1231.694
[10,     1] loss: 1217.195
[11,     1] loss: 1194.342
[12,     1] loss: 1165.185
[13,     1] loss: 1129.829
[14,     1] loss: 1117.674
[15,     1] loss: 1097.860
[16,     1] loss: 1072.671
[17,     1] loss: 1072.968
[18,     1] loss: 1046.846
[19,     1] loss: 1086.308
[20,     1] loss: 1010.451
[21,     1] loss: 1061.058
[22,     1] loss: 996.541
[23,     1] loss: 1009.360
[24,     1] loss: 1014.063
[25,     1] loss: 980.171
[26,     1] loss: 979.355
[27,     1] loss: 957.383
[28,     1] loss: 979.721
[29,     1] loss: 949.173
[30,     1] loss: 948.821
[31,     1] loss: 905.362
[32,     1] loss: 974.007
[33,     1] loss: 972.516
[34,     1] loss: 903.808
[35,     1] loss: 955.633
[36,     1] loss: 877.605
[37,     1] loss: 893.919
[38,     1] loss: 850.550
[39,     1] loss: 882.464
[40,     1] loss: 834.120
[41,     1] loss: 832.886
[42,     1] loss: 835.563
[43,     1] loss: 780.481
[44,     1] loss: 817.756
[45,     1] loss: 782.120
[46,     1] loss: 718.303
[47,     1] loss: 817.130
[48,     1] loss: 924.058
[49,     1] loss: 862.563
[50,     1] loss: 743.207
[51,     1] loss: 763.942
[52,     1] loss: 747.775
[53,     1] loss: 721.662
[54,     1] loss: 692.578
[55,     1] loss: 711.666
[56,     1] loss: 682.723
[57,     1] loss: 741.688
[58,     1] loss: 639.496
[59,     1] loss: 719.651
[60,     1] loss: 742.563
[61,     1] loss: 807.145
[62,     1] loss: 746.748
[63,     1] loss: 632.623
[64,     1] loss: 761.971
[65,     1] loss: 642.997
[66,     1] loss: 632.205
[67,     1] loss: 639.811
[68,     1] loss: 617.199
[69,     1] loss: 624.190
[70,     1] loss: 698.874
[71,     1] loss: 662.276
[72,     1] loss: 599.402
[73,     1] loss: 536.554
[74,     1] loss: 608.045
[75,     1] loss: 536.509
[76,     1] loss: 544.194
[77,     1] loss: 618.891
[78,     1] loss: 859.321
[79,     1] loss: 656.114
[80,     1] loss: 571.427
[81,     1] loss: 636.688
[82,     1] loss: 572.002
[83,     1] loss: 577.085
[84,     1] loss: 524.801
[85,     1] loss: 522.603
[86,     1] loss: 496.706
[87,     1] loss: 508.633
[88,     1] loss: 606.120
[89,     1] loss: 664.245
[90,     1] loss: 911.341
[91,     1] loss: 498.734
[92,     1] loss: 954.318
[93,     1] loss: 617.746
[94,     1] loss: 825.741
[95,     1] loss: 702.244
[96,     1] loss: 659.211
[97,     1] loss: 739.711
[98,     1] loss: 697.338
[99,     1] loss: 560.317
[100,     1] loss: 621.432
[101,     1] loss: 612.249
[102,     1] loss: 513.531
[103,     1] loss: 610.850
[104,     1] loss: 519.453
[105,     1] loss: 529.243
[106,     1] loss: 492.082
[107,     1] loss: 544.724
[108,     1] loss: 462.631
[109,     1] loss: 507.453
[110,     1] loss: 436.812
[111,     1] loss: 489.405
[112,     1] loss: 434.512
[113,     1] loss: 424.202
[114,     1] loss: 419.541
[115,     1] loss: 397.143
[116,     1] loss: 396.569
[117,     1] loss: 361.863
[118,     1] loss: 386.044
[119,     1] loss: 543.547
[120,     1] loss: 483.843
[121,     1] loss: 500.929
[122,     1] loss: 394.378
[123,     1] loss: 526.688
[124,     1] loss: 497.217
[125,     1] loss: 407.237
[126,     1] loss: 485.249
[127,     1] loss: 334.648
[128,     1] loss: 443.500
[129,     1] loss: 331.717
[130,     1] loss: 403.764
[131,     1] loss: 412.979
[132,     1] loss: 340.935
[133,     1] loss: 376.498
[134,     1] loss: 456.360
[135,     1] loss: 382.116
[136,     1] loss: 487.053
[137,     1] loss: 456.528
[138,     1] loss: 369.587
[139,     1] loss: 505.454
[140,     1] loss: 542.497
[141,     1] loss: 391.480
[142,     1] loss: 589.394
[143,     1] loss: 479.130
[144,     1] loss: 436.701
[145,     1] loss: 492.032
[146,     1] loss: 382.230
[147,     1] loss: 505.680
[148,     1] loss: 314.313
[149,     1] loss: 433.953
[150,     1] loss: 464.881
[151,     1] loss: 366.682
[152,     1] loss: 572.007
[153,     1] loss: 425.737
[154,     1] loss: 442.428
[155,     1] loss: 459.028
[156,     1] loss: 378.828
[157,     1] loss: 474.833
[158,     1] loss: 322.319
[159,     1] loss: 348.382
[160,     1] loss: 333.071
[161,     1] loss: 332.441
[162,     1] loss: 404.203
[163,     1] loss: 448.595
[164,     1] loss: 333.485
[165,     1] loss: 436.187
[166,     1] loss: 413.309
[167,     1] loss: 320.403
[168,     1] loss: 395.998
[169,     1] loss: 416.576
[170,     1] loss: 312.088
Early stopping applied (best metric=0.31164291501045227)
Finished Training
Total time taken: 25.357274055480957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.713
[2,     1] loss: 1258.862
[3,     1] loss: 1251.586
[4,     1] loss: 1253.483
[5,     1] loss: 1250.255
[6,     1] loss: 1250.449
[7,     1] loss: 1249.981
[8,     1] loss: 1248.716
[9,     1] loss: 1249.004
[10,     1] loss: 1244.072
[11,     1] loss: 1246.240
[12,     1] loss: 1241.077
[13,     1] loss: 1230.587
[14,     1] loss: 1215.193
[15,     1] loss: 1191.054
[16,     1] loss: 1162.494
[17,     1] loss: 1152.283
[18,     1] loss: 1112.897
[19,     1] loss: 1080.911
[20,     1] loss: 1088.652
[21,     1] loss: 1043.958
[22,     1] loss: 1116.213
[23,     1] loss: 1034.956
[24,     1] loss: 1020.989
[25,     1] loss: 988.303
[26,     1] loss: 1080.023
[27,     1] loss: 999.842
[28,     1] loss: 1022.628
[29,     1] loss: 989.491
[30,     1] loss: 987.674
[31,     1] loss: 966.218
[32,     1] loss: 946.792
[33,     1] loss: 1008.711
[34,     1] loss: 921.400
[35,     1] loss: 978.701
[36,     1] loss: 917.504
[37,     1] loss: 940.186
[38,     1] loss: 897.827
[39,     1] loss: 945.540
[40,     1] loss: 981.821
[41,     1] loss: 925.032
[42,     1] loss: 921.317
[43,     1] loss: 886.398
[44,     1] loss: 888.820
[45,     1] loss: 869.605
[46,     1] loss: 878.845
[47,     1] loss: 836.711
[48,     1] loss: 817.504
[49,     1] loss: 868.216
[50,     1] loss: 899.759
[51,     1] loss: 837.130
[52,     1] loss: 815.457
[53,     1] loss: 794.245
[54,     1] loss: 793.502
[55,     1] loss: 821.738
[56,     1] loss: 784.397
[57,     1] loss: 747.836
[58,     1] loss: 685.698
[59,     1] loss: 760.712
[60,     1] loss: 700.906
[61,     1] loss: 738.164
[62,     1] loss: 1097.465
[63,     1] loss: 1017.108
[64,     1] loss: 721.983
[65,     1] loss: 823.755
[66,     1] loss: 839.766
[67,     1] loss: 778.584
[68,     1] loss: 799.299
[69,     1] loss: 851.450
[70,     1] loss: 814.893
[71,     1] loss: 733.470
[72,     1] loss: 766.066
[73,     1] loss: 726.854
[74,     1] loss: 710.897
[75,     1] loss: 716.972
[76,     1] loss: 695.553
[77,     1] loss: 648.954
[78,     1] loss: 634.240
[79,     1] loss: 657.078
[80,     1] loss: 678.790
[81,     1] loss: 644.751
[82,     1] loss: 626.524
[83,     1] loss: 606.828
[84,     1] loss: 588.371
[85,     1] loss: 586.253
[86,     1] loss: 621.528
[87,     1] loss: 610.087
[88,     1] loss: 542.454
[89,     1] loss: 933.528
[90,     1] loss: 955.202
[91,     1] loss: 749.440
[92,     1] loss: 645.582
[93,     1] loss: 884.949
[94,     1] loss: 653.011
[95,     1] loss: 704.853
[96,     1] loss: 821.061
[97,     1] loss: 712.836
[98,     1] loss: 609.534
[99,     1] loss: 696.596
[100,     1] loss: 661.986
[101,     1] loss: 627.161
[102,     1] loss: 695.482
[103,     1] loss: 567.302
[104,     1] loss: 638.061
[105,     1] loss: 484.085
[106,     1] loss: 602.237
[107,     1] loss: 534.130
[108,     1] loss: 519.242
[109,     1] loss: 500.901
[110,     1] loss: 556.691
[111,     1] loss: 472.622
[112,     1] loss: 513.189
[113,     1] loss: 437.409
[114,     1] loss: 458.353
[115,     1] loss: 507.052
[116,     1] loss: 407.364
[117,     1] loss: 433.207
[118,     1] loss: 573.868
[119,     1] loss: 566.084
[120,     1] loss: 391.120
[121,     1] loss: 739.387
[122,     1] loss: 463.482
[123,     1] loss: 597.666
[124,     1] loss: 431.857
[125,     1] loss: 605.662
[126,     1] loss: 449.778
[127,     1] loss: 569.926
[128,     1] loss: 450.710
[129,     1] loss: 556.948
[130,     1] loss: 508.461
[131,     1] loss: 487.801
[132,     1] loss: 479.320
[133,     1] loss: 439.438
[134,     1] loss: 478.597
[135,     1] loss: 405.539
[136,     1] loss: 443.739
[137,     1] loss: 335.694
[138,     1] loss: 371.809
[139,     1] loss: 352.444
[140,     1] loss: 436.957
[141,     1] loss: 508.441
[142,     1] loss: 331.790
[143,     1] loss: 501.885
[144,     1] loss: 556.917
[145,     1] loss: 462.972
[146,     1] loss: 580.456
[147,     1] loss: 404.856
[148,     1] loss: 484.579
[149,     1] loss: 338.551
[150,     1] loss: 518.222
[151,     1] loss: 647.162
[152,     1] loss: 323.920
[153,     1] loss: 729.691
[154,     1] loss: 583.821
[155,     1] loss: 680.082
[156,     1] loss: 485.165
[157,     1] loss: 700.074
[158,     1] loss: 416.634
[159,     1] loss: 587.763
[160,     1] loss: 469.763
[161,     1] loss: 488.240
[162,     1] loss: 419.075
[163,     1] loss: 392.771
[164,     1] loss: 431.309
[165,     1] loss: 399.679
[166,     1] loss: 348.637
[167,     1] loss: 381.543
[168,     1] loss: 351.594
[169,     1] loss: 458.439
[170,     1] loss: 377.525
[171,     1] loss: 340.581
[172,     1] loss: 401.112
[173,     1] loss: 332.813
[174,     1] loss: 345.536
[175,     1] loss: 316.495
[176,     1] loss: 480.330
[177,     1] loss: 696.378
[178,     1] loss: 309.175
[179,     1] loss: 505.368
[180,     1] loss: 446.466
[181,     1] loss: 485.799
[182,     1] loss: 473.818
[183,     1] loss: 358.709
[184,     1] loss: 459.922
[185,     1] loss: 308.906
[186,     1] loss: 371.425
[187,     1] loss: 395.942
[188,     1] loss: 346.627
[189,     1] loss: 535.583
[190,     1] loss: 418.937
[191,     1] loss: 507.161
[192,     1] loss: 430.162
[193,     1] loss: 389.370
[194,     1] loss: 410.641
[195,     1] loss: 320.886
[196,     1] loss: 554.837
[197,     1] loss: 426.327
[198,     1] loss: 336.578
[199,     1] loss: 404.027
[200,     1] loss: 368.487
Finished Training
Total time taken: 31.61931085586548
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1255.484
[2,     1] loss: 1251.926
[3,     1] loss: 1251.237
[4,     1] loss: 1250.209
[5,     1] loss: 1251.686
[6,     1] loss: 1252.039
[7,     1] loss: 1247.097
[8,     1] loss: 1244.942
[9,     1] loss: 1238.155
[10,     1] loss: 1227.022
[11,     1] loss: 1211.141
[12,     1] loss: 1165.116
[13,     1] loss: 1148.349
[14,     1] loss: 1125.304
[15,     1] loss: 1104.981
[16,     1] loss: 1117.298
[17,     1] loss: 1049.211
[18,     1] loss: 1039.607
[19,     1] loss: 1014.123
[20,     1] loss: 1111.137
[21,     1] loss: 1064.925
[22,     1] loss: 1000.747
[23,     1] loss: 986.149
[24,     1] loss: 1090.992
[25,     1] loss: 999.548
[26,     1] loss: 1028.826
[27,     1] loss: 971.004
[28,     1] loss: 957.486
[29,     1] loss: 983.668
[30,     1] loss: 970.612
[31,     1] loss: 941.496
[32,     1] loss: 964.399
[33,     1] loss: 929.831
[34,     1] loss: 970.044
[35,     1] loss: 908.578
[36,     1] loss: 903.280
[37,     1] loss: 904.275
[38,     1] loss: 853.422
[39,     1] loss: 898.396
[40,     1] loss: 860.992
[41,     1] loss: 840.177
[42,     1] loss: 862.938
[43,     1] loss: 850.647
[44,     1] loss: 843.494
[45,     1] loss: 845.158
[46,     1] loss: 844.960
[47,     1] loss: 825.038
[48,     1] loss: 803.005
[49,     1] loss: 775.073
[50,     1] loss: 819.869
[51,     1] loss: 891.096
[52,     1] loss: 821.205
[53,     1] loss: 764.132
[54,     1] loss: 811.530
[55,     1] loss: 764.912
[56,     1] loss: 765.322
[57,     1] loss: 744.920
[58,     1] loss: 730.273
[59,     1] loss: 858.758
[60,     1] loss: 669.524
[61,     1] loss: 806.657
[62,     1] loss: 694.558
[63,     1] loss: 728.033
[64,     1] loss: 675.561
[65,     1] loss: 679.493
[66,     1] loss: 710.242
[67,     1] loss: 639.322
[68,     1] loss: 722.847
[69,     1] loss: 767.339
[70,     1] loss: 588.227
[71,     1] loss: 650.401
[72,     1] loss: 634.170
[73,     1] loss: 618.332
[74,     1] loss: 684.994
[75,     1] loss: 580.270
[76,     1] loss: 593.954
[77,     1] loss: 563.110
[78,     1] loss: 626.750
[79,     1] loss: 513.453
[80,     1] loss: 499.761
[81,     1] loss: 545.544
[82,     1] loss: 531.919
[83,     1] loss: 519.015
[84,     1] loss: 550.900
[85,     1] loss: 636.254
[86,     1] loss: 824.894
[87,     1] loss: 800.881
[88,     1] loss: 606.869
[89,     1] loss: 700.051
[90,     1] loss: 686.596
[91,     1] loss: 624.852
[92,     1] loss: 714.495
[93,     1] loss: 602.353
[94,     1] loss: 580.802
[95,     1] loss: 588.436
[96,     1] loss: 553.477
[97,     1] loss: 565.559
[98,     1] loss: 488.055
[99,     1] loss: 505.867
[100,     1] loss: 487.660
[101,     1] loss: 455.851
[102,     1] loss: 474.750
[103,     1] loss: 408.035
[104,     1] loss: 408.241
[105,     1] loss: 421.528
[106,     1] loss: 490.021
[107,     1] loss: 592.651
[108,     1] loss: 785.283
[109,     1] loss: 435.014
[110,     1] loss: 596.241
[111,     1] loss: 453.847
[112,     1] loss: 528.090
[113,     1] loss: 445.959
[114,     1] loss: 513.155
[115,     1] loss: 456.288
[116,     1] loss: 515.177
[117,     1] loss: 461.455
[118,     1] loss: 389.939
[119,     1] loss: 425.942
[120,     1] loss: 413.750
[121,     1] loss: 385.495
[122,     1] loss: 376.511
[123,     1] loss: 448.140
[124,     1] loss: 425.536
[125,     1] loss: 371.868
[126,     1] loss: 475.377
[127,     1] loss: 472.477
Early stopping applied (best metric=0.2924726903438568)
Finished Training
Total time taken: 19.130203008651733
{'Hydroxylation-K Validation Accuracy': 0.7481678486997636, 'Hydroxylation-K Validation Sensitivity': 0.7140740740740741, 'Hydroxylation-K Validation Specificity': 0.756140350877193, 'Hydroxylation-K Validation Precision': 0.4287755851532941, 'Hydroxylation-K AUC ROC': 0.8078752436647173, 'Hydroxylation-K AUC PR': 0.5821065786855889, 'Hydroxylation-K MCC': 0.40316993838435267, 'Hydroxylation-K F1': 0.5269104766658482, 'Validation Loss (Hydroxylation-K)': 0.4531329035758972, 'Hydroxylation-P Validation Accuracy': 0.7913247381689593, 'Hydroxylation-P Validation Sensitivity': 0.8087830687830688, 'Hydroxylation-P Validation Specificity': 0.7875679585016709, 'Hydroxylation-P Validation Precision': 0.4560458223040672, 'Hydroxylation-P AUC ROC': 0.8610235419254573, 'Hydroxylation-P AUC PR': 0.6207563148277161, 'Hydroxylation-P MCC': 0.4920915833771251, 'Hydroxylation-P F1': 0.5810420982262959, 'Validation Loss (Hydroxylation-P)': 0.3522376875082652, 'Validation Loss (total)': 0.8053705890973409, 'TimeToTrain': 18.183815256754556}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019261568861915724,
 'learning_rate_Hydroxylation-K': 0.006982319475602549,
 'learning_rate_Hydroxylation-P': 0.003143301238223675,
 'log_base': 1.0167572232968194,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3168891210,
 'sample_weights': [1.6278874563248797, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.868160496047986,
 'weight_decay_Hydroxylation-K': 9.356987760399335,
 'weight_decay_Hydroxylation-P': 2.0544851260121533}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32902.953
Exploding loss, terminate run (best metric=0.5344066023826599)
Finished Training
Total time taken: 0.20800042152404785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32619.902
Exploding loss, terminate run (best metric=0.5305263996124268)
Finished Training
Total time taken: 0.2219998836517334
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32614.312
Exploding loss, terminate run (best metric=0.5276257395744324)
Finished Training
Total time taken: 0.20699715614318848
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32668.334
Exploding loss, terminate run (best metric=0.5281538367271423)
Finished Training
Total time taken: 0.21199655532836914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32581.912
Exploding loss, terminate run (best metric=0.5288046598434448)
Finished Training
Total time taken: 0.22699928283691406
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32502.438
Exploding loss, terminate run (best metric=0.5343300104141235)
Finished Training
Total time taken: 0.24899864196777344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32698.551
Exploding loss, terminate run (best metric=0.5323638916015625)
Finished Training
Total time taken: 0.21900272369384766
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32620.928
Exploding loss, terminate run (best metric=0.5335971117019653)
Finished Training
Total time taken: 0.21699857711791992
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32567.273
Exploding loss, terminate run (best metric=0.5274873971939087)
Finished Training
Total time taken: 0.21200156211853027
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32940.184
Exploding loss, terminate run (best metric=0.5302444696426392)
Finished Training
Total time taken: 0.23600077629089355
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32679.492
Exploding loss, terminate run (best metric=0.5334593653678894)
Finished Training
Total time taken: 0.24199891090393066
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32514.570
Exploding loss, terminate run (best metric=0.5276498198509216)
Finished Training
Total time taken: 0.2239971160888672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32930.957
Exploding loss, terminate run (best metric=0.5282751321792603)
Finished Training
Total time taken: 0.21899914741516113
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32678.346
Exploding loss, terminate run (best metric=0.5291985273361206)
Finished Training
Total time taken: 0.23000097274780273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32609.816
Exploding loss, terminate run (best metric=0.5293094515800476)
Finished Training
Total time taken: 0.2129991054534912
{'Hydroxylation-K Validation Accuracy': 0.45661938534278956, 'Hydroxylation-K Validation Sensitivity': 0.6444444444444445, 'Hydroxylation-K Validation Specificity': 0.41403508771929826, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6806237816764132, 'Hydroxylation-K AUC PR': 0.40992430442441496, 'Hydroxylation-K MCC': 0.0537317249762414, 'Hydroxylation-K F1': 0.23928147579263198, 'Validation Loss (Hydroxylation-K)': 0.5557490269343058, 'Hydroxylation-P Validation Accuracy': 0.4549568719015955, 'Hydroxylation-P Validation Sensitivity': 0.5663492063492064, 'Hydroxylation-P Validation Specificity': 0.4316724026135967, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5735034403805914, 'Hydroxylation-P AUC PR': 0.26271056374477725, 'Hydroxylation-P MCC': -0.0019098888018073302, 'Hydroxylation-P F1': 0.18684202012453, 'Validation Loss (Hydroxylation-P)': 0.5303621610005697, 'Validation Loss (total)': 1.0861111720403036, 'TimeToTrain': 0.22246605555216473}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014784941977442072,
 'learning_rate_Hydroxylation-K': 0.00230545299195537,
 'learning_rate_Hydroxylation-P': 0.007241029954947504,
 'log_base': 2.7004697798865878,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3176981156,
 'sample_weights': [100.5322140292007, 12.540415731313754],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.24071392247151047,
 'weight_decay_Hydroxylation-K': 9.94639752906256,
 'weight_decay_Hydroxylation-P': 4.4382413728625}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.956
[2,     1] loss: 1264.018
[3,     1] loss: 1262.591
[4,     1] loss: 1263.754
[5,     1] loss: 1264.445
[6,     1] loss: 1261.588
[7,     1] loss: 1261.373
[8,     1] loss: 1260.317
[9,     1] loss: 1261.676
[10,     1] loss: 1256.703
[11,     1] loss: 1257.063
[12,     1] loss: 1253.229
[13,     1] loss: 1248.820
[14,     1] loss: 1244.902
[15,     1] loss: 1240.338
[16,     1] loss: 1228.586
[17,     1] loss: 1218.389
[18,     1] loss: 1198.000
[19,     1] loss: 1186.432
[20,     1] loss: 1150.108
[21,     1] loss: 1141.805
[22,     1] loss: 1139.012
[23,     1] loss: 1114.028
[24,     1] loss: 1129.067
[25,     1] loss: 1107.434
[26,     1] loss: 1076.487
[27,     1] loss: 1091.701
[28,     1] loss: 1020.072
[29,     1] loss: 1023.535
[30,     1] loss: 1075.941
[31,     1] loss: 1012.420
[32,     1] loss: 1016.364
[33,     1] loss: 1045.342
[34,     1] loss: 1023.369
[35,     1] loss: 981.897
[36,     1] loss: 1023.986
[37,     1] loss: 1064.559
[38,     1] loss: 1033.810
[39,     1] loss: 1017.143
[40,     1] loss: 1008.980
[41,     1] loss: 1009.185
[42,     1] loss: 984.107
[43,     1] loss: 955.945
[44,     1] loss: 962.920
[45,     1] loss: 970.167
[46,     1] loss: 989.799
[47,     1] loss: 972.305
[48,     1] loss: 927.222
[49,     1] loss: 925.539
[50,     1] loss: 974.450
[51,     1] loss: 916.313
[52,     1] loss: 950.207
[53,     1] loss: 956.489
[54,     1] loss: 951.454
[55,     1] loss: 958.856
[56,     1] loss: 898.500
[57,     1] loss: 865.978
[58,     1] loss: 883.447
[59,     1] loss: 889.458
[60,     1] loss: 908.988
[61,     1] loss: 925.021
[62,     1] loss: 909.211
[63,     1] loss: 845.559
[64,     1] loss: 909.908
[65,     1] loss: 862.885
[66,     1] loss: 896.421
[67,     1] loss: 856.249
[68,     1] loss: 861.062
[69,     1] loss: 862.426
[70,     1] loss: 838.677
[71,     1] loss: 805.460
[72,     1] loss: 830.200
[73,     1] loss: 789.617
[74,     1] loss: 792.113
[75,     1] loss: 771.320
[76,     1] loss: 800.639
[77,     1] loss: 845.730
[78,     1] loss: 771.480
[79,     1] loss: 828.922
[80,     1] loss: 766.713
[81,     1] loss: 762.219
[82,     1] loss: 750.935
[83,     1] loss: 773.789
[84,     1] loss: 808.829
[85,     1] loss: 758.000
[86,     1] loss: 761.231
[87,     1] loss: 753.438
[88,     1] loss: 774.229
[89,     1] loss: 720.192
[90,     1] loss: 706.536
[91,     1] loss: 765.896
[92,     1] loss: 713.346
[93,     1] loss: 768.494
[94,     1] loss: 737.958
[95,     1] loss: 721.272
[96,     1] loss: 662.266
[97,     1] loss: 684.130
[98,     1] loss: 689.369
[99,     1] loss: 632.137
[100,     1] loss: 636.119
[101,     1] loss: 695.293
[102,     1] loss: 673.333
[103,     1] loss: 711.521
[104,     1] loss: 624.487
[105,     1] loss: 627.245
[106,     1] loss: 591.575
[107,     1] loss: 612.967
[108,     1] loss: 678.683
[109,     1] loss: 588.412
[110,     1] loss: 597.235
[111,     1] loss: 585.031
[112,     1] loss: 625.180
[113,     1] loss: 617.215
[114,     1] loss: 561.517
[115,     1] loss: 532.825
[116,     1] loss: 592.115
[117,     1] loss: 515.260
[118,     1] loss: 608.607
[119,     1] loss: 554.002
[120,     1] loss: 549.194
[121,     1] loss: 546.668
[122,     1] loss: 559.413
[123,     1] loss: 512.329
[124,     1] loss: 537.613
[125,     1] loss: 535.707
[126,     1] loss: 517.864
[127,     1] loss: 621.318
[128,     1] loss: 458.507
[129,     1] loss: 568.499
[130,     1] loss: 492.302
Early stopping applied (best metric=0.3079443573951721)
Finished Training
Total time taken: 19.780431032180786
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.462
[2,     1] loss: 1259.691
[3,     1] loss: 1260.925
[4,     1] loss: 1259.645
[5,     1] loss: 1262.280
[6,     1] loss: 1261.182
[7,     1] loss: 1260.465
[8,     1] loss: 1256.610
[9,     1] loss: 1257.001
[10,     1] loss: 1250.106
[11,     1] loss: 1240.818
[12,     1] loss: 1232.684
[13,     1] loss: 1218.761
[14,     1] loss: 1212.400
[15,     1] loss: 1189.617
[16,     1] loss: 1161.895
[17,     1] loss: 1142.594
[18,     1] loss: 1131.201
[19,     1] loss: 1120.199
[20,     1] loss: 1072.638
[21,     1] loss: 1101.401
[22,     1] loss: 1062.944
[23,     1] loss: 1058.690
[24,     1] loss: 1056.605
[25,     1] loss: 1067.134
[26,     1] loss: 1009.956
[27,     1] loss: 1010.552
[28,     1] loss: 1068.167
[29,     1] loss: 1044.070
[30,     1] loss: 995.217
[31,     1] loss: 1008.435
[32,     1] loss: 999.121
[33,     1] loss: 986.662
[34,     1] loss: 1021.199
[35,     1] loss: 986.503
[36,     1] loss: 1020.206
[37,     1] loss: 1016.547
[38,     1] loss: 1014.932
[39,     1] loss: 962.572
[40,     1] loss: 966.009
[41,     1] loss: 953.898
[42,     1] loss: 960.312
[43,     1] loss: 925.144
[44,     1] loss: 972.144
[45,     1] loss: 972.968
[46,     1] loss: 982.004
[47,     1] loss: 965.951
[48,     1] loss: 968.350
[49,     1] loss: 950.304
[50,     1] loss: 929.336
[51,     1] loss: 926.875
[52,     1] loss: 913.703
[53,     1] loss: 890.224
[54,     1] loss: 946.372
[55,     1] loss: 947.555
[56,     1] loss: 938.623
[57,     1] loss: 907.547
[58,     1] loss: 921.999
[59,     1] loss: 926.879
[60,     1] loss: 877.722
[61,     1] loss: 890.073
[62,     1] loss: 944.417
[63,     1] loss: 895.282
[64,     1] loss: 887.833
[65,     1] loss: 873.379
[66,     1] loss: 835.295
[67,     1] loss: 850.579
[68,     1] loss: 851.165
[69,     1] loss: 792.780
[70,     1] loss: 822.908
[71,     1] loss: 833.994
[72,     1] loss: 853.151
[73,     1] loss: 774.857
[74,     1] loss: 836.989
[75,     1] loss: 849.664
[76,     1] loss: 831.569
[77,     1] loss: 786.715
[78,     1] loss: 819.080
[79,     1] loss: 758.423
[80,     1] loss: 765.499
[81,     1] loss: 783.340
[82,     1] loss: 758.999
[83,     1] loss: 798.000
[84,     1] loss: 750.769
Early stopping applied (best metric=0.34431174397468567)
Finished Training
Total time taken: 12.9119393825531
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.077
[2,     1] loss: 1269.275
[3,     1] loss: 1261.767
[4,     1] loss: 1263.065
[5,     1] loss: 1260.038
[6,     1] loss: 1260.327
[7,     1] loss: 1257.301
[8,     1] loss: 1257.465
[9,     1] loss: 1259.637
[10,     1] loss: 1251.062
[11,     1] loss: 1249.106
[12,     1] loss: 1242.083
[13,     1] loss: 1229.555
[14,     1] loss: 1209.559
[15,     1] loss: 1198.737
[16,     1] loss: 1190.767
[17,     1] loss: 1145.716
[18,     1] loss: 1139.575
[19,     1] loss: 1105.248
[20,     1] loss: 1112.325
[21,     1] loss: 1108.349
[22,     1] loss: 1053.129
[23,     1] loss: 1023.522
[24,     1] loss: 1044.337
[25,     1] loss: 1074.927
[26,     1] loss: 1034.633
[27,     1] loss: 1021.402
[28,     1] loss: 1036.876
[29,     1] loss: 1068.362
[30,     1] loss: 1030.467
[31,     1] loss: 1049.237
[32,     1] loss: 1003.149
[33,     1] loss: 993.125
[34,     1] loss: 1025.878
[35,     1] loss: 987.036
[36,     1] loss: 962.154
[37,     1] loss: 1036.609
[38,     1] loss: 998.285
[39,     1] loss: 987.709
[40,     1] loss: 1028.151
[41,     1] loss: 969.530
[42,     1] loss: 937.186
[43,     1] loss: 949.134
[44,     1] loss: 928.349
[45,     1] loss: 949.806
[46,     1] loss: 918.476
[47,     1] loss: 893.139
[48,     1] loss: 920.516
[49,     1] loss: 891.899
[50,     1] loss: 937.043
[51,     1] loss: 896.554
[52,     1] loss: 851.626
[53,     1] loss: 869.595
[54,     1] loss: 916.015
[55,     1] loss: 884.377
[56,     1] loss: 841.079
[57,     1] loss: 851.472
[58,     1] loss: 900.420
[59,     1] loss: 883.256
[60,     1] loss: 852.845
[61,     1] loss: 830.819
[62,     1] loss: 865.423
[63,     1] loss: 835.860
[64,     1] loss: 870.716
[65,     1] loss: 763.995
[66,     1] loss: 807.462
[67,     1] loss: 783.660
[68,     1] loss: 784.555
[69,     1] loss: 825.582
[70,     1] loss: 833.589
[71,     1] loss: 747.290
[72,     1] loss: 761.708
[73,     1] loss: 762.237
[74,     1] loss: 759.983
[75,     1] loss: 741.493
[76,     1] loss: 751.980
[77,     1] loss: 743.331
[78,     1] loss: 725.105
[79,     1] loss: 798.104
[80,     1] loss: 699.753
[81,     1] loss: 772.135
[82,     1] loss: 669.649
[83,     1] loss: 696.557
[84,     1] loss: 724.187
[85,     1] loss: 683.728
[86,     1] loss: 664.283
[87,     1] loss: 712.096
[88,     1] loss: 656.303
[89,     1] loss: 634.207
Early stopping applied (best metric=0.3702622950077057)
Finished Training
Total time taken: 13.434953689575195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.463
[2,     1] loss: 1262.823
[3,     1] loss: 1263.087
[4,     1] loss: 1264.683
[5,     1] loss: 1262.887
[6,     1] loss: 1262.217
[7,     1] loss: 1259.730
[8,     1] loss: 1256.718
[9,     1] loss: 1255.951
[10,     1] loss: 1252.352
[11,     1] loss: 1245.691
[12,     1] loss: 1243.759
[13,     1] loss: 1232.937
[14,     1] loss: 1222.869
[15,     1] loss: 1205.651
[16,     1] loss: 1201.387
[17,     1] loss: 1180.749
[18,     1] loss: 1152.491
[19,     1] loss: 1133.815
[20,     1] loss: 1119.306
[21,     1] loss: 1109.928
[22,     1] loss: 1076.374
[23,     1] loss: 1065.898
[24,     1] loss: 1094.711
[25,     1] loss: 1055.263
[26,     1] loss: 1039.704
[27,     1] loss: 1045.076
[28,     1] loss: 1054.920
[29,     1] loss: 1103.260
[30,     1] loss: 1063.844
[31,     1] loss: 1036.404
[32,     1] loss: 1026.315
[33,     1] loss: 1037.867
[34,     1] loss: 1022.961
[35,     1] loss: 987.623
[36,     1] loss: 1012.941
[37,     1] loss: 984.796
[38,     1] loss: 955.810
[39,     1] loss: 941.259
[40,     1] loss: 953.238
[41,     1] loss: 987.701
[42,     1] loss: 984.760
[43,     1] loss: 903.268
[44,     1] loss: 997.082
[45,     1] loss: 945.669
[46,     1] loss: 905.549
[47,     1] loss: 986.237
[48,     1] loss: 954.071
[49,     1] loss: 959.307
[50,     1] loss: 921.534
[51,     1] loss: 889.678
[52,     1] loss: 939.773
[53,     1] loss: 928.129
[54,     1] loss: 921.537
[55,     1] loss: 918.040
[56,     1] loss: 908.705
[57,     1] loss: 930.677
[58,     1] loss: 838.720
[59,     1] loss: 855.292
[60,     1] loss: 855.111
[61,     1] loss: 881.915
[62,     1] loss: 878.786
[63,     1] loss: 875.414
[64,     1] loss: 771.952
[65,     1] loss: 881.281
[66,     1] loss: 810.057
[67,     1] loss: 843.244
[68,     1] loss: 829.843
[69,     1] loss: 768.344
[70,     1] loss: 814.683
[71,     1] loss: 822.621
[72,     1] loss: 757.794
[73,     1] loss: 802.924
[74,     1] loss: 770.615
[75,     1] loss: 758.017
[76,     1] loss: 789.918
[77,     1] loss: 740.614
[78,     1] loss: 717.314
[79,     1] loss: 727.459
[80,     1] loss: 708.174
[81,     1] loss: 707.056
[82,     1] loss: 721.505
[83,     1] loss: 666.338
[84,     1] loss: 744.522
[85,     1] loss: 686.679
[86,     1] loss: 696.109
[87,     1] loss: 705.376
[88,     1] loss: 697.884
Early stopping applied (best metric=0.26296278834342957)
Finished Training
Total time taken: 13.783107280731201
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1265.348
[2,     1] loss: 1265.055
[3,     1] loss: 1259.455
[4,     1] loss: 1262.885
[5,     1] loss: 1262.079
[6,     1] loss: 1258.290
[7,     1] loss: 1261.821
[8,     1] loss: 1256.213
[9,     1] loss: 1248.210
[10,     1] loss: 1244.859
[11,     1] loss: 1238.341
[12,     1] loss: 1217.112
[13,     1] loss: 1207.648
[14,     1] loss: 1184.767
[15,     1] loss: 1157.010
[16,     1] loss: 1130.209
[17,     1] loss: 1108.447
[18,     1] loss: 1060.941
[19,     1] loss: 1089.311
[20,     1] loss: 1064.427
[21,     1] loss: 1081.767
[22,     1] loss: 1038.232
[23,     1] loss: 1049.144
[24,     1] loss: 1055.447
[25,     1] loss: 1053.212
[26,     1] loss: 1035.042
[27,     1] loss: 1046.122
[28,     1] loss: 1030.755
[29,     1] loss: 1013.048
[30,     1] loss: 986.847
[31,     1] loss: 1064.628
[32,     1] loss: 1010.834
[33,     1] loss: 1013.427
[34,     1] loss: 1000.512
[35,     1] loss: 971.458
[36,     1] loss: 1006.757
[37,     1] loss: 972.266
[38,     1] loss: 1011.918
[39,     1] loss: 1015.767
[40,     1] loss: 958.559
[41,     1] loss: 991.821
[42,     1] loss: 978.784
[43,     1] loss: 970.871
[44,     1] loss: 969.171
[45,     1] loss: 972.875
[46,     1] loss: 911.568
[47,     1] loss: 952.416
[48,     1] loss: 904.523
[49,     1] loss: 945.581
[50,     1] loss: 908.642
[51,     1] loss: 920.261
[52,     1] loss: 924.319
[53,     1] loss: 897.594
[54,     1] loss: 875.671
[55,     1] loss: 906.438
[56,     1] loss: 831.195
[57,     1] loss: 855.610
[58,     1] loss: 870.084
[59,     1] loss: 902.053
[60,     1] loss: 852.055
[61,     1] loss: 908.622
[62,     1] loss: 869.011
[63,     1] loss: 853.932
[64,     1] loss: 848.455
[65,     1] loss: 852.370
[66,     1] loss: 828.016
[67,     1] loss: 842.918
[68,     1] loss: 840.524
[69,     1] loss: 820.076
[70,     1] loss: 846.105
[71,     1] loss: 789.437
[72,     1] loss: 845.951
[73,     1] loss: 802.916
[74,     1] loss: 778.063
[75,     1] loss: 836.168
[76,     1] loss: 777.111
[77,     1] loss: 778.111
[78,     1] loss: 762.681
[79,     1] loss: 724.679
[80,     1] loss: 786.293
[81,     1] loss: 788.321
[82,     1] loss: 727.337
[83,     1] loss: 775.631
[84,     1] loss: 726.389
[85,     1] loss: 675.641
[86,     1] loss: 713.250
[87,     1] loss: 692.603
[88,     1] loss: 754.067
[89,     1] loss: 682.633
[90,     1] loss: 676.615
[91,     1] loss: 678.859
[92,     1] loss: 638.468
[93,     1] loss: 695.760
[94,     1] loss: 663.357
[95,     1] loss: 673.273
[96,     1] loss: 680.597
[97,     1] loss: 665.344
[98,     1] loss: 674.430
Early stopping applied (best metric=0.3440137505531311)
Finished Training
Total time taken: 16.088078498840332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.116
[2,     1] loss: 1262.414
[3,     1] loss: 1266.656
[4,     1] loss: 1263.394
[5,     1] loss: 1263.552
[6,     1] loss: 1262.523
[7,     1] loss: 1258.119
[8,     1] loss: 1258.368
[9,     1] loss: 1253.354
[10,     1] loss: 1249.793
[11,     1] loss: 1249.092
[12,     1] loss: 1239.345
[13,     1] loss: 1233.122
[14,     1] loss: 1206.495
[15,     1] loss: 1196.688
[16,     1] loss: 1177.884
[17,     1] loss: 1169.792
[18,     1] loss: 1144.363
[19,     1] loss: 1121.001
[20,     1] loss: 1126.283
[21,     1] loss: 1111.885
[22,     1] loss: 1072.539
[23,     1] loss: 1106.740
[24,     1] loss: 1084.189
[25,     1] loss: 1062.031
[26,     1] loss: 1078.539
[27,     1] loss: 1063.621
[28,     1] loss: 1009.132
[29,     1] loss: 1007.815
[30,     1] loss: 1039.037
[31,     1] loss: 998.427
[32,     1] loss: 997.741
[33,     1] loss: 1003.975
[34,     1] loss: 1002.866
[35,     1] loss: 1021.179
[36,     1] loss: 996.083
[37,     1] loss: 1004.921
[38,     1] loss: 975.222
[39,     1] loss: 959.725
[40,     1] loss: 977.632
[41,     1] loss: 963.697
[42,     1] loss: 944.985
[43,     1] loss: 985.417
[44,     1] loss: 962.090
[45,     1] loss: 1010.971
[46,     1] loss: 946.843
[47,     1] loss: 954.833
[48,     1] loss: 904.897
[49,     1] loss: 917.211
[50,     1] loss: 920.510
[51,     1] loss: 920.345
[52,     1] loss: 919.872
[53,     1] loss: 878.905
[54,     1] loss: 904.233
[55,     1] loss: 909.238
[56,     1] loss: 839.098
[57,     1] loss: 866.539
[58,     1] loss: 872.073
[59,     1] loss: 855.051
[60,     1] loss: 876.186
[61,     1] loss: 794.850
[62,     1] loss: 835.518
[63,     1] loss: 844.374
[64,     1] loss: 808.108
[65,     1] loss: 819.624
[66,     1] loss: 800.558
[67,     1] loss: 816.061
[68,     1] loss: 818.547
[69,     1] loss: 777.795
[70,     1] loss: 820.956
[71,     1] loss: 826.652
[72,     1] loss: 754.215
[73,     1] loss: 844.589
[74,     1] loss: 810.495
[75,     1] loss: 864.810
[76,     1] loss: 760.545
[77,     1] loss: 760.418
[78,     1] loss: 760.767
[79,     1] loss: 743.100
[80,     1] loss: 748.439
[81,     1] loss: 724.437
[82,     1] loss: 711.005
[83,     1] loss: 725.849
[84,     1] loss: 747.318
[85,     1] loss: 745.927
[86,     1] loss: 704.506
[87,     1] loss: 701.899
[88,     1] loss: 653.057
[89,     1] loss: 715.499
Early stopping applied (best metric=0.35158759355545044)
Finished Training
Total time taken: 13.261692762374878
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.878
[2,     1] loss: 1265.574
[3,     1] loss: 1264.241
[4,     1] loss: 1263.293
[5,     1] loss: 1266.447
[6,     1] loss: 1260.666
[7,     1] loss: 1263.022
[8,     1] loss: 1262.354
[9,     1] loss: 1257.135
[10,     1] loss: 1260.571
[11,     1] loss: 1255.430
[12,     1] loss: 1251.985
[13,     1] loss: 1246.455
[14,     1] loss: 1238.819
[15,     1] loss: 1229.065
[16,     1] loss: 1221.628
[17,     1] loss: 1201.565
[18,     1] loss: 1177.133
[19,     1] loss: 1171.231
[20,     1] loss: 1132.115
[21,     1] loss: 1109.101
[22,     1] loss: 1103.127
[23,     1] loss: 1103.629
[24,     1] loss: 1058.147
[25,     1] loss: 1059.827
[26,     1] loss: 1055.397
[27,     1] loss: 1064.848
[28,     1] loss: 1023.105
[29,     1] loss: 1066.059
[30,     1] loss: 1055.873
[31,     1] loss: 1040.284
[32,     1] loss: 1066.258
[33,     1] loss: 1007.232
[34,     1] loss: 1042.957
[35,     1] loss: 1076.223
[36,     1] loss: 1034.069
[37,     1] loss: 1040.081
[38,     1] loss: 1062.594
[39,     1] loss: 1005.331
[40,     1] loss: 999.174
[41,     1] loss: 968.037
[42,     1] loss: 975.123
[43,     1] loss: 985.802
[44,     1] loss: 957.681
[45,     1] loss: 940.426
[46,     1] loss: 971.246
[47,     1] loss: 921.583
[48,     1] loss: 969.667
[49,     1] loss: 972.014
[50,     1] loss: 961.134
[51,     1] loss: 920.608
[52,     1] loss: 940.774
[53,     1] loss: 924.567
[54,     1] loss: 927.376
[55,     1] loss: 933.092
[56,     1] loss: 915.600
[57,     1] loss: 888.954
[58,     1] loss: 898.219
[59,     1] loss: 888.655
[60,     1] loss: 876.936
[61,     1] loss: 880.835
[62,     1] loss: 876.232
[63,     1] loss: 860.116
[64,     1] loss: 887.349
[65,     1] loss: 834.081
[66,     1] loss: 867.829
[67,     1] loss: 869.453
[68,     1] loss: 864.881
[69,     1] loss: 835.632
[70,     1] loss: 788.961
[71,     1] loss: 857.370
[72,     1] loss: 788.236
[73,     1] loss: 800.087
[74,     1] loss: 789.425
[75,     1] loss: 798.778
[76,     1] loss: 842.284
[77,     1] loss: 754.376
[78,     1] loss: 787.130
[79,     1] loss: 781.957
[80,     1] loss: 749.554
[81,     1] loss: 793.764
[82,     1] loss: 757.907
[83,     1] loss: 771.478
[84,     1] loss: 797.222
[85,     1] loss: 737.412
[86,     1] loss: 710.024
[87,     1] loss: 684.476
[88,     1] loss: 721.288
[89,     1] loss: 698.224
[90,     1] loss: 712.671
[91,     1] loss: 717.452
[92,     1] loss: 724.936
[93,     1] loss: 710.236
[94,     1] loss: 669.889
[95,     1] loss: 653.791
[96,     1] loss: 624.628
[97,     1] loss: 616.025
[98,     1] loss: 594.987
[99,     1] loss: 609.250
[100,     1] loss: 648.225
[101,     1] loss: 639.255
[102,     1] loss: 646.616
[103,     1] loss: 609.487
[104,     1] loss: 594.224
[105,     1] loss: 620.962
[106,     1] loss: 619.323
[107,     1] loss: 581.226
[108,     1] loss: 590.438
[109,     1] loss: 594.344
[110,     1] loss: 566.531
[111,     1] loss: 543.974
[112,     1] loss: 531.519
[113,     1] loss: 504.934
[114,     1] loss: 575.115
[115,     1] loss: 546.534
[116,     1] loss: 542.136
[117,     1] loss: 530.095
[118,     1] loss: 569.411
[119,     1] loss: 499.392
[120,     1] loss: 507.890
[121,     1] loss: 560.070
[122,     1] loss: 559.834
[123,     1] loss: 516.068
[124,     1] loss: 524.618
[125,     1] loss: 530.071
[126,     1] loss: 472.582
[127,     1] loss: 492.878
[128,     1] loss: 518.604
[129,     1] loss: 534.847
[130,     1] loss: 451.060
[131,     1] loss: 499.582
[132,     1] loss: 507.992
[133,     1] loss: 491.598
Early stopping applied (best metric=0.2901444435119629)
Finished Training
Total time taken: 19.74364995956421
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.482
[2,     1] loss: 1265.448
[3,     1] loss: 1261.997
[4,     1] loss: 1261.641
[5,     1] loss: 1261.883
[6,     1] loss: 1261.210
[7,     1] loss: 1260.524
[8,     1] loss: 1259.707
[9,     1] loss: 1259.630
[10,     1] loss: 1262.146
[11,     1] loss: 1254.859
[12,     1] loss: 1252.760
[13,     1] loss: 1247.021
[14,     1] loss: 1240.743
[15,     1] loss: 1224.275
[16,     1] loss: 1215.000
[17,     1] loss: 1201.283
[18,     1] loss: 1182.188
[19,     1] loss: 1157.525
[20,     1] loss: 1149.778
[21,     1] loss: 1110.167
[22,     1] loss: 1109.503
[23,     1] loss: 1089.156
[24,     1] loss: 1053.633
[25,     1] loss: 1076.696
[26,     1] loss: 1052.045
[27,     1] loss: 1047.892
[28,     1] loss: 1056.609
[29,     1] loss: 1032.800
[30,     1] loss: 1068.188
[31,     1] loss: 1048.925
[32,     1] loss: 1043.321
[33,     1] loss: 1011.454
[34,     1] loss: 1006.789
[35,     1] loss: 1046.686
[36,     1] loss: 1014.642
[37,     1] loss: 1020.859
[38,     1] loss: 1008.706
[39,     1] loss: 1016.170
[40,     1] loss: 968.144
[41,     1] loss: 971.003
[42,     1] loss: 967.003
[43,     1] loss: 937.910
[44,     1] loss: 939.280
[45,     1] loss: 944.303
[46,     1] loss: 936.284
[47,     1] loss: 955.453
[48,     1] loss: 953.793
[49,     1] loss: 899.811
[50,     1] loss: 964.586
[51,     1] loss: 916.188
[52,     1] loss: 894.721
[53,     1] loss: 936.648
[54,     1] loss: 892.060
[55,     1] loss: 918.638
[56,     1] loss: 888.621
[57,     1] loss: 890.899
[58,     1] loss: 861.359
[59,     1] loss: 831.431
[60,     1] loss: 885.466
[61,     1] loss: 876.018
[62,     1] loss: 830.066
[63,     1] loss: 929.482
[64,     1] loss: 827.990
[65,     1] loss: 828.143
[66,     1] loss: 833.732
[67,     1] loss: 809.949
[68,     1] loss: 769.474
[69,     1] loss: 786.278
[70,     1] loss: 830.938
[71,     1] loss: 797.514
[72,     1] loss: 824.677
[73,     1] loss: 791.128
[74,     1] loss: 816.405
[75,     1] loss: 776.561
[76,     1] loss: 790.540
[77,     1] loss: 794.347
[78,     1] loss: 736.764
[79,     1] loss: 768.438
[80,     1] loss: 756.251
[81,     1] loss: 785.715
[82,     1] loss: 700.315
[83,     1] loss: 721.211
[84,     1] loss: 702.730
[85,     1] loss: 666.945
[86,     1] loss: 681.043
[87,     1] loss: 802.679
[88,     1] loss: 686.859
[89,     1] loss: 715.134
[90,     1] loss: 671.766
[91,     1] loss: 735.876
[92,     1] loss: 703.001
[93,     1] loss: 664.088
[94,     1] loss: 662.801
[95,     1] loss: 638.955
[96,     1] loss: 669.206
[97,     1] loss: 675.385
[98,     1] loss: 653.200
[99,     1] loss: 627.218
[100,     1] loss: 612.216
[101,     1] loss: 578.391
[102,     1] loss: 622.036
[103,     1] loss: 581.204
[104,     1] loss: 617.752
[105,     1] loss: 606.618
[106,     1] loss: 605.616
[107,     1] loss: 605.617
[108,     1] loss: 579.959
[109,     1] loss: 575.239
[110,     1] loss: 601.165
[111,     1] loss: 615.303
[112,     1] loss: 557.071
[113,     1] loss: 572.606
[114,     1] loss: 588.344
[115,     1] loss: 537.408
[116,     1] loss: 568.086
[117,     1] loss: 516.164
[118,     1] loss: 544.919
[119,     1] loss: 565.093
[120,     1] loss: 499.882
[121,     1] loss: 487.462
[122,     1] loss: 577.070
[123,     1] loss: 541.127
[124,     1] loss: 540.692
[125,     1] loss: 536.212
[126,     1] loss: 549.886
[127,     1] loss: 521.929
Early stopping applied (best metric=0.3504399359226227)
Finished Training
Total time taken: 19.082470417022705
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.876
[2,     1] loss: 1260.058
[3,     1] loss: 1260.600
[4,     1] loss: 1261.289
[5,     1] loss: 1259.035
[6,     1] loss: 1258.300
[7,     1] loss: 1258.049
[8,     1] loss: 1255.037
[9,     1] loss: 1248.790
[10,     1] loss: 1249.779
[11,     1] loss: 1243.745
[12,     1] loss: 1224.521
[13,     1] loss: 1212.912
[14,     1] loss: 1197.558
[15,     1] loss: 1168.242
[16,     1] loss: 1134.804
[17,     1] loss: 1110.789
[18,     1] loss: 1091.987
[19,     1] loss: 1106.810
[20,     1] loss: 1105.925
[21,     1] loss: 1045.592
[22,     1] loss: 1035.530
[23,     1] loss: 1024.957
[24,     1] loss: 1006.520
[25,     1] loss: 1040.541
[26,     1] loss: 1053.604
[27,     1] loss: 997.829
[28,     1] loss: 1051.365
[29,     1] loss: 1002.068
[30,     1] loss: 996.310
[31,     1] loss: 1014.744
[32,     1] loss: 1012.049
[33,     1] loss: 1012.371
[34,     1] loss: 1033.415
[35,     1] loss: 1035.630
[36,     1] loss: 1012.612
[37,     1] loss: 988.924
[38,     1] loss: 995.465
[39,     1] loss: 957.804
[40,     1] loss: 952.342
[41,     1] loss: 971.059
[42,     1] loss: 985.002
[43,     1] loss: 959.368
[44,     1] loss: 924.675
[45,     1] loss: 930.229
[46,     1] loss: 962.995
[47,     1] loss: 919.689
[48,     1] loss: 912.923
[49,     1] loss: 894.469
[50,     1] loss: 898.718
[51,     1] loss: 905.035
[52,     1] loss: 849.393
[53,     1] loss: 837.404
[54,     1] loss: 932.554
[55,     1] loss: 911.794
[56,     1] loss: 865.607
[57,     1] loss: 839.414
[58,     1] loss: 855.529
[59,     1] loss: 891.005
[60,     1] loss: 832.443
[61,     1] loss: 883.718
[62,     1] loss: 828.606
[63,     1] loss: 785.904
[64,     1] loss: 814.767
[65,     1] loss: 793.746
[66,     1] loss: 832.116
[67,     1] loss: 759.247
[68,     1] loss: 763.738
[69,     1] loss: 772.227
[70,     1] loss: 822.660
[71,     1] loss: 775.185
[72,     1] loss: 806.509
[73,     1] loss: 777.719
[74,     1] loss: 762.737
[75,     1] loss: 733.485
[76,     1] loss: 748.035
[77,     1] loss: 724.414
[78,     1] loss: 741.825
[79,     1] loss: 706.637
[80,     1] loss: 664.711
[81,     1] loss: 707.291
[82,     1] loss: 672.962
[83,     1] loss: 614.714
[84,     1] loss: 681.383
[85,     1] loss: 680.197
[86,     1] loss: 658.004
[87,     1] loss: 660.578
[88,     1] loss: 686.467
[89,     1] loss: 638.549
[90,     1] loss: 613.367
[91,     1] loss: 616.861
[92,     1] loss: 654.319
[93,     1] loss: 610.743
[94,     1] loss: 613.228
[95,     1] loss: 642.887
Early stopping applied (best metric=0.33989217877388)
Finished Training
Total time taken: 14.315131425857544
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1264.687
[2,     1] loss: 1267.044
[3,     1] loss: 1265.302
[4,     1] loss: 1263.831
[5,     1] loss: 1262.291
[6,     1] loss: 1261.400
[7,     1] loss: 1257.137
[8,     1] loss: 1256.784
[9,     1] loss: 1255.922
[10,     1] loss: 1249.658
[11,     1] loss: 1248.663
[12,     1] loss: 1240.127
[13,     1] loss: 1228.326
[14,     1] loss: 1210.089
[15,     1] loss: 1190.922
[16,     1] loss: 1182.883
[17,     1] loss: 1159.078
[18,     1] loss: 1148.504
[19,     1] loss: 1133.247
[20,     1] loss: 1101.187
[21,     1] loss: 1112.640
[22,     1] loss: 1086.896
[23,     1] loss: 1081.542
[24,     1] loss: 1086.495
[25,     1] loss: 1057.267
[26,     1] loss: 1044.359
[27,     1] loss: 1051.417
[28,     1] loss: 1052.755
[29,     1] loss: 1019.032
[30,     1] loss: 1041.004
[31,     1] loss: 1018.112
[32,     1] loss: 976.361
[33,     1] loss: 996.110
[34,     1] loss: 1009.053
[35,     1] loss: 997.009
[36,     1] loss: 983.715
[37,     1] loss: 977.646
[38,     1] loss: 951.170
[39,     1] loss: 971.290
[40,     1] loss: 979.101
[41,     1] loss: 974.269
[42,     1] loss: 960.838
[43,     1] loss: 972.083
[44,     1] loss: 949.721
[45,     1] loss: 966.946
[46,     1] loss: 941.840
[47,     1] loss: 984.325
[48,     1] loss: 929.107
[49,     1] loss: 922.908
[50,     1] loss: 920.017
[51,     1] loss: 935.946
[52,     1] loss: 941.130
[53,     1] loss: 918.996
[54,     1] loss: 864.816
[55,     1] loss: 839.429
[56,     1] loss: 890.231
[57,     1] loss: 893.549
[58,     1] loss: 843.786
[59,     1] loss: 859.900
[60,     1] loss: 845.271
[61,     1] loss: 879.358
[62,     1] loss: 784.985
[63,     1] loss: 867.321
[64,     1] loss: 804.669
[65,     1] loss: 843.066
[66,     1] loss: 793.466
[67,     1] loss: 806.869
[68,     1] loss: 793.265
[69,     1] loss: 807.022
[70,     1] loss: 810.664
[71,     1] loss: 801.664
[72,     1] loss: 773.360
[73,     1] loss: 794.624
[74,     1] loss: 774.132
[75,     1] loss: 744.897
[76,     1] loss: 730.360
[77,     1] loss: 727.063
[78,     1] loss: 729.719
[79,     1] loss: 717.649
[80,     1] loss: 755.745
[81,     1] loss: 696.141
[82,     1] loss: 677.774
[83,     1] loss: 684.276
[84,     1] loss: 684.498
[85,     1] loss: 735.171
[86,     1] loss: 756.871
[87,     1] loss: 673.408
[88,     1] loss: 669.834
[89,     1] loss: 677.869
[90,     1] loss: 680.386
[91,     1] loss: 681.038
[92,     1] loss: 678.263
[93,     1] loss: 690.143
[94,     1] loss: 663.539
[95,     1] loss: 651.321
[96,     1] loss: 634.929
[97,     1] loss: 621.117
[98,     1] loss: 632.858
[99,     1] loss: 597.067
[100,     1] loss: 628.197
[101,     1] loss: 638.692
[102,     1] loss: 586.462
[103,     1] loss: 596.161
Early stopping applied (best metric=0.36688169836997986)
Finished Training
Total time taken: 15.755382299423218
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.191
[2,     1] loss: 1261.272
[3,     1] loss: 1263.032
[4,     1] loss: 1261.144
[5,     1] loss: 1260.736
[6,     1] loss: 1260.821
[7,     1] loss: 1258.227
[8,     1] loss: 1260.587
[9,     1] loss: 1259.050
[10,     1] loss: 1253.298
[11,     1] loss: 1248.088
[12,     1] loss: 1243.389
[13,     1] loss: 1239.464
[14,     1] loss: 1230.359
[15,     1] loss: 1206.507
[16,     1] loss: 1187.646
[17,     1] loss: 1185.041
[18,     1] loss: 1172.648
[19,     1] loss: 1139.481
[20,     1] loss: 1136.125
[21,     1] loss: 1115.319
[22,     1] loss: 1104.136
[23,     1] loss: 1104.467
[24,     1] loss: 1100.023
[25,     1] loss: 1085.969
[26,     1] loss: 1070.665
[27,     1] loss: 1056.721
[28,     1] loss: 1054.255
[29,     1] loss: 1049.427
[30,     1] loss: 1084.243
[31,     1] loss: 1036.136
[32,     1] loss: 1020.628
[33,     1] loss: 1010.875
[34,     1] loss: 1064.132
[35,     1] loss: 993.857
[36,     1] loss: 1040.977
[37,     1] loss: 997.808
[38,     1] loss: 1043.479
[39,     1] loss: 1005.379
[40,     1] loss: 988.404
[41,     1] loss: 971.889
[42,     1] loss: 1006.800
[43,     1] loss: 984.439
[44,     1] loss: 968.235
[45,     1] loss: 931.090
[46,     1] loss: 965.663
[47,     1] loss: 930.666
[48,     1] loss: 946.848
[49,     1] loss: 945.184
[50,     1] loss: 937.038
[51,     1] loss: 914.188
[52,     1] loss: 899.137
[53,     1] loss: 933.660
[54,     1] loss: 918.238
[55,     1] loss: 971.015
[56,     1] loss: 886.814
[57,     1] loss: 873.668
[58,     1] loss: 867.073
[59,     1] loss: 909.770
[60,     1] loss: 866.531
[61,     1] loss: 826.475
[62,     1] loss: 900.234
[63,     1] loss: 836.884
[64,     1] loss: 801.766
[65,     1] loss: 850.147
[66,     1] loss: 831.804
[67,     1] loss: 847.873
[68,     1] loss: 839.838
[69,     1] loss: 787.761
[70,     1] loss: 797.637
[71,     1] loss: 778.583
[72,     1] loss: 811.781
[73,     1] loss: 809.439
[74,     1] loss: 782.660
[75,     1] loss: 782.513
[76,     1] loss: 751.860
[77,     1] loss: 766.615
[78,     1] loss: 718.431
[79,     1] loss: 750.096
[80,     1] loss: 738.704
[81,     1] loss: 729.640
[82,     1] loss: 701.285
[83,     1] loss: 755.563
[84,     1] loss: 725.639
[85,     1] loss: 702.643
[86,     1] loss: 693.235
[87,     1] loss: 714.129
[88,     1] loss: 694.099
[89,     1] loss: 725.583
[90,     1] loss: 730.784
[91,     1] loss: 704.474
[92,     1] loss: 674.396
[93,     1] loss: 690.063
[94,     1] loss: 642.070
[95,     1] loss: 694.104
[96,     1] loss: 622.199
[97,     1] loss: 649.307
[98,     1] loss: 639.520
[99,     1] loss: 625.143
[100,     1] loss: 616.145
[101,     1] loss: 608.545
[102,     1] loss: 685.006
[103,     1] loss: 664.804
[104,     1] loss: 601.304
Early stopping applied (best metric=0.30923017859458923)
Finished Training
Total time taken: 15.459250926971436
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.749
[2,     1] loss: 1265.055
[3,     1] loss: 1263.173
[4,     1] loss: 1261.890
[5,     1] loss: 1260.365
[6,     1] loss: 1261.389
[7,     1] loss: 1257.945
[8,     1] loss: 1254.850
[9,     1] loss: 1241.554
[10,     1] loss: 1234.445
[11,     1] loss: 1227.448
[12,     1] loss: 1207.274
[13,     1] loss: 1191.789
[14,     1] loss: 1167.635
[15,     1] loss: 1145.472
[16,     1] loss: 1114.619
[17,     1] loss: 1090.126
[18,     1] loss: 1074.379
[19,     1] loss: 1051.778
[20,     1] loss: 1079.081
[21,     1] loss: 1042.213
[22,     1] loss: 1027.792
[23,     1] loss: 1055.036
[24,     1] loss: 1010.878
[25,     1] loss: 1043.345
[26,     1] loss: 1077.091
[27,     1] loss: 992.288
[28,     1] loss: 1052.650
[29,     1] loss: 1015.558
[30,     1] loss: 989.232
[31,     1] loss: 1009.889
[32,     1] loss: 1007.097
[33,     1] loss: 995.147
[34,     1] loss: 968.050
[35,     1] loss: 975.900
[36,     1] loss: 992.610
[37,     1] loss: 981.928
[38,     1] loss: 961.029
[39,     1] loss: 990.591
[40,     1] loss: 953.420
[41,     1] loss: 922.908
[42,     1] loss: 957.533
[43,     1] loss: 953.646
[44,     1] loss: 915.103
[45,     1] loss: 929.254
[46,     1] loss: 906.892
[47,     1] loss: 916.839
[48,     1] loss: 962.490
[49,     1] loss: 933.847
[50,     1] loss: 903.135
[51,     1] loss: 857.934
[52,     1] loss: 902.121
[53,     1] loss: 874.886
[54,     1] loss: 920.633
[55,     1] loss: 883.607
[56,     1] loss: 861.816
[57,     1] loss: 799.449
[58,     1] loss: 872.822
[59,     1] loss: 850.337
[60,     1] loss: 842.160
[61,     1] loss: 838.776
[62,     1] loss: 865.363
[63,     1] loss: 858.851
[64,     1] loss: 780.142
[65,     1] loss: 847.060
[66,     1] loss: 821.555
[67,     1] loss: 782.827
[68,     1] loss: 788.731
[69,     1] loss: 797.050
[70,     1] loss: 783.116
[71,     1] loss: 751.041
[72,     1] loss: 816.647
[73,     1] loss: 769.908
[74,     1] loss: 787.363
[75,     1] loss: 740.967
[76,     1] loss: 729.674
[77,     1] loss: 754.816
[78,     1] loss: 752.373
[79,     1] loss: 769.390
[80,     1] loss: 753.929
[81,     1] loss: 669.438
[82,     1] loss: 741.614
[83,     1] loss: 691.745
[84,     1] loss: 689.719
[85,     1] loss: 669.253
[86,     1] loss: 673.661
[87,     1] loss: 678.281
[88,     1] loss: 718.563
[89,     1] loss: 643.769
[90,     1] loss: 642.644
[91,     1] loss: 649.491
[92,     1] loss: 623.447
[93,     1] loss: 693.884
[94,     1] loss: 588.834
[95,     1] loss: 608.319
[96,     1] loss: 630.387
[97,     1] loss: 607.569
[98,     1] loss: 595.576
[99,     1] loss: 562.515
[100,     1] loss: 592.117
[101,     1] loss: 622.330
[102,     1] loss: 620.598
[103,     1] loss: 563.288
[104,     1] loss: 558.752
[105,     1] loss: 584.290
[106,     1] loss: 539.157
[107,     1] loss: 555.424
[108,     1] loss: 526.425
[109,     1] loss: 576.183
[110,     1] loss: 571.603
[111,     1] loss: 587.152
[112,     1] loss: 501.281
[113,     1] loss: 497.061
[114,     1] loss: 550.641
[115,     1] loss: 566.323
[116,     1] loss: 541.805
[117,     1] loss: 569.068
[118,     1] loss: 555.672
[119,     1] loss: 510.362
[120,     1] loss: 563.567
[121,     1] loss: 505.420
[122,     1] loss: 518.262
[123,     1] loss: 542.847
[124,     1] loss: 498.349
[125,     1] loss: 521.319
[126,     1] loss: 522.328
[127,     1] loss: 487.758
[128,     1] loss: 485.873
[129,     1] loss: 495.277
[130,     1] loss: 477.571
[131,     1] loss: 478.895
[132,     1] loss: 448.373
[133,     1] loss: 511.670
[134,     1] loss: 445.296
[135,     1] loss: 434.152
[136,     1] loss: 475.786
[137,     1] loss: 454.042
[138,     1] loss: 453.231
[139,     1] loss: 453.665
[140,     1] loss: 438.739
[141,     1] loss: 417.470
[142,     1] loss: 380.584
[143,     1] loss: 431.539
[144,     1] loss: 413.346
[145,     1] loss: 428.092
[146,     1] loss: 384.909
[147,     1] loss: 391.536
[148,     1] loss: 383.232
[149,     1] loss: 451.916
[150,     1] loss: 392.526
Early stopping applied (best metric=0.36665159463882446)
Finished Training
Total time taken: 22.311729431152344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.376
[2,     1] loss: 1261.882
[3,     1] loss: 1262.117
[4,     1] loss: 1263.601
[5,     1] loss: 1261.510
[6,     1] loss: 1260.801
[7,     1] loss: 1260.700
[8,     1] loss: 1261.988
[9,     1] loss: 1259.760
[10,     1] loss: 1259.710
[11,     1] loss: 1258.531
[12,     1] loss: 1254.993
[13,     1] loss: 1254.014
[14,     1] loss: 1245.336
[15,     1] loss: 1240.492
[16,     1] loss: 1227.373
[17,     1] loss: 1216.456
[18,     1] loss: 1200.946
[19,     1] loss: 1183.816
[20,     1] loss: 1169.847
[21,     1] loss: 1130.261
[22,     1] loss: 1130.615
[23,     1] loss: 1077.976
[24,     1] loss: 1083.713
[25,     1] loss: 1055.517
[26,     1] loss: 1052.167
[27,     1] loss: 1040.399
[28,     1] loss: 1051.597
[29,     1] loss: 1044.725
[30,     1] loss: 1041.886
[31,     1] loss: 1077.277
[32,     1] loss: 1022.903
[33,     1] loss: 1037.995
[34,     1] loss: 1035.704
[35,     1] loss: 1004.653
[36,     1] loss: 1052.943
[37,     1] loss: 1010.603
[38,     1] loss: 1027.126
[39,     1] loss: 999.753
[40,     1] loss: 1013.825
[41,     1] loss: 990.660
[42,     1] loss: 980.581
[43,     1] loss: 988.162
[44,     1] loss: 939.051
[45,     1] loss: 978.561
[46,     1] loss: 949.176
[47,     1] loss: 944.928
[48,     1] loss: 991.634
[49,     1] loss: 967.760
[50,     1] loss: 970.570
[51,     1] loss: 935.555
[52,     1] loss: 864.932
[53,     1] loss: 909.650
[54,     1] loss: 889.201
[55,     1] loss: 908.560
[56,     1] loss: 879.364
[57,     1] loss: 918.537
[58,     1] loss: 913.478
[59,     1] loss: 874.281
[60,     1] loss: 901.360
[61,     1] loss: 869.898
[62,     1] loss: 876.697
[63,     1] loss: 891.369
[64,     1] loss: 885.653
[65,     1] loss: 848.496
[66,     1] loss: 870.776
[67,     1] loss: 902.232
[68,     1] loss: 820.225
[69,     1] loss: 864.786
[70,     1] loss: 805.607
[71,     1] loss: 828.661
[72,     1] loss: 814.610
[73,     1] loss: 827.145
[74,     1] loss: 801.829
[75,     1] loss: 781.256
[76,     1] loss: 804.130
[77,     1] loss: 841.161
[78,     1] loss: 780.864
[79,     1] loss: 796.150
[80,     1] loss: 812.917
[81,     1] loss: 810.627
[82,     1] loss: 770.012
[83,     1] loss: 710.405
[84,     1] loss: 748.520
[85,     1] loss: 723.512
[86,     1] loss: 801.130
[87,     1] loss: 762.704
[88,     1] loss: 710.489
[89,     1] loss: 721.586
[90,     1] loss: 703.246
[91,     1] loss: 705.916
[92,     1] loss: 714.875
[93,     1] loss: 661.607
[94,     1] loss: 709.157
[95,     1] loss: 718.034
[96,     1] loss: 651.083
Early stopping applied (best metric=0.3378201127052307)
Finished Training
Total time taken: 14.862210273742676
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.887
[2,     1] loss: 1265.910
[3,     1] loss: 1260.781
[4,     1] loss: 1260.891
[5,     1] loss: 1260.301
[6,     1] loss: 1261.519
[7,     1] loss: 1258.794
[8,     1] loss: 1250.673
[9,     1] loss: 1253.862
[10,     1] loss: 1252.005
[11,     1] loss: 1242.185
[12,     1] loss: 1228.900
[13,     1] loss: 1210.272
[14,     1] loss: 1190.731
[15,     1] loss: 1161.970
[16,     1] loss: 1146.761
[17,     1] loss: 1133.705
[18,     1] loss: 1063.253
[19,     1] loss: 1089.753
[20,     1] loss: 1029.670
[21,     1] loss: 1059.353
[22,     1] loss: 1055.630
[23,     1] loss: 1035.649
[24,     1] loss: 1071.127
[25,     1] loss: 1049.566
[26,     1] loss: 987.568
[27,     1] loss: 1005.459
[28,     1] loss: 963.776
[29,     1] loss: 1033.700
[30,     1] loss: 1010.517
[31,     1] loss: 985.990
[32,     1] loss: 1015.737
[33,     1] loss: 1015.731
[34,     1] loss: 967.635
[35,     1] loss: 969.125
[36,     1] loss: 934.034
[37,     1] loss: 981.926
[38,     1] loss: 962.045
[39,     1] loss: 945.686
[40,     1] loss: 931.570
[41,     1] loss: 892.367
[42,     1] loss: 942.583
[43,     1] loss: 899.300
[44,     1] loss: 913.837
[45,     1] loss: 889.848
[46,     1] loss: 907.313
[47,     1] loss: 931.090
[48,     1] loss: 879.719
[49,     1] loss: 879.992
[50,     1] loss: 860.103
[51,     1] loss: 850.454
[52,     1] loss: 815.610
[53,     1] loss: 893.383
[54,     1] loss: 876.106
[55,     1] loss: 901.559
[56,     1] loss: 865.046
[57,     1] loss: 827.759
[58,     1] loss: 820.683
[59,     1] loss: 829.765
[60,     1] loss: 833.085
[61,     1] loss: 827.931
[62,     1] loss: 821.537
[63,     1] loss: 778.668
[64,     1] loss: 785.136
[65,     1] loss: 790.482
[66,     1] loss: 768.640
[67,     1] loss: 789.724
[68,     1] loss: 793.957
[69,     1] loss: 759.046
[70,     1] loss: 765.994
[71,     1] loss: 749.419
[72,     1] loss: 763.088
[73,     1] loss: 728.958
[74,     1] loss: 704.775
[75,     1] loss: 759.814
[76,     1] loss: 715.701
[77,     1] loss: 728.855
[78,     1] loss: 681.725
[79,     1] loss: 669.065
[80,     1] loss: 654.052
[81,     1] loss: 725.930
[82,     1] loss: 656.975
[83,     1] loss: 642.771
[84,     1] loss: 662.726
[85,     1] loss: 702.011
[86,     1] loss: 699.909
[87,     1] loss: 610.696
[88,     1] loss: 613.487
[89,     1] loss: 666.348
[90,     1] loss: 584.385
[91,     1] loss: 620.173
[92,     1] loss: 589.304
[93,     1] loss: 601.844
[94,     1] loss: 612.299
[95,     1] loss: 600.142
[96,     1] loss: 589.621
[97,     1] loss: 620.704
[98,     1] loss: 546.504
Early stopping applied (best metric=0.3773411810398102)
Finished Training
Total time taken: 15.511301755905151
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1264.959
[2,     1] loss: 1267.832
[3,     1] loss: 1262.439
[4,     1] loss: 1266.045
[5,     1] loss: 1262.356
[6,     1] loss: 1261.560
[7,     1] loss: 1264.935
[8,     1] loss: 1260.734
[9,     1] loss: 1259.760
[10,     1] loss: 1255.365
[11,     1] loss: 1254.857
[12,     1] loss: 1246.211
[13,     1] loss: 1237.286
[14,     1] loss: 1226.355
[15,     1] loss: 1203.421
[16,     1] loss: 1190.368
[17,     1] loss: 1170.069
[18,     1] loss: 1149.133
[19,     1] loss: 1095.607
[20,     1] loss: 1121.079
[21,     1] loss: 1078.668
[22,     1] loss: 1055.859
[23,     1] loss: 1028.057
[24,     1] loss: 1081.979
[25,     1] loss: 1058.263
[26,     1] loss: 1051.632
[27,     1] loss: 1050.982
[28,     1] loss: 999.915
[29,     1] loss: 1006.921
[30,     1] loss: 1044.453
[31,     1] loss: 1024.657
[32,     1] loss: 1000.437
[33,     1] loss: 978.229
[34,     1] loss: 1068.029
[35,     1] loss: 1018.312
[36,     1] loss: 1001.498
[37,     1] loss: 975.772
[38,     1] loss: 966.367
[39,     1] loss: 963.176
[40,     1] loss: 1023.091
[41,     1] loss: 964.082
[42,     1] loss: 960.882
[43,     1] loss: 959.192
[44,     1] loss: 992.239
[45,     1] loss: 924.408
[46,     1] loss: 929.304
[47,     1] loss: 938.463
[48,     1] loss: 924.002
[49,     1] loss: 916.432
[50,     1] loss: 883.982
[51,     1] loss: 859.545
[52,     1] loss: 914.808
[53,     1] loss: 880.229
[54,     1] loss: 872.705
[55,     1] loss: 874.960
[56,     1] loss: 919.863
[57,     1] loss: 869.910
[58,     1] loss: 895.152
[59,     1] loss: 859.463
[60,     1] loss: 817.672
[61,     1] loss: 817.852
[62,     1] loss: 826.148
[63,     1] loss: 852.363
[64,     1] loss: 827.006
[65,     1] loss: 830.710
[66,     1] loss: 822.692
[67,     1] loss: 825.930
[68,     1] loss: 799.911
[69,     1] loss: 825.829
[70,     1] loss: 788.978
[71,     1] loss: 805.283
[72,     1] loss: 812.997
[73,     1] loss: 767.098
[74,     1] loss: 812.128
[75,     1] loss: 782.174
[76,     1] loss: 812.742
[77,     1] loss: 752.611
[78,     1] loss: 723.129
[79,     1] loss: 782.189
[80,     1] loss: 764.632
[81,     1] loss: 774.850
[82,     1] loss: 725.783
[83,     1] loss: 731.384
[84,     1] loss: 709.009
[85,     1] loss: 657.073
[86,     1] loss: 699.754
[87,     1] loss: 699.022
[88,     1] loss: 695.637
[89,     1] loss: 668.123
[90,     1] loss: 650.954
[91,     1] loss: 691.709
[92,     1] loss: 683.963
[93,     1] loss: 672.352
[94,     1] loss: 610.824
[95,     1] loss: 672.664
[96,     1] loss: 645.831
[97,     1] loss: 712.687
[98,     1] loss: 642.125
[99,     1] loss: 632.417
[100,     1] loss: 641.199
[101,     1] loss: 603.256
[102,     1] loss: 616.983
Early stopping applied (best metric=0.35913825035095215)
Finished Training
Total time taken: 17.17359161376953
{'Hydroxylation-K Validation Accuracy': 0.743321513002364, 'Hydroxylation-K Validation Sensitivity': 0.5814814814814815, 'Hydroxylation-K Validation Specificity': 0.7842105263157895, 'Hydroxylation-K Validation Precision': 0.4160482273717568, 'Hydroxylation-K AUC ROC': 0.7361598440545809, 'Hydroxylation-K AUC PR': 0.5232641963620626, 'Hydroxylation-K MCC': 0.32975894197985856, 'Hydroxylation-K F1': 0.478500973696376, 'Validation Loss (Hydroxylation-K)': 0.5597782949606578, 'Hydroxylation-P Validation Accuracy': 0.8016798639662961, 'Hydroxylation-P Validation Sensitivity': 0.8276190476190476, 'Hydroxylation-P Validation Specificity': 0.7961269888772508, 'Hydroxylation-P Validation Precision': 0.4709068154067037, 'Hydroxylation-P AUC ROC': 0.8756054234585583, 'Hydroxylation-P AUC PR': 0.6215260439329313, 'Hydroxylation-P MCC': 0.5156129072197803, 'Hydroxylation-P F1': 0.5981157210340953, 'Validation Loss (Hydroxylation-P)': 0.3385748068491618, 'Validation Loss (total)': 0.8983530918757121, 'TimeToTrain': 16.231661383310954}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007344674854492954,
 'learning_rate_Hydroxylation-K': 0.008926621673771363,
 'learning_rate_Hydroxylation-P': 0.009509289483722186,
 'log_base': 1.0117933591329795,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2032881329,
 'sample_weights': [1.6817377010395873, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.4856974021240563,
 'weight_decay_Hydroxylation-K': 7.745829649210911,
 'weight_decay_Hydroxylation-P': 4.855781396426853}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46259.148
Exploding loss, terminate run (best metric=0.5312551856040955)
Finished Training
Total time taken: 0.24600934982299805
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46157.746
Exploding loss, terminate run (best metric=0.5343329310417175)
Finished Training
Total time taken: 0.2393655776977539
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46369.898
Exploding loss, terminate run (best metric=0.526960551738739)
Finished Training
Total time taken: 0.2240002155303955
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46105.504
Exploding loss, terminate run (best metric=0.5269898176193237)
Finished Training
Total time taken: 0.24199700355529785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 46407.852
Exploding loss, terminate run (best metric=0.5340503454208374)
Finished Training
Total time taken: 0.2539982795715332
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46184.836
Exploding loss, terminate run (best metric=0.5373595356941223)
Finished Training
Total time taken: 0.25099968910217285
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46469.957
Exploding loss, terminate run (best metric=0.5521862506866455)
Finished Training
Total time taken: 0.26503491401672363
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46312.824
Exploding loss, terminate run (best metric=0.5340807437896729)
Finished Training
Total time taken: 0.25800013542175293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46447.609
Exploding loss, terminate run (best metric=0.5317070484161377)
Finished Training
Total time taken: 0.2610015869140625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 46251.348
Exploding loss, terminate run (best metric=0.5301745533943176)
Finished Training
Total time taken: 0.23999786376953125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46029.824
Exploding loss, terminate run (best metric=0.5464233756065369)
Finished Training
Total time taken: 0.26199889183044434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46576.125
Exploding loss, terminate run (best metric=0.5272758603096008)
Finished Training
Total time taken: 0.2330310344696045
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46258.117
Exploding loss, terminate run (best metric=0.5432798266410828)
Finished Training
Total time taken: 0.2629985809326172
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 46230.461
Exploding loss, terminate run (best metric=0.5290023684501648)
Finished Training
Total time taken: 0.23512887954711914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 46159.285
Exploding loss, terminate run (best metric=0.5379589796066284)
Finished Training
Total time taken: 0.24400091171264648
{'Hydroxylation-K Validation Accuracy': 0.5216903073286052, 'Hydroxylation-K Validation Sensitivity': 0.42962962962962964, 'Hydroxylation-K Validation Specificity': 0.5421052631578948, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5133528265107212, 'Hydroxylation-K AUC PR': 0.2709705492640631, 'Hydroxylation-K MCC': -0.027174579224003004, 'Hydroxylation-K F1': 0.14796530306275435, 'Validation Loss (Hydroxylation-K)': 0.5624076207478841, 'Hydroxylation-P Validation Accuracy': 0.5310853256179889, 'Hydroxylation-P Validation Sensitivity': 0.46476190476190476, 'Hydroxylation-P Validation Specificity': 0.5451219512195122, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5594282935379749, 'Hydroxylation-P AUC PR': 0.25210914778792404, 'Hydroxylation-P MCC': 0.010516673115328456, 'Hydroxylation-P F1': 0.14267226809599692, 'Validation Loss (Hydroxylation-P)': 0.5348691582679749, 'Validation Loss (total)': 1.0972767750422159, 'TimeToTrain': 0.2478375275929769}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007792266123872623,
 'learning_rate_Hydroxylation-K': 0.0037708430809386922,
 'learning_rate_Hydroxylation-P': 0.009496206333005488,
 'log_base': 1.8541286782537765,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3141866230,
 'sample_weights': [142.49661543292004, 17.77506658029945],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4979123369103005,
 'weight_decay_Hydroxylation-K': 8.495670462506787,
 'weight_decay_Hydroxylation-P': 0.04318853265759204}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1489.184
[2,     1] loss: 1480.018
[3,     1] loss: 1478.678
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030687273045512694,
 'learning_rate_Hydroxylation-K': 0.003424095566230155,
 'learning_rate_Hydroxylation-P': 0.0007709925156231638,
 'log_base': 2.2336226592158965,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 832861595,
 'sample_weights': [2.7039244230083814, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.63642709276437,
 'weight_decay_Hydroxylation-K': 8.757864589047701,
 'weight_decay_Hydroxylation-P': 8.889920902822496}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1346.921
[2,     1] loss: 1351.805
[3,     1] loss: 1349.375
[4,     1] loss: 1343.382
[5,     1] loss: 1342.581
[6,     1] loss: 1342.839
[7,     1] loss: 1336.666
[8,     1] loss: 1331.109
[9,     1] loss: 1322.881
[10,     1] loss: 1298.507
[11,     1] loss: 1279.278
[12,     1] loss: 1254.701
[13,     1] loss: 1207.154
[14,     1] loss: 1217.103
[15,     1] loss: 1166.909
[16,     1] loss: 1167.378
[17,     1] loss: 1161.799
[18,     1] loss: 1140.203
[19,     1] loss: 1112.026
[20,     1] loss: 1106.459
[21,     1] loss: 1062.124
[22,     1] loss: 1063.261
[23,     1] loss: 1095.275
[24,     1] loss: 1054.150
[25,     1] loss: 1067.928
[26,     1] loss: 1070.078
[27,     1] loss: 1086.034
[28,     1] loss: 1034.999
[29,     1] loss: 1053.495
[30,     1] loss: 1002.931
[31,     1] loss: 1036.276
[32,     1] loss: 1036.998
[33,     1] loss: 1009.468
[34,     1] loss: 949.294
[35,     1] loss: 967.621
[36,     1] loss: 959.188
[37,     1] loss: 1002.739
[38,     1] loss: 981.614
[39,     1] loss: 903.060
[40,     1] loss: 933.246
[41,     1] loss: 936.857
[42,     1] loss: 911.705
[43,     1] loss: 882.525
[44,     1] loss: 892.315
[45,     1] loss: 887.700
[46,     1] loss: 912.650
[47,     1] loss: 854.584
[48,     1] loss: 907.577
[49,     1] loss: 857.118
[50,     1] loss: 859.240
[51,     1] loss: 779.197
[52,     1] loss: 850.896
[53,     1] loss: 811.106
[54,     1] loss: 826.493
[55,     1] loss: 784.027
[56,     1] loss: 812.238
[57,     1] loss: 816.886
[58,     1] loss: 729.974
[59,     1] loss: 897.806
[60,     1] loss: 798.037
[61,     1] loss: 880.919
[62,     1] loss: 793.892
[63,     1] loss: 775.809
[64,     1] loss: 712.242
[65,     1] loss: 754.334
[66,     1] loss: 714.916
[67,     1] loss: 760.092
[68,     1] loss: 702.721
[69,     1] loss: 711.739
[70,     1] loss: 683.671
[71,     1] loss: 718.047
[72,     1] loss: 664.459
[73,     1] loss: 684.722
[74,     1] loss: 647.505
[75,     1] loss: 620.041
[76,     1] loss: 636.715
[77,     1] loss: 666.627
[78,     1] loss: 611.925
[79,     1] loss: 613.396
[80,     1] loss: 605.581
[81,     1] loss: 636.163
[82,     1] loss: 632.286
[83,     1] loss: 571.822
[84,     1] loss: 580.986
[85,     1] loss: 586.047
[86,     1] loss: 610.051
[87,     1] loss: 581.492
[88,     1] loss: 582.503
[89,     1] loss: 592.737
[90,     1] loss: 545.515
[91,     1] loss: 505.012
[92,     1] loss: 510.414
[93,     1] loss: 559.838
[94,     1] loss: 570.190
[95,     1] loss: 530.678
[96,     1] loss: 518.507
[97,     1] loss: 551.856
[98,     1] loss: 532.353
[99,     1] loss: 548.092
Early stopping applied (best metric=0.3750034272670746)
Finished Training
Total time taken: 16.99021577835083
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1343.448
[2,     1] loss: 1344.886
[3,     1] loss: 1351.108
[4,     1] loss: 1348.097
[5,     1] loss: 1347.267
[6,     1] loss: 1341.796
[7,     1] loss: 1341.280
[8,     1] loss: 1341.687
[9,     1] loss: 1331.540
[10,     1] loss: 1323.611
[11,     1] loss: 1310.310
[12,     1] loss: 1290.127
[13,     1] loss: 1254.105
[14,     1] loss: 1230.468
[15,     1] loss: 1199.206
[16,     1] loss: 1160.398
[17,     1] loss: 1149.464
[18,     1] loss: 1157.450
[19,     1] loss: 1108.422
[20,     1] loss: 1171.471
[21,     1] loss: 1069.143
[22,     1] loss: 1173.352
[23,     1] loss: 1120.459
[24,     1] loss: 1132.412
[25,     1] loss: 1064.260
[26,     1] loss: 1122.742
[27,     1] loss: 1088.597
[28,     1] loss: 1067.690
[29,     1] loss: 1085.534
[30,     1] loss: 1040.356
[31,     1] loss: 1058.753
[32,     1] loss: 1051.909
[33,     1] loss: 1032.224
[34,     1] loss: 1055.309
[35,     1] loss: 988.685
[36,     1] loss: 1064.244
[37,     1] loss: 1011.627
[38,     1] loss: 985.349
[39,     1] loss: 1000.768
[40,     1] loss: 1014.608
[41,     1] loss: 993.278
[42,     1] loss: 950.692
[43,     1] loss: 946.681
[44,     1] loss: 972.688
[45,     1] loss: 956.636
[46,     1] loss: 900.852
[47,     1] loss: 921.378
[48,     1] loss: 928.147
[49,     1] loss: 886.222
[50,     1] loss: 874.515
[51,     1] loss: 855.689
[52,     1] loss: 822.460
[53,     1] loss: 897.292
[54,     1] loss: 839.912
[55,     1] loss: 883.203
[56,     1] loss: 823.164
[57,     1] loss: 773.102
[58,     1] loss: 783.838
[59,     1] loss: 806.054
[60,     1] loss: 841.041
[61,     1] loss: 794.127
[62,     1] loss: 786.897
[63,     1] loss: 829.486
[64,     1] loss: 753.545
[65,     1] loss: 756.349
[66,     1] loss: 730.895
[67,     1] loss: 707.961
[68,     1] loss: 731.892
[69,     1] loss: 738.732
[70,     1] loss: 705.925
[71,     1] loss: 713.356
[72,     1] loss: 592.766
[73,     1] loss: 661.160
[74,     1] loss: 654.113
[75,     1] loss: 654.565
[76,     1] loss: 621.037
[77,     1] loss: 636.805
[78,     1] loss: 637.839
[79,     1] loss: 568.582
[80,     1] loss: 669.082
[81,     1] loss: 621.180
[82,     1] loss: 548.821
[83,     1] loss: 549.421
[84,     1] loss: 597.534
[85,     1] loss: 591.454
[86,     1] loss: 610.326
[87,     1] loss: 545.412
[88,     1] loss: 582.766
[89,     1] loss: 611.040
[90,     1] loss: 575.084
[91,     1] loss: 562.934
[92,     1] loss: 599.249
[93,     1] loss: 503.858
[94,     1] loss: 529.721
[95,     1] loss: 608.749
[96,     1] loss: 464.426
[97,     1] loss: 570.084
[98,     1] loss: 561.775
[99,     1] loss: 527.703
[100,     1] loss: 551.160
[101,     1] loss: 490.990
[102,     1] loss: 535.908
[103,     1] loss: 545.143
[104,     1] loss: 569.810
[105,     1] loss: 517.850
[106,     1] loss: 541.739
[107,     1] loss: 489.029
[108,     1] loss: 510.666
[109,     1] loss: 526.174
[110,     1] loss: 458.116
Early stopping applied (best metric=0.3232022821903229)
Finished Training
Total time taken: 18.872265577316284
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.220
[2,     1] loss: 1344.526
[3,     1] loss: 1341.966
[4,     1] loss: 1339.938
[5,     1] loss: 1333.850
[6,     1] loss: 1332.188
[7,     1] loss: 1323.555
[8,     1] loss: 1290.969
[9,     1] loss: 1274.832
[10,     1] loss: 1216.798
[11,     1] loss: 1208.412
[12,     1] loss: 1150.450
[13,     1] loss: 1157.541
[14,     1] loss: 1159.148
[15,     1] loss: 1171.922
[16,     1] loss: 1119.636
[17,     1] loss: 1118.025
[18,     1] loss: 1114.680
[19,     1] loss: 1075.280
[20,     1] loss: 1105.296
[21,     1] loss: 1089.032
[22,     1] loss: 1125.471
[23,     1] loss: 1101.552
[24,     1] loss: 1075.132
[25,     1] loss: 1050.112
[26,     1] loss: 1066.696
[27,     1] loss: 1018.215
[28,     1] loss: 1026.744
[29,     1] loss: 975.425
[30,     1] loss: 954.896
[31,     1] loss: 1011.642
[32,     1] loss: 945.709
[33,     1] loss: 956.640
[34,     1] loss: 885.524
[35,     1] loss: 928.292
[36,     1] loss: 972.677
[37,     1] loss: 956.220
[38,     1] loss: 866.227
[39,     1] loss: 984.109
[40,     1] loss: 928.816
[41,     1] loss: 938.351
[42,     1] loss: 900.144
[43,     1] loss: 905.522
[44,     1] loss: 867.018
[45,     1] loss: 913.569
[46,     1] loss: 914.732
[47,     1] loss: 838.987
[48,     1] loss: 889.713
[49,     1] loss: 806.479
[50,     1] loss: 874.607
[51,     1] loss: 827.861
[52,     1] loss: 883.373
[53,     1] loss: 836.058
[54,     1] loss: 970.141
[55,     1] loss: 799.155
[56,     1] loss: 956.169
[57,     1] loss: 785.663
[58,     1] loss: 879.008
[59,     1] loss: 847.637
[60,     1] loss: 827.647
[61,     1] loss: 817.203
[62,     1] loss: 721.824
[63,     1] loss: 799.654
[64,     1] loss: 789.077
[65,     1] loss: 747.459
[66,     1] loss: 707.615
[67,     1] loss: 672.283
[68,     1] loss: 738.746
[69,     1] loss: 674.348
[70,     1] loss: 697.291
[71,     1] loss: 652.971
[72,     1] loss: 706.911
[73,     1] loss: 578.005
[74,     1] loss: 667.206
[75,     1] loss: 624.028
[76,     1] loss: 664.690
[77,     1] loss: 631.435
[78,     1] loss: 580.585
[79,     1] loss: 623.025
[80,     1] loss: 644.098
[81,     1] loss: 593.083
Early stopping applied (best metric=0.3368450999259949)
Finished Training
Total time taken: 13.994987964630127
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1346.929
[2,     1] loss: 1347.138
[3,     1] loss: 1342.453
[4,     1] loss: 1348.315
[5,     1] loss: 1344.989
[6,     1] loss: 1337.923
[7,     1] loss: 1338.733
[8,     1] loss: 1335.168
[9,     1] loss: 1324.478
[10,     1] loss: 1313.331
[11,     1] loss: 1282.990
[12,     1] loss: 1252.298
[13,     1] loss: 1220.809
[14,     1] loss: 1222.358
[15,     1] loss: 1163.798
[16,     1] loss: 1217.396
[17,     1] loss: 1133.263
[18,     1] loss: 1180.076
[19,     1] loss: 1089.672
[20,     1] loss: 1140.745
[21,     1] loss: 1104.217
[22,     1] loss: 1126.403
[23,     1] loss: 1115.534
[24,     1] loss: 1105.119
[25,     1] loss: 1103.106
[26,     1] loss: 1041.762
[27,     1] loss: 1074.545
[28,     1] loss: 1083.218
[29,     1] loss: 1046.942
[30,     1] loss: 1043.208
[31,     1] loss: 1092.528
[32,     1] loss: 1051.759
[33,     1] loss: 1006.655
[34,     1] loss: 1016.443
[35,     1] loss: 1028.073
[36,     1] loss: 992.626
[37,     1] loss: 1034.959
[38,     1] loss: 973.529
[39,     1] loss: 979.282
[40,     1] loss: 964.606
[41,     1] loss: 935.243
[42,     1] loss: 958.914
[43,     1] loss: 975.089
[44,     1] loss: 959.150
[45,     1] loss: 918.088
[46,     1] loss: 981.325
[47,     1] loss: 954.086
[48,     1] loss: 979.468
[49,     1] loss: 944.475
[50,     1] loss: 878.334
[51,     1] loss: 953.611
[52,     1] loss: 891.111
[53,     1] loss: 932.394
[54,     1] loss: 874.370
[55,     1] loss: 921.740
[56,     1] loss: 881.265
[57,     1] loss: 826.828
[58,     1] loss: 844.114
[59,     1] loss: 790.663
[60,     1] loss: 815.579
[61,     1] loss: 817.511
[62,     1] loss: 890.014
[63,     1] loss: 803.778
[64,     1] loss: 823.343
[65,     1] loss: 820.597
[66,     1] loss: 756.175
[67,     1] loss: 794.017
[68,     1] loss: 726.670
[69,     1] loss: 737.380
[70,     1] loss: 685.162
[71,     1] loss: 653.887
[72,     1] loss: 700.733
[73,     1] loss: 681.907
[74,     1] loss: 669.358
[75,     1] loss: 632.289
[76,     1] loss: 671.531
[77,     1] loss: 662.379
[78,     1] loss: 662.062
[79,     1] loss: 739.202
[80,     1] loss: 647.772
[81,     1] loss: 601.969
[82,     1] loss: 697.352
[83,     1] loss: 624.996
[84,     1] loss: 641.483
[85,     1] loss: 657.855
[86,     1] loss: 682.086
[87,     1] loss: 587.749
[88,     1] loss: 628.482
[89,     1] loss: 642.915
[90,     1] loss: 539.402
[91,     1] loss: 581.748
[92,     1] loss: 616.987
[93,     1] loss: 526.859
[94,     1] loss: 568.038
[95,     1] loss: 570.719
[96,     1] loss: 542.612
[97,     1] loss: 508.445
[98,     1] loss: 593.822
[99,     1] loss: 558.398
[100,     1] loss: 533.824
[101,     1] loss: 588.436
[102,     1] loss: 549.923
[103,     1] loss: 518.455
[104,     1] loss: 559.303
[105,     1] loss: 491.635
[106,     1] loss: 609.359
[107,     1] loss: 491.642
[108,     1] loss: 545.132
Early stopping applied (best metric=0.3053484857082367)
Finished Training
Total time taken: 17.93208408355713
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1357.370
[2,     1] loss: 1350.220
[3,     1] loss: 1346.745
[4,     1] loss: 1346.628
[5,     1] loss: 1348.354
[6,     1] loss: 1346.989
[7,     1] loss: 1347.099
[8,     1] loss: 1346.197
[9,     1] loss: 1341.016
[10,     1] loss: 1336.996
[11,     1] loss: 1335.369
[12,     1] loss: 1323.094
[13,     1] loss: 1304.494
[14,     1] loss: 1294.596
[15,     1] loss: 1253.985
[16,     1] loss: 1258.657
[17,     1] loss: 1231.659
[18,     1] loss: 1205.342
[19,     1] loss: 1172.944
[20,     1] loss: 1149.174
[21,     1] loss: 1113.771
[22,     1] loss: 1143.958
[23,     1] loss: 1086.943
[24,     1] loss: 1066.866
[25,     1] loss: 1111.103
[26,     1] loss: 1077.050
[27,     1] loss: 1079.433
[28,     1] loss: 1083.238
[29,     1] loss: 1059.148
[30,     1] loss: 1043.450
[31,     1] loss: 1030.372
[32,     1] loss: 1021.253
[33,     1] loss: 1052.348
[34,     1] loss: 1039.349
[35,     1] loss: 1060.288
[36,     1] loss: 928.024
[37,     1] loss: 994.485
[38,     1] loss: 959.649
[39,     1] loss: 961.164
[40,     1] loss: 946.406
[41,     1] loss: 923.580
[42,     1] loss: 925.456
[43,     1] loss: 941.212
[44,     1] loss: 867.743
[45,     1] loss: 910.878
[46,     1] loss: 868.938
[47,     1] loss: 873.074
[48,     1] loss: 875.278
[49,     1] loss: 864.616
[50,     1] loss: 797.971
[51,     1] loss: 854.198
[52,     1] loss: 838.069
[53,     1] loss: 824.636
[54,     1] loss: 801.805
[55,     1] loss: 731.648
[56,     1] loss: 781.185
[57,     1] loss: 797.969
[58,     1] loss: 768.696
[59,     1] loss: 760.123
[60,     1] loss: 751.492
[61,     1] loss: 735.031
[62,     1] loss: 658.988
[63,     1] loss: 738.969
[64,     1] loss: 727.374
[65,     1] loss: 668.189
[66,     1] loss: 723.388
[67,     1] loss: 766.908
[68,     1] loss: 710.976
[69,     1] loss: 665.081
[70,     1] loss: 705.149
[71,     1] loss: 589.717
[72,     1] loss: 668.366
[73,     1] loss: 600.674
[74,     1] loss: 584.722
[75,     1] loss: 690.171
[76,     1] loss: 610.971
[77,     1] loss: 590.636
[78,     1] loss: 625.902
Early stopping applied (best metric=0.35637426376342773)
Finished Training
Total time taken: 12.411163330078125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.139
[2,     1] loss: 1348.155
[3,     1] loss: 1350.296
[4,     1] loss: 1345.139
[5,     1] loss: 1337.822
[6,     1] loss: 1338.844
[7,     1] loss: 1320.448
[8,     1] loss: 1307.697
[9,     1] loss: 1273.799
[10,     1] loss: 1234.564
[11,     1] loss: 1206.359
[12,     1] loss: 1207.253
[13,     1] loss: 1118.368
[14,     1] loss: 1121.317
[15,     1] loss: 1143.193
[16,     1] loss: 1154.388
[17,     1] loss: 1118.758
[18,     1] loss: 1127.555
[19,     1] loss: 1109.825
[20,     1] loss: 1078.786
[21,     1] loss: 1073.855
[22,     1] loss: 1067.370
[23,     1] loss: 1071.868
[24,     1] loss: 1076.027
[25,     1] loss: 1099.174
[26,     1] loss: 1071.597
[27,     1] loss: 1050.967
[28,     1] loss: 1021.185
[29,     1] loss: 1043.179
[30,     1] loss: 1021.071
[31,     1] loss: 948.664
[32,     1] loss: 980.267
[33,     1] loss: 1006.835
[34,     1] loss: 1013.424
[35,     1] loss: 949.379
[36,     1] loss: 984.370
[37,     1] loss: 976.866
[38,     1] loss: 979.770
[39,     1] loss: 945.944
[40,     1] loss: 941.920
[41,     1] loss: 927.456
[42,     1] loss: 896.818
[43,     1] loss: 944.862
[44,     1] loss: 871.130
[45,     1] loss: 875.523
[46,     1] loss: 881.190
[47,     1] loss: 911.845
[48,     1] loss: 853.281
[49,     1] loss: 904.969
[50,     1] loss: 847.857
[51,     1] loss: 867.858
[52,     1] loss: 866.520
[53,     1] loss: 857.410
[54,     1] loss: 830.213
[55,     1] loss: 822.435
[56,     1] loss: 786.318
[57,     1] loss: 764.253
[58,     1] loss: 773.657
[59,     1] loss: 745.641
[60,     1] loss: 754.410
[61,     1] loss: 726.666
[62,     1] loss: 749.831
[63,     1] loss: 716.941
[64,     1] loss: 789.670
[65,     1] loss: 770.107
[66,     1] loss: 691.545
[67,     1] loss: 669.925
[68,     1] loss: 651.178
[69,     1] loss: 626.556
[70,     1] loss: 648.245
[71,     1] loss: 617.224
[72,     1] loss: 613.303
[73,     1] loss: 730.000
[74,     1] loss: 713.150
[75,     1] loss: 623.911
[76,     1] loss: 612.841
[77,     1] loss: 618.875
[78,     1] loss: 593.359
[79,     1] loss: 646.489
[80,     1] loss: 556.057
[81,     1] loss: 608.359
[82,     1] loss: 563.140
[83,     1] loss: 561.282
[84,     1] loss: 583.696
[85,     1] loss: 582.601
[86,     1] loss: 563.801
[87,     1] loss: 553.417
[88,     1] loss: 518.414
[89,     1] loss: 511.510
Early stopping applied (best metric=0.41974005103111267)
Finished Training
Total time taken: 12.982537269592285
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1353.617
[2,     1] loss: 1345.403
[3,     1] loss: 1345.800
[4,     1] loss: 1349.791
[5,     1] loss: 1346.219
[6,     1] loss: 1345.567
[7,     1] loss: 1338.286
[8,     1] loss: 1336.458
[9,     1] loss: 1336.847
[10,     1] loss: 1333.122
[11,     1] loss: 1325.069
[12,     1] loss: 1321.237
[13,     1] loss: 1304.500
[14,     1] loss: 1278.760
[15,     1] loss: 1263.309
[16,     1] loss: 1240.146
[17,     1] loss: 1255.651
[18,     1] loss: 1202.668
[19,     1] loss: 1175.996
[20,     1] loss: 1135.983
[21,     1] loss: 1171.711
[22,     1] loss: 1119.845
[23,     1] loss: 1115.502
[24,     1] loss: 1127.791
[25,     1] loss: 1101.401
[26,     1] loss: 1034.225
[27,     1] loss: 1085.699
[28,     1] loss: 1053.941
[29,     1] loss: 1126.403
[30,     1] loss: 1095.934
[31,     1] loss: 1049.516
[32,     1] loss: 1025.767
[33,     1] loss: 1013.905
[34,     1] loss: 984.647
[35,     1] loss: 997.382
[36,     1] loss: 1000.882
[37,     1] loss: 931.565
[38,     1] loss: 1008.446
[39,     1] loss: 932.799
[40,     1] loss: 934.639
[41,     1] loss: 1004.852
[42,     1] loss: 936.675
[43,     1] loss: 943.820
[44,     1] loss: 901.210
[45,     1] loss: 960.175
[46,     1] loss: 936.493
[47,     1] loss: 921.227
[48,     1] loss: 892.506
[49,     1] loss: 907.266
[50,     1] loss: 911.593
[51,     1] loss: 957.893
[52,     1] loss: 851.832
[53,     1] loss: 837.384
[54,     1] loss: 869.139
[55,     1] loss: 799.850
[56,     1] loss: 831.563
[57,     1] loss: 838.424
[58,     1] loss: 855.031
[59,     1] loss: 786.009
[60,     1] loss: 772.483
[61,     1] loss: 713.107
[62,     1] loss: 781.141
[63,     1] loss: 755.954
[64,     1] loss: 785.424
[65,     1] loss: 701.983
[66,     1] loss: 782.976
[67,     1] loss: 751.857
[68,     1] loss: 809.859
[69,     1] loss: 728.892
[70,     1] loss: 715.421
[71,     1] loss: 665.317
[72,     1] loss: 734.853
[73,     1] loss: 635.190
[74,     1] loss: 727.925
[75,     1] loss: 663.044
[76,     1] loss: 724.892
[77,     1] loss: 672.915
[78,     1] loss: 648.515
[79,     1] loss: 724.037
[80,     1] loss: 667.712
[81,     1] loss: 670.254
[82,     1] loss: 585.458
[83,     1] loss: 656.722
[84,     1] loss: 620.892
[85,     1] loss: 631.281
[86,     1] loss: 610.543
[87,     1] loss: 549.401
[88,     1] loss: 608.518
[89,     1] loss: 593.113
[90,     1] loss: 613.194
[91,     1] loss: 559.192
[92,     1] loss: 601.696
[93,     1] loss: 554.810
[94,     1] loss: 531.854
[95,     1] loss: 516.559
[96,     1] loss: 517.246
[97,     1] loss: 545.580
[98,     1] loss: 574.987
[99,     1] loss: 512.770
[100,     1] loss: 548.536
[101,     1] loss: 518.988
[102,     1] loss: 538.006
[103,     1] loss: 518.475
[104,     1] loss: 536.181
Early stopping applied (best metric=0.23875807225704193)
Finished Training
Total time taken: 16.081019401550293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1353.802
[2,     1] loss: 1346.885
[3,     1] loss: 1354.647
[4,     1] loss: 1341.655
[5,     1] loss: 1346.846
[6,     1] loss: 1344.344
[7,     1] loss: 1337.981
[8,     1] loss: 1341.110
[9,     1] loss: 1323.124
[10,     1] loss: 1314.085
[11,     1] loss: 1278.327
[12,     1] loss: 1249.569
[13,     1] loss: 1224.506
[14,     1] loss: 1177.692
[15,     1] loss: 1147.613
[16,     1] loss: 1152.856
[17,     1] loss: 1111.102
[18,     1] loss: 1111.841
[19,     1] loss: 1086.695
[20,     1] loss: 1090.302
[21,     1] loss: 1107.189
[22,     1] loss: 1059.431
[23,     1] loss: 1075.383
[24,     1] loss: 1051.238
[25,     1] loss: 1079.641
[26,     1] loss: 1062.768
[27,     1] loss: 1059.829
[28,     1] loss: 1060.052
[29,     1] loss: 1008.111
[30,     1] loss: 997.229
[31,     1] loss: 981.237
[32,     1] loss: 963.897
[33,     1] loss: 1019.145
[34,     1] loss: 958.628
[35,     1] loss: 1013.691
[36,     1] loss: 962.400
[37,     1] loss: 959.626
[38,     1] loss: 971.479
[39,     1] loss: 947.164
[40,     1] loss: 934.149
[41,     1] loss: 923.958
[42,     1] loss: 924.536
[43,     1] loss: 938.369
[44,     1] loss: 909.749
[45,     1] loss: 916.602
[46,     1] loss: 869.480
[47,     1] loss: 922.978
[48,     1] loss: 888.564
[49,     1] loss: 862.442
[50,     1] loss: 862.044
[51,     1] loss: 800.371
[52,     1] loss: 834.978
[53,     1] loss: 862.484
[54,     1] loss: 750.864
[55,     1] loss: 846.450
[56,     1] loss: 829.125
[57,     1] loss: 772.535
[58,     1] loss: 724.714
[59,     1] loss: 768.408
[60,     1] loss: 736.837
[61,     1] loss: 725.421
[62,     1] loss: 770.261
[63,     1] loss: 833.352
[64,     1] loss: 703.095
[65,     1] loss: 741.302
[66,     1] loss: 705.960
[67,     1] loss: 692.047
[68,     1] loss: 716.266
[69,     1] loss: 694.480
[70,     1] loss: 657.157
[71,     1] loss: 638.573
[72,     1] loss: 633.163
[73,     1] loss: 644.365
[74,     1] loss: 600.250
[75,     1] loss: 603.815
[76,     1] loss: 694.756
[77,     1] loss: 690.047
[78,     1] loss: 575.087
[79,     1] loss: 610.837
[80,     1] loss: 584.440
[81,     1] loss: 541.982
[82,     1] loss: 550.031
[83,     1] loss: 589.023
[84,     1] loss: 548.781
[85,     1] loss: 554.726
[86,     1] loss: 550.769
[87,     1] loss: 511.683
[88,     1] loss: 519.847
Early stopping applied (best metric=0.415004163980484)
Finished Training
Total time taken: 13.964108228683472
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1346.056
[2,     1] loss: 1347.657
[3,     1] loss: 1343.296
[4,     1] loss: 1343.276
[5,     1] loss: 1344.939
[6,     1] loss: 1334.847
[7,     1] loss: 1324.394
[8,     1] loss: 1310.755
[9,     1] loss: 1278.724
[10,     1] loss: 1240.573
[11,     1] loss: 1214.848
[12,     1] loss: 1148.725
[13,     1] loss: 1132.225
[14,     1] loss: 1140.376
[15,     1] loss: 1128.957
[16,     1] loss: 1136.610
[17,     1] loss: 1109.876
[18,     1] loss: 1137.492
[19,     1] loss: 1103.423
[20,     1] loss: 1114.880
[21,     1] loss: 1112.007
[22,     1] loss: 1129.945
[23,     1] loss: 1109.688
[24,     1] loss: 1083.715
[25,     1] loss: 1065.534
[26,     1] loss: 1068.825
[27,     1] loss: 1026.889
[28,     1] loss: 1020.681
[29,     1] loss: 1042.160
[30,     1] loss: 980.852
[31,     1] loss: 1024.988
[32,     1] loss: 975.107
[33,     1] loss: 1016.019
[34,     1] loss: 975.387
[35,     1] loss: 954.890
[36,     1] loss: 979.654
[37,     1] loss: 941.429
[38,     1] loss: 987.785
[39,     1] loss: 976.054
[40,     1] loss: 973.492
[41,     1] loss: 927.948
[42,     1] loss: 984.890
[43,     1] loss: 912.420
[44,     1] loss: 858.137
[45,     1] loss: 927.989
[46,     1] loss: 846.087
[47,     1] loss: 923.929
[48,     1] loss: 863.305
[49,     1] loss: 840.133
[50,     1] loss: 785.697
[51,     1] loss: 848.871
[52,     1] loss: 871.480
[53,     1] loss: 793.939
[54,     1] loss: 802.914
[55,     1] loss: 793.792
[56,     1] loss: 818.698
[57,     1] loss: 816.286
[58,     1] loss: 741.968
[59,     1] loss: 787.423
[60,     1] loss: 717.085
[61,     1] loss: 708.762
[62,     1] loss: 694.383
[63,     1] loss: 775.919
[64,     1] loss: 701.690
[65,     1] loss: 615.988
[66,     1] loss: 682.309
[67,     1] loss: 654.214
[68,     1] loss: 783.950
[69,     1] loss: 832.846
[70,     1] loss: 583.557
[71,     1] loss: 786.060
[72,     1] loss: 627.730
[73,     1] loss: 744.412
[74,     1] loss: 672.583
[75,     1] loss: 624.495
[76,     1] loss: 725.743
[77,     1] loss: 671.109
[78,     1] loss: 652.156
[79,     1] loss: 591.757
[80,     1] loss: 731.661
[81,     1] loss: 607.725
[82,     1] loss: 601.609
[83,     1] loss: 564.688
[84,     1] loss: 713.490
[85,     1] loss: 550.468
[86,     1] loss: 675.562
[87,     1] loss: 505.452
[88,     1] loss: 569.007
[89,     1] loss: 530.252
[90,     1] loss: 519.188
[91,     1] loss: 556.676
[92,     1] loss: 560.557
[93,     1] loss: 491.875
[94,     1] loss: 483.799
[95,     1] loss: 483.536
[96,     1] loss: 496.952
[97,     1] loss: 490.305
[98,     1] loss: 551.096
[99,     1] loss: 505.350
[100,     1] loss: 511.947
[101,     1] loss: 484.185
[102,     1] loss: 569.445
[103,     1] loss: 498.384
[104,     1] loss: 532.745
[105,     1] loss: 445.953
[106,     1] loss: 457.516
[107,     1] loss: 536.628
[108,     1] loss: 445.991
[109,     1] loss: 415.327
[110,     1] loss: 424.191
[111,     1] loss: 414.424
[112,     1] loss: 428.694
[113,     1] loss: 461.642
[114,     1] loss: 431.726
[115,     1] loss: 398.940
[116,     1] loss: 426.593
[117,     1] loss: 408.298
[118,     1] loss: 486.338
[119,     1] loss: 449.700
[120,     1] loss: 429.494
[121,     1] loss: 434.976
[122,     1] loss: 532.664
Early stopping applied (best metric=0.35632771253585815)
Finished Training
Total time taken: 20.15282917022705
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1346.958
[2,     1] loss: 1348.267
[3,     1] loss: 1349.562
[4,     1] loss: 1349.228
[5,     1] loss: 1348.124
[6,     1] loss: 1345.694
[7,     1] loss: 1345.045
[8,     1] loss: 1344.712
[9,     1] loss: 1340.081
[10,     1] loss: 1337.459
[11,     1] loss: 1332.548
[12,     1] loss: 1315.127
[13,     1] loss: 1309.132
[14,     1] loss: 1275.112
[15,     1] loss: 1251.932
[16,     1] loss: 1229.152
[17,     1] loss: 1201.042
[18,     1] loss: 1158.006
[19,     1] loss: 1147.822
[20,     1] loss: 1127.127
[21,     1] loss: 1116.017
[22,     1] loss: 1112.607
[23,     1] loss: 1116.517
[24,     1] loss: 1146.706
[25,     1] loss: 1133.149
[26,     1] loss: 1095.920
[27,     1] loss: 1097.666
[28,     1] loss: 1067.245
[29,     1] loss: 1057.728
[30,     1] loss: 1116.733
[31,     1] loss: 1059.424
[32,     1] loss: 1106.266
[33,     1] loss: 1020.918
[34,     1] loss: 1032.950
[35,     1] loss: 1039.059
[36,     1] loss: 1051.495
[37,     1] loss: 1059.225
[38,     1] loss: 1006.980
[39,     1] loss: 994.401
[40,     1] loss: 1022.593
[41,     1] loss: 1005.287
[42,     1] loss: 1024.210
[43,     1] loss: 1001.948
[44,     1] loss: 964.311
[45,     1] loss: 958.473
[46,     1] loss: 957.069
[47,     1] loss: 898.578
[48,     1] loss: 912.729
[49,     1] loss: 922.206
[50,     1] loss: 934.456
[51,     1] loss: 917.807
[52,     1] loss: 887.204
[53,     1] loss: 878.005
[54,     1] loss: 872.991
[55,     1] loss: 908.426
[56,     1] loss: 868.327
[57,     1] loss: 847.324
[58,     1] loss: 848.756
[59,     1] loss: 822.441
[60,     1] loss: 854.637
[61,     1] loss: 846.395
[62,     1] loss: 785.583
[63,     1] loss: 770.339
[64,     1] loss: 797.821
[65,     1] loss: 818.049
[66,     1] loss: 726.577
[67,     1] loss: 723.077
[68,     1] loss: 769.166
[69,     1] loss: 806.684
[70,     1] loss: 732.614
[71,     1] loss: 772.149
[72,     1] loss: 730.501
[73,     1] loss: 678.819
[74,     1] loss: 649.466
[75,     1] loss: 675.476
[76,     1] loss: 675.875
[77,     1] loss: 671.720
[78,     1] loss: 704.085
[79,     1] loss: 657.829
[80,     1] loss: 647.630
[81,     1] loss: 673.459
[82,     1] loss: 652.905
[83,     1] loss: 635.694
[84,     1] loss: 646.465
[85,     1] loss: 673.167
[86,     1] loss: 613.334
[87,     1] loss: 576.483
[88,     1] loss: 620.877
[89,     1] loss: 590.420
[90,     1] loss: 636.459
[91,     1] loss: 602.833
[92,     1] loss: 590.724
Early stopping applied (best metric=0.33250707387924194)
Finished Training
Total time taken: 13.909205675125122
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.691
[2,     1] loss: 1343.361
[3,     1] loss: 1345.845
[4,     1] loss: 1354.605
[5,     1] loss: 1339.040
[6,     1] loss: 1339.764
[7,     1] loss: 1340.365
[8,     1] loss: 1341.093
[9,     1] loss: 1336.881
[10,     1] loss: 1322.627
[11,     1] loss: 1305.376
[12,     1] loss: 1270.007
[13,     1] loss: 1246.041
[14,     1] loss: 1219.870
[15,     1] loss: 1150.635
[16,     1] loss: 1166.808
[17,     1] loss: 1134.427
[18,     1] loss: 1194.222
[19,     1] loss: 1114.132
[20,     1] loss: 1113.907
[21,     1] loss: 1079.606
[22,     1] loss: 1110.100
[23,     1] loss: 1104.570
[24,     1] loss: 1074.301
[25,     1] loss: 1076.887
[26,     1] loss: 1100.463
[27,     1] loss: 1140.667
[28,     1] loss: 1044.075
[29,     1] loss: 1056.156
[30,     1] loss: 1034.297
[31,     1] loss: 1078.770
[32,     1] loss: 1032.215
[33,     1] loss: 1056.406
[34,     1] loss: 1016.330
[35,     1] loss: 991.494
[36,     1] loss: 1025.030
[37,     1] loss: 1013.126
[38,     1] loss: 1019.757
[39,     1] loss: 1028.440
[40,     1] loss: 957.601
[41,     1] loss: 931.728
[42,     1] loss: 930.664
[43,     1] loss: 973.036
[44,     1] loss: 952.091
[45,     1] loss: 897.991
[46,     1] loss: 926.618
[47,     1] loss: 919.092
[48,     1] loss: 853.891
[49,     1] loss: 919.417
[50,     1] loss: 881.516
[51,     1] loss: 890.034
[52,     1] loss: 828.618
[53,     1] loss: 868.921
[54,     1] loss: 836.248
[55,     1] loss: 885.119
[56,     1] loss: 896.729
[57,     1] loss: 785.884
[58,     1] loss: 812.422
[59,     1] loss: 809.190
[60,     1] loss: 781.228
[61,     1] loss: 745.088
[62,     1] loss: 730.439
[63,     1] loss: 770.691
[64,     1] loss: 747.355
[65,     1] loss: 692.023
[66,     1] loss: 783.079
[67,     1] loss: 856.501
[68,     1] loss: 659.876
[69,     1] loss: 793.075
[70,     1] loss: 719.028
[71,     1] loss: 679.171
[72,     1] loss: 830.315
[73,     1] loss: 683.559
[74,     1] loss: 721.392
[75,     1] loss: 675.124
[76,     1] loss: 729.667
[77,     1] loss: 686.234
[78,     1] loss: 592.578
[79,     1] loss: 699.005
[80,     1] loss: 644.532
[81,     1] loss: 646.454
[82,     1] loss: 623.797
[83,     1] loss: 568.902
Early stopping applied (best metric=0.3447663187980652)
Finished Training
Total time taken: 12.208804607391357
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.464
[2,     1] loss: 1352.513
[3,     1] loss: 1348.678
[4,     1] loss: 1349.573
[5,     1] loss: 1348.470
[6,     1] loss: 1344.913
[7,     1] loss: 1343.621
[8,     1] loss: 1348.191
[9,     1] loss: 1343.926
[10,     1] loss: 1340.165
[11,     1] loss: 1335.487
[12,     1] loss: 1335.470
[13,     1] loss: 1321.904
[14,     1] loss: 1304.093
[15,     1] loss: 1293.332
[16,     1] loss: 1262.668
[17,     1] loss: 1244.140
[18,     1] loss: 1209.407
[19,     1] loss: 1143.274
[20,     1] loss: 1157.934
[21,     1] loss: 1152.563
[22,     1] loss: 1114.323
[23,     1] loss: 1150.708
[24,     1] loss: 1164.299
[25,     1] loss: 1111.827
[26,     1] loss: 1132.709
[27,     1] loss: 1132.086
[28,     1] loss: 1092.668
[29,     1] loss: 1121.683
[30,     1] loss: 1122.229
[31,     1] loss: 1118.779
[32,     1] loss: 1075.264
[33,     1] loss: 1087.740
[34,     1] loss: 1051.862
[35,     1] loss: 1036.184
[36,     1] loss: 1047.629
[37,     1] loss: 1003.033
[38,     1] loss: 1036.397
[39,     1] loss: 994.476
[40,     1] loss: 993.551
[41,     1] loss: 993.725
[42,     1] loss: 1036.420
[43,     1] loss: 995.238
[44,     1] loss: 965.989
[45,     1] loss: 959.973
[46,     1] loss: 930.444
[47,     1] loss: 957.560
[48,     1] loss: 968.645
[49,     1] loss: 969.058
[50,     1] loss: 932.903
[51,     1] loss: 926.964
[52,     1] loss: 986.228
[53,     1] loss: 856.379
[54,     1] loss: 968.568
[55,     1] loss: 885.031
[56,     1] loss: 934.820
[57,     1] loss: 904.210
[58,     1] loss: 885.034
[59,     1] loss: 856.367
[60,     1] loss: 862.311
[61,     1] loss: 906.354
[62,     1] loss: 831.791
[63,     1] loss: 821.848
[64,     1] loss: 781.609
[65,     1] loss: 809.705
[66,     1] loss: 767.413
[67,     1] loss: 854.619
[68,     1] loss: 713.173
[69,     1] loss: 750.329
[70,     1] loss: 674.742
[71,     1] loss: 793.259
[72,     1] loss: 766.228
[73,     1] loss: 750.897
[74,     1] loss: 789.310
[75,     1] loss: 689.033
[76,     1] loss: 817.867
[77,     1] loss: 691.349
[78,     1] loss: 725.672
[79,     1] loss: 655.147
[80,     1] loss: 616.509
[81,     1] loss: 630.352
[82,     1] loss: 689.809
[83,     1] loss: 670.459
[84,     1] loss: 629.582
[85,     1] loss: 613.597
[86,     1] loss: 572.200
[87,     1] loss: 613.916
[88,     1] loss: 613.091
[89,     1] loss: 632.349
[90,     1] loss: 560.501
[91,     1] loss: 621.954
[92,     1] loss: 593.344
[93,     1] loss: 509.195
[94,     1] loss: 651.003
[95,     1] loss: 573.353
[96,     1] loss: 536.109
[97,     1] loss: 559.303
[98,     1] loss: 516.133
[99,     1] loss: 573.728
[100,     1] loss: 642.569
[101,     1] loss: 538.523
[102,     1] loss: 617.674
[103,     1] loss: 539.621
[104,     1] loss: 594.748
[105,     1] loss: 526.145
[106,     1] loss: 538.816
[107,     1] loss: 520.630
[108,     1] loss: 514.676
[109,     1] loss: 473.353
[110,     1] loss: 526.896
[111,     1] loss: 490.961
[112,     1] loss: 507.001
[113,     1] loss: 430.878
[114,     1] loss: 585.769
[115,     1] loss: 437.313
[116,     1] loss: 482.060
Early stopping applied (best metric=0.3373692035675049)
Finished Training
Total time taken: 17.03580331802368
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.837
[2,     1] loss: 1348.864
[3,     1] loss: 1344.361
[4,     1] loss: 1349.075
[5,     1] loss: 1343.491
[6,     1] loss: 1333.376
[7,     1] loss: 1323.546
[8,     1] loss: 1304.173
[9,     1] loss: 1285.986
[10,     1] loss: 1255.422
[11,     1] loss: 1218.127
[12,     1] loss: 1168.266
[13,     1] loss: 1176.402
[14,     1] loss: 1135.213
[15,     1] loss: 1170.993
[16,     1] loss: 1148.481
[17,     1] loss: 1167.638
[18,     1] loss: 1092.920
[19,     1] loss: 1116.905
[20,     1] loss: 1096.859
[21,     1] loss: 1101.385
[22,     1] loss: 1087.614
[23,     1] loss: 1112.625
[24,     1] loss: 1112.335
[25,     1] loss: 1114.033
[26,     1] loss: 1098.358
[27,     1] loss: 1076.677
[28,     1] loss: 1027.860
[29,     1] loss: 1062.165
[30,     1] loss: 1024.290
[31,     1] loss: 1036.374
[32,     1] loss: 1064.414
[33,     1] loss: 999.091
[34,     1] loss: 1006.926
[35,     1] loss: 1017.328
[36,     1] loss: 946.957
[37,     1] loss: 1024.991
[38,     1] loss: 1025.681
[39,     1] loss: 966.275
[40,     1] loss: 939.036
[41,     1] loss: 895.724
[42,     1] loss: 946.208
[43,     1] loss: 922.085
[44,     1] loss: 937.824
[45,     1] loss: 967.629
[46,     1] loss: 885.654
[47,     1] loss: 857.816
[48,     1] loss: 885.378
[49,     1] loss: 857.694
[50,     1] loss: 833.890
[51,     1] loss: 821.215
[52,     1] loss: 915.991
[53,     1] loss: 871.239
[54,     1] loss: 806.012
[55,     1] loss: 819.798
[56,     1] loss: 831.219
[57,     1] loss: 843.846
[58,     1] loss: 810.703
[59,     1] loss: 822.969
[60,     1] loss: 788.431
[61,     1] loss: 753.769
[62,     1] loss: 798.422
[63,     1] loss: 686.717
[64,     1] loss: 697.424
[65,     1] loss: 672.013
[66,     1] loss: 725.671
[67,     1] loss: 703.099
[68,     1] loss: 741.101
[69,     1] loss: 682.542
[70,     1] loss: 675.924
[71,     1] loss: 704.230
[72,     1] loss: 630.431
[73,     1] loss: 635.487
[74,     1] loss: 611.673
[75,     1] loss: 614.978
[76,     1] loss: 645.042
[77,     1] loss: 674.676
[78,     1] loss: 673.989
[79,     1] loss: 699.305
[80,     1] loss: 550.738
[81,     1] loss: 628.127
[82,     1] loss: 616.866
[83,     1] loss: 606.720
[84,     1] loss: 579.010
[85,     1] loss: 581.577
[86,     1] loss: 546.940
[87,     1] loss: 568.821
[88,     1] loss: 612.201
[89,     1] loss: 546.739
[90,     1] loss: 534.382
[91,     1] loss: 622.551
[92,     1] loss: 538.352
[93,     1] loss: 560.356
[94,     1] loss: 511.183
[95,     1] loss: 570.772
[96,     1] loss: 523.583
[97,     1] loss: 466.081
[98,     1] loss: 462.044
[99,     1] loss: 533.207
[100,     1] loss: 577.940
[101,     1] loss: 554.736
[102,     1] loss: 598.280
[103,     1] loss: 502.457
[104,     1] loss: 503.531
[105,     1] loss: 653.358
[106,     1] loss: 507.429
[107,     1] loss: 523.265
Early stopping applied (best metric=0.282243013381958)
Finished Training
Total time taken: 15.730401992797852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1349.246
[2,     1] loss: 1344.856
[3,     1] loss: 1347.507
[4,     1] loss: 1348.558
[5,     1] loss: 1340.460
[6,     1] loss: 1343.020
[7,     1] loss: 1343.764
[8,     1] loss: 1336.875
[9,     1] loss: 1328.372
[10,     1] loss: 1314.576
[11,     1] loss: 1295.598
[12,     1] loss: 1274.187
[13,     1] loss: 1235.805
[14,     1] loss: 1202.345
[15,     1] loss: 1158.574
[16,     1] loss: 1172.096
[17,     1] loss: 1113.826
[18,     1] loss: 1147.231
[19,     1] loss: 1156.435
[20,     1] loss: 1105.306
[21,     1] loss: 1130.135
[22,     1] loss: 1112.361
[23,     1] loss: 1083.598
[24,     1] loss: 1112.647
[25,     1] loss: 1084.519
[26,     1] loss: 1061.607
[27,     1] loss: 1054.168
[28,     1] loss: 1067.986
[29,     1] loss: 1075.463
[30,     1] loss: 1035.999
[31,     1] loss: 1003.550
[32,     1] loss: 1010.161
[33,     1] loss: 983.850
[34,     1] loss: 1034.224
[35,     1] loss: 1031.835
[36,     1] loss: 989.201
[37,     1] loss: 939.067
[38,     1] loss: 1007.751
[39,     1] loss: 976.282
[40,     1] loss: 1009.008
[41,     1] loss: 910.682
[42,     1] loss: 971.417
[43,     1] loss: 924.866
[44,     1] loss: 996.688
[45,     1] loss: 924.572
[46,     1] loss: 978.978
[47,     1] loss: 928.696
[48,     1] loss: 972.746
[49,     1] loss: 901.239
[50,     1] loss: 976.981
[51,     1] loss: 862.128
[52,     1] loss: 889.908
[53,     1] loss: 868.908
[54,     1] loss: 890.406
[55,     1] loss: 852.748
[56,     1] loss: 927.347
[57,     1] loss: 818.563
[58,     1] loss: 853.585
[59,     1] loss: 816.481
[60,     1] loss: 911.562
[61,     1] loss: 789.747
[62,     1] loss: 804.106
[63,     1] loss: 796.784
[64,     1] loss: 776.147
[65,     1] loss: 735.584
[66,     1] loss: 793.889
[67,     1] loss: 725.924
[68,     1] loss: 756.571
[69,     1] loss: 677.619
[70,     1] loss: 759.803
[71,     1] loss: 688.967
[72,     1] loss: 741.776
[73,     1] loss: 776.066
[74,     1] loss: 700.717
[75,     1] loss: 731.743
[76,     1] loss: 700.258
[77,     1] loss: 644.916
[78,     1] loss: 669.666
[79,     1] loss: 668.201
[80,     1] loss: 624.674
[81,     1] loss: 620.397
[82,     1] loss: 684.308
[83,     1] loss: 654.968
[84,     1] loss: 619.847
[85,     1] loss: 575.753
[86,     1] loss: 593.987
[87,     1] loss: 642.687
[88,     1] loss: 520.520
[89,     1] loss: 567.777
[90,     1] loss: 517.936
[91,     1] loss: 525.696
[92,     1] loss: 514.370
[93,     1] loss: 574.087
[94,     1] loss: 520.161
[95,     1] loss: 539.926
[96,     1] loss: 570.120
[97,     1] loss: 561.365
[98,     1] loss: 482.403
[99,     1] loss: 557.444
Early stopping applied (best metric=0.34303927421569824)
Finished Training
Total time taken: 14.5978684425354
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1348.647
[2,     1] loss: 1346.196
[3,     1] loss: 1347.500
[4,     1] loss: 1347.773
[5,     1] loss: 1344.337
[6,     1] loss: 1338.839
[7,     1] loss: 1337.759
[8,     1] loss: 1327.750
[9,     1] loss: 1302.995
[10,     1] loss: 1286.712
[11,     1] loss: 1255.064
[12,     1] loss: 1215.388
[13,     1] loss: 1179.484
[14,     1] loss: 1146.739
[15,     1] loss: 1148.805
[16,     1] loss: 1133.292
[17,     1] loss: 1106.903
[18,     1] loss: 1121.506
[19,     1] loss: 1127.879
[20,     1] loss: 1126.195
[21,     1] loss: 1140.327
[22,     1] loss: 1136.865
[23,     1] loss: 1114.126
[24,     1] loss: 1084.133
[25,     1] loss: 1107.221
[26,     1] loss: 1080.134
[27,     1] loss: 1057.923
[28,     1] loss: 1086.033
[29,     1] loss: 1052.887
[30,     1] loss: 1039.323
[31,     1] loss: 995.426
[32,     1] loss: 997.194
[33,     1] loss: 1014.018
[34,     1] loss: 986.452
[35,     1] loss: 997.228
[36,     1] loss: 1001.159
[37,     1] loss: 1002.356
[38,     1] loss: 1035.672
[39,     1] loss: 976.293
[40,     1] loss: 966.146
[41,     1] loss: 967.992
[42,     1] loss: 919.783
[43,     1] loss: 941.037
[44,     1] loss: 990.546
[45,     1] loss: 922.300
[46,     1] loss: 878.506
[47,     1] loss: 894.564
[48,     1] loss: 935.328
[49,     1] loss: 927.316
[50,     1] loss: 844.456
[51,     1] loss: 887.344
[52,     1] loss: 859.403
[53,     1] loss: 839.181
[54,     1] loss: 851.474
[55,     1] loss: 837.576
[56,     1] loss: 817.278
[57,     1] loss: 798.182
[58,     1] loss: 767.370
[59,     1] loss: 786.491
[60,     1] loss: 765.355
[61,     1] loss: 761.097
[62,     1] loss: 788.029
[63,     1] loss: 707.044
[64,     1] loss: 734.509
[65,     1] loss: 742.537
[66,     1] loss: 742.737
[67,     1] loss: 788.031
[68,     1] loss: 719.299
[69,     1] loss: 690.160
[70,     1] loss: 639.830
[71,     1] loss: 627.132
[72,     1] loss: 634.700
[73,     1] loss: 651.589
[74,     1] loss: 638.891
[75,     1] loss: 614.457
[76,     1] loss: 591.758
[77,     1] loss: 592.758
[78,     1] loss: 651.981
[79,     1] loss: 585.282
Early stopping applied (best metric=0.3914388418197632)
Finished Training
Total time taken: 11.595633745193481
{'Hydroxylation-K Validation Accuracy': 0.7296985815602837, 'Hydroxylation-K Validation Sensitivity': 0.6251851851851852, 'Hydroxylation-K Validation Specificity': 0.756140350877193, 'Hydroxylation-K Validation Precision': 0.39587716377190063, 'Hydroxylation-K AUC ROC': 0.7860818713450293, 'Hydroxylation-K AUC PR': 0.5392027575617314, 'Hydroxylation-K MCC': 0.3303176802379397, 'Hydroxylation-K F1': 0.4776222897768742, 'Validation Loss (Hydroxylation-K)': 0.509278021256129, 'Hydroxylation-P Validation Accuracy': 0.8050282219176692, 'Hydroxylation-P Validation Sensitivity': 0.8127513227513228, 'Hydroxylation-P Validation Specificity': 0.8034216170382562, 'Hydroxylation-P Validation Precision': 0.4797241646486318, 'Hydroxylation-P AUC ROC': 0.8731726991094042, 'Hydroxylation-P AUC PR': 0.6236061202089629, 'Hydroxylation-P MCC': 0.5155472986951203, 'Hydroxylation-P F1': 0.6002462328635705, 'Validation Loss (Hydroxylation-P)': 0.34386448562145233, 'Validation Loss (total)': 0.853142515818278, 'TimeToTrain': 15.230595239003499}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004154533977160979,
 'learning_rate_Hydroxylation-K': 0.009180622566804976,
 'learning_rate_Hydroxylation-P': 0.007151132980158667,
 'log_base': 1.2134275945187758,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3413752549,
 'sample_weights': [2.0789323362231937, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.297445831645458,
 'weight_decay_Hydroxylation-K': 9.132199641208132,
 'weight_decay_Hydroxylation-P': 5.2762163324259905}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2793.406
[2,     1] loss: 2811.324
[3,     1] loss: 2801.167
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009594564535970762,
 'learning_rate_Hydroxylation-K': 0.006669462340407356,
 'learning_rate_Hydroxylation-P': 0.002793002854666473,
 'log_base': 1.897818215307458,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1253074719,
 'sample_weights': [8.629884240407357, 1.0787770872753388],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.747033318321176,
 'weight_decay_Hydroxylation-K': 9.449965495013705,
 'weight_decay_Hydroxylation-P': 6.392096254827925}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1457.385
[2,     1] loss: 1456.827
[3,     1] loss: 1457.243
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00703234321688209,
 'learning_rate_Hydroxylation-K': 0.009607756416437671,
 'learning_rate_Hydroxylation-P': 0.008390150243126409,
 'log_base': 1.344939010472027,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 773239842,
 'sample_weights': [2.605634979353697, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.61072745830737,
 'weight_decay_Hydroxylation-K': 3.457640959263194,
 'weight_decay_Hydroxylation-P': 7.081453802630138}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2099.308
[2,     1] loss: 2098.745
[3,     1] loss: 2099.589
[4,     1] loss: 2105.353
[5,     1] loss: 2097.761
[6,     1] loss: 2097.479
[7,     1] loss: 2095.616
[8,     1] loss: 2096.678
[9,     1] loss: 2096.894
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000836648493376125,
 'learning_rate_Hydroxylation-K': 0.006271903643969973,
 'learning_rate_Hydroxylation-P': 0.006288544891647091,
 'log_base': 1.3322199403510462,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 984090614,
 'sample_weights': [5.6333749238214335, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3095403541330541,
 'weight_decay_Hydroxylation-K': 2.3338759346518434,
 'weight_decay_Hydroxylation-P': 6.221864425160042}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2146.972
[2,     1] loss: 2137.239
[3,     1] loss: 2132.701
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000776792951736615,
 'learning_rate_Hydroxylation-K': 0.008084225433271607,
 'learning_rate_Hydroxylation-P': 0.009487087808009907,
 'log_base': 2.8993029052464268,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3447551963,
 'sample_weights': [5.819984229883574, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7774581610138256,
 'weight_decay_Hydroxylation-K': 6.720988824921502,
 'weight_decay_Hydroxylation-P': 7.9186215225231}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.093
[2,     1] loss: 1243.589
[3,     1] loss: 1238.705
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0056130856264144605,
 'learning_rate_Hydroxylation-K': 0.005749780706006328,
 'learning_rate_Hydroxylation-P': 0.004598560837103974,
 'log_base': 1.5443236072414577,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3065935575,
 'sample_weights': [1.5683322488557407, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.487272330559179,
 'weight_decay_Hydroxylation-K': 4.190366015435304,
 'weight_decay_Hydroxylation-P': 4.947126183216665}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1724.537
[2,     1] loss: 1722.625
[3,     1] loss: 1725.915
[4,     1] loss: 1715.951
[5,     1] loss: 1717.557
[6,     1] loss: 1716.086
[7,     1] loss: 1712.333
[8,     1] loss: 1707.480
[9,     1] loss: 1701.072
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009907946033409906,
 'learning_rate_Hydroxylation-K': 0.00897292185822341,
 'learning_rate_Hydroxylation-P': 0.007291604037300459,
 'log_base': 1.5573940656890293,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2000998704,
 'sample_weights': [3.841456170403431, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.2766618439429687,
 'weight_decay_Hydroxylation-K': 3.870627431727427,
 'weight_decay_Hydroxylation-P': 5.3539281946839665}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1710.492
[2,     1] loss: 1713.097
[3,     1] loss: 1704.646
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003169307668405355,
 'learning_rate_Hydroxylation-K': 0.007503743967591968,
 'learning_rate_Hydroxylation-P': 0.006049049992994346,
 'log_base': 1.0463314338780807,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 50635882,
 'sample_weights': [3.7683759919176865, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.883567767446158,
 'weight_decay_Hydroxylation-K': 9.998269931238193,
 'weight_decay_Hydroxylation-P': 5.16916946908894}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11935.246
[2,     1] loss: 11925.951
[3,     1] loss: 11970.596
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005475991639660247,
 'learning_rate_Hydroxylation-K': 0.009428995047719145,
 'learning_rate_Hydroxylation-P': 0.00771201417811706,
 'log_base': 2.0127460794875667,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3013734685,
 'sample_weights': [36.86104524811843, 4.607808160450209],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.64273321192143,
 'weight_decay_Hydroxylation-K': 7.472271651760608,
 'weight_decay_Hydroxylation-P': 2.590206633094325}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1410.757
[2,     1] loss: 1408.708
[3,     1] loss: 1407.573
[4,     1] loss: 1408.682
[5,     1] loss: 1407.064
[6,     1] loss: 1409.443
[7,     1] loss: 1398.105
[8,     1] loss: 1398.113
[9,     1] loss: 1383.959
[10,     1] loss: 1370.201
[11,     1] loss: 1337.780
[12,     1] loss: 1281.999
[13,     1] loss: 1223.847
[14,     1] loss: 1239.047
[15,     1] loss: 1248.138
[16,     1] loss: 1233.853
[17,     1] loss: 1149.161
[18,     1] loss: 1170.693
[19,     1] loss: 1097.054
[20,     1] loss: 1166.551
[21,     1] loss: 1159.344
[22,     1] loss: 1141.567
[23,     1] loss: 1129.218
[24,     1] loss: 1162.711
[25,     1] loss: 1114.007
[26,     1] loss: 1140.571
[27,     1] loss: 1102.701
[28,     1] loss: 1158.333
[29,     1] loss: 1066.381
[30,     1] loss: 1080.983
[31,     1] loss: 1031.829
[32,     1] loss: 1029.512
[33,     1] loss: 1050.459
[34,     1] loss: 1015.680
[35,     1] loss: 1054.692
[36,     1] loss: 1005.627
[37,     1] loss: 991.143
[38,     1] loss: 1015.470
[39,     1] loss: 963.272
[40,     1] loss: 1000.643
[41,     1] loss: 966.144
[42,     1] loss: 948.849
[43,     1] loss: 955.144
[44,     1] loss: 963.008
[45,     1] loss: 902.126
[46,     1] loss: 929.820
[47,     1] loss: 821.389
[48,     1] loss: 975.735
[49,     1] loss: 952.473
[50,     1] loss: 796.913
[51,     1] loss: 921.572
[52,     1] loss: 794.793
[53,     1] loss: 871.760
[54,     1] loss: 804.380
[55,     1] loss: 791.205
[56,     1] loss: 834.292
[57,     1] loss: 724.836
[58,     1] loss: 746.565
[59,     1] loss: 814.630
[60,     1] loss: 763.160
[61,     1] loss: 746.726
[62,     1] loss: 842.408
[63,     1] loss: 774.647
[64,     1] loss: 736.599
[65,     1] loss: 874.985
[66,     1] loss: 738.615
[67,     1] loss: 674.173
[68,     1] loss: 696.963
[69,     1] loss: 668.734
[70,     1] loss: 683.836
[71,     1] loss: 768.100
[72,     1] loss: 637.796
[73,     1] loss: 628.602
[74,     1] loss: 744.060
[75,     1] loss: 818.784
[76,     1] loss: 627.661
[77,     1] loss: 705.177
[78,     1] loss: 685.820
[79,     1] loss: 548.615
[80,     1] loss: 618.759
[81,     1] loss: 552.333
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005193747572278171,
 'learning_rate_Hydroxylation-K': 0.007794890659185453,
 'learning_rate_Hydroxylation-P': 0.005287023797383092,
 'log_base': 1.355269810461696,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3854481956,
 'sample_weights': [2.386623518719302, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6569380378282688,
 'weight_decay_Hydroxylation-K': 2.1105247630899044,
 'weight_decay_Hydroxylation-P': 6.463120263082271}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2061.540
[2,     1] loss: 2081.556
[3,     1] loss: 2076.818
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018372125766599307,
 'learning_rate_Hydroxylation-K': 0.005766998954435839,
 'learning_rate_Hydroxylation-P': 0.007453339684849793,
 'log_base': 1.8051961382025246,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3069413338,
 'sample_weights': [5.491579245700399, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.547754592338238,
 'weight_decay_Hydroxylation-K': 9.273649404863871,
 'weight_decay_Hydroxylation-P': 1.1632909589121248}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1502.681
[2,     1] loss: 1503.983
[3,     1] loss: 1507.860
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004479368544388121,
 'learning_rate_Hydroxylation-K': 0.0074055069639529306,
 'learning_rate_Hydroxylation-P': 0.0004425031799923547,
 'log_base': 1.1545296334462554,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2764356285,
 'sample_weights': [2.8263586571606947, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.934163965566432,
 'weight_decay_Hydroxylation-K': 9.66045890767992,
 'weight_decay_Hydroxylation-P': 7.771551430752657}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3785.976
[2,     1] loss: 3763.134
[3,     1] loss: 3782.842
[4,     1] loss: 3786.021
[5,     1] loss: 3756.376
[6,     1] loss: 3756.079
[7,     1] loss: 3760.417
[8,     1] loss: 3743.119
[9,     1] loss: 3755.930
[10,     1] loss: 3726.904
[11,     1] loss: 3755.663
[12,     1] loss: 3712.362
[13,     1] loss: 3694.144
[14,     1] loss: 3618.071
[15,     1] loss: 3599.518
[16,     1] loss: 3523.990
[17,     1] loss: 3437.222
[18,     1] loss: 3372.172
[19,     1] loss: 3349.627
[20,     1] loss: 3412.757
[21,     1] loss: 3212.671
[22,     1] loss: 3216.012
[23,     1] loss: 3084.307
[24,     1] loss: 2814.965
[25,     1] loss: 3036.944
[26,     1] loss: 2988.479
[27,     1] loss: 3107.811
[28,     1] loss: 2996.627
[29,     1] loss: 3009.010
[30,     1] loss: 3059.715
[31,     1] loss: 3115.242
[32,     1] loss: 3091.259
[33,     1] loss: 2778.647
[34,     1] loss: 2915.690
[35,     1] loss: 2803.011
[36,     1] loss: 3036.670
[37,     1] loss: 2642.858
[38,     1] loss: 2906.332
[39,     1] loss: 2432.668
[40,     1] loss: 2684.640
[41,     1] loss: 2499.662
[42,     1] loss: 2331.689
[43,     1] loss: 2321.358
[44,     1] loss: 2379.803
[45,     1] loss: 2486.499
[46,     1] loss: 2570.352
[47,     1] loss: 2202.846
[48,     1] loss: 2665.015
[49,     1] loss: 2377.321
[50,     1] loss: 2314.497
[51,     1] loss: 2167.652
[52,     1] loss: 2160.106
[53,     1] loss: 2618.708
[54,     1] loss: 3030.587
[55,     1] loss: 2197.348
[56,     1] loss: 2132.804
[57,     1] loss: 1940.039
[58,     1] loss: 2295.992
[59,     1] loss: 2040.220
[60,     1] loss: 2293.135
[61,     1] loss: 1993.048
[62,     1] loss: 2161.417
[63,     1] loss: 1927.666
[64,     1] loss: 1962.112
[65,     1] loss: 2106.892
[66,     1] loss: 1797.153
[67,     1] loss: 1872.739
[68,     1] loss: 1856.465
[69,     1] loss: 1806.681
[70,     1] loss: 2225.009
[71,     1] loss: 2816.698
[72,     1] loss: 1893.227
[73,     1] loss: 1970.262
[74,     1] loss: 2053.197
[75,     1] loss: 1973.631
[76,     1] loss: 1890.292
[77,     1] loss: 1845.908
[78,     1] loss: 1709.581
[79,     1] loss: 1865.407
[80,     1] loss: 1867.927
[81,     1] loss: 1886.764
[82,     1] loss: 1604.502
[83,     1] loss: 1695.882
[84,     1] loss: 2484.014
[85,     1] loss: 2789.056
[86,     1] loss: 1972.956
[87,     1] loss: 2128.122
[88,     1] loss: 1920.181
[89,     1] loss: 1991.399
[90,     1] loss: 1959.691
[91,     1] loss: 2010.893
[92,     1] loss: 2084.592
[93,     1] loss: 2604.353
[94,     1] loss: 1731.937
[95,     1] loss: 2159.280
[96,     1] loss: 1739.566
[97,     1] loss: 2243.568
[98,     1] loss: 1775.604
[99,     1] loss: 2091.212
[100,     1] loss: 1722.531
[101,     1] loss: 2000.841
[102,     1] loss: 1705.286
[103,     1] loss: 1989.608
[104,     1] loss: 2005.830
[105,     1] loss: 1584.507
[106,     1] loss: 2052.036
[107,     1] loss: 2210.244
[108,     1] loss: 1574.038
[109,     1] loss: 1833.517
Early stopping applied (best metric=0.3183574974536896)
Finished Training
Total time taken: 16.144591569900513
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3764.506
[2,     1] loss: 3772.817
[3,     1] loss: 3766.009
[4,     1] loss: 3776.916
[5,     1] loss: 3773.233
[6,     1] loss: 3764.873
[7,     1] loss: 3765.046
[8,     1] loss: 3758.067
[9,     1] loss: 3775.434
[10,     1] loss: 3758.238
[11,     1] loss: 3762.892
[12,     1] loss: 3759.642
[13,     1] loss: 3757.916
[14,     1] loss: 3759.491
[15,     1] loss: 3754.266
[16,     1] loss: 3731.906
[17,     1] loss: 3719.989
[18,     1] loss: 3672.417
[19,     1] loss: 3657.721
[20,     1] loss: 3557.656
[21,     1] loss: 3471.732
[22,     1] loss: 3413.763
[23,     1] loss: 3369.486
[24,     1] loss: 3179.782
[25,     1] loss: 3193.768
[26,     1] loss: 3163.915
[27,     1] loss: 3180.334
[28,     1] loss: 3035.172
[29,     1] loss: 3226.636
[30,     1] loss: 3086.477
[31,     1] loss: 3085.236
[32,     1] loss: 2727.465
[33,     1] loss: 3038.599
[34,     1] loss: 2998.910
[35,     1] loss: 2698.672
[36,     1] loss: 2903.242
[37,     1] loss: 2895.973
[38,     1] loss: 3083.342
[39,     1] loss: 2693.509
[40,     1] loss: 2850.252
[41,     1] loss: 2722.338
[42,     1] loss: 2790.375
[43,     1] loss: 2567.948
[44,     1] loss: 2644.822
[45,     1] loss: 2729.936
[46,     1] loss: 2687.166
[47,     1] loss: 2573.621
[48,     1] loss: 2647.477
[49,     1] loss: 2611.480
[50,     1] loss: 2468.055
[51,     1] loss: 2489.300
[52,     1] loss: 2371.009
[53,     1] loss: 2421.409
[54,     1] loss: 2258.741
[55,     1] loss: 2243.854
[56,     1] loss: 2054.702
[57,     1] loss: 2157.305
[58,     1] loss: 2400.306
[59,     1] loss: 2818.227
[60,     1] loss: 2329.424
[61,     1] loss: 2387.283
[62,     1] loss: 2250.696
[63,     1] loss: 2235.640
[64,     1] loss: 2196.062
[65,     1] loss: 2205.876
[66,     1] loss: 2061.709
[67,     1] loss: 1996.744
[68,     1] loss: 2205.876
[69,     1] loss: 1952.038
[70,     1] loss: 2142.395
[71,     1] loss: 1993.507
[72,     1] loss: 1817.833
[73,     1] loss: 1817.813
[74,     1] loss: 1816.394
[75,     1] loss: 1687.759
[76,     1] loss: 1861.506
[77,     1] loss: 2414.698
[78,     1] loss: 4415.400
[79,     1] loss: 1965.346
[80,     1] loss: 2974.226
[81,     1] loss: 2253.277
[82,     1] loss: 2431.877
[83,     1] loss: 2561.304
[84,     1] loss: 2220.335
[85,     1] loss: 2306.446
[86,     1] loss: 2224.808
[87,     1] loss: 2316.557
[88,     1] loss: 2113.813
[89,     1] loss: 2198.277
[90,     1] loss: 2003.698
[91,     1] loss: 1846.803
[92,     1] loss: 1890.600
[93,     1] loss: 1919.128
[94,     1] loss: 1887.198
[95,     1] loss: 3045.166
[96,     1] loss: 2457.355
[97,     1] loss: 1827.938
[98,     1] loss: 2001.557
[99,     1] loss: 2325.584
[100,     1] loss: 2104.458
[101,     1] loss: 2534.532
Early stopping applied (best metric=0.3854033946990967)
Finished Training
Total time taken: 14.952274322509766
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3762.213
[2,     1] loss: 3761.523
[3,     1] loss: 3776.817
[4,     1] loss: 3785.855
[5,     1] loss: 3759.938
[6,     1] loss: 3760.565
[7,     1] loss: 3802.889
[8,     1] loss: 3775.677
[9,     1] loss: 3769.728
[10,     1] loss: 3755.142
[11,     1] loss: 3744.528
[12,     1] loss: 3756.097
[13,     1] loss: 3740.073
[14,     1] loss: 3741.772
[15,     1] loss: 3696.282
[16,     1] loss: 3662.989
[17,     1] loss: 3606.094
[18,     1] loss: 3521.743
[19,     1] loss: 3447.326
[20,     1] loss: 3470.047
[21,     1] loss: 3287.735
[22,     1] loss: 3296.716
[23,     1] loss: 3192.279
[24,     1] loss: 3139.598
[25,     1] loss: 2977.689
[26,     1] loss: 3220.638
[27,     1] loss: 3084.443
[28,     1] loss: 3068.786
[29,     1] loss: 3074.299
[30,     1] loss: 3103.851
[31,     1] loss: 3016.034
[32,     1] loss: 3114.633
[33,     1] loss: 3009.563
[34,     1] loss: 3020.471
[35,     1] loss: 2916.094
[36,     1] loss: 2778.650
[37,     1] loss: 2878.938
[38,     1] loss: 2763.107
[39,     1] loss: 2773.222
[40,     1] loss: 2818.909
[41,     1] loss: 2673.419
[42,     1] loss: 3042.950
[43,     1] loss: 2593.015
[44,     1] loss: 2784.574
[45,     1] loss: 2717.301
[46,     1] loss: 2544.216
[47,     1] loss: 2470.817
[48,     1] loss: 2361.463
[49,     1] loss: 2332.531
[50,     1] loss: 2580.157
[51,     1] loss: 2718.488
[52,     1] loss: 2292.329
[53,     1] loss: 2419.485
[54,     1] loss: 2122.483
[55,     1] loss: 2734.172
[56,     1] loss: 2276.615
[57,     1] loss: 2386.505
[58,     1] loss: 2147.556
[59,     1] loss: 2546.851
[60,     1] loss: 2252.904
[61,     1] loss: 2284.722
[62,     1] loss: 2464.024
[63,     1] loss: 1976.293
[64,     1] loss: 1897.826
[65,     1] loss: 2205.213
[66,     1] loss: 2557.331
[67,     1] loss: 1928.519
[68,     1] loss: 2323.129
[69,     1] loss: 1859.813
[70,     1] loss: 2115.522
[71,     1] loss: 2059.962
[72,     1] loss: 1942.699
[73,     1] loss: 2014.130
[74,     1] loss: 2005.115
[75,     1] loss: 2210.244
[76,     1] loss: 1903.269
[77,     1] loss: 2445.391
[78,     1] loss: 1994.119
[79,     1] loss: 2752.706
[80,     1] loss: 1844.721
[81,     1] loss: 2251.819
[82,     1] loss: 2028.915
[83,     1] loss: 2121.836
[84,     1] loss: 2092.168
[85,     1] loss: 1878.775
[86,     1] loss: 1840.735
[87,     1] loss: 1719.356
[88,     1] loss: 1904.741
[89,     1] loss: 1738.925
[90,     1] loss: 1664.081
[91,     1] loss: 2039.728
[92,     1] loss: 2259.096
[93,     1] loss: 1740.185
[94,     1] loss: 1844.409
[95,     1] loss: 1798.001
Early stopping applied (best metric=0.43711990118026733)
Finished Training
Total time taken: 13.989209651947021
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3758.636
[2,     1] loss: 3765.262
[3,     1] loss: 3761.586
[4,     1] loss: 3769.534
[5,     1] loss: 3769.158
[6,     1] loss: 3762.180
[7,     1] loss: 3770.183
[8,     1] loss: 3755.306
[9,     1] loss: 3759.788
[10,     1] loss: 3745.048
[11,     1] loss: 3747.999
[12,     1] loss: 3715.142
[13,     1] loss: 3659.783
[14,     1] loss: 3603.575
[15,     1] loss: 3522.608
[16,     1] loss: 3419.310
[17,     1] loss: 3316.075
[18,     1] loss: 3315.676
[19,     1] loss: 3233.883
[20,     1] loss: 3402.613
[21,     1] loss: 3175.206
[22,     1] loss: 3140.331
[23,     1] loss: 3072.138
[24,     1] loss: 3038.781
[25,     1] loss: 3058.346
[26,     1] loss: 3098.249
[27,     1] loss: 3026.888
[28,     1] loss: 3071.604
[29,     1] loss: 3310.821
[30,     1] loss: 2908.637
[31,     1] loss: 2942.959
[32,     1] loss: 2809.897
[33,     1] loss: 2983.214
[34,     1] loss: 2682.547
[35,     1] loss: 2736.227
[36,     1] loss: 2563.034
[37,     1] loss: 2623.209
[38,     1] loss: 2599.363
[39,     1] loss: 2620.890
[40,     1] loss: 2471.110
[41,     1] loss: 2740.420
[42,     1] loss: 2552.837
[43,     1] loss: 2518.536
[44,     1] loss: 2478.962
[45,     1] loss: 2591.809
[46,     1] loss: 2469.479
[47,     1] loss: 2113.729
[48,     1] loss: 2172.999
[49,     1] loss: 2076.029
[50,     1] loss: 2436.400
[51,     1] loss: 3575.438
[52,     1] loss: 2142.045
[53,     1] loss: 2588.811
[54,     1] loss: 2392.358
[55,     1] loss: 2472.374
[56,     1] loss: 2596.589
[57,     1] loss: 2455.132
[58,     1] loss: 2325.066
[59,     1] loss: 2181.929
[60,     1] loss: 2571.920
[61,     1] loss: 2193.926
[62,     1] loss: 2473.070
[63,     1] loss: 2343.861
[64,     1] loss: 2089.797
[65,     1] loss: 2352.720
[66,     1] loss: 1985.921
[67,     1] loss: 2333.504
[68,     1] loss: 1979.842
[69,     1] loss: 2049.875
[70,     1] loss: 2209.458
[71,     1] loss: 1922.999
[72,     1] loss: 1984.967
[73,     1] loss: 1876.781
[74,     1] loss: 2327.093
[75,     1] loss: 1995.054
[76,     1] loss: 2343.941
[77,     1] loss: 1780.410
Early stopping applied (best metric=0.3988257944583893)
Finished Training
Total time taken: 11.408531188964844
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3762.146
[2,     1] loss: 3777.799
[3,     1] loss: 3769.100
[4,     1] loss: 3797.643
[5,     1] loss: 3741.606
[6,     1] loss: 3768.593
[7,     1] loss: 3745.135
[8,     1] loss: 3751.869
[9,     1] loss: 3734.604
[10,     1] loss: 3712.850
[11,     1] loss: 3694.395
[12,     1] loss: 3615.252
[13,     1] loss: 3556.552
[14,     1] loss: 3502.551
[15,     1] loss: 3469.120
[16,     1] loss: 3336.672
[17,     1] loss: 3287.016
[18,     1] loss: 3101.891
[19,     1] loss: 3217.542
[20,     1] loss: 3111.602
[21,     1] loss: 3178.129
[22,     1] loss: 3134.981
[23,     1] loss: 3208.973
[24,     1] loss: 3092.665
[25,     1] loss: 3018.723
[26,     1] loss: 3182.023
[27,     1] loss: 3060.803
[28,     1] loss: 3001.607
[29,     1] loss: 2881.587
[30,     1] loss: 2653.056
[31,     1] loss: 2834.825
[32,     1] loss: 2838.615
[33,     1] loss: 2946.753
[34,     1] loss: 2676.288
[35,     1] loss: 2841.092
[36,     1] loss: 2659.041
[37,     1] loss: 2718.703
[38,     1] loss: 2604.884
[39,     1] loss: 2686.022
[40,     1] loss: 2507.366
[41,     1] loss: 2488.831
[42,     1] loss: 2262.818
[43,     1] loss: 2324.731
[44,     1] loss: 2196.778
[45,     1] loss: 2449.271
[46,     1] loss: 2772.318
[47,     1] loss: 3735.041
[48,     1] loss: 2226.544
[49,     1] loss: 2729.837
[50,     1] loss: 2464.975
[51,     1] loss: 2526.692
[52,     1] loss: 2831.853
[53,     1] loss: 2849.502
[54,     1] loss: 2662.401
[55,     1] loss: 2567.567
[56,     1] loss: 2577.634
[57,     1] loss: 2619.406
[58,     1] loss: 2370.665
[59,     1] loss: 2248.344
[60,     1] loss: 2321.658
[61,     1] loss: 2200.884
[62,     1] loss: 2451.383
[63,     1] loss: 2511.357
Early stopping applied (best metric=0.5025385618209839)
Finished Training
Total time taken: 9.341185092926025
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3767.812
[2,     1] loss: 3774.263
[3,     1] loss: 3782.273
[4,     1] loss: 3770.523
[5,     1] loss: 3765.697
[6,     1] loss: 3783.108
[7,     1] loss: 3764.111
[8,     1] loss: 3762.844
[9,     1] loss: 3775.684
[10,     1] loss: 3755.951
[11,     1] loss: 3756.924
[12,     1] loss: 3759.487
[13,     1] loss: 3751.459
[14,     1] loss: 3750.007
[15,     1] loss: 3733.592
[16,     1] loss: 3724.146
[17,     1] loss: 3713.287
[18,     1] loss: 3677.505
[19,     1] loss: 3622.739
[20,     1] loss: 3502.737
[21,     1] loss: 3433.624
[22,     1] loss: 3385.495
[23,     1] loss: 3377.283
[24,     1] loss: 3061.986
[25,     1] loss: 3236.708
[26,     1] loss: 3475.938
[27,     1] loss: 3149.055
[28,     1] loss: 3149.370
[29,     1] loss: 3383.396
[30,     1] loss: 2962.357
[31,     1] loss: 3019.763
[32,     1] loss: 3124.001
[33,     1] loss: 2838.056
[34,     1] loss: 3139.540
[35,     1] loss: 2843.391
[36,     1] loss: 2799.448
[37,     1] loss: 2682.371
[38,     1] loss: 2962.064
[39,     1] loss: 2628.139
[40,     1] loss: 2628.703
[41,     1] loss: 2569.904
[42,     1] loss: 2576.480
[43,     1] loss: 2633.800
[44,     1] loss: 2590.089
[45,     1] loss: 2444.944
[46,     1] loss: 2479.897
[47,     1] loss: 2520.558
[48,     1] loss: 2444.848
[49,     1] loss: 2402.541
[50,     1] loss: 2288.854
[51,     1] loss: 2265.052
[52,     1] loss: 2345.080
[53,     1] loss: 2146.363
[54,     1] loss: 2028.472
[55,     1] loss: 2725.630
[56,     1] loss: 2427.115
[57,     1] loss: 2215.037
[58,     1] loss: 2467.194
[59,     1] loss: 2054.431
[60,     1] loss: 2160.271
[61,     1] loss: 1955.541
[62,     1] loss: 1919.251
[63,     1] loss: 1841.704
[64,     1] loss: 2082.743
[65,     1] loss: 2592.696
[66,     1] loss: 4443.686
[67,     1] loss: 2381.770
[68,     1] loss: 3328.296
[69,     1] loss: 2727.198
[70,     1] loss: 2515.634
[71,     1] loss: 2902.207
[72,     1] loss: 2980.726
[73,     1] loss: 3009.273
[74,     1] loss: 2850.274
[75,     1] loss: 2741.793
[76,     1] loss: 2626.796
[77,     1] loss: 2737.570
[78,     1] loss: 2677.638
[79,     1] loss: 2344.271
[80,     1] loss: 2357.626
[81,     1] loss: 2538.379
[82,     1] loss: 2496.989
[83,     1] loss: 2067.187
[84,     1] loss: 2512.112
[85,     1] loss: 2078.810
[86,     1] loss: 2136.023
[87,     1] loss: 2137.990
Early stopping applied (best metric=0.4066077470779419)
Finished Training
Total time taken: 12.819199323654175
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3772.049
[2,     1] loss: 3763.266
[3,     1] loss: 3786.665
[4,     1] loss: 3763.990
[5,     1] loss: 3768.920
[6,     1] loss: 3764.120
[7,     1] loss: 3765.872
[8,     1] loss: 3753.900
[9,     1] loss: 3761.988
[10,     1] loss: 3761.971
[11,     1] loss: 3750.561
[12,     1] loss: 3739.911
[13,     1] loss: 3720.783
[14,     1] loss: 3700.185
[15,     1] loss: 3645.771
[16,     1] loss: 3572.712
[17,     1] loss: 3481.206
[18,     1] loss: 3383.273
[19,     1] loss: 3247.055
[20,     1] loss: 3114.568
[21,     1] loss: 3340.941
[22,     1] loss: 3187.328
[23,     1] loss: 3062.768
[24,     1] loss: 3019.533
[25,     1] loss: 2867.845
[26,     1] loss: 2844.360
[27,     1] loss: 2828.206
[28,     1] loss: 2893.465
[29,     1] loss: 2967.043
[30,     1] loss: 2904.919
[31,     1] loss: 3004.642
[32,     1] loss: 2948.655
[33,     1] loss: 2949.713
[34,     1] loss: 2824.716
[35,     1] loss: 2778.703
[36,     1] loss: 2680.521
[37,     1] loss: 2679.339
[38,     1] loss: 2353.261
[39,     1] loss: 2497.479
[40,     1] loss: 2350.637
[41,     1] loss: 2213.199
[42,     1] loss: 2357.266
[43,     1] loss: 2277.167
[44,     1] loss: 2946.213
[45,     1] loss: 2523.280
[46,     1] loss: 2716.113
[47,     1] loss: 2350.801
[48,     1] loss: 2707.800
[49,     1] loss: 2420.451
[50,     1] loss: 2487.469
[51,     1] loss: 2542.082
[52,     1] loss: 2394.081
[53,     1] loss: 2275.830
[54,     1] loss: 2196.544
[55,     1] loss: 2212.655
[56,     1] loss: 2134.755
[57,     1] loss: 2059.361
[58,     1] loss: 2139.012
[59,     1] loss: 2051.512
[60,     1] loss: 1786.803
[61,     1] loss: 1794.010
[62,     1] loss: 1927.913
[63,     1] loss: 2434.202
[64,     1] loss: 2478.649
[65,     1] loss: 2188.447
[66,     1] loss: 1765.326
[67,     1] loss: 1989.938
[68,     1] loss: 1784.832
[69,     1] loss: 1863.260
[70,     1] loss: 1824.656
[71,     1] loss: 1877.742
[72,     1] loss: 1730.817
[73,     1] loss: 1705.982
[74,     1] loss: 3884.819
[75,     1] loss: 4119.191
[76,     1] loss: 3027.142
[77,     1] loss: 2232.707
[78,     1] loss: 3051.649
[79,     1] loss: 2895.832
[80,     1] loss: 2801.880
[81,     1] loss: 2910.921
[82,     1] loss: 3059.080
Early stopping applied (best metric=0.46424323320388794)
Finished Training
Total time taken: 12.155688047409058
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3776.131
[2,     1] loss: 3759.043
[3,     1] loss: 3777.017
[4,     1] loss: 3758.887
[5,     1] loss: 3777.779
[6,     1] loss: 3764.762
[7,     1] loss: 3763.072
[8,     1] loss: 3774.688
[9,     1] loss: 3743.019
[10,     1] loss: 3766.632
[11,     1] loss: 3764.448
[12,     1] loss: 3751.830
[13,     1] loss: 3732.416
[14,     1] loss: 3740.705
[15,     1] loss: 3684.101
[16,     1] loss: 3667.520
[17,     1] loss: 3595.218
[18,     1] loss: 3549.013
[19,     1] loss: 3448.970
[20,     1] loss: 3405.316
[21,     1] loss: 3285.016
[22,     1] loss: 3304.066
[23,     1] loss: 3253.302
[24,     1] loss: 3145.599
[25,     1] loss: 3025.410
[26,     1] loss: 3333.164
[27,     1] loss: 2890.487
[28,     1] loss: 3138.604
[29,     1] loss: 2848.089
[30,     1] loss: 3044.278
[31,     1] loss: 2830.568
[32,     1] loss: 2909.249
[33,     1] loss: 2784.764
[34,     1] loss: 2656.146
[35,     1] loss: 2623.598
[36,     1] loss: 2409.085
[37,     1] loss: 2746.348
[38,     1] loss: 2626.672
[39,     1] loss: 2417.266
[40,     1] loss: 2644.206
[41,     1] loss: 2425.514
[42,     1] loss: 2332.495
[43,     1] loss: 2344.042
[44,     1] loss: 2263.949
[45,     1] loss: 2261.130
[46,     1] loss: 2155.282
[47,     1] loss: 2228.861
[48,     1] loss: 2371.857
[49,     1] loss: 2132.154
[50,     1] loss: 2346.980
[51,     1] loss: 2008.848
[52,     1] loss: 2134.678
[53,     1] loss: 2230.703
[54,     1] loss: 1968.287
[55,     1] loss: 2179.591
[56,     1] loss: 1966.258
[57,     1] loss: 2232.196
[58,     1] loss: 1985.185
[59,     1] loss: 2050.799
[60,     1] loss: 1998.045
[61,     1] loss: 1979.661
[62,     1] loss: 1945.491
[63,     1] loss: 1738.026
[64,     1] loss: 1728.889
[65,     1] loss: 1807.648
[66,     1] loss: 1654.952
[67,     1] loss: 1805.835
[68,     1] loss: 1812.551
[69,     1] loss: 2003.343
[70,     1] loss: 1734.831
[71,     1] loss: 1919.243
[72,     1] loss: 2751.697
[73,     1] loss: 1824.497
[74,     1] loss: 1899.590
[75,     1] loss: 1722.413
[76,     1] loss: 1827.831
[77,     1] loss: 1878.051
[78,     1] loss: 1805.223
[79,     1] loss: 1798.139
[80,     1] loss: 1636.949
[81,     1] loss: 1657.092
[82,     1] loss: 2386.995
[83,     1] loss: 2884.084
[84,     1] loss: 2305.175
[85,     1] loss: 2310.063
[86,     1] loss: 1929.246
[87,     1] loss: 2522.246
[88,     1] loss: 1938.464
[89,     1] loss: 2035.687
[90,     1] loss: 2405.849
[91,     1] loss: 1900.711
[92,     1] loss: 1855.296
[93,     1] loss: 2236.174
Early stopping applied (best metric=0.3731066882610321)
Finished Training
Total time taken: 13.776601552963257
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3769.368
[2,     1] loss: 3825.534
[3,     1] loss: 3791.273
[4,     1] loss: 3779.420
[5,     1] loss: 3766.193
[6,     1] loss: 3768.679
[7,     1] loss: 3764.067
[8,     1] loss: 3760.026
[9,     1] loss: 3763.692
[10,     1] loss: 3760.708
[11,     1] loss: 3761.139
[12,     1] loss: 3750.193
[13,     1] loss: 3745.337
[14,     1] loss: 3730.912
[15,     1] loss: 3710.655
[16,     1] loss: 3681.918
[17,     1] loss: 3601.880
[18,     1] loss: 3580.901
[19,     1] loss: 3435.582
[20,     1] loss: 3367.373
[21,     1] loss: 3279.579
[22,     1] loss: 3300.563
[23,     1] loss: 3114.495
[24,     1] loss: 3230.379
[25,     1] loss: 3107.467
[26,     1] loss: 3139.519
[27,     1] loss: 3020.544
[28,     1] loss: 3059.369
[29,     1] loss: 2967.515
[30,     1] loss: 2920.069
[31,     1] loss: 2850.778
[32,     1] loss: 2876.446
[33,     1] loss: 2777.912
[34,     1] loss: 2602.066
[35,     1] loss: 2841.759
[36,     1] loss: 2690.914
[37,     1] loss: 2913.258
[38,     1] loss: 2795.682
[39,     1] loss: 2472.733
[40,     1] loss: 2526.438
[41,     1] loss: 2552.790
[42,     1] loss: 2453.250
[43,     1] loss: 2675.419
[44,     1] loss: 2441.079
[45,     1] loss: 2440.281
[46,     1] loss: 2416.401
[47,     1] loss: 2485.302
[48,     1] loss: 2695.574
[49,     1] loss: 2264.970
[50,     1] loss: 2501.322
[51,     1] loss: 2200.919
[52,     1] loss: 2484.636
[53,     1] loss: 2187.068
[54,     1] loss: 2504.634
[55,     1] loss: 2376.482
[56,     1] loss: 2320.794
[57,     1] loss: 2426.523
[58,     1] loss: 2277.614
[59,     1] loss: 2271.420
[60,     1] loss: 2121.818
[61,     1] loss: 2556.167
[62,     1] loss: 2014.576
[63,     1] loss: 2607.193
[64,     1] loss: 1983.766
[65,     1] loss: 2398.354
[66,     1] loss: 2111.712
[67,     1] loss: 2203.034
[68,     1] loss: 2061.911
[69,     1] loss: 1820.256
[70,     1] loss: 2090.646
[71,     1] loss: 1719.015
[72,     1] loss: 2121.372
[73,     1] loss: 1769.736
[74,     1] loss: 2047.886
[75,     1] loss: 1756.451
[76,     1] loss: 2120.466
[77,     1] loss: 1976.536
[78,     1] loss: 2124.536
[79,     1] loss: 2131.085
[80,     1] loss: 2088.202
[81,     1] loss: 2002.447
[82,     1] loss: 2100.698
[83,     1] loss: 1766.948
[84,     1] loss: 2054.154
[85,     1] loss: 1705.104
[86,     1] loss: 1775.725
[87,     1] loss: 1666.755
[88,     1] loss: 1824.897
[89,     1] loss: 1791.767
[90,     1] loss: 1763.306
[91,     1] loss: 1690.240
[92,     1] loss: 1732.143
[93,     1] loss: 1589.399
[94,     1] loss: 1564.815
[95,     1] loss: 1605.219
[96,     1] loss: 1390.252
[97,     1] loss: 1515.691
[98,     1] loss: 1653.998
[99,     1] loss: 3104.460
[100,     1] loss: 7564.465
[101,     1] loss: 5191.277
[102,     1] loss: 3095.044
[103,     1] loss: 3168.499
[104,     1] loss: 3174.002
[105,     1] loss: 3718.302
[106,     1] loss: 3372.520
[107,     1] loss: 3340.452
[108,     1] loss: 3424.914
[109,     1] loss: 3458.059
[110,     1] loss: 3416.275
[111,     1] loss: 3368.645
[112,     1] loss: 3313.768
[113,     1] loss: 3355.974
[114,     1] loss: 3400.958
[115,     1] loss: 3335.233
Early stopping applied (best metric=0.41677799820899963)
Finished Training
Total time taken: 16.957371950149536
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3773.750
[2,     1] loss: 3770.637
[3,     1] loss: 3773.352
[4,     1] loss: 3741.979
[5,     1] loss: 3763.444
[6,     1] loss: 3798.357
[7,     1] loss: 3780.714
[8,     1] loss: 3761.918
[9,     1] loss: 3784.318
[10,     1] loss: 3776.752
[11,     1] loss: 3791.838
[12,     1] loss: 3760.681
[13,     1] loss: 3769.765
[14,     1] loss: 3764.812
[15,     1] loss: 3765.310
[16,     1] loss: 3767.000
[17,     1] loss: 3763.137
[18,     1] loss: 3761.156
[19,     1] loss: 3761.654
[20,     1] loss: 3755.182
[21,     1] loss: 3753.282
[22,     1] loss: 3740.985
[23,     1] loss: 3740.563
[24,     1] loss: 3711.039
[25,     1] loss: 3669.375
[26,     1] loss: 3620.545
[27,     1] loss: 3561.699
[28,     1] loss: 3470.197
[29,     1] loss: 3512.076
[30,     1] loss: 3291.977
[31,     1] loss: 3237.782
[32,     1] loss: 3307.297
[33,     1] loss: 3194.514
[34,     1] loss: 2979.777
[35,     1] loss: 3485.516
[36,     1] loss: 3184.877
[37,     1] loss: 3080.357
[38,     1] loss: 3002.635
[39,     1] loss: 2935.004
[40,     1] loss: 2929.153
[41,     1] loss: 2742.730
[42,     1] loss: 2807.191
[43,     1] loss: 2922.209
[44,     1] loss: 2822.670
[45,     1] loss: 2930.853
[46,     1] loss: 2572.343
[47,     1] loss: 2526.731
[48,     1] loss: 2778.265
[49,     1] loss: 2640.419
[50,     1] loss: 2421.400
[51,     1] loss: 2605.553
[52,     1] loss: 2391.810
[53,     1] loss: 2469.720
[54,     1] loss: 3280.164
[55,     1] loss: 2522.321
[56,     1] loss: 2361.765
[57,     1] loss: 2425.141
[58,     1] loss: 2413.447
[59,     1] loss: 2392.269
[60,     1] loss: 2426.020
[61,     1] loss: 2216.265
[62,     1] loss: 2317.187
[63,     1] loss: 2230.799
[64,     1] loss: 2002.040
[65,     1] loss: 1810.321
[66,     1] loss: 1973.263
[67,     1] loss: 2440.727
[68,     1] loss: 3069.603
[69,     1] loss: 1975.728
[70,     1] loss: 2354.331
[71,     1] loss: 2140.721
[72,     1] loss: 2433.634
[73,     1] loss: 2112.676
[74,     1] loss: 2113.504
[75,     1] loss: 2155.235
[76,     1] loss: 2159.621
[77,     1] loss: 1957.069
[78,     1] loss: 2159.124
[79,     1] loss: 1702.308
[80,     1] loss: 2241.325
[81,     1] loss: 2221.547
[82,     1] loss: 1986.133
[83,     1] loss: 1966.136
[84,     1] loss: 2220.990
[85,     1] loss: 1807.116
[86,     1] loss: 1920.311
[87,     1] loss: 1799.487
[88,     1] loss: 1869.141
[89,     1] loss: 2023.150
[90,     1] loss: 1695.191
[91,     1] loss: 2147.358
[92,     1] loss: 2753.995
[93,     1] loss: 1956.241
[94,     1] loss: 2398.050
[95,     1] loss: 2314.029
[96,     1] loss: 2103.557
[97,     1] loss: 2190.307
[98,     1] loss: 2177.916
[99,     1] loss: 1810.641
[100,     1] loss: 1995.253
[101,     1] loss: 1812.103
[102,     1] loss: 2118.200
[103,     1] loss: 1916.793
[104,     1] loss: 1874.762
[105,     1] loss: 1724.065
[106,     1] loss: 1876.561
[107,     1] loss: 2060.786
[108,     1] loss: 2064.409
[109,     1] loss: 1704.096
[110,     1] loss: 1809.374
[111,     1] loss: 1939.794
[112,     1] loss: 1943.673
[113,     1] loss: 1766.822
[114,     1] loss: 1634.986
[115,     1] loss: 1906.895
[116,     1] loss: 1930.244
[117,     1] loss: 1660.867
[118,     1] loss: 1806.793
[119,     1] loss: 3291.160
[120,     1] loss: 5182.853
[121,     1] loss: 2572.608
[122,     1] loss: 3115.521
[123,     1] loss: 3222.397
Early stopping applied (best metric=0.40646639466285706)
Finished Training
Total time taken: 18.245643615722656
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3778.630
[2,     1] loss: 3778.048
[3,     1] loss: 3763.286
[4,     1] loss: 3780.615
[5,     1] loss: 3760.475
[6,     1] loss: 3755.452
[7,     1] loss: 3764.167
[8,     1] loss: 3758.071
[9,     1] loss: 3737.564
[10,     1] loss: 3715.599
[11,     1] loss: 3681.287
[12,     1] loss: 3551.448
[13,     1] loss: 3474.648
[14,     1] loss: 3368.576
[15,     1] loss: 3353.651
[16,     1] loss: 3147.044
[17,     1] loss: 3175.688
[18,     1] loss: 2882.100
[19,     1] loss: 3080.044
[20,     1] loss: 2822.994
[21,     1] loss: 2828.499
[22,     1] loss: 2916.237
[23,     1] loss: 3005.701
[24,     1] loss: 2914.001
[25,     1] loss: 2796.888
[26,     1] loss: 2797.447
[27,     1] loss: 2714.825
[28,     1] loss: 2737.150
[29,     1] loss: 2671.831
[30,     1] loss: 2465.699
[31,     1] loss: 2427.729
[32,     1] loss: 2562.554
[33,     1] loss: 2422.203
[34,     1] loss: 2421.639
[35,     1] loss: 2407.997
[36,     1] loss: 2546.286
[37,     1] loss: 2598.220
[38,     1] loss: 3205.604
[39,     1] loss: 3228.629
[40,     1] loss: 2563.643
[41,     1] loss: 3111.307
[42,     1] loss: 2593.164
[43,     1] loss: 2757.563
[44,     1] loss: 2853.401
[45,     1] loss: 2631.317
[46,     1] loss: 2405.384
[47,     1] loss: 2515.646
[48,     1] loss: 2436.364
[49,     1] loss: 2413.107
[50,     1] loss: 2394.089
[51,     1] loss: 2157.112
[52,     1] loss: 2260.706
[53,     1] loss: 2188.905
[54,     1] loss: 2163.021
[55,     1] loss: 2220.627
[56,     1] loss: 2177.358
[57,     1] loss: 2021.378
[58,     1] loss: 2019.098
[59,     1] loss: 1992.492
[60,     1] loss: 2171.705
[61,     1] loss: 2079.748
[62,     1] loss: 1862.909
[63,     1] loss: 2023.417
[64,     1] loss: 2294.252
Early stopping applied (best metric=0.5123778581619263)
Finished Training
Total time taken: 9.452604055404663
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3792.569
[2,     1] loss: 3758.434
[3,     1] loss: 3773.139
[4,     1] loss: 3763.067
[5,     1] loss: 3763.457
[6,     1] loss: 3766.103
[7,     1] loss: 3771.842
[8,     1] loss: 3760.686
[9,     1] loss: 3750.844
[10,     1] loss: 3745.243
[11,     1] loss: 3744.010
[12,     1] loss: 3721.197
[13,     1] loss: 3676.394
[14,     1] loss: 3633.747
[15,     1] loss: 3571.261
[16,     1] loss: 3495.016
[17,     1] loss: 3388.186
[18,     1] loss: 3352.077
[19,     1] loss: 3303.977
[20,     1] loss: 3313.454
[21,     1] loss: 3137.979
[22,     1] loss: 3115.684
[23,     1] loss: 3192.774
[24,     1] loss: 3367.229
[25,     1] loss: 3181.782
[26,     1] loss: 2949.078
[27,     1] loss: 3087.691
[28,     1] loss: 3038.087
[29,     1] loss: 2954.006
[30,     1] loss: 2917.238
[31,     1] loss: 2947.011
[32,     1] loss: 2901.120
[33,     1] loss: 2838.030
[34,     1] loss: 2666.477
[35,     1] loss: 2684.175
[36,     1] loss: 2546.605
[37,     1] loss: 2660.471
[38,     1] loss: 2492.408
[39,     1] loss: 2335.311
[40,     1] loss: 2843.986
[41,     1] loss: 2702.872
[42,     1] loss: 2442.155
[43,     1] loss: 2440.680
[44,     1] loss: 2638.395
[45,     1] loss: 2652.531
[46,     1] loss: 2562.655
[47,     1] loss: 2464.469
[48,     1] loss: 2478.935
[49,     1] loss: 2481.412
[50,     1] loss: 2340.539
[51,     1] loss: 2220.686
[52,     1] loss: 2397.061
[53,     1] loss: 2256.668
[54,     1] loss: 2045.500
[55,     1] loss: 2154.747
[56,     1] loss: 2103.502
[57,     1] loss: 2317.518
[58,     1] loss: 2833.090
[59,     1] loss: 3258.036
[60,     1] loss: 2238.428
[61,     1] loss: 2889.934
[62,     1] loss: 2503.999
[63,     1] loss: 2491.075
[64,     1] loss: 2499.380
[65,     1] loss: 2685.605
[66,     1] loss: 2442.935
[67,     1] loss: 2348.074
[68,     1] loss: 2187.590
[69,     1] loss: 2414.425
[70,     1] loss: 2363.767
[71,     1] loss: 2446.050
[72,     1] loss: 1900.590
[73,     1] loss: 2318.544
[74,     1] loss: 1861.084
[75,     1] loss: 2345.171
[76,     1] loss: 2192.611
[77,     1] loss: 2446.491
[78,     1] loss: 2047.366
[79,     1] loss: 2570.962
[80,     1] loss: 2076.665
[81,     1] loss: 2393.444
[82,     1] loss: 2035.072
Early stopping applied (best metric=0.4050310254096985)
Finished Training
Total time taken: 12.13053584098816
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3795.819
[2,     1] loss: 3798.963
[3,     1] loss: 3764.906
[4,     1] loss: 3771.658
[5,     1] loss: 3762.732
[6,     1] loss: 3762.078
[7,     1] loss: 3779.972
[8,     1] loss: 3771.230
[9,     1] loss: 3776.103
[10,     1] loss: 3770.992
[11,     1] loss: 3768.354
[12,     1] loss: 3768.491
[13,     1] loss: 3766.461
[14,     1] loss: 3765.330
[15,     1] loss: 3759.584
[16,     1] loss: 3762.766
[17,     1] loss: 3759.828
[18,     1] loss: 3758.612
[19,     1] loss: 3757.582
[20,     1] loss: 3755.022
[21,     1] loss: 3749.954
[22,     1] loss: 3738.372
[23,     1] loss: 3713.156
[24,     1] loss: 3692.924
[25,     1] loss: 3659.586
[26,     1] loss: 3613.635
[27,     1] loss: 3557.365
[28,     1] loss: 3557.309
[29,     1] loss: 3479.194
[30,     1] loss: 3446.624
[31,     1] loss: 3427.940
[32,     1] loss: 3410.772
[33,     1] loss: 3249.383
[34,     1] loss: 3178.438
[35,     1] loss: 3114.744
[36,     1] loss: 3143.212
[37,     1] loss: 3127.847
[38,     1] loss: 3065.606
[39,     1] loss: 3176.837
[40,     1] loss: 3023.920
[41,     1] loss: 3019.088
[42,     1] loss: 3227.538
[43,     1] loss: 3073.846
[44,     1] loss: 2990.560
[45,     1] loss: 3086.250
[46,     1] loss: 3055.652
[47,     1] loss: 2868.470
[48,     1] loss: 2796.177
[49,     1] loss: 2754.395
[50,     1] loss: 2684.571
[51,     1] loss: 2680.671
[52,     1] loss: 2912.789
[53,     1] loss: 2442.563
[54,     1] loss: 2644.705
[55,     1] loss: 2714.443
[56,     1] loss: 2553.432
[57,     1] loss: 2477.292
[58,     1] loss: 2854.205
[59,     1] loss: 2960.946
[60,     1] loss: 2578.048
[61,     1] loss: 2736.499
[62,     1] loss: 2434.978
[63,     1] loss: 2397.387
[64,     1] loss: 2170.664
[65,     1] loss: 2602.417
[66,     1] loss: 2258.328
[67,     1] loss: 2429.276
[68,     1] loss: 2092.928
[69,     1] loss: 1989.168
[70,     1] loss: 2059.168
[71,     1] loss: 1863.300
[72,     1] loss: 1897.682
[73,     1] loss: 2025.148
[74,     1] loss: 1654.186
[75,     1] loss: 1942.080
[76,     1] loss: 2264.030
[77,     1] loss: 3487.669
[78,     1] loss: 2682.989
[79,     1] loss: 2305.733
[80,     1] loss: 2359.209
[81,     1] loss: 2650.939
[82,     1] loss: 2645.329
[83,     1] loss: 2205.101
[84,     1] loss: 2181.182
[85,     1] loss: 2436.403
[86,     1] loss: 2037.340
[87,     1] loss: 2052.356
[88,     1] loss: 1985.529
[89,     1] loss: 1869.773
[90,     1] loss: 1961.844
[91,     1] loss: 1823.135
[92,     1] loss: 1806.244
[93,     1] loss: 1696.022
[94,     1] loss: 1745.081
[95,     1] loss: 1958.890
[96,     1] loss: 2051.027
[97,     1] loss: 2040.165
[98,     1] loss: 1639.518
[99,     1] loss: 2086.273
[100,     1] loss: 1763.035
[101,     1] loss: 2256.993
[102,     1] loss: 2192.223
[103,     1] loss: 1726.931
[104,     1] loss: 2453.398
[105,     1] loss: 1657.146
Early stopping applied (best metric=0.41214102506637573)
Finished Training
Total time taken: 15.727603673934937
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3788.900
[2,     1] loss: 3775.861
[3,     1] loss: 3785.333
[4,     1] loss: 3767.364
[5,     1] loss: 3764.031
[6,     1] loss: 3780.372
[7,     1] loss: 3762.235
[8,     1] loss: 3761.033
[9,     1] loss: 3759.459
[10,     1] loss: 3772.935
[11,     1] loss: 3763.553
[12,     1] loss: 3767.604
[13,     1] loss: 3751.790
[14,     1] loss: 3749.426
[15,     1] loss: 3735.706
[16,     1] loss: 3717.953
[17,     1] loss: 3675.143
[18,     1] loss: 3667.084
[19,     1] loss: 3593.186
[20,     1] loss: 3518.441
[21,     1] loss: 3466.477
[22,     1] loss: 3446.113
[23,     1] loss: 3199.969
[24,     1] loss: 3311.106
[25,     1] loss: 3121.020
[26,     1] loss: 2989.628
[27,     1] loss: 3153.538
[28,     1] loss: 3084.895
[29,     1] loss: 2889.631
[30,     1] loss: 2939.382
[31,     1] loss: 3035.403
[32,     1] loss: 3390.764
[33,     1] loss: 3026.599
[34,     1] loss: 3077.720
[35,     1] loss: 2952.556
[36,     1] loss: 3211.826
[37,     1] loss: 2992.637
[38,     1] loss: 2842.232
[39,     1] loss: 2807.635
[40,     1] loss: 2834.841
[41,     1] loss: 2731.979
[42,     1] loss: 2885.451
[43,     1] loss: 2972.149
[44,     1] loss: 2789.070
[45,     1] loss: 2643.477
[46,     1] loss: 2694.653
[47,     1] loss: 2522.250
[48,     1] loss: 2649.229
[49,     1] loss: 2783.283
[50,     1] loss: 2255.603
[51,     1] loss: 2602.158
[52,     1] loss: 2459.780
[53,     1] loss: 2443.385
[54,     1] loss: 2342.369
[55,     1] loss: 2296.843
[56,     1] loss: 2239.486
[57,     1] loss: 1999.958
[58,     1] loss: 2123.085
[59,     1] loss: 2368.510
[60,     1] loss: 2253.493
[61,     1] loss: 2001.034
[62,     1] loss: 2516.059
[63,     1] loss: 2867.678
[64,     1] loss: 1902.837
[65,     1] loss: 2440.433
[66,     1] loss: 2381.015
[67,     1] loss: 2366.881
[68,     1] loss: 2280.144
[69,     1] loss: 2306.886
[70,     1] loss: 2173.763
[71,     1] loss: 2153.074
[72,     1] loss: 1963.140
[73,     1] loss: 1904.320
[74,     1] loss: 1712.007
[75,     1] loss: 1933.531
[76,     1] loss: 2027.533
[77,     1] loss: 1725.088
[78,     1] loss: 1986.708
[79,     1] loss: 2190.978
[80,     1] loss: 1814.013
[81,     1] loss: 1831.959
[82,     1] loss: 2110.797
[83,     1] loss: 1856.622
[84,     1] loss: 1883.822
[85,     1] loss: 1653.772
[86,     1] loss: 1705.108
[87,     1] loss: 1637.439
[88,     1] loss: 1767.583
[89,     1] loss: 1519.440
Early stopping applied (best metric=0.4464637339115143)
Finished Training
Total time taken: 13.168672323226929
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 3782.491
[2,     1] loss: 3767.821
[3,     1] loss: 3769.966
[4,     1] loss: 3768.697
[5,     1] loss: 3760.422
[6,     1] loss: 3765.674
[7,     1] loss: 3764.380
[8,     1] loss: 3764.456
[9,     1] loss: 3760.768
[10,     1] loss: 3765.865
[11,     1] loss: 3754.858
[12,     1] loss: 3736.950
[13,     1] loss: 3715.346
[14,     1] loss: 3665.044
[15,     1] loss: 3599.330
[16,     1] loss: 3510.621
[17,     1] loss: 3419.043
[18,     1] loss: 3300.707
[19,     1] loss: 3318.237
[20,     1] loss: 3353.748
[21,     1] loss: 3224.575
[22,     1] loss: 3234.005
[23,     1] loss: 3122.289
[24,     1] loss: 2982.141
[25,     1] loss: 3105.236
[26,     1] loss: 2927.583
[27,     1] loss: 3020.192
[28,     1] loss: 2899.567
[29,     1] loss: 3129.340
[30,     1] loss: 3058.568
[31,     1] loss: 3028.181
[32,     1] loss: 2843.446
[33,     1] loss: 2821.377
[34,     1] loss: 2835.426
[35,     1] loss: 2708.233
[36,     1] loss: 2568.471
[37,     1] loss: 2596.827
[38,     1] loss: 2724.685
[39,     1] loss: 2495.167
[40,     1] loss: 3163.489
[41,     1] loss: 2416.274
[42,     1] loss: 2667.750
[43,     1] loss: 2412.365
[44,     1] loss: 2474.180
[45,     1] loss: 2660.011
[46,     1] loss: 2362.772
[47,     1] loss: 2374.904
[48,     1] loss: 2175.211
[49,     1] loss: 2354.500
[50,     1] loss: 2094.467
[51,     1] loss: 2207.885
[52,     1] loss: 2064.474
[53,     1] loss: 2426.526
[54,     1] loss: 3151.515
[55,     1] loss: 2065.652
[56,     1] loss: 2700.094
[57,     1] loss: 2315.588
[58,     1] loss: 2679.466
[59,     1] loss: 2524.185
[60,     1] loss: 2509.138
[61,     1] loss: 2451.792
[62,     1] loss: 2364.498
[63,     1] loss: 2332.209
[64,     1] loss: 2139.470
[65,     1] loss: 2305.774
[66,     1] loss: 2187.010
[67,     1] loss: 2300.369
[68,     1] loss: 1842.697
[69,     1] loss: 2066.746
[70,     1] loss: 2021.891
[71,     1] loss: 1913.298
[72,     1] loss: 2444.784
[73,     1] loss: 1987.167
[74,     1] loss: 2003.195
[75,     1] loss: 1843.893
[76,     1] loss: 2324.314
[77,     1] loss: 1765.986
[78,     1] loss: 2217.791
[79,     1] loss: 1824.596
[80,     1] loss: 2332.127
[81,     1] loss: 1722.771
[82,     1] loss: 2053.306
[83,     1] loss: 1692.563
[84,     1] loss: 1783.108
[85,     1] loss: 1737.638
[86,     1] loss: 2036.124
[87,     1] loss: 1798.751
[88,     1] loss: 1577.270
[89,     1] loss: 1928.387
[90,     1] loss: 1710.524
Early stopping applied (best metric=0.32302749156951904)
Finished Training
Total time taken: 13.286205768585205
{'Hydroxylation-K Validation Accuracy': 0.7225768321513002, 'Hydroxylation-K Validation Sensitivity': 0.7214814814814815, 'Hydroxylation-K Validation Specificity': 0.7228070175438597, 'Hydroxylation-K Validation Precision': 0.4231639420855107, 'Hydroxylation-K AUC ROC': 0.8004873294346979, 'Hydroxylation-K AUC PR': 0.5750936160044318, 'Hydroxylation-K MCC': 0.3849132306198666, 'Hydroxylation-K F1': 0.521544985275832, 'Validation Loss (Hydroxylation-K)': 0.45897583961486815, 'Hydroxylation-P Validation Accuracy': 0.7353945485000761, 'Hydroxylation-P Validation Sensitivity': 0.7441798941798942, 'Hydroxylation-P Validation Specificity': 0.7334505461619033, 'Hydroxylation-P Validation Precision': 0.38874020076298843, 'Hydroxylation-P AUC ROC': 0.7928692599955506, 'Hydroxylation-P AUC PR': 0.5456455262709666, 'Hydroxylation-P MCC': 0.38937725598263073, 'Hydroxylation-P F1': 0.5067150348754719, 'Validation Loss (Hydroxylation-P)': 0.4138992230097453, 'Validation Loss (total)': 0.8728750626246135, 'TimeToTrain': 13.570394531885784}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006455111888188445,
 'learning_rate_Hydroxylation-K': 0.008295704213562214,
 'learning_rate_Hydroxylation-P': 0.006501384045616175,
 'log_base': 2.2069104586253183,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3178471367,
 'sample_weights': [11.626741295820413, 1.4503228727029838],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.442007671366142,
 'weight_decay_Hydroxylation-K': 1.433439314723155,
 'weight_decay_Hydroxylation-P': 6.323092675453378}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1362.760
[2,     1] loss: 1358.255
[3,     1] loss: 1355.305
[4,     1] loss: 1356.504
[5,     1] loss: 1351.820
[6,     1] loss: 1354.945
[7,     1] loss: 1355.117
[8,     1] loss: 1352.885
[9,     1] loss: 1352.839
[10,     1] loss: 1353.230
[11,     1] loss: 1352.300
[12,     1] loss: 1349.995
[13,     1] loss: 1353.479
[14,     1] loss: 1350.898
[15,     1] loss: 1351.261
[16,     1] loss: 1351.016
[17,     1] loss: 1351.315
[18,     1] loss: 1350.650
[19,     1] loss: 1347.330
[20,     1] loss: 1346.585
[21,     1] loss: 1340.980
[22,     1] loss: 1340.111
[23,     1] loss: 1327.984
[24,     1] loss: 1319.880
[25,     1] loss: 1289.182
[26,     1] loss: 1254.065
[27,     1] loss: 1238.087
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003232197032282438,
 'learning_rate_Hydroxylation-K': 0.004280810376149842,
 'learning_rate_Hydroxylation-P': 0.0007664524187015679,
 'log_base': 1.0595010397223719,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2182507598,
 'sample_weights': [2.1089650579541837, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.869679005958968,
 'weight_decay_Hydroxylation-K': 8.942354351047662,
 'weight_decay_Hydroxylation-P': 6.809508898660965}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9379.620
[2,     1] loss: 9379.094
[3,     1] loss: 9358.525
[4,     1] loss: 9367.602
[5,     1] loss: 9339.835
[6,     1] loss: 9335.284
[7,     1] loss: 9341.698
[8,     1] loss: 9312.421
[9,     1] loss: 9284.555
[10,     1] loss: 9198.793
[11,     1] loss: 9092.036
[12,     1] loss: 8983.686
[13,     1] loss: 8721.841
[14,     1] loss: 8610.831
[15,     1] loss: 8211.716
[16,     1] loss: 8370.388
[17,     1] loss: 8115.787
[18,     1] loss: 8074.597
[19,     1] loss: 7792.261
[20,     1] loss: 7477.037
[21,     1] loss: 7801.452
[22,     1] loss: 8150.468
[23,     1] loss: 7639.703
[24,     1] loss: 7278.684
[25,     1] loss: 7090.693
[26,     1] loss: 7541.701
[27,     1] loss: 7126.042
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006053756135849034,
 'learning_rate_Hydroxylation-K': 0.00963540004094303,
 'learning_rate_Hydroxylation-P': 0.003447376270820722,
 'log_base': 1.0734460680673483,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1525932909,
 'sample_weights': [28.88405887748469, 3.6106464509275393],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.946939113348153,
 'weight_decay_Hydroxylation-K': 6.4612877806763525,
 'weight_decay_Hydroxylation-P': 7.6702375111115115}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7653.271
[2,     1] loss: 7669.833
[3,     1] loss: 7640.405
[4,     1] loss: 7636.594
[5,     1] loss: 7608.195
[6,     1] loss: 7643.131
[7,     1] loss: 7618.543
[8,     1] loss: 7635.661
[9,     1] loss: 7605.485
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019823081510462687,
 'learning_rate_Hydroxylation-K': 0.0010637408390492937,
 'learning_rate_Hydroxylation-P': 0.0024321389118805137,
 'log_base': 2.6216270227817287,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 627280256,
 'sample_weights': [23.555053267827738, 2.9444950878834963],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.2012311997446297,
 'weight_decay_Hydroxylation-K': 7.805309296157559,
 'weight_decay_Hydroxylation-P': 7.633453137550139}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.746
[2,     1] loss: 1272.873
[3,     1] loss: 1274.473
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010810454642765435,
 'learning_rate_Hydroxylation-K': 0.007870532309685166,
 'learning_rate_Hydroxylation-P': 0.0034978842591054204,
 'log_base': 1.1500822835150053,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3894933563,
 'sample_weights': [1.7321556233538669, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.939739491780248,
 'weight_decay_Hydroxylation-K': 8.19128040344187,
 'weight_decay_Hydroxylation-P': 7.020761005731425}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3885.062
[2,     1] loss: 3891.024
[3,     1] loss: 3858.845
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008807246752490441,
 'learning_rate_Hydroxylation-K': 0.006677327185370016,
 'learning_rate_Hydroxylation-P': 0.0029754685712277333,
 'log_base': 1.200022178163041,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1774353551,
 'sample_weights': [11.938793339770367, 1.4924066587539475],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6469308732874373,
 'weight_decay_Hydroxylation-K': 7.290278139691295,
 'weight_decay_Hydroxylation-P': 4.472901619987595}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2969.825
[2,     1] loss: 2975.033
[3,     1] loss: 2964.348
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008680866015618013,
 'learning_rate_Hydroxylation-K': 0.00454918085197873,
 'learning_rate_Hydroxylation-P': 0.0004057289520990621,
 'log_base': 1.4637949772994692,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3909669574,
 'sample_weights': [9.15565863557672, 1.1445014185391365],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.122674875608679,
 'weight_decay_Hydroxylation-K': 8.634061795574704,
 'weight_decay_Hydroxylation-P': 8.20418658974826}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1828.175
[2,     1] loss: 1835.583
[3,     1] loss: 1845.808
[4,     1] loss: 1825.421
[5,     1] loss: 1833.533
[6,     1] loss: 1828.042
[7,     1] loss: 1833.443
[8,     1] loss: 1825.770
[9,     1] loss: 1822.274
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004589338554732857,
 'learning_rate_Hydroxylation-K': 8.626804120124968e-05,
 'learning_rate_Hydroxylation-P': 0.007415349090462956,
 'log_base': 2.7249951921042337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3513683628,
 'sample_weights': [4.381368382203244, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.1709983176239085,
 'weight_decay_Hydroxylation-K': 7.393730461988727,
 'weight_decay_Hydroxylation-P': 3.0478167063381667}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.083
[2,     1] loss: 1262.273
[3,     1] loss: 1254.495
[4,     1] loss: 1254.599
[5,     1] loss: 1255.139
[6,     1] loss: 1247.659
[7,     1] loss: 1236.941
[8,     1] loss: 1205.391
[9,     1] loss: 1172.573
[10,     1] loss: 1156.184
[11,     1] loss: 1100.138
[12,     1] loss: 1087.318
[13,     1] loss: 1075.376
[14,     1] loss: 1078.494
[15,     1] loss: 1044.324
[16,     1] loss: 1052.329
[17,     1] loss: 1054.972
[18,     1] loss: 1022.909
[19,     1] loss: 1067.065
[20,     1] loss: 1047.477
[21,     1] loss: 1034.965
[22,     1] loss: 1025.218
[23,     1] loss: 1056.713
[24,     1] loss: 1005.084
[25,     1] loss: 1007.530
[26,     1] loss: 1030.907
[27,     1] loss: 969.117
[28,     1] loss: 1001.542
[29,     1] loss: 923.917
[30,     1] loss: 996.751
[31,     1] loss: 934.228
[32,     1] loss: 943.284
[33,     1] loss: 930.004
[34,     1] loss: 866.190
[35,     1] loss: 891.326
[36,     1] loss: 855.777
[37,     1] loss: 863.814
[38,     1] loss: 870.032
[39,     1] loss: 867.519
[40,     1] loss: 905.482
[41,     1] loss: 818.224
[42,     1] loss: 938.130
[43,     1] loss: 843.473
[44,     1] loss: 869.476
[45,     1] loss: 794.136
[46,     1] loss: 908.506
[47,     1] loss: 756.661
[48,     1] loss: 825.504
[49,     1] loss: 770.909
[50,     1] loss: 754.307
[51,     1] loss: 815.307
[52,     1] loss: 766.495
[53,     1] loss: 713.957
[54,     1] loss: 668.847
[55,     1] loss: 721.907
[56,     1] loss: 647.215
[57,     1] loss: 760.199
[58,     1] loss: 722.639
[59,     1] loss: 725.733
[60,     1] loss: 775.876
[61,     1] loss: 855.104
[62,     1] loss: 674.510
[63,     1] loss: 825.832
[64,     1] loss: 684.853
[65,     1] loss: 766.530
[66,     1] loss: 731.262
[67,     1] loss: 711.447
[68,     1] loss: 685.799
[69,     1] loss: 665.589
[70,     1] loss: 647.584
[71,     1] loss: 630.872
[72,     1] loss: 679.761
[73,     1] loss: 713.874
[74,     1] loss: 880.133
[75,     1] loss: 644.304
[76,     1] loss: 707.852
[77,     1] loss: 689.576
[78,     1] loss: 680.381
[79,     1] loss: 649.949
[80,     1] loss: 669.595
[81,     1] loss: 636.759
[82,     1] loss: 619.714
[83,     1] loss: 813.954
[84,     1] loss: 627.037
[85,     1] loss: 764.894
[86,     1] loss: 644.641
[87,     1] loss: 684.606
[88,     1] loss: 609.609
[89,     1] loss: 636.557
[90,     1] loss: 495.760
[91,     1] loss: 588.287
[92,     1] loss: 593.783
[93,     1] loss: 517.962
[94,     1] loss: 639.658
[95,     1] loss: 510.018
[96,     1] loss: 490.631
[97,     1] loss: 543.222
[98,     1] loss: 484.015
[99,     1] loss: 587.430
[100,     1] loss: 471.796
[101,     1] loss: 781.906
Early stopping applied (best metric=0.3268834352493286)
Finished Training
Total time taken: 14.943316459655762
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.666
[2,     1] loss: 1257.310
[3,     1] loss: 1258.686
[4,     1] loss: 1263.023
[5,     1] loss: 1255.224
[6,     1] loss: 1255.082
[7,     1] loss: 1243.342
[8,     1] loss: 1225.091
[9,     1] loss: 1191.561
[10,     1] loss: 1161.835
[11,     1] loss: 1120.913
[12,     1] loss: 1118.065
[13,     1] loss: 1078.548
[14,     1] loss: 1061.763
[15,     1] loss: 1032.529
[16,     1] loss: 1026.027
[17,     1] loss: 1041.454
[18,     1] loss: 989.157
[19,     1] loss: 1013.878
[20,     1] loss: 976.691
[21,     1] loss: 957.796
[22,     1] loss: 1035.726
[23,     1] loss: 986.684
[24,     1] loss: 965.072
[25,     1] loss: 1004.639
[26,     1] loss: 935.767
[27,     1] loss: 946.643
[28,     1] loss: 928.901
[29,     1] loss: 974.641
[30,     1] loss: 904.951
[31,     1] loss: 944.474
[32,     1] loss: 857.186
[33,     1] loss: 904.533
[34,     1] loss: 848.815
[35,     1] loss: 906.826
[36,     1] loss: 862.467
[37,     1] loss: 848.558
[38,     1] loss: 863.515
[39,     1] loss: 819.309
[40,     1] loss: 808.728
[41,     1] loss: 814.797
[42,     1] loss: 823.893
[43,     1] loss: 930.457
[44,     1] loss: 846.881
[45,     1] loss: 793.460
[46,     1] loss: 778.804
[47,     1] loss: 762.496
[48,     1] loss: 794.507
[49,     1] loss: 745.269
[50,     1] loss: 803.554
[51,     1] loss: 705.201
[52,     1] loss: 689.838
[53,     1] loss: 764.434
[54,     1] loss: 740.410
[55,     1] loss: 681.486
[56,     1] loss: 712.578
[57,     1] loss: 846.679
[58,     1] loss: 765.409
[59,     1] loss: 680.788
[60,     1] loss: 726.826
[61,     1] loss: 660.426
[62,     1] loss: 725.645
[63,     1] loss: 667.411
[64,     1] loss: 629.697
[65,     1] loss: 677.756
[66,     1] loss: 690.625
[67,     1] loss: 659.825
[68,     1] loss: 550.601
[69,     1] loss: 634.228
[70,     1] loss: 619.910
[71,     1] loss: 600.925
[72,     1] loss: 516.073
[73,     1] loss: 573.408
[74,     1] loss: 583.644
[75,     1] loss: 630.083
[76,     1] loss: 739.582
[77,     1] loss: 582.423
[78,     1] loss: 656.749
[79,     1] loss: 772.901
[80,     1] loss: 722.867
[81,     1] loss: 652.641
[82,     1] loss: 741.690
[83,     1] loss: 587.599
[84,     1] loss: 656.540
[85,     1] loss: 539.565
[86,     1] loss: 737.988
[87,     1] loss: 534.527
[88,     1] loss: 687.426
[89,     1] loss: 524.481
[90,     1] loss: 635.331
[91,     1] loss: 534.036
[92,     1] loss: 597.116
[93,     1] loss: 469.784
[94,     1] loss: 539.399
[95,     1] loss: 524.881
[96,     1] loss: 441.860
[97,     1] loss: 501.404
[98,     1] loss: 467.860
[99,     1] loss: 400.662
[100,     1] loss: 463.021
[101,     1] loss: 615.599
[102,     1] loss: 526.943
[103,     1] loss: 566.223
[104,     1] loss: 469.562
[105,     1] loss: 521.806
[106,     1] loss: 647.294
[107,     1] loss: 427.604
[108,     1] loss: 719.999
[109,     1] loss: 1145.111
[110,     1] loss: 680.556
[111,     1] loss: 746.582
[112,     1] loss: 899.090
[113,     1] loss: 688.869
[114,     1] loss: 719.983
[115,     1] loss: 765.456
[116,     1] loss: 622.519
[117,     1] loss: 796.617
[118,     1] loss: 572.085
[119,     1] loss: 694.173
[120,     1] loss: 609.472
[121,     1] loss: 690.408
[122,     1] loss: 560.520
[123,     1] loss: 596.883
[124,     1] loss: 487.466
[125,     1] loss: 472.305
[126,     1] loss: 507.509
[127,     1] loss: 482.668
[128,     1] loss: 410.883
[129,     1] loss: 427.260
[130,     1] loss: 448.260
[131,     1] loss: 457.105
[132,     1] loss: 524.492
[133,     1] loss: 767.538
[134,     1] loss: 1280.862
[135,     1] loss: 658.938
[136,     1] loss: 834.896
[137,     1] loss: 939.398
[138,     1] loss: 787.758
[139,     1] loss: 784.563
[140,     1] loss: 753.765
[141,     1] loss: 683.027
[142,     1] loss: 709.150
[143,     1] loss: 692.173
[144,     1] loss: 583.116
[145,     1] loss: 917.685
[146,     1] loss: 781.089
[147,     1] loss: 789.899
[148,     1] loss: 682.626
[149,     1] loss: 795.341
[150,     1] loss: 716.117
[151,     1] loss: 720.306
[152,     1] loss: 708.394
Early stopping applied (best metric=0.3637835681438446)
Finished Training
Total time taken: 22.305176734924316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.992
[2,     1] loss: 1264.266
[3,     1] loss: 1264.546
[4,     1] loss: 1256.115
[5,     1] loss: 1258.295
[6,     1] loss: 1259.248
[7,     1] loss: 1253.331
[8,     1] loss: 1247.671
[9,     1] loss: 1231.992
[10,     1] loss: 1211.199
[11,     1] loss: 1188.158
[12,     1] loss: 1153.100
[13,     1] loss: 1122.932
[14,     1] loss: 1082.708
[15,     1] loss: 1054.241
[16,     1] loss: 1025.077
[17,     1] loss: 1021.938
[18,     1] loss: 996.300
[19,     1] loss: 1063.722
[20,     1] loss: 1004.305
[21,     1] loss: 1005.239
[22,     1] loss: 1016.277
[23,     1] loss: 1007.856
[24,     1] loss: 973.109
[25,     1] loss: 1032.234
[26,     1] loss: 930.869
[27,     1] loss: 970.685
[28,     1] loss: 1001.371
[29,     1] loss: 912.921
[30,     1] loss: 904.093
[31,     1] loss: 889.449
[32,     1] loss: 902.046
[33,     1] loss: 907.033
[34,     1] loss: 880.390
[35,     1] loss: 888.567
[36,     1] loss: 879.931
[37,     1] loss: 834.767
[38,     1] loss: 878.504
[39,     1] loss: 833.221
[40,     1] loss: 810.341
[41,     1] loss: 812.805
[42,     1] loss: 817.663
[43,     1] loss: 793.227
[44,     1] loss: 783.811
[45,     1] loss: 800.579
[46,     1] loss: 770.438
[47,     1] loss: 743.305
[48,     1] loss: 746.401
[49,     1] loss: 793.348
[50,     1] loss: 851.288
[51,     1] loss: 739.315
[52,     1] loss: 765.442
[53,     1] loss: 686.929
[54,     1] loss: 829.367
[55,     1] loss: 761.173
[56,     1] loss: 688.807
[57,     1] loss: 676.828
[58,     1] loss: 676.001
[59,     1] loss: 669.743
[60,     1] loss: 607.987
[61,     1] loss: 628.727
[62,     1] loss: 664.261
[63,     1] loss: 902.093
[64,     1] loss: 1009.353
[65,     1] loss: 687.121
[66,     1] loss: 845.641
[67,     1] loss: 788.411
[68,     1] loss: 745.127
[69,     1] loss: 825.247
[70,     1] loss: 808.075
[71,     1] loss: 708.780
[72,     1] loss: 777.347
[73,     1] loss: 784.458
[74,     1] loss: 741.525
[75,     1] loss: 664.459
[76,     1] loss: 702.837
[77,     1] loss: 697.847
[78,     1] loss: 709.146
[79,     1] loss: 618.963
[80,     1] loss: 616.916
[81,     1] loss: 584.657
[82,     1] loss: 547.425
[83,     1] loss: 542.053
[84,     1] loss: 606.648
[85,     1] loss: 533.691
[86,     1] loss: 545.407
[87,     1] loss: 482.310
[88,     1] loss: 466.446
[89,     1] loss: 510.480
[90,     1] loss: 471.160
Early stopping applied (best metric=0.4507266581058502)
Finished Training
Total time taken: 13.232141017913818
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.620
[2,     1] loss: 1259.023
[3,     1] loss: 1262.364
[4,     1] loss: 1263.800
[5,     1] loss: 1259.644
[6,     1] loss: 1260.644
[7,     1] loss: 1258.809
[8,     1] loss: 1260.535
[9,     1] loss: 1256.690
[10,     1] loss: 1254.875
[11,     1] loss: 1250.692
[12,     1] loss: 1242.773
[13,     1] loss: 1233.368
[14,     1] loss: 1214.399
[15,     1] loss: 1189.812
[16,     1] loss: 1162.079
[17,     1] loss: 1137.032
[18,     1] loss: 1092.854
[19,     1] loss: 1062.210
[20,     1] loss: 1043.673
[21,     1] loss: 1039.311
[22,     1] loss: 999.858
[23,     1] loss: 1023.891
[24,     1] loss: 1023.491
[25,     1] loss: 1002.436
[26,     1] loss: 986.863
[27,     1] loss: 989.177
[28,     1] loss: 992.500
[29,     1] loss: 938.019
[30,     1] loss: 960.420
[31,     1] loss: 937.055
[32,     1] loss: 969.818
[33,     1] loss: 967.373
[34,     1] loss: 904.247
[35,     1] loss: 908.392
[36,     1] loss: 893.572
[37,     1] loss: 857.984
[38,     1] loss: 859.272
[39,     1] loss: 917.246
[40,     1] loss: 891.359
[41,     1] loss: 868.926
[42,     1] loss: 807.383
[43,     1] loss: 838.113
[44,     1] loss: 897.066
[45,     1] loss: 826.209
[46,     1] loss: 763.743
[47,     1] loss: 824.706
[48,     1] loss: 826.025
[49,     1] loss: 769.708
[50,     1] loss: 872.657
[51,     1] loss: 851.874
[52,     1] loss: 716.848
[53,     1] loss: 774.445
[54,     1] loss: 796.365
[55,     1] loss: 744.721
[56,     1] loss: 671.039
[57,     1] loss: 676.019
[58,     1] loss: 776.834
[59,     1] loss: 855.723
[60,     1] loss: 933.807
[61,     1] loss: 737.611
[62,     1] loss: 880.487
[63,     1] loss: 741.465
[64,     1] loss: 851.537
[65,     1] loss: 740.382
[66,     1] loss: 746.922
[67,     1] loss: 759.607
[68,     1] loss: 686.877
[69,     1] loss: 764.753
[70,     1] loss: 686.516
[71,     1] loss: 667.291
[72,     1] loss: 594.146
[73,     1] loss: 616.365
[74,     1] loss: 564.185
[75,     1] loss: 566.467
[76,     1] loss: 651.308
[77,     1] loss: 1142.844
[78,     1] loss: 721.579
[79,     1] loss: 575.861
[80,     1] loss: 710.936
[81,     1] loss: 664.610
[82,     1] loss: 650.840
[83,     1] loss: 696.920
[84,     1] loss: 602.328
[85,     1] loss: 665.435
[86,     1] loss: 537.939
[87,     1] loss: 550.401
[88,     1] loss: 648.158
[89,     1] loss: 502.219
[90,     1] loss: 508.981
[91,     1] loss: 516.509
[92,     1] loss: 609.981
[93,     1] loss: 738.177
[94,     1] loss: 471.664
[95,     1] loss: 644.100
[96,     1] loss: 886.591
[97,     1] loss: 651.417
[98,     1] loss: 676.649
[99,     1] loss: 675.254
[100,     1] loss: 629.612
[101,     1] loss: 682.299
[102,     1] loss: 590.459
[103,     1] loss: 606.282
[104,     1] loss: 536.840
[105,     1] loss: 555.356
[106,     1] loss: 468.674
Early stopping applied (best metric=0.34827035665512085)
Finished Training
Total time taken: 15.592425107955933
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1262.698
[2,     1] loss: 1259.061
[3,     1] loss: 1269.473
[4,     1] loss: 1256.518
[5,     1] loss: 1258.532
[6,     1] loss: 1248.125
[7,     1] loss: 1237.532
[8,     1] loss: 1213.557
[9,     1] loss: 1192.145
[10,     1] loss: 1165.402
[11,     1] loss: 1117.882
[12,     1] loss: 1085.538
[13,     1] loss: 1104.480
[14,     1] loss: 1048.337
[15,     1] loss: 1084.421
[16,     1] loss: 1082.912
[17,     1] loss: 1025.787
[18,     1] loss: 1091.546
[19,     1] loss: 1026.731
[20,     1] loss: 1074.386
[21,     1] loss: 1010.300
[22,     1] loss: 1047.844
[23,     1] loss: 991.723
[24,     1] loss: 982.877
[25,     1] loss: 985.256
[26,     1] loss: 975.319
[27,     1] loss: 1036.309
[28,     1] loss: 951.387
[29,     1] loss: 1007.368
[30,     1] loss: 930.446
[31,     1] loss: 927.492
[32,     1] loss: 922.955
[33,     1] loss: 940.949
[34,     1] loss: 892.429
[35,     1] loss: 884.626
[36,     1] loss: 880.447
[37,     1] loss: 906.035
[38,     1] loss: 901.452
[39,     1] loss: 836.269
[40,     1] loss: 827.758
[41,     1] loss: 856.652
[42,     1] loss: 942.625
[43,     1] loss: 1097.989
[44,     1] loss: 833.348
[45,     1] loss: 989.241
[46,     1] loss: 864.500
[47,     1] loss: 895.172
[48,     1] loss: 945.227
[49,     1] loss: 893.224
[50,     1] loss: 853.763
[51,     1] loss: 878.150
[52,     1] loss: 910.162
[53,     1] loss: 842.416
[54,     1] loss: 864.504
[55,     1] loss: 823.095
[56,     1] loss: 818.891
[57,     1] loss: 849.639
[58,     1] loss: 822.568
[59,     1] loss: 788.504
[60,     1] loss: 733.756
[61,     1] loss: 781.511
[62,     1] loss: 757.157
[63,     1] loss: 753.017
[64,     1] loss: 707.723
[65,     1] loss: 671.741
[66,     1] loss: 751.558
[67,     1] loss: 860.939
[68,     1] loss: 703.576
[69,     1] loss: 679.212
[70,     1] loss: 733.414
[71,     1] loss: 670.717
[72,     1] loss: 741.294
[73,     1] loss: 762.551
[74,     1] loss: 601.411
[75,     1] loss: 786.946
[76,     1] loss: 667.695
[77,     1] loss: 628.648
[78,     1] loss: 621.219
[79,     1] loss: 570.680
[80,     1] loss: 669.299
[81,     1] loss: 626.235
[82,     1] loss: 661.316
[83,     1] loss: 543.841
[84,     1] loss: 535.238
[85,     1] loss: 474.475
[86,     1] loss: 528.004
[87,     1] loss: 620.531
[88,     1] loss: 931.736
[89,     1] loss: 681.706
[90,     1] loss: 509.108
[91,     1] loss: 714.523
[92,     1] loss: 550.977
[93,     1] loss: 623.227
[94,     1] loss: 529.197
[95,     1] loss: 611.043
[96,     1] loss: 501.900
[97,     1] loss: 513.041
[98,     1] loss: 581.112
[99,     1] loss: 489.319
[100,     1] loss: 490.759
[101,     1] loss: 541.208
[102,     1] loss: 533.051
[103,     1] loss: 511.248
[104,     1] loss: 410.849
[105,     1] loss: 440.363
[106,     1] loss: 533.463
[107,     1] loss: 495.112
[108,     1] loss: 494.470
[109,     1] loss: 623.153
[110,     1] loss: 656.573
[111,     1] loss: 442.808
[112,     1] loss: 610.585
[113,     1] loss: 668.919
[114,     1] loss: 467.678
[115,     1] loss: 808.907
[116,     1] loss: 663.013
[117,     1] loss: 644.416
[118,     1] loss: 566.804
[119,     1] loss: 704.807
[120,     1] loss: 467.506
[121,     1] loss: 574.769
[122,     1] loss: 451.764
[123,     1] loss: 604.368
[124,     1] loss: 706.414
[125,     1] loss: 489.927
[126,     1] loss: 648.847
[127,     1] loss: 516.776
[128,     1] loss: 617.792
[129,     1] loss: 492.849
[130,     1] loss: 615.990
[131,     1] loss: 446.739
[132,     1] loss: 542.488
[133,     1] loss: 494.648
[134,     1] loss: 403.081
[135,     1] loss: 494.726
Early stopping applied (best metric=0.2903459072113037)
Finished Training
Total time taken: 19.787189245224
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.618
[2,     1] loss: 1260.683
[3,     1] loss: 1257.152
[4,     1] loss: 1262.260
[5,     1] loss: 1257.800
[6,     1] loss: 1257.289
[7,     1] loss: 1258.167
[8,     1] loss: 1250.034
[9,     1] loss: 1242.550
[10,     1] loss: 1233.865
[11,     1] loss: 1211.896
[12,     1] loss: 1187.763
[13,     1] loss: 1153.271
[14,     1] loss: 1122.312
[15,     1] loss: 1083.641
[16,     1] loss: 1064.067
[17,     1] loss: 1028.677
[18,     1] loss: 1050.625
[19,     1] loss: 1067.588
[20,     1] loss: 1012.707
[21,     1] loss: 1051.714
[22,     1] loss: 1010.504
[23,     1] loss: 1014.145
[24,     1] loss: 975.428
[25,     1] loss: 992.969
[26,     1] loss: 979.975
[27,     1] loss: 959.130
[28,     1] loss: 970.801
[29,     1] loss: 900.715
[30,     1] loss: 917.022
[31,     1] loss: 948.374
[32,     1] loss: 926.791
[33,     1] loss: 909.911
[34,     1] loss: 891.767
[35,     1] loss: 861.647
[36,     1] loss: 871.348
[37,     1] loss: 875.895
[38,     1] loss: 862.916
[39,     1] loss: 838.436
[40,     1] loss: 964.167
[41,     1] loss: 943.893
[42,     1] loss: 873.914
[43,     1] loss: 928.862
[44,     1] loss: 891.534
[45,     1] loss: 855.706
[46,     1] loss: 861.589
[47,     1] loss: 825.778
[48,     1] loss: 882.482
[49,     1] loss: 753.631
[50,     1] loss: 824.111
[51,     1] loss: 767.908
[52,     1] loss: 833.486
[53,     1] loss: 780.585
[54,     1] loss: 817.485
[55,     1] loss: 741.116
[56,     1] loss: 809.958
[57,     1] loss: 792.811
[58,     1] loss: 794.076
[59,     1] loss: 707.946
[60,     1] loss: 777.670
[61,     1] loss: 688.100
[62,     1] loss: 723.999
[63,     1] loss: 705.993
[64,     1] loss: 679.911
[65,     1] loss: 626.106
[66,     1] loss: 639.623
[67,     1] loss: 619.657
[68,     1] loss: 682.169
[69,     1] loss: 1186.115
[70,     1] loss: 1480.683
[71,     1] loss: 990.826
[72,     1] loss: 910.976
[73,     1] loss: 966.877
[74,     1] loss: 1072.690
[75,     1] loss: 1072.178
[76,     1] loss: 1072.370
[77,     1] loss: 1060.214
[78,     1] loss: 1056.392
[79,     1] loss: 1072.961
[80,     1] loss: 1052.376
[81,     1] loss: 1026.340
[82,     1] loss: 988.692
[83,     1] loss: 961.961
[84,     1] loss: 947.865
[85,     1] loss: 927.441
[86,     1] loss: 926.728
[87,     1] loss: 931.743
[88,     1] loss: 916.125
[89,     1] loss: 908.845
[90,     1] loss: 883.705
[91,     1] loss: 874.775
[92,     1] loss: 893.942
[93,     1] loss: 865.026
[94,     1] loss: 855.803
[95,     1] loss: 869.800
[96,     1] loss: 881.287
[97,     1] loss: 870.913
[98,     1] loss: 807.957
[99,     1] loss: 784.860
[100,     1] loss: 792.815
[101,     1] loss: 784.165
[102,     1] loss: 783.496
[103,     1] loss: 765.250
[104,     1] loss: 738.127
[105,     1] loss: 775.854
[106,     1] loss: 813.150
[107,     1] loss: 720.084
[108,     1] loss: 751.924
[109,     1] loss: 797.052
[110,     1] loss: 660.750
[111,     1] loss: 794.273
[112,     1] loss: 875.151
[113,     1] loss: 691.279
[114,     1] loss: 817.326
[115,     1] loss: 699.488
[116,     1] loss: 771.520
[117,     1] loss: 678.720
[118,     1] loss: 744.591
[119,     1] loss: 738.840
[120,     1] loss: 651.843
[121,     1] loss: 798.514
[122,     1] loss: 717.505
[123,     1] loss: 681.167
[124,     1] loss: 788.131
[125,     1] loss: 693.857
[126,     1] loss: 646.229
[127,     1] loss: 695.266
[128,     1] loss: 843.012
[129,     1] loss: 622.658
[130,     1] loss: 900.866
[131,     1] loss: 737.236
[132,     1] loss: 855.066
[133,     1] loss: 665.727
[134,     1] loss: 868.497
[135,     1] loss: 651.483
[136,     1] loss: 750.605
[137,     1] loss: 700.669
[138,     1] loss: 688.556
[139,     1] loss: 722.789
[140,     1] loss: 693.371
[141,     1] loss: 681.428
[142,     1] loss: 631.501
[143,     1] loss: 618.735
[144,     1] loss: 564.288
[145,     1] loss: 617.640
[146,     1] loss: 795.705
[147,     1] loss: 1100.950
[148,     1] loss: 698.730
[149,     1] loss: 883.538
[150,     1] loss: 830.233
[151,     1] loss: 748.423
[152,     1] loss: 763.451
[153,     1] loss: 766.215
[154,     1] loss: 695.479
[155,     1] loss: 715.767
[156,     1] loss: 676.895
[157,     1] loss: 690.546
[158,     1] loss: 654.859
[159,     1] loss: 609.182
[160,     1] loss: 640.490
[161,     1] loss: 602.040
[162,     1] loss: 576.230
[163,     1] loss: 562.865
[164,     1] loss: 519.914
[165,     1] loss: 588.529
[166,     1] loss: 747.163
[167,     1] loss: 621.902
[168,     1] loss: 585.658
[169,     1] loss: 570.909
[170,     1] loss: 579.231
[171,     1] loss: 563.880
[172,     1] loss: 537.355
[173,     1] loss: 486.999
[174,     1] loss: 523.391
[175,     1] loss: 944.311
[176,     1] loss: 2098.021
[177,     1] loss: 1100.338
[178,     1] loss: 1186.453
[179,     1] loss: 1153.177
[180,     1] loss: 1165.159
[181,     1] loss: 1200.283
[182,     1] loss: 1211.526
[183,     1] loss: 1193.562
[184,     1] loss: 1164.243
[185,     1] loss: 1130.530
[186,     1] loss: 1161.721
[187,     1] loss: 1169.080
[188,     1] loss: 1119.883
[189,     1] loss: 1113.841
[190,     1] loss: 1136.010
[191,     1] loss: 1118.937
[192,     1] loss: 1112.151
[193,     1] loss: 1093.802
[194,     1] loss: 1119.073
[195,     1] loss: 1105.828
[196,     1] loss: 1079.219
[197,     1] loss: 1104.542
[198,     1] loss: 1104.245
[199,     1] loss: 1084.202
[200,     1] loss: 1081.815
Finished Training
Total time taken: 29.024391174316406
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.025
[2,     1] loss: 1265.261
[3,     1] loss: 1262.715
[4,     1] loss: 1258.291
[5,     1] loss: 1258.521
[6,     1] loss: 1257.497
[7,     1] loss: 1257.096
[8,     1] loss: 1256.406
[9,     1] loss: 1259.494
[10,     1] loss: 1261.175
[11,     1] loss: 1256.112
[12,     1] loss: 1254.591
[13,     1] loss: 1256.648
[14,     1] loss: 1254.318
[15,     1] loss: 1242.437
[16,     1] loss: 1232.446
[17,     1] loss: 1208.284
[18,     1] loss: 1187.583
[19,     1] loss: 1138.702
[20,     1] loss: 1131.153
[21,     1] loss: 1078.054
[22,     1] loss: 1074.719
[23,     1] loss: 1040.156
[24,     1] loss: 1023.889
[25,     1] loss: 1061.251
[26,     1] loss: 1042.805
[27,     1] loss: 1044.357
[28,     1] loss: 1015.869
[29,     1] loss: 1006.030
[30,     1] loss: 986.076
[31,     1] loss: 976.364
[32,     1] loss: 1004.626
[33,     1] loss: 960.838
[34,     1] loss: 963.366
[35,     1] loss: 954.661
[36,     1] loss: 888.009
[37,     1] loss: 972.398
[38,     1] loss: 971.059
[39,     1] loss: 930.518
[40,     1] loss: 896.745
[41,     1] loss: 891.123
[42,     1] loss: 897.877
[43,     1] loss: 924.442
[44,     1] loss: 902.259
[45,     1] loss: 971.604
[46,     1] loss: 871.439
[47,     1] loss: 960.827
[48,     1] loss: 850.021
[49,     1] loss: 896.509
[50,     1] loss: 905.670
[51,     1] loss: 915.129
[52,     1] loss: 856.935
[53,     1] loss: 803.136
[54,     1] loss: 828.249
[55,     1] loss: 825.427
[56,     1] loss: 782.638
[57,     1] loss: 815.870
[58,     1] loss: 771.781
[59,     1] loss: 844.045
[60,     1] loss: 757.461
[61,     1] loss: 751.523
[62,     1] loss: 790.775
[63,     1] loss: 782.979
[64,     1] loss: 917.860
[65,     1] loss: 809.821
[66,     1] loss: 742.822
[67,     1] loss: 751.114
[68,     1] loss: 763.647
[69,     1] loss: 789.787
[70,     1] loss: 753.764
[71,     1] loss: 748.524
[72,     1] loss: 725.185
[73,     1] loss: 717.294
[74,     1] loss: 669.785
[75,     1] loss: 712.824
[76,     1] loss: 645.472
[77,     1] loss: 643.072
[78,     1] loss: 652.779
[79,     1] loss: 663.432
[80,     1] loss: 983.079
[81,     1] loss: 1079.908
[82,     1] loss: 726.984
[83,     1] loss: 858.380
[84,     1] loss: 940.002
[85,     1] loss: 807.419
[86,     1] loss: 797.680
[87,     1] loss: 823.344
[88,     1] loss: 829.487
[89,     1] loss: 771.150
[90,     1] loss: 783.831
[91,     1] loss: 728.145
[92,     1] loss: 717.888
[93,     1] loss: 741.020
[94,     1] loss: 673.444
[95,     1] loss: 709.288
[96,     1] loss: 633.896
[97,     1] loss: 651.955
[98,     1] loss: 587.857
[99,     1] loss: 643.382
[100,     1] loss: 760.852
[101,     1] loss: 547.696
[102,     1] loss: 741.971
[103,     1] loss: 664.144
[104,     1] loss: 532.936
[105,     1] loss: 604.940
[106,     1] loss: 534.476
[107,     1] loss: 593.026
[108,     1] loss: 629.812
[109,     1] loss: 779.576
[110,     1] loss: 505.402
[111,     1] loss: 807.672
[112,     1] loss: 565.436
[113,     1] loss: 742.826
[114,     1] loss: 492.271
[115,     1] loss: 754.398
[116,     1] loss: 649.078
[117,     1] loss: 576.605
[118,     1] loss: 617.224
[119,     1] loss: 484.965
[120,     1] loss: 571.353
[121,     1] loss: 516.746
[122,     1] loss: 487.588
[123,     1] loss: 436.039
[124,     1] loss: 457.873
[125,     1] loss: 448.189
[126,     1] loss: 635.635
[127,     1] loss: 1504.118
[128,     1] loss: 922.875
[129,     1] loss: 1049.194
[130,     1] loss: 1074.216
[131,     1] loss: 1127.226
[132,     1] loss: 1128.289
[133,     1] loss: 1084.679
[134,     1] loss: 987.231
[135,     1] loss: 920.728
[136,     1] loss: 892.404
[137,     1] loss: 882.766
[138,     1] loss: 890.046
Early stopping applied (best metric=0.3654853403568268)
Finished Training
Total time taken: 20.14373517036438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.219
[2,     1] loss: 1257.003
[3,     1] loss: 1256.957
[4,     1] loss: 1262.584
[5,     1] loss: 1262.149
[6,     1] loss: 1257.546
[7,     1] loss: 1256.036
[8,     1] loss: 1254.016
[9,     1] loss: 1254.732
[10,     1] loss: 1249.086
[11,     1] loss: 1237.702
[12,     1] loss: 1214.366
[13,     1] loss: 1183.067
[14,     1] loss: 1163.451
[15,     1] loss: 1101.269
[16,     1] loss: 1092.311
[17,     1] loss: 1074.108
[18,     1] loss: 1053.170
[19,     1] loss: 1057.029
[20,     1] loss: 1039.333
[21,     1] loss: 992.875
[22,     1] loss: 978.835
[23,     1] loss: 1007.048
[24,     1] loss: 997.796
[25,     1] loss: 994.809
[26,     1] loss: 973.804
[27,     1] loss: 950.846
[28,     1] loss: 938.217
[29,     1] loss: 927.542
[30,     1] loss: 1001.851
[31,     1] loss: 878.602
[32,     1] loss: 998.834
[33,     1] loss: 925.804
[34,     1] loss: 864.533
[35,     1] loss: 930.397
[36,     1] loss: 889.919
[37,     1] loss: 859.714
[38,     1] loss: 897.359
[39,     1] loss: 841.505
[40,     1] loss: 795.822
[41,     1] loss: 903.883
[42,     1] loss: 861.153
[43,     1] loss: 784.389
[44,     1] loss: 935.162
[45,     1] loss: 818.337
[46,     1] loss: 842.661
[47,     1] loss: 798.487
[48,     1] loss: 780.124
[49,     1] loss: 814.557
[50,     1] loss: 771.056
[51,     1] loss: 818.641
[52,     1] loss: 772.150
[53,     1] loss: 734.772
[54,     1] loss: 684.105
[55,     1] loss: 678.800
[56,     1] loss: 721.399
[57,     1] loss: 661.483
[58,     1] loss: 649.630
[59,     1] loss: 591.446
[60,     1] loss: 669.997
[61,     1] loss: 862.988
[62,     1] loss: 1703.205
[63,     1] loss: 815.038
[64,     1] loss: 1004.445
[65,     1] loss: 954.376
[66,     1] loss: 929.210
[67,     1] loss: 973.310
[68,     1] loss: 1002.802
[69,     1] loss: 1009.964
[70,     1] loss: 991.237
[71,     1] loss: 947.350
[72,     1] loss: 874.029
[73,     1] loss: 916.232
[74,     1] loss: 931.139
[75,     1] loss: 896.699
[76,     1] loss: 844.797
[77,     1] loss: 869.852
[78,     1] loss: 841.767
Early stopping applied (best metric=0.40526944398880005)
Finished Training
Total time taken: 11.35756254196167
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.635
[2,     1] loss: 1258.563
[3,     1] loss: 1260.033
[4,     1] loss: 1253.892
[5,     1] loss: 1258.130
[6,     1] loss: 1255.619
[7,     1] loss: 1249.373
[8,     1] loss: 1231.086
[9,     1] loss: 1219.408
[10,     1] loss: 1184.359
[11,     1] loss: 1142.522
[12,     1] loss: 1121.908
[13,     1] loss: 1066.115
[14,     1] loss: 1040.887
[15,     1] loss: 1116.147
[16,     1] loss: 1113.953
[17,     1] loss: 1064.377
[18,     1] loss: 1032.798
[19,     1] loss: 1058.265
[20,     1] loss: 1017.543
[21,     1] loss: 1027.840
[22,     1] loss: 1027.161
[23,     1] loss: 995.470
[24,     1] loss: 1007.593
[25,     1] loss: 944.723
[26,     1] loss: 969.648
[27,     1] loss: 1008.950
[28,     1] loss: 953.365
[29,     1] loss: 961.765
[30,     1] loss: 937.015
[31,     1] loss: 920.470
[32,     1] loss: 927.595
[33,     1] loss: 914.272
[34,     1] loss: 876.601
[35,     1] loss: 924.220
[36,     1] loss: 882.850
[37,     1] loss: 873.868
[38,     1] loss: 853.059
[39,     1] loss: 841.161
[40,     1] loss: 878.757
[41,     1] loss: 849.176
[42,     1] loss: 847.872
[43,     1] loss: 816.120
[44,     1] loss: 759.885
[45,     1] loss: 818.117
[46,     1] loss: 777.865
[47,     1] loss: 795.010
[48,     1] loss: 752.882
[49,     1] loss: 797.970
[50,     1] loss: 746.325
[51,     1] loss: 939.229
[52,     1] loss: 1322.710
[53,     1] loss: 878.439
[54,     1] loss: 930.051
[55,     1] loss: 1036.769
[56,     1] loss: 1010.213
[57,     1] loss: 949.738
[58,     1] loss: 971.827
[59,     1] loss: 938.141
[60,     1] loss: 931.983
[61,     1] loss: 897.496
[62,     1] loss: 889.684
[63,     1] loss: 882.021
[64,     1] loss: 837.812
[65,     1] loss: 842.572
[66,     1] loss: 849.774
[67,     1] loss: 782.772
[68,     1] loss: 788.309
[69,     1] loss: 748.616
[70,     1] loss: 710.273
[71,     1] loss: 777.334
[72,     1] loss: 711.760
[73,     1] loss: 735.155
[74,     1] loss: 766.077
[75,     1] loss: 737.347
[76,     1] loss: 652.436
[77,     1] loss: 620.860
[78,     1] loss: 673.119
[79,     1] loss: 725.264
[80,     1] loss: 728.934
[81,     1] loss: 731.242
[82,     1] loss: 637.378
[83,     1] loss: 698.668
[84,     1] loss: 818.058
[85,     1] loss: 637.557
[86,     1] loss: 885.168
[87,     1] loss: 702.652
[88,     1] loss: 803.340
[89,     1] loss: 636.656
[90,     1] loss: 786.769
[91,     1] loss: 603.581
[92,     1] loss: 651.402
[93,     1] loss: 580.567
[94,     1] loss: 734.654
[95,     1] loss: 566.903
[96,     1] loss: 700.445
[97,     1] loss: 715.107
[98,     1] loss: 568.045
[99,     1] loss: 624.113
[100,     1] loss: 554.370
[101,     1] loss: 614.248
[102,     1] loss: 704.255
[103,     1] loss: 509.790
[104,     1] loss: 788.247
[105,     1] loss: 845.850
[106,     1] loss: 680.097
[107,     1] loss: 722.274
[108,     1] loss: 767.219
[109,     1] loss: 658.350
[110,     1] loss: 695.946
[111,     1] loss: 547.172
[112,     1] loss: 766.695
[113,     1] loss: 560.166
[114,     1] loss: 689.112
[115,     1] loss: 540.242
[116,     1] loss: 550.923
[117,     1] loss: 512.637
[118,     1] loss: 535.362
[119,     1] loss: 588.691
[120,     1] loss: 471.478
[121,     1] loss: 515.439
[122,     1] loss: 552.661
[123,     1] loss: 607.503
[124,     1] loss: 524.303
[125,     1] loss: 442.547
Early stopping applied (best metric=0.34019091725349426)
Finished Training
Total time taken: 18.236642837524414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1268.609
[2,     1] loss: 1261.136
[3,     1] loss: 1262.679
[4,     1] loss: 1259.690
[5,     1] loss: 1263.069
[6,     1] loss: 1259.558
[7,     1] loss: 1258.838
[8,     1] loss: 1257.011
[9,     1] loss: 1258.490
[10,     1] loss: 1253.584
[11,     1] loss: 1244.758
[12,     1] loss: 1230.035
[13,     1] loss: 1213.839
[14,     1] loss: 1181.591
[15,     1] loss: 1164.142
[16,     1] loss: 1105.083
[17,     1] loss: 1124.099
[18,     1] loss: 1079.122
[19,     1] loss: 1047.954
[20,     1] loss: 1077.076
[21,     1] loss: 1024.277
[22,     1] loss: 1027.947
[23,     1] loss: 998.296
[24,     1] loss: 1011.906
[25,     1] loss: 975.517
[26,     1] loss: 1000.903
[27,     1] loss: 957.652
[28,     1] loss: 980.999
[29,     1] loss: 933.508
[30,     1] loss: 932.002
[31,     1] loss: 919.213
[32,     1] loss: 953.031
[33,     1] loss: 945.080
[34,     1] loss: 941.995
[35,     1] loss: 933.379
[36,     1] loss: 868.015
[37,     1] loss: 867.276
[38,     1] loss: 880.845
[39,     1] loss: 894.478
[40,     1] loss: 915.386
[41,     1] loss: 868.173
[42,     1] loss: 842.980
[43,     1] loss: 844.062
[44,     1] loss: 857.240
[45,     1] loss: 780.611
[46,     1] loss: 839.779
[47,     1] loss: 814.356
[48,     1] loss: 801.255
[49,     1] loss: 758.971
[50,     1] loss: 771.674
[51,     1] loss: 762.238
[52,     1] loss: 836.544
[53,     1] loss: 808.811
[54,     1] loss: 725.376
[55,     1] loss: 771.898
[56,     1] loss: 769.899
[57,     1] loss: 722.554
[58,     1] loss: 770.015
[59,     1] loss: 756.924
[60,     1] loss: 694.469
[61,     1] loss: 750.747
[62,     1] loss: 694.238
[63,     1] loss: 662.934
[64,     1] loss: 627.477
[65,     1] loss: 642.800
[66,     1] loss: 613.517
[67,     1] loss: 625.023
[68,     1] loss: 781.004
[69,     1] loss: 1614.631
[70,     1] loss: 729.567
[71,     1] loss: 957.604
[72,     1] loss: 833.279
[73,     1] loss: 840.348
[74,     1] loss: 909.369
[75,     1] loss: 944.024
[76,     1] loss: 904.790
[77,     1] loss: 857.905
[78,     1] loss: 831.354
[79,     1] loss: 850.183
[80,     1] loss: 876.156
[81,     1] loss: 814.711
[82,     1] loss: 837.509
[83,     1] loss: 883.519
[84,     1] loss: 813.205
[85,     1] loss: 797.111
[86,     1] loss: 827.957
[87,     1] loss: 798.483
[88,     1] loss: 761.046
[89,     1] loss: 784.744
[90,     1] loss: 752.248
[91,     1] loss: 709.200
[92,     1] loss: 761.190
[93,     1] loss: 673.531
[94,     1] loss: 670.042
[95,     1] loss: 688.256
[96,     1] loss: 633.124
[97,     1] loss: 590.041
[98,     1] loss: 617.989
[99,     1] loss: 587.471
[100,     1] loss: 679.249
[101,     1] loss: 659.871
[102,     1] loss: 885.999
[103,     1] loss: 1168.089
[104,     1] loss: 671.922
[105,     1] loss: 933.639
[106,     1] loss: 795.779
[107,     1] loss: 770.219
[108,     1] loss: 831.189
[109,     1] loss: 765.598
[110,     1] loss: 755.710
[111,     1] loss: 799.520
[112,     1] loss: 711.399
[113,     1] loss: 806.529
[114,     1] loss: 716.038
[115,     1] loss: 763.032
[116,     1] loss: 697.344
[117,     1] loss: 752.457
[118,     1] loss: 640.921
[119,     1] loss: 729.371
[120,     1] loss: 599.526
[121,     1] loss: 644.755
[122,     1] loss: 627.309
[123,     1] loss: 566.421
[124,     1] loss: 543.716
[125,     1] loss: 546.423
[126,     1] loss: 496.379
[127,     1] loss: 546.397
[128,     1] loss: 774.302
[129,     1] loss: 1513.966
[130,     1] loss: 825.340
[131,     1] loss: 963.052
[132,     1] loss: 877.024
[133,     1] loss: 924.685
[134,     1] loss: 1040.926
[135,     1] loss: 909.422
Early stopping applied (best metric=0.40867576003074646)
Finished Training
Total time taken: 19.766181468963623
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.781
[2,     1] loss: 1260.404
[3,     1] loss: 1259.572
[4,     1] loss: 1255.781
[5,     1] loss: 1253.065
[6,     1] loss: 1252.539
[7,     1] loss: 1241.176
[8,     1] loss: 1223.643
[9,     1] loss: 1204.681
[10,     1] loss: 1180.632
[11,     1] loss: 1142.117
[12,     1] loss: 1136.659
[13,     1] loss: 1114.019
[14,     1] loss: 1106.329
[15,     1] loss: 1075.743
[16,     1] loss: 1084.613
[17,     1] loss: 1027.212
[18,     1] loss: 1051.693
[19,     1] loss: 1041.355
[20,     1] loss: 1022.743
[21,     1] loss: 1025.942
[22,     1] loss: 1032.469
[23,     1] loss: 1009.754
[24,     1] loss: 980.892
[25,     1] loss: 980.462
[26,     1] loss: 988.455
[27,     1] loss: 944.248
[28,     1] loss: 937.613
[29,     1] loss: 944.134
[30,     1] loss: 933.634
[31,     1] loss: 907.724
[32,     1] loss: 948.772
[33,     1] loss: 912.529
[34,     1] loss: 934.437
[35,     1] loss: 886.722
[36,     1] loss: 819.179
[37,     1] loss: 813.638
[38,     1] loss: 863.744
[39,     1] loss: 872.613
[40,     1] loss: 880.874
[41,     1] loss: 998.238
[42,     1] loss: 852.113
[43,     1] loss: 867.648
[44,     1] loss: 819.172
[45,     1] loss: 873.759
[46,     1] loss: 839.923
[47,     1] loss: 791.687
[48,     1] loss: 792.577
[49,     1] loss: 758.628
[50,     1] loss: 728.664
[51,     1] loss: 748.875
[52,     1] loss: 743.397
[53,     1] loss: 788.093
[54,     1] loss: 901.559
[55,     1] loss: 834.816
[56,     1] loss: 734.343
[57,     1] loss: 791.460
[58,     1] loss: 762.049
[59,     1] loss: 763.237
[60,     1] loss: 714.306
[61,     1] loss: 749.016
[62,     1] loss: 693.485
[63,     1] loss: 700.214
[64,     1] loss: 677.405
[65,     1] loss: 727.209
[66,     1] loss: 785.115
[67,     1] loss: 900.126
[68,     1] loss: 650.450
[69,     1] loss: 821.085
[70,     1] loss: 695.456
[71,     1] loss: 731.600
[72,     1] loss: 752.157
[73,     1] loss: 674.236
[74,     1] loss: 811.340
[75,     1] loss: 645.503
[76,     1] loss: 792.867
[77,     1] loss: 627.976
[78,     1] loss: 751.432
[79,     1] loss: 735.425
[80,     1] loss: 662.897
[81,     1] loss: 695.727
[82,     1] loss: 624.037
[83,     1] loss: 645.453
[84,     1] loss: 588.983
[85,     1] loss: 673.341
[86,     1] loss: 575.967
[87,     1] loss: 622.122
[88,     1] loss: 768.569
[89,     1] loss: 845.035
[90,     1] loss: 590.413
[91,     1] loss: 752.369
[92,     1] loss: 618.339
[93,     1] loss: 660.087
[94,     1] loss: 697.540
[95,     1] loss: 605.622
[96,     1] loss: 641.782
[97,     1] loss: 526.780
[98,     1] loss: 617.813
[99,     1] loss: 602.227
[100,     1] loss: 479.045
[101,     1] loss: 579.011
[102,     1] loss: 464.377
[103,     1] loss: 574.583
[104,     1] loss: 778.643
[105,     1] loss: 505.202
[106,     1] loss: 765.089
[107,     1] loss: 667.433
[108,     1] loss: 682.590
[109,     1] loss: 581.143
[110,     1] loss: 631.938
[111,     1] loss: 520.151
[112,     1] loss: 589.191
[113,     1] loss: 506.325
[114,     1] loss: 497.861
[115,     1] loss: 677.978
[116,     1] loss: 453.719
[117,     1] loss: 543.610
[118,     1] loss: 646.534
[119,     1] loss: 457.984
[120,     1] loss: 676.457
[121,     1] loss: 711.859
[122,     1] loss: 557.845
[123,     1] loss: 637.418
[124,     1] loss: 488.825
[125,     1] loss: 659.577
[126,     1] loss: 446.559
[127,     1] loss: 644.208
[128,     1] loss: 861.228
[129,     1] loss: 541.313
[130,     1] loss: 708.362
[131,     1] loss: 511.422
[132,     1] loss: 664.542
[133,     1] loss: 526.694
[134,     1] loss: 632.159
[135,     1] loss: 519.886
[136,     1] loss: 511.647
Early stopping applied (best metric=0.3349066972732544)
Finished Training
Total time taken: 19.781866312026978
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.187
[2,     1] loss: 1260.062
[3,     1] loss: 1255.253
[4,     1] loss: 1257.602
[5,     1] loss: 1255.391
[6,     1] loss: 1257.374
[7,     1] loss: 1259.278
[8,     1] loss: 1251.829
[9,     1] loss: 1250.410
[10,     1] loss: 1236.114
[11,     1] loss: 1218.471
[12,     1] loss: 1173.085
[13,     1] loss: 1147.707
[14,     1] loss: 1115.252
[15,     1] loss: 1076.781
[16,     1] loss: 1072.117
[17,     1] loss: 1023.528
[18,     1] loss: 1010.297
[19,     1] loss: 1045.566
[20,     1] loss: 997.870
[21,     1] loss: 1038.379
[22,     1] loss: 1011.471
[23,     1] loss: 1023.040
[24,     1] loss: 994.150
[25,     1] loss: 967.319
[26,     1] loss: 1003.752
[27,     1] loss: 969.600
[28,     1] loss: 965.065
[29,     1] loss: 932.419
[30,     1] loss: 922.091
[31,     1] loss: 981.688
[32,     1] loss: 916.664
[33,     1] loss: 928.633
[34,     1] loss: 952.094
[35,     1] loss: 899.965
[36,     1] loss: 935.782
[37,     1] loss: 851.676
[38,     1] loss: 866.109
[39,     1] loss: 924.001
[40,     1] loss: 829.135
[41,     1] loss: 874.291
[42,     1] loss: 927.607
[43,     1] loss: 832.983
[44,     1] loss: 882.127
[45,     1] loss: 836.633
[46,     1] loss: 840.765
[47,     1] loss: 812.450
[48,     1] loss: 803.198
[49,     1] loss: 786.282
[50,     1] loss: 770.378
[51,     1] loss: 744.817
[52,     1] loss: 777.034
[53,     1] loss: 858.690
[54,     1] loss: 749.685
[55,     1] loss: 793.445
[56,     1] loss: 740.559
[57,     1] loss: 786.336
[58,     1] loss: 763.771
[59,     1] loss: 718.569
[60,     1] loss: 780.065
[61,     1] loss: 699.991
[62,     1] loss: 712.230
[63,     1] loss: 701.175
[64,     1] loss: 711.727
[65,     1] loss: 693.087
[66,     1] loss: 644.161
[67,     1] loss: 562.836
[68,     1] loss: 623.687
[69,     1] loss: 541.476
[70,     1] loss: 558.580
[71,     1] loss: 837.477
[72,     1] loss: 2261.690
[73,     1] loss: 1256.707
[74,     1] loss: 842.151
[75,     1] loss: 1066.091
[76,     1] loss: 1106.189
[77,     1] loss: 1083.241
[78,     1] loss: 1104.519
[79,     1] loss: 1145.487
[80,     1] loss: 1140.872
[81,     1] loss: 1116.337
[82,     1] loss: 1077.723
[83,     1] loss: 1059.339
[84,     1] loss: 1094.902
[85,     1] loss: 1092.737
[86,     1] loss: 1061.407
[87,     1] loss: 1050.794
[88,     1] loss: 1074.222
[89,     1] loss: 1041.464
[90,     1] loss: 1028.418
[91,     1] loss: 1016.171
[92,     1] loss: 1019.760
[93,     1] loss: 1007.511
[94,     1] loss: 1026.301
[95,     1] loss: 1004.790
[96,     1] loss: 985.360
[97,     1] loss: 971.452
[98,     1] loss: 985.277
Early stopping applied (best metric=0.36676761507987976)
Finished Training
Total time taken: 14.267120599746704
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.757
[2,     1] loss: 1261.111
[3,     1] loss: 1259.294
[4,     1] loss: 1256.870
[5,     1] loss: 1257.774
[6,     1] loss: 1261.407
[7,     1] loss: 1257.359
[8,     1] loss: 1257.721
[9,     1] loss: 1259.948
[10,     1] loss: 1255.079
[11,     1] loss: 1253.331
[12,     1] loss: 1252.803
[13,     1] loss: 1247.328
[14,     1] loss: 1238.321
[15,     1] loss: 1229.763
[16,     1] loss: 1208.415
[17,     1] loss: 1178.476
[18,     1] loss: 1155.800
[19,     1] loss: 1138.638
[20,     1] loss: 1116.977
[21,     1] loss: 1077.461
[22,     1] loss: 1069.688
[23,     1] loss: 1047.034
[24,     1] loss: 1084.726
[25,     1] loss: 1040.614
[26,     1] loss: 1066.470
[27,     1] loss: 1044.148
[28,     1] loss: 995.497
[29,     1] loss: 1016.870
[30,     1] loss: 1019.172
[31,     1] loss: 956.755
[32,     1] loss: 986.402
[33,     1] loss: 971.829
[34,     1] loss: 970.777
[35,     1] loss: 906.991
[36,     1] loss: 902.887
[37,     1] loss: 894.243
[38,     1] loss: 929.267
[39,     1] loss: 903.398
[40,     1] loss: 873.607
[41,     1] loss: 970.050
[42,     1] loss: 909.803
[43,     1] loss: 880.940
[44,     1] loss: 884.447
[45,     1] loss: 807.029
[46,     1] loss: 842.683
[47,     1] loss: 842.733
[48,     1] loss: 827.894
[49,     1] loss: 900.547
[50,     1] loss: 1016.773
[51,     1] loss: 783.802
[52,     1] loss: 1004.279
[53,     1] loss: 836.914
[54,     1] loss: 889.176
[55,     1] loss: 914.965
[56,     1] loss: 856.784
[57,     1] loss: 862.127
[58,     1] loss: 904.694
[59,     1] loss: 818.149
[60,     1] loss: 820.551
[61,     1] loss: 832.452
[62,     1] loss: 798.514
[63,     1] loss: 848.330
[64,     1] loss: 786.918
[65,     1] loss: 771.971
[66,     1] loss: 731.672
[67,     1] loss: 736.882
[68,     1] loss: 722.289
[69,     1] loss: 709.418
[70,     1] loss: 776.717
[71,     1] loss: 779.982
[72,     1] loss: 708.380
[73,     1] loss: 756.572
[74,     1] loss: 750.825
[75,     1] loss: 657.795
[76,     1] loss: 745.104
[77,     1] loss: 661.418
[78,     1] loss: 645.211
[79,     1] loss: 687.755
[80,     1] loss: 614.875
[81,     1] loss: 582.501
[82,     1] loss: 633.445
[83,     1] loss: 867.941
[84,     1] loss: 806.328
[85,     1] loss: 609.934
[86,     1] loss: 667.861
[87,     1] loss: 627.516
[88,     1] loss: 630.354
[89,     1] loss: 609.906
[90,     1] loss: 563.560
[91,     1] loss: 554.061
[92,     1] loss: 514.355
[93,     1] loss: 513.202
[94,     1] loss: 501.332
[95,     1] loss: 686.707
[96,     1] loss: 1763.089
[97,     1] loss: 563.751
[98,     1] loss: 1170.996
[99,     1] loss: 784.948
[100,     1] loss: 892.014
[101,     1] loss: 987.927
[102,     1] loss: 983.516
[103,     1] loss: 983.435
[104,     1] loss: 872.106
[105,     1] loss: 868.394
[106,     1] loss: 889.563
[107,     1] loss: 878.254
[108,     1] loss: 805.710
[109,     1] loss: 868.943
[110,     1] loss: 857.352
[111,     1] loss: 787.622
[112,     1] loss: 823.200
[113,     1] loss: 845.505
[114,     1] loss: 799.735
[115,     1] loss: 777.781
[116,     1] loss: 747.340
[117,     1] loss: 688.125
[118,     1] loss: 745.045
[119,     1] loss: 681.618
[120,     1] loss: 746.409
[121,     1] loss: 602.543
[122,     1] loss: 686.546
[123,     1] loss: 753.172
[124,     1] loss: 537.767
[125,     1] loss: 686.542
[126,     1] loss: 703.997
[127,     1] loss: 647.837
[128,     1] loss: 854.360
[129,     1] loss: 698.706
[130,     1] loss: 759.252
[131,     1] loss: 587.761
[132,     1] loss: 714.165
[133,     1] loss: 601.015
[134,     1] loss: 549.267
[135,     1] loss: 600.000
[136,     1] loss: 500.323
[137,     1] loss: 543.459
[138,     1] loss: 509.126
[139,     1] loss: 542.749
[140,     1] loss: 463.894
[141,     1] loss: 457.415
[142,     1] loss: 470.832
[143,     1] loss: 445.474
[144,     1] loss: 466.869
[145,     1] loss: 475.756
[146,     1] loss: 1128.997
Early stopping applied (best metric=0.35512909293174744)
Finished Training
Total time taken: 21.394710540771484
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.477
[2,     1] loss: 1263.117
[3,     1] loss: 1256.486
[4,     1] loss: 1261.343
[5,     1] loss: 1260.948
[6,     1] loss: 1259.151
[7,     1] loss: 1257.962
[8,     1] loss: 1257.967
[9,     1] loss: 1256.430
[10,     1] loss: 1253.088
[11,     1] loss: 1249.228
[12,     1] loss: 1247.573
[13,     1] loss: 1226.922
[14,     1] loss: 1202.618
[15,     1] loss: 1185.420
[16,     1] loss: 1144.877
[17,     1] loss: 1117.917
[18,     1] loss: 1111.239
[19,     1] loss: 1066.591
[20,     1] loss: 1076.818
[21,     1] loss: 1100.787
[22,     1] loss: 1087.631
[23,     1] loss: 1012.992
[24,     1] loss: 1050.644
[25,     1] loss: 1043.686
[26,     1] loss: 1052.957
[27,     1] loss: 1017.248
[28,     1] loss: 1004.177
[29,     1] loss: 1044.305
[30,     1] loss: 971.508
[31,     1] loss: 996.769
[32,     1] loss: 984.756
[33,     1] loss: 928.092
[34,     1] loss: 942.278
[35,     1] loss: 887.219
[36,     1] loss: 855.562
[37,     1] loss: 935.942
[38,     1] loss: 896.294
[39,     1] loss: 873.026
[40,     1] loss: 878.400
[41,     1] loss: 843.265
[42,     1] loss: 853.844
[43,     1] loss: 800.666
[44,     1] loss: 824.941
[45,     1] loss: 844.393
[46,     1] loss: 961.680
[47,     1] loss: 845.520
[48,     1] loss: 799.022
[49,     1] loss: 822.911
[50,     1] loss: 780.989
[51,     1] loss: 820.241
[52,     1] loss: 756.625
[53,     1] loss: 757.519
[54,     1] loss: 707.350
[55,     1] loss: 759.519
[56,     1] loss: 793.214
[57,     1] loss: 920.904
[58,     1] loss: 696.977
[59,     1] loss: 804.051
[60,     1] loss: 667.250
[61,     1] loss: 755.418
[62,     1] loss: 700.771
[63,     1] loss: 759.339
[64,     1] loss: 692.349
[65,     1] loss: 623.555
[66,     1] loss: 760.029
[67,     1] loss: 600.220
[68,     1] loss: 635.601
[69,     1] loss: 665.277
[70,     1] loss: 618.885
[71,     1] loss: 620.714
[72,     1] loss: 631.912
[73,     1] loss: 853.335
[74,     1] loss: 683.389
[75,     1] loss: 600.589
[76,     1] loss: 690.467
[77,     1] loss: 634.966
[78,     1] loss: 658.539
[79,     1] loss: 554.465
[80,     1] loss: 608.497
[81,     1] loss: 579.384
[82,     1] loss: 529.695
[83,     1] loss: 534.850
[84,     1] loss: 497.221
[85,     1] loss: 489.563
[86,     1] loss: 638.967
[87,     1] loss: 1114.039
[88,     1] loss: 883.861
Early stopping applied (best metric=0.3843238949775696)
Finished Training
Total time taken: 12.890471935272217
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1267.680
[2,     1] loss: 1260.795
[3,     1] loss: 1264.056
[4,     1] loss: 1262.649
[5,     1] loss: 1260.149
[6,     1] loss: 1260.135
[7,     1] loss: 1261.419
[8,     1] loss: 1258.044
[9,     1] loss: 1257.520
[10,     1] loss: 1251.528
[11,     1] loss: 1243.761
[12,     1] loss: 1228.651
[13,     1] loss: 1192.409
[14,     1] loss: 1173.958
[15,     1] loss: 1134.876
[16,     1] loss: 1067.469
[17,     1] loss: 1090.767
[18,     1] loss: 1093.600
[19,     1] loss: 1014.131
[20,     1] loss: 1053.094
[21,     1] loss: 1019.066
[22,     1] loss: 979.655
[23,     1] loss: 1001.542
[24,     1] loss: 984.435
[25,     1] loss: 962.811
[26,     1] loss: 1004.393
[27,     1] loss: 943.501
[28,     1] loss: 997.910
[29,     1] loss: 963.371
[30,     1] loss: 950.496
[31,     1] loss: 934.709
[32,     1] loss: 946.637
[33,     1] loss: 903.980
[34,     1] loss: 953.743
[35,     1] loss: 907.116
[36,     1] loss: 925.856
[37,     1] loss: 898.409
[38,     1] loss: 891.250
[39,     1] loss: 917.563
[40,     1] loss: 864.012
[41,     1] loss: 879.636
[42,     1] loss: 853.627
[43,     1] loss: 898.645
[44,     1] loss: 853.726
[45,     1] loss: 855.739
[46,     1] loss: 806.274
[47,     1] loss: 888.783
[48,     1] loss: 870.371
[49,     1] loss: 796.243
[50,     1] loss: 782.131
[51,     1] loss: 801.190
[52,     1] loss: 807.814
[53,     1] loss: 726.263
[54,     1] loss: 804.127
[55,     1] loss: 743.198
[56,     1] loss: 735.502
[57,     1] loss: 717.043
[58,     1] loss: 866.356
[59,     1] loss: 1143.337
[60,     1] loss: 715.813
[61,     1] loss: 910.772
[62,     1] loss: 826.471
[63,     1] loss: 822.180
[64,     1] loss: 915.735
[65,     1] loss: 835.823
[66,     1] loss: 834.829
[67,     1] loss: 802.778
[68,     1] loss: 810.077
[69,     1] loss: 773.236
[70,     1] loss: 785.573
[71,     1] loss: 737.147
[72,     1] loss: 718.949
[73,     1] loss: 708.945
[74,     1] loss: 691.683
[75,     1] loss: 693.507
[76,     1] loss: 608.765
[77,     1] loss: 640.548
[78,     1] loss: 659.198
[79,     1] loss: 581.317
[80,     1] loss: 628.468
[81,     1] loss: 604.338
[82,     1] loss: 727.377
[83,     1] loss: 1097.125
[84,     1] loss: 1011.258
[85,     1] loss: 886.332
[86,     1] loss: 827.323
[87,     1] loss: 897.303
[88,     1] loss: 897.624
[89,     1] loss: 869.093
[90,     1] loss: 819.163
[91,     1] loss: 862.312
[92,     1] loss: 901.034
[93,     1] loss: 781.432
[94,     1] loss: 807.472
[95,     1] loss: 858.006
[96,     1] loss: 773.117
[97,     1] loss: 823.836
[98,     1] loss: 788.026
[99,     1] loss: 744.655
[100,     1] loss: 710.323
[101,     1] loss: 742.771
[102,     1] loss: 685.726
[103,     1] loss: 677.274
[104,     1] loss: 647.402
[105,     1] loss: 675.236
[106,     1] loss: 723.908
[107,     1] loss: 603.917
[108,     1] loss: 706.252
[109,     1] loss: 646.802
[110,     1] loss: 627.358
[111,     1] loss: 720.238
[112,     1] loss: 578.066
[113,     1] loss: 596.756
[114,     1] loss: 679.261
[115,     1] loss: 545.107
[116,     1] loss: 533.307
[117,     1] loss: 763.619
[118,     1] loss: 1026.415
[119,     1] loss: 602.852
[120,     1] loss: 734.579
[121,     1] loss: 645.353
[122,     1] loss: 771.981
[123,     1] loss: 610.124
[124,     1] loss: 698.069
[125,     1] loss: 540.619
[126,     1] loss: 720.898
[127,     1] loss: 609.437
[128,     1] loss: 508.856
[129,     1] loss: 592.132
[130,     1] loss: 530.151
[131,     1] loss: 547.716
[132,     1] loss: 582.906
[133,     1] loss: 566.893
[134,     1] loss: 500.452
[135,     1] loss: 516.549
[136,     1] loss: 485.870
[137,     1] loss: 549.703
[138,     1] loss: 606.349
[139,     1] loss: 794.134
[140,     1] loss: 627.211
[141,     1] loss: 490.377
[142,     1] loss: 601.588
[143,     1] loss: 470.594
[144,     1] loss: 607.383
[145,     1] loss: 640.856
[146,     1] loss: 449.774
[147,     1] loss: 857.123
[148,     1] loss: 1114.433
[149,     1] loss: 750.713
[150,     1] loss: 721.990
[151,     1] loss: 927.980
[152,     1] loss: 767.841
[153,     1] loss: 728.225
[154,     1] loss: 754.934
[155,     1] loss: 637.846
[156,     1] loss: 768.045
[157,     1] loss: 615.656
[158,     1] loss: 640.937
[159,     1] loss: 598.653
[160,     1] loss: 609.337
[161,     1] loss: 592.827
[162,     1] loss: 582.099
Early stopping applied (best metric=0.37335851788520813)
Finished Training
Total time taken: 23.718334913253784
{'Hydroxylation-K Validation Accuracy': 0.7377068557919622, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.756140350877193, 'Hydroxylation-K Validation Precision': 0.41813766086144605, 'Hydroxylation-K AUC ROC': 0.8002729044834308, 'Hydroxylation-K AUC PR': 0.6380960398331098, 'Hydroxylation-K MCC': 0.3666048304609905, 'Hydroxylation-K F1': 0.5072137793950703, 'Validation Loss (Hydroxylation-K)': 0.45377312699953715, 'Hydroxylation-P Validation Accuracy': 0.7896126423362605, 'Hydroxylation-P Validation Sensitivity': 0.798994708994709, 'Hydroxylation-P Validation Specificity': 0.7875480073819143, 'Hydroxylation-P Validation Precision': 0.45216229657994556, 'Hydroxylation-P AUC ROC': 0.850648720159419, 'Hydroxylation-P AUC PR': 0.5705618495393026, 'Hydroxylation-P MCC': 0.48434703977695825, 'Hydroxylation-P F1': 0.5751992764259605, 'Validation Loss (Hydroxylation-P)': 0.36541344920794167, 'Validation Loss (total)': 0.8191865801811218, 'TimeToTrain': 18.429417737325032}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005723264378393713,
 'learning_rate_Hydroxylation-K': 0.007941158263931871,
 'learning_rate_Hydroxylation-P': 0.0003535723506585082,
 'log_base': 1.1334358151825834,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1195408567,
 'sample_weights': [1.666570668223645, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.44847226389577,
 'weight_decay_Hydroxylation-K': 7.764620599312961,
 'weight_decay_Hydroxylation-P': 8.14481168528239}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4297.168
[2,     1] loss: 4330.935
[3,     1] loss: 4320.440
[4,     1] loss: 4317.769
[5,     1] loss: 4367.102
[6,     1] loss: 4299.648
[7,     1] loss: 4311.474
[8,     1] loss: 4307.517
[9,     1] loss: 4345.402
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011372143401701196,
 'learning_rate_Hydroxylation-K': 0.002153686697182446,
 'learning_rate_Hydroxylation-P': 0.001716901866710595,
 'log_base': 1.6403645587265285,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3414342241,
 'sample_weights': [13.328508137810713, 1.6661277006833157],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.599414736460236,
 'weight_decay_Hydroxylation-K': 8.980445970357225,
 'weight_decay_Hydroxylation-P': 2.115729655318075}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1629.431
[2,     1] loss: 1619.722
[3,     1] loss: 1621.649
[4,     1] loss: 1619.895
[5,     1] loss: 1616.166
[6,     1] loss: 1617.974
[7,     1] loss: 1616.897
[8,     1] loss: 1618.068
[9,     1] loss: 1613.456
[10,     1] loss: 1620.968
[11,     1] loss: 1617.355
[12,     1] loss: 1607.234
[13,     1] loss: 1610.721
[14,     1] loss: 1612.901
[15,     1] loss: 1604.853
[16,     1] loss: 1601.395
[17,     1] loss: 1589.521
[18,     1] loss: 1584.253
[19,     1] loss: 1578.331
[20,     1] loss: 1560.509
[21,     1] loss: 1540.063
[22,     1] loss: 1528.847
[23,     1] loss: 1518.701
[24,     1] loss: 1521.211
[25,     1] loss: 1463.633
[26,     1] loss: 1437.574
[27,     1] loss: 1419.030
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004362587966735189,
 'learning_rate_Hydroxylation-K': 0.009670036041385607,
 'learning_rate_Hydroxylation-P': 0.0010785636080263995,
 'log_base': 1.4665817806060528,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3421559085,
 'sample_weights': [3.3731677375819573, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.52954154044113,
 'weight_decay_Hydroxylation-K': 9.93612715032419,
 'weight_decay_Hydroxylation-P': 5.102337569448757}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1839.473
[2,     1] loss: 1830.008
[3,     1] loss: 1827.812
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030093902542648416,
 'learning_rate_Hydroxylation-K': 0.008788670031309204,
 'learning_rate_Hydroxylation-P': 8.33014462334188e-05,
 'log_base': 1.4233209528884516,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2564059648,
 'sample_weights': [4.359606402524465, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.213444989848517,
 'weight_decay_Hydroxylation-K': 9.383363568886523,
 'weight_decay_Hydroxylation-P': 9.228496157265369}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1914.101
[2,     1] loss: 1908.454
[3,     1] loss: 1907.767
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006108277470340662,
 'learning_rate_Hydroxylation-K': 0.0018495246223819203,
 'learning_rate_Hydroxylation-P': 0.009664443013400238,
 'log_base': 2.73069711757244,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3125924468,
 'sample_weights': [4.729396624203798, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.19050344025235,
 'weight_decay_Hydroxylation-K': 8.53172141171744,
 'weight_decay_Hydroxylation-P': 3.8940938297645644}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.171
[2,     1] loss: 1260.505
[3,     1] loss: 1254.515
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009489372589991801,
 'learning_rate_Hydroxylation-K': 0.006201297223574071,
 'learning_rate_Hydroxylation-P': 0.007457727179642611,
 'log_base': 1.030903882050525,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 85622874,
 'sample_weights': [1.6618701199203103, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0990693896616452,
 'weight_decay_Hydroxylation-K': 5.122668462013078,
 'weight_decay_Hydroxylation-P': 6.680298124506569}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17792.770
Exploding loss, terminate run (best metric=0.5347650051116943)
Finished Training
Total time taken: 0.22000718116760254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18001.295
Exploding loss, terminate run (best metric=0.5301361680030823)
Finished Training
Total time taken: 0.20800042152404785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17807.457
Exploding loss, terminate run (best metric=0.5580465793609619)
Finished Training
Total time taken: 0.21700310707092285
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17881.619
Exploding loss, terminate run (best metric=0.5260152220726013)
Finished Training
Total time taken: 0.20099806785583496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17774.107
Exploding loss, terminate run (best metric=0.5375206470489502)
Finished Training
Total time taken: 0.23000192642211914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17809.068
Exploding loss, terminate run (best metric=0.5324812531471252)
Finished Training
Total time taken: 0.2030034065246582
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17846.203
Exploding loss, terminate run (best metric=0.5352538228034973)
Finished Training
Total time taken: 0.2200007438659668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17897.250
Exploding loss, terminate run (best metric=0.5838402509689331)
Finished Training
Total time taken: 0.20888495445251465
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17775.951
Exploding loss, terminate run (best metric=0.5275994539260864)
Finished Training
Total time taken: 0.22099900245666504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17889.848
Exploding loss, terminate run (best metric=0.5436224341392517)
Finished Training
Total time taken: 0.20999670028686523
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17825.102
Exploding loss, terminate run (best metric=0.5309648513793945)
Finished Training
Total time taken: 0.21899867057800293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17919.035
Exploding loss, terminate run (best metric=0.5395508408546448)
Finished Training
Total time taken: 0.21799969673156738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17934.945
Exploding loss, terminate run (best metric=0.5431904792785645)
Finished Training
Total time taken: 0.22600173950195312
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17869.617
Exploding loss, terminate run (best metric=0.5408108830451965)
Finished Training
Total time taken: 0.20451688766479492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17814.023
Exploding loss, terminate run (best metric=0.5280349254608154)
Finished Training
Total time taken: 0.2270030975341797
{'Hydroxylation-K Validation Accuracy': 0.5194444444444444, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5567056530214425, 'Hydroxylation-K AUC PR': 0.2736859542325915, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.15623973727422005, 'Validation Loss (Hydroxylation-K)': 0.5659327189127604, 'Hydroxylation-P Validation Accuracy': 0.5223729421518366, 'Hydroxylation-P Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-P Validation Specificity': 0.5333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5738740284715146, 'Hydroxylation-P AUC PR': 0.2637334215424519, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.14087462128489264, 'Validation Loss (Hydroxylation-P)': 0.53945552110672, 'Validation Loss (total)': 1.1053882360458374, 'TimeToTrain': 0.21556104024251302}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030395295681911504,
 'learning_rate_Hydroxylation-K': 0.003994803870535895,
 'learning_rate_Hydroxylation-P': 0.0036299207794506013,
 'log_base': 1.0608261780216028,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4189588959,
 'sample_weights': [54.89167533578963, 6.8472025165899595],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.075995277460299,
 'weight_decay_Hydroxylation-K': 9.872473637417752,
 'weight_decay_Hydroxylation-P': 1.284622774772902}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9193.244
[2,     1] loss: 9172.444
[3,     1] loss: 9161.072
[4,     1] loss: 9162.239
[5,     1] loss: 9191.042
[6,     1] loss: 9182.073
[7,     1] loss: 9150.107
[8,     1] loss: 9146.986
[9,     1] loss: 9151.858
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00048012804877479954,
 'learning_rate_Hydroxylation-K': 0.008758996966216145,
 'learning_rate_Hydroxylation-P': 0.003691129267548733,
 'log_base': 1.40754170145157,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 898958173,
 'sample_weights': [28.272636585482154, 3.5342157201219813],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.151762315095752,
 'weight_decay_Hydroxylation-K': 8.883950495037862,
 'weight_decay_Hydroxylation-P': 3.6883207205715394}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1944.948
[2,     1] loss: 1941.317
[3,     1] loss: 1948.648
[4,     1] loss: 1925.524
[5,     1] loss: 1935.536
[6,     1] loss: 1941.407
[7,     1] loss: 1936.760
[8,     1] loss: 1938.668
[9,     1] loss: 1945.566
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010158760623345357,
 'learning_rate_Hydroxylation-K': 0.005306280250372199,
 'learning_rate_Hydroxylation-P': 0.0021375953570624464,
 'log_base': 1.2846256175005721,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3654568098,
 'sample_weights': [4.883630213706236, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.08251045243552,
 'weight_decay_Hydroxylation-K': 6.049861449192783,
 'weight_decay_Hydroxylation-P': 3.6208791687353497}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2326.877
[2,     1] loss: 2311.569
[3,     1] loss: 2314.185
[4,     1] loss: 2305.153
[5,     1] loss: 2319.509
[6,     1] loss: 2315.730
[7,     1] loss: 2313.663
[8,     1] loss: 2313.468
[9,     1] loss: 2324.405
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007156110257651198,
 'learning_rate_Hydroxylation-K': 0.0032933953588005537,
 'learning_rate_Hydroxylation-P': 0.0007930903115825668,
 'log_base': 1.1295842188528742,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2623983271,
 'sample_weights': [6.665313049400326, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.74802882217519,
 'weight_decay_Hydroxylation-K': 7.732915989791362,
 'weight_decay_Hydroxylation-P': 9.474976436543784}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4439.825
[2,     1] loss: 4440.083
[3,     1] loss: 4427.191
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004909855593219499,
 'learning_rate_Hydroxylation-K': 0.007054185625847499,
 'learning_rate_Hydroxylation-P': 0.0013109199992788884,
 'log_base': 1.043576924308241,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1571029736,
 'sample_weights': [13.700848531142867, 1.7126720428557265],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.768720249495614,
 'weight_decay_Hydroxylation-K': 7.145587181686857,
 'weight_decay_Hydroxylation-P': 3.0457537710447635}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12696.496
[2,     1] loss: 12692.807
[3,     1] loss: 12716.558
[4,     1] loss: 12684.346
[5,     1] loss: 12715.551
[6,     1] loss: 12650.420
[7,     1] loss: 12669.727
[8,     1] loss: 12643.314
[9,     1] loss: 12597.979
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0055725256203881576,
 'learning_rate_Hydroxylation-K': 0.004003199320551896,
 'learning_rate_Hydroxylation-P': 0.005125501374802163,
 'log_base': 1.688542403404448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3397296769,
 'sample_weights': [39.139044188355214, 4.892568997688154],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0293591464366592,
 'weight_decay_Hydroxylation-K': 8.205814948805536,
 'weight_decay_Hydroxylation-P': 9.945933419449744}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1579.202
[2,     1] loss: 1588.503
[3,     1] loss: 1576.933
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002159241707170047,
 'learning_rate_Hydroxylation-K': 0.0076014698149708595,
 'learning_rate_Hydroxylation-P': 0.0030719175724065917,
 'log_base': 1.7714024634218655,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3330329755,
 'sample_weights': [3.186777129304989, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.163018695654413,
 'weight_decay_Hydroxylation-K': 9.296048587717866,
 'weight_decay_Hydroxylation-P': 0.36729644004514683}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1524.752
[2,     1] loss: 1522.496
[3,     1] loss: 1524.497
[4,     1] loss: 1520.303
[5,     1] loss: 1520.666
[6,     1] loss: 1518.914
[7,     1] loss: 1517.801
[8,     1] loss: 1519.850
[9,     1] loss: 1517.226
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006245479662518875,
 'learning_rate_Hydroxylation-K': 0.009849693237139447,
 'learning_rate_Hydroxylation-P': 0.0030421330792679497,
 'log_base': 1.662302379684525,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1198670850,
 'sample_weights': [2.9197728447097053, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.29098836509929,
 'weight_decay_Hydroxylation-K': 1.2931462620701561,
 'weight_decay_Hydroxylation-P': 7.4780585836927385}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1603.707
[2,     1] loss: 1608.723
[3,     1] loss: 1601.878
[4,     1] loss: 1599.836
[5,     1] loss: 1603.454
[6,     1] loss: 1604.473
[7,     1] loss: 1603.705
[8,     1] loss: 1602.682
[9,     1] loss: 1597.319
[10,     1] loss: 1599.448
[11,     1] loss: 1597.766
[12,     1] loss: 1594.058
[13,     1] loss: 1585.302
[14,     1] loss: 1573.616
[15,     1] loss: 1564.535
[16,     1] loss: 1535.809
[17,     1] loss: 1526.171
[18,     1] loss: 1452.290
[19,     1] loss: 1445.665
[20,     1] loss: 1395.036
[21,     1] loss: 1347.096
[22,     1] loss: 1387.623
[23,     1] loss: 1323.770
[24,     1] loss: 1318.205
[25,     1] loss: 1310.197
[26,     1] loss: 1275.220
[27,     1] loss: 1329.619
[28,     1] loss: 1284.202
[29,     1] loss: 1266.586
[30,     1] loss: 1254.652
[31,     1] loss: 1267.503
[32,     1] loss: 1236.684
[33,     1] loss: 1319.424
[34,     1] loss: 1186.269
[35,     1] loss: 1151.918
[36,     1] loss: 1253.789
[37,     1] loss: 1125.573
[38,     1] loss: 1148.925
[39,     1] loss: 1108.412
[40,     1] loss: 1112.757
[41,     1] loss: 1084.576
[42,     1] loss: 1080.555
[43,     1] loss: 1025.072
[44,     1] loss: 1180.049
[45,     1] loss: 1311.764
[46,     1] loss: 1004.698
[47,     1] loss: 1260.018
[48,     1] loss: 1115.568
[49,     1] loss: 1118.236
[50,     1] loss: 1136.680
[51,     1] loss: 1019.104
[52,     1] loss: 1226.385
[53,     1] loss: 987.901
[54,     1] loss: 1120.205
[55,     1] loss: 1064.742
[56,     1] loss: 1034.582
[57,     1] loss: 974.643
[58,     1] loss: 1053.231
[59,     1] loss: 1011.654
[60,     1] loss: 993.757
[61,     1] loss: 898.849
[62,     1] loss: 886.969
[63,     1] loss: 833.858
[64,     1] loss: 887.202
[65,     1] loss: 985.310
[66,     1] loss: 856.465
[67,     1] loss: 887.739
[68,     1] loss: 932.125
[69,     1] loss: 1094.002
[70,     1] loss: 974.936
[71,     1] loss: 909.915
[72,     1] loss: 889.925
[73,     1] loss: 999.147
[74,     1] loss: 1037.768
[75,     1] loss: 905.889
[76,     1] loss: 993.012
Early stopping applied (best metric=0.4020270109176636)
Finished Training
Total time taken: 11.77890133857727
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1613.515
[2,     1] loss: 1602.195
[3,     1] loss: 1600.864
[4,     1] loss: 1598.939
[5,     1] loss: 1604.358
[6,     1] loss: 1600.341
[7,     1] loss: 1600.625
[8,     1] loss: 1598.567
[9,     1] loss: 1592.296
[10,     1] loss: 1594.322
[11,     1] loss: 1595.052
[12,     1] loss: 1585.990
[13,     1] loss: 1582.294
[14,     1] loss: 1557.876
[15,     1] loss: 1531.665
[16,     1] loss: 1483.967
[17,     1] loss: 1453.619
[18,     1] loss: 1420.163
[19,     1] loss: 1427.738
[20,     1] loss: 1363.532
[21,     1] loss: 1383.148
[22,     1] loss: 1316.250
[23,     1] loss: 1340.417
[24,     1] loss: 1392.910
[25,     1] loss: 1345.704
[26,     1] loss: 1352.856
[27,     1] loss: 1292.748
[28,     1] loss: 1327.287
[29,     1] loss: 1232.970
[30,     1] loss: 1291.028
[31,     1] loss: 1239.635
[32,     1] loss: 1181.251
[33,     1] loss: 1284.910
[34,     1] loss: 1252.766
[35,     1] loss: 1202.696
[36,     1] loss: 1206.478
[37,     1] loss: 1243.958
[38,     1] loss: 1180.050
[39,     1] loss: 1106.807
[40,     1] loss: 1134.218
[41,     1] loss: 1146.024
[42,     1] loss: 1165.932
[43,     1] loss: 1041.554
[44,     1] loss: 1072.076
[45,     1] loss: 1049.105
[46,     1] loss: 1056.185
[47,     1] loss: 1176.043
[48,     1] loss: 1063.397
[49,     1] loss: 1030.476
[50,     1] loss: 1042.679
[51,     1] loss: 965.448
[52,     1] loss: 1096.308
[53,     1] loss: 944.823
[54,     1] loss: 1044.294
[55,     1] loss: 1126.507
[56,     1] loss: 900.578
[57,     1] loss: 1047.879
[58,     1] loss: 925.483
[59,     1] loss: 1065.930
[60,     1] loss: 814.078
[61,     1] loss: 941.728
[62,     1] loss: 812.584
[63,     1] loss: 892.408
[64,     1] loss: 1127.472
[65,     1] loss: 886.407
[66,     1] loss: 925.752
[67,     1] loss: 800.623
[68,     1] loss: 1024.761
[69,     1] loss: 793.143
[70,     1] loss: 929.839
[71,     1] loss: 896.989
[72,     1] loss: 943.706
[73,     1] loss: 942.159
[74,     1] loss: 749.639
[75,     1] loss: 1063.301
[76,     1] loss: 770.142
Early stopping applied (best metric=0.4033711552619934)
Finished Training
Total time taken: 11.09773850440979
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1601.011
[2,     1] loss: 1604.064
[3,     1] loss: 1601.271
[4,     1] loss: 1602.638
[5,     1] loss: 1596.070
[6,     1] loss: 1595.737
[7,     1] loss: 1592.974
[8,     1] loss: 1595.841
[9,     1] loss: 1589.499
[10,     1] loss: 1580.876
[11,     1] loss: 1570.466
[12,     1] loss: 1540.653
[13,     1] loss: 1504.016
[14,     1] loss: 1468.595
[15,     1] loss: 1477.198
[16,     1] loss: 1422.471
[17,     1] loss: 1402.535
[18,     1] loss: 1450.899
[19,     1] loss: 1366.184
[20,     1] loss: 1383.913
[21,     1] loss: 1350.909
[22,     1] loss: 1379.371
[23,     1] loss: 1315.506
[24,     1] loss: 1330.781
[25,     1] loss: 1341.784
[26,     1] loss: 1294.454
[27,     1] loss: 1268.461
[28,     1] loss: 1277.206
[29,     1] loss: 1292.667
[30,     1] loss: 1282.543
[31,     1] loss: 1260.413
[32,     1] loss: 1284.960
[33,     1] loss: 1187.675
[34,     1] loss: 1195.365
[35,     1] loss: 1182.181
[36,     1] loss: 1189.007
[37,     1] loss: 1097.055
[38,     1] loss: 1164.432
[39,     1] loss: 1281.066
[40,     1] loss: 1136.137
[41,     1] loss: 1145.670
[42,     1] loss: 1134.332
[43,     1] loss: 1099.164
[44,     1] loss: 1083.118
[45,     1] loss: 997.328
[46,     1] loss: 1000.681
[47,     1] loss: 957.285
[48,     1] loss: 1631.556
[49,     1] loss: 1736.672
[50,     1] loss: 1078.873
[51,     1] loss: 1170.972
[52,     1] loss: 1311.146
[53,     1] loss: 1213.866
[54,     1] loss: 1256.020
[55,     1] loss: 1223.603
[56,     1] loss: 1233.340
[57,     1] loss: 1201.151
[58,     1] loss: 1169.079
[59,     1] loss: 1116.732
[60,     1] loss: 1066.849
[61,     1] loss: 1066.741
[62,     1] loss: 994.763
[63,     1] loss: 984.454
[64,     1] loss: 1042.006
[65,     1] loss: 1018.790
[66,     1] loss: 972.954
[67,     1] loss: 1010.195
[68,     1] loss: 963.228
[69,     1] loss: 1077.833
[70,     1] loss: 941.798
[71,     1] loss: 914.972
[72,     1] loss: 947.983
[73,     1] loss: 896.409
[74,     1] loss: 938.340
[75,     1] loss: 1024.266
[76,     1] loss: 1153.304
[77,     1] loss: 840.572
[78,     1] loss: 901.131
[79,     1] loss: 965.762
[80,     1] loss: 887.676
[81,     1] loss: 891.021
[82,     1] loss: 918.892
[83,     1] loss: 837.025
[84,     1] loss: 768.596
[85,     1] loss: 747.927
[86,     1] loss: 792.842
[87,     1] loss: 854.386
[88,     1] loss: 1188.085
Early stopping applied (best metric=0.41110941767692566)
Finished Training
Total time taken: 12.862610578536987
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1602.313
[2,     1] loss: 1600.128
[3,     1] loss: 1600.120
[4,     1] loss: 1603.223
[5,     1] loss: 1606.786
[6,     1] loss: 1603.511
[7,     1] loss: 1599.900
[8,     1] loss: 1601.020
[9,     1] loss: 1597.236
[10,     1] loss: 1598.383
[11,     1] loss: 1594.079
[12,     1] loss: 1589.249
[13,     1] loss: 1589.105
[14,     1] loss: 1581.840
[15,     1] loss: 1563.909
[16,     1] loss: 1534.342
[17,     1] loss: 1520.862
[18,     1] loss: 1452.064
[19,     1] loss: 1431.886
[20,     1] loss: 1395.682
[21,     1] loss: 1379.862
[22,     1] loss: 1362.371
[23,     1] loss: 1305.290
[24,     1] loss: 1304.361
[25,     1] loss: 1325.938
[26,     1] loss: 1275.385
[27,     1] loss: 1328.498
[28,     1] loss: 1266.242
[29,     1] loss: 1319.978
[30,     1] loss: 1258.915
[31,     1] loss: 1244.226
[32,     1] loss: 1221.643
[33,     1] loss: 1246.688
[34,     1] loss: 1216.192
[35,     1] loss: 1206.422
[36,     1] loss: 1187.287
[37,     1] loss: 1132.814
[38,     1] loss: 1265.846
[39,     1] loss: 1280.312
[40,     1] loss: 1091.077
[41,     1] loss: 1266.798
[42,     1] loss: 1113.794
[43,     1] loss: 1224.014
[44,     1] loss: 1145.146
[45,     1] loss: 1147.344
[46,     1] loss: 1068.478
[47,     1] loss: 1022.209
[48,     1] loss: 1004.778
[49,     1] loss: 910.883
[50,     1] loss: 1112.641
[51,     1] loss: 1004.130
[52,     1] loss: 892.520
[53,     1] loss: 899.168
[54,     1] loss: 958.166
[55,     1] loss: 917.815
[56,     1] loss: 884.969
[57,     1] loss: 946.147
[58,     1] loss: 956.784
[59,     1] loss: 859.412
[60,     1] loss: 870.136
[61,     1] loss: 873.089
[62,     1] loss: 853.454
[63,     1] loss: 838.203
[64,     1] loss: 817.939
[65,     1] loss: 1216.207
[66,     1] loss: 1831.196
[67,     1] loss: 1040.719
[68,     1] loss: 1138.409
[69,     1] loss: 1182.513
[70,     1] loss: 1186.328
[71,     1] loss: 1163.290
[72,     1] loss: 1124.191
[73,     1] loss: 1140.183
[74,     1] loss: 1160.019
[75,     1] loss: 1061.039
[76,     1] loss: 1161.155
[77,     1] loss: 1072.422
[78,     1] loss: 1028.965
[79,     1] loss: 997.904
[80,     1] loss: 1028.743
[81,     1] loss: 1054.825
[82,     1] loss: 992.610
[83,     1] loss: 941.471
[84,     1] loss: 855.496
[85,     1] loss: 845.290
[86,     1] loss: 833.484
Early stopping applied (best metric=0.3636033535003662)
Finished Training
Total time taken: 12.61401915550232
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1609.863
[2,     1] loss: 1607.394
[3,     1] loss: 1603.817
[4,     1] loss: 1595.153
[5,     1] loss: 1613.645
[6,     1] loss: 1612.009
[7,     1] loss: 1598.167
[8,     1] loss: 1600.748
[9,     1] loss: 1601.305
[10,     1] loss: 1602.533
[11,     1] loss: 1601.174
[12,     1] loss: 1602.607
[13,     1] loss: 1598.650
[14,     1] loss: 1603.391
[15,     1] loss: 1604.286
[16,     1] loss: 1598.816
[17,     1] loss: 1599.968
[18,     1] loss: 1596.696
[19,     1] loss: 1594.551
[20,     1] loss: 1593.867
[21,     1] loss: 1584.137
[22,     1] loss: 1572.505
[23,     1] loss: 1547.185
[24,     1] loss: 1509.820
[25,     1] loss: 1466.898
[26,     1] loss: 1419.848
[27,     1] loss: 1385.312
[28,     1] loss: 1315.985
[29,     1] loss: 1379.686
[30,     1] loss: 1606.584
[31,     1] loss: 1344.995
[32,     1] loss: 1414.569
[33,     1] loss: 1368.638
[34,     1] loss: 1342.064
[35,     1] loss: 1340.722
[36,     1] loss: 1406.366
[37,     1] loss: 1318.333
[38,     1] loss: 1320.462
[39,     1] loss: 1287.466
[40,     1] loss: 1402.856
[41,     1] loss: 1283.686
[42,     1] loss: 1296.252
[43,     1] loss: 1267.725
[44,     1] loss: 1214.668
[45,     1] loss: 1200.511
[46,     1] loss: 1199.057
[47,     1] loss: 1225.845
[48,     1] loss: 1153.351
[49,     1] loss: 1155.044
[50,     1] loss: 1121.876
[51,     1] loss: 1060.724
[52,     1] loss: 1123.188
[53,     1] loss: 1112.840
[54,     1] loss: 1162.093
[55,     1] loss: 1090.501
[56,     1] loss: 1149.364
[57,     1] loss: 1051.565
[58,     1] loss: 1092.340
[59,     1] loss: 1009.326
[60,     1] loss: 1072.584
[61,     1] loss: 1004.232
[62,     1] loss: 1010.453
[63,     1] loss: 1036.831
[64,     1] loss: 993.342
[65,     1] loss: 974.553
[66,     1] loss: 923.272
[67,     1] loss: 1000.281
[68,     1] loss: 1241.305
[69,     1] loss: 942.167
[70,     1] loss: 1056.585
[71,     1] loss: 926.718
[72,     1] loss: 1020.816
[73,     1] loss: 905.393
[74,     1] loss: 1079.497
[75,     1] loss: 865.968
[76,     1] loss: 928.307
[77,     1] loss: 887.199
[78,     1] loss: 800.113
[79,     1] loss: 869.157
[80,     1] loss: 840.257
[81,     1] loss: 959.259
[82,     1] loss: 1292.295
[83,     1] loss: 1044.670
[84,     1] loss: 907.564
[85,     1] loss: 955.448
[86,     1] loss: 1080.059
[87,     1] loss: 873.948
[88,     1] loss: 1156.634
[89,     1] loss: 824.068
[90,     1] loss: 1119.672
[91,     1] loss: 899.445
[92,     1] loss: 971.156
[93,     1] loss: 976.312
[94,     1] loss: 837.410
[95,     1] loss: 965.080
[96,     1] loss: 801.373
[97,     1] loss: 904.097
[98,     1] loss: 811.851
[99,     1] loss: 881.958
[100,     1] loss: 902.566
[101,     1] loss: 797.979
[102,     1] loss: 731.665
[103,     1] loss: 856.386
[104,     1] loss: 740.062
[105,     1] loss: 766.364
[106,     1] loss: 820.957
[107,     1] loss: 1028.270
[108,     1] loss: 1132.843
[109,     1] loss: 895.942
[110,     1] loss: 883.426
[111,     1] loss: 882.460
[112,     1] loss: 990.431
[113,     1] loss: 832.875
[114,     1] loss: 912.682
[115,     1] loss: 938.181
[116,     1] loss: 756.828
[117,     1] loss: 820.361
[118,     1] loss: 825.532
[119,     1] loss: 802.799
[120,     1] loss: 761.856
[121,     1] loss: 787.374
[122,     1] loss: 736.312
[123,     1] loss: 785.093
[124,     1] loss: 829.533
[125,     1] loss: 1109.985
[126,     1] loss: 1815.307
[127,     1] loss: 967.516
[128,     1] loss: 1252.278
[129,     1] loss: 1165.470
[130,     1] loss: 1088.450
[131,     1] loss: 1143.598
[132,     1] loss: 1085.007
[133,     1] loss: 1049.766
[134,     1] loss: 1064.334
Early stopping applied (best metric=0.3978014588356018)
Finished Training
Total time taken: 19.643269538879395
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1600.930
[2,     1] loss: 1609.126
[3,     1] loss: 1599.550
[4,     1] loss: 1594.590
[5,     1] loss: 1601.376
[6,     1] loss: 1600.222
[7,     1] loss: 1603.928
[8,     1] loss: 1594.409
[9,     1] loss: 1590.629
[10,     1] loss: 1578.708
[11,     1] loss: 1575.613
[12,     1] loss: 1563.636
[13,     1] loss: 1513.673
[14,     1] loss: 1530.899
[15,     1] loss: 1474.041
[16,     1] loss: 1466.419
[17,     1] loss: 1421.079
[18,     1] loss: 1474.542
[19,     1] loss: 1418.229
[20,     1] loss: 1492.047
[21,     1] loss: 1418.278
[22,     1] loss: 1362.685
[23,     1] loss: 1365.709
[24,     1] loss: 1355.713
[25,     1] loss: 1335.968
[26,     1] loss: 1441.693
[27,     1] loss: 1309.427
[28,     1] loss: 1350.827
[29,     1] loss: 1349.272
[30,     1] loss: 1279.816
[31,     1] loss: 1367.003
[32,     1] loss: 1237.939
[33,     1] loss: 1217.448
[34,     1] loss: 1291.526
[35,     1] loss: 1216.109
[36,     1] loss: 1227.329
[37,     1] loss: 1213.769
[38,     1] loss: 1160.419
[39,     1] loss: 1233.698
[40,     1] loss: 1154.987
[41,     1] loss: 1199.096
[42,     1] loss: 1085.504
[43,     1] loss: 1137.384
[44,     1] loss: 1111.190
[45,     1] loss: 1125.951
[46,     1] loss: 1335.117
[47,     1] loss: 1294.551
[48,     1] loss: 1146.880
[49,     1] loss: 1187.607
[50,     1] loss: 1226.126
[51,     1] loss: 1161.695
[52,     1] loss: 1139.191
[53,     1] loss: 1252.152
[54,     1] loss: 1066.921
[55,     1] loss: 1094.946
[56,     1] loss: 1034.199
[57,     1] loss: 1096.099
[58,     1] loss: 999.684
[59,     1] loss: 1124.445
[60,     1] loss: 965.689
[61,     1] loss: 1045.784
[62,     1] loss: 917.520
[63,     1] loss: 964.661
[64,     1] loss: 946.224
[65,     1] loss: 949.259
[66,     1] loss: 1130.604
[67,     1] loss: 865.132
[68,     1] loss: 900.896
[69,     1] loss: 1032.202
[70,     1] loss: 838.954
[71,     1] loss: 995.550
[72,     1] loss: 842.987
[73,     1] loss: 845.451
[74,     1] loss: 848.594
[75,     1] loss: 737.805
[76,     1] loss: 794.405
[77,     1] loss: 832.995
[78,     1] loss: 1777.022
[79,     1] loss: 807.686
[80,     1] loss: 1160.122
[81,     1] loss: 973.066
[82,     1] loss: 1178.113
[83,     1] loss: 1083.213
[84,     1] loss: 1010.205
[85,     1] loss: 1072.455
[86,     1] loss: 906.213
[87,     1] loss: 939.176
[88,     1] loss: 947.721
[89,     1] loss: 908.575
[90,     1] loss: 827.749
[91,     1] loss: 812.124
[92,     1] loss: 843.473
[93,     1] loss: 772.381
[94,     1] loss: 797.747
[95,     1] loss: 738.736
[96,     1] loss: 740.728
[97,     1] loss: 704.723
[98,     1] loss: 1096.035
[99,     1] loss: 1515.742
[100,     1] loss: 1540.881
[101,     1] loss: 1396.510
[102,     1] loss: 1304.859
[103,     1] loss: 1370.476
[104,     1] loss: 1400.295
[105,     1] loss: 1450.180
[106,     1] loss: 1418.977
[107,     1] loss: 1380.988
[108,     1] loss: 1326.208
[109,     1] loss: 1347.618
[110,     1] loss: 1289.052
[111,     1] loss: 1206.340
[112,     1] loss: 1332.535
[113,     1] loss: 1233.180
[114,     1] loss: 1268.744
[115,     1] loss: 1278.152
[116,     1] loss: 1164.995
[117,     1] loss: 1202.944
[118,     1] loss: 1199.232
[119,     1] loss: 1152.808
[120,     1] loss: 1233.134
[121,     1] loss: 1198.517
[122,     1] loss: 1090.927
[123,     1] loss: 1300.716
[124,     1] loss: 1146.203
[125,     1] loss: 1093.281
[126,     1] loss: 1132.026
[127,     1] loss: 1123.480
[128,     1] loss: 1146.023
[129,     1] loss: 998.225
[130,     1] loss: 1129.215
[131,     1] loss: 996.316
[132,     1] loss: 1153.184
[133,     1] loss: 973.168
Early stopping applied (best metric=0.34224051237106323)
Finished Training
Total time taken: 19.428715229034424
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1604.706
[2,     1] loss: 1599.754
[3,     1] loss: 1607.367
[4,     1] loss: 1605.336
[5,     1] loss: 1598.056
[6,     1] loss: 1598.452
[7,     1] loss: 1592.669
[8,     1] loss: 1597.409
[9,     1] loss: 1585.407
[10,     1] loss: 1582.784
[11,     1] loss: 1561.866
[12,     1] loss: 1535.918
[13,     1] loss: 1498.567
[14,     1] loss: 1450.361
[15,     1] loss: 1409.949
[16,     1] loss: 1359.475
[17,     1] loss: 1348.495
[18,     1] loss: 1325.999
[19,     1] loss: 1318.261
[20,     1] loss: 1260.528
[21,     1] loss: 1335.838
[22,     1] loss: 1264.542
[23,     1] loss: 1308.567
[24,     1] loss: 1271.740
[25,     1] loss: 1250.834
[26,     1] loss: 1235.404
[27,     1] loss: 1211.602
[28,     1] loss: 1160.680
[29,     1] loss: 1174.090
[30,     1] loss: 1162.521
[31,     1] loss: 1178.814
[32,     1] loss: 1274.777
[33,     1] loss: 1186.950
[34,     1] loss: 1165.829
[35,     1] loss: 1198.297
[36,     1] loss: 1185.801
[37,     1] loss: 1112.333
[38,     1] loss: 1247.921
[39,     1] loss: 1076.393
[40,     1] loss: 1134.117
[41,     1] loss: 1043.931
[42,     1] loss: 1100.309
[43,     1] loss: 1054.149
[44,     1] loss: 1044.780
[45,     1] loss: 1245.064
[46,     1] loss: 975.611
[47,     1] loss: 1090.555
[48,     1] loss: 1013.933
[49,     1] loss: 1032.786
[50,     1] loss: 1106.230
[51,     1] loss: 894.062
[52,     1] loss: 1004.256
[53,     1] loss: 949.390
[54,     1] loss: 927.444
[55,     1] loss: 1027.010
[56,     1] loss: 1069.261
[57,     1] loss: 834.613
[58,     1] loss: 1143.938
[59,     1] loss: 864.734
[60,     1] loss: 987.186
[61,     1] loss: 923.489
[62,     1] loss: 965.500
[63,     1] loss: 855.455
[64,     1] loss: 908.282
[65,     1] loss: 844.789
[66,     1] loss: 828.150
[67,     1] loss: 711.483
[68,     1] loss: 797.112
[69,     1] loss: 924.752
Early stopping applied (best metric=0.41046595573425293)
Finished Training
Total time taken: 10.21267294883728
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1600.711
[2,     1] loss: 1604.089
[3,     1] loss: 1603.889
[4,     1] loss: 1602.942
[5,     1] loss: 1601.422
[6,     1] loss: 1594.796
[7,     1] loss: 1598.288
[8,     1] loss: 1589.370
[9,     1] loss: 1592.084
[10,     1] loss: 1578.205
[11,     1] loss: 1561.996
[12,     1] loss: 1552.919
[13,     1] loss: 1512.660
[14,     1] loss: 1469.091
[15,     1] loss: 1393.067
[16,     1] loss: 1406.429
[17,     1] loss: 1378.100
[18,     1] loss: 1443.858
[19,     1] loss: 1270.490
[20,     1] loss: 1374.737
[21,     1] loss: 1331.153
[22,     1] loss: 1332.278
[23,     1] loss: 1300.769
[24,     1] loss: 1332.770
[25,     1] loss: 1289.407
[26,     1] loss: 1254.801
[27,     1] loss: 1320.290
[28,     1] loss: 1216.297
[29,     1] loss: 1238.960
[30,     1] loss: 1201.789
[31,     1] loss: 1163.060
[32,     1] loss: 1135.531
[33,     1] loss: 1168.035
[34,     1] loss: 1133.576
[35,     1] loss: 1104.452
[36,     1] loss: 1186.959
[37,     1] loss: 1216.279
[38,     1] loss: 1017.007
[39,     1] loss: 1149.113
[40,     1] loss: 1221.766
[41,     1] loss: 1040.032
[42,     1] loss: 1040.464
[43,     1] loss: 1087.128
[44,     1] loss: 1077.731
[45,     1] loss: 1034.554
[46,     1] loss: 978.970
[47,     1] loss: 954.718
[48,     1] loss: 906.637
[49,     1] loss: 910.531
[50,     1] loss: 949.873
[51,     1] loss: 1043.415
[52,     1] loss: 1273.623
[53,     1] loss: 927.170
[54,     1] loss: 1217.251
[55,     1] loss: 992.424
[56,     1] loss: 1153.755
[57,     1] loss: 1146.656
[58,     1] loss: 1006.004
[59,     1] loss: 1005.206
[60,     1] loss: 939.388
[61,     1] loss: 1002.385
[62,     1] loss: 797.696
[63,     1] loss: 885.316
[64,     1] loss: 764.059
[65,     1] loss: 811.467
[66,     1] loss: 895.694
[67,     1] loss: 802.638
[68,     1] loss: 895.244
[69,     1] loss: 703.895
[70,     1] loss: 820.087
[71,     1] loss: 947.863
[72,     1] loss: 909.906
[73,     1] loss: 763.845
[74,     1] loss: 861.095
[75,     1] loss: 753.860
[76,     1] loss: 810.806
[77,     1] loss: 677.830
[78,     1] loss: 732.170
[79,     1] loss: 937.036
[80,     1] loss: 1146.808
Early stopping applied (best metric=0.39607641100883484)
Finished Training
Total time taken: 11.750016927719116
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1602.461
[2,     1] loss: 1601.674
[3,     1] loss: 1603.934
[4,     1] loss: 1601.490
[5,     1] loss: 1595.570
[6,     1] loss: 1592.091
[7,     1] loss: 1590.244
[8,     1] loss: 1575.712
[9,     1] loss: 1567.560
[10,     1] loss: 1541.766
[11,     1] loss: 1474.871
[12,     1] loss: 1428.843
[13,     1] loss: 1429.447
[14,     1] loss: 1414.019
[15,     1] loss: 1415.324
[16,     1] loss: 1288.984
[17,     1] loss: 1325.934
[18,     1] loss: 1309.157
[19,     1] loss: 1318.595
[20,     1] loss: 1276.474
[21,     1] loss: 1282.743
[22,     1] loss: 1215.193
[23,     1] loss: 1238.371
[24,     1] loss: 1189.408
[25,     1] loss: 1151.694
[26,     1] loss: 1139.206
[27,     1] loss: 1141.253
[28,     1] loss: 1104.991
[29,     1] loss: 1230.679
[30,     1] loss: 1110.522
[31,     1] loss: 1089.210
[32,     1] loss: 1067.038
[33,     1] loss: 1058.209
[34,     1] loss: 1080.314
[35,     1] loss: 1018.894
[36,     1] loss: 1033.400
[37,     1] loss: 1015.325
[38,     1] loss: 959.564
[39,     1] loss: 1181.156
[40,     1] loss: 1204.587
[41,     1] loss: 919.242
[42,     1] loss: 1076.867
[43,     1] loss: 922.820
[44,     1] loss: 905.169
[45,     1] loss: 865.056
[46,     1] loss: 961.561
[47,     1] loss: 829.164
[48,     1] loss: 924.077
[49,     1] loss: 947.487
[50,     1] loss: 880.507
[51,     1] loss: 1131.944
[52,     1] loss: 1258.881
[53,     1] loss: 1033.298
[54,     1] loss: 993.442
[55,     1] loss: 1057.867
[56,     1] loss: 1021.052
[57,     1] loss: 998.844
[58,     1] loss: 1052.811
[59,     1] loss: 892.227
[60,     1] loss: 799.536
[61,     1] loss: 888.895
[62,     1] loss: 903.817
[63,     1] loss: 799.217
[64,     1] loss: 889.316
[65,     1] loss: 769.771
[66,     1] loss: 853.805
[67,     1] loss: 788.831
[68,     1] loss: 763.676
[69,     1] loss: 718.926
[70,     1] loss: 670.697
[71,     1] loss: 892.202
[72,     1] loss: 1094.607
[73,     1] loss: 672.423
[74,     1] loss: 1006.886
[75,     1] loss: 919.686
[76,     1] loss: 924.374
[77,     1] loss: 753.281
[78,     1] loss: 1022.955
[79,     1] loss: 758.497
[80,     1] loss: 988.349
[81,     1] loss: 766.801
Early stopping applied (best metric=0.4295288026332855)
Finished Training
Total time taken: 11.84871530532837
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1600.501
[2,     1] loss: 1622.311
[3,     1] loss: 1611.169
[4,     1] loss: 1602.823
[5,     1] loss: 1601.689
[6,     1] loss: 1595.887
[7,     1] loss: 1610.437
[8,     1] loss: 1604.888
[9,     1] loss: 1603.243
[10,     1] loss: 1600.849
[11,     1] loss: 1601.543
[12,     1] loss: 1602.075
[13,     1] loss: 1598.938
[14,     1] loss: 1599.620
[15,     1] loss: 1599.079
[16,     1] loss: 1591.991
[17,     1] loss: 1588.784
[18,     1] loss: 1581.412
[19,     1] loss: 1566.022
[20,     1] loss: 1557.901
[21,     1] loss: 1526.742
[22,     1] loss: 1508.743
[23,     1] loss: 1456.233
[24,     1] loss: 1472.796
[25,     1] loss: 1411.258
[26,     1] loss: 1402.236
[27,     1] loss: 1397.118
[28,     1] loss: 1351.380
[29,     1] loss: 1348.431
[30,     1] loss: 1330.032
[31,     1] loss: 1284.134
[32,     1] loss: 1337.162
[33,     1] loss: 1300.568
[34,     1] loss: 1332.576
[35,     1] loss: 1287.305
[36,     1] loss: 1228.796
[37,     1] loss: 1237.425
[38,     1] loss: 1173.465
[39,     1] loss: 1156.100
[40,     1] loss: 1242.191
[41,     1] loss: 1322.332
[42,     1] loss: 1131.644
[43,     1] loss: 1194.953
[44,     1] loss: 1153.120
[45,     1] loss: 1155.127
[46,     1] loss: 1117.196
[47,     1] loss: 1088.597
[48,     1] loss: 1082.218
[49,     1] loss: 1103.802
[50,     1] loss: 1143.035
[51,     1] loss: 1027.924
[52,     1] loss: 962.219
[53,     1] loss: 1077.442
[54,     1] loss: 990.934
[55,     1] loss: 907.440
[56,     1] loss: 994.778
[57,     1] loss: 1019.835
[58,     1] loss: 1206.298
[59,     1] loss: 1698.769
[60,     1] loss: 1049.319
[61,     1] loss: 1269.211
[62,     1] loss: 1272.172
[63,     1] loss: 1154.243
[64,     1] loss: 1194.283
[65,     1] loss: 1258.597
[66,     1] loss: 1201.860
[67,     1] loss: 1165.235
[68,     1] loss: 1109.553
[69,     1] loss: 1204.784
[70,     1] loss: 1126.087
[71,     1] loss: 1088.612
[72,     1] loss: 1065.285
[73,     1] loss: 1021.392
[74,     1] loss: 1063.854
[75,     1] loss: 983.360
[76,     1] loss: 966.526
[77,     1] loss: 909.632
[78,     1] loss: 873.258
[79,     1] loss: 897.812
[80,     1] loss: 909.488
[81,     1] loss: 864.820
[82,     1] loss: 827.624
[83,     1] loss: 877.928
[84,     1] loss: 976.794
[85,     1] loss: 1634.187
[86,     1] loss: 1594.288
[87,     1] loss: 1139.185
[88,     1] loss: 1207.369
[89,     1] loss: 1289.673
[90,     1] loss: 1328.385
[91,     1] loss: 1291.265
Early stopping applied (best metric=0.38463321328163147)
Finished Training
Total time taken: 13.342065811157227
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1599.071
[2,     1] loss: 1606.896
[3,     1] loss: 1601.731
[4,     1] loss: 1601.715
[5,     1] loss: 1595.572
[6,     1] loss: 1603.928
[7,     1] loss: 1589.025
[8,     1] loss: 1580.912
[9,     1] loss: 1562.570
[10,     1] loss: 1521.442
[11,     1] loss: 1482.370
[12,     1] loss: 1435.222
[13,     1] loss: 1364.000
[14,     1] loss: 1341.173
[15,     1] loss: 1374.888
[16,     1] loss: 1280.830
[17,     1] loss: 1304.332
[18,     1] loss: 1277.629
[19,     1] loss: 1283.874
[20,     1] loss: 1234.910
[21,     1] loss: 1235.553
[22,     1] loss: 1218.998
[23,     1] loss: 1233.353
[24,     1] loss: 1202.848
[25,     1] loss: 1202.693
[26,     1] loss: 1166.059
[27,     1] loss: 1093.745
[28,     1] loss: 1126.391
[29,     1] loss: 1117.011
[30,     1] loss: 1049.669
[31,     1] loss: 1047.678
[32,     1] loss: 1039.246
[33,     1] loss: 1029.174
[34,     1] loss: 999.230
[35,     1] loss: 991.259
[36,     1] loss: 1057.721
[37,     1] loss: 1183.048
[38,     1] loss: 1248.469
[39,     1] loss: 985.408
[40,     1] loss: 1161.001
[41,     1] loss: 1077.588
[42,     1] loss: 1098.129
[43,     1] loss: 1064.875
[44,     1] loss: 1009.049
[45,     1] loss: 979.058
[46,     1] loss: 935.901
[47,     1] loss: 908.430
[48,     1] loss: 1057.363
[49,     1] loss: 847.675
[50,     1] loss: 947.425
[51,     1] loss: 853.222
[52,     1] loss: 860.184
[53,     1] loss: 941.578
[54,     1] loss: 813.384
[55,     1] loss: 796.976
[56,     1] loss: 851.966
[57,     1] loss: 1073.030
[58,     1] loss: 1175.877
[59,     1] loss: 925.493
[60,     1] loss: 912.054
[61,     1] loss: 863.325
[62,     1] loss: 872.163
[63,     1] loss: 836.693
[64,     1] loss: 785.442
[65,     1] loss: 799.288
Early stopping applied (best metric=0.4670996367931366)
Finished Training
Total time taken: 9.318190336227417
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1597.425
[2,     1] loss: 1607.292
[3,     1] loss: 1600.968
[4,     1] loss: 1600.398
[5,     1] loss: 1606.203
[6,     1] loss: 1597.630
[7,     1] loss: 1605.594
[8,     1] loss: 1597.744
[9,     1] loss: 1597.847
[10,     1] loss: 1596.954
[11,     1] loss: 1596.186
[12,     1] loss: 1592.639
[13,     1] loss: 1588.273
[14,     1] loss: 1585.976
[15,     1] loss: 1564.740
[16,     1] loss: 1550.557
[17,     1] loss: 1508.859
[18,     1] loss: 1476.122
[19,     1] loss: 1443.768
[20,     1] loss: 1410.710
[21,     1] loss: 1359.336
[22,     1] loss: 1378.461
[23,     1] loss: 1413.346
[24,     1] loss: 1337.397
[25,     1] loss: 1345.206
[26,     1] loss: 1343.798
[27,     1] loss: 1382.591
[28,     1] loss: 1331.516
[29,     1] loss: 1335.527
[30,     1] loss: 1290.113
[31,     1] loss: 1330.901
[32,     1] loss: 1262.129
[33,     1] loss: 1333.715
[34,     1] loss: 1280.052
[35,     1] loss: 1257.896
[36,     1] loss: 1212.646
[37,     1] loss: 1183.535
[38,     1] loss: 1238.131
[39,     1] loss: 1197.228
[40,     1] loss: 1229.256
[41,     1] loss: 1177.871
[42,     1] loss: 1164.931
[43,     1] loss: 1154.512
[44,     1] loss: 1140.352
[45,     1] loss: 1105.355
[46,     1] loss: 1040.308
[47,     1] loss: 1051.287
[48,     1] loss: 1092.188
[49,     1] loss: 1391.438
[50,     1] loss: 1426.578
[51,     1] loss: 1171.156
[52,     1] loss: 1159.597
[53,     1] loss: 1368.375
[54,     1] loss: 1254.142
[55,     1] loss: 1229.312
[56,     1] loss: 1213.343
[57,     1] loss: 1206.048
[58,     1] loss: 1185.495
[59,     1] loss: 1089.389
[60,     1] loss: 1168.483
[61,     1] loss: 1071.574
[62,     1] loss: 1055.238
[63,     1] loss: 1084.457
[64,     1] loss: 1024.746
[65,     1] loss: 949.792
[66,     1] loss: 976.898
[67,     1] loss: 978.496
[68,     1] loss: 938.617
[69,     1] loss: 887.586
[70,     1] loss: 1027.206
[71,     1] loss: 945.294
[72,     1] loss: 853.248
[73,     1] loss: 981.816
[74,     1] loss: 1139.983
[75,     1] loss: 848.095
[76,     1] loss: 870.154
[77,     1] loss: 869.819
[78,     1] loss: 782.140
[79,     1] loss: 1155.087
[80,     1] loss: 1416.917
[81,     1] loss: 892.880
[82,     1] loss: 1056.287
[83,     1] loss: 1070.466
[84,     1] loss: 1024.144
[85,     1] loss: 1043.723
[86,     1] loss: 966.790
[87,     1] loss: 917.465
[88,     1] loss: 841.563
[89,     1] loss: 919.839
[90,     1] loss: 802.679
[91,     1] loss: 856.871
[92,     1] loss: 781.719
[93,     1] loss: 767.740
[94,     1] loss: 1022.870
[95,     1] loss: 1466.202
[96,     1] loss: 947.288
Early stopping applied (best metric=0.3188028633594513)
Finished Training
Total time taken: 13.971245765686035
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1605.580
[2,     1] loss: 1601.420
[3,     1] loss: 1602.819
[4,     1] loss: 1611.467
[5,     1] loss: 1602.972
[6,     1] loss: 1597.335
[7,     1] loss: 1599.868
[8,     1] loss: 1602.083
[9,     1] loss: 1600.945
[10,     1] loss: 1600.001
[11,     1] loss: 1598.628
[12,     1] loss: 1598.702
[13,     1] loss: 1598.738
[14,     1] loss: 1595.259
[15,     1] loss: 1593.707
[16,     1] loss: 1588.381
[17,     1] loss: 1585.762
[18,     1] loss: 1566.610
[19,     1] loss: 1547.947
[20,     1] loss: 1510.324
[21,     1] loss: 1489.620
[22,     1] loss: 1461.732
[23,     1] loss: 1432.093
[24,     1] loss: 1402.388
[25,     1] loss: 1393.075
[26,     1] loss: 1410.792
[27,     1] loss: 1360.528
[28,     1] loss: 1338.373
[29,     1] loss: 1312.030
[30,     1] loss: 1321.387
[31,     1] loss: 1323.314
[32,     1] loss: 1283.858
[33,     1] loss: 1272.456
[34,     1] loss: 1210.048
[35,     1] loss: 1219.482
[36,     1] loss: 1205.768
[37,     1] loss: 1110.443
[38,     1] loss: 1139.103
[39,     1] loss: 1144.985
[40,     1] loss: 1219.760
[41,     1] loss: 1269.285
[42,     1] loss: 1132.743
[43,     1] loss: 1123.425
[44,     1] loss: 1130.674
[45,     1] loss: 1067.342
[46,     1] loss: 1008.321
[47,     1] loss: 1030.727
[48,     1] loss: 1240.624
[49,     1] loss: 1401.066
[50,     1] loss: 1098.592
[51,     1] loss: 1178.475
[52,     1] loss: 1118.609
[53,     1] loss: 1164.802
[54,     1] loss: 1175.559
[55,     1] loss: 1132.783
[56,     1] loss: 1079.584
[57,     1] loss: 1064.775
[58,     1] loss: 1079.621
[59,     1] loss: 1029.137
[60,     1] loss: 1092.023
[61,     1] loss: 1017.460
[62,     1] loss: 958.669
[63,     1] loss: 1011.488
[64,     1] loss: 843.649
[65,     1] loss: 936.336
[66,     1] loss: 890.508
[67,     1] loss: 836.126
[68,     1] loss: 823.345
[69,     1] loss: 797.327
[70,     1] loss: 830.679
[71,     1] loss: 1043.843
[72,     1] loss: 983.432
[73,     1] loss: 912.872
[74,     1] loss: 833.445
[75,     1] loss: 965.040
[76,     1] loss: 840.136
[77,     1] loss: 899.028
[78,     1] loss: 876.372
[79,     1] loss: 776.952
[80,     1] loss: 880.166
[81,     1] loss: 1129.960
[82,     1] loss: 1540.800
[83,     1] loss: 814.250
[84,     1] loss: 1205.414
[85,     1] loss: 1024.697
[86,     1] loss: 1119.660
[87,     1] loss: 1120.629
[88,     1] loss: 990.444
[89,     1] loss: 1074.188
[90,     1] loss: 940.337
[91,     1] loss: 1004.016
[92,     1] loss: 814.156
[93,     1] loss: 945.481
Early stopping applied (best metric=0.40110576152801514)
Finished Training
Total time taken: 13.667343139648438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1609.824
[2,     1] loss: 1601.624
[3,     1] loss: 1605.670
[4,     1] loss: 1597.616
[5,     1] loss: 1597.419
[6,     1] loss: 1605.833
[7,     1] loss: 1600.877
[8,     1] loss: 1593.903
[9,     1] loss: 1592.899
[10,     1] loss: 1592.003
[11,     1] loss: 1586.570
[12,     1] loss: 1566.377
[13,     1] loss: 1543.485
[14,     1] loss: 1518.484
[15,     1] loss: 1478.957
[16,     1] loss: 1473.262
[17,     1] loss: 1445.580
[18,     1] loss: 1417.973
[19,     1] loss: 1389.324
[20,     1] loss: 1361.549
[21,     1] loss: 1401.763
[22,     1] loss: 1377.809
[23,     1] loss: 1378.962
[24,     1] loss: 1320.041
[25,     1] loss: 1316.384
[26,     1] loss: 1303.191
[27,     1] loss: 1300.118
[28,     1] loss: 1326.135
[29,     1] loss: 1284.189
[30,     1] loss: 1263.847
[31,     1] loss: 1203.201
[32,     1] loss: 1237.897
[33,     1] loss: 1266.518
[34,     1] loss: 1263.205
[35,     1] loss: 1185.848
[36,     1] loss: 1311.245
[37,     1] loss: 1227.204
[38,     1] loss: 1318.851
[39,     1] loss: 1262.943
[40,     1] loss: 1164.153
[41,     1] loss: 1253.150
[42,     1] loss: 1083.826
[43,     1] loss: 1080.166
[44,     1] loss: 1168.148
[45,     1] loss: 1054.714
[46,     1] loss: 1084.719
[47,     1] loss: 1074.182
[48,     1] loss: 1069.745
[49,     1] loss: 1046.917
[50,     1] loss: 997.458
[51,     1] loss: 962.602
[52,     1] loss: 984.322
[53,     1] loss: 966.253
[54,     1] loss: 901.063
[55,     1] loss: 1058.297
[56,     1] loss: 1053.235
[57,     1] loss: 938.160
[58,     1] loss: 944.673
[59,     1] loss: 1025.823
[60,     1] loss: 917.030
[61,     1] loss: 913.541
[62,     1] loss: 962.627
[63,     1] loss: 891.965
[64,     1] loss: 830.592
[65,     1] loss: 929.210
[66,     1] loss: 953.470
[67,     1] loss: 1190.467
[68,     1] loss: 884.017
[69,     1] loss: 917.438
[70,     1] loss: 940.687
[71,     1] loss: 925.847
[72,     1] loss: 904.956
[73,     1] loss: 943.511
[74,     1] loss: 945.563
[75,     1] loss: 914.250
[76,     1] loss: 917.044
[77,     1] loss: 800.341
[78,     1] loss: 972.591
[79,     1] loss: 780.927
[80,     1] loss: 908.081
[81,     1] loss: 882.059
[82,     1] loss: 773.216
[83,     1] loss: 816.204
[84,     1] loss: 756.034
[85,     1] loss: 863.253
[86,     1] loss: 807.520
[87,     1] loss: 708.755
[88,     1] loss: 917.090
[89,     1] loss: 965.425
[90,     1] loss: 921.735
[91,     1] loss: 775.036
[92,     1] loss: 911.938
[93,     1] loss: 840.728
[94,     1] loss: 943.983
[95,     1] loss: 790.922
[96,     1] loss: 872.666
[97,     1] loss: 1114.918
[98,     1] loss: 888.419
[99,     1] loss: 740.789
[100,     1] loss: 796.698
[101,     1] loss: 698.503
[102,     1] loss: 811.196
[103,     1] loss: 732.182
[104,     1] loss: 715.194
[105,     1] loss: 737.670
[106,     1] loss: 834.597
[107,     1] loss: 1099.652
[108,     1] loss: 1810.192
[109,     1] loss: 936.950
[110,     1] loss: 1188.003
[111,     1] loss: 1306.287
[112,     1] loss: 1152.829
[113,     1] loss: 1074.920
[114,     1] loss: 1152.215
[115,     1] loss: 1047.524
[116,     1] loss: 988.394
[117,     1] loss: 1035.949
[118,     1] loss: 1072.485
[119,     1] loss: 929.971
[120,     1] loss: 1007.592
[121,     1] loss: 1014.569
[122,     1] loss: 956.347
[123,     1] loss: 976.785
[124,     1] loss: 882.600
[125,     1] loss: 939.439
[126,     1] loss: 808.583
[127,     1] loss: 858.428
[128,     1] loss: 928.742
[129,     1] loss: 1013.605
[130,     1] loss: 820.822
[131,     1] loss: 1121.035
[132,     1] loss: 977.308
[133,     1] loss: 915.833
[134,     1] loss: 978.808
[135,     1] loss: 844.467
[136,     1] loss: 993.478
[137,     1] loss: 768.110
[138,     1] loss: 935.001
[139,     1] loss: 1114.565
[140,     1] loss: 794.422
[141,     1] loss: 1130.065
[142,     1] loss: 1017.612
[143,     1] loss: 1047.453
[144,     1] loss: 847.992
[145,     1] loss: 1061.101
[146,     1] loss: 841.788
[147,     1] loss: 951.724
[148,     1] loss: 856.394
[149,     1] loss: 927.272
[150,     1] loss: 859.758
[151,     1] loss: 801.045
[152,     1] loss: 759.295
Early stopping applied (best metric=0.2720353305339813)
Finished Training
Total time taken: 22.1505024433136
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1604.473
[2,     1] loss: 1607.580
[3,     1] loss: 1605.534
[4,     1] loss: 1603.618
[5,     1] loss: 1599.706
[6,     1] loss: 1599.066
[7,     1] loss: 1599.530
[8,     1] loss: 1597.383
[9,     1] loss: 1590.681
[10,     1] loss: 1587.464
[11,     1] loss: 1582.092
[12,     1] loss: 1556.609
[13,     1] loss: 1539.749
[14,     1] loss: 1504.575
[15,     1] loss: 1444.243
[16,     1] loss: 1448.039
[17,     1] loss: 1420.340
[18,     1] loss: 1379.021
[19,     1] loss: 1369.271
[20,     1] loss: 1332.948
[21,     1] loss: 1267.126
[22,     1] loss: 1269.924
[23,     1] loss: 1327.131
[24,     1] loss: 1248.022
[25,     1] loss: 1259.384
[26,     1] loss: 1292.205
[27,     1] loss: 1201.357
[28,     1] loss: 1200.897
[29,     1] loss: 1240.511
[30,     1] loss: 1198.016
[31,     1] loss: 1143.513
[32,     1] loss: 1214.495
[33,     1] loss: 1115.428
[34,     1] loss: 1170.431
[35,     1] loss: 1134.198
[36,     1] loss: 1102.006
[37,     1] loss: 1207.027
[38,     1] loss: 1108.020
[39,     1] loss: 1064.185
[40,     1] loss: 1165.310
[41,     1] loss: 1071.846
[42,     1] loss: 1065.778
[43,     1] loss: 1234.992
[44,     1] loss: 1119.750
[45,     1] loss: 1104.928
[46,     1] loss: 1201.536
[47,     1] loss: 1160.400
[48,     1] loss: 1071.809
[49,     1] loss: 1220.545
[50,     1] loss: 1003.436
[51,     1] loss: 1044.987
[52,     1] loss: 1060.353
[53,     1] loss: 1016.126
[54,     1] loss: 1040.868
[55,     1] loss: 993.354
[56,     1] loss: 1067.213
[57,     1] loss: 1029.302
[58,     1] loss: 950.387
[59,     1] loss: 941.222
[60,     1] loss: 970.041
[61,     1] loss: 866.698
[62,     1] loss: 884.907
[63,     1] loss: 796.893
[64,     1] loss: 898.437
[65,     1] loss: 1245.980
[66,     1] loss: 1125.761
[67,     1] loss: 896.834
[68,     1] loss: 997.264
[69,     1] loss: 1014.019
[70,     1] loss: 901.265
[71,     1] loss: 942.344
[72,     1] loss: 807.471
[73,     1] loss: 749.501
[74,     1] loss: 925.596
[75,     1] loss: 845.280
[76,     1] loss: 871.963
[77,     1] loss: 795.573
[78,     1] loss: 782.215
[79,     1] loss: 754.565
[80,     1] loss: 715.156
[81,     1] loss: 718.671
[82,     1] loss: 827.597
[83,     1] loss: 1302.645
[84,     1] loss: 1316.251
[85,     1] loss: 788.802
[86,     1] loss: 1010.992
[87,     1] loss: 921.808
[88,     1] loss: 897.320
[89,     1] loss: 942.788
[90,     1] loss: 911.566
[91,     1] loss: 993.907
[92,     1] loss: 867.547
[93,     1] loss: 1050.241
[94,     1] loss: 971.618
[95,     1] loss: 856.811
[96,     1] loss: 912.969
[97,     1] loss: 785.926
[98,     1] loss: 802.223
[99,     1] loss: 721.097
[100,     1] loss: 806.625
[101,     1] loss: 717.409
[102,     1] loss: 672.867
[103,     1] loss: 860.225
[104,     1] loss: 864.922
[105,     1] loss: 662.610
[106,     1] loss: 862.488
[107,     1] loss: 953.113
[108,     1] loss: 701.084
Early stopping applied (best metric=0.3866352438926697)
Finished Training
Total time taken: 15.841199398040771
{'Hydroxylation-K Validation Accuracy': 0.729403073286052, 'Hydroxylation-K Validation Sensitivity': 0.6911111111111111, 'Hydroxylation-K Validation Specificity': 0.7385964912280701, 'Hydroxylation-K Validation Precision': 0.4067673232527183, 'Hydroxylation-K AUC ROC': 0.7891228070175439, 'Hydroxylation-K AUC PR': 0.5716008313898764, 'Hydroxylation-K MCC': 0.3658920003561972, 'Hydroxylation-K F1': 0.5099159852493186, 'Validation Loss (Hydroxylation-K)': 0.46803579131762185, 'Hydroxylation-P Validation Accuracy': 0.7484144628868247, 'Hydroxylation-P Validation Sensitivity': 0.8143386243386244, 'Hydroxylation-P Validation Specificity': 0.734208688712654, 'Hydroxylation-P Validation Precision': 0.4039711844439569, 'Hydroxylation-P AUC ROC': 0.8315354148209901, 'Hydroxylation-P AUC PR': 0.5551895931147386, 'Hydroxylation-P MCC': 0.4396957732202927, 'Hydroxylation-P F1': 0.537653028712734, 'Validation Loss (Hydroxylation-P)': 0.38576907515525816, 'Validation Loss (total)': 0.853804870446523, 'TimeToTrain': 13.968480428059896}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017287253226864752,
 'learning_rate_Hydroxylation-K': 0.0075500935318391525,
 'learning_rate_Hydroxylation-P': 0.001077066081077493,
 'log_base': 2.341027560818046,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1667454439,
 'sample_weights': [3.287425514882655, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8933706496054654,
 'weight_decay_Hydroxylation-K': 9.031115689391948,
 'weight_decay_Hydroxylation-P': 7.393627247660235}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1320.864
[2,     1] loss: 1321.107
[3,     1] loss: 1320.217
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00017669855315022797,
 'learning_rate_Hydroxylation-K': 0.006073878502143332,
 'learning_rate_Hydroxylation-P': 0.0022855015502826447,
 'log_base': 1.164122294365024,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1381167157,
 'sample_weights': [1.9626885137578634, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.534419453572308,
 'weight_decay_Hydroxylation-K': 9.24441896712579,
 'weight_decay_Hydroxylation-P': 1.2729727630203642}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3567.829
[2,     1] loss: 3574.070
[3,     1] loss: 3585.157
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004855156438051189,
 'learning_rate_Hydroxylation-K': 0.0094074380070448,
 'learning_rate_Hydroxylation-P': 0.005458913371797834,
 'log_base': 2.705742041244089,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3330148443,
 'sample_weights': [10.985534154547876, 1.3732446701796674],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.373644051292104,
 'weight_decay_Hydroxylation-K': 9.527569056122175,
 'weight_decay_Hydroxylation-P': 9.922046757225052}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.588
[2,     1] loss: 1264.130
[3,     1] loss: 1265.930
[4,     1] loss: 1261.337
[5,     1] loss: 1258.978
[6,     1] loss: 1259.045
[7,     1] loss: 1261.227
[8,     1] loss: 1259.180
[9,     1] loss: 1259.548
[10,     1] loss: 1253.948
[11,     1] loss: 1249.313
[12,     1] loss: 1241.619
[13,     1] loss: 1220.107
[14,     1] loss: 1196.701
[15,     1] loss: 1165.257
[16,     1] loss: 1133.830
[17,     1] loss: 1107.538
[18,     1] loss: 1104.967
[19,     1] loss: 1083.111
[20,     1] loss: 1022.165
[21,     1] loss: 1031.874
[22,     1] loss: 1054.962
[23,     1] loss: 986.142
[24,     1] loss: 1028.379
[25,     1] loss: 942.797
[26,     1] loss: 997.676
[27,     1] loss: 942.774
[28,     1] loss: 994.875
[29,     1] loss: 927.000
[30,     1] loss: 946.575
[31,     1] loss: 930.901
[32,     1] loss: 958.659
[33,     1] loss: 924.817
[34,     1] loss: 937.900
[35,     1] loss: 887.224
[36,     1] loss: 858.858
[37,     1] loss: 885.655
[38,     1] loss: 887.288
[39,     1] loss: 872.388
[40,     1] loss: 849.222
[41,     1] loss: 862.738
[42,     1] loss: 841.595
[43,     1] loss: 830.526
[44,     1] loss: 772.598
[45,     1] loss: 1025.487
[46,     1] loss: 1255.623
[47,     1] loss: 833.035
[48,     1] loss: 952.764
[49,     1] loss: 974.127
[50,     1] loss: 940.255
[51,     1] loss: 932.391
[52,     1] loss: 955.453
[53,     1] loss: 930.633
[54,     1] loss: 904.701
[55,     1] loss: 941.931
[56,     1] loss: 902.737
[57,     1] loss: 827.516
[58,     1] loss: 922.488
[59,     1] loss: 841.799
[60,     1] loss: 844.697
[61,     1] loss: 847.122
[62,     1] loss: 840.542
[63,     1] loss: 817.264
[64,     1] loss: 831.210
[65,     1] loss: 801.259
[66,     1] loss: 774.761
[67,     1] loss: 757.083
[68,     1] loss: 774.366
[69,     1] loss: 728.669
[70,     1] loss: 917.650
[71,     1] loss: 1042.756
[72,     1] loss: 779.148
[73,     1] loss: 895.197
[74,     1] loss: 855.341
[75,     1] loss: 835.631
[76,     1] loss: 863.875
[77,     1] loss: 754.639
[78,     1] loss: 838.588
[79,     1] loss: 720.006
[80,     1] loss: 817.859
[81,     1] loss: 748.850
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013411978349599984,
 'learning_rate_Hydroxylation-K': 0.0006037641297586753,
 'learning_rate_Hydroxylation-P': 0.009221875031728078,
 'log_base': 2.3695384285956105,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3617469560,
 'sample_weights': [1.6771981818014825, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.313105383929061,
 'weight_decay_Hydroxylation-K': 7.1397267871156,
 'weight_decay_Hydroxylation-P': 6.555055972211259}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1320.115
[2,     1] loss: 1319.591
[3,     1] loss: 1315.813
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010578085981509666,
 'learning_rate_Hydroxylation-K': 0.007009746850613512,
 'learning_rate_Hydroxylation-P': 0.006603946355042033,
 'log_base': 1.0602387255426273,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1213472623,
 'sample_weights': [1.9351483415246564, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.167110634021538,
 'weight_decay_Hydroxylation-K': 6.767296453259261,
 'weight_decay_Hydroxylation-P': 0.5787500410901949}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9264.573
[2,     1] loss: 9263.626
[3,     1] loss: 9267.455
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002760334671383273,
 'learning_rate_Hydroxylation-K': 0.008133627929676158,
 'learning_rate_Hydroxylation-P': 0.004204215948970929,
 'log_base': 1.018713640855353,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 927042544,
 'sample_weights': [28.540370304274905, 3.5676837242433206],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.689813882443586,
 'weight_decay_Hydroxylation-K': 8.163875002204222,
 'weight_decay_Hydroxylation-P': 1.2629714426920826}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29281.068
Exploding loss, terminate run (best metric=0.532756507396698)
Finished Training
Total time taken: 0.19499969482421875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29320.127
Exploding loss, terminate run (best metric=0.5342393517494202)
Finished Training
Total time taken: 0.205003023147583
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29269.490
Exploding loss, terminate run (best metric=0.5289319753646851)
Finished Training
Total time taken: 0.19700098037719727
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29256.289
Exploding loss, terminate run (best metric=0.5274335145950317)
Finished Training
Total time taken: 0.2030012607574463
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29408.059
Exploding loss, terminate run (best metric=0.5281369090080261)
Finished Training
Total time taken: 0.1999971866607666
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29332.582
Exploding loss, terminate run (best metric=0.533727765083313)
Finished Training
Total time taken: 0.21400094032287598
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29351.389
Exploding loss, terminate run (best metric=0.5291651487350464)
Finished Training
Total time taken: 0.21700048446655273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29125.926
Exploding loss, terminate run (best metric=0.5314756631851196)
Finished Training
Total time taken: 0.2200005054473877
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29226.428
Exploding loss, terminate run (best metric=0.5357908010482788)
Finished Training
Total time taken: 0.2180004119873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29362.555
Exploding loss, terminate run (best metric=0.5281386971473694)
Finished Training
Total time taken: 0.21799969673156738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29237.613
Exploding loss, terminate run (best metric=0.5336518883705139)
Finished Training
Total time taken: 0.2199997901916504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29276.439
Exploding loss, terminate run (best metric=0.5296959280967712)
Finished Training
Total time taken: 0.22600078582763672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29161.875
Exploding loss, terminate run (best metric=0.5320227742195129)
Finished Training
Total time taken: 0.21199870109558105
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 29281.805
Exploding loss, terminate run (best metric=0.528268575668335)
Finished Training
Total time taken: 0.20800042152404785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 29227.031
Exploding loss, terminate run (best metric=0.5292652249336243)
Finished Training
Total time taken: 0.21000051498413086
{'Hydroxylation-K Validation Accuracy': 0.5163416075650118, 'Hydroxylation-K Validation Sensitivity': 0.48, 'Hydroxylation-K Validation Specificity': 0.531578947368421, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6514619883040936, 'Hydroxylation-K AUC PR': 0.37955887736961164, 'Hydroxylation-K MCC': 0.0194245360342249, 'Hydroxylation-K F1': 0.17257079480309973, 'Validation Loss (Hydroxylation-K)': 0.5557786424954733, 'Hydroxylation-P Validation Accuracy': 0.5225160144155119, 'Hydroxylation-P Validation Sensitivity': 0.4685714285714286, 'Hydroxylation-P Validation Specificity': 0.5341463414634147, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6541965874084683, 'Hydroxylation-P AUC PR': 0.35079731108158363, 'Hydroxylation-P MCC': 0.008773504576916964, 'Hydroxylation-P F1': 0.14703854863394394, 'Validation Loss (Hydroxylation-P)': 0.5308467149734497, 'Validation Loss (total)': 1.086625369389852, 'TimeToTrain': 0.2108669598897298}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008230740933507747,
 'learning_rate_Hydroxylation-K': 0.004610833032888121,
 'learning_rate_Hydroxylation-P': 0.006464247234335531,
 'log_base': 2.9580834832223113,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3684229539,
 'sample_weights': [90.10889503594039, 11.240208084065321],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.768521995385517,
 'weight_decay_Hydroxylation-K': 4.512919503469865,
 'weight_decay_Hydroxylation-P': 0.017386600363230587}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.726
[2,     1] loss: 1235.984
[3,     1] loss: 1234.810
[4,     1] loss: 1233.952
[5,     1] loss: 1233.802
[6,     1] loss: 1231.823
[7,     1] loss: 1231.792
[8,     1] loss: 1232.426
[9,     1] loss: 1233.594
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006555128071970583,
 'learning_rate_Hydroxylation-K': 0.0062220876097277625,
 'learning_rate_Hydroxylation-P': 0.00046891376605654226,
 'log_base': 2.7447039069556145,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3933711866,
 'sample_weights': [1.5393076372706163, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4790299977665393,
 'weight_decay_Hydroxylation-K': 8.146840117188933,
 'weight_decay_Hydroxylation-P': 9.663894751167305}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.747
[2,     1] loss: 1261.549
[3,     1] loss: 1259.558
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027693671605180294,
 'learning_rate_Hydroxylation-K': 0.006520078788127176,
 'learning_rate_Hydroxylation-P': 0.0020792884506586843,
 'log_base': 1.5075586366379479,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4162253105,
 'sample_weights': [1.653448999887027, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.973048089231066,
 'weight_decay_Hydroxylation-K': 9.43290788743782,
 'weight_decay_Hydroxylation-P': 3.634642141762168}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1779.502
[2,     1] loss: 1766.704
[3,     1] loss: 1766.942
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009099261262185023,
 'learning_rate_Hydroxylation-K': 0.007686837608152282,
 'learning_rate_Hydroxylation-P': 0.008441547894209029,
 'log_base': 1.719861318581725,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1326514793,
 'sample_weights': [4.066936739233978, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.825849494001558,
 'weight_decay_Hydroxylation-K': 7.854078659889431,
 'weight_decay_Hydroxylation-P': 5.457715714439138}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1554.500
[2,     1] loss: 1560.250
[3,     1] loss: 1563.587
[4,     1] loss: 1557.164
[5,     1] loss: 1560.174
[6,     1] loss: 1564.283
[7,     1] loss: 1558.699
[8,     1] loss: 1555.247
[9,     1] loss: 1557.793
[10,     1] loss: 1556.024
[11,     1] loss: 1557.216
[12,     1] loss: 1556.042
[13,     1] loss: 1557.364
[14,     1] loss: 1555.382
[15,     1] loss: 1556.398
[16,     1] loss: 1555.825
[17,     1] loss: 1556.316
[18,     1] loss: 1556.474
[19,     1] loss: 1557.404
[20,     1] loss: 1556.041
[21,     1] loss: 1555.397
[22,     1] loss: 1555.923
[23,     1] loss: 1556.000
[24,     1] loss: 1554.707
[25,     1] loss: 1551.750
[26,     1] loss: 1547.220
[27,     1] loss: 1541.724
[28,     1] loss: 1526.601
[29,     1] loss: 1493.898
[30,     1] loss: 1459.013
[31,     1] loss: 1416.069
[32,     1] loss: 1362.128
[33,     1] loss: 1504.700
[34,     1] loss: 1331.353
[35,     1] loss: 1294.920
[36,     1] loss: 1307.992
[37,     1] loss: 1254.034
[38,     1] loss: 1410.970
[39,     1] loss: 1308.862
[40,     1] loss: 1285.557
[41,     1] loss: 1250.868
[42,     1] loss: 1268.766
[43,     1] loss: 1245.032
[44,     1] loss: 1194.319
[45,     1] loss: 1207.241
[46,     1] loss: 1153.107
[47,     1] loss: 1145.787
[48,     1] loss: 1158.876
[49,     1] loss: 1075.972
[50,     1] loss: 1191.792
[51,     1] loss: 1205.985
[52,     1] loss: 1259.128
[53,     1] loss: 1109.199
[54,     1] loss: 1184.264
[55,     1] loss: 1105.807
[56,     1] loss: 1250.808
[57,     1] loss: 1151.782
[58,     1] loss: 1112.620
[59,     1] loss: 1095.594
[60,     1] loss: 1117.467
[61,     1] loss: 1101.233
[62,     1] loss: 1077.668
[63,     1] loss: 1089.375
[64,     1] loss: 1149.366
[65,     1] loss: 1050.639
[66,     1] loss: 1072.512
[67,     1] loss: 1148.354
[68,     1] loss: 1035.549
[69,     1] loss: 1011.855
[70,     1] loss: 1058.900
[71,     1] loss: 1036.263
[72,     1] loss: 1005.937
[73,     1] loss: 974.060
[74,     1] loss: 957.435
[75,     1] loss: 1007.560
[76,     1] loss: 2351.885
[77,     1] loss: 1037.518
[78,     1] loss: 1446.873
[79,     1] loss: 1400.288
[80,     1] loss: 1501.138
[81,     1] loss: 1519.015
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009727089621686377,
 'learning_rate_Hydroxylation-K': 0.009946121448229952,
 'learning_rate_Hydroxylation-P': 0.004160514534051991,
 'log_base': 1.091094014512733,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2265720733,
 'sample_weights': [3.0787693321954026, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.194014394124892,
 'weight_decay_Hydroxylation-K': 7.031641442906983,
 'weight_decay_Hydroxylation-P': 6.987870485016792}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6228.838
[2,     1] loss: 6202.485
[3,     1] loss: 6137.317
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001473141812785075,
 'learning_rate_Hydroxylation-K': 0.009616055937303008,
 'learning_rate_Hydroxylation-P': 0.0019541978588372357,
 'log_base': 1.079860027772735,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3522640720,
 'sample_weights': [19.14918988445938, 2.3937409485187215],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.936334821651752,
 'weight_decay_Hydroxylation-K': 6.607334348964617,
 'weight_decay_Hydroxylation-P': 0.9557197064155685}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7031.950
[2,     1] loss: 7015.742
[3,     1] loss: 7056.024
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004990568014865595,
 'learning_rate_Hydroxylation-K': 0.0015821039651819848,
 'learning_rate_Hydroxylation-P': 9.651017023689181e-05,
 'log_base': 1.5818096442715865,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 782244327,
 'sample_weights': [21.72864897874765, 2.7161857566950633],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.342012506171772,
 'weight_decay_Hydroxylation-K': 9.353942570930863,
 'weight_decay_Hydroxylation-P': 9.971578818644868}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1669.479
[2,     1] loss: 1681.140
[3,     1] loss: 1689.276
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004102714236872294,
 'learning_rate_Hydroxylation-K': 0.005253088776100718,
 'learning_rate_Hydroxylation-P': 0.005310416916363711,
 'log_base': 1.4180251918370488,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 28609454,
 'sample_weights': [3.640545252074011, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.383973983204847,
 'weight_decay_Hydroxylation-K': 8.288760652509776,
 'weight_decay_Hydroxylation-P': 1.7279868592905803}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1911.818
[2,     1] loss: 1938.018
[3,     1] loss: 1912.939
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009737701702055339,
 'learning_rate_Hydroxylation-K': 0.0071063029810239445,
 'learning_rate_Hydroxylation-P': 0.0044514655313610775,
 'log_base': 1.330995353948589,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2479680692,
 'sample_weights': [4.779872651253954, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1823014395699205,
 'weight_decay_Hydroxylation-K': 4.854628000638073,
 'weight_decay_Hydroxylation-P': 2.525433125193918}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2144.393
[2,     1] loss: 2138.464
[3,     1] loss: 2128.906
[4,     1] loss: 2150.568
[5,     1] loss: 2145.693
[6,     1] loss: 2137.868
[7,     1] loss: 2131.881
[8,     1] loss: 2136.926
[9,     1] loss: 2142.791
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002719181826470839,
 'learning_rate_Hydroxylation-K': 0.007783612332643077,
 'learning_rate_Hydroxylation-P': 4.8060498633046925e-05,
 'log_base': 1.3569828963965787,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3861365024,
 'sample_weights': [5.8387031050943445, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.787157482802407,
 'weight_decay_Hydroxylation-K': 1.0416183700177317,
 'weight_decay_Hydroxylation-P': 0.5495507029993851}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2069.353
[2,     1] loss: 2060.951
[3,     1] loss: 2055.669
[4,     1] loss: 2059.362
[5,     1] loss: 2059.783
[6,     1] loss: 2062.643
[7,     1] loss: 2059.348
[8,     1] loss: 2055.755
[9,     1] loss: 2058.462
[10,     1] loss: 2050.781
[11,     1] loss: 2058.787
[12,     1] loss: 2048.301
[13,     1] loss: 2055.228
[14,     1] loss: 2052.012
[15,     1] loss: 2029.031
[16,     1] loss: 2020.585
[17,     1] loss: 2008.335
[18,     1] loss: 1988.016
[19,     1] loss: 1976.895
[20,     1] loss: 1936.671
[21,     1] loss: 1882.372
[22,     1] loss: 1871.871
[23,     1] loss: 1872.719
[24,     1] loss: 1847.928
[25,     1] loss: 1829.328
[26,     1] loss: 1730.995
[27,     1] loss: 1721.841
[28,     1] loss: 1677.440
[29,     1] loss: 1788.587
[30,     1] loss: 1666.914
[31,     1] loss: 1634.568
[32,     1] loss: 1612.787
[33,     1] loss: 1661.573
[34,     1] loss: 1603.133
[35,     1] loss: 1548.073
[36,     1] loss: 1539.321
[37,     1] loss: 1471.319
[38,     1] loss: 1546.528
[39,     1] loss: 1528.653
[40,     1] loss: 1527.963
[41,     1] loss: 1648.224
[42,     1] loss: 1554.945
[43,     1] loss: 1456.758
[44,     1] loss: 1360.390
[45,     1] loss: 1374.934
[46,     1] loss: 1464.217
[47,     1] loss: 1365.219
[48,     1] loss: 1361.860
[49,     1] loss: 1434.337
[50,     1] loss: 1331.495
[51,     1] loss: 1222.298
[52,     1] loss: 1301.191
[53,     1] loss: 1229.768
[54,     1] loss: 1342.424
[55,     1] loss: 1244.142
[56,     1] loss: 1337.266
[57,     1] loss: 1180.575
[58,     1] loss: 1355.863
[59,     1] loss: 1149.799
[60,     1] loss: 1226.283
[61,     1] loss: 1150.897
[62,     1] loss: 1219.430
[63,     1] loss: 1100.772
[64,     1] loss: 1248.576
[65,     1] loss: 1033.701
[66,     1] loss: 1062.734
[67,     1] loss: 1066.764
[68,     1] loss: 1121.445
[69,     1] loss: 964.831
[70,     1] loss: 1025.318
[71,     1] loss: 1018.031
[72,     1] loss: 979.065
[73,     1] loss: 1063.712
[74,     1] loss: 991.686
[75,     1] loss: 1104.384
[76,     1] loss: 1022.829
[77,     1] loss: 952.695
[78,     1] loss: 902.830
[79,     1] loss: 898.617
[80,     1] loss: 985.779
[81,     1] loss: 853.748
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0073231375819380155,
 'learning_rate_Hydroxylation-K': 0.0017670857236531112,
 'learning_rate_Hydroxylation-P': 0.0065455870880418,
 'log_base': 2.987417422412699,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2960207276,
 'sample_weights': [5.468854395274026, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.0817879351079,
 'weight_decay_Hydroxylation-K': 5.376083235925442,
 'weight_decay_Hydroxylation-P': 5.122058752279563}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1226.848
[2,     1] loss: 1234.857
[3,     1] loss: 1238.681
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007153471152445553,
 'learning_rate_Hydroxylation-K': 0.008521561408881027,
 'learning_rate_Hydroxylation-P': 0.0067035331820008695,
 'log_base': 2.1108096427954592,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2317586172,
 'sample_weights': [1.5254285433015475, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.218987464672736,
 'weight_decay_Hydroxylation-K': 1.0398798865436543,
 'weight_decay_Hydroxylation-P': 6.9359803946471565}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1384.023
[2,     1] loss: 1392.337
[3,     1] loss: 1381.370
[4,     1] loss: 1377.707
[5,     1] loss: 1376.165
[6,     1] loss: 1376.430
[7,     1] loss: 1377.978
[8,     1] loss: 1378.532
[9,     1] loss: 1377.755
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038340352158376364,
 'learning_rate_Hydroxylation-K': 0.0071103058945893045,
 'learning_rate_Hydroxylation-P': 0.0047405821837412775,
 'log_base': 1.1074830493128212,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1559943518,
 'sample_weights': [2.234649487232518, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.798255864304707,
 'weight_decay_Hydroxylation-K': 9.279027312727784,
 'weight_decay_Hydroxylation-P': 1.6005016990644263}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5295.676
[2,     1] loss: 5322.179
[3,     1] loss: 5302.224
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017342967593940073,
 'learning_rate_Hydroxylation-K': 0.009734102832173654,
 'learning_rate_Hydroxylation-P': 0.006135159215213364,
 'log_base': 1.143547810326364,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3670011375,
 'sample_weights': [16.35267408352854, 2.044163006775064],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.737005903505377,
 'weight_decay_Hydroxylation-K': 7.681011735971175,
 'weight_decay_Hydroxylation-P': 0.299253115936708}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4047.282
[2,     1] loss: 4033.197
[3,     1] loss: 4031.531
[4,     1] loss: 4038.212
[5,     1] loss: 4029.454
[6,     1] loss: 4012.280
[7,     1] loss: 4015.679
[8,     1] loss: 4013.533
[9,     1] loss: 3989.996
[10,     1] loss: 3968.834
[11,     1] loss: 3969.184
[12,     1] loss: 3958.717
[13,     1] loss: 3919.729
[14,     1] loss: 3880.404
[15,     1] loss: 3839.292
[16,     1] loss: 3699.762
[17,     1] loss: 3610.593
[18,     1] loss: 3597.370
[19,     1] loss: 3551.897
[20,     1] loss: 3472.676
[21,     1] loss: 3403.944
[22,     1] loss: 3261.413
[23,     1] loss: 3191.541
[24,     1] loss: 3338.234
[25,     1] loss: 3133.747
[26,     1] loss: 3398.512
[27,     1] loss: 3265.908
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030946347099973463,
 'learning_rate_Hydroxylation-K': 0.0067466397671138,
 'learning_rate_Hydroxylation-P': 0.004627794814428603,
 'log_base': 1.0304880025474925,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1776570516,
 'sample_weights': [12.445941590863223, 1.5558026323139484],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.576475988920021,
 'weight_decay_Hydroxylation-K': 7.99227972259332,
 'weight_decay_Hydroxylation-P': 4.223009603726899}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18018.836
Exploding loss, terminate run (best metric=0.5381246209144592)
Finished Training
Total time taken: 0.20800280570983887
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18103.111
Exploding loss, terminate run (best metric=0.5286387205123901)
Finished Training
Total time taken: 0.2160015106201172
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18055.605
Exploding loss, terminate run (best metric=0.52763432264328)
Finished Training
Total time taken: 0.20800042152404785
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17918.443
Exploding loss, terminate run (best metric=0.5267351865768433)
Finished Training
Total time taken: 0.23103690147399902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17992.195
Exploding loss, terminate run (best metric=0.533170223236084)
Finished Training
Total time taken: 0.21575498580932617
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18004.102
Exploding loss, terminate run (best metric=0.5382642149925232)
Finished Training
Total time taken: 0.21699953079223633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18177.891
Exploding loss, terminate run (best metric=0.5281339287757874)
Finished Training
Total time taken: 0.2070014476776123
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18096.338
Exploding loss, terminate run (best metric=0.5274726152420044)
Finished Training
Total time taken: 0.22103476524353027
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17992.500
Exploding loss, terminate run (best metric=0.533371090888977)
Finished Training
Total time taken: 0.21152257919311523
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18033.967
Exploding loss, terminate run (best metric=0.5301784873008728)
Finished Training
Total time taken: 0.21199941635131836
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18184.602
Exploding loss, terminate run (best metric=0.5319375395774841)
Finished Training
Total time taken: 0.21300196647644043
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18066.705
Exploding loss, terminate run (best metric=0.5277095437049866)
Finished Training
Total time taken: 0.21399998664855957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18036.973
Exploding loss, terminate run (best metric=0.5328035354614258)
Finished Training
Total time taken: 0.21500062942504883
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18128.529
Exploding loss, terminate run (best metric=0.5314228534698486)
Finished Training
Total time taken: 0.21100068092346191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18053.396
Exploding loss, terminate run (best metric=0.52976393699646)
Finished Training
Total time taken: 0.2070026397705078
{'Hydroxylation-K Validation Accuracy': 0.4828014184397163, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6933528265107213, 'Hydroxylation-K AUC PR': 0.381544032465664, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1807881773399015, 'Validation Loss (Hydroxylation-K)': 0.5564403851826986, 'Hydroxylation-P Validation Accuracy': 0.4780580342791398, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6015170491027142, 'Hydroxylation-P AUC PR': 0.30211341310375495, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.16002571501225754, 'Validation Loss (Hydroxylation-P)': 0.5310240546862285, 'Validation Loss (total)': 1.087464459737142, 'TimeToTrain': 0.21382401784261068}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002311611144850883,
 'learning_rate_Hydroxylation-K': 0.007302455902837062,
 'learning_rate_Hydroxylation-P': 0.0038920848511962277,
 'log_base': 1.0890920146330558,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2038734891,
 'sample_weights': [55.62915875576021, 6.939196399055294],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.360845291120153,
 'weight_decay_Hydroxylation-K': 7.483897985689872,
 'weight_decay_Hydroxylation-P': 2.575836979776814}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6316.590
[2,     1] loss: 6434.877
[3,     1] loss: 6360.485
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006886445916149246,
 'learning_rate_Hydroxylation-K': 0.0011792792039989146,
 'learning_rate_Hydroxylation-P': 0.005848742600142558,
 'log_base': 2.9833635516389903,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2137719109,
 'sample_weights': [19.561264933906905, 2.4452523140478677],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.813470512010465,
 'weight_decay_Hydroxylation-K': 1.6401120928589297,
 'weight_decay_Hydroxylation-P': 9.583550284314018}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.219
[2,     1] loss: 1239.496
[3,     1] loss: 1228.289
[4,     1] loss: 1231.649
[5,     1] loss: 1227.640
[6,     1] loss: 1227.927
[7,     1] loss: 1222.285
[8,     1] loss: 1210.427
[9,     1] loss: 1191.775
[10,     1] loss: 1161.953
[11,     1] loss: 1138.365
[12,     1] loss: 1103.370
[13,     1] loss: 1079.034
[14,     1] loss: 1127.191
[15,     1] loss: 1052.644
[16,     1] loss: 1085.627
[17,     1] loss: 1048.576
[18,     1] loss: 1039.124
[19,     1] loss: 1012.153
[20,     1] loss: 967.515
[21,     1] loss: 1022.538
[22,     1] loss: 968.829
[23,     1] loss: 1005.506
[24,     1] loss: 968.603
[25,     1] loss: 984.471
[26,     1] loss: 895.149
[27,     1] loss: 899.405
[28,     1] loss: 909.978
[29,     1] loss: 931.768
[30,     1] loss: 917.189
[31,     1] loss: 862.491
[32,     1] loss: 887.437
[33,     1] loss: 911.327
[34,     1] loss: 838.484
[35,     1] loss: 926.835
[36,     1] loss: 938.679
[37,     1] loss: 843.645
[38,     1] loss: 862.772
[39,     1] loss: 849.800
[40,     1] loss: 828.250
[41,     1] loss: 836.604
[42,     1] loss: 832.878
[43,     1] loss: 788.904
[44,     1] loss: 891.832
[45,     1] loss: 857.706
[46,     1] loss: 759.317
[47,     1] loss: 736.054
[48,     1] loss: 743.198
[49,     1] loss: 750.723
[50,     1] loss: 733.427
[51,     1] loss: 759.172
[52,     1] loss: 861.523
[53,     1] loss: 1144.919
[54,     1] loss: 830.334
[55,     1] loss: 923.341
[56,     1] loss: 901.987
[57,     1] loss: 856.609
[58,     1] loss: 914.529
[59,     1] loss: 834.084
[60,     1] loss: 864.910
[61,     1] loss: 845.147
[62,     1] loss: 801.228
[63,     1] loss: 815.910
[64,     1] loss: 789.637
[65,     1] loss: 796.372
[66,     1] loss: 778.354
[67,     1] loss: 757.123
[68,     1] loss: 728.743
[69,     1] loss: 708.460
[70,     1] loss: 706.888
[71,     1] loss: 655.522
[72,     1] loss: 698.073
[73,     1] loss: 658.857
[74,     1] loss: 834.191
[75,     1] loss: 1638.253
[76,     1] loss: 833.909
[77,     1] loss: 964.129
[78,     1] loss: 1069.152
[79,     1] loss: 1028.358
[80,     1] loss: 1045.115
[81,     1] loss: 1039.563
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007068659736251749,
 'learning_rate_Hydroxylation-K': 0.009331569915031571,
 'learning_rate_Hydroxylation-P': 0.0072453666443141294,
 'log_base': 1.0706541635331364,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4082584854,
 'sample_weights': [1.5273235908755738, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.613718960253499,
 'weight_decay_Hydroxylation-K': 4.904113830954846,
 'weight_decay_Hydroxylation-P': 6.9955874424262445}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7933.655
[2,     1] loss: 7948.486
[3,     1] loss: 7933.587
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006053374960610046,
 'learning_rate_Hydroxylation-K': 0.0005911023266120043,
 'learning_rate_Hydroxylation-P': 0.007061020936164878,
 'log_base': 2.8252075670730505,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1497095926,
 'sample_weights': [24.45360067263303, 3.0568178404409414],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.877883976276275,
 'weight_decay_Hydroxylation-K': 4.470482115576148,
 'weight_decay_Hydroxylation-P': 0.19911206042838092}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.758
[2,     1] loss: 1249.766
[3,     1] loss: 1247.119
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006570645987009208,
 'learning_rate_Hydroxylation-K': 0.009914752423494124,
 'learning_rate_Hydroxylation-P': 0.007470284810475988,
 'log_base': 1.9583157792207633,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1657826251,
 'sample_weights': [1.6074257107126169, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3321932941893517,
 'weight_decay_Hydroxylation-K': 5.376715610613821,
 'weight_decay_Hydroxylation-P': 4.094752133070494}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1435.032
[2,     1] loss: 1436.791
[3,     1] loss: 1432.652
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0012230742348429868,
 'learning_rate_Hydroxylation-K': 0.003475145696015666,
 'learning_rate_Hydroxylation-P': 0.005155570449991831,
 'log_base': 2.77886605386109,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2084589533,
 'sample_weights': [2.483976915925976, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.651274440975785,
 'weight_decay_Hydroxylation-K': 9.727693803235598,
 'weight_decay_Hydroxylation-P': 1.012654921708928}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.702
[2,     1] loss: 1252.408
[3,     1] loss: 1253.849
[4,     1] loss: 1249.074
[5,     1] loss: 1248.009
[6,     1] loss: 1249.021
[7,     1] loss: 1244.540
[8,     1] loss: 1245.834
[9,     1] loss: 1237.261
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001481356944822519,
 'learning_rate_Hydroxylation-K': 0.0006002859208253434,
 'learning_rate_Hydroxylation-P': 0.00415895253600462,
 'log_base': 2.9489123175371827,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4180397956,
 'sample_weights': [1.6334373688529966, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.351574774469052,
 'weight_decay_Hydroxylation-K': 6.078790301280298,
 'weight_decay_Hydroxylation-P': 2.0839293698043897}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.736
[2,     1] loss: 1236.633
[3,     1] loss: 1232.231
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005751961745586498,
 'learning_rate_Hydroxylation-K': 0.0005337939691651234,
 'learning_rate_Hydroxylation-P': 0.006603218370436403,
 'log_base': 2.8133851701776105,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3255338078,
 'sample_weights': [1.5437275395151395, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.05241115018976,
 'weight_decay_Hydroxylation-K': 9.813791052444305,
 'weight_decay_Hydroxylation-P': 1.3340479861817476}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.991
[2,     1] loss: 1251.161
[3,     1] loss: 1247.040
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037863841035537435,
 'learning_rate_Hydroxylation-K': 0.004655471479798718,
 'learning_rate_Hydroxylation-P': 0.005172434692643057,
 'log_base': 2.36753350169303,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3332755467,
 'sample_weights': [1.6139421850553404, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.649230573430168,
 'weight_decay_Hydroxylation-K': 9.867293667432527,
 'weight_decay_Hydroxylation-P': 4.812287600136582}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.656
[2,     1] loss: 1317.491
[3,     1] loss: 1312.504
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005523283734638038,
 'learning_rate_Hydroxylation-K': 0.006706006942620164,
 'learning_rate_Hydroxylation-P': 0.0013304533600371625,
 'log_base': 1.4694616542147556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3420467651,
 'sample_weights': [1.9370489900740033, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.328901140980626,
 'weight_decay_Hydroxylation-K': 9.785734804335329,
 'weight_decay_Hydroxylation-P': 6.886626348391814}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1832.146
[2,     1] loss: 1825.109
[3,     1] loss: 1821.462
[4,     1] loss: 1814.960
[5,     1] loss: 1825.494
[6,     1] loss: 1826.013
[7,     1] loss: 1821.330
[8,     1] loss: 1824.234
[9,     1] loss: 1820.307
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030322546441244602,
 'learning_rate_Hydroxylation-K': 0.005608842613720783,
 'learning_rate_Hydroxylation-P': 0.007053044327889248,
 'log_base': 1.0582606928427116,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3141148737,
 'sample_weights': [4.337386362121531, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.009575984048538,
 'weight_decay_Hydroxylation-K': 7.9306207671105255,
 'weight_decay_Hydroxylation-P': 5.68790891661809}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9612.912
[2,     1] loss: 9586.690
[3,     1] loss: 9553.043
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021129106858390283,
 'learning_rate_Hydroxylation-K': 0.007199011029368697,
 'learning_rate_Hydroxylation-P': 0.0053503679273728955,
 'log_base': 1.1047699489070335,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1582037148,
 'sample_weights': [29.481552178053967, 3.685335990728855],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.9084643060309245,
 'weight_decay_Hydroxylation-K': 9.575183233525737,
 'weight_decay_Hydroxylation-P': 3.556155884398345}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5431.561
[2,     1] loss: 5461.358
[3,     1] loss: 5443.925
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004238932218722147,
 'learning_rate_Hydroxylation-K': 0.002169602152065531,
 'learning_rate_Hydroxylation-P': 0.0007608107803548029,
 'log_base': 2.0263564168731434,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3910105527,
 'sample_weights': [16.75523248310337, 2.094484745242677],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.816349160991905,
 'weight_decay_Hydroxylation-K': 9.689006662401441,
 'weight_decay_Hydroxylation-P': 6.7063782909082805}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.770
[2,     1] loss: 1408.272
[3,     1] loss: 1405.694
[4,     1] loss: 1404.975
[5,     1] loss: 1404.954
[6,     1] loss: 1407.244
[7,     1] loss: 1404.078
[8,     1] loss: 1397.343
[9,     1] loss: 1396.503
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004913251080048955,
 'learning_rate_Hydroxylation-K': 0.008265195714434693,
 'learning_rate_Hydroxylation-P': 0.004558234159340695,
 'log_base': 1.3161729817809442,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2452848896,
 'sample_weights': [2.3638490799116956, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.625000034459953,
 'weight_decay_Hydroxylation-K': 5.401114952232304,
 'weight_decay_Hydroxylation-P': 1.7662144934703428}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2191.246
[2,     1] loss: 2201.043
[3,     1] loss: 2191.273
[4,     1] loss: 2185.907
[5,     1] loss: 2195.753
[6,     1] loss: 2191.716
[7,     1] loss: 2184.075
[8,     1] loss: 2186.018
[9,     1] loss: 2188.656
[10,     1] loss: 2176.354
[11,     1] loss: 2176.220
[12,     1] loss: 2169.653
[13,     1] loss: 2170.106
[14,     1] loss: 2151.435
[15,     1] loss: 2114.034
[16,     1] loss: 2074.606
[17,     1] loss: 2039.493
[18,     1] loss: 1961.079
[19,     1] loss: 1888.972
[20,     1] loss: 1876.106
[21,     1] loss: 1791.014
[22,     1] loss: 1826.125
[23,     1] loss: 1914.533
[24,     1] loss: 1948.046
[25,     1] loss: 1862.200
[26,     1] loss: 1887.596
[27,     1] loss: 1834.475
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005385965585174043,
 'learning_rate_Hydroxylation-K': 0.004361243497763938,
 'learning_rate_Hydroxylation-P': 0.002923956600419498,
 'log_base': 2.372564377224057,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1146024975,
 'sample_weights': [6.07670681705618, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.30304470721083,
 'weight_decay_Hydroxylation-K': 8.226188557081912,
 'weight_decay_Hydroxylation-P': 0.7826699167845668}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1316.573
[2,     1] loss: 1319.741
[3,     1] loss: 1312.457
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002588888591202386,
 'learning_rate_Hydroxylation-K': 0.006770867787350958,
 'learning_rate_Hydroxylation-P': 0.0011881673023903731,
 'log_base': 2.492814662861755,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 978239451,
 'sample_weights': [1.9322898589884787, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.891037905195315,
 'weight_decay_Hydroxylation-K': 1.9550168770933851,
 'weight_decay_Hydroxylation-P': 7.92840424947479}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1295.550
[2,     1] loss: 1293.548
[3,     1] loss: 1292.300
[4,     1] loss: 1291.575
[5,     1] loss: 1287.694
[6,     1] loss: 1286.576
[7,     1] loss: 1282.856
[8,     1] loss: 1267.185
[9,     1] loss: 1249.764
[10,     1] loss: 1226.765
[11,     1] loss: 1179.859
[12,     1] loss: 1141.091
[13,     1] loss: 1119.804
[14,     1] loss: 1094.536
[15,     1] loss: 1076.323
[16,     1] loss: 1049.748
[17,     1] loss: 1023.253
[18,     1] loss: 1010.700
[19,     1] loss: 1037.194
[20,     1] loss: 972.175
[21,     1] loss: 983.175
[22,     1] loss: 979.567
[23,     1] loss: 957.878
[24,     1] loss: 960.575
[25,     1] loss: 999.009
[26,     1] loss: 951.263
[27,     1] loss: 993.650
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002481924838324201,
 'learning_rate_Hydroxylation-K': 0.0018898146626702703,
 'learning_rate_Hydroxylation-P': 0.00642827238593052,
 'log_base': 1.983890475295139,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4130899831,
 'sample_weights': [1.8276991203800543, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.673190748887783,
 'weight_decay_Hydroxylation-K': 5.539930012370654,
 'weight_decay_Hydroxylation-P': 5.871181288131192}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1426.383
[2,     1] loss: 1420.944
[3,     1] loss: 1419.920
[4,     1] loss: 1421.870
[5,     1] loss: 1416.695
[6,     1] loss: 1417.165
[7,     1] loss: 1413.030
[8,     1] loss: 1407.241
[9,     1] loss: 1398.823
[10,     1] loss: 1386.038
[11,     1] loss: 1361.617
[12,     1] loss: 1334.842
[13,     1] loss: 1311.154
[14,     1] loss: 1279.638
[15,     1] loss: 1237.890
[16,     1] loss: 1250.528
[17,     1] loss: 1221.894
[18,     1] loss: 1215.652
[19,     1] loss: 1218.616
[20,     1] loss: 1192.515
[21,     1] loss: 1176.101
[22,     1] loss: 1188.824
[23,     1] loss: 1171.742
[24,     1] loss: 1143.242
[25,     1] loss: 1174.259
[26,     1] loss: 1135.741
[27,     1] loss: 1130.192
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008481199636031847,
 'learning_rate_Hydroxylation-K': 0.007701616491888594,
 'learning_rate_Hydroxylation-P': 0.009925099255292731,
 'log_base': 1.4447673106822676,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2198237739,
 'sample_weights': [2.43693052672014, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.915773162111956,
 'weight_decay_Hydroxylation-K': 0.28696313507525417,
 'weight_decay_Hydroxylation-P': 8.466061568888348}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1876.968
[2,     1] loss: 1880.802
[3,     1] loss: 1887.520
[4,     1] loss: 1860.497
[5,     1] loss: 1864.371
[6,     1] loss: 1868.035
[7,     1] loss: 1863.070
[8,     1] loss: 1862.341
[9,     1] loss: 1873.554
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003339065042191041,
 'learning_rate_Hydroxylation-K': 0.0024111984011226805,
 'learning_rate_Hydroxylation-P': 0.004360565443623137,
 'log_base': 2.0984181128075114,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4177410157,
 'sample_weights': [4.537167986830504, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.097508844933202,
 'weight_decay_Hydroxylation-K': 6.399997961395396,
 'weight_decay_Hydroxylation-P': 2.295632927438251}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.828
[2,     1] loss: 1383.549
[3,     1] loss: 1387.553
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004757698736822237,
 'learning_rate_Hydroxylation-K': 0.004036511323404979,
 'learning_rate_Hydroxylation-P': 0.005098734456742535,
 'log_base': 1.5180944616586,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3628940647,
 'sample_weights': [2.2524010773604277, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.708363152472511,
 'weight_decay_Hydroxylation-K': 9.810809910475523,
 'weight_decay_Hydroxylation-P': 9.981461742700544}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1759.082
[2,     1] loss: 1752.275
[3,     1] loss: 1750.591
[4,     1] loss: 1754.609
[5,     1] loss: 1753.412
[6,     1] loss: 1749.756
[7,     1] loss: 1750.606
[8,     1] loss: 1746.725
[9,     1] loss: 1733.420
[10,     1] loss: 1726.784
[11,     1] loss: 1711.228
[12,     1] loss: 1675.595
[13,     1] loss: 1646.113
[14,     1] loss: 1632.636
[15,     1] loss: 1577.737
[16,     1] loss: 1539.622
[17,     1] loss: 1527.601
[18,     1] loss: 1539.584
[19,     1] loss: 1478.760
[20,     1] loss: 1543.924
[21,     1] loss: 1435.846
[22,     1] loss: 1406.865
[23,     1] loss: 1466.864
[24,     1] loss: 1443.548
[25,     1] loss: 1437.297
[26,     1] loss: 1437.641
[27,     1] loss: 1409.351
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008232534351532062,
 'learning_rate_Hydroxylation-K': 0.007725093336249876,
 'learning_rate_Hydroxylation-P': 0.007497744545591622,
 'log_base': 1.5851362742848498,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1637098964,
 'sample_weights': [3.9990885946327417, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6905406820857216,
 'weight_decay_Hydroxylation-K': 5.464774707511743,
 'weight_decay_Hydroxylation-P': 9.818335615974284}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1677.785
[2,     1] loss: 1681.243
[3,     1] loss: 1674.346
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004125016708764938,
 'learning_rate_Hydroxylation-K': 0.0074112801689894545,
 'learning_rate_Hydroxylation-P': 0.005638501075307616,
 'log_base': 1.4736869800063632,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1017580530,
 'sample_weights': [3.623942879584962, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.99155824471229,
 'weight_decay_Hydroxylation-K': 4.67598476553776,
 'weight_decay_Hydroxylation-P': 4.496210729031296}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1819.374
[2,     1] loss: 1811.117
[3,     1] loss: 1821.114
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009421810842775009,
 'learning_rate_Hydroxylation-K': 0.008040666923957407,
 'learning_rate_Hydroxylation-P': 0.0051184979964360965,
 'log_base': 1.576091933944902,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3788796327,
 'sample_weights': [4.305269352431859, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.195573622725437,
 'weight_decay_Hydroxylation-K': 5.871629855087038,
 'weight_decay_Hydroxylation-P': 5.245521811672025}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1684.745
[2,     1] loss: 1677.918
[3,     1] loss: 1687.072
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008907774740124488,
 'learning_rate_Hydroxylation-K': 0.00493496631276605,
 'learning_rate_Hydroxylation-P': 0.0036039221857731515,
 'log_base': 1.2461563322341498,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1985610119,
 'sample_weights': [3.6695225838899805, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.769014634710397,
 'weight_decay_Hydroxylation-K': 9.913368136441438,
 'weight_decay_Hydroxylation-P': 4.713524584297448}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2523.198
[2,     1] loss: 2517.407
[3,     1] loss: 2512.963
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008968456179448029,
 'learning_rate_Hydroxylation-K': 0.00909479535130109,
 'learning_rate_Hydroxylation-P': 0.009054180982312952,
 'log_base': 1.0833556682982897,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1923110665,
 'sample_weights': [7.586175201088934, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2879728753218633,
 'weight_decay_Hydroxylation-K': 8.842395075485877,
 'weight_decay_Hydroxylation-P': 4.768836179219351}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6755.216
[2,     1] loss: 6833.683
[3,     1] loss: 6751.600
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061670476305379215,
 'learning_rate_Hydroxylation-K': 0.009178660735878152,
 'learning_rate_Hydroxylation-P': 0.009912410270847442,
 'log_base': 1.1877132099627543,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 732157108,
 'sample_weights': [20.851534219205664, 2.606542188004477],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.451964010829235,
 'weight_decay_Hydroxylation-K': 8.014901784136308,
 'weight_decay_Hydroxylation-P': 4.105795722545151}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3152.547
[2,     1] loss: 3149.516
[3,     1] loss: 3164.031
[4,     1] loss: 3153.883
[5,     1] loss: 3145.360
[6,     1] loss: 3150.791
[7,     1] loss: 3165.521
[8,     1] loss: 3143.868
[9,     1] loss: 3142.465
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004863328509320128,
 'learning_rate_Hydroxylation-K': 0.0025643703098716104,
 'learning_rate_Hydroxylation-P': 0.0038036203659142912,
 'log_base': 1.7657488479709378,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2247347643,
 'sample_weights': [9.704384257123742, 1.2130947636218166],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0167363242858034,
 'weight_decay_Hydroxylation-K': 5.291808001629372,
 'weight_decay_Hydroxylation-P': 3.1870978636517973}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1533.246
[2,     1] loss: 1531.578
[3,     1] loss: 1528.483
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005175501111944629,
 'learning_rate_Hydroxylation-K': 0.002060101344265602,
 'learning_rate_Hydroxylation-P': 0.0002157844361979095,
 'log_base': 2.610260552181084,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 57700957,
 'sample_weights': [2.9361887331280574, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9719857930992313,
 'weight_decay_Hydroxylation-K': 9.73721157372005,
 'weight_decay_Hydroxylation-P': 9.366874819716754}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.034
[2,     1] loss: 1274.743
[3,     1] loss: 1275.495
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0047480721996317286,
 'learning_rate_Hydroxylation-K': 0.001525645855975045,
 'learning_rate_Hydroxylation-P': 0.0021790477481048284,
 'log_base': 2.6800112385227908,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4015258979,
 'sample_weights': [1.7400000724550517, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.762129297382124,
 'weight_decay_Hydroxylation-K': 4.950932574477648,
 'weight_decay_Hydroxylation-P': 9.117047180280435}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.933
[2,     1] loss: 1265.913
[3,     1] loss: 1267.731
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00959156672598451,
 'learning_rate_Hydroxylation-K': 0.0027495512589416266,
 'learning_rate_Hydroxylation-P': 0.0013414589289851956,
 'log_base': 2.1560332180800925,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2183046834,
 'sample_weights': [1.6934546614016297, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.40851583630322,
 'weight_decay_Hydroxylation-K': 0.5721577367001107,
 'weight_decay_Hydroxylation-P': 7.763191217162769}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1366.633
[2,     1] loss: 1372.271
[3,     1] loss: 1376.132
[4,     1] loss: 1376.049
[5,     1] loss: 1368.086
[6,     1] loss: 1367.122
[7,     1] loss: 1366.626
[8,     1] loss: 1367.391
[9,     1] loss: 1367.810
[10,     1] loss: 1366.052
[11,     1] loss: 1365.692
[12,     1] loss: 1365.510
[13,     1] loss: 1366.241
[14,     1] loss: 1368.004
[15,     1] loss: 1367.302
[16,     1] loss: 1365.146
[17,     1] loss: 1364.363
[18,     1] loss: 1367.132
[19,     1] loss: 1366.371
[20,     1] loss: 1365.557
[21,     1] loss: 1365.073
[22,     1] loss: 1366.594
[23,     1] loss: 1366.050
[24,     1] loss: 1365.596
[25,     1] loss: 1364.928
[26,     1] loss: 1364.687
[27,     1] loss: 1363.677
[28,     1] loss: 1362.943
[29,     1] loss: 1362.828
[30,     1] loss: 1358.690
[31,     1] loss: 1353.036
[32,     1] loss: 1333.853
[33,     1] loss: 1294.551
[34,     1] loss: 1259.807
[35,     1] loss: 1245.590
[36,     1] loss: 1177.753
[37,     1] loss: 1220.147
[38,     1] loss: 1159.710
[39,     1] loss: 1122.672
[40,     1] loss: 1194.909
[41,     1] loss: 1136.244
[42,     1] loss: 1136.212
[43,     1] loss: 1174.458
[44,     1] loss: 1099.719
[45,     1] loss: 1081.608
[46,     1] loss: 1129.564
[47,     1] loss: 1087.968
[48,     1] loss: 1079.793
[49,     1] loss: 1061.504
[50,     1] loss: 1036.940
[51,     1] loss: 1109.394
[52,     1] loss: 1026.255
[53,     1] loss: 1089.874
[54,     1] loss: 991.805
[55,     1] loss: 1077.749
[56,     1] loss: 1020.578
[57,     1] loss: 1046.402
[58,     1] loss: 980.574
[59,     1] loss: 994.172
[60,     1] loss: 958.219
[61,     1] loss: 1009.969
[62,     1] loss: 886.344
[63,     1] loss: 970.454
[64,     1] loss: 923.156
[65,     1] loss: 956.520
[66,     1] loss: 879.787
[67,     1] loss: 855.776
[68,     1] loss: 918.322
[69,     1] loss: 1120.120
[70,     1] loss: 973.836
[71,     1] loss: 893.397
[72,     1] loss: 884.218
[73,     1] loss: 827.789
[74,     1] loss: 915.442
[75,     1] loss: 811.304
[76,     1] loss: 831.615
[77,     1] loss: 807.526
[78,     1] loss: 846.618
[79,     1] loss: 860.065
[80,     1] loss: 812.145
[81,     1] loss: 841.288
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007094454541342145,
 'learning_rate_Hydroxylation-K': 0.009738272833245035,
 'learning_rate_Hydroxylation-P': 0.009590306780944106,
 'log_base': 1.2208772057720287,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1728960153,
 'sample_weights': [2.1729899860828503, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6963950405942967,
 'weight_decay_Hydroxylation-K': 6.790434966652353,
 'weight_decay_Hydroxylation-P': 6.912550109451896}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2712.327
[2,     1] loss: 2770.738
[3,     1] loss: 2717.587
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005681658000238011,
 'learning_rate_Hydroxylation-K': 0.006722049263316722,
 'learning_rate_Hydroxylation-P': 0.007581932495549785,
 'log_base': 1.2251751060095761,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2512806582,
 'sample_weights': [8.365216784464197, 1.0456923807757927],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.500485453099622,
 'weight_decay_Hydroxylation-K': 9.292472262835593,
 'weight_decay_Hydroxylation-P': 7.5706351159547935}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2668.082
[2,     1] loss: 2673.840
[3,     1] loss: 2689.448
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037386670087421244,
 'learning_rate_Hydroxylation-K': 0.000693184916058868,
 'learning_rate_Hydroxylation-P': 0.009726916857974229,
 'log_base': 2.5064105714296634,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3278457221,
 'sample_weights': [8.220465308864565, 1.0275977492748252],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.9039755939308,
 'weight_decay_Hydroxylation-K': 6.456931075119626,
 'weight_decay_Hydroxylation-P': 2.3741885950270376}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.837
[2,     1] loss: 1291.781
[3,     1] loss: 1289.792
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000576466094943401,
 'learning_rate_Hydroxylation-K': 0.005949901459489633,
 'learning_rate_Hydroxylation-P': 0.009710675874245582,
 'log_base': 1.9072573593310689,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1015514784,
 'sample_weights': [1.8168799019793453, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.558049770515993,
 'weight_decay_Hydroxylation-K': 7.121410593180849,
 'weight_decay_Hydroxylation-P': 8.653446694392068}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1457.058
[2,     1] loss: 1454.490
[3,     1] loss: 1457.511
[4,     1] loss: 1455.809
[5,     1] loss: 1452.804
[6,     1] loss: 1453.571
[7,     1] loss: 1452.105
[8,     1] loss: 1454.275
[9,     1] loss: 1450.163
[10,     1] loss: 1452.605
[11,     1] loss: 1452.841
[12,     1] loss: 1449.698
[13,     1] loss: 1449.675
[14,     1] loss: 1450.364
[15,     1] loss: 1449.703
[16,     1] loss: 1447.798
[17,     1] loss: 1447.164
[18,     1] loss: 1447.041
[19,     1] loss: 1446.732
[20,     1] loss: 1447.060
[21,     1] loss: 1445.150
[22,     1] loss: 1440.415
[23,     1] loss: 1437.493
[24,     1] loss: 1439.282
[25,     1] loss: 1426.897
[26,     1] loss: 1423.513
[27,     1] loss: 1419.529
[28,     1] loss: 1408.263
[29,     1] loss: 1400.046
[30,     1] loss: 1392.273
[31,     1] loss: 1374.262
[32,     1] loss: 1359.718
[33,     1] loss: 1350.106
[34,     1] loss: 1346.682
[35,     1] loss: 1327.913
[36,     1] loss: 1312.637
[37,     1] loss: 1318.469
[38,     1] loss: 1305.007
[39,     1] loss: 1299.518
[40,     1] loss: 1248.335
[41,     1] loss: 1257.954
[42,     1] loss: 1203.538
[43,     1] loss: 1245.439
[44,     1] loss: 1243.934
[45,     1] loss: 1215.074
[46,     1] loss: 1248.011
[47,     1] loss: 1216.818
[48,     1] loss: 1215.563
[49,     1] loss: 1233.315
[50,     1] loss: 1193.588
[51,     1] loss: 1230.060
[52,     1] loss: 1198.658
[53,     1] loss: 1200.325
[54,     1] loss: 1242.865
[55,     1] loss: 1215.942
[56,     1] loss: 1182.894
[57,     1] loss: 1232.349
[58,     1] loss: 1168.770
[59,     1] loss: 1177.913
[60,     1] loss: 1197.817
[61,     1] loss: 1220.314
[62,     1] loss: 1179.113
[63,     1] loss: 1193.964
[64,     1] loss: 1206.496
[65,     1] loss: 1154.298
[66,     1] loss: 1185.240
[67,     1] loss: 1140.455
[68,     1] loss: 1158.720
[69,     1] loss: 1173.070
[70,     1] loss: 1114.109
[71,     1] loss: 1127.916
[72,     1] loss: 1127.806
[73,     1] loss: 1104.788
[74,     1] loss: 1116.002
[75,     1] loss: 1118.510
[76,     1] loss: 1057.931
[77,     1] loss: 1079.810
[78,     1] loss: 1103.779
[79,     1] loss: 1035.917
[80,     1] loss: 1093.846
[81,     1] loss: 1025.799
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006384506536958703,
 'learning_rate_Hydroxylation-K': 0.004458404323278361,
 'learning_rate_Hydroxylation-P': 0.0005473312631845409,
 'log_base': 2.3679259955829735,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2585835452,
 'sample_weights': [2.5856130609530608, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.199806135296997,
 'weight_decay_Hydroxylation-K': 1.802737096596434,
 'weight_decay_Hydroxylation-P': 9.347568609451262}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1318.641
[2,     1] loss: 1319.740
[3,     1] loss: 1313.872
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0075856445757835084,
 'learning_rate_Hydroxylation-K': 0.006044968981781156,
 'learning_rate_Hydroxylation-P': 0.0029833869711962018,
 'log_base': 1.4879611855670414,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2649082403,
 'sample_weights': [1.9366764896067419, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.92310527879823,
 'weight_decay_Hydroxylation-K': 5.22118461931804,
 'weight_decay_Hydroxylation-P': 4.974154416588598}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1797.626
[2,     1] loss: 1801.585
[3,     1] loss: 1796.142
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004205926123949927,
 'learning_rate_Hydroxylation-K': 0.002612005449478704,
 'learning_rate_Hydroxylation-P': 0.00741523833642318,
 'log_base': 1.1676700298660563,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1795116003,
 'sample_weights': [4.2008413865868315, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.655704929942736,
 'weight_decay_Hydroxylation-K': 1.8409638476668122,
 'weight_decay_Hydroxylation-P': 0.5989388542972636}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3493.674
[2,     1] loss: 3496.428
[3,     1] loss: 3492.737
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00873365638596009,
 'learning_rate_Hydroxylation-K': 0.0013899738553271239,
 'learning_rate_Hydroxylation-P': 0.005397665560936645,
 'log_base': 2.5774483169642983,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 529974220,
 'sample_weights': [10.769882786388205, 1.3462872106901238],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.646476745823971,
 'weight_decay_Hydroxylation-K': 9.590765991104558,
 'weight_decay_Hydroxylation-P': 1.0486258865484448}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1286.423
[2,     1] loss: 1283.853
[3,     1] loss: 1281.865
[4,     1] loss: 1280.560
[5,     1] loss: 1280.405
[6,     1] loss: 1279.597
[7,     1] loss: 1278.389
[8,     1] loss: 1281.632
[9,     1] loss: 1280.806
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0048921563168973295,
 'learning_rate_Hydroxylation-K': 0.006612027698079569,
 'learning_rate_Hydroxylation-P': 0.005713292131196555,
 'log_base': 2.6946849688792494,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3847963560,
 'sample_weights': [1.7632481517025658, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.12640667026264,
 'weight_decay_Hydroxylation-K': 5.015149656673188,
 'weight_decay_Hydroxylation-P': 5.802539761847428}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.706
[2,     1] loss: 1267.529
[3,     1] loss: 1264.662
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008784195354058796,
 'learning_rate_Hydroxylation-K': 0.0076840216612662215,
 'learning_rate_Hydroxylation-P': 0.008956911677033908,
 'log_base': 1.3056404203365588,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 810421201,
 'sample_weights': [1.684126536635678, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.8711804658130424,
 'weight_decay_Hydroxylation-K': 7.66303290460732,
 'weight_decay_Hydroxylation-P': 1.5603696785566195}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2229.878
[2,     1] loss: 2218.866
[3,     1] loss: 2258.812
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007347638699593911,
 'learning_rate_Hydroxylation-K': 0.005578603579858322,
 'learning_rate_Hydroxylation-P': 0.0017306673740599247,
 'log_base': 1.5620099813129176,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 280504252,
 'sample_weights': [6.2597780640263805, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.171347000836963,
 'weight_decay_Hydroxylation-K': 8.780985737389136,
 'weight_decay_Hydroxylation-P': 6.147683328656663}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1712.381
[2,     1] loss: 1699.049
[3,     1] loss: 1704.234
[4,     1] loss: 1697.415
[5,     1] loss: 1700.735
[6,     1] loss: 1702.149
[7,     1] loss: 1698.922
[8,     1] loss: 1693.768
[9,     1] loss: 1696.301
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021370873025099604,
 'learning_rate_Hydroxylation-K': 0.009660799735391381,
 'learning_rate_Hydroxylation-P': 0.0007497558693356256,
 'log_base': 2.137741919795853,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1978570238,
 'sample_weights': [3.7433689815145827, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.2903801992074726,
 'weight_decay_Hydroxylation-K': 3.674782653696223,
 'weight_decay_Hydroxylation-P': 1.8063201028682214}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1370.222
[2,     1] loss: 1371.202
[3,     1] loss: 1366.809
[4,     1] loss: 1369.623
[5,     1] loss: 1371.119
[6,     1] loss: 1364.692
[7,     1] loss: 1366.663
[8,     1] loss: 1361.746
[9,     1] loss: 1356.813
[10,     1] loss: 1357.831
[11,     1] loss: 1333.902
[12,     1] loss: 1316.409
[13,     1] loss: 1292.377
[14,     1] loss: 1270.137
[15,     1] loss: 1242.176
[16,     1] loss: 1228.553
[17,     1] loss: 1166.687
[18,     1] loss: 1145.519
[19,     1] loss: 1134.838
[20,     1] loss: 1160.718
[21,     1] loss: 1121.630
[22,     1] loss: 1148.612
[23,     1] loss: 1074.982
[24,     1] loss: 1076.848
[25,     1] loss: 1071.474
[26,     1] loss: 1032.023
[27,     1] loss: 1084.087
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0075587889863288835,
 'learning_rate_Hydroxylation-K': 0.006058943227871048,
 'learning_rate_Hydroxylation-P': 0.0069992427867780205,
 'log_base': 1.2857299230348878,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3290274130,
 'sample_weights': [2.197358262934721, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9831468615613723,
 'weight_decay_Hydroxylation-K': 2.9769627311988964,
 'weight_decay_Hydroxylation-P': 3.8346999923517937}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2315.822
[2,     1] loss: 2318.282
[3,     1] loss: 2315.287
[4,     1] loss: 2306.344
[5,     1] loss: 2303.143
[6,     1] loss: 2312.219
[7,     1] loss: 2309.964
[8,     1] loss: 2307.115
[9,     1] loss: 2318.847
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013463023253665353,
 'learning_rate_Hydroxylation-K': 0.004671854606108535,
 'learning_rate_Hydroxylation-P': 0.006427628128704657,
 'log_base': 2.0286730580093106,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3800970786,
 'sample_weights': [6.642524945946287, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.620388160191384,
 'weight_decay_Hydroxylation-K': 8.345624592678844,
 'weight_decay_Hydroxylation-P': 7.995901425748928}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.254
[2,     1] loss: 1406.910
[3,     1] loss: 1408.459
[4,     1] loss: 1403.963
[5,     1] loss: 1405.848
[6,     1] loss: 1409.186
[7,     1] loss: 1404.949
[8,     1] loss: 1401.729
[9,     1] loss: 1404.802
[10,     1] loss: 1400.939
[11,     1] loss: 1398.115
[12,     1] loss: 1392.327
[13,     1] loss: 1389.908
[14,     1] loss: 1381.283
[15,     1] loss: 1361.637
[16,     1] loss: 1348.593
[17,     1] loss: 1337.561
[18,     1] loss: 1306.646
[19,     1] loss: 1288.767
[20,     1] loss: 1265.444
[21,     1] loss: 1248.777
[22,     1] loss: 1220.962
[23,     1] loss: 1235.816
[24,     1] loss: 1180.405
[25,     1] loss: 1192.729
[26,     1] loss: 1211.864
[27,     1] loss: 1135.623
[28,     1] loss: 1193.430
[29,     1] loss: 1186.187
[30,     1] loss: 1165.107
[31,     1] loss: 1173.209
[32,     1] loss: 1128.955
[33,     1] loss: 1123.374
[34,     1] loss: 1155.728
[35,     1] loss: 1115.889
[36,     1] loss: 1177.247
[37,     1] loss: 1111.451
[38,     1] loss: 1098.679
[39,     1] loss: 1072.666
[40,     1] loss: 1115.535
[41,     1] loss: 1037.019
[42,     1] loss: 1054.941
[43,     1] loss: 1106.844
[44,     1] loss: 1052.651
[45,     1] loss: 1035.427
[46,     1] loss: 1015.719
[47,     1] loss: 1074.681
[48,     1] loss: 992.915
[49,     1] loss: 999.235
[50,     1] loss: 1017.062
[51,     1] loss: 1041.976
[52,     1] loss: 974.880
[53,     1] loss: 951.714
[54,     1] loss: 1023.747
[55,     1] loss: 943.516
[56,     1] loss: 1002.420
[57,     1] loss: 923.522
[58,     1] loss: 973.455
[59,     1] loss: 913.799
[60,     1] loss: 977.229
[61,     1] loss: 929.018
[62,     1] loss: 908.145
[63,     1] loss: 887.505
[64,     1] loss: 948.473
[65,     1] loss: 897.023
[66,     1] loss: 852.588
[67,     1] loss: 926.418
[68,     1] loss: 879.973
[69,     1] loss: 841.240
[70,     1] loss: 858.366
[71,     1] loss: 904.795
[72,     1] loss: 844.936
[73,     1] loss: 852.515
[74,     1] loss: 925.861
[75,     1] loss: 788.939
[76,     1] loss: 848.716
[77,     1] loss: 773.922
[78,     1] loss: 782.454
[79,     1] loss: 777.665
[80,     1] loss: 821.474
[81,     1] loss: 764.713
[82,     1] loss: 748.564
[83,     1] loss: 772.328
[84,     1] loss: 716.410
[85,     1] loss: 746.213
[86,     1] loss: 698.889
[87,     1] loss: 717.458
[88,     1] loss: 742.604
[89,     1] loss: 776.216
[90,     1] loss: 674.714
[91,     1] loss: 825.193
[92,     1] loss: 624.119
[93,     1] loss: 702.141
[94,     1] loss: 669.158
[95,     1] loss: 708.203
[96,     1] loss: 663.995
[97,     1] loss: 664.542
[98,     1] loss: 647.425
[99,     1] loss: 653.531
[100,     1] loss: 685.595
[101,     1] loss: 589.500
[102,     1] loss: 528.786
[103,     1] loss: 678.953
[104,     1] loss: 564.455
[105,     1] loss: 596.334
[106,     1] loss: 689.514
[107,     1] loss: 610.016
[108,     1] loss: 679.948
[109,     1] loss: 607.427
[110,     1] loss: 631.055
[111,     1] loss: 598.863
[112,     1] loss: 570.415
[113,     1] loss: 572.861
[114,     1] loss: 627.885
[115,     1] loss: 556.504
[116,     1] loss: 640.930
Early stopping applied (best metric=0.3295874297618866)
Finished Training
Total time taken: 17.0752010345459
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.417
[2,     1] loss: 1407.924
[3,     1] loss: 1409.655
[4,     1] loss: 1414.150
[5,     1] loss: 1402.336
[6,     1] loss: 1407.762
[7,     1] loss: 1401.499
[8,     1] loss: 1403.649
[9,     1] loss: 1395.589
[10,     1] loss: 1397.446
[11,     1] loss: 1393.518
[12,     1] loss: 1386.634
[13,     1] loss: 1382.573
[14,     1] loss: 1363.083
[15,     1] loss: 1354.569
[16,     1] loss: 1338.421
[17,     1] loss: 1315.165
[18,     1] loss: 1289.044
[19,     1] loss: 1270.709
[20,     1] loss: 1242.504
[21,     1] loss: 1235.147
[22,     1] loss: 1213.599
[23,     1] loss: 1217.384
[24,     1] loss: 1191.562
[25,     1] loss: 1173.755
[26,     1] loss: 1175.197
[27,     1] loss: 1153.896
[28,     1] loss: 1132.248
[29,     1] loss: 1147.041
[30,     1] loss: 1153.570
[31,     1] loss: 1098.319
[32,     1] loss: 1176.445
[33,     1] loss: 1114.993
[34,     1] loss: 1158.478
[35,     1] loss: 1088.093
[36,     1] loss: 1130.125
[37,     1] loss: 1105.294
[38,     1] loss: 1088.674
[39,     1] loss: 1056.900
[40,     1] loss: 1088.496
[41,     1] loss: 1087.955
[42,     1] loss: 1099.307
[43,     1] loss: 1057.901
[44,     1] loss: 1058.430
[45,     1] loss: 1003.145
[46,     1] loss: 1026.830
[47,     1] loss: 1025.675
[48,     1] loss: 1052.857
[49,     1] loss: 1005.854
[50,     1] loss: 1042.913
[51,     1] loss: 939.784
[52,     1] loss: 1045.219
[53,     1] loss: 931.629
[54,     1] loss: 1008.553
[55,     1] loss: 967.887
[56,     1] loss: 958.259
[57,     1] loss: 965.997
[58,     1] loss: 906.911
[59,     1] loss: 932.232
[60,     1] loss: 926.997
[61,     1] loss: 933.868
[62,     1] loss: 895.931
[63,     1] loss: 923.885
[64,     1] loss: 861.170
[65,     1] loss: 953.014
[66,     1] loss: 871.574
[67,     1] loss: 929.940
[68,     1] loss: 863.092
[69,     1] loss: 877.732
[70,     1] loss: 833.811
[71,     1] loss: 822.982
[72,     1] loss: 843.427
[73,     1] loss: 795.381
[74,     1] loss: 775.322
[75,     1] loss: 803.955
[76,     1] loss: 796.597
[77,     1] loss: 730.710
[78,     1] loss: 757.438
[79,     1] loss: 691.960
[80,     1] loss: 704.733
[81,     1] loss: 677.212
[82,     1] loss: 682.891
[83,     1] loss: 681.412
[84,     1] loss: 779.324
[85,     1] loss: 684.330
[86,     1] loss: 719.739
[87,     1] loss: 801.705
[88,     1] loss: 623.859
[89,     1] loss: 754.389
[90,     1] loss: 690.274
[91,     1] loss: 702.241
[92,     1] loss: 686.142
[93,     1] loss: 655.797
[94,     1] loss: 688.679
[95,     1] loss: 581.728
[96,     1] loss: 686.448
[97,     1] loss: 630.804
[98,     1] loss: 661.349
[99,     1] loss: 597.642
[100,     1] loss: 593.289
[101,     1] loss: 575.000
[102,     1] loss: 599.616
[103,     1] loss: 594.047
[104,     1] loss: 601.122
[105,     1] loss: 583.474
[106,     1] loss: 636.533
[107,     1] loss: 562.162
[108,     1] loss: 598.222
[109,     1] loss: 552.971
[110,     1] loss: 620.502
[111,     1] loss: 519.890
[112,     1] loss: 485.109
[113,     1] loss: 548.023
[114,     1] loss: 586.011
[115,     1] loss: 513.981
[116,     1] loss: 562.868
[117,     1] loss: 526.727
Early stopping applied (best metric=0.36359262466430664)
Finished Training
Total time taken: 17.069167613983154
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1414.169
[2,     1] loss: 1403.999
[3,     1] loss: 1403.132
[4,     1] loss: 1406.677
[5,     1] loss: 1406.240
[6,     1] loss: 1404.149
[7,     1] loss: 1404.704
[8,     1] loss: 1404.347
[9,     1] loss: 1400.314
[10,     1] loss: 1406.736
[11,     1] loss: 1399.515
[12,     1] loss: 1394.721
[13,     1] loss: 1391.645
[14,     1] loss: 1390.076
[15,     1] loss: 1378.382
[16,     1] loss: 1377.198
[17,     1] loss: 1362.024
[18,     1] loss: 1350.157
[19,     1] loss: 1308.523
[20,     1] loss: 1297.309
[21,     1] loss: 1293.209
[22,     1] loss: 1273.662
[23,     1] loss: 1246.059
[24,     1] loss: 1243.182
[25,     1] loss: 1249.885
[26,     1] loss: 1205.943
[27,     1] loss: 1193.301
[28,     1] loss: 1216.095
[29,     1] loss: 1191.609
[30,     1] loss: 1172.891
[31,     1] loss: 1200.111
[32,     1] loss: 1141.082
[33,     1] loss: 1210.496
[34,     1] loss: 1185.327
[35,     1] loss: 1126.990
[36,     1] loss: 1165.399
[37,     1] loss: 1092.650
[38,     1] loss: 1082.782
[39,     1] loss: 1146.968
[40,     1] loss: 1121.729
[41,     1] loss: 1106.175
[42,     1] loss: 1122.542
[43,     1] loss: 1081.421
[44,     1] loss: 1048.319
[45,     1] loss: 1088.645
[46,     1] loss: 1078.964
[47,     1] loss: 1017.523
[48,     1] loss: 1017.715
[49,     1] loss: 1009.410
[50,     1] loss: 1014.851
[51,     1] loss: 967.958
[52,     1] loss: 1025.093
[53,     1] loss: 995.192
[54,     1] loss: 1001.685
[55,     1] loss: 981.665
[56,     1] loss: 1044.229
[57,     1] loss: 1065.734
[58,     1] loss: 958.646
[59,     1] loss: 965.035
[60,     1] loss: 941.450
[61,     1] loss: 956.625
[62,     1] loss: 910.585
[63,     1] loss: 940.425
[64,     1] loss: 916.158
[65,     1] loss: 961.028
[66,     1] loss: 902.785
[67,     1] loss: 899.470
[68,     1] loss: 911.973
[69,     1] loss: 879.371
[70,     1] loss: 870.162
[71,     1] loss: 838.054
[72,     1] loss: 842.714
[73,     1] loss: 837.157
[74,     1] loss: 811.361
[75,     1] loss: 890.080
[76,     1] loss: 923.884
[77,     1] loss: 844.631
[78,     1] loss: 837.709
[79,     1] loss: 830.770
[80,     1] loss: 864.712
[81,     1] loss: 739.122
[82,     1] loss: 781.498
[83,     1] loss: 739.136
[84,     1] loss: 825.150
[85,     1] loss: 758.324
[86,     1] loss: 790.554
[87,     1] loss: 705.625
[88,     1] loss: 764.992
[89,     1] loss: 726.025
[90,     1] loss: 741.361
[91,     1] loss: 682.754
[92,     1] loss: 707.718
[93,     1] loss: 664.965
[94,     1] loss: 643.652
[95,     1] loss: 664.599
[96,     1] loss: 654.831
[97,     1] loss: 668.677
[98,     1] loss: 615.911
[99,     1] loss: 655.251
Early stopping applied (best metric=0.37494635581970215)
Finished Training
Total time taken: 14.52470088005066
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.836
[2,     1] loss: 1404.302
[3,     1] loss: 1407.049
[4,     1] loss: 1406.672
[5,     1] loss: 1405.257
[6,     1] loss: 1407.421
[7,     1] loss: 1399.202
[8,     1] loss: 1401.539
[9,     1] loss: 1399.375
[10,     1] loss: 1394.966
[11,     1] loss: 1388.429
[12,     1] loss: 1382.944
[13,     1] loss: 1370.168
[14,     1] loss: 1351.235
[15,     1] loss: 1339.178
[16,     1] loss: 1322.296
[17,     1] loss: 1292.208
[18,     1] loss: 1278.471
[19,     1] loss: 1242.254
[20,     1] loss: 1238.766
[21,     1] loss: 1245.787
[22,     1] loss: 1216.748
[23,     1] loss: 1177.712
[24,     1] loss: 1158.368
[25,     1] loss: 1163.783
[26,     1] loss: 1171.152
[27,     1] loss: 1186.367
[28,     1] loss: 1167.927
[29,     1] loss: 1153.114
[30,     1] loss: 1136.297
[31,     1] loss: 1088.589
[32,     1] loss: 1089.006
[33,     1] loss: 1104.458
[34,     1] loss: 1079.277
[35,     1] loss: 1059.083
[36,     1] loss: 1067.672
[37,     1] loss: 1044.518
[38,     1] loss: 1034.256
[39,     1] loss: 1054.550
[40,     1] loss: 1058.514
[41,     1] loss: 1024.775
[42,     1] loss: 1019.450
[43,     1] loss: 1007.212
[44,     1] loss: 1002.061
[45,     1] loss: 954.354
[46,     1] loss: 946.479
[47,     1] loss: 973.049
[48,     1] loss: 1000.324
[49,     1] loss: 929.166
[50,     1] loss: 935.881
[51,     1] loss: 892.735
[52,     1] loss: 955.206
[53,     1] loss: 907.114
[54,     1] loss: 954.395
[55,     1] loss: 959.945
[56,     1] loss: 933.669
[57,     1] loss: 919.690
[58,     1] loss: 890.732
[59,     1] loss: 900.345
[60,     1] loss: 876.889
[61,     1] loss: 907.273
[62,     1] loss: 880.393
[63,     1] loss: 842.362
[64,     1] loss: 842.647
[65,     1] loss: 782.127
[66,     1] loss: 841.477
[67,     1] loss: 808.610
[68,     1] loss: 783.707
[69,     1] loss: 772.083
[70,     1] loss: 758.140
[71,     1] loss: 771.622
[72,     1] loss: 824.253
[73,     1] loss: 730.499
[74,     1] loss: 760.406
[75,     1] loss: 719.588
[76,     1] loss: 771.393
[77,     1] loss: 806.528
[78,     1] loss: 764.015
[79,     1] loss: 758.552
Early stopping applied (best metric=0.37114354968070984)
Finished Training
Total time taken: 11.558702230453491
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1402.093
[2,     1] loss: 1412.091
[3,     1] loss: 1407.477
[4,     1] loss: 1403.961
[5,     1] loss: 1403.163
[6,     1] loss: 1402.668
[7,     1] loss: 1407.094
[8,     1] loss: 1403.116
[9,     1] loss: 1399.335
[10,     1] loss: 1395.507
[11,     1] loss: 1393.467
[12,     1] loss: 1375.838
[13,     1] loss: 1366.731
[14,     1] loss: 1346.407
[15,     1] loss: 1336.299
[16,     1] loss: 1312.856
[17,     1] loss: 1280.854
[18,     1] loss: 1279.075
[19,     1] loss: 1249.490
[20,     1] loss: 1228.991
[21,     1] loss: 1214.335
[22,     1] loss: 1234.418
[23,     1] loss: 1166.260
[24,     1] loss: 1152.994
[25,     1] loss: 1167.270
[26,     1] loss: 1187.207
[27,     1] loss: 1161.436
[28,     1] loss: 1138.409
[29,     1] loss: 1198.416
[30,     1] loss: 1114.156
[31,     1] loss: 1156.170
[32,     1] loss: 1136.684
[33,     1] loss: 1165.547
[34,     1] loss: 1097.804
[35,     1] loss: 1145.002
[36,     1] loss: 1062.158
[37,     1] loss: 1116.966
[38,     1] loss: 1077.997
[39,     1] loss: 1072.430
[40,     1] loss: 1051.722
[41,     1] loss: 1067.722
[42,     1] loss: 1026.688
[43,     1] loss: 1119.591
[44,     1] loss: 1029.445
[45,     1] loss: 1011.248
[46,     1] loss: 1021.637
[47,     1] loss: 1016.549
[48,     1] loss: 1028.656
[49,     1] loss: 1020.389
[50,     1] loss: 958.978
[51,     1] loss: 1027.418
[52,     1] loss: 955.723
[53,     1] loss: 988.166
[54,     1] loss: 934.514
[55,     1] loss: 953.843
[56,     1] loss: 934.227
[57,     1] loss: 934.392
[58,     1] loss: 903.035
[59,     1] loss: 964.305
[60,     1] loss: 921.838
[61,     1] loss: 897.702
[62,     1] loss: 898.180
[63,     1] loss: 817.509
[64,     1] loss: 940.331
[65,     1] loss: 772.607
[66,     1] loss: 822.635
[67,     1] loss: 844.316
[68,     1] loss: 860.545
[69,     1] loss: 827.653
[70,     1] loss: 793.335
[71,     1] loss: 813.466
[72,     1] loss: 803.957
[73,     1] loss: 751.957
[74,     1] loss: 804.438
[75,     1] loss: 784.920
[76,     1] loss: 724.404
[77,     1] loss: 765.032
[78,     1] loss: 739.374
[79,     1] loss: 741.034
[80,     1] loss: 728.406
[81,     1] loss: 734.058
[82,     1] loss: 711.229
[83,     1] loss: 773.776
[84,     1] loss: 783.156
[85,     1] loss: 770.865
[86,     1] loss: 762.929
[87,     1] loss: 722.876
[88,     1] loss: 737.408
[89,     1] loss: 745.044
[90,     1] loss: 691.908
[91,     1] loss: 729.661
[92,     1] loss: 700.880
[93,     1] loss: 654.363
[94,     1] loss: 669.502
[95,     1] loss: 680.878
[96,     1] loss: 668.339
[97,     1] loss: 657.140
[98,     1] loss: 653.135
Early stopping applied (best metric=0.3855576813220978)
Finished Training
Total time taken: 14.302243709564209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1413.968
[2,     1] loss: 1408.116
[3,     1] loss: 1408.683
[4,     1] loss: 1404.393
[5,     1] loss: 1404.407
[6,     1] loss: 1402.907
[7,     1] loss: 1397.945
[8,     1] loss: 1402.411
[9,     1] loss: 1404.786
[10,     1] loss: 1401.893
[11,     1] loss: 1400.589
[12,     1] loss: 1394.647
[13,     1] loss: 1387.814
[14,     1] loss: 1378.473
[15,     1] loss: 1371.599
[16,     1] loss: 1355.866
[17,     1] loss: 1338.587
[18,     1] loss: 1324.499
[19,     1] loss: 1315.064
[20,     1] loss: 1280.233
[21,     1] loss: 1269.325
[22,     1] loss: 1243.621
[23,     1] loss: 1218.774
[24,     1] loss: 1232.622
[25,     1] loss: 1241.205
[26,     1] loss: 1242.416
[27,     1] loss: 1201.255
[28,     1] loss: 1171.613
[29,     1] loss: 1216.971
[30,     1] loss: 1156.935
[31,     1] loss: 1149.175
[32,     1] loss: 1176.083
[33,     1] loss: 1095.343
[34,     1] loss: 1182.169
[35,     1] loss: 1108.437
[36,     1] loss: 1122.069
[37,     1] loss: 1101.364
[38,     1] loss: 1104.479
[39,     1] loss: 1091.535
[40,     1] loss: 1091.516
[41,     1] loss: 1093.661
[42,     1] loss: 1102.608
[43,     1] loss: 1062.574
[44,     1] loss: 1056.450
[45,     1] loss: 1077.504
[46,     1] loss: 1023.676
[47,     1] loss: 1037.133
[48,     1] loss: 1004.939
[49,     1] loss: 952.381
[50,     1] loss: 995.943
[51,     1] loss: 989.471
[52,     1] loss: 951.862
[53,     1] loss: 968.187
[54,     1] loss: 913.778
[55,     1] loss: 911.183
[56,     1] loss: 905.399
[57,     1] loss: 924.639
[58,     1] loss: 924.102
[59,     1] loss: 876.223
[60,     1] loss: 854.613
[61,     1] loss: 866.205
[62,     1] loss: 834.759
[63,     1] loss: 821.927
[64,     1] loss: 858.226
[65,     1] loss: 817.820
[66,     1] loss: 819.409
[67,     1] loss: 800.957
[68,     1] loss: 789.077
[69,     1] loss: 723.037
[70,     1] loss: 773.302
[71,     1] loss: 773.958
[72,     1] loss: 726.093
[73,     1] loss: 776.730
[74,     1] loss: 759.431
[75,     1] loss: 747.552
[76,     1] loss: 837.508
[77,     1] loss: 710.893
[78,     1] loss: 722.991
[79,     1] loss: 699.029
[80,     1] loss: 793.689
[81,     1] loss: 718.393
[82,     1] loss: 715.247
[83,     1] loss: 695.433
[84,     1] loss: 688.317
[85,     1] loss: 726.707
[86,     1] loss: 686.789
[87,     1] loss: 667.769
[88,     1] loss: 642.324
[89,     1] loss: 660.099
[90,     1] loss: 565.931
[91,     1] loss: 614.700
[92,     1] loss: 651.710
[93,     1] loss: 666.802
[94,     1] loss: 572.004
[95,     1] loss: 639.524
[96,     1] loss: 594.568
Early stopping applied (best metric=0.40905916690826416)
Finished Training
Total time taken: 14.055339813232422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1412.299
[2,     1] loss: 1406.161
[3,     1] loss: 1406.529
[4,     1] loss: 1403.755
[5,     1] loss: 1408.448
[6,     1] loss: 1402.990
[7,     1] loss: 1407.650
[8,     1] loss: 1399.684
[9,     1] loss: 1399.568
[10,     1] loss: 1401.974
[11,     1] loss: 1396.046
[12,     1] loss: 1390.615
[13,     1] loss: 1386.315
[14,     1] loss: 1380.184
[15,     1] loss: 1371.917
[16,     1] loss: 1358.333
[17,     1] loss: 1336.128
[18,     1] loss: 1319.659
[19,     1] loss: 1281.220
[20,     1] loss: 1286.139
[21,     1] loss: 1252.812
[22,     1] loss: 1224.920
[23,     1] loss: 1210.674
[24,     1] loss: 1220.013
[25,     1] loss: 1217.055
[26,     1] loss: 1169.645
[27,     1] loss: 1161.378
[28,     1] loss: 1162.638
[29,     1] loss: 1165.729
[30,     1] loss: 1138.657
[31,     1] loss: 1158.000
[32,     1] loss: 1121.474
[33,     1] loss: 1115.791
[34,     1] loss: 1123.682
[35,     1] loss: 1121.843
[36,     1] loss: 1121.426
[37,     1] loss: 1160.773
[38,     1] loss: 1130.353
[39,     1] loss: 1067.214
[40,     1] loss: 1104.451
[41,     1] loss: 1073.221
[42,     1] loss: 1107.870
[43,     1] loss: 1040.321
[44,     1] loss: 1052.380
[45,     1] loss: 1076.259
[46,     1] loss: 1021.018
[47,     1] loss: 1024.807
[48,     1] loss: 1013.198
[49,     1] loss: 1041.654
[50,     1] loss: 1005.152
[51,     1] loss: 1018.960
[52,     1] loss: 1043.100
[53,     1] loss: 944.100
[54,     1] loss: 968.248
[55,     1] loss: 1028.507
[56,     1] loss: 914.160
[57,     1] loss: 935.752
[58,     1] loss: 969.668
[59,     1] loss: 948.778
[60,     1] loss: 944.752
[61,     1] loss: 969.814
[62,     1] loss: 946.030
[63,     1] loss: 973.291
[64,     1] loss: 889.983
[65,     1] loss: 814.819
[66,     1] loss: 891.100
[67,     1] loss: 837.918
[68,     1] loss: 845.806
[69,     1] loss: 1022.983
[70,     1] loss: 850.237
[71,     1] loss: 849.945
[72,     1] loss: 782.363
[73,     1] loss: 830.993
[74,     1] loss: 841.106
[75,     1] loss: 801.771
[76,     1] loss: 801.971
[77,     1] loss: 772.649
[78,     1] loss: 773.089
[79,     1] loss: 774.117
[80,     1] loss: 774.571
[81,     1] loss: 806.137
[82,     1] loss: 709.593
[83,     1] loss: 701.916
[84,     1] loss: 776.844
[85,     1] loss: 759.057
[86,     1] loss: 717.412
[87,     1] loss: 714.576
[88,     1] loss: 759.756
[89,     1] loss: 671.654
[90,     1] loss: 703.126
[91,     1] loss: 657.649
[92,     1] loss: 717.235
[93,     1] loss: 639.569
[94,     1] loss: 704.933
[95,     1] loss: 646.381
[96,     1] loss: 668.388
[97,     1] loss: 783.193
[98,     1] loss: 660.146
[99,     1] loss: 751.334
[100,     1] loss: 647.551
Early stopping applied (best metric=0.3292244076728821)
Finished Training
Total time taken: 14.681689977645874
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1408.688
[2,     1] loss: 1406.196
[3,     1] loss: 1406.335
[4,     1] loss: 1405.638
[5,     1] loss: 1405.666
[6,     1] loss: 1405.750
[7,     1] loss: 1405.653
[8,     1] loss: 1404.190
[9,     1] loss: 1401.620
[10,     1] loss: 1398.548
[11,     1] loss: 1392.596
[12,     1] loss: 1393.249
[13,     1] loss: 1383.970
[14,     1] loss: 1377.194
[15,     1] loss: 1364.214
[16,     1] loss: 1348.463
[17,     1] loss: 1330.264
[18,     1] loss: 1300.888
[19,     1] loss: 1291.032
[20,     1] loss: 1268.269
[21,     1] loss: 1257.641
[22,     1] loss: 1245.284
[23,     1] loss: 1225.391
[24,     1] loss: 1202.516
[25,     1] loss: 1196.306
[26,     1] loss: 1193.931
[27,     1] loss: 1169.920
[28,     1] loss: 1192.026
[29,     1] loss: 1157.498
[30,     1] loss: 1144.445
[31,     1] loss: 1136.295
[32,     1] loss: 1135.360
[33,     1] loss: 1161.396
[34,     1] loss: 1169.634
[35,     1] loss: 1128.659
[36,     1] loss: 1138.201
[37,     1] loss: 1136.904
[38,     1] loss: 1143.994
[39,     1] loss: 1137.144
[40,     1] loss: 1112.834
[41,     1] loss: 1140.632
[42,     1] loss: 1051.647
[43,     1] loss: 1060.817
[44,     1] loss: 1065.862
[45,     1] loss: 1008.270
[46,     1] loss: 1071.690
[47,     1] loss: 1051.782
[48,     1] loss: 1042.144
[49,     1] loss: 1022.745
[50,     1] loss: 1015.486
[51,     1] loss: 1006.871
[52,     1] loss: 1038.457
[53,     1] loss: 1046.955
[54,     1] loss: 1021.818
[55,     1] loss: 1006.647
[56,     1] loss: 940.433
[57,     1] loss: 962.031
[58,     1] loss: 988.870
[59,     1] loss: 881.942
[60,     1] loss: 918.657
[61,     1] loss: 986.273
[62,     1] loss: 907.602
[63,     1] loss: 924.014
[64,     1] loss: 914.185
[65,     1] loss: 906.730
[66,     1] loss: 989.294
[67,     1] loss: 896.353
[68,     1] loss: 911.970
[69,     1] loss: 958.238
[70,     1] loss: 870.818
[71,     1] loss: 890.992
[72,     1] loss: 857.655
[73,     1] loss: 942.417
[74,     1] loss: 827.426
[75,     1] loss: 885.208
[76,     1] loss: 833.005
[77,     1] loss: 830.810
[78,     1] loss: 807.024
[79,     1] loss: 825.878
[80,     1] loss: 818.688
[81,     1] loss: 858.930
[82,     1] loss: 775.040
[83,     1] loss: 863.234
[84,     1] loss: 725.217
[85,     1] loss: 752.062
[86,     1] loss: 734.590
[87,     1] loss: 738.026
[88,     1] loss: 689.684
[89,     1] loss: 769.471
[90,     1] loss: 642.640
[91,     1] loss: 684.274
[92,     1] loss: 671.351
[93,     1] loss: 621.185
[94,     1] loss: 742.801
[95,     1] loss: 670.624
[96,     1] loss: 650.493
[97,     1] loss: 703.148
[98,     1] loss: 698.175
[99,     1] loss: 876.677
[100,     1] loss: 638.831
[101,     1] loss: 773.879
[102,     1] loss: 611.327
[103,     1] loss: 674.017
[104,     1] loss: 650.019
[105,     1] loss: 663.004
[106,     1] loss: 723.198
[107,     1] loss: 629.345
[108,     1] loss: 751.096
[109,     1] loss: 629.837
[110,     1] loss: 649.360
[111,     1] loss: 632.870
[112,     1] loss: 570.774
Early stopping applied (best metric=0.3613799512386322)
Finished Training
Total time taken: 16.334707736968994
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.637
[2,     1] loss: 1406.782
[3,     1] loss: 1404.829
[4,     1] loss: 1404.352
[5,     1] loss: 1400.843
[6,     1] loss: 1403.203
[7,     1] loss: 1403.760
[8,     1] loss: 1395.890
[9,     1] loss: 1395.376
[10,     1] loss: 1385.099
[11,     1] loss: 1378.653
[12,     1] loss: 1363.544
[13,     1] loss: 1349.888
[14,     1] loss: 1326.117
[15,     1] loss: 1310.041
[16,     1] loss: 1280.785
[17,     1] loss: 1259.135
[18,     1] loss: 1248.677
[19,     1] loss: 1216.819
[20,     1] loss: 1201.490
[21,     1] loss: 1195.200
[22,     1] loss: 1188.777
[23,     1] loss: 1168.416
[24,     1] loss: 1154.283
[25,     1] loss: 1147.993
[26,     1] loss: 1154.417
[27,     1] loss: 1091.227
[28,     1] loss: 1136.028
[29,     1] loss: 1133.031
[30,     1] loss: 1092.433
[31,     1] loss: 1156.762
[32,     1] loss: 1131.052
[33,     1] loss: 1104.136
[34,     1] loss: 1083.202
[35,     1] loss: 1119.157
[36,     1] loss: 1137.637
[37,     1] loss: 1078.406
[38,     1] loss: 1080.629
[39,     1] loss: 1083.984
[40,     1] loss: 1073.046
[41,     1] loss: 1083.846
[42,     1] loss: 1021.635
[43,     1] loss: 1057.024
[44,     1] loss: 1080.424
[45,     1] loss: 1065.577
[46,     1] loss: 977.708
[47,     1] loss: 1024.497
[48,     1] loss: 987.477
[49,     1] loss: 1008.109
[50,     1] loss: 1011.013
[51,     1] loss: 1002.410
[52,     1] loss: 986.915
[53,     1] loss: 975.000
[54,     1] loss: 976.777
[55,     1] loss: 968.746
[56,     1] loss: 999.793
[57,     1] loss: 953.047
[58,     1] loss: 943.071
[59,     1] loss: 888.072
[60,     1] loss: 940.303
[61,     1] loss: 903.570
[62,     1] loss: 889.812
[63,     1] loss: 897.035
[64,     1] loss: 901.020
[65,     1] loss: 854.435
[66,     1] loss: 855.871
[67,     1] loss: 790.844
[68,     1] loss: 886.496
[69,     1] loss: 816.367
[70,     1] loss: 830.715
[71,     1] loss: 833.884
[72,     1] loss: 869.093
[73,     1] loss: 784.426
[74,     1] loss: 815.661
[75,     1] loss: 869.759
[76,     1] loss: 805.632
Early stopping applied (best metric=0.4049268960952759)
Finished Training
Total time taken: 11.239077806472778
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1410.665
[2,     1] loss: 1407.112
[3,     1] loss: 1412.022
[4,     1] loss: 1407.858
[5,     1] loss: 1405.389
[6,     1] loss: 1407.372
[7,     1] loss: 1406.461
[8,     1] loss: 1405.064
[9,     1] loss: 1400.777
[10,     1] loss: 1400.143
[11,     1] loss: 1394.078
[12,     1] loss: 1386.197
[13,     1] loss: 1380.962
[14,     1] loss: 1364.056
[15,     1] loss: 1342.621
[16,     1] loss: 1330.665
[17,     1] loss: 1310.558
[18,     1] loss: 1280.730
[19,     1] loss: 1263.364
[20,     1] loss: 1231.625
[21,     1] loss: 1229.936
[22,     1] loss: 1224.470
[23,     1] loss: 1159.806
[24,     1] loss: 1188.811
[25,     1] loss: 1201.931
[26,     1] loss: 1138.002
[27,     1] loss: 1133.789
[28,     1] loss: 1143.649
[29,     1] loss: 1153.035
[30,     1] loss: 1129.034
[31,     1] loss: 1162.367
[32,     1] loss: 1116.283
[33,     1] loss: 1118.707
[34,     1] loss: 1107.014
[35,     1] loss: 1064.287
[36,     1] loss: 1116.600
[37,     1] loss: 1077.855
[38,     1] loss: 1125.288
[39,     1] loss: 1098.553
[40,     1] loss: 1016.888
[41,     1] loss: 1076.006
[42,     1] loss: 1046.792
[43,     1] loss: 1041.780
[44,     1] loss: 1022.414
[45,     1] loss: 991.285
[46,     1] loss: 1045.500
[47,     1] loss: 1012.927
[48,     1] loss: 1094.489
[49,     1] loss: 1023.300
[50,     1] loss: 991.189
[51,     1] loss: 1003.557
[52,     1] loss: 1001.633
[53,     1] loss: 997.423
[54,     1] loss: 938.097
[55,     1] loss: 1026.879
[56,     1] loss: 960.722
[57,     1] loss: 952.931
[58,     1] loss: 885.101
[59,     1] loss: 927.058
[60,     1] loss: 985.822
[61,     1] loss: 952.462
[62,     1] loss: 997.674
[63,     1] loss: 923.107
[64,     1] loss: 857.491
[65,     1] loss: 936.830
[66,     1] loss: 916.483
[67,     1] loss: 948.286
[68,     1] loss: 919.693
[69,     1] loss: 920.660
[70,     1] loss: 902.512
[71,     1] loss: 951.333
[72,     1] loss: 848.805
[73,     1] loss: 944.351
[74,     1] loss: 808.335
[75,     1] loss: 826.479
[76,     1] loss: 827.828
[77,     1] loss: 854.587
[78,     1] loss: 850.055
[79,     1] loss: 858.218
[80,     1] loss: 762.712
[81,     1] loss: 802.089
[82,     1] loss: 821.751
[83,     1] loss: 771.223
[84,     1] loss: 760.881
[85,     1] loss: 800.931
[86,     1] loss: 740.145
[87,     1] loss: 781.649
[88,     1] loss: 712.981
[89,     1] loss: 719.259
[90,     1] loss: 722.506
[91,     1] loss: 744.717
[92,     1] loss: 723.985
[93,     1] loss: 703.081
[94,     1] loss: 673.592
[95,     1] loss: 697.359
[96,     1] loss: 644.090
[97,     1] loss: 657.791
[98,     1] loss: 681.803
[99,     1] loss: 656.139
[100,     1] loss: 723.708
[101,     1] loss: 687.306
[102,     1] loss: 586.372
[103,     1] loss: 665.736
[104,     1] loss: 605.568
[105,     1] loss: 624.824
[106,     1] loss: 600.649
[107,     1] loss: 701.125
[108,     1] loss: 613.830
[109,     1] loss: 648.328
[110,     1] loss: 679.958
[111,     1] loss: 624.219
[112,     1] loss: 649.196
[113,     1] loss: 688.689
[114,     1] loss: 588.651
[115,     1] loss: 662.703
[116,     1] loss: 588.355
[117,     1] loss: 675.512
[118,     1] loss: 592.614
[119,     1] loss: 670.937
[120,     1] loss: 599.533
[121,     1] loss: 584.411
[122,     1] loss: 596.386
[123,     1] loss: 577.243
[124,     1] loss: 621.987
[125,     1] loss: 569.299
[126,     1] loss: 627.655
[127,     1] loss: 503.381
[128,     1] loss: 637.057
[129,     1] loss: 535.630
[130,     1] loss: 599.174
Early stopping applied (best metric=0.38693660497665405)
Finished Training
Total time taken: 18.876133680343628
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1408.174
[2,     1] loss: 1403.391
[3,     1] loss: 1402.713
[4,     1] loss: 1401.981
[5,     1] loss: 1401.655
[6,     1] loss: 1406.453
[7,     1] loss: 1396.809
[8,     1] loss: 1391.936
[9,     1] loss: 1385.489
[10,     1] loss: 1380.166
[11,     1] loss: 1371.901
[12,     1] loss: 1344.918
[13,     1] loss: 1321.553
[14,     1] loss: 1302.435
[15,     1] loss: 1268.911
[16,     1] loss: 1237.927
[17,     1] loss: 1213.230
[18,     1] loss: 1182.908
[19,     1] loss: 1157.119
[20,     1] loss: 1169.174
[21,     1] loss: 1160.677
[22,     1] loss: 1158.669
[23,     1] loss: 1137.257
[24,     1] loss: 1179.771
[25,     1] loss: 1067.566
[26,     1] loss: 1153.412
[27,     1] loss: 1113.717
[28,     1] loss: 1106.739
[29,     1] loss: 1089.510
[30,     1] loss: 1125.368
[31,     1] loss: 1144.811
[32,     1] loss: 1099.142
[33,     1] loss: 1133.290
[34,     1] loss: 1110.383
[35,     1] loss: 1083.992
[36,     1] loss: 1057.943
[37,     1] loss: 1095.407
[38,     1] loss: 1058.942
[39,     1] loss: 1062.790
[40,     1] loss: 1033.133
[41,     1] loss: 1088.201
[42,     1] loss: 1041.017
[43,     1] loss: 1003.888
[44,     1] loss: 1037.077
[45,     1] loss: 974.804
[46,     1] loss: 1049.465
[47,     1] loss: 1011.232
[48,     1] loss: 1000.701
[49,     1] loss: 948.599
[50,     1] loss: 943.750
[51,     1] loss: 1043.319
[52,     1] loss: 1023.542
[53,     1] loss: 998.968
[54,     1] loss: 1003.669
[55,     1] loss: 973.985
[56,     1] loss: 963.821
[57,     1] loss: 965.691
[58,     1] loss: 952.779
[59,     1] loss: 937.655
[60,     1] loss: 956.007
[61,     1] loss: 899.543
[62,     1] loss: 897.394
[63,     1] loss: 882.919
[64,     1] loss: 920.335
[65,     1] loss: 904.028
[66,     1] loss: 874.719
[67,     1] loss: 866.720
[68,     1] loss: 811.556
[69,     1] loss: 864.531
[70,     1] loss: 838.354
[71,     1] loss: 882.381
[72,     1] loss: 955.659
[73,     1] loss: 865.794
[74,     1] loss: 834.012
[75,     1] loss: 789.797
[76,     1] loss: 808.467
[77,     1] loss: 813.984
[78,     1] loss: 787.835
[79,     1] loss: 786.817
[80,     1] loss: 821.448
[81,     1] loss: 748.440
[82,     1] loss: 765.604
[83,     1] loss: 744.436
[84,     1] loss: 713.800
[85,     1] loss: 710.577
[86,     1] loss: 789.354
[87,     1] loss: 694.500
[88,     1] loss: 723.254
[89,     1] loss: 699.709
[90,     1] loss: 693.390
[91,     1] loss: 712.611
[92,     1] loss: 740.025
[93,     1] loss: 701.756
[94,     1] loss: 676.600
[95,     1] loss: 714.847
[96,     1] loss: 635.073
[97,     1] loss: 675.704
[98,     1] loss: 634.119
[99,     1] loss: 657.421
[100,     1] loss: 614.104
[101,     1] loss: 672.669
[102,     1] loss: 663.485
[103,     1] loss: 678.866
[104,     1] loss: 684.632
[105,     1] loss: 592.832
[106,     1] loss: 663.941
[107,     1] loss: 592.300
[108,     1] loss: 650.799
[109,     1] loss: 617.310
[110,     1] loss: 596.080
[111,     1] loss: 575.512
[112,     1] loss: 570.741
[113,     1] loss: 607.437
[114,     1] loss: 595.854
[115,     1] loss: 538.589
[116,     1] loss: 603.025
[117,     1] loss: 546.930
[118,     1] loss: 572.850
[119,     1] loss: 558.644
[120,     1] loss: 615.748
[121,     1] loss: 611.148
[122,     1] loss: 577.010
[123,     1] loss: 705.892
[124,     1] loss: 606.704
[125,     1] loss: 604.494
[126,     1] loss: 556.313
[127,     1] loss: 568.860
[128,     1] loss: 552.423
[129,     1] loss: 549.529
[130,     1] loss: 529.169
[131,     1] loss: 518.136
[132,     1] loss: 524.724
[133,     1] loss: 501.156
[134,     1] loss: 488.660
[135,     1] loss: 554.510
[136,     1] loss: 513.861
[137,     1] loss: 577.861
[138,     1] loss: 525.041
[139,     1] loss: 549.048
[140,     1] loss: 506.156
[141,     1] loss: 475.774
[142,     1] loss: 543.160
[143,     1] loss: 474.301
[144,     1] loss: 493.563
[145,     1] loss: 469.307
[146,     1] loss: 515.512
[147,     1] loss: 459.548
[148,     1] loss: 456.155
Early stopping applied (best metric=0.37949997186660767)
Finished Training
Total time taken: 21.771098136901855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1408.868
[2,     1] loss: 1407.676
[3,     1] loss: 1404.244
[4,     1] loss: 1402.639
[5,     1] loss: 1405.451
[6,     1] loss: 1404.804
[7,     1] loss: 1406.047
[8,     1] loss: 1404.237
[9,     1] loss: 1396.901
[10,     1] loss: 1394.077
[11,     1] loss: 1391.874
[12,     1] loss: 1375.097
[13,     1] loss: 1357.847
[14,     1] loss: 1359.930
[15,     1] loss: 1327.795
[16,     1] loss: 1304.664
[17,     1] loss: 1294.011
[18,     1] loss: 1268.018
[19,     1] loss: 1246.807
[20,     1] loss: 1256.129
[21,     1] loss: 1232.590
[22,     1] loss: 1213.571
[23,     1] loss: 1183.913
[24,     1] loss: 1154.249
[25,     1] loss: 1170.755
[26,     1] loss: 1177.114
[27,     1] loss: 1120.153
[28,     1] loss: 1146.716
[29,     1] loss: 1102.219
[30,     1] loss: 1174.010
[31,     1] loss: 1160.933
[32,     1] loss: 1133.964
[33,     1] loss: 1114.088
[34,     1] loss: 1077.189
[35,     1] loss: 1096.317
[36,     1] loss: 1162.582
[37,     1] loss: 1099.038
[38,     1] loss: 1105.480
[39,     1] loss: 1083.833
[40,     1] loss: 1083.822
[41,     1] loss: 1054.723
[42,     1] loss: 1035.941
[43,     1] loss: 1036.684
[44,     1] loss: 1038.861
[45,     1] loss: 969.096
[46,     1] loss: 1000.224
[47,     1] loss: 1043.292
[48,     1] loss: 969.156
[49,     1] loss: 987.451
[50,     1] loss: 955.246
[51,     1] loss: 917.534
[52,     1] loss: 976.049
[53,     1] loss: 930.934
[54,     1] loss: 944.783
[55,     1] loss: 958.109
[56,     1] loss: 955.874
[57,     1] loss: 951.017
[58,     1] loss: 902.176
[59,     1] loss: 957.578
[60,     1] loss: 901.980
[61,     1] loss: 941.035
[62,     1] loss: 918.718
[63,     1] loss: 867.607
[64,     1] loss: 845.633
[65,     1] loss: 888.854
[66,     1] loss: 876.559
[67,     1] loss: 868.104
[68,     1] loss: 798.260
[69,     1] loss: 795.887
[70,     1] loss: 791.031
[71,     1] loss: 841.064
[72,     1] loss: 784.299
[73,     1] loss: 748.077
[74,     1] loss: 805.338
[75,     1] loss: 750.589
[76,     1] loss: 735.750
[77,     1] loss: 701.716
[78,     1] loss: 729.347
[79,     1] loss: 748.057
[80,     1] loss: 809.076
[81,     1] loss: 759.130
[82,     1] loss: 703.928
[83,     1] loss: 715.708
[84,     1] loss: 702.189
[85,     1] loss: 758.936
[86,     1] loss: 706.962
[87,     1] loss: 677.244
[88,     1] loss: 675.880
[89,     1] loss: 697.905
[90,     1] loss: 620.042
[91,     1] loss: 656.287
[92,     1] loss: 696.596
[93,     1] loss: 625.837
[94,     1] loss: 590.659
[95,     1] loss: 615.414
[96,     1] loss: 601.967
[97,     1] loss: 632.933
[98,     1] loss: 632.203
[99,     1] loss: 676.308
[100,     1] loss: 635.751
[101,     1] loss: 683.976
[102,     1] loss: 639.877
[103,     1] loss: 608.454
Early stopping applied (best metric=0.4010973274707794)
Finished Training
Total time taken: 15.845651865005493
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1414.629
[2,     1] loss: 1406.164
[3,     1] loss: 1403.307
[4,     1] loss: 1407.786
[5,     1] loss: 1406.850
[6,     1] loss: 1405.245
[7,     1] loss: 1403.049
[8,     1] loss: 1399.821
[9,     1] loss: 1397.020
[10,     1] loss: 1399.636
[11,     1] loss: 1397.711
[12,     1] loss: 1392.192
[13,     1] loss: 1384.947
[14,     1] loss: 1368.077
[15,     1] loss: 1350.676
[16,     1] loss: 1332.543
[17,     1] loss: 1315.902
[18,     1] loss: 1286.444
[19,     1] loss: 1280.409
[20,     1] loss: 1256.563
[21,     1] loss: 1237.259
[22,     1] loss: 1237.002
[23,     1] loss: 1238.308
[24,     1] loss: 1212.987
[25,     1] loss: 1194.696
[26,     1] loss: 1220.126
[27,     1] loss: 1171.969
[28,     1] loss: 1182.397
[29,     1] loss: 1200.835
[30,     1] loss: 1189.916
[31,     1] loss: 1148.278
[32,     1] loss: 1155.440
[33,     1] loss: 1120.407
[34,     1] loss: 1174.283
[35,     1] loss: 1152.177
[36,     1] loss: 1126.723
[37,     1] loss: 1103.856
[38,     1] loss: 1155.729
[39,     1] loss: 1129.105
[40,     1] loss: 1100.074
[41,     1] loss: 1126.552
[42,     1] loss: 1116.352
[43,     1] loss: 1126.191
[44,     1] loss: 1084.355
[45,     1] loss: 1082.781
[46,     1] loss: 1103.069
[47,     1] loss: 1053.321
[48,     1] loss: 1021.257
[49,     1] loss: 1077.825
[50,     1] loss: 1044.230
[51,     1] loss: 1050.623
[52,     1] loss: 1001.057
[53,     1] loss: 1004.375
[54,     1] loss: 1038.938
[55,     1] loss: 1012.403
[56,     1] loss: 998.071
[57,     1] loss: 967.444
[58,     1] loss: 973.983
[59,     1] loss: 994.364
[60,     1] loss: 963.821
[61,     1] loss: 1024.677
[62,     1] loss: 963.883
[63,     1] loss: 1008.239
[64,     1] loss: 957.354
[65,     1] loss: 996.092
[66,     1] loss: 1014.293
[67,     1] loss: 996.601
[68,     1] loss: 950.121
[69,     1] loss: 925.767
[70,     1] loss: 979.927
[71,     1] loss: 969.195
[72,     1] loss: 932.550
[73,     1] loss: 894.179
[74,     1] loss: 872.162
[75,     1] loss: 863.865
[76,     1] loss: 910.770
[77,     1] loss: 861.400
[78,     1] loss: 794.151
[79,     1] loss: 863.815
[80,     1] loss: 789.029
[81,     1] loss: 960.131
[82,     1] loss: 805.644
[83,     1] loss: 829.661
[84,     1] loss: 819.553
[85,     1] loss: 868.524
[86,     1] loss: 820.382
[87,     1] loss: 756.049
[88,     1] loss: 737.640
[89,     1] loss: 796.385
[90,     1] loss: 724.709
[91,     1] loss: 745.938
[92,     1] loss: 713.411
[93,     1] loss: 703.319
[94,     1] loss: 694.394
[95,     1] loss: 739.049
[96,     1] loss: 700.349
[97,     1] loss: 685.365
[98,     1] loss: 740.506
[99,     1] loss: 716.267
[100,     1] loss: 685.008
[101,     1] loss: 645.948
[102,     1] loss: 665.451
[103,     1] loss: 671.186
[104,     1] loss: 648.538
[105,     1] loss: 634.901
[106,     1] loss: 686.724
[107,     1] loss: 671.796
[108,     1] loss: 630.527
[109,     1] loss: 665.448
[110,     1] loss: 659.273
[111,     1] loss: 641.896
[112,     1] loss: 628.877
[113,     1] loss: 642.636
[114,     1] loss: 648.133
[115,     1] loss: 610.419
[116,     1] loss: 601.912
[117,     1] loss: 636.842
[118,     1] loss: 618.983
[119,     1] loss: 600.120
[120,     1] loss: 561.213
[121,     1] loss: 597.021
[122,     1] loss: 572.565
[123,     1] loss: 578.141
[124,     1] loss: 591.557
Early stopping applied (best metric=0.3189801275730133)
Finished Training
Total time taken: 18.229246616363525
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1407.230
[2,     1] loss: 1404.379
[3,     1] loss: 1406.476
[4,     1] loss: 1406.571
[5,     1] loss: 1405.924
[6,     1] loss: 1403.747
[7,     1] loss: 1405.286
[8,     1] loss: 1404.538
[9,     1] loss: 1404.263
[10,     1] loss: 1401.925
[11,     1] loss: 1398.486
[12,     1] loss: 1399.846
[13,     1] loss: 1397.232
[14,     1] loss: 1395.325
[15,     1] loss: 1390.625
[16,     1] loss: 1385.792
[17,     1] loss: 1369.640
[18,     1] loss: 1355.264
[19,     1] loss: 1345.753
[20,     1] loss: 1316.025
[21,     1] loss: 1294.702
[22,     1] loss: 1278.808
[23,     1] loss: 1253.641
[24,     1] loss: 1224.528
[25,     1] loss: 1211.188
[26,     1] loss: 1227.476
[27,     1] loss: 1190.039
[28,     1] loss: 1199.343
[29,     1] loss: 1196.082
[30,     1] loss: 1160.664
[31,     1] loss: 1176.476
[32,     1] loss: 1166.757
[33,     1] loss: 1140.796
[34,     1] loss: 1076.385
[35,     1] loss: 1132.135
[36,     1] loss: 1163.268
[37,     1] loss: 1123.476
[38,     1] loss: 1140.003
[39,     1] loss: 1099.537
[40,     1] loss: 1111.762
[41,     1] loss: 1104.134
[42,     1] loss: 1069.025
[43,     1] loss: 1064.621
[44,     1] loss: 1078.452
[45,     1] loss: 1055.370
[46,     1] loss: 1010.574
[47,     1] loss: 1023.316
[48,     1] loss: 988.564
[49,     1] loss: 1010.650
[50,     1] loss: 1037.163
[51,     1] loss: 985.481
[52,     1] loss: 1017.473
[53,     1] loss: 998.693
[54,     1] loss: 973.310
[55,     1] loss: 996.669
[56,     1] loss: 944.610
[57,     1] loss: 993.380
[58,     1] loss: 922.137
[59,     1] loss: 905.852
[60,     1] loss: 889.658
[61,     1] loss: 857.164
[62,     1] loss: 874.562
[63,     1] loss: 901.182
[64,     1] loss: 896.950
[65,     1] loss: 905.790
[66,     1] loss: 854.292
[67,     1] loss: 843.237
[68,     1] loss: 840.599
[69,     1] loss: 830.989
[70,     1] loss: 783.556
[71,     1] loss: 838.806
[72,     1] loss: 806.399
[73,     1] loss: 779.390
[74,     1] loss: 806.764
[75,     1] loss: 754.466
[76,     1] loss: 845.462
[77,     1] loss: 777.678
[78,     1] loss: 698.865
[79,     1] loss: 700.460
[80,     1] loss: 679.191
[81,     1] loss: 663.316
[82,     1] loss: 728.746
[83,     1] loss: 732.811
[84,     1] loss: 670.363
[85,     1] loss: 739.309
[86,     1] loss: 697.699
[87,     1] loss: 802.667
[88,     1] loss: 620.155
[89,     1] loss: 709.495
[90,     1] loss: 673.274
[91,     1] loss: 692.483
[92,     1] loss: 626.370
[93,     1] loss: 668.945
[94,     1] loss: 748.797
[95,     1] loss: 577.672
[96,     1] loss: 644.430
[97,     1] loss: 628.201
[98,     1] loss: 692.799
Early stopping applied (best metric=0.40765097737312317)
Finished Training
Total time taken: 14.326418161392212
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1409.502
[2,     1] loss: 1408.820
[3,     1] loss: 1405.176
[4,     1] loss: 1406.306
[5,     1] loss: 1402.711
[6,     1] loss: 1401.427
[7,     1] loss: 1402.265
[8,     1] loss: 1390.432
[9,     1] loss: 1384.283
[10,     1] loss: 1371.030
[11,     1] loss: 1352.580
[12,     1] loss: 1332.671
[13,     1] loss: 1307.242
[14,     1] loss: 1294.255
[15,     1] loss: 1265.224
[16,     1] loss: 1265.836
[17,     1] loss: 1238.415
[18,     1] loss: 1226.699
[19,     1] loss: 1219.956
[20,     1] loss: 1187.365
[21,     1] loss: 1194.090
[22,     1] loss: 1175.036
[23,     1] loss: 1152.171
[24,     1] loss: 1171.995
[25,     1] loss: 1133.202
[26,     1] loss: 1140.140
[27,     1] loss: 1162.701
[28,     1] loss: 1175.372
[29,     1] loss: 1158.731
[30,     1] loss: 1107.292
[31,     1] loss: 1140.826
[32,     1] loss: 1137.910
[33,     1] loss: 1075.960
[34,     1] loss: 1110.514
[35,     1] loss: 1098.000
[36,     1] loss: 1063.447
[37,     1] loss: 1107.898
[38,     1] loss: 1114.160
[39,     1] loss: 1097.256
[40,     1] loss: 1140.501
[41,     1] loss: 1095.295
[42,     1] loss: 1082.813
[43,     1] loss: 1094.479
[44,     1] loss: 1013.186
[45,     1] loss: 1056.475
[46,     1] loss: 964.453
[47,     1] loss: 1038.083
[48,     1] loss: 1019.677
[49,     1] loss: 1017.682
[50,     1] loss: 994.496
[51,     1] loss: 1018.290
[52,     1] loss: 1014.324
[53,     1] loss: 988.397
[54,     1] loss: 998.994
[55,     1] loss: 945.649
[56,     1] loss: 1011.002
[57,     1] loss: 957.786
[58,     1] loss: 964.984
[59,     1] loss: 905.010
[60,     1] loss: 982.877
[61,     1] loss: 889.683
[62,     1] loss: 935.900
[63,     1] loss: 942.993
[64,     1] loss: 947.694
[65,     1] loss: 891.204
[66,     1] loss: 967.673
[67,     1] loss: 979.424
[68,     1] loss: 944.476
[69,     1] loss: 890.162
[70,     1] loss: 952.238
[71,     1] loss: 862.380
[72,     1] loss: 876.369
[73,     1] loss: 826.783
[74,     1] loss: 886.840
[75,     1] loss: 875.545
[76,     1] loss: 818.877
[77,     1] loss: 815.311
[78,     1] loss: 805.094
[79,     1] loss: 843.515
[80,     1] loss: 798.060
[81,     1] loss: 786.658
[82,     1] loss: 793.258
[83,     1] loss: 750.026
[84,     1] loss: 773.977
[85,     1] loss: 803.885
[86,     1] loss: 738.362
[87,     1] loss: 729.125
[88,     1] loss: 719.941
[89,     1] loss: 732.510
[90,     1] loss: 776.643
[91,     1] loss: 704.555
[92,     1] loss: 708.749
[93,     1] loss: 694.954
[94,     1] loss: 717.881
[95,     1] loss: 719.326
[96,     1] loss: 680.978
[97,     1] loss: 656.788
[98,     1] loss: 719.912
[99,     1] loss: 647.503
[100,     1] loss: 619.972
[101,     1] loss: 661.576
[102,     1] loss: 637.376
[103,     1] loss: 688.955
[104,     1] loss: 632.602
[105,     1] loss: 683.321
[106,     1] loss: 625.231
[107,     1] loss: 626.300
[108,     1] loss: 622.964
[109,     1] loss: 597.070
[110,     1] loss: 666.001
[111,     1] loss: 592.728
[112,     1] loss: 615.223
[113,     1] loss: 593.946
[114,     1] loss: 583.293
[115,     1] loss: 535.123
[116,     1] loss: 568.022
[117,     1] loss: 581.000
[118,     1] loss: 592.436
[119,     1] loss: 640.706
[120,     1] loss: 548.570
[121,     1] loss: 598.271
[122,     1] loss: 481.302
[123,     1] loss: 577.119
[124,     1] loss: 525.017
[125,     1] loss: 585.532
[126,     1] loss: 537.663
[127,     1] loss: 599.431
[128,     1] loss: 531.178
[129,     1] loss: 574.321
[130,     1] loss: 567.562
[131,     1] loss: 544.779
[132,     1] loss: 529.457
[133,     1] loss: 498.841
[134,     1] loss: 585.529
[135,     1] loss: 447.961
[136,     1] loss: 482.188
[137,     1] loss: 446.314
[138,     1] loss: 504.604
[139,     1] loss: 485.836
[140,     1] loss: 531.295
[141,     1] loss: 517.353
[142,     1] loss: 533.439
[143,     1] loss: 485.774
[144,     1] loss: 464.612
[145,     1] loss: 523.066
[146,     1] loss: 423.563
[147,     1] loss: 480.490
[148,     1] loss: 481.408
[149,     1] loss: 530.014
[150,     1] loss: 507.741
[151,     1] loss: 588.330
[152,     1] loss: 448.054
[153,     1] loss: 515.636
[154,     1] loss: 572.534
[155,     1] loss: 487.287
[156,     1] loss: 527.642
[157,     1] loss: 429.289
[158,     1] loss: 469.311
[159,     1] loss: 453.159
[160,     1] loss: 464.180
[161,     1] loss: 398.193
[162,     1] loss: 508.052
[163,     1] loss: 473.569
[164,     1] loss: 399.448
[165,     1] loss: 420.646
[166,     1] loss: 435.535
[167,     1] loss: 451.754
[168,     1] loss: 422.012
[169,     1] loss: 450.855
[170,     1] loss: 445.638
[171,     1] loss: 428.997
[172,     1] loss: 474.078
Early stopping applied (best metric=0.3282300531864166)
Finished Training
Total time taken: 25.09292435646057
{'Hydroxylation-K Validation Accuracy': 0.7633569739952719, 'Hydroxylation-K Validation Sensitivity': 0.6377777777777778, 'Hydroxylation-K Validation Specificity': 0.7947368421052632, 'Hydroxylation-K Validation Precision': 0.44162930751166046, 'Hydroxylation-K AUC ROC': 0.7923976608187134, 'Hydroxylation-K AUC PR': 0.5737284669477045, 'Hydroxylation-K MCC': 0.38304892050972034, 'Hydroxylation-K F1': 0.5179224132649, 'Validation Loss (Hydroxylation-K)': 0.4637230853239695, 'Hydroxylation-P Validation Accuracy': 0.7855890225538467, 'Hydroxylation-P Validation Sensitivity': 0.7896825396825398, 'Hydroxylation-P Validation Specificity': 0.7846825278068732, 'Hydroxylation-P Validation Precision': 0.44271646319920915, 'Hydroxylation-P AUC ROC': 0.8473145654258178, 'Hydroxylation-P AUC PR': 0.596081298267748, 'Hydroxylation-P MCC': 0.47225575514525625, 'Hydroxylation-P F1': 0.5663954843451425, 'Validation Loss (Hydroxylation-P)': 0.3701208750406901, 'Validation Loss (total)': 0.8338439623514812, 'TimeToTrain': 16.33215357462565}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004841030881260358,
 'learning_rate_Hydroxylation-K': 0.004148979105522933,
 'learning_rate_Hydroxylation-P': 0.0066189082531516626,
 'log_base': 1.0306105810587596,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 573790526,
 'sample_weights': [2.3617815302118053, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.334329727082375,
 'weight_decay_Hydroxylation-K': 2.2757475599310153,
 'weight_decay_Hydroxylation-P': 4.717978584973643}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17990.000
Exploding loss, terminate run (best metric=0.5317348837852478)
Finished Training
Total time taken: 0.20199871063232422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17978.166
Exploding loss, terminate run (best metric=0.5294314026832581)
Finished Training
Total time taken: 0.2149982452392578
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18011.113
Exploding loss, terminate run (best metric=0.5276713967323303)
Finished Training
Total time taken: 0.21051263809204102
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17932.783
Exploding loss, terminate run (best metric=0.527223527431488)
Finished Training
Total time taken: 0.2220001220703125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 18078.182
Exploding loss, terminate run (best metric=0.5329769253730774)
Finished Training
Total time taken: 0.20200181007385254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17975.531
Exploding loss, terminate run (best metric=0.5334638953208923)
Finished Training
Total time taken: 0.21500062942504883
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17953.020
Exploding loss, terminate run (best metric=0.5334323644638062)
Finished Training
Total time taken: 0.20800161361694336
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17945.803
Exploding loss, terminate run (best metric=0.5343823432922363)
Finished Training
Total time taken: 0.21999692916870117
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18035.994
Exploding loss, terminate run (best metric=0.5268749594688416)
Finished Training
Total time taken: 0.2089998722076416
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17973.594
Exploding loss, terminate run (best metric=0.5287441611289978)
Finished Training
Total time taken: 0.2160358428955078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 18071.559
Exploding loss, terminate run (best metric=0.533732533454895)
Finished Training
Total time taken: 0.20800256729125977
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17986.254
Exploding loss, terminate run (best metric=0.5290467739105225)
Finished Training
Total time taken: 0.22400116920471191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17956.814
Exploding loss, terminate run (best metric=0.5345292091369629)
Finished Training
Total time taken: 0.20903754234313965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17993.080
Exploding loss, terminate run (best metric=0.5298619270324707)
Finished Training
Total time taken: 0.21399736404418945
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17988.125
Exploding loss, terminate run (best metric=0.5292128920555115)
Finished Training
Total time taken: 0.2070024013519287
{'Hydroxylation-K Validation Accuracy': 0.5491430260047281, 'Hydroxylation-K Validation Sensitivity': 0.4066666666666667, 'Hydroxylation-K Validation Specificity': 0.5894736842105263, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6973294346978557, 'Hydroxylation-K AUC PR': 0.38171093784824406, 'Hydroxylation-K MCC': -0.004441183064849528, 'Hydroxylation-K F1': 0.13953443446344055, 'Validation Loss (Hydroxylation-K)': 0.5560048381487529, 'Hydroxylation-P Validation Accuracy': 0.5639386156371081, 'Hydroxylation-P Validation Sensitivity': 0.4057142857142857, 'Hydroxylation-P Validation Specificity': 0.5979674796747968, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6576382386961468, 'Hydroxylation-P AUC PR': 0.33504179499517134, 'Hydroxylation-P MCC': 0.007135926691959258, 'Hydroxylation-P F1': 0.12952759306788347, 'Validation Loss (Hydroxylation-P)': 0.5308212796847026, 'Validation Loss (total)': 1.0868261257807414, 'TimeToTrain': 0.21210583051045737}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00745029200015826,
 'learning_rate_Hydroxylation-K': 0.007592965938187959,
 'learning_rate_Hydroxylation-P': 0.007922451578266567,
 'log_base': 1.549582488244214,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2357361614,
 'sample_weights': [55.40970640270472, 6.911821852825451],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.10991305580224942,
 'weight_decay_Hydroxylation-K': 5.4359906101644,
 'weight_decay_Hydroxylation-P': 7.71276502698062}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1710.109
[2,     1] loss: 1731.807
[3,     1] loss: 1712.158
[4,     1] loss: 1714.632
[5,     1] loss: 1709.545
[6,     1] loss: 1710.814
[7,     1] loss: 1715.910
[8,     1] loss: 1709.365
[9,     1] loss: 1709.850
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008290354799985274,
 'learning_rate_Hydroxylation-K': 0.006123012932140904,
 'learning_rate_Hydroxylation-P': 0.008846583406996612,
 'log_base': 1.0599999352190166,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3529495092,
 'sample_weights': [3.811639939220169, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0937011946128434,
 'weight_decay_Hydroxylation-K': 5.195342694467121,
 'weight_decay_Hydroxylation-P': 7.767518171245912}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9332.145
[2,     1] loss: 9321.713
[3,     1] loss: 9299.672
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009806999368071119,
 'learning_rate_Hydroxylation-K': 0.007609868028516975,
 'learning_rate_Hydroxylation-P': 0.005561214185393042,
 'log_base': 1.7777873854029467,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2797240373,
 'sample_weights': [28.650698153368115, 3.5814752366639446],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7308163784785959,
 'weight_decay_Hydroxylation-K': 6.9669879200354154,
 'weight_decay_Hydroxylation-P': 6.217757644895998}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1529.155
[2,     1] loss: 1522.406
[3,     1] loss: 1514.943
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007603373621915378,
 'learning_rate_Hydroxylation-K': 0.0053582911704138855,
 'learning_rate_Hydroxylation-P': 0.004381494709610252,
 'log_base': 1.3747560801648744,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 633120675,
 'sample_weights': [2.9015146001528587, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9084224518226067,
 'weight_decay_Hydroxylation-K': 4.115385759780789,
 'weight_decay_Hydroxylation-P': 6.306200955827949}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2022.165
[2,     1] loss: 2024.936
[3,     1] loss: 2019.390
[4,     1] loss: 2015.972
[5,     1] loss: 2015.868
[6,     1] loss: 2018.661
[7,     1] loss: 2018.129
[8,     1] loss: 2015.587
[9,     1] loss: 2016.968
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0009584381667183138,
 'learning_rate_Hydroxylation-K': 0.009845513648385287,
 'learning_rate_Hydroxylation-P': 0.001214082643977164,
 'log_base': 1.1779804941174126,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3308726201,
 'sample_weights': [5.245263461459753, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.971504100460438,
 'weight_decay_Hydroxylation-K': 9.49420594964041,
 'weight_decay_Hydroxylation-P': 1.5436823102077941}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3329.199
[2,     1] loss: 3323.919
[3,     1] loss: 3311.818
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008940580201792434,
 'learning_rate_Hydroxylation-K': 0.002961909735840085,
 'learning_rate_Hydroxylation-P': 0.007398852522678783,
 'log_base': 1.2651828445926672,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3442785261,
 'sample_weights': [10.191865618662712, 1.2740322812816367],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.3612450913699522,
 'weight_decay_Hydroxylation-K': 4.034411982571612,
 'weight_decay_Hydroxylation-P': 8.7095568141931}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2415.461
[2,     1] loss: 2403.276
[3,     1] loss: 2403.977
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004436392247577751,
 'learning_rate_Hydroxylation-K': 0.005338235720726968,
 'learning_rate_Hydroxylation-P': 0.007823332663310712,
 'log_base': 1.11835635766385,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3956750175,
 'sample_weights': [7.097470041933225, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.948411284452541,
 'weight_decay_Hydroxylation-K': 1.2070206186700272,
 'weight_decay_Hydroxylation-P': 3.0233088285360745}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4857.043
[2,     1] loss: 4837.463
[3,     1] loss: 4849.132
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004791752940295668,
 'learning_rate_Hydroxylation-K': 0.0037882605539643205,
 'learning_rate_Hydroxylation-P': 0.009921087441979812,
 'log_base': 1.2277610086370696,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 42836436,
 'sample_weights': [14.924388596539492, 1.8656204429898797],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.807482421227977,
 'weight_decay_Hydroxylation-K': 0.7778204086788225,
 'weight_decay_Hydroxylation-P': 8.651450363186964}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2633.316
[2,     1] loss: 2642.752
[3,     1] loss: 2635.253
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009579740112445787,
 'learning_rate_Hydroxylation-K': 0.003017794025878965,
 'learning_rate_Hydroxylation-P': 0.005091415381033377,
 'log_base': 1.1102389767352807,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1925499259,
 'sample_weights': [8.135997416913915, 1.0170388560256953],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.629976808903544,
 'weight_decay_Hydroxylation-K': 2.8131292436605637,
 'weight_decay_Hydroxylation-P': 7.186350892018833}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5166.728
[2,     1] loss: 5220.780
[3,     1] loss: 5196.928
[4,     1] loss: 5202.753
[5,     1] loss: 5193.706
[6,     1] loss: 5173.393
[7,     1] loss: 5178.951
[8,     1] loss: 5202.248
[9,     1] loss: 5156.058
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00592671307466878,
 'learning_rate_Hydroxylation-K': 0.0014154376630614827,
 'learning_rate_Hydroxylation-P': 0.009739643249584876,
 'log_base': 1.0361155613048891,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 406286639,
 'sample_weights': [15.964031303881672, 1.9955807878091405],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.750855924541414,
 'weight_decay_Hydroxylation-K': 2.666769598074813,
 'weight_decay_Hydroxylation-P': 1.3632038318212225}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15252.957
Exploding loss, terminate run (best metric=0.5319787263870239)
Finished Training
Total time taken: 0.20800185203552246
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15274.207
Exploding loss, terminate run (best metric=0.5271486639976501)
Finished Training
Total time taken: 0.207000732421875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15308.289
Exploding loss, terminate run (best metric=0.5340185165405273)
Finished Training
Total time taken: 0.21300077438354492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15430.105
Exploding loss, terminate run (best metric=0.5280064940452576)
Finished Training
Total time taken: 0.21400022506713867
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15422.530
Exploding loss, terminate run (best metric=0.5273653268814087)
Finished Training
Total time taken: 0.21600079536437988
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15294.426
Exploding loss, terminate run (best metric=0.5359905362129211)
Finished Training
Total time taken: 0.21899938583374023
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15349.371
Exploding loss, terminate run (best metric=0.5375024676322937)
Finished Training
Total time taken: 0.21204352378845215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15355.189
Exploding loss, terminate run (best metric=0.5296233892440796)
Finished Training
Total time taken: 0.22300052642822266
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15271.426
Exploding loss, terminate run (best metric=0.5315822958946228)
Finished Training
Total time taken: 0.2070002555847168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15310.326
Exploding loss, terminate run (best metric=0.534537136554718)
Finished Training
Total time taken: 0.2090001106262207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15327.357
Exploding loss, terminate run (best metric=0.5319886207580566)
Finished Training
Total time taken: 0.2069993019104004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15389.211
Exploding loss, terminate run (best metric=0.5330899953842163)
Finished Training
Total time taken: 0.21799898147583008
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15277.770
Exploding loss, terminate run (best metric=0.5302317142486572)
Finished Training
Total time taken: 0.2090003490447998
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15277.014
Exploding loss, terminate run (best metric=0.5297568440437317)
Finished Training
Total time taken: 0.21000289916992188
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15412.990
Exploding loss, terminate run (best metric=0.5274710655212402)
Finished Training
Total time taken: 0.21300601959228516
{'Hydroxylation-K Validation Accuracy': 0.5569444444444445, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.5982456140350877, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5805068226120857, 'Hydroxylation-K AUC PR': 0.3202767449683927, 'Hydroxylation-K MCC': -0.004988477453464162, 'Hydroxylation-K F1': 0.13325123152709362, 'Validation Loss (Hydroxylation-K)': 0.5565036018689473, 'Hydroxylation-P Validation Accuracy': 0.5651580461228702, 'Hydroxylation-P Validation Sensitivity': 0.4, 'Hydroxylation-P Validation Specificity': 0.6, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5663203168111144, 'Hydroxylation-P AUC PR': 0.27801115163422524, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.12062123059431565, 'Validation Loss (Hydroxylation-P)': 0.5313527862230937, 'Validation Loss (total)': 1.087856396039327, 'TimeToTrain': 0.21233704884847004}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003403622789039972,
 'learning_rate_Hydroxylation-K': 0.0026863000063550796,
 'learning_rate_Hydroxylation-P': 0.0026957894624502735,
 'log_base': 2.343544911629,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3765420279,
 'sample_weights': [47.08972778390506, 5.873985456174684],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.053374280905442,
 'weight_decay_Hydroxylation-K': 3.5860621133143997,
 'weight_decay_Hydroxylation-P': 0.8375004527200249}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1320.979
[2,     1] loss: 1318.377
[3,     1] loss: 1317.714
[4,     1] loss: 1317.145
[5,     1] loss: 1315.148
[6,     1] loss: 1305.307
[7,     1] loss: 1300.573
[8,     1] loss: 1273.457
[9,     1] loss: 1237.177
[10,     1] loss: 1225.731
[11,     1] loss: 1180.855
[12,     1] loss: 1175.714
[13,     1] loss: 1121.850
[14,     1] loss: 1133.978
[15,     1] loss: 1093.349
[16,     1] loss: 1075.735
[17,     1] loss: 1096.177
[18,     1] loss: 1082.918
[19,     1] loss: 1107.197
[20,     1] loss: 1089.761
[21,     1] loss: 1059.859
[22,     1] loss: 1089.891
[23,     1] loss: 1053.182
[24,     1] loss: 1053.198
[25,     1] loss: 991.871
[26,     1] loss: 1008.159
[27,     1] loss: 1020.848
[28,     1] loss: 1024.507
[29,     1] loss: 1008.498
[30,     1] loss: 941.175
[31,     1] loss: 1015.915
[32,     1] loss: 1014.138
[33,     1] loss: 986.751
[34,     1] loss: 939.452
[35,     1] loss: 992.439
[36,     1] loss: 911.935
[37,     1] loss: 985.859
[38,     1] loss: 952.013
[39,     1] loss: 940.630
[40,     1] loss: 896.927
[41,     1] loss: 918.722
[42,     1] loss: 873.248
[43,     1] loss: 867.562
[44,     1] loss: 864.353
[45,     1] loss: 850.950
[46,     1] loss: 839.708
[47,     1] loss: 838.765
[48,     1] loss: 894.853
[49,     1] loss: 945.620
[50,     1] loss: 766.377
[51,     1] loss: 968.337
[52,     1] loss: 838.061
[53,     1] loss: 933.139
[54,     1] loss: 767.710
[55,     1] loss: 843.490
[56,     1] loss: 810.733
[57,     1] loss: 761.904
[58,     1] loss: 786.379
[59,     1] loss: 746.893
[60,     1] loss: 814.009
[61,     1] loss: 761.441
[62,     1] loss: 786.738
[63,     1] loss: 720.747
[64,     1] loss: 882.171
[65,     1] loss: 733.782
[66,     1] loss: 775.275
[67,     1] loss: 691.533
[68,     1] loss: 761.679
[69,     1] loss: 712.952
[70,     1] loss: 760.620
[71,     1] loss: 625.241
[72,     1] loss: 748.549
[73,     1] loss: 629.465
[74,     1] loss: 740.953
[75,     1] loss: 661.906
[76,     1] loss: 645.959
[77,     1] loss: 606.735
[78,     1] loss: 588.603
[79,     1] loss: 562.246
[80,     1] loss: 582.876
[81,     1] loss: 585.046
[82,     1] loss: 571.488
[83,     1] loss: 661.496
[84,     1] loss: 822.155
[85,     1] loss: 567.859
[86,     1] loss: 682.998
[87,     1] loss: 681.718
[88,     1] loss: 628.928
[89,     1] loss: 606.395
[90,     1] loss: 619.595
[91,     1] loss: 558.863
[92,     1] loss: 604.162
[93,     1] loss: 549.166
[94,     1] loss: 535.573
[95,     1] loss: 626.907
[96,     1] loss: 484.709
[97,     1] loss: 577.545
[98,     1] loss: 490.923
[99,     1] loss: 504.937
[100,     1] loss: 572.094
[101,     1] loss: 468.172
Early stopping applied (best metric=0.28106966614723206)
Finished Training
Total time taken: 14.817639589309692
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1324.997
[2,     1] loss: 1326.067
[3,     1] loss: 1318.716
[4,     1] loss: 1320.323
[5,     1] loss: 1317.081
[6,     1] loss: 1315.446
[7,     1] loss: 1303.990
[8,     1] loss: 1292.763
[9,     1] loss: 1269.528
[10,     1] loss: 1227.047
[11,     1] loss: 1194.644
[12,     1] loss: 1184.139
[13,     1] loss: 1146.569
[14,     1] loss: 1127.785
[15,     1] loss: 1080.823
[16,     1] loss: 1101.304
[17,     1] loss: 1139.590
[18,     1] loss: 1131.516
[19,     1] loss: 1069.095
[20,     1] loss: 1074.960
[21,     1] loss: 1055.821
[22,     1] loss: 1100.238
[23,     1] loss: 1058.761
[24,     1] loss: 1022.643
[25,     1] loss: 1076.570
[26,     1] loss: 1052.506
[27,     1] loss: 1021.510
[28,     1] loss: 981.028
[29,     1] loss: 998.925
[30,     1] loss: 1007.791
[31,     1] loss: 1001.006
[32,     1] loss: 1006.358
[33,     1] loss: 947.243
[34,     1] loss: 960.193
[35,     1] loss: 978.100
[36,     1] loss: 951.624
[37,     1] loss: 945.812
[38,     1] loss: 868.569
[39,     1] loss: 892.126
[40,     1] loss: 913.687
[41,     1] loss: 849.638
[42,     1] loss: 878.888
[43,     1] loss: 947.418
[44,     1] loss: 887.426
[45,     1] loss: 834.835
[46,     1] loss: 828.143
[47,     1] loss: 800.319
[48,     1] loss: 776.442
[49,     1] loss: 820.248
[50,     1] loss: 869.715
[51,     1] loss: 766.609
[52,     1] loss: 775.266
[53,     1] loss: 786.259
[54,     1] loss: 769.869
[55,     1] loss: 771.036
[56,     1] loss: 703.453
[57,     1] loss: 716.782
[58,     1] loss: 692.284
[59,     1] loss: 713.168
[60,     1] loss: 693.925
[61,     1] loss: 700.828
[62,     1] loss: 867.928
[63,     1] loss: 977.693
[64,     1] loss: 691.556
[65,     1] loss: 913.044
[66,     1] loss: 741.844
[67,     1] loss: 831.309
[68,     1] loss: 828.324
[69,     1] loss: 751.012
[70,     1] loss: 836.724
[71,     1] loss: 704.060
[72,     1] loss: 663.683
[73,     1] loss: 680.837
[74,     1] loss: 647.160
[75,     1] loss: 642.005
[76,     1] loss: 624.297
[77,     1] loss: 605.345
[78,     1] loss: 589.713
[79,     1] loss: 560.017
[80,     1] loss: 531.942
[81,     1] loss: 582.649
[82,     1] loss: 609.199
[83,     1] loss: 536.044
[84,     1] loss: 580.978
[85,     1] loss: 528.448
[86,     1] loss: 491.013
[87,     1] loss: 612.051
[88,     1] loss: 622.766
[89,     1] loss: 581.923
[90,     1] loss: 521.648
[91,     1] loss: 663.101
[92,     1] loss: 643.861
[93,     1] loss: 480.172
[94,     1] loss: 643.777
[95,     1] loss: 459.002
[96,     1] loss: 569.808
[97,     1] loss: 468.752
[98,     1] loss: 529.597
[99,     1] loss: 451.823
[100,     1] loss: 449.359
[101,     1] loss: 463.227
[102,     1] loss: 402.633
[103,     1] loss: 425.109
[104,     1] loss: 439.466
[105,     1] loss: 369.784
[106,     1] loss: 395.113
[107,     1] loss: 395.865
[108,     1] loss: 378.465
[109,     1] loss: 456.027
[110,     1] loss: 478.485
[111,     1] loss: 635.122
[112,     1] loss: 447.112
[113,     1] loss: 460.151
[114,     1] loss: 538.398
[115,     1] loss: 413.492
[116,     1] loss: 633.627
[117,     1] loss: 471.003
[118,     1] loss: 592.728
[119,     1] loss: 456.224
[120,     1] loss: 505.898
[121,     1] loss: 461.128
[122,     1] loss: 510.054
Early stopping applied (best metric=0.3575604259967804)
Finished Training
Total time taken: 17.84877920150757
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.199
[2,     1] loss: 1323.904
[3,     1] loss: 1323.959
[4,     1] loss: 1319.924
[5,     1] loss: 1320.146
[6,     1] loss: 1320.660
[7,     1] loss: 1315.046
[8,     1] loss: 1313.798
[9,     1] loss: 1307.543
[10,     1] loss: 1291.358
[11,     1] loss: 1284.390
[12,     1] loss: 1250.060
[13,     1] loss: 1220.842
[14,     1] loss: 1183.381
[15,     1] loss: 1160.982
[16,     1] loss: 1140.402
[17,     1] loss: 1101.679
[18,     1] loss: 1113.292
[19,     1] loss: 1137.820
[20,     1] loss: 1121.311
[21,     1] loss: 1098.940
[22,     1] loss: 1116.266
[23,     1] loss: 1061.450
[24,     1] loss: 1042.011
[25,     1] loss: 1060.772
[26,     1] loss: 1057.149
[27,     1] loss: 1062.523
[28,     1] loss: 1044.524
[29,     1] loss: 963.354
[30,     1] loss: 991.992
[31,     1] loss: 1055.394
[32,     1] loss: 1011.555
[33,     1] loss: 982.155
[34,     1] loss: 908.711
[35,     1] loss: 991.610
[36,     1] loss: 956.410
[37,     1] loss: 941.340
[38,     1] loss: 926.220
[39,     1] loss: 961.071
[40,     1] loss: 950.293
[41,     1] loss: 909.954
[42,     1] loss: 885.104
[43,     1] loss: 891.141
[44,     1] loss: 895.507
[45,     1] loss: 867.765
[46,     1] loss: 909.701
[47,     1] loss: 959.793
[48,     1] loss: 805.292
[49,     1] loss: 954.965
[50,     1] loss: 806.937
[51,     1] loss: 840.820
[52,     1] loss: 861.426
[53,     1] loss: 856.347
[54,     1] loss: 780.588
[55,     1] loss: 819.870
[56,     1] loss: 801.642
[57,     1] loss: 761.543
[58,     1] loss: 835.761
[59,     1] loss: 765.311
[60,     1] loss: 743.036
[61,     1] loss: 719.991
[62,     1] loss: 726.171
[63,     1] loss: 729.695
[64,     1] loss: 677.379
[65,     1] loss: 935.820
[66,     1] loss: 705.899
[67,     1] loss: 685.896
[68,     1] loss: 708.068
[69,     1] loss: 665.098
[70,     1] loss: 656.428
[71,     1] loss: 642.373
[72,     1] loss: 610.697
[73,     1] loss: 582.246
[74,     1] loss: 591.923
[75,     1] loss: 596.542
[76,     1] loss: 533.727
[77,     1] loss: 641.872
[78,     1] loss: 683.924
[79,     1] loss: 561.710
[80,     1] loss: 530.446
[81,     1] loss: 573.898
[82,     1] loss: 591.824
[83,     1] loss: 566.510
[84,     1] loss: 634.862
[85,     1] loss: 598.852
[86,     1] loss: 531.771
[87,     1] loss: 556.730
[88,     1] loss: 535.898
[89,     1] loss: 502.621
[90,     1] loss: 503.581
[91,     1] loss: 459.442
[92,     1] loss: 494.975
[93,     1] loss: 523.293
[94,     1] loss: 479.292
[95,     1] loss: 508.046
[96,     1] loss: 429.766
[97,     1] loss: 497.411
[98,     1] loss: 525.833
[99,     1] loss: 474.142
[100,     1] loss: 440.495
[101,     1] loss: 487.474
[102,     1] loss: 401.762
[103,     1] loss: 479.385
[104,     1] loss: 517.142
[105,     1] loss: 387.298
[106,     1] loss: 399.860
Early stopping applied (best metric=0.363377183675766)
Finished Training
Total time taken: 15.53959846496582
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1336.883
[2,     1] loss: 1324.872
[3,     1] loss: 1322.747
[4,     1] loss: 1326.183
[5,     1] loss: 1320.789
[6,     1] loss: 1321.530
[7,     1] loss: 1317.809
[8,     1] loss: 1312.758
[9,     1] loss: 1303.730
[10,     1] loss: 1303.445
[11,     1] loss: 1281.815
[12,     1] loss: 1261.642
[13,     1] loss: 1239.673
[14,     1] loss: 1202.219
[15,     1] loss: 1186.003
[16,     1] loss: 1157.435
[17,     1] loss: 1131.624
[18,     1] loss: 1142.659
[19,     1] loss: 1108.532
[20,     1] loss: 1091.987
[21,     1] loss: 1092.969
[22,     1] loss: 1059.386
[23,     1] loss: 1067.757
[24,     1] loss: 1078.936
[25,     1] loss: 1021.734
[26,     1] loss: 1057.200
[27,     1] loss: 1004.477
[28,     1] loss: 1001.990
[29,     1] loss: 1038.791
[30,     1] loss: 1007.703
[31,     1] loss: 1020.490
[32,     1] loss: 1034.159
[33,     1] loss: 996.654
[34,     1] loss: 980.554
[35,     1] loss: 964.126
[36,     1] loss: 935.704
[37,     1] loss: 961.654
[38,     1] loss: 969.024
[39,     1] loss: 883.502
[40,     1] loss: 887.614
[41,     1] loss: 919.333
[42,     1] loss: 879.944
[43,     1] loss: 859.084
[44,     1] loss: 915.952
[45,     1] loss: 855.954
[46,     1] loss: 908.469
[47,     1] loss: 826.254
[48,     1] loss: 864.096
[49,     1] loss: 826.000
[50,     1] loss: 791.647
[51,     1] loss: 794.159
[52,     1] loss: 811.413
[53,     1] loss: 785.869
[54,     1] loss: 767.349
[55,     1] loss: 704.313
[56,     1] loss: 750.885
[57,     1] loss: 704.085
[58,     1] loss: 742.911
[59,     1] loss: 902.647
[60,     1] loss: 1080.320
[61,     1] loss: 741.501
[62,     1] loss: 763.614
[63,     1] loss: 745.925
[64,     1] loss: 821.140
[65,     1] loss: 757.081
[66,     1] loss: 726.408
[67,     1] loss: 760.261
[68,     1] loss: 678.620
[69,     1] loss: 753.727
[70,     1] loss: 658.887
[71,     1] loss: 703.089
[72,     1] loss: 648.335
[73,     1] loss: 752.055
[74,     1] loss: 625.590
[75,     1] loss: 656.948
[76,     1] loss: 691.018
[77,     1] loss: 597.506
[78,     1] loss: 650.391
[79,     1] loss: 577.969
[80,     1] loss: 614.591
[81,     1] loss: 618.292
[82,     1] loss: 560.087
[83,     1] loss: 543.783
[84,     1] loss: 527.561
[85,     1] loss: 539.740
[86,     1] loss: 426.435
[87,     1] loss: 491.603
[88,     1] loss: 490.290
[89,     1] loss: 493.759
[90,     1] loss: 534.189
[91,     1] loss: 610.730
[92,     1] loss: 699.084
[93,     1] loss: 612.949
[94,     1] loss: 498.158
[95,     1] loss: 586.634
[96,     1] loss: 518.279
[97,     1] loss: 538.036
[98,     1] loss: 501.171
Early stopping applied (best metric=0.40924301743507385)
Finished Training
Total time taken: 14.397582292556763
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1334.364
[2,     1] loss: 1321.787
[3,     1] loss: 1322.227
[4,     1] loss: 1320.596
[5,     1] loss: 1320.275
[6,     1] loss: 1324.636
[7,     1] loss: 1320.158
[8,     1] loss: 1315.172
[9,     1] loss: 1314.135
[10,     1] loss: 1307.482
[11,     1] loss: 1292.424
[12,     1] loss: 1271.830
[13,     1] loss: 1232.952
[14,     1] loss: 1185.850
[15,     1] loss: 1143.697
[16,     1] loss: 1134.688
[17,     1] loss: 1085.156
[18,     1] loss: 1094.080
[19,     1] loss: 1069.151
[20,     1] loss: 1053.156
[21,     1] loss: 1059.340
[22,     1] loss: 1103.880
[23,     1] loss: 1038.428
[24,     1] loss: 1014.672
[25,     1] loss: 1002.010
[26,     1] loss: 1060.350
[27,     1] loss: 1048.849
[28,     1] loss: 1003.827
[29,     1] loss: 982.290
[30,     1] loss: 987.221
[31,     1] loss: 950.412
[32,     1] loss: 945.239
[33,     1] loss: 924.000
[34,     1] loss: 897.096
[35,     1] loss: 926.076
[36,     1] loss: 900.336
[37,     1] loss: 923.188
[38,     1] loss: 880.466
[39,     1] loss: 853.435
[40,     1] loss: 853.590
[41,     1] loss: 904.077
[42,     1] loss: 863.432
[43,     1] loss: 858.391
[44,     1] loss: 922.385
[45,     1] loss: 830.609
[46,     1] loss: 860.197
[47,     1] loss: 936.504
[48,     1] loss: 783.104
[49,     1] loss: 849.838
[50,     1] loss: 836.844
[51,     1] loss: 862.168
[52,     1] loss: 792.829
[53,     1] loss: 799.528
[54,     1] loss: 732.044
[55,     1] loss: 750.109
[56,     1] loss: 671.470
[57,     1] loss: 724.673
[58,     1] loss: 655.661
[59,     1] loss: 675.440
[60,     1] loss: 828.366
[61,     1] loss: 1143.303
[62,     1] loss: 722.707
[63,     1] loss: 837.945
[64,     1] loss: 746.949
[65,     1] loss: 808.805
[66,     1] loss: 806.563
[67,     1] loss: 749.585
[68,     1] loss: 728.020
[69,     1] loss: 797.050
[70,     1] loss: 712.188
[71,     1] loss: 696.697
[72,     1] loss: 702.084
[73,     1] loss: 631.315
[74,     1] loss: 737.831
[75,     1] loss: 657.660
[76,     1] loss: 658.566
[77,     1] loss: 627.543
[78,     1] loss: 625.992
[79,     1] loss: 611.416
[80,     1] loss: 624.415
[81,     1] loss: 553.135
[82,     1] loss: 566.633
[83,     1] loss: 520.190
[84,     1] loss: 493.309
[85,     1] loss: 516.564
[86,     1] loss: 503.442
[87,     1] loss: 525.705
[88,     1] loss: 490.819
[89,     1] loss: 463.458
[90,     1] loss: 520.077
[91,     1] loss: 488.927
[92,     1] loss: 581.859
[93,     1] loss: 537.181
[94,     1] loss: 511.817
[95,     1] loss: 482.621
[96,     1] loss: 553.212
[97,     1] loss: 571.108
[98,     1] loss: 454.741
[99,     1] loss: 567.468
[100,     1] loss: 473.385
[101,     1] loss: 565.687
[102,     1] loss: 659.960
[103,     1] loss: 461.261
[104,     1] loss: 540.372
[105,     1] loss: 449.251
[106,     1] loss: 444.549
[107,     1] loss: 473.796
[108,     1] loss: 415.510
[109,     1] loss: 510.609
[110,     1] loss: 565.177
[111,     1] loss: 389.372
[112,     1] loss: 534.165
[113,     1] loss: 518.734
[114,     1] loss: 407.916
[115,     1] loss: 571.695
[116,     1] loss: 403.534
[117,     1] loss: 457.335
[118,     1] loss: 407.862
Early stopping applied (best metric=0.3903995156288147)
Finished Training
Total time taken: 17.131759881973267
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.109
[2,     1] loss: 1322.510
[3,     1] loss: 1320.925
[4,     1] loss: 1315.171
[5,     1] loss: 1308.870
[6,     1] loss: 1294.518
[7,     1] loss: 1271.517
[8,     1] loss: 1233.705
[9,     1] loss: 1185.674
[10,     1] loss: 1137.325
[11,     1] loss: 1139.134
[12,     1] loss: 1090.655
[13,     1] loss: 1026.898
[14,     1] loss: 1121.985
[15,     1] loss: 1096.745
[16,     1] loss: 1057.799
[17,     1] loss: 1035.931
[18,     1] loss: 1003.867
[19,     1] loss: 1069.467
[20,     1] loss: 1014.239
[21,     1] loss: 994.293
[22,     1] loss: 999.499
[23,     1] loss: 990.664
[24,     1] loss: 937.817
[25,     1] loss: 1010.661
[26,     1] loss: 956.973
[27,     1] loss: 987.353
[28,     1] loss: 932.721
[29,     1] loss: 929.256
[30,     1] loss: 928.964
[31,     1] loss: 936.794
[32,     1] loss: 920.329
[33,     1] loss: 899.268
[34,     1] loss: 882.060
[35,     1] loss: 858.347
[36,     1] loss: 851.895
[37,     1] loss: 880.202
[38,     1] loss: 828.502
[39,     1] loss: 815.463
[40,     1] loss: 820.104
[41,     1] loss: 872.294
[42,     1] loss: 867.080
[43,     1] loss: 742.726
[44,     1] loss: 742.142
[45,     1] loss: 754.263
[46,     1] loss: 741.725
[47,     1] loss: 692.206
[48,     1] loss: 783.431
[49,     1] loss: 740.691
[50,     1] loss: 719.303
[51,     1] loss: 699.362
[52,     1] loss: 678.153
[53,     1] loss: 678.732
[54,     1] loss: 671.645
[55,     1] loss: 684.057
[56,     1] loss: 691.886
[57,     1] loss: 678.686
[58,     1] loss: 716.818
[59,     1] loss: 760.245
[60,     1] loss: 613.257
[61,     1] loss: 617.622
[62,     1] loss: 663.450
[63,     1] loss: 577.857
[64,     1] loss: 579.304
[65,     1] loss: 626.530
[66,     1] loss: 548.127
[67,     1] loss: 557.484
[68,     1] loss: 614.509
[69,     1] loss: 700.093
[70,     1] loss: 674.922
[71,     1] loss: 578.368
[72,     1] loss: 650.128
[73,     1] loss: 602.125
[74,     1] loss: 600.297
[75,     1] loss: 632.237
[76,     1] loss: 607.416
[77,     1] loss: 622.995
[78,     1] loss: 495.253
[79,     1] loss: 626.848
[80,     1] loss: 524.248
[81,     1] loss: 562.253
[82,     1] loss: 555.788
[83,     1] loss: 545.480
[84,     1] loss: 533.118
[85,     1] loss: 466.147
[86,     1] loss: 575.503
[87,     1] loss: 516.169
[88,     1] loss: 494.973
[89,     1] loss: 447.925
[90,     1] loss: 508.537
[91,     1] loss: 412.836
[92,     1] loss: 474.006
[93,     1] loss: 429.930
[94,     1] loss: 484.052
[95,     1] loss: 478.262
[96,     1] loss: 425.262
[97,     1] loss: 432.598
[98,     1] loss: 431.340
[99,     1] loss: 448.079
[100,     1] loss: 439.812
[101,     1] loss: 389.034
[102,     1] loss: 401.715
[103,     1] loss: 384.644
[104,     1] loss: 387.639
[105,     1] loss: 414.089
Early stopping applied (best metric=0.38194236159324646)
Finished Training
Total time taken: 15.427563905715942
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1320.424
[2,     1] loss: 1326.240
[3,     1] loss: 1321.936
[4,     1] loss: 1316.447
[5,     1] loss: 1313.902
[6,     1] loss: 1300.880
[7,     1] loss: 1284.501
[8,     1] loss: 1253.636
[9,     1] loss: 1232.547
[10,     1] loss: 1186.281
[11,     1] loss: 1158.225
[12,     1] loss: 1147.549
[13,     1] loss: 1131.656
[14,     1] loss: 1086.602
[15,     1] loss: 1097.981
[16,     1] loss: 1090.702
[17,     1] loss: 1067.659
[18,     1] loss: 1046.772
[19,     1] loss: 1089.965
[20,     1] loss: 1018.794
[21,     1] loss: 1069.375
[22,     1] loss: 1005.264
[23,     1] loss: 1027.957
[24,     1] loss: 1030.178
[25,     1] loss: 1029.270
[26,     1] loss: 1031.557
[27,     1] loss: 962.730
[28,     1] loss: 1005.179
[29,     1] loss: 951.505
[30,     1] loss: 924.508
[31,     1] loss: 917.040
[32,     1] loss: 915.525
[33,     1] loss: 964.020
[34,     1] loss: 899.555
[35,     1] loss: 993.589
[36,     1] loss: 890.812
[37,     1] loss: 926.223
[38,     1] loss: 881.506
[39,     1] loss: 841.586
[40,     1] loss: 881.757
[41,     1] loss: 959.319
[42,     1] loss: 1018.572
[43,     1] loss: 852.688
[44,     1] loss: 979.086
[45,     1] loss: 861.345
[46,     1] loss: 949.063
[47,     1] loss: 856.703
[48,     1] loss: 838.153
[49,     1] loss: 867.703
[50,     1] loss: 828.207
[51,     1] loss: 790.257
[52,     1] loss: 806.193
[53,     1] loss: 784.389
[54,     1] loss: 873.453
[55,     1] loss: 748.586
[56,     1] loss: 782.348
[57,     1] loss: 717.436
[58,     1] loss: 787.432
[59,     1] loss: 695.913
[60,     1] loss: 803.869
[61,     1] loss: 693.000
[62,     1] loss: 721.454
[63,     1] loss: 655.589
[64,     1] loss: 730.192
[65,     1] loss: 681.528
[66,     1] loss: 730.617
[67,     1] loss: 650.961
[68,     1] loss: 665.553
[69,     1] loss: 707.021
[70,     1] loss: 597.467
[71,     1] loss: 639.987
[72,     1] loss: 641.858
[73,     1] loss: 593.821
[74,     1] loss: 762.817
[75,     1] loss: 647.973
[76,     1] loss: 605.145
[77,     1] loss: 648.931
[78,     1] loss: 598.207
[79,     1] loss: 595.083
[80,     1] loss: 523.875
[81,     1] loss: 498.895
[82,     1] loss: 567.427
[83,     1] loss: 569.440
[84,     1] loss: 534.432
[85,     1] loss: 512.888
[86,     1] loss: 512.633
[87,     1] loss: 544.298
[88,     1] loss: 475.803
[89,     1] loss: 485.021
[90,     1] loss: 477.193
[91,     1] loss: 445.407
[92,     1] loss: 524.465
[93,     1] loss: 490.527
[94,     1] loss: 453.556
[95,     1] loss: 509.272
[96,     1] loss: 720.912
[97,     1] loss: 898.148
[98,     1] loss: 509.781
[99,     1] loss: 852.649
[100,     1] loss: 538.858
[101,     1] loss: 672.087
[102,     1] loss: 679.270
[103,     1] loss: 569.573
[104,     1] loss: 615.176
Early stopping applied (best metric=0.35835549235343933)
Finished Training
Total time taken: 15.354777574539185
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.151
[2,     1] loss: 1323.074
[3,     1] loss: 1319.463
[4,     1] loss: 1321.873
[5,     1] loss: 1317.132
[6,     1] loss: 1321.290
[7,     1] loss: 1313.682
[8,     1] loss: 1305.090
[9,     1] loss: 1300.021
[10,     1] loss: 1285.050
[11,     1] loss: 1260.766
[12,     1] loss: 1242.690
[13,     1] loss: 1220.005
[14,     1] loss: 1192.813
[15,     1] loss: 1130.904
[16,     1] loss: 1145.002
[17,     1] loss: 1077.625
[18,     1] loss: 1091.341
[19,     1] loss: 1054.398
[20,     1] loss: 1115.114
[21,     1] loss: 1081.587
[22,     1] loss: 1070.149
[23,     1] loss: 1055.288
[24,     1] loss: 1077.936
[25,     1] loss: 1067.621
[26,     1] loss: 1074.232
[27,     1] loss: 1036.931
[28,     1] loss: 1020.452
[29,     1] loss: 1019.250
[30,     1] loss: 1029.946
[31,     1] loss: 1004.334
[32,     1] loss: 959.181
[33,     1] loss: 975.796
[34,     1] loss: 973.718
[35,     1] loss: 964.102
[36,     1] loss: 976.143
[37,     1] loss: 965.362
[38,     1] loss: 965.717
[39,     1] loss: 948.754
[40,     1] loss: 933.334
[41,     1] loss: 930.168
[42,     1] loss: 915.308
[43,     1] loss: 930.674
[44,     1] loss: 908.522
[45,     1] loss: 859.747
[46,     1] loss: 901.056
[47,     1] loss: 847.732
[48,     1] loss: 865.260
[49,     1] loss: 865.138
[50,     1] loss: 868.434
[51,     1] loss: 864.627
[52,     1] loss: 804.561
[53,     1] loss: 822.157
[54,     1] loss: 816.017
[55,     1] loss: 719.341
[56,     1] loss: 847.547
[57,     1] loss: 852.223
[58,     1] loss: 687.148
[59,     1] loss: 801.125
[60,     1] loss: 807.865
[61,     1] loss: 707.605
[62,     1] loss: 816.551
[63,     1] loss: 722.883
[64,     1] loss: 764.180
[65,     1] loss: 742.315
[66,     1] loss: 692.343
[67,     1] loss: 686.969
[68,     1] loss: 671.671
[69,     1] loss: 643.172
[70,     1] loss: 669.057
[71,     1] loss: 636.348
[72,     1] loss: 610.591
[73,     1] loss: 615.593
[74,     1] loss: 570.639
[75,     1] loss: 584.765
[76,     1] loss: 663.748
[77,     1] loss: 760.784
[78,     1] loss: 1078.481
[79,     1] loss: 799.293
[80,     1] loss: 870.707
[81,     1] loss: 726.481
[82,     1] loss: 839.386
[83,     1] loss: 850.942
[84,     1] loss: 792.650
[85,     1] loss: 731.091
[86,     1] loss: 802.050
[87,     1] loss: 767.985
[88,     1] loss: 637.563
[89,     1] loss: 699.706
[90,     1] loss: 632.822
[91,     1] loss: 604.011
[92,     1] loss: 621.098
[93,     1] loss: 553.603
[94,     1] loss: 559.597
[95,     1] loss: 534.124
[96,     1] loss: 539.634
[97,     1] loss: 508.769
[98,     1] loss: 531.937
Early stopping applied (best metric=0.3950255811214447)
Finished Training
Total time taken: 14.388161420822144
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.164
[2,     1] loss: 1319.234
[3,     1] loss: 1316.530
[4,     1] loss: 1321.389
[5,     1] loss: 1314.233
[6,     1] loss: 1314.343
[7,     1] loss: 1307.274
[8,     1] loss: 1293.429
[9,     1] loss: 1252.102
[10,     1] loss: 1227.069
[11,     1] loss: 1177.498
[12,     1] loss: 1149.436
[13,     1] loss: 1112.797
[14,     1] loss: 1088.899
[15,     1] loss: 1071.899
[16,     1] loss: 1074.971
[17,     1] loss: 1062.253
[18,     1] loss: 1081.637
[19,     1] loss: 1069.791
[20,     1] loss: 1085.476
[21,     1] loss: 1004.296
[22,     1] loss: 1073.441
[23,     1] loss: 1009.953
[24,     1] loss: 1009.494
[25,     1] loss: 995.418
[26,     1] loss: 1032.932
[27,     1] loss: 986.830
[28,     1] loss: 971.333
[29,     1] loss: 992.591
[30,     1] loss: 948.899
[31,     1] loss: 1031.455
[32,     1] loss: 926.717
[33,     1] loss: 983.476
[34,     1] loss: 902.323
[35,     1] loss: 938.003
[36,     1] loss: 893.824
[37,     1] loss: 951.528
[38,     1] loss: 881.108
[39,     1] loss: 964.923
[40,     1] loss: 839.005
[41,     1] loss: 859.939
[42,     1] loss: 885.685
[43,     1] loss: 805.403
[44,     1] loss: 844.502
[45,     1] loss: 799.248
[46,     1] loss: 847.068
[47,     1] loss: 756.210
[48,     1] loss: 828.417
[49,     1] loss: 804.482
[50,     1] loss: 763.271
[51,     1] loss: 743.795
[52,     1] loss: 756.012
[53,     1] loss: 846.866
[54,     1] loss: 729.273
[55,     1] loss: 759.591
[56,     1] loss: 736.330
[57,     1] loss: 729.630
[58,     1] loss: 768.223
[59,     1] loss: 754.044
[60,     1] loss: 750.462
[61,     1] loss: 708.918
[62,     1] loss: 663.340
[63,     1] loss: 687.461
[64,     1] loss: 613.389
[65,     1] loss: 665.701
[66,     1] loss: 637.658
[67,     1] loss: 673.295
[68,     1] loss: 701.038
[69,     1] loss: 858.557
[70,     1] loss: 663.619
[71,     1] loss: 764.315
[72,     1] loss: 668.639
[73,     1] loss: 760.047
[74,     1] loss: 653.084
[75,     1] loss: 655.511
[76,     1] loss: 672.047
[77,     1] loss: 633.292
[78,     1] loss: 669.947
[79,     1] loss: 609.098
[80,     1] loss: 615.869
[81,     1] loss: 596.539
[82,     1] loss: 595.175
[83,     1] loss: 628.098
[84,     1] loss: 553.746
[85,     1] loss: 618.037
[86,     1] loss: 545.739
[87,     1] loss: 577.629
[88,     1] loss: 559.112
[89,     1] loss: 481.709
[90,     1] loss: 522.216
[91,     1] loss: 515.308
[92,     1] loss: 505.726
[93,     1] loss: 485.265
[94,     1] loss: 541.422
[95,     1] loss: 656.530
Early stopping applied (best metric=0.37341535091400146)
Finished Training
Total time taken: 13.858255624771118
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1324.859
[2,     1] loss: 1326.435
[3,     1] loss: 1321.487
[4,     1] loss: 1319.631
[5,     1] loss: 1326.360
[6,     1] loss: 1312.921
[7,     1] loss: 1311.149
[8,     1] loss: 1303.145
[9,     1] loss: 1282.497
[10,     1] loss: 1258.654
[11,     1] loss: 1223.887
[12,     1] loss: 1188.228
[13,     1] loss: 1163.526
[14,     1] loss: 1156.864
[15,     1] loss: 1124.928
[16,     1] loss: 1126.865
[17,     1] loss: 1087.113
[18,     1] loss: 1139.522
[19,     1] loss: 1079.008
[20,     1] loss: 1057.372
[21,     1] loss: 1058.918
[22,     1] loss: 1008.244
[23,     1] loss: 1029.410
[24,     1] loss: 1032.527
[25,     1] loss: 1055.453
[26,     1] loss: 1016.499
[27,     1] loss: 1025.107
[28,     1] loss: 1059.588
[29,     1] loss: 1008.672
[30,     1] loss: 1038.964
[31,     1] loss: 998.542
[32,     1] loss: 975.480
[33,     1] loss: 951.670
[34,     1] loss: 985.107
[35,     1] loss: 945.443
[36,     1] loss: 938.543
[37,     1] loss: 965.448
[38,     1] loss: 921.102
[39,     1] loss: 888.246
[40,     1] loss: 855.215
[41,     1] loss: 948.916
[42,     1] loss: 895.572
[43,     1] loss: 831.776
[44,     1] loss: 833.746
[45,     1] loss: 946.458
[46,     1] loss: 1101.434
[47,     1] loss: 847.949
[48,     1] loss: 916.871
[49,     1] loss: 861.291
[50,     1] loss: 956.010
[51,     1] loss: 886.631
[52,     1] loss: 857.384
[53,     1] loss: 891.413
[54,     1] loss: 813.049
[55,     1] loss: 832.406
[56,     1] loss: 838.233
[57,     1] loss: 860.656
[58,     1] loss: 816.005
[59,     1] loss: 783.917
[60,     1] loss: 778.922
[61,     1] loss: 757.357
[62,     1] loss: 741.514
[63,     1] loss: 696.809
[64,     1] loss: 676.601
[65,     1] loss: 717.631
[66,     1] loss: 747.175
[67,     1] loss: 748.880
[68,     1] loss: 743.001
[69,     1] loss: 825.112
[70,     1] loss: 693.111
[71,     1] loss: 734.147
[72,     1] loss: 864.545
[73,     1] loss: 642.147
[74,     1] loss: 793.740
[75,     1] loss: 621.926
[76,     1] loss: 740.946
[77,     1] loss: 597.431
[78,     1] loss: 714.659
[79,     1] loss: 640.343
[80,     1] loss: 715.266
[81,     1] loss: 576.048
[82,     1] loss: 722.488
[83,     1] loss: 688.399
[84,     1] loss: 638.626
[85,     1] loss: 639.190
[86,     1] loss: 563.213
[87,     1] loss: 607.028
[88,     1] loss: 607.361
[89,     1] loss: 627.278
[90,     1] loss: 547.731
[91,     1] loss: 566.171
[92,     1] loss: 520.862
[93,     1] loss: 527.909
[94,     1] loss: 504.489
[95,     1] loss: 483.493
[96,     1] loss: 644.615
[97,     1] loss: 605.541
[98,     1] loss: 501.290
[99,     1] loss: 586.673
[100,     1] loss: 530.406
[101,     1] loss: 535.337
[102,     1] loss: 455.788
[103,     1] loss: 546.660
[104,     1] loss: 477.715
[105,     1] loss: 522.655
[106,     1] loss: 559.032
[107,     1] loss: 384.222
[108,     1] loss: 502.095
[109,     1] loss: 470.670
[110,     1] loss: 520.184
[111,     1] loss: 517.959
[112,     1] loss: 458.767
[113,     1] loss: 473.562
[114,     1] loss: 391.858
[115,     1] loss: 417.124
[116,     1] loss: 460.345
[117,     1] loss: 362.387
[118,     1] loss: 404.805
[119,     1] loss: 357.268
[120,     1] loss: 364.135
[121,     1] loss: 376.809
[122,     1] loss: 499.416
[123,     1] loss: 506.277
[124,     1] loss: 547.359
[125,     1] loss: 548.958
[126,     1] loss: 397.927
[127,     1] loss: 525.667
[128,     1] loss: 486.185
[129,     1] loss: 447.858
Early stopping applied (best metric=0.304066002368927)
Finished Training
Total time taken: 18.976173877716064
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.475
[2,     1] loss: 1323.867
[3,     1] loss: 1323.064
[4,     1] loss: 1325.719
[5,     1] loss: 1323.236
[6,     1] loss: 1322.421
[7,     1] loss: 1323.541
[8,     1] loss: 1321.500
[9,     1] loss: 1316.216
[10,     1] loss: 1316.197
[11,     1] loss: 1312.601
[12,     1] loss: 1311.972
[13,     1] loss: 1296.970
[14,     1] loss: 1291.913
[15,     1] loss: 1266.234
[16,     1] loss: 1233.744
[17,     1] loss: 1221.928
[18,     1] loss: 1198.584
[19,     1] loss: 1171.988
[20,     1] loss: 1161.765
[21,     1] loss: 1131.803
[22,     1] loss: 1156.550
[23,     1] loss: 1119.887
[24,     1] loss: 1106.792
[25,     1] loss: 1117.116
[26,     1] loss: 1054.287
[27,     1] loss: 1072.713
[28,     1] loss: 1065.123
[29,     1] loss: 1058.110
[30,     1] loss: 1013.159
[31,     1] loss: 1026.603
[32,     1] loss: 1013.963
[33,     1] loss: 1046.677
[34,     1] loss: 998.323
[35,     1] loss: 1029.772
[36,     1] loss: 985.126
[37,     1] loss: 996.133
[38,     1] loss: 988.694
[39,     1] loss: 1034.808
[40,     1] loss: 992.520
[41,     1] loss: 961.722
[42,     1] loss: 937.176
[43,     1] loss: 976.897
[44,     1] loss: 943.786
[45,     1] loss: 941.752
[46,     1] loss: 926.841
[47,     1] loss: 938.247
[48,     1] loss: 872.674
[49,     1] loss: 993.738
[50,     1] loss: 923.551
[51,     1] loss: 861.807
[52,     1] loss: 909.311
[53,     1] loss: 928.652
[54,     1] loss: 890.523
[55,     1] loss: 826.672
[56,     1] loss: 876.314
[57,     1] loss: 827.583
[58,     1] loss: 826.178
[59,     1] loss: 797.053
[60,     1] loss: 801.820
[61,     1] loss: 825.077
[62,     1] loss: 817.024
[63,     1] loss: 757.800
[64,     1] loss: 789.662
[65,     1] loss: 716.117
[66,     1] loss: 790.281
[67,     1] loss: 733.793
[68,     1] loss: 730.656
[69,     1] loss: 693.749
[70,     1] loss: 706.679
[71,     1] loss: 716.442
[72,     1] loss: 670.117
[73,     1] loss: 639.165
[74,     1] loss: 682.543
[75,     1] loss: 657.075
[76,     1] loss: 752.303
[77,     1] loss: 677.375
[78,     1] loss: 657.002
[79,     1] loss: 624.947
[80,     1] loss: 638.559
[81,     1] loss: 578.790
[82,     1] loss: 624.345
[83,     1] loss: 656.804
[84,     1] loss: 609.468
[85,     1] loss: 627.942
[86,     1] loss: 536.476
[87,     1] loss: 567.518
[88,     1] loss: 595.394
[89,     1] loss: 631.189
[90,     1] loss: 574.400
[91,     1] loss: 587.454
[92,     1] loss: 517.905
[93,     1] loss: 496.055
[94,     1] loss: 508.226
[95,     1] loss: 543.919
[96,     1] loss: 515.822
[97,     1] loss: 488.701
[98,     1] loss: 487.562
[99,     1] loss: 516.434
[100,     1] loss: 505.708
[101,     1] loss: 708.377
[102,     1] loss: 652.727
[103,     1] loss: 560.924
[104,     1] loss: 535.238
[105,     1] loss: 578.056
[106,     1] loss: 542.053
[107,     1] loss: 530.519
[108,     1] loss: 476.841
[109,     1] loss: 463.310
[110,     1] loss: 445.082
[111,     1] loss: 471.571
[112,     1] loss: 431.896
[113,     1] loss: 459.464
[114,     1] loss: 441.766
[115,     1] loss: 439.222
[116,     1] loss: 442.314
[117,     1] loss: 432.735
[118,     1] loss: 425.313
[119,     1] loss: 423.872
[120,     1] loss: 427.558
Early stopping applied (best metric=0.37642818689346313)
Finished Training
Total time taken: 17.57758116722107
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1334.983
[2,     1] loss: 1323.566
[3,     1] loss: 1320.456
[4,     1] loss: 1321.458
[5,     1] loss: 1318.694
[6,     1] loss: 1317.876
[7,     1] loss: 1322.157
[8,     1] loss: 1313.696
[9,     1] loss: 1307.577
[10,     1] loss: 1298.450
[11,     1] loss: 1279.184
[12,     1] loss: 1262.239
[13,     1] loss: 1238.318
[14,     1] loss: 1205.812
[15,     1] loss: 1165.496
[16,     1] loss: 1232.513
[17,     1] loss: 1124.301
[18,     1] loss: 1159.669
[19,     1] loss: 1162.110
[20,     1] loss: 1108.068
[21,     1] loss: 1119.132
[22,     1] loss: 1110.168
[23,     1] loss: 1090.199
[24,     1] loss: 1101.963
[25,     1] loss: 1071.147
[26,     1] loss: 1088.974
[27,     1] loss: 1078.866
[28,     1] loss: 1079.482
[29,     1] loss: 977.738
[30,     1] loss: 1015.933
[31,     1] loss: 1039.216
[32,     1] loss: 1056.783
[33,     1] loss: 1009.683
[34,     1] loss: 997.937
[35,     1] loss: 1019.920
[36,     1] loss: 987.404
[37,     1] loss: 1001.462
[38,     1] loss: 996.359
[39,     1] loss: 1013.960
[40,     1] loss: 932.339
[41,     1] loss: 958.556
[42,     1] loss: 924.341
[43,     1] loss: 888.552
[44,     1] loss: 915.864
[45,     1] loss: 903.557
[46,     1] loss: 898.447
[47,     1] loss: 882.189
[48,     1] loss: 968.198
[49,     1] loss: 843.255
[50,     1] loss: 882.839
[51,     1] loss: 911.050
[52,     1] loss: 839.009
[53,     1] loss: 794.909
[54,     1] loss: 845.635
[55,     1] loss: 789.297
[56,     1] loss: 783.994
[57,     1] loss: 810.378
[58,     1] loss: 870.569
[59,     1] loss: 993.352
[60,     1] loss: 788.531
[61,     1] loss: 788.027
[62,     1] loss: 748.882
[63,     1] loss: 768.589
[64,     1] loss: 758.216
[65,     1] loss: 727.732
[66,     1] loss: 744.237
[67,     1] loss: 731.930
[68,     1] loss: 651.076
[69,     1] loss: 724.619
[70,     1] loss: 724.839
[71,     1] loss: 701.198
[72,     1] loss: 698.963
[73,     1] loss: 696.698
[74,     1] loss: 683.806
[75,     1] loss: 630.526
[76,     1] loss: 566.354
[77,     1] loss: 637.488
[78,     1] loss: 636.666
[79,     1] loss: 588.864
[80,     1] loss: 528.300
[81,     1] loss: 566.061
[82,     1] loss: 567.907
[83,     1] loss: 583.725
[84,     1] loss: 584.765
[85,     1] loss: 540.781
[86,     1] loss: 590.965
[87,     1] loss: 690.572
[88,     1] loss: 614.757
[89,     1] loss: 557.677
[90,     1] loss: 636.848
[91,     1] loss: 519.265
[92,     1] loss: 659.444
[93,     1] loss: 604.889
[94,     1] loss: 564.006
[95,     1] loss: 683.880
[96,     1] loss: 519.316
[97,     1] loss: 583.700
[98,     1] loss: 620.991
[99,     1] loss: 491.974
[100,     1] loss: 547.113
[101,     1] loss: 468.394
[102,     1] loss: 484.349
[103,     1] loss: 442.731
[104,     1] loss: 506.979
[105,     1] loss: 470.563
[106,     1] loss: 447.856
[107,     1] loss: 536.352
[108,     1] loss: 603.218
[109,     1] loss: 525.586
[110,     1] loss: 417.764
[111,     1] loss: 476.022
[112,     1] loss: 436.368
[113,     1] loss: 501.268
[114,     1] loss: 441.378
[115,     1] loss: 423.869
[116,     1] loss: 406.862
[117,     1] loss: 376.896
[118,     1] loss: 390.397
[119,     1] loss: 440.618
[120,     1] loss: 579.315
[121,     1] loss: 870.136
[122,     1] loss: 418.904
[123,     1] loss: 661.017
[124,     1] loss: 452.761
[125,     1] loss: 607.397
[126,     1] loss: 497.697
[127,     1] loss: 564.460
[128,     1] loss: 496.097
[129,     1] loss: 457.269
[130,     1] loss: 487.853
[131,     1] loss: 421.062
[132,     1] loss: 439.348
[133,     1] loss: 329.773
[134,     1] loss: 399.233
[135,     1] loss: 341.440
[136,     1] loss: 436.352
[137,     1] loss: 468.190
[138,     1] loss: 352.943
[139,     1] loss: 508.982
[140,     1] loss: 374.850
[141,     1] loss: 471.119
[142,     1] loss: 357.110
[143,     1] loss: 417.656
[144,     1] loss: 345.951
[145,     1] loss: 441.418
[146,     1] loss: 379.002
[147,     1] loss: 375.949
[148,     1] loss: 375.947
[149,     1] loss: 334.673
[150,     1] loss: 362.943
[151,     1] loss: 330.121
[152,     1] loss: 348.679
[153,     1] loss: 370.787
[154,     1] loss: 302.205
[155,     1] loss: 472.479
[156,     1] loss: 364.740
[157,     1] loss: 311.854
[158,     1] loss: 329.107
[159,     1] loss: 310.506
[160,     1] loss: 332.117
[161,     1] loss: 289.261
[162,     1] loss: 326.898
[163,     1] loss: 413.112
[164,     1] loss: 414.427
[165,     1] loss: 263.870
[166,     1] loss: 397.314
[167,     1] loss: 485.645
[168,     1] loss: 286.081
[169,     1] loss: 363.299
[170,     1] loss: 336.387
[171,     1] loss: 331.652
[172,     1] loss: 361.313
[173,     1] loss: 286.670
[174,     1] loss: 340.220
[175,     1] loss: 305.393
[176,     1] loss: 344.781
[177,     1] loss: 303.342
[178,     1] loss: 281.357
[179,     1] loss: 340.853
[180,     1] loss: 330.608
[181,     1] loss: 277.422
[182,     1] loss: 360.944
[183,     1] loss: 324.015
[184,     1] loss: 340.242
[185,     1] loss: 356.486
[186,     1] loss: 288.880
[187,     1] loss: 412.619
[188,     1] loss: 394.494
[189,     1] loss: 268.738
[190,     1] loss: 313.966
Early stopping applied (best metric=0.3343563377857208)
Finished Training
Total time taken: 27.57918953895569
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1325.312
[2,     1] loss: 1322.179
[3,     1] loss: 1318.895
[4,     1] loss: 1318.295
[5,     1] loss: 1320.568
[6,     1] loss: 1303.558
[7,     1] loss: 1284.038
[8,     1] loss: 1265.198
[9,     1] loss: 1230.513
[10,     1] loss: 1181.464
[11,     1] loss: 1177.255
[12,     1] loss: 1112.346
[13,     1] loss: 1145.678
[14,     1] loss: 1070.003
[15,     1] loss: 1070.548
[16,     1] loss: 1128.989
[17,     1] loss: 1063.434
[18,     1] loss: 1103.004
[19,     1] loss: 1080.423
[20,     1] loss: 1064.550
[21,     1] loss: 1044.686
[22,     1] loss: 1022.736
[23,     1] loss: 1032.118
[24,     1] loss: 1019.934
[25,     1] loss: 1000.286
[26,     1] loss: 983.398
[27,     1] loss: 961.602
[28,     1] loss: 1009.790
[29,     1] loss: 977.542
[30,     1] loss: 991.816
[31,     1] loss: 969.578
[32,     1] loss: 959.548
[33,     1] loss: 936.037
[34,     1] loss: 928.781
[35,     1] loss: 932.707
[36,     1] loss: 878.454
[37,     1] loss: 907.994
[38,     1] loss: 905.853
[39,     1] loss: 913.865
[40,     1] loss: 901.960
[41,     1] loss: 848.003
[42,     1] loss: 953.895
[43,     1] loss: 915.988
[44,     1] loss: 851.401
[45,     1] loss: 869.217
[46,     1] loss: 832.006
[47,     1] loss: 918.179
[48,     1] loss: 809.670
[49,     1] loss: 874.123
[50,     1] loss: 810.038
[51,     1] loss: 849.724
[52,     1] loss: 726.147
[53,     1] loss: 775.564
[54,     1] loss: 802.147
[55,     1] loss: 733.931
[56,     1] loss: 716.628
[57,     1] loss: 713.914
[58,     1] loss: 759.637
[59,     1] loss: 764.513
[60,     1] loss: 720.606
[61,     1] loss: 712.224
[62,     1] loss: 665.179
[63,     1] loss: 723.866
[64,     1] loss: 762.121
[65,     1] loss: 780.871
[66,     1] loss: 777.722
[67,     1] loss: 671.290
[68,     1] loss: 724.952
[69,     1] loss: 675.038
[70,     1] loss: 630.522
[71,     1] loss: 637.273
[72,     1] loss: 688.745
[73,     1] loss: 647.240
[74,     1] loss: 626.475
[75,     1] loss: 732.551
[76,     1] loss: 683.559
[77,     1] loss: 567.683
[78,     1] loss: 645.287
[79,     1] loss: 574.308
[80,     1] loss: 556.550
[81,     1] loss: 574.539
[82,     1] loss: 537.985
[83,     1] loss: 560.489
[84,     1] loss: 531.088
[85,     1] loss: 542.631
[86,     1] loss: 532.569
[87,     1] loss: 556.360
[88,     1] loss: 606.207
[89,     1] loss: 723.648
[90,     1] loss: 514.112
[91,     1] loss: 625.461
[92,     1] loss: 560.137
[93,     1] loss: 584.804
[94,     1] loss: 541.422
[95,     1] loss: 528.808
[96,     1] loss: 531.763
[97,     1] loss: 519.076
[98,     1] loss: 519.809
[99,     1] loss: 448.711
[100,     1] loss: 539.492
[101,     1] loss: 476.293
[102,     1] loss: 556.992
[103,     1] loss: 589.307
[104,     1] loss: 426.301
[105,     1] loss: 618.815
[106,     1] loss: 639.560
[107,     1] loss: 475.582
[108,     1] loss: 619.887
[109,     1] loss: 464.765
[110,     1] loss: 583.160
[111,     1] loss: 502.374
[112,     1] loss: 549.619
[113,     1] loss: 423.669
[114,     1] loss: 484.019
[115,     1] loss: 482.757
[116,     1] loss: 562.818
[117,     1] loss: 417.081
[118,     1] loss: 500.300
[119,     1] loss: 450.503
[120,     1] loss: 402.217
[121,     1] loss: 391.340
[122,     1] loss: 384.840
[123,     1] loss: 380.501
[124,     1] loss: 380.491
[125,     1] loss: 331.845
[126,     1] loss: 330.397
[127,     1] loss: 349.431
[128,     1] loss: 438.318
[129,     1] loss: 451.105
[130,     1] loss: 352.220
[131,     1] loss: 377.843
[132,     1] loss: 614.036
[133,     1] loss: 565.216
[134,     1] loss: 483.055
[135,     1] loss: 512.202
[136,     1] loss: 513.858
[137,     1] loss: 442.573
[138,     1] loss: 566.515
[139,     1] loss: 395.925
[140,     1] loss: 544.651
[141,     1] loss: 430.330
[142,     1] loss: 567.635
[143,     1] loss: 391.167
[144,     1] loss: 460.765
[145,     1] loss: 384.373
[146,     1] loss: 478.275
[147,     1] loss: 340.402
[148,     1] loss: 428.109
[149,     1] loss: 364.756
[150,     1] loss: 462.304
[151,     1] loss: 320.769
[152,     1] loss: 443.419
[153,     1] loss: 399.393
[154,     1] loss: 410.566
[155,     1] loss: 322.404
[156,     1] loss: 381.927
[157,     1] loss: 408.567
[158,     1] loss: 338.333
[159,     1] loss: 337.458
[160,     1] loss: 385.796
[161,     1] loss: 319.957
[162,     1] loss: 300.582
[163,     1] loss: 305.302
[164,     1] loss: 291.335
[165,     1] loss: 308.275
[166,     1] loss: 284.463
[167,     1] loss: 399.251
[168,     1] loss: 572.322
[169,     1] loss: 326.104
[170,     1] loss: 346.439
[171,     1] loss: 330.661
[172,     1] loss: 358.926
[173,     1] loss: 364.184
[174,     1] loss: 307.275
[175,     1] loss: 315.672
[176,     1] loss: 320.727
[177,     1] loss: 296.822
[178,     1] loss: 308.968
[179,     1] loss: 315.826
[180,     1] loss: 349.619
[181,     1] loss: 331.898
[182,     1] loss: 310.628
[183,     1] loss: 377.652
[184,     1] loss: 325.023
[185,     1] loss: 295.271
[186,     1] loss: 400.656
[187,     1] loss: 336.966
[188,     1] loss: 282.780
[189,     1] loss: 311.557
[190,     1] loss: 293.082
[191,     1] loss: 302.677
Early stopping applied (best metric=0.320482462644577)
Finished Training
Total time taken: 28.054871797561646
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1326.062
[2,     1] loss: 1323.016
[3,     1] loss: 1323.629
[4,     1] loss: 1319.036
[5,     1] loss: 1321.472
[6,     1] loss: 1317.775
[7,     1] loss: 1317.588
[8,     1] loss: 1313.651
[9,     1] loss: 1314.813
[10,     1] loss: 1306.869
[11,     1] loss: 1288.358
[12,     1] loss: 1282.020
[13,     1] loss: 1259.551
[14,     1] loss: 1232.986
[15,     1] loss: 1202.373
[16,     1] loss: 1180.784
[17,     1] loss: 1133.250
[18,     1] loss: 1109.839
[19,     1] loss: 1089.003
[20,     1] loss: 1103.459
[21,     1] loss: 1068.681
[22,     1] loss: 1085.938
[23,     1] loss: 1063.797
[24,     1] loss: 1097.330
[25,     1] loss: 1070.318
[26,     1] loss: 1024.999
[27,     1] loss: 1049.234
[28,     1] loss: 1011.346
[29,     1] loss: 1002.388
[30,     1] loss: 1036.813
[31,     1] loss: 978.500
[32,     1] loss: 964.480
[33,     1] loss: 981.603
[34,     1] loss: 936.678
[35,     1] loss: 951.736
[36,     1] loss: 940.594
[37,     1] loss: 921.973
[38,     1] loss: 878.065
[39,     1] loss: 912.057
[40,     1] loss: 918.569
[41,     1] loss: 937.542
[42,     1] loss: 857.307
[43,     1] loss: 917.048
[44,     1] loss: 815.061
[45,     1] loss: 887.763
[46,     1] loss: 815.881
[47,     1] loss: 894.995
[48,     1] loss: 839.321
[49,     1] loss: 966.806
[50,     1] loss: 816.207
[51,     1] loss: 899.521
[52,     1] loss: 818.326
[53,     1] loss: 790.727
[54,     1] loss: 812.457
[55,     1] loss: 761.967
[56,     1] loss: 813.696
[57,     1] loss: 765.135
[58,     1] loss: 793.651
[59,     1] loss: 724.638
[60,     1] loss: 743.183
[61,     1] loss: 761.818
[62,     1] loss: 701.862
[63,     1] loss: 736.812
[64,     1] loss: 675.136
[65,     1] loss: 734.976
[66,     1] loss: 708.784
[67,     1] loss: 654.709
[68,     1] loss: 622.909
[69,     1] loss: 597.100
[70,     1] loss: 628.123
[71,     1] loss: 670.674
[72,     1] loss: 616.976
[73,     1] loss: 655.891
[74,     1] loss: 607.540
[75,     1] loss: 628.511
[76,     1] loss: 562.379
[77,     1] loss: 658.551
[78,     1] loss: 709.334
[79,     1] loss: 664.377
[80,     1] loss: 552.809
[81,     1] loss: 562.055
[82,     1] loss: 562.331
[83,     1] loss: 563.216
[84,     1] loss: 503.232
[85,     1] loss: 597.044
Early stopping applied (best metric=0.3806566298007965)
Finished Training
Total time taken: 12.494157075881958
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1329.763
[2,     1] loss: 1328.826
[3,     1] loss: 1322.652
[4,     1] loss: 1319.369
[5,     1] loss: 1322.490
[6,     1] loss: 1314.958
[7,     1] loss: 1309.391
[8,     1] loss: 1298.337
[9,     1] loss: 1289.485
[10,     1] loss: 1263.975
[11,     1] loss: 1228.323
[12,     1] loss: 1189.619
[13,     1] loss: 1169.935
[14,     1] loss: 1141.672
[15,     1] loss: 1161.527
[16,     1] loss: 1135.523
[17,     1] loss: 1109.553
[18,     1] loss: 1093.435
[19,     1] loss: 1079.512
[20,     1] loss: 1073.348
[21,     1] loss: 1023.151
[22,     1] loss: 1051.180
[23,     1] loss: 1076.006
[24,     1] loss: 1044.185
[25,     1] loss: 1034.979
[26,     1] loss: 980.779
[27,     1] loss: 1050.290
[28,     1] loss: 1056.449
[29,     1] loss: 1051.984
[30,     1] loss: 980.661
[31,     1] loss: 998.704
[32,     1] loss: 967.525
[33,     1] loss: 976.236
[34,     1] loss: 1026.856
[35,     1] loss: 991.196
[36,     1] loss: 989.023
[37,     1] loss: 938.314
[38,     1] loss: 878.109
[39,     1] loss: 913.950
[40,     1] loss: 907.470
[41,     1] loss: 930.041
[42,     1] loss: 921.075
[43,     1] loss: 872.130
[44,     1] loss: 860.686
[45,     1] loss: 855.301
[46,     1] loss: 850.879
[47,     1] loss: 868.484
[48,     1] loss: 841.610
[49,     1] loss: 814.393
[50,     1] loss: 864.402
[51,     1] loss: 860.310
[52,     1] loss: 1024.705
[53,     1] loss: 843.912
[54,     1] loss: 843.121
[55,     1] loss: 856.929
[56,     1] loss: 813.970
[57,     1] loss: 792.137
[58,     1] loss: 834.140
[59,     1] loss: 745.454
[60,     1] loss: 873.553
[61,     1] loss: 749.690
[62,     1] loss: 790.991
[63,     1] loss: 816.380
[64,     1] loss: 744.932
[65,     1] loss: 801.892
[66,     1] loss: 698.839
[67,     1] loss: 675.380
[68,     1] loss: 683.293
[69,     1] loss: 740.241
[70,     1] loss: 688.354
[71,     1] loss: 712.567
[72,     1] loss: 643.480
[73,     1] loss: 659.279
[74,     1] loss: 710.881
[75,     1] loss: 588.703
[76,     1] loss: 637.232
[77,     1] loss: 669.177
[78,     1] loss: 626.067
[79,     1] loss: 588.849
[80,     1] loss: 570.416
[81,     1] loss: 572.088
[82,     1] loss: 642.518
[83,     1] loss: 779.825
[84,     1] loss: 739.192
[85,     1] loss: 609.535
[86,     1] loss: 612.991
[87,     1] loss: 574.675
[88,     1] loss: 629.605
[89,     1] loss: 590.800
[90,     1] loss: 560.042
[91,     1] loss: 610.384
[92,     1] loss: 533.559
[93,     1] loss: 616.756
[94,     1] loss: 624.460
[95,     1] loss: 493.905
[96,     1] loss: 501.125
[97,     1] loss: 554.094
[98,     1] loss: 496.668
[99,     1] loss: 460.403
Early stopping applied (best metric=0.3583998680114746)
Finished Training
Total time taken: 14.657682657241821
{'Hydroxylation-K Validation Accuracy': 0.716725768321513, 'Hydroxylation-K Validation Sensitivity': 0.7281481481481481, 'Hydroxylation-K Validation Specificity': 0.7140350877192982, 'Hydroxylation-K Validation Precision': 0.40085700291874293, 'Hydroxylation-K AUC ROC': 0.8184990253411306, 'Hydroxylation-K AUC PR': 0.5953145735437431, 'Hydroxylation-K MCC': 0.3734548624194128, 'Hydroxylation-K F1': 0.508605550838316, 'Validation Loss (Hydroxylation-K)': 0.45189302364985146, 'Hydroxylation-P Validation Accuracy': 0.7838870615704786, 'Hydroxylation-P Validation Sensitivity': 0.8088888888888889, 'Hydroxylation-P Validation Specificity': 0.7785550401516285, 'Hydroxylation-P Validation Precision': 0.4479318514669741, 'Hydroxylation-P AUC ROC': 0.8556835673235493, 'Hydroxylation-P AUC PR': 0.5974894464274716, 'Hydroxylation-P MCC': 0.48318193509942875, 'Hydroxylation-P F1': 0.5738682232178535, 'Validation Loss (Hydroxylation-P)': 0.35898520549138385, 'Validation Loss (total)': 0.8108782172203064, 'TimeToTrain': 17.20691827138265}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009600118850696067,
 'learning_rate_Hydroxylation-K': 0.004887289065602127,
 'learning_rate_Hydroxylation-P': 0.007441988843027245,
 'log_base': 1.4401445963851884,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1674568307,
 'sample_weights': [1.961665820715113, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.41123959781464803,
 'weight_decay_Hydroxylation-K': 7.222472482820404,
 'weight_decay_Hydroxylation-P': 7.488866436939069}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1877.415
[2,     1] loss: 1883.830
[3,     1] loss: 1891.890
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023113984597418003,
 'learning_rate_Hydroxylation-K': 0.009374455463615802,
 'learning_rate_Hydroxylation-P': 0.004451039403990424,
 'log_base': 1.2096841814334263,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 646273512,
 'sample_weights': [4.577033020494442, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.198458768028445,
 'weight_decay_Hydroxylation-K': 5.341047443290105,
 'weight_decay_Hydroxylation-P': 4.236430714473031}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2853.648
[2,     1] loss: 2862.609
[3,     1] loss: 2842.246
[4,     1] loss: 2850.024
[5,     1] loss: 2842.719
[6,     1] loss: 2844.195
[7,     1] loss: 2832.853
[8,     1] loss: 2832.781
[9,     1] loss: 2838.859
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0055584612080255,
 'learning_rate_Hydroxylation-K': 5.126254522938318e-05,
 'learning_rate_Hydroxylation-P': 0.0005372502596482226,
 'log_base': 1.752808148635156,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1795279098,
 'sample_weights': [8.769957577724536, 1.0962869289634156],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4061601380965314,
 'weight_decay_Hydroxylation-K': 8.88977899815575,
 'weight_decay_Hydroxylation-P': 6.749092763847544}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1529.059
[2,     1] loss: 1556.142
[3,     1] loss: 1535.434
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009058971245910253,
 'learning_rate_Hydroxylation-K': 0.001533743596882691,
 'learning_rate_Hydroxylation-P': 0.00698899948958553,
 'log_base': 2.804132778311789,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3865987827,
 'sample_weights': [2.9746724127156856, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.583833263250698,
 'weight_decay_Hydroxylation-K': 0.805248106977015,
 'weight_decay_Hydroxylation-P': 9.86027323383286}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.183
[2,     1] loss: 1259.892
[3,     1] loss: 1255.984
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006752668416687223,
 'learning_rate_Hydroxylation-K': 0.0015657804013616944,
 'learning_rate_Hydroxylation-P': 0.005129552491254092,
 'log_base': 2.6354009255344417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1235909910,
 'sample_weights': [1.6190983822246812, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.227340559600351,
 'weight_decay_Hydroxylation-K': 3.298294141643038,
 'weight_decay_Hydroxylation-P': 8.984614296961935}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.200
[2,     1] loss: 1269.006
[3,     1] loss: 1270.923
[4,     1] loss: 1267.696
[5,     1] loss: 1254.434
[6,     1] loss: 1221.337
[7,     1] loss: 1178.998
[8,     1] loss: 1136.974
[9,     1] loss: 1126.590
[10,     1] loss: 1066.666
[11,     1] loss: 1083.251
[12,     1] loss: 1062.589
[13,     1] loss: 1056.504
[14,     1] loss: 1062.434
[15,     1] loss: 1041.005
[16,     1] loss: 1066.023
[17,     1] loss: 1048.287
[18,     1] loss: 1032.941
[19,     1] loss: 1017.784
[20,     1] loss: 1005.476
[21,     1] loss: 990.833
[22,     1] loss: 1006.117
[23,     1] loss: 964.865
[24,     1] loss: 945.262
[25,     1] loss: 952.685
[26,     1] loss: 952.674
[27,     1] loss: 979.164
[28,     1] loss: 921.386
[29,     1] loss: 918.732
[30,     1] loss: 903.832
[31,     1] loss: 913.607
[32,     1] loss: 887.303
[33,     1] loss: 898.648
[34,     1] loss: 900.936
[35,     1] loss: 864.938
[36,     1] loss: 846.800
[37,     1] loss: 835.079
[38,     1] loss: 846.563
[39,     1] loss: 797.151
[40,     1] loss: 786.926
[41,     1] loss: 950.604
[42,     1] loss: 824.820
[43,     1] loss: 834.747
[44,     1] loss: 853.519
[45,     1] loss: 860.975
[46,     1] loss: 805.052
[47,     1] loss: 793.134
[48,     1] loss: 743.688
[49,     1] loss: 728.250
[50,     1] loss: 688.314
[51,     1] loss: 712.201
[52,     1] loss: 810.522
[53,     1] loss: 637.044
[54,     1] loss: 833.804
[55,     1] loss: 775.088
[56,     1] loss: 695.270
[57,     1] loss: 723.657
[58,     1] loss: 738.712
[59,     1] loss: 664.162
[60,     1] loss: 772.075
[61,     1] loss: 651.218
[62,     1] loss: 641.766
[63,     1] loss: 600.611
[64,     1] loss: 609.793
[65,     1] loss: 707.115
[66,     1] loss: 750.043
[67,     1] loss: 544.344
[68,     1] loss: 639.557
[69,     1] loss: 682.483
[70,     1] loss: 618.747
[71,     1] loss: 698.204
[72,     1] loss: 558.815
[73,     1] loss: 668.241
[74,     1] loss: 526.714
[75,     1] loss: 593.799
[76,     1] loss: 501.952
[77,     1] loss: 581.053
[78,     1] loss: 477.569
[79,     1] loss: 498.986
Early stopping applied (best metric=0.3747747838497162)
Finished Training
Total time taken: 11.624694108963013
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1275.225
[2,     1] loss: 1272.165
[3,     1] loss: 1278.025
[4,     1] loss: 1270.962
[5,     1] loss: 1273.496
[6,     1] loss: 1275.574
[7,     1] loss: 1269.651
[8,     1] loss: 1269.156
[9,     1] loss: 1269.510
[10,     1] loss: 1268.772
[11,     1] loss: 1264.770
[12,     1] loss: 1263.257
[13,     1] loss: 1256.604
[14,     1] loss: 1249.843
[15,     1] loss: 1225.139
[16,     1] loss: 1190.687
[17,     1] loss: 1152.287
[18,     1] loss: 1119.485
[19,     1] loss: 1102.155
[20,     1] loss: 1050.741
[21,     1] loss: 1059.221
[22,     1] loss: 1099.757
[23,     1] loss: 1056.333
[24,     1] loss: 1018.066
[25,     1] loss: 1044.228
[26,     1] loss: 1012.965
[27,     1] loss: 1036.238
[28,     1] loss: 1010.288
[29,     1] loss: 979.466
[30,     1] loss: 942.856
[31,     1] loss: 950.339
[32,     1] loss: 982.331
[33,     1] loss: 892.238
[34,     1] loss: 944.558
[35,     1] loss: 930.048
[36,     1] loss: 938.404
[37,     1] loss: 890.807
[38,     1] loss: 994.318
[39,     1] loss: 893.407
[40,     1] loss: 920.291
[41,     1] loss: 917.016
[42,     1] loss: 919.392
[43,     1] loss: 882.354
[44,     1] loss: 897.543
[45,     1] loss: 857.462
[46,     1] loss: 857.758
[47,     1] loss: 798.889
[48,     1] loss: 841.030
[49,     1] loss: 790.869
[50,     1] loss: 862.769
[51,     1] loss: 807.463
[52,     1] loss: 837.714
[53,     1] loss: 830.422
[54,     1] loss: 755.806
[55,     1] loss: 800.048
[56,     1] loss: 740.124
[57,     1] loss: 776.441
[58,     1] loss: 684.044
[59,     1] loss: 678.899
[60,     1] loss: 666.047
[61,     1] loss: 662.037
[62,     1] loss: 764.770
[63,     1] loss: 1620.052
[64,     1] loss: 842.033
[65,     1] loss: 950.490
[66,     1] loss: 828.461
[67,     1] loss: 949.583
[68,     1] loss: 996.210
[69,     1] loss: 962.805
[70,     1] loss: 892.188
[71,     1] loss: 853.944
[72,     1] loss: 886.207
[73,     1] loss: 958.815
[74,     1] loss: 856.793
[75,     1] loss: 846.361
[76,     1] loss: 868.041
[77,     1] loss: 806.950
[78,     1] loss: 785.562
[79,     1] loss: 800.279
[80,     1] loss: 737.666
[81,     1] loss: 759.926
[82,     1] loss: 777.906
[83,     1] loss: 706.702
[84,     1] loss: 762.465
[85,     1] loss: 668.810
[86,     1] loss: 660.306
[87,     1] loss: 568.566
[88,     1] loss: 593.368
[89,     1] loss: 577.026
[90,     1] loss: 557.799
[91,     1] loss: 698.072
[92,     1] loss: 992.538
[93,     1] loss: 1013.277
[94,     1] loss: 811.443
[95,     1] loss: 726.280
[96,     1] loss: 924.111
[97,     1] loss: 839.646
[98,     1] loss: 757.953
[99,     1] loss: 749.130
[100,     1] loss: 720.084
[101,     1] loss: 681.848
[102,     1] loss: 780.229
[103,     1] loss: 633.388
[104,     1] loss: 692.573
[105,     1] loss: 553.977
[106,     1] loss: 640.102
[107,     1] loss: 582.549
[108,     1] loss: 585.725
[109,     1] loss: 575.208
[110,     1] loss: 587.339
[111,     1] loss: 666.627
[112,     1] loss: 512.308
[113,     1] loss: 823.147
[114,     1] loss: 721.365
[115,     1] loss: 625.749
[116,     1] loss: 627.796
[117,     1] loss: 577.802
[118,     1] loss: 575.889
[119,     1] loss: 615.678
[120,     1] loss: 497.831
[121,     1] loss: 616.831
[122,     1] loss: 462.510
[123,     1] loss: 434.925
Early stopping applied (best metric=0.35088497400283813)
Finished Training
Total time taken: 18.008875131607056
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.180
[2,     1] loss: 1270.355
[3,     1] loss: 1271.249
[4,     1] loss: 1270.251
[5,     1] loss: 1269.715
[6,     1] loss: 1264.318
[7,     1] loss: 1261.997
[8,     1] loss: 1247.020
[9,     1] loss: 1224.660
[10,     1] loss: 1202.213
[11,     1] loss: 1162.105
[12,     1] loss: 1131.100
[13,     1] loss: 1077.949
[14,     1] loss: 1092.615
[15,     1] loss: 1055.619
[16,     1] loss: 1038.266
[17,     1] loss: 1083.650
[18,     1] loss: 1036.978
[19,     1] loss: 1037.053
[20,     1] loss: 1036.677
[21,     1] loss: 1021.348
[22,     1] loss: 1019.150
[23,     1] loss: 968.151
[24,     1] loss: 1003.280
[25,     1] loss: 916.988
[26,     1] loss: 1008.292
[27,     1] loss: 961.144
[28,     1] loss: 990.792
[29,     1] loss: 945.868
[30,     1] loss: 961.090
[31,     1] loss: 925.208
[32,     1] loss: 941.346
[33,     1] loss: 913.232
[34,     1] loss: 882.730
[35,     1] loss: 888.294
[36,     1] loss: 873.376
[37,     1] loss: 914.833
[38,     1] loss: 826.990
[39,     1] loss: 897.551
[40,     1] loss: 835.837
[41,     1] loss: 819.458
[42,     1] loss: 850.771
[43,     1] loss: 753.205
[44,     1] loss: 767.945
[45,     1] loss: 811.647
[46,     1] loss: 866.368
[47,     1] loss: 942.437
[48,     1] loss: 832.516
[49,     1] loss: 791.804
[50,     1] loss: 886.417
[51,     1] loss: 827.283
[52,     1] loss: 838.139
[53,     1] loss: 817.323
[54,     1] loss: 784.068
[55,     1] loss: 771.260
[56,     1] loss: 687.863
[57,     1] loss: 759.136
[58,     1] loss: 694.870
[59,     1] loss: 788.872
[60,     1] loss: 739.380
[61,     1] loss: 606.974
[62,     1] loss: 677.317
[63,     1] loss: 612.044
[64,     1] loss: 634.892
[65,     1] loss: 580.091
[66,     1] loss: 568.666
[67,     1] loss: 599.068
[68,     1] loss: 862.186
[69,     1] loss: 2066.956
[70,     1] loss: 1026.685
[71,     1] loss: 827.351
[72,     1] loss: 1076.200
[73,     1] loss: 936.108
[74,     1] loss: 962.173
[75,     1] loss: 1021.029
[76,     1] loss: 1037.898
[77,     1] loss: 1044.929
[78,     1] loss: 1028.586
[79,     1] loss: 992.843
[80,     1] loss: 922.153
[81,     1] loss: 932.505
[82,     1] loss: 912.072
[83,     1] loss: 944.274
[84,     1] loss: 899.700
[85,     1] loss: 895.323
Early stopping applied (best metric=0.4108867943286896)
Finished Training
Total time taken: 12.540761947631836
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.199
[2,     1] loss: 1272.563
[3,     1] loss: 1268.937
[4,     1] loss: 1271.494
[5,     1] loss: 1272.207
[6,     1] loss: 1267.874
[7,     1] loss: 1272.047
[8,     1] loss: 1272.064
[9,     1] loss: 1271.298
[10,     1] loss: 1270.058
[11,     1] loss: 1265.334
[12,     1] loss: 1264.462
[13,     1] loss: 1263.579
[14,     1] loss: 1255.061
[15,     1] loss: 1244.057
[16,     1] loss: 1227.694
[17,     1] loss: 1193.735
[18,     1] loss: 1162.884
[19,     1] loss: 1123.628
[20,     1] loss: 1096.229
[21,     1] loss: 1060.227
[22,     1] loss: 1005.210
[23,     1] loss: 1037.365
[24,     1] loss: 996.809
[25,     1] loss: 1026.455
[26,     1] loss: 959.958
[27,     1] loss: 966.724
[28,     1] loss: 1004.487
[29,     1] loss: 929.452
[30,     1] loss: 962.856
[31,     1] loss: 952.467
[32,     1] loss: 970.112
[33,     1] loss: 940.276
[34,     1] loss: 932.469
[35,     1] loss: 938.861
[36,     1] loss: 882.047
[37,     1] loss: 880.774
[38,     1] loss: 899.109
[39,     1] loss: 848.106
[40,     1] loss: 870.574
[41,     1] loss: 871.373
[42,     1] loss: 945.229
[43,     1] loss: 1026.280
[44,     1] loss: 896.919
[45,     1] loss: 888.835
[46,     1] loss: 881.771
[47,     1] loss: 979.620
[48,     1] loss: 878.146
[49,     1] loss: 889.306
[50,     1] loss: 879.543
[51,     1] loss: 860.619
[52,     1] loss: 847.809
[53,     1] loss: 817.352
[54,     1] loss: 829.961
[55,     1] loss: 790.610
[56,     1] loss: 811.412
[57,     1] loss: 753.109
[58,     1] loss: 805.260
[59,     1] loss: 784.900
[60,     1] loss: 731.258
[61,     1] loss: 772.203
[62,     1] loss: 699.783
[63,     1] loss: 723.056
[64,     1] loss: 845.981
[65,     1] loss: 682.367
[66,     1] loss: 749.094
[67,     1] loss: 752.493
[68,     1] loss: 660.146
[69,     1] loss: 767.965
[70,     1] loss: 673.740
[71,     1] loss: 583.181
[72,     1] loss: 769.388
[73,     1] loss: 750.621
[74,     1] loss: 614.992
[75,     1] loss: 604.072
[76,     1] loss: 614.805
[77,     1] loss: 604.606
[78,     1] loss: 605.122
[79,     1] loss: 619.909
[80,     1] loss: 505.301
[81,     1] loss: 497.258
[82,     1] loss: 475.717
[83,     1] loss: 690.731
[84,     1] loss: 1477.299
[85,     1] loss: 868.478
[86,     1] loss: 972.361
[87,     1] loss: 751.113
[88,     1] loss: 824.011
[89,     1] loss: 935.685
[90,     1] loss: 976.099
[91,     1] loss: 888.085
[92,     1] loss: 830.776
[93,     1] loss: 853.477
[94,     1] loss: 853.358
[95,     1] loss: 847.828
[96,     1] loss: 759.590
[97,     1] loss: 768.949
[98,     1] loss: 770.554
[99,     1] loss: 660.045
[100,     1] loss: 707.536
[101,     1] loss: 686.204
[102,     1] loss: 599.554
[103,     1] loss: 636.219
[104,     1] loss: 578.674
[105,     1] loss: 590.046
[106,     1] loss: 507.689
[107,     1] loss: 578.074
[108,     1] loss: 531.282
[109,     1] loss: 480.313
[110,     1] loss: 506.594
[111,     1] loss: 494.939
[112,     1] loss: 414.671
[113,     1] loss: 403.384
[114,     1] loss: 375.983
[115,     1] loss: 462.947
[116,     1] loss: 859.953
[117,     1] loss: 1333.584
[118,     1] loss: 744.949
[119,     1] loss: 948.154
[120,     1] loss: 828.017
[121,     1] loss: 875.067
[122,     1] loss: 922.500
[123,     1] loss: 812.216
[124,     1] loss: 711.679
[125,     1] loss: 868.620
Early stopping applied (best metric=0.35560959577560425)
Finished Training
Total time taken: 18.972123622894287
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1274.447
[2,     1] loss: 1272.177
[3,     1] loss: 1278.651
[4,     1] loss: 1276.153
[5,     1] loss: 1270.223
[6,     1] loss: 1270.551
[7,     1] loss: 1268.695
[8,     1] loss: 1264.014
[9,     1] loss: 1258.195
[10,     1] loss: 1250.315
[11,     1] loss: 1221.687
[12,     1] loss: 1184.435
[13,     1] loss: 1132.961
[14,     1] loss: 1095.971
[15,     1] loss: 1117.996
[16,     1] loss: 1132.876
[17,     1] loss: 1081.712
[18,     1] loss: 1078.270
[19,     1] loss: 1030.033
[20,     1] loss: 1090.681
[21,     1] loss: 1043.237
[22,     1] loss: 1046.003
[23,     1] loss: 1038.012
[24,     1] loss: 1035.538
[25,     1] loss: 1002.472
[26,     1] loss: 1032.298
[27,     1] loss: 983.953
[28,     1] loss: 975.397
[29,     1] loss: 1016.890
[30,     1] loss: 932.911
[31,     1] loss: 938.289
[32,     1] loss: 951.810
[33,     1] loss: 1018.065
[34,     1] loss: 988.843
[35,     1] loss: 912.725
[36,     1] loss: 925.794
[37,     1] loss: 917.283
[38,     1] loss: 933.059
[39,     1] loss: 888.899
[40,     1] loss: 934.367
[41,     1] loss: 871.924
[42,     1] loss: 914.517
[43,     1] loss: 919.223
[44,     1] loss: 838.623
[45,     1] loss: 847.843
[46,     1] loss: 867.018
[47,     1] loss: 795.825
[48,     1] loss: 828.576
[49,     1] loss: 786.767
[50,     1] loss: 794.375
[51,     1] loss: 777.513
[52,     1] loss: 788.202
[53,     1] loss: 752.295
[54,     1] loss: 733.620
[55,     1] loss: 726.157
[56,     1] loss: 683.153
[57,     1] loss: 730.029
[58,     1] loss: 824.875
[59,     1] loss: 1601.563
[60,     1] loss: 709.158
[61,     1] loss: 1104.430
[62,     1] loss: 880.703
[63,     1] loss: 934.959
[64,     1] loss: 963.247
[65,     1] loss: 980.862
[66,     1] loss: 927.533
[67,     1] loss: 875.442
[68,     1] loss: 922.184
[69,     1] loss: 940.628
[70,     1] loss: 859.015
[71,     1] loss: 850.170
[72,     1] loss: 846.139
[73,     1] loss: 860.797
[74,     1] loss: 797.575
[75,     1] loss: 791.844
[76,     1] loss: 784.379
[77,     1] loss: 765.279
[78,     1] loss: 737.339
[79,     1] loss: 728.917
[80,     1] loss: 673.843
[81,     1] loss: 685.508
[82,     1] loss: 672.172
[83,     1] loss: 607.764
[84,     1] loss: 639.752
[85,     1] loss: 636.329
[86,     1] loss: 586.411
[87,     1] loss: 688.612
[88,     1] loss: 742.612
Early stopping applied (best metric=0.3225279450416565)
Finished Training
Total time taken: 12.912533521652222
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.957
[2,     1] loss: 1273.643
[3,     1] loss: 1271.500
[4,     1] loss: 1274.564
[5,     1] loss: 1268.480
[6,     1] loss: 1266.267
[7,     1] loss: 1261.288
[8,     1] loss: 1251.871
[9,     1] loss: 1233.446
[10,     1] loss: 1187.980
[11,     1] loss: 1162.450
[12,     1] loss: 1145.191
[13,     1] loss: 1096.115
[14,     1] loss: 1149.106
[15,     1] loss: 1083.101
[16,     1] loss: 1113.193
[17,     1] loss: 1026.659
[18,     1] loss: 1092.051
[19,     1] loss: 1074.815
[20,     1] loss: 993.719
[21,     1] loss: 1038.085
[22,     1] loss: 1040.074
[23,     1] loss: 1033.683
[24,     1] loss: 1010.679
[25,     1] loss: 986.939
[26,     1] loss: 985.526
[27,     1] loss: 1012.093
[28,     1] loss: 958.980
[29,     1] loss: 937.637
[30,     1] loss: 968.091
[31,     1] loss: 930.901
[32,     1] loss: 945.475
[33,     1] loss: 940.755
[34,     1] loss: 861.607
[35,     1] loss: 944.595
[36,     1] loss: 920.185
[37,     1] loss: 902.582
[38,     1] loss: 859.032
[39,     1] loss: 935.807
[40,     1] loss: 894.006
[41,     1] loss: 875.901
[42,     1] loss: 873.623
[43,     1] loss: 847.734
[44,     1] loss: 855.192
[45,     1] loss: 819.103
[46,     1] loss: 782.863
[47,     1] loss: 857.012
[48,     1] loss: 875.942
[49,     1] loss: 958.594
[50,     1] loss: 764.125
[51,     1] loss: 892.807
[52,     1] loss: 753.838
[53,     1] loss: 851.724
[54,     1] loss: 691.752
[55,     1] loss: 812.318
[56,     1] loss: 728.461
[57,     1] loss: 838.159
[58,     1] loss: 732.102
[59,     1] loss: 678.586
[60,     1] loss: 723.592
[61,     1] loss: 691.384
[62,     1] loss: 699.572
[63,     1] loss: 776.756
[64,     1] loss: 709.091
[65,     1] loss: 636.534
[66,     1] loss: 636.737
[67,     1] loss: 564.245
[68,     1] loss: 585.588
[69,     1] loss: 650.717
[70,     1] loss: 1011.071
[71,     1] loss: 1423.407
[72,     1] loss: 976.321
[73,     1] loss: 708.046
[74,     1] loss: 976.462
[75,     1] loss: 1021.365
[76,     1] loss: 928.228
[77,     1] loss: 907.529
[78,     1] loss: 920.020
[79,     1] loss: 895.562
[80,     1] loss: 907.801
[81,     1] loss: 839.143
[82,     1] loss: 832.096
[83,     1] loss: 796.374
[84,     1] loss: 811.208
[85,     1] loss: 763.688
Early stopping applied (best metric=0.41059139370918274)
Finished Training
Total time taken: 12.659214496612549
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1277.994
[2,     1] loss: 1273.214
[3,     1] loss: 1270.606
[4,     1] loss: 1274.071
[5,     1] loss: 1271.400
[6,     1] loss: 1265.963
[7,     1] loss: 1269.687
[8,     1] loss: 1261.694
[9,     1] loss: 1254.429
[10,     1] loss: 1247.444
[11,     1] loss: 1216.952
[12,     1] loss: 1185.012
[13,     1] loss: 1139.199
[14,     1] loss: 1104.428
[15,     1] loss: 1088.988
[16,     1] loss: 1048.523
[17,     1] loss: 1044.301
[18,     1] loss: 1047.630
[19,     1] loss: 1010.712
[20,     1] loss: 1021.108
[21,     1] loss: 971.299
[22,     1] loss: 981.886
[23,     1] loss: 994.456
[24,     1] loss: 989.937
[25,     1] loss: 987.867
[26,     1] loss: 985.154
[27,     1] loss: 945.705
[28,     1] loss: 947.162
[29,     1] loss: 938.735
[30,     1] loss: 891.157
[31,     1] loss: 889.368
[32,     1] loss: 948.556
[33,     1] loss: 952.369
[34,     1] loss: 913.110
[35,     1] loss: 860.193
[36,     1] loss: 813.342
[37,     1] loss: 826.086
[38,     1] loss: 807.066
[39,     1] loss: 852.719
[40,     1] loss: 872.444
[41,     1] loss: 982.485
[42,     1] loss: 1353.787
[43,     1] loss: 817.750
[44,     1] loss: 1052.437
[45,     1] loss: 975.414
[46,     1] loss: 941.659
[47,     1] loss: 961.244
[48,     1] loss: 974.914
[49,     1] loss: 917.177
[50,     1] loss: 896.579
[51,     1] loss: 923.121
[52,     1] loss: 887.142
[53,     1] loss: 873.376
[54,     1] loss: 846.583
[55,     1] loss: 849.833
[56,     1] loss: 851.347
[57,     1] loss: 852.117
[58,     1] loss: 774.661
[59,     1] loss: 830.385
[60,     1] loss: 774.429
[61,     1] loss: 809.352
[62,     1] loss: 762.780
[63,     1] loss: 751.178
[64,     1] loss: 782.562
[65,     1] loss: 761.287
[66,     1] loss: 748.454
[67,     1] loss: 675.384
[68,     1] loss: 751.148
[69,     1] loss: 879.079
[70,     1] loss: 801.150
[71,     1] loss: 715.186
[72,     1] loss: 746.284
[73,     1] loss: 692.792
[74,     1] loss: 730.004
[75,     1] loss: 719.188
[76,     1] loss: 662.646
[77,     1] loss: 813.824
[78,     1] loss: 697.930
[79,     1] loss: 634.762
[80,     1] loss: 764.687
[81,     1] loss: 603.738
[82,     1] loss: 723.107
Early stopping applied (best metric=0.3705017566680908)
Finished Training
Total time taken: 12.605223894119263
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1278.557
[2,     1] loss: 1278.689
[3,     1] loss: 1270.983
[4,     1] loss: 1271.907
[5,     1] loss: 1270.536
[6,     1] loss: 1268.190
[7,     1] loss: 1270.498
[8,     1] loss: 1268.963
[9,     1] loss: 1268.682
[10,     1] loss: 1269.018
[11,     1] loss: 1263.474
[12,     1] loss: 1260.635
[13,     1] loss: 1256.650
[14,     1] loss: 1236.897
[15,     1] loss: 1223.805
[16,     1] loss: 1194.675
[17,     1] loss: 1154.863
[18,     1] loss: 1147.492
[19,     1] loss: 1069.856
[20,     1] loss: 1076.133
[21,     1] loss: 1067.667
[22,     1] loss: 1148.640
[23,     1] loss: 1055.251
[24,     1] loss: 1034.015
[25,     1] loss: 1043.795
[26,     1] loss: 1050.370
[27,     1] loss: 992.354
[28,     1] loss: 1026.605
[29,     1] loss: 1008.365
[30,     1] loss: 1001.527
[31,     1] loss: 984.450
[32,     1] loss: 961.357
[33,     1] loss: 969.470
[34,     1] loss: 1005.061
[35,     1] loss: 892.873
[36,     1] loss: 980.713
[37,     1] loss: 932.194
[38,     1] loss: 957.638
[39,     1] loss: 887.073
[40,     1] loss: 920.518
[41,     1] loss: 849.563
[42,     1] loss: 854.224
[43,     1] loss: 895.466
[44,     1] loss: 900.053
[45,     1] loss: 842.380
[46,     1] loss: 855.125
[47,     1] loss: 1016.054
[48,     1] loss: 1193.825
[49,     1] loss: 864.337
[50,     1] loss: 1076.066
[51,     1] loss: 893.192
[52,     1] loss: 908.157
[53,     1] loss: 978.222
[54,     1] loss: 890.685
[55,     1] loss: 884.808
[56,     1] loss: 865.707
[57,     1] loss: 910.157
[58,     1] loss: 897.763
[59,     1] loss: 830.962
[60,     1] loss: 857.086
[61,     1] loss: 821.519
[62,     1] loss: 861.816
[63,     1] loss: 823.467
[64,     1] loss: 785.023
[65,     1] loss: 807.377
[66,     1] loss: 739.765
[67,     1] loss: 810.353
[68,     1] loss: 737.275
[69,     1] loss: 756.890
[70,     1] loss: 722.832
[71,     1] loss: 675.181
[72,     1] loss: 685.445
[73,     1] loss: 764.723
[74,     1] loss: 701.265
[75,     1] loss: 646.748
[76,     1] loss: 752.259
[77,     1] loss: 715.376
[78,     1] loss: 697.586
[79,     1] loss: 901.893
[80,     1] loss: 647.469
[81,     1] loss: 787.270
[82,     1] loss: 671.834
[83,     1] loss: 750.758
[84,     1] loss: 706.889
[85,     1] loss: 665.128
[86,     1] loss: 590.195
[87,     1] loss: 624.215
[88,     1] loss: 561.337
[89,     1] loss: 595.616
[90,     1] loss: 550.979
[91,     1] loss: 484.563
[92,     1] loss: 498.529
[93,     1] loss: 479.434
[94,     1] loss: 495.159
[95,     1] loss: 569.128
[96,     1] loss: 825.930
[97,     1] loss: 1344.267
[98,     1] loss: 606.964
[99,     1] loss: 857.045
[100,     1] loss: 819.272
[101,     1] loss: 696.935
[102,     1] loss: 751.251
[103,     1] loss: 771.975
[104,     1] loss: 726.230
[105,     1] loss: 726.580
[106,     1] loss: 647.735
[107,     1] loss: 676.472
[108,     1] loss: 626.550
[109,     1] loss: 627.586
[110,     1] loss: 592.265
[111,     1] loss: 599.730
[112,     1] loss: 520.736
[113,     1] loss: 542.405
[114,     1] loss: 685.167
[115,     1] loss: 408.548
[116,     1] loss: 558.937
[117,     1] loss: 491.229
[118,     1] loss: 441.293
[119,     1] loss: 525.456
[120,     1] loss: 386.316
[121,     1] loss: 444.932
Early stopping applied (best metric=0.3380405902862549)
Finished Training
Total time taken: 17.868778228759766
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.338
[2,     1] loss: 1278.399
[3,     1] loss: 1272.987
[4,     1] loss: 1266.904
[5,     1] loss: 1266.520
[6,     1] loss: 1257.062
[7,     1] loss: 1245.881
[8,     1] loss: 1214.126
[9,     1] loss: 1179.817
[10,     1] loss: 1163.503
[11,     1] loss: 1127.605
[12,     1] loss: 1060.376
[13,     1] loss: 1095.743
[14,     1] loss: 1082.253
[15,     1] loss: 1052.178
[16,     1] loss: 1031.571
[17,     1] loss: 1053.679
[18,     1] loss: 998.459
[19,     1] loss: 1042.778
[20,     1] loss: 999.473
[21,     1] loss: 969.939
[22,     1] loss: 1016.067
[23,     1] loss: 961.234
[24,     1] loss: 961.538
[25,     1] loss: 948.255
[26,     1] loss: 928.062
[27,     1] loss: 934.253
[28,     1] loss: 921.663
[29,     1] loss: 894.396
[30,     1] loss: 951.434
[31,     1] loss: 906.136
[32,     1] loss: 886.790
[33,     1] loss: 945.037
[34,     1] loss: 863.841
[35,     1] loss: 885.833
[36,     1] loss: 876.593
[37,     1] loss: 851.994
[38,     1] loss: 940.344
[39,     1] loss: 930.414
[40,     1] loss: 882.134
[41,     1] loss: 864.716
[42,     1] loss: 825.008
[43,     1] loss: 767.460
[44,     1] loss: 788.386
[45,     1] loss: 824.858
[46,     1] loss: 830.637
[47,     1] loss: 750.186
[48,     1] loss: 696.607
[49,     1] loss: 716.509
[50,     1] loss: 866.588
[51,     1] loss: 1511.926
[52,     1] loss: 908.295
[53,     1] loss: 984.359
[54,     1] loss: 869.140
[55,     1] loss: 911.821
[56,     1] loss: 912.948
[57,     1] loss: 979.203
[58,     1] loss: 902.399
[59,     1] loss: 831.358
[60,     1] loss: 888.688
[61,     1] loss: 869.048
[62,     1] loss: 851.633
[63,     1] loss: 800.793
[64,     1] loss: 829.770
[65,     1] loss: 816.678
[66,     1] loss: 798.828
[67,     1] loss: 804.192
[68,     1] loss: 733.924
[69,     1] loss: 748.272
[70,     1] loss: 744.903
[71,     1] loss: 711.566
[72,     1] loss: 716.498
[73,     1] loss: 686.207
[74,     1] loss: 667.228
[75,     1] loss: 651.489
[76,     1] loss: 789.585
[77,     1] loss: 880.339
[78,     1] loss: 669.814
[79,     1] loss: 724.197
[80,     1] loss: 694.259
[81,     1] loss: 664.730
[82,     1] loss: 625.326
[83,     1] loss: 658.964
[84,     1] loss: 642.458
[85,     1] loss: 579.733
[86,     1] loss: 660.962
[87,     1] loss: 617.359
[88,     1] loss: 590.872
[89,     1] loss: 501.908
[90,     1] loss: 477.653
[91,     1] loss: 490.130
[92,     1] loss: 649.810
[93,     1] loss: 1258.768
[94,     1] loss: 586.339
[95,     1] loss: 955.950
[96,     1] loss: 676.916
[97,     1] loss: 846.418
[98,     1] loss: 775.575
[99,     1] loss: 591.897
[100,     1] loss: 790.606
[101,     1] loss: 648.106
[102,     1] loss: 659.091
[103,     1] loss: 673.399
[104,     1] loss: 584.376
[105,     1] loss: 671.844
[106,     1] loss: 532.141
[107,     1] loss: 595.810
[108,     1] loss: 509.719
[109,     1] loss: 536.194
[110,     1] loss: 432.760
[111,     1] loss: 531.068
[112,     1] loss: 377.306
[113,     1] loss: 523.022
[114,     1] loss: 608.308
[115,     1] loss: 344.755
[116,     1] loss: 643.640
[117,     1] loss: 720.129
[118,     1] loss: 421.507
[119,     1] loss: 654.821
[120,     1] loss: 457.357
[121,     1] loss: 602.727
[122,     1] loss: 426.937
[123,     1] loss: 529.196
[124,     1] loss: 476.014
[125,     1] loss: 392.284
[126,     1] loss: 477.850
[127,     1] loss: 396.948
[128,     1] loss: 477.362
[129,     1] loss: 574.418
[130,     1] loss: 426.745
[131,     1] loss: 537.104
[132,     1] loss: 451.935
[133,     1] loss: 423.618
[134,     1] loss: 629.916
[135,     1] loss: 394.163
[136,     1] loss: 356.822
[137,     1] loss: 497.143
[138,     1] loss: 324.874
[139,     1] loss: 432.613
[140,     1] loss: 708.467
[141,     1] loss: 418.550
[142,     1] loss: 451.444
[143,     1] loss: 511.097
[144,     1] loss: 389.826
[145,     1] loss: 461.618
[146,     1] loss: 343.242
[147,     1] loss: 343.711
[148,     1] loss: 443.507
[149,     1] loss: 523.375
[150,     1] loss: 376.072
[151,     1] loss: 367.782
[152,     1] loss: 362.431
[153,     1] loss: 387.702
[154,     1] loss: 369.886
[155,     1] loss: 415.006
Early stopping applied (best metric=0.33834463357925415)
Finished Training
Total time taken: 23.119460105895996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1273.127
[2,     1] loss: 1277.151
[3,     1] loss: 1276.020
[4,     1] loss: 1271.335
[5,     1] loss: 1269.962
[6,     1] loss: 1269.035
[7,     1] loss: 1256.570
[8,     1] loss: 1242.731
[9,     1] loss: 1214.888
[10,     1] loss: 1163.675
[11,     1] loss: 1116.324
[12,     1] loss: 1093.758
[13,     1] loss: 1065.531
[14,     1] loss: 1084.455
[15,     1] loss: 1050.588
[16,     1] loss: 1038.713
[17,     1] loss: 1056.378
[18,     1] loss: 1031.219
[19,     1] loss: 1021.377
[20,     1] loss: 998.483
[21,     1] loss: 1009.199
[22,     1] loss: 988.637
[23,     1] loss: 979.362
[24,     1] loss: 958.214
[25,     1] loss: 946.458
[26,     1] loss: 918.704
[27,     1] loss: 948.694
[28,     1] loss: 897.531
[29,     1] loss: 908.817
[30,     1] loss: 902.938
[31,     1] loss: 870.098
[32,     1] loss: 869.230
[33,     1] loss: 855.350
[34,     1] loss: 893.943
[35,     1] loss: 829.234
[36,     1] loss: 848.759
[37,     1] loss: 841.000
[38,     1] loss: 820.573
[39,     1] loss: 850.830
[40,     1] loss: 1066.250
[41,     1] loss: 1302.960
[42,     1] loss: 862.292
[43,     1] loss: 1029.199
[44,     1] loss: 1047.254
[45,     1] loss: 975.917
[46,     1] loss: 983.634
[47,     1] loss: 987.130
[48,     1] loss: 925.822
[49,     1] loss: 919.998
[50,     1] loss: 873.241
[51,     1] loss: 914.581
[52,     1] loss: 886.275
[53,     1] loss: 842.057
[54,     1] loss: 811.111
[55,     1] loss: 827.549
[56,     1] loss: 845.900
[57,     1] loss: 810.759
[58,     1] loss: 777.815
[59,     1] loss: 810.100
[60,     1] loss: 765.705
[61,     1] loss: 730.524
[62,     1] loss: 709.267
[63,     1] loss: 785.747
[64,     1] loss: 723.792
[65,     1] loss: 691.867
[66,     1] loss: 789.083
[67,     1] loss: 914.624
[68,     1] loss: 884.810
[69,     1] loss: 663.770
[70,     1] loss: 766.536
[71,     1] loss: 642.087
[72,     1] loss: 838.538
[73,     1] loss: 716.178
[74,     1] loss: 673.199
[75,     1] loss: 711.279
[76,     1] loss: 597.516
[77,     1] loss: 576.469
[78,     1] loss: 559.976
[79,     1] loss: 503.204
[80,     1] loss: 551.284
[81,     1] loss: 623.807
[82,     1] loss: 1032.952
[83,     1] loss: 1125.133
[84,     1] loss: 615.728
Early stopping applied (best metric=0.3559573292732239)
Finished Training
Total time taken: 12.359993696212769
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.280
[2,     1] loss: 1278.783
[3,     1] loss: 1269.979
[4,     1] loss: 1269.449
[5,     1] loss: 1264.029
[6,     1] loss: 1270.083
[7,     1] loss: 1256.656
[8,     1] loss: 1256.442
[9,     1] loss: 1233.528
[10,     1] loss: 1191.088
[11,     1] loss: 1194.081
[12,     1] loss: 1144.059
[13,     1] loss: 1127.486
[14,     1] loss: 1061.796
[15,     1] loss: 1056.892
[16,     1] loss: 1067.795
[17,     1] loss: 1011.917
[18,     1] loss: 1047.469
[19,     1] loss: 1024.960
[20,     1] loss: 1053.632
[21,     1] loss: 997.689
[22,     1] loss: 971.319
[23,     1] loss: 969.624
[24,     1] loss: 931.909
[25,     1] loss: 930.466
[26,     1] loss: 963.815
[27,     1] loss: 936.692
[28,     1] loss: 915.225
[29,     1] loss: 893.315
[30,     1] loss: 906.166
[31,     1] loss: 926.482
[32,     1] loss: 879.889
[33,     1] loss: 890.029
[34,     1] loss: 839.214
[35,     1] loss: 827.337
[36,     1] loss: 876.749
[37,     1] loss: 1056.716
[38,     1] loss: 794.788
[39,     1] loss: 895.033
[40,     1] loss: 827.073
[41,     1] loss: 923.925
[42,     1] loss: 818.453
[43,     1] loss: 800.381
[44,     1] loss: 836.242
[45,     1] loss: 787.274
[46,     1] loss: 823.855
[47,     1] loss: 708.990
[48,     1] loss: 836.271
[49,     1] loss: 808.233
[50,     1] loss: 790.657
[51,     1] loss: 739.642
[52,     1] loss: 736.328
[53,     1] loss: 716.625
[54,     1] loss: 754.456
[55,     1] loss: 730.509
[56,     1] loss: 637.970
[57,     1] loss: 765.641
[58,     1] loss: 650.545
[59,     1] loss: 863.412
[60,     1] loss: 743.448
[61,     1] loss: 689.174
[62,     1] loss: 646.076
[63,     1] loss: 669.040
[64,     1] loss: 577.679
[65,     1] loss: 703.001
[66,     1] loss: 562.162
[67,     1] loss: 661.599
[68,     1] loss: 562.504
[69,     1] loss: 587.347
[70,     1] loss: 684.805
[71,     1] loss: 605.017
[72,     1] loss: 572.489
[73,     1] loss: 575.531
[74,     1] loss: 562.465
[75,     1] loss: 580.777
[76,     1] loss: 528.506
[77,     1] loss: 524.423
[78,     1] loss: 561.513
[79,     1] loss: 519.982
[80,     1] loss: 474.122
Early stopping applied (best metric=0.3809942603111267)
Finished Training
Total time taken: 11.790640354156494
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.885
[2,     1] loss: 1272.465
[3,     1] loss: 1272.039
[4,     1] loss: 1269.488
[5,     1] loss: 1271.590
[6,     1] loss: 1268.754
[7,     1] loss: 1261.209
[8,     1] loss: 1259.281
[9,     1] loss: 1237.687
[10,     1] loss: 1211.635
[11,     1] loss: 1186.977
[12,     1] loss: 1152.466
[13,     1] loss: 1101.400
[14,     1] loss: 1087.858
[15,     1] loss: 1065.016
[16,     1] loss: 1057.741
[17,     1] loss: 1079.247
[18,     1] loss: 1059.335
[19,     1] loss: 1029.962
[20,     1] loss: 1050.901
[21,     1] loss: 1037.235
[22,     1] loss: 1017.089
[23,     1] loss: 1027.966
[24,     1] loss: 993.310
[25,     1] loss: 1072.040
[26,     1] loss: 1005.737
[27,     1] loss: 979.877
[28,     1] loss: 946.517
[29,     1] loss: 967.698
[30,     1] loss: 950.578
[31,     1] loss: 952.164
[32,     1] loss: 931.631
[33,     1] loss: 952.248
[34,     1] loss: 930.520
[35,     1] loss: 949.549
[36,     1] loss: 1010.816
[37,     1] loss: 990.837
[38,     1] loss: 927.197
[39,     1] loss: 908.520
[40,     1] loss: 840.677
[41,     1] loss: 907.084
[42,     1] loss: 911.643
[43,     1] loss: 904.988
[44,     1] loss: 861.034
[45,     1] loss: 875.552
[46,     1] loss: 861.735
[47,     1] loss: 839.854
[48,     1] loss: 847.220
[49,     1] loss: 828.437
[50,     1] loss: 789.208
[51,     1] loss: 788.939
[52,     1] loss: 785.754
[53,     1] loss: 856.143
[54,     1] loss: 925.570
[55,     1] loss: 802.495
[56,     1] loss: 839.762
[57,     1] loss: 785.972
[58,     1] loss: 762.556
[59,     1] loss: 749.181
[60,     1] loss: 716.879
[61,     1] loss: 678.329
[62,     1] loss: 694.933
[63,     1] loss: 692.079
[64,     1] loss: 878.580
[65,     1] loss: 1447.160
[66,     1] loss: 651.704
[67,     1] loss: 1080.417
[68,     1] loss: 855.121
[69,     1] loss: 849.079
[70,     1] loss: 985.205
[71,     1] loss: 1010.516
[72,     1] loss: 960.272
[73,     1] loss: 890.064
[74,     1] loss: 858.142
[75,     1] loss: 893.787
[76,     1] loss: 857.221
[77,     1] loss: 772.899
[78,     1] loss: 816.954
[79,     1] loss: 809.792
[80,     1] loss: 776.171
[81,     1] loss: 738.665
[82,     1] loss: 647.826
[83,     1] loss: 693.126
[84,     1] loss: 699.975
[85,     1] loss: 651.780
[86,     1] loss: 651.552
[87,     1] loss: 783.156
[88,     1] loss: 575.912
[89,     1] loss: 761.638
[90,     1] loss: 622.157
[91,     1] loss: 652.378
[92,     1] loss: 668.721
[93,     1] loss: 514.306
[94,     1] loss: 589.491
[95,     1] loss: 492.150
[96,     1] loss: 483.859
[97,     1] loss: 564.549
[98,     1] loss: 692.531
[99,     1] loss: 790.980
[100,     1] loss: 508.563
[101,     1] loss: 678.584
[102,     1] loss: 554.578
[103,     1] loss: 624.961
[104,     1] loss: 492.417
[105,     1] loss: 621.029
[106,     1] loss: 482.611
[107,     1] loss: 528.800
[108,     1] loss: 721.581
[109,     1] loss: 882.638
[110,     1] loss: 491.683
[111,     1] loss: 632.961
Early stopping applied (best metric=0.31332558393478394)
Finished Training
Total time taken: 16.325093746185303
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1277.281
[2,     1] loss: 1264.032
[3,     1] loss: 1277.855
[4,     1] loss: 1271.997
[5,     1] loss: 1273.401
[6,     1] loss: 1271.995
[7,     1] loss: 1271.172
[8,     1] loss: 1268.986
[9,     1] loss: 1266.033
[10,     1] loss: 1265.136
[11,     1] loss: 1258.556
[12,     1] loss: 1258.094
[13,     1] loss: 1244.786
[14,     1] loss: 1224.842
[15,     1] loss: 1194.530
[16,     1] loss: 1182.431
[17,     1] loss: 1144.450
[18,     1] loss: 1096.587
[19,     1] loss: 1104.976
[20,     1] loss: 1018.802
[21,     1] loss: 1021.636
[22,     1] loss: 1013.430
[23,     1] loss: 1032.013
[24,     1] loss: 1022.012
[25,     1] loss: 1051.640
[26,     1] loss: 964.580
[27,     1] loss: 1039.945
[28,     1] loss: 1006.880
[29,     1] loss: 1034.367
[30,     1] loss: 966.093
[31,     1] loss: 934.755
[32,     1] loss: 924.101
[33,     1] loss: 886.187
[34,     1] loss: 885.153
[35,     1] loss: 926.534
[36,     1] loss: 920.290
[37,     1] loss: 908.142
[38,     1] loss: 904.034
[39,     1] loss: 879.021
[40,     1] loss: 894.192
[41,     1] loss: 849.191
[42,     1] loss: 861.046
[43,     1] loss: 893.428
[44,     1] loss: 807.577
[45,     1] loss: 898.882
[46,     1] loss: 947.028
[47,     1] loss: 824.644
[48,     1] loss: 819.158
[49,     1] loss: 844.757
[50,     1] loss: 826.457
[51,     1] loss: 771.434
[52,     1] loss: 834.302
[53,     1] loss: 830.783
[54,     1] loss: 734.607
[55,     1] loss: 773.910
[56,     1] loss: 691.470
[57,     1] loss: 701.170
[58,     1] loss: 700.008
[59,     1] loss: 666.275
[60,     1] loss: 696.775
[61,     1] loss: 962.708
[62,     1] loss: 936.709
[63,     1] loss: 697.782
[64,     1] loss: 873.014
[65,     1] loss: 750.020
[66,     1] loss: 760.497
[67,     1] loss: 786.202
[68,     1] loss: 661.857
[69,     1] loss: 706.928
Early stopping applied (best metric=0.39766356348991394)
Finished Training
Total time taken: 10.123696565628052
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1278.503
[2,     1] loss: 1275.797
[3,     1] loss: 1272.366
[4,     1] loss: 1271.414
[5,     1] loss: 1273.317
[6,     1] loss: 1269.590
[7,     1] loss: 1267.336
[8,     1] loss: 1265.044
[9,     1] loss: 1259.998
[10,     1] loss: 1253.408
[11,     1] loss: 1228.571
[12,     1] loss: 1191.822
[13,     1] loss: 1146.414
[14,     1] loss: 1123.032
[15,     1] loss: 1099.623
[16,     1] loss: 1091.219
[17,     1] loss: 1058.454
[18,     1] loss: 1083.799
[19,     1] loss: 1024.640
[20,     1] loss: 1099.500
[21,     1] loss: 1040.640
[22,     1] loss: 1028.481
[23,     1] loss: 1042.557
[24,     1] loss: 1031.429
[25,     1] loss: 1039.796
[26,     1] loss: 1040.826
[27,     1] loss: 1008.529
[28,     1] loss: 969.363
[29,     1] loss: 1014.360
[30,     1] loss: 939.662
[31,     1] loss: 957.498
[32,     1] loss: 914.588
[33,     1] loss: 920.247
[34,     1] loss: 939.899
[35,     1] loss: 903.982
[36,     1] loss: 883.825
[37,     1] loss: 924.271
[38,     1] loss: 852.943
[39,     1] loss: 897.332
[40,     1] loss: 869.427
[41,     1] loss: 858.014
[42,     1] loss: 895.730
[43,     1] loss: 845.277
[44,     1] loss: 1033.280
[45,     1] loss: 915.553
[46,     1] loss: 885.051
[47,     1] loss: 893.322
[48,     1] loss: 860.935
[49,     1] loss: 875.023
[50,     1] loss: 824.955
[51,     1] loss: 783.642
[52,     1] loss: 806.263
[53,     1] loss: 738.832
[54,     1] loss: 796.262
[55,     1] loss: 811.627
[56,     1] loss: 804.516
[57,     1] loss: 757.244
[58,     1] loss: 733.399
[59,     1] loss: 758.024
[60,     1] loss: 666.882
[61,     1] loss: 733.788
[62,     1] loss: 879.978
[63,     1] loss: 832.155
[64,     1] loss: 748.321
[65,     1] loss: 758.944
[66,     1] loss: 747.017
[67,     1] loss: 739.027
[68,     1] loss: 712.795
[69,     1] loss: 694.672
[70,     1] loss: 668.848
[71,     1] loss: 641.480
[72,     1] loss: 558.347
[73,     1] loss: 674.753
[74,     1] loss: 908.875
[75,     1] loss: 755.722
[76,     1] loss: 572.416
[77,     1] loss: 708.421
[78,     1] loss: 629.424
[79,     1] loss: 694.713
[80,     1] loss: 597.678
[81,     1] loss: 564.343
[82,     1] loss: 612.905
[83,     1] loss: 575.387
[84,     1] loss: 478.061
[85,     1] loss: 495.102
[86,     1] loss: 455.367
[87,     1] loss: 519.611
[88,     1] loss: 433.910
[89,     1] loss: 523.976
[90,     1] loss: 997.227
[91,     1] loss: 650.827
[92,     1] loss: 575.107
[93,     1] loss: 678.111
[94,     1] loss: 574.068
[95,     1] loss: 659.013
[96,     1] loss: 500.766
[97,     1] loss: 638.096
[98,     1] loss: 514.379
[99,     1] loss: 610.808
[100,     1] loss: 703.865
[101,     1] loss: 493.838
[102,     1] loss: 738.989
[103,     1] loss: 588.548
[104,     1] loss: 621.219
[105,     1] loss: 495.771
[106,     1] loss: 613.424
[107,     1] loss: 569.589
[108,     1] loss: 572.125
[109,     1] loss: 455.516
[110,     1] loss: 603.239
[111,     1] loss: 499.316
[112,     1] loss: 449.479
[113,     1] loss: 468.936
[114,     1] loss: 409.157
[115,     1] loss: 490.659
[116,     1] loss: 398.994
[117,     1] loss: 533.693
[118,     1] loss: 441.325
[119,     1] loss: 405.349
[120,     1] loss: 559.074
Early stopping applied (best metric=0.35465022921562195)
Finished Training
Total time taken: 17.559480667114258
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1278.066
[2,     1] loss: 1273.734
[3,     1] loss: 1276.491
[4,     1] loss: 1270.816
[5,     1] loss: 1274.061
[6,     1] loss: 1274.941
[7,     1] loss: 1268.113
[8,     1] loss: 1268.442
[9,     1] loss: 1269.847
[10,     1] loss: 1262.286
[11,     1] loss: 1257.204
[12,     1] loss: 1252.293
[13,     1] loss: 1218.410
[14,     1] loss: 1186.931
[15,     1] loss: 1135.764
[16,     1] loss: 1098.594
[17,     1] loss: 1125.038
[18,     1] loss: 1071.485
[19,     1] loss: 1051.664
[20,     1] loss: 1063.417
[21,     1] loss: 1003.984
[22,     1] loss: 1065.597
[23,     1] loss: 1028.181
[24,     1] loss: 1036.964
[25,     1] loss: 1003.823
[26,     1] loss: 1048.331
[27,     1] loss: 1029.117
[28,     1] loss: 965.946
[29,     1] loss: 973.046
[30,     1] loss: 957.918
[31,     1] loss: 965.864
[32,     1] loss: 921.111
[33,     1] loss: 944.768
[34,     1] loss: 909.618
[35,     1] loss: 957.253
[36,     1] loss: 885.424
[37,     1] loss: 939.456
[38,     1] loss: 920.469
[39,     1] loss: 845.674
[40,     1] loss: 870.714
[41,     1] loss: 863.899
[42,     1] loss: 819.875
[43,     1] loss: 807.444
[44,     1] loss: 808.822
[45,     1] loss: 818.510
[46,     1] loss: 897.802
[47,     1] loss: 844.156
[48,     1] loss: 793.557
[49,     1] loss: 799.060
[50,     1] loss: 795.730
[51,     1] loss: 805.722
[52,     1] loss: 733.877
[53,     1] loss: 777.288
[54,     1] loss: 752.396
[55,     1] loss: 724.668
[56,     1] loss: 688.175
[57,     1] loss: 739.829
[58,     1] loss: 738.187
[59,     1] loss: 703.701
[60,     1] loss: 747.587
[61,     1] loss: 880.176
[62,     1] loss: 680.442
[63,     1] loss: 666.669
[64,     1] loss: 746.260
[65,     1] loss: 633.032
[66,     1] loss: 707.347
[67,     1] loss: 663.666
[68,     1] loss: 572.501
[69,     1] loss: 625.157
[70,     1] loss: 609.613
[71,     1] loss: 577.776
[72,     1] loss: 583.182
[73,     1] loss: 539.406
[74,     1] loss: 498.453
[75,     1] loss: 588.801
[76,     1] loss: 696.899
[77,     1] loss: 748.932
[78,     1] loss: 723.217
[79,     1] loss: 624.684
[80,     1] loss: 616.974
[81,     1] loss: 616.678
[82,     1] loss: 611.805
[83,     1] loss: 523.176
[84,     1] loss: 622.172
[85,     1] loss: 537.164
[86,     1] loss: 548.145
[87,     1] loss: 671.111
[88,     1] loss: 661.199
[89,     1] loss: 518.279
[90,     1] loss: 584.735
[91,     1] loss: 482.558
[92,     1] loss: 570.748
[93,     1] loss: 502.248
[94,     1] loss: 528.924
[95,     1] loss: 423.844
[96,     1] loss: 497.049
[97,     1] loss: 519.257
[98,     1] loss: 436.410
[99,     1] loss: 389.734
[100,     1] loss: 501.066
[101,     1] loss: 791.537
[102,     1] loss: 402.942
[103,     1] loss: 658.778
[104,     1] loss: 624.697
[105,     1] loss: 743.338
Early stopping applied (best metric=0.3572976589202881)
Finished Training
Total time taken: 15.388059616088867
{'Hydroxylation-K Validation Accuracy': 0.7477541371158393, 'Hydroxylation-K Validation Sensitivity': 0.5822222222222222, 'Hydroxylation-K Validation Specificity': 0.7894736842105263, 'Hydroxylation-K Validation Precision': 0.41745300995300993, 'Hydroxylation-K AUC ROC': 0.7670760233918128, 'Hydroxylation-K AUC PR': 0.5417004373531308, 'Hydroxylation-K MCC': 0.3336495526566281, 'Hydroxylation-K F1': 0.48143241378786766, 'Validation Loss (Hydroxylation-K)': 0.49468885858853656, 'Hydroxylation-P Validation Accuracy': 0.8053530785239328, 'Hydroxylation-P Validation Sensitivity': 0.792010582010582, 'Hydroxylation-P Validation Specificity': 0.8082871963688962, 'Hydroxylation-P Validation Precision': 0.47672495015731153, 'Hydroxylation-P AUC ROC': 0.85500464338561, 'Hydroxylation-P AUC PR': 0.6030854494032932, 'Hydroxylation-P MCC': 0.5044034123125904, 'Hydroxylation-P F1': 0.5923324748538278, 'Validation Loss (Hydroxylation-P)': 0.36213673949241637, 'Validation Loss (total)': 0.856825610001882, 'TimeToTrain': 14.923908646901449}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007629213298672778,
 'learning_rate_Hydroxylation-K': 0.0021270383435094855,
 'learning_rate_Hydroxylation-P': 0.004908076576052094,
 'log_base': 2.2852260381980125,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 44804478,
 'sample_weights': [1.7240667040115891, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.810579796020532,
 'weight_decay_Hydroxylation-K': 5.83589971243064,
 'weight_decay_Hydroxylation-P': 8.830308341340556}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1338.929
[2,     1] loss: 1338.934
[3,     1] loss: 1336.284
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006758434428134005,
 'learning_rate_Hydroxylation-K': 0.001373766448893927,
 'learning_rate_Hydroxylation-P': 0.00512634310360001,
 'log_base': 2.8036772730903876,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 270510274,
 'sample_weights': [2.0199805973922422, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4087554990919848,
 'weight_decay_Hydroxylation-K': 5.061438232524299,
 'weight_decay_Hydroxylation-P': 8.379451929002688}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.897
[2,     1] loss: 1250.523
[3,     1] loss: 1252.417
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017402660605009626,
 'learning_rate_Hydroxylation-K': 0.0012525459919991138,
 'learning_rate_Hydroxylation-P': 0.0029447224840353147,
 'log_base': 1.8872560582328677,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1251871320,
 'sample_weights': [1.619353519160839, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.948528586741411,
 'weight_decay_Hydroxylation-K': 1.2385615548412736,
 'weight_decay_Hydroxylation-P': 1.782220856194212}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1465.321
[2,     1] loss: 1465.827
[3,     1] loss: 1458.357
[4,     1] loss: 1462.619
[5,     1] loss: 1457.422
[6,     1] loss: 1459.719
[7,     1] loss: 1459.322
[8,     1] loss: 1453.578
[9,     1] loss: 1453.759
[10,     1] loss: 1446.514
[11,     1] loss: 1438.845
[12,     1] loss: 1429.632
[13,     1] loss: 1420.416
[14,     1] loss: 1378.540
[15,     1] loss: 1367.168
[16,     1] loss: 1336.398
[17,     1] loss: 1338.409
[18,     1] loss: 1310.272
[19,     1] loss: 1273.151
[20,     1] loss: 1284.586
[21,     1] loss: 1250.830
[22,     1] loss: 1275.362
[23,     1] loss: 1229.874
[24,     1] loss: 1193.303
[25,     1] loss: 1211.555
[26,     1] loss: 1220.534
[27,     1] loss: 1206.142
[28,     1] loss: 1233.399
[29,     1] loss: 1213.397
[30,     1] loss: 1180.540
[31,     1] loss: 1165.518
[32,     1] loss: 1184.716
[33,     1] loss: 1142.419
[34,     1] loss: 1146.409
[35,     1] loss: 1173.825
[36,     1] loss: 1094.914
[37,     1] loss: 1090.096
[38,     1] loss: 1095.536
[39,     1] loss: 1083.313
[40,     1] loss: 1102.609
[41,     1] loss: 1102.260
[42,     1] loss: 1002.252
[43,     1] loss: 1039.679
[44,     1] loss: 1041.840
[45,     1] loss: 1062.212
[46,     1] loss: 985.165
[47,     1] loss: 974.556
[48,     1] loss: 1019.460
[49,     1] loss: 1008.461
[50,     1] loss: 986.247
[51,     1] loss: 1008.448
[52,     1] loss: 930.959
[53,     1] loss: 964.727
[54,     1] loss: 958.161
[55,     1] loss: 890.409
[56,     1] loss: 908.906
[57,     1] loss: 925.929
[58,     1] loss: 912.581
[59,     1] loss: 853.454
[60,     1] loss: 890.276
[61,     1] loss: 861.088
[62,     1] loss: 906.627
[63,     1] loss: 834.892
[64,     1] loss: 805.417
[65,     1] loss: 861.982
[66,     1] loss: 756.449
[67,     1] loss: 824.303
[68,     1] loss: 772.676
[69,     1] loss: 775.151
[70,     1] loss: 786.352
[71,     1] loss: 751.224
[72,     1] loss: 865.661
[73,     1] loss: 726.249
[74,     1] loss: 738.688
[75,     1] loss: 689.236
[76,     1] loss: 797.446
[77,     1] loss: 739.226
[78,     1] loss: 689.701
[79,     1] loss: 720.925
[80,     1] loss: 760.501
[81,     1] loss: 692.070
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004474609733108703,
 'learning_rate_Hydroxylation-K': 0.0033538018163436958,
 'learning_rate_Hydroxylation-P': 0.0012564656976791044,
 'log_base': 1.0618557864358285,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1463852281,
 'sample_weights': [2.6285312327293577, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.172114635772059,
 'weight_decay_Hydroxylation-K': 5.897298287108609,
 'weight_decay_Hydroxylation-P': 6.727893424014769}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9074.837
[2,     1] loss: 9019.989
[3,     1] loss: 9062.104
[4,     1] loss: 9039.133
[5,     1] loss: 8997.682
[6,     1] loss: 9019.292
[7,     1] loss: 9001.535
[8,     1] loss: 8999.241
[9,     1] loss: 8993.002
[10,     1] loss: 8956.521
[11,     1] loss: 8971.201
[12,     1] loss: 8935.028
[13,     1] loss: 8906.368
[14,     1] loss: 8853.805
[15,     1] loss: 8768.589
[16,     1] loss: 8645.014
[17,     1] loss: 8392.669
[18,     1] loss: 8148.017
[19,     1] loss: 7835.291
[20,     1] loss: 7560.229
[21,     1] loss: 7826.368
[22,     1] loss: 7279.011
[23,     1] loss: 7839.498
[24,     1] loss: 7000.334
[25,     1] loss: 7714.468
[26,     1] loss: 7493.074
[27,     1] loss: 7388.625
[28,     1] loss: 7766.434
[29,     1] loss: 7347.079
[30,     1] loss: 7221.888
[31,     1] loss: 7200.302
[32,     1] loss: 6786.670
[33,     1] loss: 6855.224
[34,     1] loss: 6638.027
[35,     1] loss: 6657.034
[36,     1] loss: 6442.896
[37,     1] loss: 6169.749
[38,     1] loss: 6177.160
[39,     1] loss: 6012.367
[40,     1] loss: 5833.903
[41,     1] loss: 5807.645
[42,     1] loss: 5825.708
[43,     1] loss: 5591.612
[44,     1] loss: 5468.524
[45,     1] loss: 5345.494
[46,     1] loss: 5191.539
[47,     1] loss: 5532.171
[48,     1] loss: 5689.803
[49,     1] loss: 4937.793
[50,     1] loss: 5273.200
[51,     1] loss: 5208.886
[52,     1] loss: 6269.070
[53,     1] loss: 7469.331
[54,     1] loss: 5243.162
[55,     1] loss: 6006.812
[56,     1] loss: 5429.805
[57,     1] loss: 5297.233
[58,     1] loss: 5946.809
[59,     1] loss: 5696.774
[60,     1] loss: 5603.040
[61,     1] loss: 5536.919
[62,     1] loss: 5358.666
[63,     1] loss: 5091.091
[64,     1] loss: 4522.735
[65,     1] loss: 5081.053
[66,     1] loss: 4803.663
[67,     1] loss: 4951.278
[68,     1] loss: 5335.844
[69,     1] loss: 4669.910
[70,     1] loss: 4566.071
[71,     1] loss: 4298.011
[72,     1] loss: 4534.879
[73,     1] loss: 4474.276
[74,     1] loss: 4261.809
[75,     1] loss: 4232.141
[76,     1] loss: 4127.568
[77,     1] loss: 4057.713
[78,     1] loss: 3767.642
[79,     1] loss: 4179.608
[80,     1] loss: 3941.655
[81,     1] loss: 3884.101
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002160132583626898,
 'learning_rate_Hydroxylation-K': 0.004251691326034748,
 'learning_rate_Hydroxylation-P': 0.0061303000630625465,
 'log_base': 2.549248635813169,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2358237297,
 'sample_weights': [27.815652462730494, 3.4770905041700875],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.548083270357517,
 'weight_decay_Hydroxylation-K': 3.7293331472394584,
 'weight_decay_Hydroxylation-P': 9.097950019948351}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.185
[2,     1] loss: 1291.566
[3,     1] loss: 1290.789
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010745863814690386,
 'learning_rate_Hydroxylation-K': 0.003252880620514307,
 'learning_rate_Hydroxylation-P': 0.007714463658005652,
 'log_base': 1.349292923286056,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 237957522,
 'sample_weights': [1.7839768458749796, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.3129859826915204,
 'weight_decay_Hydroxylation-K': 5.294811787448288,
 'weight_decay_Hydroxylation-P': 4.597699449874559}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2081.283
[2,     1] loss: 2080.166
[3,     1] loss: 2077.817
[4,     1] loss: 2093.355
[5,     1] loss: 2075.295
[6,     1] loss: 2090.574
[7,     1] loss: 2080.149
[8,     1] loss: 2080.324
[9,     1] loss: 2080.185
[10,     1] loss: 2081.201
[11,     1] loss: 2086.364
[12,     1] loss: 2079.336
[13,     1] loss: 2079.258
[14,     1] loss: 2076.912
[15,     1] loss: 2076.772
[16,     1] loss: 2076.734
[17,     1] loss: 2072.792
[18,     1] loss: 2070.339
[19,     1] loss: 2065.121
[20,     1] loss: 2058.044
[21,     1] loss: 2046.898
[22,     1] loss: 2044.759
[23,     1] loss: 2019.018
[24,     1] loss: 2015.969
[25,     1] loss: 1987.852
[26,     1] loss: 1967.398
[27,     1] loss: 1951.504
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001406740729799458,
 'learning_rate_Hydroxylation-K': 0.004023051407178901,
 'learning_rate_Hydroxylation-P': 0.004734923461603428,
 'log_base': 1.0702822864629729,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 245338470,
 'sample_weights': [5.572599225841213, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.364687150844828,
 'weight_decay_Hydroxylation-K': 3.121674812080099,
 'weight_decay_Hydroxylation-P': 7.797417562493646}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7970.140
[2,     1] loss: 7976.732
[3,     1] loss: 7965.696
[4,     1] loss: 7964.833
[5,     1] loss: 7987.370
[6,     1] loss: 7978.602
[7,     1] loss: 7968.172
[8,     1] loss: 7919.531
[9,     1] loss: 7927.760
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004939682644176651,
 'learning_rate_Hydroxylation-K': 0.00037880706945890314,
 'learning_rate_Hydroxylation-P': 0.004009958511569233,
 'log_base': 1.0433623019667935,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3406423625,
 'sample_weights': [24.57867126524308, 3.0724522667948806],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.453813303835515,
 'weight_decay_Hydroxylation-K': 0.5834114758456206,
 'weight_decay_Hydroxylation-P': 6.424697023058955}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12794.257
[2,     1] loss: 12749.268
[3,     1] loss: 12749.753
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013529581530112664,
 'learning_rate_Hydroxylation-K': 0.003951925125718816,
 'learning_rate_Hydroxylation-P': 3.901094582812027e-05,
 'log_base': 2.359821262396094,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1745999405,
 'sample_weights': [39.328689969445925, 4.916275633564123],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5452676780442576,
 'weight_decay_Hydroxylation-K': 7.742818831512867,
 'weight_decay_Hydroxylation-P': 8.677849732489737}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1330.454
[2,     1] loss: 1319.688
[3,     1] loss: 1319.042
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009826216017886571,
 'learning_rate_Hydroxylation-K': 0.00982202869872566,
 'learning_rate_Hydroxylation-P': 0.008536209226127115,
 'log_base': 1.5062537051986118,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 610328561,
 'sample_weights': [1.9444102057965822, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.416904613301026,
 'weight_decay_Hydroxylation-K': 5.598809119670892,
 'weight_decay_Hydroxylation-P': 6.655827307461088}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1765.945
[2,     1] loss: 1768.757
[3,     1] loss: 1776.928
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004819804349914317,
 'learning_rate_Hydroxylation-K': 0.0005229343731739619,
 'learning_rate_Hydroxylation-P': 0.004617752024532634,
 'log_base': 2.9319604392074434,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2245209949,
 'sample_weights': [4.075534430778523, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.290178217973025,
 'weight_decay_Hydroxylation-K': 9.126171476735456,
 'weight_decay_Hydroxylation-P': 5.0894615167593615}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.334
[2,     1] loss: 1234.802
[3,     1] loss: 1236.002
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006296542900095718,
 'learning_rate_Hydroxylation-K': 0.0012810565688964057,
 'learning_rate_Hydroxylation-P': 0.003526693350713919,
 'log_base': 2.4161589133025614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 686445081,
 'sample_weights': [1.5520012118323823, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.926984057262586,
 'weight_decay_Hydroxylation-K': 1.680872632597111,
 'weight_decay_Hydroxylation-P': 9.324663330876408}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1309.793
[2,     1] loss: 1313.713
[3,     1] loss: 1306.488
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008779531044304355,
 'learning_rate_Hydroxylation-K': 0.006714113997020222,
 'learning_rate_Hydroxylation-P': 0.007255892619214257,
 'log_base': 1.0215076136844843,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3087182582,
 'sample_weights': [1.8924085095999719, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6946560807172482,
 'weight_decay_Hydroxylation-K': 4.927706006047784,
 'weight_decay_Hydroxylation-P': 6.0150906455867945}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25526.039
Exploding loss, terminate run (best metric=0.533255398273468)
Finished Training
Total time taken: 0.2109994888305664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25527.910
Exploding loss, terminate run (best metric=0.5281378030776978)
Finished Training
Total time taken: 0.2200024127960205
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25656.969
Exploding loss, terminate run (best metric=0.5582930445671082)
Finished Training
Total time taken: 0.21500301361083984
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25463.938
Exploding loss, terminate run (best metric=0.5290948748588562)
Finished Training
Total time taken: 0.23200058937072754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 25416.943
Exploding loss, terminate run (best metric=0.5280659794807434)
Finished Training
Total time taken: 0.20100045204162598
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25509.441
Exploding loss, terminate run (best metric=0.5430830121040344)
Finished Training
Total time taken: 0.22900104522705078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25342.578
Exploding loss, terminate run (best metric=0.5273438692092896)
Finished Training
Total time taken: 0.21199846267700195
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25460.906
Exploding loss, terminate run (best metric=0.5306280851364136)
Finished Training
Total time taken: 0.22003507614135742
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25538.361
Exploding loss, terminate run (best metric=0.5270575881004333)
Finished Training
Total time taken: 0.21099853515625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 25570.387
Exploding loss, terminate run (best metric=0.532537043094635)
Finished Training
Total time taken: 0.20199871063232422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25416.688
Exploding loss, terminate run (best metric=0.5314961075782776)
Finished Training
Total time taken: 0.21399998664855957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25498.301
Exploding loss, terminate run (best metric=0.5381491780281067)
Finished Training
Total time taken: 0.2090005874633789
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25551.508
Exploding loss, terminate run (best metric=0.5293485522270203)
Finished Training
Total time taken: 0.21599960327148438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 25683.039
Exploding loss, terminate run (best metric=0.5339482426643372)
Finished Training
Total time taken: 0.21100306510925293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 25509.363
Exploding loss, terminate run (best metric=0.5275092720985413)
Finished Training
Total time taken: 0.21900248527526855
{'Hydroxylation-K Validation Accuracy': 0.4011229314420804, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3368421052631579, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5769005847953217, 'Hydroxylation-K AUC PR': 0.3053629121579441, 'Hydroxylation-K MCC': 0.006839855680567693, 'Hydroxylation-K F1': 0.22287903667214012, 'Validation Loss (Hydroxylation-K)': 0.5598636865615845, 'Hydroxylation-P Validation Accuracy': 0.3936337580156676, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.33495934959349594, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5630005910123372, 'Hydroxylation-P AUC PR': 0.23913808659909447, 'Hydroxylation-P MCC': 0.004410964160187484, 'Hydroxylation-P F1': 0.20074014549583924, 'Validation Loss (Hydroxylation-P)': 0.5331965366999308, 'Validation Loss (total)': 1.0930602312088014, 'TimeToTrain': 0.21480290095011392}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0003033900339496523,
 'learning_rate_Hydroxylation-K': 0.004069635275661671,
 'learning_rate_Hydroxylation-P': 0.003916661528035974,
 'log_base': 2.556914611673301,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4179485642,
 'sample_weights': [78.51098827768021, 9.793482039423047],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7691809317737794,
 'weight_decay_Hydroxylation-K': 4.0330048293143435,
 'weight_decay_Hydroxylation-P': 0.45384478241656634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1291.027
[2,     1] loss: 1284.406
[3,     1] loss: 1286.979
[4,     1] loss: 1283.781
[5,     1] loss: 1278.832
[6,     1] loss: 1285.897
[7,     1] loss: 1284.677
[8,     1] loss: 1285.725
[9,     1] loss: 1283.500
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007383522894131687,
 'learning_rate_Hydroxylation-K': 0.0024300441933411136,
 'learning_rate_Hydroxylation-P': 0.005276805336344344,
 'log_base': 1.3468296519703875,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1983469643,
 'sample_weights': [1.7782710188170698, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.025080092610828,
 'weight_decay_Hydroxylation-K': 2.4764061524711227,
 'weight_decay_Hydroxylation-P': 2.770263069269619}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2091.417
[2,     1] loss: 2084.839
[3,     1] loss: 2084.667
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007803338005925312,
 'learning_rate_Hydroxylation-K': 0.007795973497429492,
 'learning_rate_Hydroxylation-P': 0.009184458500703898,
 'log_base': 1.030789268232295,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 718417109,
 'sample_weights': [5.606797467603204, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0115923950310726,
 'weight_decay_Hydroxylation-K': 5.530220454629591,
 'weight_decay_Hydroxylation-P': 4.947020590530321}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17876.773
Exploding loss, terminate run (best metric=0.5337057113647461)
Finished Training
Total time taken: 0.22151851654052734
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17846.246
Exploding loss, terminate run (best metric=0.5419592261314392)
Finished Training
Total time taken: 0.22051167488098145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17874.697
Exploding loss, terminate run (best metric=0.5269994139671326)
Finished Training
Total time taken: 0.21200156211853027
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17874.043
Exploding loss, terminate run (best metric=0.5355854034423828)
Finished Training
Total time taken: 0.21300244331359863
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17884.270
Exploding loss, terminate run (best metric=0.5268905162811279)
Finished Training
Total time taken: 0.21400070190429688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17894.508
Exploding loss, terminate run (best metric=0.5316193699836731)
Finished Training
Total time taken: 0.21001172065734863
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17960.062
Exploding loss, terminate run (best metric=0.539456844329834)
Finished Training
Total time taken: 0.22218751907348633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17963.789
Exploding loss, terminate run (best metric=0.526985228061676)
Finished Training
Total time taken: 0.21100068092346191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17838.535
Exploding loss, terminate run (best metric=0.5302091836929321)
Finished Training
Total time taken: 0.21000051498413086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17935.090
Exploding loss, terminate run (best metric=0.5459742546081543)
Finished Training
Total time taken: 0.2110004425048828
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17922.686
Exploding loss, terminate run (best metric=0.5325831770896912)
Finished Training
Total time taken: 0.2169971466064453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17933.115
Exploding loss, terminate run (best metric=0.5286005735397339)
Finished Training
Total time taken: 0.21100091934204102
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17866.602
Exploding loss, terminate run (best metric=0.549143373966217)
Finished Training
Total time taken: 0.21500158309936523
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17861.188
Exploding loss, terminate run (best metric=0.5413193106651306)
Finished Training
Total time taken: 0.2089998722076416
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17874.648
Exploding loss, terminate run (best metric=0.5378388166427612)
Finished Training
Total time taken: 0.21599984169006348
{'Hydroxylation-K Validation Accuracy': 0.4805555555555555, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6920662768031189, 'Hydroxylation-K AUC PR': 0.3882486610580323, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1792282430213465, 'Validation Loss (Hydroxylation-K)': 0.5616823554039001, 'Hydroxylation-P Validation Accuracy': 0.47872588531885013, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6175712084757423, 'Hydroxylation-P AUC PR': 0.2902456504130201, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.16050727048035554, 'Validation Loss (Hydroxylation-P)': 0.5352580269177755, 'Validation Loss (total)': 1.0969403982162476, 'TimeToTrain': 0.21421567598978677}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014512576309708724,
 'learning_rate_Hydroxylation-K': 0.000920683318077671,
 'learning_rate_Hydroxylation-P': 0.007465259102208247,
 'log_base': 2.8391737092822407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2627255324,
 'sample_weights': [55.092932640559916, 6.872307407546107],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.371085330657115,
 'weight_decay_Hydroxylation-K': 9.639888715258472,
 'weight_decay_Hydroxylation-P': 2.0486124878142653}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.438
[2,     1] loss: 1248.316
[3,     1] loss: 1244.507
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061881495553642,
 'learning_rate_Hydroxylation-K': 0.0001576876385649534,
 'learning_rate_Hydroxylation-P': 0.005237851459781733,
 'log_base': 2.444372825001563,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2641829037,
 'sample_weights': [1.5998296596095254, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.113361101931823,
 'weight_decay_Hydroxylation-K': 3.193337840829978,
 'weight_decay_Hydroxylation-P': 9.073991189031316}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1304.577
[2,     1] loss: 1303.064
[3,     1] loss: 1310.129
[4,     1] loss: 1303.222
[5,     1] loss: 1301.903
[6,     1] loss: 1301.206
[7,     1] loss: 1294.432
[8,     1] loss: 1297.487
[9,     1] loss: 1285.286
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00793751639035665,
 'learning_rate_Hydroxylation-K': 0.007609489215515287,
 'learning_rate_Hydroxylation-P': 0.006304715560378777,
 'log_base': 1.0852370361665253,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3725442071,
 'sample_weights': [1.8678277960674934, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5986733309495804,
 'weight_decay_Hydroxylation-K': 6.6927627947927455,
 'weight_decay_Hydroxylation-P': 4.172812320243166}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6620.060
[2,     1] loss: 6631.285
[3,     1] loss: 6618.608
[4,     1] loss: 6608.419
[5,     1] loss: 6637.508
[6,     1] loss: 6632.806
[7,     1] loss: 6597.737
[8,     1] loss: 6569.633
[9,     1] loss: 6588.911
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004988447730515132,
 'learning_rate_Hydroxylation-K': 0.007290652241736533,
 'learning_rate_Hydroxylation-P': 0.008446209982421583,
 'log_base': 1.1071937449590559,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 152392862,
 'sample_weights': [20.40923223915943, 2.5512523105926483],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3667399223448236,
 'weight_decay_Hydroxylation-K': 5.476079785930248,
 'weight_decay_Hydroxylation-P': 5.45499006344629}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5317.247
[2,     1] loss: 5330.982
[3,     1] loss: 5335.641
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008543511093647146,
 'learning_rate_Hydroxylation-K': 0.007956138027040097,
 'learning_rate_Hydroxylation-P': 0.009934588430847916,
 'log_base': 1.1827805633056665,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3904092248,
 'sample_weights': [16.39463001622235, 2.049407700400666],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5529329790972692,
 'weight_decay_Hydroxylation-K': 4.0028096789551215,
 'weight_decay_Hydroxylation-P': 5.1070020805152705}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3236.983
[2,     1] loss: 3243.017
[3,     1] loss: 3229.591
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004489527189086476,
 'learning_rate_Hydroxylation-K': 0.001765270888024105,
 'learning_rate_Hydroxylation-P': 0.0006638663413239504,
 'log_base': 2.8300756741629067,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3793250739,
 'sample_weights': [9.944970993975991, 1.2431692642742491],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.928314717585504,
 'weight_decay_Hydroxylation-K': 5.1159034152285745,
 'weight_decay_Hydroxylation-P': 6.3048038010928185}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.993
[2,     1] loss: 1247.710
[3,     1] loss: 1252.601
[4,     1] loss: 1246.647
[5,     1] loss: 1245.647
[6,     1] loss: 1250.447
[7,     1] loss: 1245.689
[8,     1] loss: 1245.359
[9,     1] loss: 1243.312
[10,     1] loss: 1240.244
[11,     1] loss: 1234.390
[12,     1] loss: 1229.060
[13,     1] loss: 1211.167
[14,     1] loss: 1188.227
[15,     1] loss: 1139.602
[16,     1] loss: 1117.790
[17,     1] loss: 1071.479
[18,     1] loss: 1057.621
[19,     1] loss: 1052.441
[20,     1] loss: 1063.237
[21,     1] loss: 976.904
[22,     1] loss: 1014.984
[23,     1] loss: 979.395
[24,     1] loss: 990.828
[25,     1] loss: 994.482
[26,     1] loss: 967.946
[27,     1] loss: 1001.897
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035830508246952946,
 'learning_rate_Hydroxylation-K': 0.004069991033851439,
 'learning_rate_Hydroxylation-P': 0.004170752432580752,
 'log_base': 1.4558876295547785,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1723144535,
 'sample_weights': [1.6047655569750343, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.572864921001152,
 'weight_decay_Hydroxylation-K': 1.2365281527395047,
 'weight_decay_Hydroxylation-P': 4.819931906160944}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1851.274
[2,     1] loss: 1848.715
[3,     1] loss: 1846.816
[4,     1] loss: 1848.282
[5,     1] loss: 1839.979
[6,     1] loss: 1852.505
[7,     1] loss: 1841.047
[8,     1] loss: 1849.433
[9,     1] loss: 1834.050
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007113191335552224,
 'learning_rate_Hydroxylation-K': 0.005017793709294972,
 'learning_rate_Hydroxylation-P': 0.004717368095702897,
 'log_base': 2.9227570130075966,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1805796331,
 'sample_weights': [4.4445502130218735, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.151525937179687,
 'weight_decay_Hydroxylation-K': 4.105293881220777,
 'weight_decay_Hydroxylation-P': 7.627735085374303}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.202
[2,     1] loss: 1240.038
[3,     1] loss: 1243.608
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002788049261225282,
 'learning_rate_Hydroxylation-K': 0.0013053691038532162,
 'learning_rate_Hydroxylation-P': 0.002781142241390513,
 'log_base': 1.6727348402340183,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2075256194,
 'sample_weights': [1.5565506485202492, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.75410255965801,
 'weight_decay_Hydroxylation-K': 6.768751413576996,
 'weight_decay_Hydroxylation-P': 0.190079099384739}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1598.653
[2,     1] loss: 1593.711
[3,     1] loss: 1595.091
[4,     1] loss: 1592.802
[5,     1] loss: 1589.963
[6,     1] loss: 1589.193
[7,     1] loss: 1589.683
[8,     1] loss: 1589.870
[9,     1] loss: 1590.459
[10,     1] loss: 1591.266
[11,     1] loss: 1590.153
[12,     1] loss: 1586.076
[13,     1] loss: 1586.848
[14,     1] loss: 1580.493
[15,     1] loss: 1568.943
[16,     1] loss: 1560.765
[17,     1] loss: 1550.949
[18,     1] loss: 1523.096
[19,     1] loss: 1500.346
[20,     1] loss: 1469.413
[21,     1] loss: 1441.634
[22,     1] loss: 1432.652
[23,     1] loss: 1405.754
[24,     1] loss: 1431.257
[25,     1] loss: 1383.611
[26,     1] loss: 1309.274
[27,     1] loss: 1349.906
[28,     1] loss: 1262.145
[29,     1] loss: 1321.709
[30,     1] loss: 1239.813
[31,     1] loss: 1274.497
[32,     1] loss: 1306.504
[33,     1] loss: 1325.875
[34,     1] loss: 1241.122
[35,     1] loss: 1241.726
[36,     1] loss: 1235.884
[37,     1] loss: 1299.217
[38,     1] loss: 1201.865
[39,     1] loss: 1176.699
[40,     1] loss: 1120.516
[41,     1] loss: 1168.057
[42,     1] loss: 1239.405
[43,     1] loss: 1181.619
[44,     1] loss: 1109.812
[45,     1] loss: 1130.429
[46,     1] loss: 1090.504
[47,     1] loss: 1191.815
[48,     1] loss: 1072.172
[49,     1] loss: 1161.698
[50,     1] loss: 1127.279
[51,     1] loss: 1141.552
[52,     1] loss: 1072.815
[53,     1] loss: 1065.466
[54,     1] loss: 1050.701
[55,     1] loss: 1028.022
[56,     1] loss: 952.364
[57,     1] loss: 973.438
[58,     1] loss: 934.864
[59,     1] loss: 993.354
[60,     1] loss: 940.609
[61,     1] loss: 970.331
[62,     1] loss: 886.296
[63,     1] loss: 904.683
[64,     1] loss: 909.773
[65,     1] loss: 958.016
[66,     1] loss: 856.120
[67,     1] loss: 839.394
[68,     1] loss: 814.572
[69,     1] loss: 847.442
[70,     1] loss: 920.597
[71,     1] loss: 853.870
[72,     1] loss: 825.628
[73,     1] loss: 815.372
[74,     1] loss: 868.507
[75,     1] loss: 810.373
[76,     1] loss: 847.717
[77,     1] loss: 769.081
[78,     1] loss: 809.765
[79,     1] loss: 796.893
[80,     1] loss: 757.109
[81,     1] loss: 795.884
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026113485324478617,
 'learning_rate_Hydroxylation-K': 0.0058428831913153205,
 'learning_rate_Hydroxylation-P': 0.004034467932782805,
 'log_base': 1.1221326870901556,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2464769272,
 'sample_weights': [3.245040276141376, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.727986270977716,
 'weight_decay_Hydroxylation-K': 6.297397060783579,
 'weight_decay_Hydroxylation-P': 6.010557301254946}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4734.705
[2,     1] loss: 4713.749
[3,     1] loss: 4697.925
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008032824548763253,
 'learning_rate_Hydroxylation-K': 0.0003178501504568689,
 'learning_rate_Hydroxylation-P': 0.008164926579675377,
 'log_base': 2.9854866119869756,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3170559217,
 'sample_weights': [14.487787875820285, 1.8110432504483553],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2528762725461626,
 'weight_decay_Hydroxylation-K': 1.456184042971583,
 'weight_decay_Hydroxylation-P': 7.658309063612007}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.934
[2,     1] loss: 1231.273
[3,     1] loss: 1237.918
[4,     1] loss: 1226.887
[5,     1] loss: 1233.188
[6,     1] loss: 1230.595
[7,     1] loss: 1223.450
[8,     1] loss: 1224.808
[9,     1] loss: 1223.766
[10,     1] loss: 1219.500
[11,     1] loss: 1211.769
[12,     1] loss: 1200.276
[13,     1] loss: 1180.671
[14,     1] loss: 1155.665
[15,     1] loss: 1125.545
[16,     1] loss: 1067.373
[17,     1] loss: 1061.671
[18,     1] loss: 1077.709
[19,     1] loss: 1002.260
[20,     1] loss: 1022.405
[21,     1] loss: 982.929
[22,     1] loss: 986.548
[23,     1] loss: 1008.953
[24,     1] loss: 962.643
[25,     1] loss: 961.244
[26,     1] loss: 947.132
[27,     1] loss: 932.536
[28,     1] loss: 1058.561
[29,     1] loss: 929.625
[30,     1] loss: 905.256
[31,     1] loss: 887.657
[32,     1] loss: 884.151
[33,     1] loss: 864.941
[34,     1] loss: 821.557
[35,     1] loss: 830.455
[36,     1] loss: 870.470
[37,     1] loss: 863.599
[38,     1] loss: 846.122
[39,     1] loss: 959.669
[40,     1] loss: 821.038
[41,     1] loss: 960.605
[42,     1] loss: 843.045
[43,     1] loss: 875.393
[44,     1] loss: 852.174
[45,     1] loss: 845.359
[46,     1] loss: 824.750
[47,     1] loss: 786.925
[48,     1] loss: 730.110
[49,     1] loss: 761.723
[50,     1] loss: 804.344
[51,     1] loss: 746.477
[52,     1] loss: 744.135
[53,     1] loss: 764.578
[54,     1] loss: 688.622
[55,     1] loss: 673.630
[56,     1] loss: 715.034
[57,     1] loss: 771.708
[58,     1] loss: 966.862
[59,     1] loss: 818.071
[60,     1] loss: 700.695
[61,     1] loss: 716.817
[62,     1] loss: 688.920
[63,     1] loss: 721.874
[64,     1] loss: 660.689
[65,     1] loss: 658.881
[66,     1] loss: 629.810
[67,     1] loss: 625.563
[68,     1] loss: 593.011
[69,     1] loss: 590.454
[70,     1] loss: 557.177
[71,     1] loss: 575.797
[72,     1] loss: 534.201
[73,     1] loss: 643.854
[74,     1] loss: 1124.411
[75,     1] loss: 521.654
[76,     1] loss: 958.297
[77,     1] loss: 698.658
[78,     1] loss: 953.451
[79,     1] loss: 861.363
[80,     1] loss: 699.560
[81,     1] loss: 781.485
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004675534290920261,
 'learning_rate_Hydroxylation-K': 0.007075849835332927,
 'learning_rate_Hydroxylation-P': 0.0033203780996599137,
 'log_base': 1.571737915445755,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 725209226,
 'sample_weights': [1.5263302241547747, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.7512421056356615,
 'weight_decay_Hydroxylation-K': 5.644284026788274,
 'weight_decay_Hydroxylation-P': 3.1002875834495156}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1685.517
[2,     1] loss: 1686.659
[3,     1] loss: 1683.250
[4,     1] loss: 1689.878
[5,     1] loss: 1680.783
[6,     1] loss: 1683.744
[7,     1] loss: 1667.233
[8,     1] loss: 1649.171
[9,     1] loss: 1612.385
[10,     1] loss: 1589.192
[11,     1] loss: 1531.304
[12,     1] loss: 1504.374
[13,     1] loss: 1482.168
[14,     1] loss: 1411.403
[15,     1] loss: 1434.445
[16,     1] loss: 1392.476
[17,     1] loss: 1364.128
[18,     1] loss: 1360.940
[19,     1] loss: 1388.482
[20,     1] loss: 1343.651
[21,     1] loss: 1310.407
[22,     1] loss: 1338.123
[23,     1] loss: 1377.381
[24,     1] loss: 1301.617
[25,     1] loss: 1301.929
[26,     1] loss: 1393.040
[27,     1] loss: 1286.613
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009087123770933389,
 'learning_rate_Hydroxylation-K': 0.002576479322505873,
 'learning_rate_Hydroxylation-P': 0.0007747174109615107,
 'log_base': 1.4305075449936164,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3711065994,
 'sample_weights': [3.6919720298081433, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4852524128802014,
 'weight_decay_Hydroxylation-K': 1.918429127633302,
 'weight_decay_Hydroxylation-P': 5.7799429154175455}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1897.735
[2,     1] loss: 1897.979
[3,     1] loss: 1918.194
[4,     1] loss: 1893.319
[5,     1] loss: 1894.821
[6,     1] loss: 1885.515
[7,     1] loss: 1890.608
[8,     1] loss: 1892.592
[9,     1] loss: 1900.082
[10,     1] loss: 1890.504
[11,     1] loss: 1890.096
[12,     1] loss: 1894.537
[13,     1] loss: 1888.487
[14,     1] loss: 1888.793
[15,     1] loss: 1896.729
[16,     1] loss: 1891.106
[17,     1] loss: 1894.040
[18,     1] loss: 1888.577
[19,     1] loss: 1890.042
[20,     1] loss: 1889.219
[21,     1] loss: 1891.625
[22,     1] loss: 1889.251
[23,     1] loss: 1888.287
[24,     1] loss: 1887.813
[25,     1] loss: 1889.938
[26,     1] loss: 1890.170
[27,     1] loss: 1890.088
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0058587957400999605,
 'learning_rate_Hydroxylation-K': 0.009209144040897222,
 'learning_rate_Hydroxylation-P': 0.006219079675460678,
 'log_base': 1.0165105775263645,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1448987432,
 'sample_weights': [4.662867286924378, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.162102042589211,
 'weight_decay_Hydroxylation-K': 9.44576670468992,
 'weight_decay_Hydroxylation-P': 2.007356514173866}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33075.562
Exploding loss, terminate run (best metric=0.537662148475647)
Finished Training
Total time taken: 0.20801019668579102
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33310.691
Exploding loss, terminate run (best metric=0.5272320508956909)
Finished Training
Total time taken: 0.21899795532226562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32980.414
Exploding loss, terminate run (best metric=0.5307615399360657)
Finished Training
Total time taken: 0.21300053596496582
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33128.535
Exploding loss, terminate run (best metric=0.5257456302642822)
Finished Training
Total time taken: 0.2199993133544922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 33557.766
Exploding loss, terminate run (best metric=0.534931480884552)
Finished Training
Total time taken: 0.2199997901916504
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33086.668
Exploding loss, terminate run (best metric=0.5312923192977905)
Finished Training
Total time taken: 0.2049999237060547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32973.133
Exploding loss, terminate run (best metric=0.5343438386917114)
Finished Training
Total time taken: 0.2199993133544922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33232.438
Exploding loss, terminate run (best metric=0.5334328413009644)
Finished Training
Total time taken: 0.20901989936828613
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33172.844
Exploding loss, terminate run (best metric=0.5273683071136475)
Finished Training
Total time taken: 0.21300148963928223
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 33031.859
Exploding loss, terminate run (best metric=0.534214973449707)
Finished Training
Total time taken: 0.2130119800567627
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33259.000
Exploding loss, terminate run (best metric=0.536470353603363)
Finished Training
Total time taken: 0.21400237083435059
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33111.984
Exploding loss, terminate run (best metric=0.5298367142677307)
Finished Training
Total time taken: 0.24599909782409668
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33141.871
Exploding loss, terminate run (best metric=0.528641402721405)
Finished Training
Total time taken: 0.20999765396118164
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 33083.586
Exploding loss, terminate run (best metric=0.529748260974884)
Finished Training
Total time taken: 0.21400046348571777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 33207.641
Exploding loss, terminate run (best metric=0.5606281161308289)
Finished Training
Total time taken: 0.22002887725830078
{'Hydroxylation-K Validation Accuracy': 0.5194444444444444, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6533723196881092, 'Hydroxylation-K AUC PR': 0.3395186763738457, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.15623973727422005, 'Validation Loss (Hydroxylation-K)': 0.5598480423291524, 'Hydroxylation-P Validation Accuracy': 0.5208206690015735, 'Hydroxylation-P Validation Sensitivity': 0.46476190476190476, 'Hydroxylation-P Validation Specificity': 0.5333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.622699036170905, 'Hydroxylation-P AUC PR': 0.3015267114964944, 'Hydroxylation-P MCC': -0.010255670228972776, 'Hydroxylation-P F1': 0.1395104490034293, 'Validation Loss (Hydroxylation-P)': 0.533487331867218, 'Validation Loss (total)': 1.0933353583017984, 'TimeToTrain': 0.2162712574005127}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004651652614561333,
 'learning_rate_Hydroxylation-K': 0.009627822045064266,
 'learning_rate_Hydroxylation-P': 0.005383888926112502,
 'log_base': 1.2247458859313138,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2143096133,
 'sample_weights': [102.0216192545651, 12.726204544370587],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.457875709349289,
 'weight_decay_Hydroxylation-K': 9.155078248070081,
 'weight_decay_Hydroxylation-P': 1.3492457150977133}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2686.174
[2,     1] loss: 2679.120
[3,     1] loss: 2657.573
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008666444851888614,
 'learning_rate_Hydroxylation-K': 0.008385760717391963,
 'learning_rate_Hydroxylation-P': 0.006891695671572631,
 'log_base': 1.11677503082876,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1126762082,
 'sample_weights': [8.234673182480654, 1.0293738018948793],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.240608273696415,
 'weight_decay_Hydroxylation-K': 9.207721189142548,
 'weight_decay_Hydroxylation-P': 1.1217752380550552}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4928.031
[2,     1] loss: 4955.791
[3,     1] loss: 4911.465
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003904598960964717,
 'learning_rate_Hydroxylation-K': 0.006001143146378125,
 'learning_rate_Hydroxylation-P': 0.0016776973217841166,
 'log_base': 1.3655909654035367,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 31075549,
 'sample_weights': [15.115593386179565, 1.889521962441927],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.342860085453635,
 'weight_decay_Hydroxylation-K': 9.39729983181819,
 'weight_decay_Hydroxylation-P': 8.462796382217933}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2033.720
[2,     1] loss: 2037.242
[3,     1] loss: 2033.137
[4,     1] loss: 2039.414
[5,     1] loss: 2035.873
[6,     1] loss: 2044.043
[7,     1] loss: 2028.345
[8,     1] loss: 2026.702
[9,     1] loss: 2022.830
[10,     1] loss: 2012.861
[11,     1] loss: 1994.743
[12,     1] loss: 1964.158
[13,     1] loss: 1933.583
[14,     1] loss: 1866.236
[15,     1] loss: 1798.312
[16,     1] loss: 1803.251
[17,     1] loss: 1811.941
[18,     1] loss: 1750.195
[19,     1] loss: 1693.300
[20,     1] loss: 1675.357
[21,     1] loss: 1724.568
[22,     1] loss: 1595.173
[23,     1] loss: 1616.325
[24,     1] loss: 1686.328
[25,     1] loss: 1652.923
[26,     1] loss: 1541.150
[27,     1] loss: 1549.741
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004446164305353775,
 'learning_rate_Hydroxylation-K': 0.009628045214849262,
 'learning_rate_Hydroxylation-P': 0.005030942499115206,
 'log_base': 1.7298703738308305,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 102708748,
 'sample_weights': [5.357866872702389, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.799184005278848,
 'weight_decay_Hydroxylation-K': 8.929249863049929,
 'weight_decay_Hydroxylation-P': 2.476800391870171}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1548.955
[2,     1] loss: 1551.827
[3,     1] loss: 1552.045
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0077337354426377965,
 'learning_rate_Hydroxylation-K': 0.009225644394799754,
 'learning_rate_Hydroxylation-P': 0.009016461305651483,
 'log_base': 1.306367759771137,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 215108054,
 'sample_weights': [3.0461707476096374, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.47980097842781,
 'weight_decay_Hydroxylation-K': 8.857153758059503,
 'weight_decay_Hydroxylation-P': 3.7672324602607987}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2236.728
[2,     1] loss: 2224.956
[3,     1] loss: 2230.256
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005271185642802998,
 'learning_rate_Hydroxylation-K': 0.00745351131257589,
 'learning_rate_Hydroxylation-P': 0.004101132065544206,
 'log_base': 1.0236857372612216,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 181607545,
 'sample_weights': [6.24673340047392, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.625989362170883,
 'weight_decay_Hydroxylation-K': 9.064541395951803,
 'weight_decay_Hydroxylation-P': 5.071782009399305}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23220.779
Exploding loss, terminate run (best metric=0.5312023758888245)
Finished Training
Total time taken: 0.20806336402893066
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23200.941
Exploding loss, terminate run (best metric=0.5301624536514282)
Finished Training
Total time taken: 0.21803760528564453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23107.607
Exploding loss, terminate run (best metric=0.5297885537147522)
Finished Training
Total time taken: 0.21500015258789062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23185.775
Exploding loss, terminate run (best metric=0.5298072695732117)
Finished Training
Total time taken: 0.22700023651123047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23116.754
Exploding loss, terminate run (best metric=0.5278023481369019)
Finished Training
Total time taken: 0.2089996337890625
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23311.131
Exploding loss, terminate run (best metric=0.5322812795639038)
Finished Training
Total time taken: 0.23703217506408691
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23137.973
Exploding loss, terminate run (best metric=0.529530942440033)
Finished Training
Total time taken: 0.21299958229064941
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23192.705
Exploding loss, terminate run (best metric=0.5261216759681702)
Finished Training
Total time taken: 0.2389991283416748
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23212.811
Exploding loss, terminate run (best metric=0.5292809009552002)
Finished Training
Total time taken: 0.2090001106262207
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23283.602
Exploding loss, terminate run (best metric=0.5278420448303223)
Finished Training
Total time taken: 0.2120037078857422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23165.598
Exploding loss, terminate run (best metric=0.5326735377311707)
Finished Training
Total time taken: 0.21699905395507812
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23229.375
Exploding loss, terminate run (best metric=0.5325099229812622)
Finished Training
Total time taken: 0.21000051498413086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23225.293
Exploding loss, terminate run (best metric=0.5267546772956848)
Finished Training
Total time taken: 0.21701598167419434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23215.074
Exploding loss, terminate run (best metric=0.5296171307563782)
Finished Training
Total time taken: 0.20100021362304688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23181.609
Exploding loss, terminate run (best metric=0.536410927772522)
Finished Training
Total time taken: 0.2040247917175293
{'Hydroxylation-K Validation Accuracy': 0.5046690307328605, 'Hydroxylation-K Validation Sensitivity': 0.4740740740740741, 'Hydroxylation-K Validation Specificity': 0.5105263157894737, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6035672514619883, 'Hydroxylation-K AUC PR': 0.3262554754615339, 'Hydroxylation-K MCC': -0.01324957022788384, 'Hydroxylation-K F1': 0.16359677304205042, 'Validation Loss (Hydroxylation-K)': 0.5574519832928976, 'Hydroxylation-P Validation Accuracy': 0.5037309104444783, 'Hydroxylation-P Validation Sensitivity': 0.4895238095238095, 'Hydroxylation-P Validation Specificity': 0.5056910569105691, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6046523091837697, 'Hydroxylation-P AUC PR': 0.3159229806269339, 'Hydroxylation-P MCC': -0.00747511707089063, 'Hydroxylation-P F1': 0.15511950568056132, 'Validation Loss (Hydroxylation-P)': 0.5301190694173177, 'Validation Loss (total)': 1.0875710566838583, 'TimeToTrain': 0.2157450834910075}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00929181904280154,
 'learning_rate_Hydroxylation-K': 0.007534656023478262,
 'learning_rate_Hydroxylation-P': 0.0024097832239336502,
 'log_base': 1.6059726370081666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4052980638,
 'sample_weights': [71.36742192278048, 8.902391628655332],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5628783923267477,
 'weight_decay_Hydroxylation-K': 7.5435415719232335,
 'weight_decay_Hydroxylation-P': 1.1251059132209207}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1660.998
[2,     1] loss: 1654.736
[3,     1] loss: 1651.572
[4,     1] loss: 1656.449
[5,     1] loss: 1662.162
[6,     1] loss: 1651.576
[7,     1] loss: 1654.896
[8,     1] loss: 1649.998
[9,     1] loss: 1646.537
[10,     1] loss: 1653.384
[11,     1] loss: 1649.397
[12,     1] loss: 1646.605
[13,     1] loss: 1648.628
[14,     1] loss: 1651.833
[15,     1] loss: 1645.397
[16,     1] loss: 1646.616
[17,     1] loss: 1642.769
[18,     1] loss: 1636.041
[19,     1] loss: 1628.449
[20,     1] loss: 1623.408
[21,     1] loss: 1601.137
[22,     1] loss: 1593.136
[23,     1] loss: 1565.649
[24,     1] loss: 1554.192
[25,     1] loss: 1486.359
[26,     1] loss: 1462.516
[27,     1] loss: 1436.641
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005328939432616751,
 'learning_rate_Hydroxylation-K': 0.009503206421714549,
 'learning_rate_Hydroxylation-P': 0.004393626295402087,
 'log_base': 1.3693009257415851,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2890064836,
 'sample_weights': [3.5240424644358086, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.274192029715351,
 'weight_decay_Hydroxylation-K': 7.19778294234483,
 'weight_decay_Hydroxylation-P': 3.495300088360252}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2040.067
[2,     1] loss: 2025.067
[3,     1] loss: 2022.065
[4,     1] loss: 2036.233
[5,     1] loss: 2024.746
[6,     1] loss: 2026.986
[7,     1] loss: 2020.810
[8,     1] loss: 2021.748
[9,     1] loss: 2023.716
[10,     1] loss: 2025.202
[11,     1] loss: 2017.820
[12,     1] loss: 1995.065
[13,     1] loss: 1984.980
[14,     1] loss: 1957.211
[15,     1] loss: 1909.609
[16,     1] loss: 1890.001
[17,     1] loss: 1859.374
[18,     1] loss: 1800.963
[19,     1] loss: 1766.469
[20,     1] loss: 1769.041
[21,     1] loss: 1585.320
[22,     1] loss: 1826.656
[23,     1] loss: 1693.297
[24,     1] loss: 1762.016
[25,     1] loss: 1683.987
[26,     1] loss: 1619.769
[27,     1] loss: 1647.289
[28,     1] loss: 1648.039
[29,     1] loss: 1651.035
[30,     1] loss: 1688.496
[31,     1] loss: 1573.563
[32,     1] loss: 1566.800
[33,     1] loss: 1534.023
[34,     1] loss: 1472.386
[35,     1] loss: 1542.489
[36,     1] loss: 1500.668
[37,     1] loss: 1469.641
[38,     1] loss: 1471.042
[39,     1] loss: 1587.050
[40,     1] loss: 1376.390
[41,     1] loss: 1443.222
[42,     1] loss: 1355.491
[43,     1] loss: 1341.686
[44,     1] loss: 1426.380
[45,     1] loss: 1272.127
[46,     1] loss: 1283.733
[47,     1] loss: 1216.387
[48,     1] loss: 1137.926
[49,     1] loss: 1227.688
[50,     1] loss: 1642.193
[51,     1] loss: 1247.536
[52,     1] loss: 1279.186
[53,     1] loss: 1278.284
[54,     1] loss: 1266.900
[55,     1] loss: 1273.354
[56,     1] loss: 1227.032
[57,     1] loss: 1184.664
[58,     1] loss: 1093.486
[59,     1] loss: 1186.469
[60,     1] loss: 1441.106
[61,     1] loss: 1038.107
[62,     1] loss: 1361.838
[63,     1] loss: 1196.296
[64,     1] loss: 1187.739
[65,     1] loss: 1118.454
[66,     1] loss: 1198.252
[67,     1] loss: 1026.829
[68,     1] loss: 1221.725
[69,     1] loss: 1029.523
[70,     1] loss: 1012.051
[71,     1] loss: 1041.048
[72,     1] loss: 1174.828
[73,     1] loss: 865.428
[74,     1] loss: 1086.788
[75,     1] loss: 1036.227
[76,     1] loss: 1021.350
[77,     1] loss: 953.476
[78,     1] loss: 959.639
[79,     1] loss: 1094.930
[80,     1] loss: 1017.589
[81,     1] loss: 1143.612
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002981404219464611,
 'learning_rate_Hydroxylation-K': 0.007566121249075244,
 'learning_rate_Hydroxylation-P': 0.0038036999648683985,
 'log_base': 1.1610287135146022,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4289730798,
 'sample_weights': [5.31161743704277, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.333697834928733,
 'weight_decay_Hydroxylation-K': 8.890555066949458,
 'weight_decay_Hydroxylation-P': 0.9189759035928997}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3630.378
[2,     1] loss: 3638.691
[3,     1] loss: 3628.506
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031011691537837964,
 'learning_rate_Hydroxylation-K': 0.008340888250838322,
 'learning_rate_Hydroxylation-P': 0.003269088993501181,
 'log_base': 1.9943233822803967,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 106868654,
 'sample_weights': [11.181320867187173, 1.3977189520708766],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.07735738351794,
 'weight_decay_Hydroxylation-K': 5.299915113605303,
 'weight_decay_Hydroxylation-P': 2.947245405150647}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.669
[2,     1] loss: 1419.540
[3,     1] loss: 1416.552
[4,     1] loss: 1414.373
[5,     1] loss: 1417.206
[6,     1] loss: 1409.778
[7,     1] loss: 1405.600
[8,     1] loss: 1398.629
[9,     1] loss: 1375.044
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0068182654620094485,
 'learning_rate_Hydroxylation-K': 0.0012150591287213494,
 'learning_rate_Hydroxylation-P': 0.0030092807455188047,
 'log_base': 2.463465019570323,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4031943723,
 'sample_weights': [2.4184143876838395, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.839795439170351,
 'weight_decay_Hydroxylation-K': 4.428922863500486,
 'weight_decay_Hydroxylation-P': 1.2319371772565257}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1297.489
[2,     1] loss: 1299.820
[3,     1] loss: 1293.402
[4,     1] loss: 1296.719
[5,     1] loss: 1297.304
[6,     1] loss: 1292.137
[7,     1] loss: 1290.193
[8,     1] loss: 1286.578
[9,     1] loss: 1263.439
[10,     1] loss: 1252.783
[11,     1] loss: 1203.101
[12,     1] loss: 1152.799
[13,     1] loss: 1114.259
[14,     1] loss: 1079.072
[15,     1] loss: 1141.959
[16,     1] loss: 1110.136
[17,     1] loss: 1035.692
[18,     1] loss: 1034.744
[19,     1] loss: 1045.508
[20,     1] loss: 1058.192
[21,     1] loss: 996.795
[22,     1] loss: 1040.192
[23,     1] loss: 997.760
[24,     1] loss: 1034.070
[25,     1] loss: 992.267
[26,     1] loss: 961.414
[27,     1] loss: 895.839
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005354134060950097,
 'learning_rate_Hydroxylation-K': 0.00704959377481105,
 'learning_rate_Hydroxylation-P': 0.0019810918892483216,
 'log_base': 1.0582340432432233,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 214608518,
 'sample_weights': [1.851708883366421, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.215267743142965,
 'weight_decay_Hydroxylation-K': 8.849434938603748,
 'weight_decay_Hydroxylation-P': 5.675462631748401}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9622.303
[2,     1] loss: 9596.020
[3,     1] loss: 9617.449
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.001727461567816396,
 'learning_rate_Hydroxylation-K': 0.007083920633278523,
 'learning_rate_Hydroxylation-P': 0.0004774449338258882,
 'log_base': 1.1610327723677762,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 551987014,
 'sample_weights': [29.494668912498746, 3.686975645698113],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.473179890897493,
 'weight_decay_Hydroxylation-K': 8.470640292312174,
 'weight_decay_Hydroxylation-P': 5.040934879417767}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3618.756
[2,     1] loss: 3634.941
[3,     1] loss: 3631.119
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006316896738717937,
 'learning_rate_Hydroxylation-K': 0.007076241228417091,
 'learning_rate_Hydroxylation-P': 0.0043947879991089395,
 'log_base': 1.0480469648307407,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 881626997,
 'sample_weights': [11.181059070564082, 1.3976862262323082],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.545928668512311,
 'weight_decay_Hydroxylation-K': 9.205728743427937,
 'weight_decay_Hydroxylation-P': 4.596428301156115}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11572.950
[2,     1] loss: 11518.347
[3,     1] loss: 11464.262
[4,     1] loss: 11625.813
[5,     1] loss: 11696.898
[6,     1] loss: 11487.502
[7,     1] loss: 11514.889
[8,     1] loss: 11519.768
[9,     1] loss: 11563.785
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005489022774771108,
 'learning_rate_Hydroxylation-K': 0.008451512588081538,
 'learning_rate_Hydroxylation-P': 0.0006242017970024079,
 'log_base': 1.013547575059299,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 743539720,
 'sample_weights': [35.574261959322804, 4.4469540528433145],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.596563453290244,
 'weight_decay_Hydroxylation-K': 8.356604595499396,
 'weight_decay_Hydroxylation-P': 6.8222310449882455}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40314.250
Exploding loss, terminate run (best metric=0.535984456539154)
Finished Training
Total time taken: 0.19500017166137695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40230.270
Exploding loss, terminate run (best metric=0.5307649970054626)
Finished Training
Total time taken: 0.20499777793884277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40469.312
Exploding loss, terminate run (best metric=0.5313160419464111)
Finished Training
Total time taken: 0.20400071144104004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40390.691
Exploding loss, terminate run (best metric=0.5278878211975098)
Finished Training
Total time taken: 0.21599841117858887
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40290.555
Exploding loss, terminate run (best metric=0.5336942672729492)
Finished Training
Total time taken: 0.20501136779785156
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40152.070
Exploding loss, terminate run (best metric=0.5321589708328247)
Finished Training
Total time taken: 0.21000266075134277
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40318.211
Exploding loss, terminate run (best metric=0.5295708179473877)
Finished Training
Total time taken: 0.19200968742370605
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40521.215
Exploding loss, terminate run (best metric=0.5377939343452454)
Finished Training
Total time taken: 0.22299718856811523
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40496.047
Exploding loss, terminate run (best metric=0.5321534276008606)
Finished Training
Total time taken: 0.21002554893493652
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40264.070
Exploding loss, terminate run (best metric=0.5285085439682007)
Finished Training
Total time taken: 0.2070024013519287
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40277.562
Exploding loss, terminate run (best metric=0.5372818112373352)
Finished Training
Total time taken: 0.21217727661132812
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40275.117
Exploding loss, terminate run (best metric=0.5283654928207397)
Finished Training
Total time taken: 0.2109994888305664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40654.363
Exploding loss, terminate run (best metric=0.5265782475471497)
Finished Training
Total time taken: 0.21599936485290527
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 40330.781
Exploding loss, terminate run (best metric=0.5305432081222534)
Finished Training
Total time taken: 0.20999717712402344
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 40166.305
Exploding loss, terminate run (best metric=0.5393032431602478)
Finished Training
Total time taken: 0.21001124382019043
{'Hydroxylation-K Validation Accuracy': 0.6272458628841607, 'Hydroxylation-K Validation Sensitivity': 0.32666666666666666, 'Hydroxylation-K Validation Specificity': 0.7035087719298245, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.648187134502924, 'Hydroxylation-K AUC PR': 0.4064142779553297, 'Hydroxylation-K MCC': 0.0407776728510374, 'Hydroxylation-K F1': 0.13850735696577482, 'Validation Loss (Hydroxylation-K)': 0.556851597627004, 'Hydroxylation-P Validation Accuracy': 0.6328319036258735, 'Hydroxylation-P Validation Sensitivity': 0.3142857142857143, 'Hydroxylation-P Validation Specificity': 0.7012195121951219, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5852049561906661, 'Hydroxylation-P AUC PR': 0.2661574180865313, 'Hydroxylation-P MCC': 0.014387251305242542, 'Hydroxylation-P F1': 0.10729181893470821, 'Validation Loss (Hydroxylation-P)': 0.5321270187695821, 'Validation Loss (total)': 1.088978616396586, 'TimeToTrain': 0.20841536521911622}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006688075478131487,
 'learning_rate_Hydroxylation-K': 0.00940486699007727,
 'learning_rate_Hydroxylation-P': 0.0021782824964727103,
 'log_base': 1.1147318448028853,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1935526904,
 'sample_weights': [124.15306732522001, 15.486887398339967],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.932305066369036,
 'weight_decay_Hydroxylation-K': 5.865779809689281,
 'weight_decay_Hydroxylation-P': 6.5142922679970505}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4979.289
[2,     1] loss: 5067.411
[3,     1] loss: 4977.402
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007160268829249937,
 'learning_rate_Hydroxylation-K': 0.007410205578055518,
 'learning_rate_Hydroxylation-P': 0.0021754084085769294,
 'log_base': 1.546414068373754,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2215727085,
 'sample_weights': [15.37044047544465, 1.9213790758167189],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.548500230167004,
 'weight_decay_Hydroxylation-K': 3.6896172378064076,
 'weight_decay_Hydroxylation-P': 4.601149010203274}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1715.905
[2,     1] loss: 1719.973
[3,     1] loss: 1712.088
[4,     1] loss: 1725.861
[5,     1] loss: 1715.436
[6,     1] loss: 1720.302
[7,     1] loss: 1719.437
[8,     1] loss: 1712.832
[9,     1] loss: 1716.624
[10,     1] loss: 1714.827
[11,     1] loss: 1712.312
[12,     1] loss: 1714.656
[13,     1] loss: 1713.895
[14,     1] loss: 1712.408
[15,     1] loss: 1713.530
[16,     1] loss: 1706.651
[17,     1] loss: 1702.832
[18,     1] loss: 1694.127
[19,     1] loss: 1682.437
[20,     1] loss: 1651.174
[21,     1] loss: 1601.594
[22,     1] loss: 1571.194
[23,     1] loss: 1524.876
[24,     1] loss: 1483.028
[25,     1] loss: 1429.171
[26,     1] loss: 1421.686
[27,     1] loss: 1416.203
[28,     1] loss: 1463.997
[29,     1] loss: 1479.756
[30,     1] loss: 1451.165
[31,     1] loss: 1409.734
[32,     1] loss: 1392.183
[33,     1] loss: 1380.198
[34,     1] loss: 1365.386
[35,     1] loss: 1392.611
[36,     1] loss: 1314.374
[37,     1] loss: 1300.071
[38,     1] loss: 1242.705
[39,     1] loss: 1255.839
[40,     1] loss: 1260.144
[41,     1] loss: 1279.070
[42,     1] loss: 1394.172
[43,     1] loss: 1263.192
[44,     1] loss: 1249.250
[45,     1] loss: 1229.822
[46,     1] loss: 1306.164
[47,     1] loss: 1176.009
[48,     1] loss: 1325.124
[49,     1] loss: 1223.433
[50,     1] loss: 1084.765
[51,     1] loss: 1185.616
[52,     1] loss: 1143.643
[53,     1] loss: 1076.719
[54,     1] loss: 1231.841
[55,     1] loss: 1187.321
[56,     1] loss: 1063.279
[57,     1] loss: 1234.903
[58,     1] loss: 1088.546
[59,     1] loss: 1089.558
[60,     1] loss: 1071.286
[61,     1] loss: 1069.371
[62,     1] loss: 888.483
[63,     1] loss: 1239.794
[64,     1] loss: 1041.491
[65,     1] loss: 905.023
[66,     1] loss: 1070.346
[67,     1] loss: 998.163
[68,     1] loss: 922.678
[69,     1] loss: 1160.805
[70,     1] loss: 810.614
[71,     1] loss: 885.202
[72,     1] loss: 1060.995
[73,     1] loss: 919.485
[74,     1] loss: 827.341
[75,     1] loss: 834.556
[76,     1] loss: 824.271
[77,     1] loss: 828.503
[78,     1] loss: 751.968
[79,     1] loss: 879.672
[80,     1] loss: 2550.231
[81,     1] loss: 2685.479
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033134891292149363,
 'learning_rate_Hydroxylation-K': 0.005717015765563405,
 'learning_rate_Hydroxylation-P': 0.0032244788522915006,
 'log_base': 1.1933094537166666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 47668714,
 'sample_weights': [3.8295360559784872, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.918241473921399,
 'weight_decay_Hydroxylation-K': 5.278664171133152,
 'weight_decay_Hydroxylation-P': 2.7432855350038063}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3071.602
[2,     1] loss: 3057.942
[3,     1] loss: 3057.942
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0036218119529388107,
 'learning_rate_Hydroxylation-K': 0.0069134173691020205,
 'learning_rate_Hydroxylation-P': 0.0034972777586194893,
 'log_base': 2.6235368834158823,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1457187286,
 'sample_weights': [9.446265024338159, 1.180828616549949],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.36678529525082,
 'weight_decay_Hydroxylation-K': 4.733801959601456,
 'weight_decay_Hydroxylation-P': 3.460667487239198}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.572
[2,     1] loss: 1274.903
[3,     1] loss: 1271.100
[4,     1] loss: 1267.856
[5,     1] loss: 1263.340
[6,     1] loss: 1258.731
[7,     1] loss: 1239.002
[8,     1] loss: 1212.594
[9,     1] loss: 1173.240
[10,     1] loss: 1135.204
[11,     1] loss: 1116.665
[12,     1] loss: 1103.270
[13,     1] loss: 1079.139
[14,     1] loss: 1072.770
[15,     1] loss: 1037.105
[16,     1] loss: 1047.673
[17,     1] loss: 1031.967
[18,     1] loss: 1089.110
[19,     1] loss: 1079.449
[20,     1] loss: 1030.424
[21,     1] loss: 1022.445
[22,     1] loss: 994.741
[23,     1] loss: 994.794
[24,     1] loss: 974.466
[25,     1] loss: 987.686
[26,     1] loss: 975.415
[27,     1] loss: 925.388
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004567017666363027,
 'learning_rate_Hydroxylation-K': 0.005208853502772162,
 'learning_rate_Hydroxylation-P': 0.008649314565616014,
 'log_base': 1.2499697524758167,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1166021253,
 'sample_weights': [1.7308478069378874, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.533052119535855,
 'weight_decay_Hydroxylation-K': 3.5897636065102825,
 'weight_decay_Hydroxylation-P': 4.974103403791691}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2478.796
[2,     1] loss: 2472.836
[3,     1] loss: 2491.849
[4,     1] loss: 2498.280
[5,     1] loss: 2475.219
[6,     1] loss: 2492.671
[7,     1] loss: 2497.402
[8,     1] loss: 2481.577
[9,     1] loss: 2478.690
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007995142375606843,
 'learning_rate_Hydroxylation-K': 0.007149251552862626,
 'learning_rate_Hydroxylation-P': 0.0013917887355904273,
 'log_base': 1.1842446440863572,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3901908615,
 'sample_weights': [7.482287506490135, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.445941652750111,
 'weight_decay_Hydroxylation-K': 8.696187598309214,
 'weight_decay_Hydroxylation-P': 8.597471947338203}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3219.925
[2,     1] loss: 3261.577
[3,     1] loss: 3196.324
[4,     1] loss: 3208.978
[5,     1] loss: 3210.799
[6,     1] loss: 3199.646
[7,     1] loss: 3192.863
[8,     1] loss: 3192.610
[9,     1] loss: 3204.562
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003829052482079153,
 'learning_rate_Hydroxylation-K': 0.006638957950456449,
 'learning_rate_Hydroxylation-P': 0.0025468867110688405,
 'log_base': 1.2902484473572877,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1240114794,
 'sample_weights': [9.872220001951089, 1.2340750399385922],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.113592668460818,
 'weight_decay_Hydroxylation-K': 6.760991573736313,
 'weight_decay_Hydroxylation-P': 2.2390995655222983}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2312.306
[2,     1] loss: 2290.320
[3,     1] loss: 2273.502
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005869229857088976,
 'learning_rate_Hydroxylation-K': 0.009555016640082384,
 'learning_rate_Hydroxylation-P': 0.00046962735038841064,
 'log_base': 1.2381689792754975,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2098615989,
 'sample_weights': [6.551080081190686, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.834610614504602,
 'weight_decay_Hydroxylation-K': 9.554760128265038,
 'weight_decay_Hydroxylation-P': 7.949418325733862}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2570.819
[2,     1] loss: 2548.894
[3,     1] loss: 2583.868
[4,     1] loss: 2555.365
[5,     1] loss: 2548.340
[6,     1] loss: 2564.621
[7,     1] loss: 2558.961
[8,     1] loss: 2549.137
[9,     1] loss: 2552.121
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002762919892532147,
 'learning_rate_Hydroxylation-K': 0.008200759765327092,
 'learning_rate_Hydroxylation-P': 0.004110927905439601,
 'log_base': 1.0590050825058073,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1158421103,
 'sample_weights': [7.814513675028053, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.64262181802149,
 'weight_decay_Hydroxylation-K': 9.744868380249756,
 'weight_decay_Hydroxylation-P': 4.131039554703616}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9493.100
[2,     1] loss: 9456.622
[3,     1] loss: 9444.318
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00357484306315095,
 'learning_rate_Hydroxylation-K': 0.007582838240544827,
 'learning_rate_Hydroxylation-P': 0.0008035272002585789,
 'log_base': 1.1193171625672456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1030628414,
 'sample_weights': [29.119955536169858, 3.640134669224006],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.51188636303998,
 'weight_decay_Hydroxylation-K': 8.373700915587001,
 'weight_decay_Hydroxylation-P': 7.614946188148078}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4833.265
[2,     1] loss: 4823.951
[3,     1] loss: 4798.424
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000585360796416007,
 'learning_rate_Hydroxylation-K': 0.0018368966126565312,
 'learning_rate_Hydroxylation-P': 9.024111503736865e-05,
 'log_base': 2.9860967589694543,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 772703472,
 'sample_weights': [14.810686463151896, 1.851407128783585],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.6040563735589135,
 'weight_decay_Hydroxylation-K': 6.295578670873029,
 'weight_decay_Hydroxylation-P': 1.8460745798064626}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.389
[2,     1] loss: 1233.134
[3,     1] loss: 1229.487
[4,     1] loss: 1229.874
[5,     1] loss: 1232.501
[6,     1] loss: 1228.512
[7,     1] loss: 1228.347
[8,     1] loss: 1226.490
[9,     1] loss: 1229.669
[10,     1] loss: 1227.854
[11,     1] loss: 1225.460
[12,     1] loss: 1223.506
[13,     1] loss: 1221.274
[14,     1] loss: 1220.435
[15,     1] loss: 1219.680
[16,     1] loss: 1215.271
[17,     1] loss: 1207.098
[18,     1] loss: 1202.915
[19,     1] loss: 1192.394
[20,     1] loss: 1187.827
[21,     1] loss: 1175.989
[22,     1] loss: 1166.778
[23,     1] loss: 1157.860
[24,     1] loss: 1155.380
[25,     1] loss: 1143.827
[26,     1] loss: 1125.845
[27,     1] loss: 1116.650
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00022667043983813553,
 'learning_rate_Hydroxylation-K': 0.008790642115470036,
 'learning_rate_Hydroxylation-P': 0.005425148431463219,
 'log_base': 2.3885437952259054,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2789210393,
 'sample_weights': [1.526045109719044, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.525441314711903,
 'weight_decay_Hydroxylation-K': 6.365778848704439,
 'weight_decay_Hydroxylation-P': 8.888648356300283}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1311.638
[2,     1] loss: 1315.381
[3,     1] loss: 1315.552
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030206068962039248,
 'learning_rate_Hydroxylation-K': 0.005521265544711291,
 'learning_rate_Hydroxylation-P': 0.005834872208396836,
 'log_base': 2.111038621000861,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3546464660,
 'sample_weights': [1.917392945316345, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.915150541775846,
 'weight_decay_Hydroxylation-K': 0.11535342042438579,
 'weight_decay_Hydroxylation-P': 0.7951881927536741}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1381.013
[2,     1] loss: 1377.466
[3,     1] loss: 1389.103
[4,     1] loss: 1375.895
[5,     1] loss: 1377.275
[6,     1] loss: 1380.379
[7,     1] loss: 1377.269
[8,     1] loss: 1372.183
[9,     1] loss: 1368.991
[10,     1] loss: 1360.879
[11,     1] loss: 1338.900
[12,     1] loss: 1314.098
[13,     1] loss: 1300.261
[14,     1] loss: 1259.478
[15,     1] loss: 1231.289
[16,     1] loss: 1195.292
[17,     1] loss: 1205.997
[18,     1] loss: 1156.355
[19,     1] loss: 1203.795
[20,     1] loss: 1264.453
[21,     1] loss: 1185.220
[22,     1] loss: 1116.656
[23,     1] loss: 1151.641
[24,     1] loss: 1135.541
[25,     1] loss: 1172.663
[26,     1] loss: 1093.337
[27,     1] loss: 1097.378
[28,     1] loss: 1076.047
[29,     1] loss: 1077.710
[30,     1] loss: 1052.778
[31,     1] loss: 1093.411
[32,     1] loss: 1045.752
[33,     1] loss: 1082.582
[34,     1] loss: 1015.719
[35,     1] loss: 995.199
[36,     1] loss: 1054.010
[37,     1] loss: 973.841
[38,     1] loss: 971.937
[39,     1] loss: 999.204
[40,     1] loss: 942.916
[41,     1] loss: 967.711
[42,     1] loss: 998.510
[43,     1] loss: 934.442
[44,     1] loss: 951.657
[45,     1] loss: 951.282
[46,     1] loss: 928.998
[47,     1] loss: 959.612
[48,     1] loss: 953.791
[49,     1] loss: 912.297
[50,     1] loss: 877.574
[51,     1] loss: 924.561
[52,     1] loss: 942.559
[53,     1] loss: 883.484
[54,     1] loss: 834.660
[55,     1] loss: 780.106
[56,     1] loss: 839.160
[57,     1] loss: 825.995
[58,     1] loss: 876.928
[59,     1] loss: 782.380
[60,     1] loss: 837.797
[61,     1] loss: 758.950
[62,     1] loss: 810.101
[63,     1] loss: 763.997
[64,     1] loss: 758.164
[65,     1] loss: 891.925
[66,     1] loss: 804.701
[67,     1] loss: 743.219
[68,     1] loss: 797.326
[69,     1] loss: 683.685
[70,     1] loss: 837.351
[71,     1] loss: 714.478
[72,     1] loss: 731.118
[73,     1] loss: 664.095
[74,     1] loss: 717.712
[75,     1] loss: 655.954
[76,     1] loss: 697.161
[77,     1] loss: 668.443
[78,     1] loss: 585.452
[79,     1] loss: 680.641
[80,     1] loss: 628.530
[81,     1] loss: 635.622
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00534074249573136,
 'learning_rate_Hydroxylation-K': 0.008268535308844098,
 'learning_rate_Hydroxylation-P': 0.0025107160535013484,
 'log_base': 1.0338948387636573,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1948911613,
 'sample_weights': [2.2343250686969287, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.810557832252874,
 'weight_decay_Hydroxylation-K': 9.418843970157178,
 'weight_decay_Hydroxylation-P': 7.323681247956812}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16264.349
Exploding loss, terminate run (best metric=0.5318225622177124)
Finished Training
Total time taken: 0.21400785446166992
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16339.828
Exploding loss, terminate run (best metric=0.5271146893501282)
Finished Training
Total time taken: 0.21100306510925293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16262.381
Exploding loss, terminate run (best metric=0.527155876159668)
Finished Training
Total time taken: 0.2089986801147461
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16244.547
Exploding loss, terminate run (best metric=0.5330113768577576)
Finished Training
Total time taken: 0.23302412033081055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16255.460
Exploding loss, terminate run (best metric=0.5301237106323242)
Finished Training
Total time taken: 0.20622897148132324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16253.061
Exploding loss, terminate run (best metric=0.5336427092552185)
Finished Training
Total time taken: 0.20400357246398926
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16234.664
Exploding loss, terminate run (best metric=0.5284215211868286)
Finished Training
Total time taken: 0.2109997272491455
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16247.699
Exploding loss, terminate run (best metric=0.5434051752090454)
Finished Training
Total time taken: 0.23699712753295898
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16265.195
Exploding loss, terminate run (best metric=0.5309750437736511)
Finished Training
Total time taken: 0.21400237083435059
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16282.605
Exploding loss, terminate run (best metric=0.5317659378051758)
Finished Training
Total time taken: 0.22099876403808594
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16245.313
Exploding loss, terminate run (best metric=0.5331118702888489)
Finished Training
Total time taken: 0.21500229835510254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16307.545
Exploding loss, terminate run (best metric=0.5304484367370605)
Finished Training
Total time taken: 0.2200016975402832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16256.039
Exploding loss, terminate run (best metric=0.5396811366081238)
Finished Training
Total time taken: 0.22299742698669434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16270.404
Exploding loss, terminate run (best metric=0.5339483618736267)
Finished Training
Total time taken: 0.20600247383117676
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16260.748
Exploding loss, terminate run (best metric=0.5339792966842651)
Finished Training
Total time taken: 0.2119755744934082
{'Hydroxylation-K Validation Accuracy': 0.5840130023640662, 'Hydroxylation-K Validation Sensitivity': 0.3718518518518519, 'Hydroxylation-K Validation Specificity': 0.6333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6503118908382066, 'Hydroxylation-K AUC PR': 0.35981874284594795, 'Hydroxylation-K MCC': 0.004495419297876637, 'Hydroxylation-K F1': 0.13623416054775847, 'Validation Loss (Hydroxylation-K)': 0.5573454976081849, 'Hydroxylation-P Validation Accuracy': 0.589547992487691, 'Hydroxylation-P Validation Sensitivity': 0.3754497354497354, 'Hydroxylation-P Validation Specificity': 0.6351039952117312, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.591344501605986, 'Hydroxylation-P AUC PR': 0.2754664679950337, 'Hydroxylation-P MCC': 0.009333255449632706, 'Hydroxylation-P F1': 0.12207962314635677, 'Validation Loss (Hydroxylation-P)': 0.5325738469759623, 'Validation Loss (total)': 1.0899193207422893, 'TimeToTrain': 0.21574958165486655}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004888111066078541,
 'learning_rate_Hydroxylation-K': 0.00745847838900593,
 'learning_rate_Hydroxylation-P': 0.003765174349793663,
 'log_base': 1.2284547352310435,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1578739672,
 'sample_weights': [50.12084570752777, 6.252087930689216],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.149619813139315,
 'weight_decay_Hydroxylation-K': 7.94152717585239,
 'weight_decay_Hydroxylation-P': 6.828550793978743}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2639.805
[2,     1] loss: 2637.992
[3,     1] loss: 2640.755
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006835661007368129,
 'learning_rate_Hydroxylation-K': 0.007489248818277663,
 'learning_rate_Hydroxylation-P': 0.006538405558049836,
 'log_base': 1.0588094609683507,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 101432003,
 'sample_weights': [8.113661289315532, 1.0142467325162312],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.644548570901627,
 'weight_decay_Hydroxylation-K': 8.171000915680922,
 'weight_decay_Hydroxylation-P': 2.008893058219946}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9519.302
[2,     1] loss: 9481.415
[3,     1] loss: 9519.398
[4,     1] loss: 9474.072
[5,     1] loss: 9479.369
[6,     1] loss: 9443.431
[7,     1] loss: 9449.867
[8,     1] loss: 9459.500
[9,     1] loss: 9424.015
[10,     1] loss: 9379.799
[11,     1] loss: 9338.615
[12,     1] loss: 9202.923
[13,     1] loss: 9059.849
[14,     1] loss: 8797.677
[15,     1] loss: 8562.909
[16,     1] loss: 8162.889
[17,     1] loss: 8138.159
[18,     1] loss: 8012.093
[19,     1] loss: 8882.529
[20,     1] loss: 8149.884
[21,     1] loss: 7647.171
[22,     1] loss: 7753.370
[23,     1] loss: 7596.512
[24,     1] loss: 7461.893
[25,     1] loss: 7310.016
[26,     1] loss: 7473.782
[27,     1] loss: 7143.186
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005076568826781288,
 'learning_rate_Hydroxylation-K': 0.0010391960716089159,
 'learning_rate_Hydroxylation-P': 0.0030452258682119137,
 'log_base': 2.5880014021652555,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 617071038,
 'sample_weights': [29.2140946800663, 3.6519025155382927],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.45056005588932,
 'weight_decay_Hydroxylation-K': 1.4379492724594751,
 'weight_decay_Hydroxylation-P': 4.8960843677959325}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1280.627
[2,     1] loss: 1276.194
[3,     1] loss: 1278.148
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002850764965379738,
 'learning_rate_Hydroxylation-K': 0.003957653598576836,
 'learning_rate_Hydroxylation-P': 0.003248536011783313,
 'log_base': 2.0856101079890963,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3621183558,
 'sample_weights': [1.7556713324496984, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0826367158407844,
 'weight_decay_Hydroxylation-K': 1.8259842716268904,
 'weight_decay_Hydroxylation-P': 5.6771813974703935}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1387.708
[2,     1] loss: 1386.651
[3,     1] loss: 1386.971
[4,     1] loss: 1384.008
[5,     1] loss: 1387.252
[6,     1] loss: 1383.464
[7,     1] loss: 1384.202
[8,     1] loss: 1384.616
[9,     1] loss: 1381.591
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005173798411667209,
 'learning_rate_Hydroxylation-K': 0.00853130205968038,
 'learning_rate_Hydroxylation-P': 0.006392791339581614,
 'log_base': 1.2976882830288763,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 528730636,
 'sample_weights': [2.2711614007535377, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.970305896888776,
 'weight_decay_Hydroxylation-K': 5.416184950143045,
 'weight_decay_Hydroxylation-P': 2.237279796019106}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2266.250
[2,     1] loss: 2257.443
[3,     1] loss: 2268.820
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006837078186968137,
 'learning_rate_Hydroxylation-K': 0.005915642939337408,
 'learning_rate_Hydroxylation-P': 0.0010593150767901332,
 'log_base': 1.9847107500192551,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2366079821,
 'sample_weights': [6.406534335724223, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.44995210378627,
 'weight_decay_Hydroxylation-K': 4.683323748172855,
 'weight_decay_Hydroxylation-P': 1.6971467121137769}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1420.613
[2,     1] loss: 1430.049
[3,     1] loss: 1421.028
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00011060356457724403,
 'learning_rate_Hydroxylation-K': 0.003340145046140012,
 'learning_rate_Hydroxylation-P': 0.00045221284350053115,
 'log_base': 1.6147273102042135,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 703527852,
 'sample_weights': [2.4354609084355556, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.5774179460362636,
 'weight_decay_Hydroxylation-K': 4.268481217652524,
 'weight_decay_Hydroxylation-P': 4.351405390721535}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1641.260
[2,     1] loss: 1645.765
[3,     1] loss: 1641.240
[4,     1] loss: 1639.386
[5,     1] loss: 1641.610
[6,     1] loss: 1639.919
[7,     1] loss: 1639.279
[8,     1] loss: 1643.136
[9,     1] loss: 1644.495
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006880415270295813,
 'learning_rate_Hydroxylation-K': 0.007736558023090273,
 'learning_rate_Hydroxylation-P': 0.0020025998846705056,
 'log_base': 1.2713148779663317,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 461958004,
 'sample_weights': [3.4840594262951474, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.64601218238386,
 'weight_decay_Hydroxylation-K': 9.797367288889506,
 'weight_decay_Hydroxylation-P': 5.186041991708278}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2376.650
[2,     1] loss: 2381.634
[3,     1] loss: 2374.025
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009742103634591676,
 'learning_rate_Hydroxylation-K': 0.007904428018206663,
 'learning_rate_Hydroxylation-P': 0.005410274842172036,
 'log_base': 1.4403448165410837,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1666028144,
 'sample_weights': [6.954514941935588, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.5162939964475335,
 'weight_decay_Hydroxylation-K': 2.6438016038972534,
 'weight_decay_Hydroxylation-P': 4.583935189113992}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1871.945
[2,     1] loss: 1893.946
[3,     1] loss: 1879.036
[4,     1] loss: 1874.209
[5,     1] loss: 1873.296
[6,     1] loss: 1868.185
[7,     1] loss: 1877.322
[8,     1] loss: 1874.766
[9,     1] loss: 1874.638
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004776002879153807,
 'learning_rate_Hydroxylation-K': 0.0025636286219881364,
 'learning_rate_Hydroxylation-P': 0.0016195489353776674,
 'log_base': 2.501816847431427,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3930491758,
 'sample_weights': [4.575289197217715, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6115481802272296,
 'weight_decay_Hydroxylation-K': 1.9743646723594181,
 'weight_decay_Hydroxylation-P': 0.4387082730762627}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1293.010
[2,     1] loss: 1287.789
[3,     1] loss: 1292.761
[4,     1] loss: 1284.852
[5,     1] loss: 1270.470
[6,     1] loss: 1243.225
[7,     1] loss: 1209.566
[8,     1] loss: 1166.193
[9,     1] loss: 1103.324
[10,     1] loss: 1146.457
[11,     1] loss: 1162.547
[12,     1] loss: 1031.140
[13,     1] loss: 1082.929
[14,     1] loss: 1054.930
[15,     1] loss: 1059.737
[16,     1] loss: 1035.168
[17,     1] loss: 1030.985
[18,     1] loss: 1029.816
[19,     1] loss: 1036.796
[20,     1] loss: 1028.234
[21,     1] loss: 1005.571
[22,     1] loss: 1005.329
[23,     1] loss: 1044.276
[24,     1] loss: 983.186
[25,     1] loss: 1003.208
[26,     1] loss: 989.990
[27,     1] loss: 1008.018
[28,     1] loss: 956.919
[29,     1] loss: 1010.659
[30,     1] loss: 952.893
[31,     1] loss: 940.887
[32,     1] loss: 994.081
[33,     1] loss: 932.015
[34,     1] loss: 942.346
[35,     1] loss: 947.914
[36,     1] loss: 909.535
[37,     1] loss: 921.099
[38,     1] loss: 911.073
[39,     1] loss: 907.754
[40,     1] loss: 879.825
[41,     1] loss: 869.973
[42,     1] loss: 897.027
[43,     1] loss: 864.785
[44,     1] loss: 901.548
[45,     1] loss: 902.375
[46,     1] loss: 856.805
[47,     1] loss: 841.028
[48,     1] loss: 789.477
[49,     1] loss: 844.062
[50,     1] loss: 852.120
[51,     1] loss: 803.494
[52,     1] loss: 791.117
[53,     1] loss: 798.381
[54,     1] loss: 867.122
[55,     1] loss: 747.124
[56,     1] loss: 826.204
[57,     1] loss: 813.685
[58,     1] loss: 787.501
[59,     1] loss: 807.566
[60,     1] loss: 772.998
[61,     1] loss: 826.370
[62,     1] loss: 738.443
[63,     1] loss: 786.582
[64,     1] loss: 779.516
[65,     1] loss: 771.115
[66,     1] loss: 681.108
[67,     1] loss: 700.944
[68,     1] loss: 711.915
[69,     1] loss: 769.540
[70,     1] loss: 719.268
[71,     1] loss: 689.159
[72,     1] loss: 699.705
[73,     1] loss: 713.786
[74,     1] loss: 685.475
[75,     1] loss: 649.689
[76,     1] loss: 656.007
[77,     1] loss: 669.220
[78,     1] loss: 637.791
[79,     1] loss: 643.066
[80,     1] loss: 611.097
[81,     1] loss: 638.156
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0053260676017161165,
 'learning_rate_Hydroxylation-K': 0.007551737378360191,
 'learning_rate_Hydroxylation-P': 0.001954049613699221,
 'log_base': 1.081911614595979,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2310662490,
 'sample_weights': [1.8205145278217565, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.688564726394183,
 'weight_decay_Hydroxylation-K': 8.888786161437064,
 'weight_decay_Hydroxylation-P': 8.76852456887259}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6879.625
[2,     1] loss: 6876.055
[3,     1] loss: 6896.840
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005471099550156097,
 'learning_rate_Hydroxylation-K': 0.0028880318361967413,
 'learning_rate_Hydroxylation-P': 0.008274291644203652,
 'log_base': 2.327954439339809,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2263326839,
 'sample_weights': [21.204800728873597, 2.6507022028685356],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.863928688149291,
 'weight_decay_Hydroxylation-K': 3.211649511564178,
 'weight_decay_Hydroxylation-P': 6.2439456176095085}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1327.427
[2,     1] loss: 1327.167
[3,     1] loss: 1325.921
[4,     1] loss: 1322.247
[5,     1] loss: 1321.118
[6,     1] loss: 1321.810
[7,     1] loss: 1313.667
[8,     1] loss: 1305.098
[9,     1] loss: 1291.318
[10,     1] loss: 1264.573
[11,     1] loss: 1242.485
[12,     1] loss: 1218.342
[13,     1] loss: 1150.868
[14,     1] loss: 1126.741
[15,     1] loss: 1103.798
[16,     1] loss: 1093.061
[17,     1] loss: 1103.786
[18,     1] loss: 1057.859
[19,     1] loss: 1095.299
[20,     1] loss: 1069.693
[21,     1] loss: 1062.959
[22,     1] loss: 1036.983
[23,     1] loss: 997.771
[24,     1] loss: 985.288
[25,     1] loss: 1054.166
[26,     1] loss: 957.109
[27,     1] loss: 1036.570
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0040426934764290325,
 'learning_rate_Hydroxylation-K': 0.0037572814915255654,
 'learning_rate_Hydroxylation-P': 0.0025795355641795694,
 'log_base': 2.117225480283403,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1249502289,
 'sample_weights': [1.9756958408161083, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.578859117474147,
 'weight_decay_Hydroxylation-K': 9.569264276405594,
 'weight_decay_Hydroxylation-P': 0.6878693539379658}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1378.044
[2,     1] loss: 1377.610
[3,     1] loss: 1376.442
[4,     1] loss: 1380.946
[5,     1] loss: 1377.812
[6,     1] loss: 1376.745
[7,     1] loss: 1370.868
[8,     1] loss: 1377.446
[9,     1] loss: 1369.593
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002570861045758475,
 'learning_rate_Hydroxylation-K': 0.006932173932722741,
 'learning_rate_Hydroxylation-P': 0.003238798992758028,
 'log_base': 1.0237960305279699,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4180328369,
 'sample_weights': [2.225608172590555, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.358617794854112,
 'weight_decay_Hydroxylation-K': 9.237273344909672,
 'weight_decay_Hydroxylation-P': 3.1726152555144034}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23099.078
Exploding loss, terminate run (best metric=0.5342036485671997)
Finished Training
Total time taken: 0.2050032615661621
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23018.752
Exploding loss, terminate run (best metric=0.5275800824165344)
Finished Training
Total time taken: 0.20200061798095703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22985.262
Exploding loss, terminate run (best metric=0.526939868927002)
Finished Training
Total time taken: 0.21600031852722168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23029.779
Exploding loss, terminate run (best metric=0.5285219550132751)
Finished Training
Total time taken: 0.2180004119873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23155.803
Exploding loss, terminate run (best metric=0.531857967376709)
Finished Training
Total time taken: 0.21799969673156738
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23123.430
Exploding loss, terminate run (best metric=0.5336459875106812)
Finished Training
Total time taken: 0.21000194549560547
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23097.008
Exploding loss, terminate run (best metric=0.5268123149871826)
Finished Training
Total time taken: 0.21600008010864258
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23003.941
Exploding loss, terminate run (best metric=0.5344567894935608)
Finished Training
Total time taken: 0.2129993438720703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23061.479
Exploding loss, terminate run (best metric=0.5270540118217468)
Finished Training
Total time taken: 0.21599841117858887
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23219.000
Exploding loss, terminate run (best metric=0.5322834849357605)
Finished Training
Total time taken: 0.20600008964538574
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 22954.486
Exploding loss, terminate run (best metric=0.5405420660972595)
Finished Training
Total time taken: 0.21200227737426758
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23085.320
Exploding loss, terminate run (best metric=0.5356259942054749)
Finished Training
Total time taken: 0.2090003490447998
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23130.113
Exploding loss, terminate run (best metric=0.5287758708000183)
Finished Training
Total time taken: 0.21500039100646973
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23206.904
Exploding loss, terminate run (best metric=0.5281009078025818)
Finished Training
Total time taken: 0.2050011157989502
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23219.736
Exploding loss, terminate run (best metric=0.5346409678459167)
Finished Training
Total time taken: 0.21100091934204102
{'Hydroxylation-K Validation Accuracy': 0.5569444444444445, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.5982456140350877, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6452631578947369, 'Hydroxylation-K AUC PR': 0.3539772378603725, 'Hydroxylation-K MCC': -0.004988477453464162, 'Hydroxylation-K F1': 0.13325123152709362, 'Validation Loss (Hydroxylation-K)': 0.556263534228007, 'Hydroxylation-P Validation Accuracy': 0.5638223440434496, 'Hydroxylation-P Validation Sensitivity': 0.40190476190476193, 'Hydroxylation-P Validation Specificity': 0.5995934959349594, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6304842560330365, 'Hydroxylation-P AUC PR': 0.31648102538160233, 'Hydroxylation-P MCC': 0.005718661548172364, 'Hydroxylation-P F1': 0.12326172326172329, 'Validation Loss (Hydroxylation-P)': 0.5314027945200602, 'Validation Loss (total)': 1.0876663446426391, 'TimeToTrain': 0.2114672819773356}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014454309793224505,
 'learning_rate_Hydroxylation-K': 0.005045602942336279,
 'learning_rate_Hydroxylation-P': 0.002744272213942264,
 'log_base': 1.5617781275734317,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3254371958,
 'sample_weights': [71.04047957806321, 8.86160875161918],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.25852172661155,
 'weight_decay_Hydroxylation-K': 9.660279614397215,
 'weight_decay_Hydroxylation-P': 2.705949978363401}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1700.145
[2,     1] loss: 1694.588
[3,     1] loss: 1699.130
[4,     1] loss: 1696.832
[5,     1] loss: 1696.757
[6,     1] loss: 1689.849
[7,     1] loss: 1688.229
[8,     1] loss: 1695.294
[9,     1] loss: 1684.700
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007471042706832143,
 'learning_rate_Hydroxylation-K': 0.0029000052239486146,
 'learning_rate_Hydroxylation-P': 0.0025115406341368815,
 'log_base': 1.8650890190039258,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2370921772,
 'sample_weights': [3.74461539102892, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.098298272590929,
 'weight_decay_Hydroxylation-K': 8.274905079173653,
 'weight_decay_Hydroxylation-P': 6.697317132675381}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1478.278
[2,     1] loss: 1475.706
[3,     1] loss: 1470.880
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005781663036524034,
 'learning_rate_Hydroxylation-K': 0.0062181455276509884,
 'learning_rate_Hydroxylation-P': 0.000536534297936121,
 'log_base': 1.078633043904292,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4139525710,
 'sample_weights': [2.678356525822193, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.802166001569034,
 'weight_decay_Hydroxylation-K': 6.465950597444673,
 'weight_decay_Hydroxylation-P': 5.81196869634603}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7158.056
[2,     1] loss: 7156.323
[3,     1] loss: 7161.067
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024953775927163686,
 'learning_rate_Hydroxylation-K': 0.001270749664521822,
 'learning_rate_Hydroxylation-P': 0.004747496634811339,
 'log_base': 2.7728917670026334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 260633930,
 'sample_weights': [22.055001077493376, 2.756981340587448],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.555280331539441,
 'weight_decay_Hydroxylation-K': 4.830262961818756,
 'weight_decay_Hydroxylation-P': 6.789063840445721}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.525
[2,     1] loss: 1254.606
[3,     1] loss: 1251.344
[4,     1] loss: 1247.186
[5,     1] loss: 1243.246
[6,     1] loss: 1239.472
[7,     1] loss: 1225.800
[8,     1] loss: 1203.831
[9,     1] loss: 1167.684
[10,     1] loss: 1131.385
[11,     1] loss: 1092.390
[12,     1] loss: 1075.708
[13,     1] loss: 1060.543
[14,     1] loss: 1034.222
[15,     1] loss: 1027.362
[16,     1] loss: 970.845
[17,     1] loss: 1031.103
[18,     1] loss: 978.362
[19,     1] loss: 962.893
[20,     1] loss: 938.622
[21,     1] loss: 983.830
[22,     1] loss: 996.383
[23,     1] loss: 960.672
[24,     1] loss: 984.464
[25,     1] loss: 931.610
[26,     1] loss: 1019.552
[27,     1] loss: 992.393
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002506240046394514,
 'learning_rate_Hydroxylation-K': 0.005925037629134697,
 'learning_rate_Hydroxylation-P': 0.003412700638762521,
 'log_base': 1.0395657226347776,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1575928000,
 'sample_weights': [1.6368843155081956, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.360920992366724,
 'weight_decay_Hydroxylation-K': 8.025680304162297,
 'weight_decay_Hydroxylation-P': 1.9670860285987521}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13956.818
[2,     1] loss: 13927.008
[3,     1] loss: 13975.394
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006434217526750431,
 'learning_rate_Hydroxylation-K': 0.0034749274907719735,
 'learning_rate_Hydroxylation-P': 0.005845356578357627,
 'log_base': 2.676123229987549,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 927965164,
 'sample_weights': [43.023501516358884, 5.378144869300465],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.596319486594886,
 'weight_decay_Hydroxylation-K': 2.9371018832328137,
 'weight_decay_Hydroxylation-P': 9.197312976373494}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1267.103
[2,     1] loss: 1264.029
[3,     1] loss: 1266.607
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0037336195210718745,
 'learning_rate_Hydroxylation-K': 0.008981868738567449,
 'learning_rate_Hydroxylation-P': 0.001555630015513498,
 'log_base': 1.4566866970880206,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2798931672,
 'sample_weights': [1.6959522526908668, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.1721760748546215,
 'weight_decay_Hydroxylation-K': 9.143432613569157,
 'weight_decay_Hydroxylation-P': 3.4636235589403555}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1846.544
[2,     1] loss: 1834.438
[3,     1] loss: 1835.617
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003382144701164191,
 'learning_rate_Hydroxylation-K': 0.0060734087257013,
 'learning_rate_Hydroxylation-P': 0.0059579061459186285,
 'log_base': 1.1661461590207,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2410176258,
 'sample_weights': [4.438067056334502, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.998409044322587,
 'weight_decay_Hydroxylation-K': 8.736295271561701,
 'weight_decay_Hydroxylation-P': 3.7022091715613206}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3525.475
[2,     1] loss: 3528.479
[3,     1] loss: 3523.979
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033524787080002013,
 'learning_rate_Hydroxylation-K': 0.009107001991650293,
 'learning_rate_Hydroxylation-P': 0.0068678090788076036,
 'log_base': 1.0994985951455831,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 447282956,
 'sample_weights': [10.861385960439238, 1.3577255480802164],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.606262655658322,
 'weight_decay_Hydroxylation-K': 9.993807036023453,
 'weight_decay_Hydroxylation-P': 6.065334077512247}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5737.507
[2,     1] loss: 5702.729
[3,     1] loss: 5714.925
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003079008333894568,
 'learning_rate_Hydroxylation-K': 0.0068660769108693145,
 'learning_rate_Hydroxylation-P': 0.0011352744565628612,
 'log_base': 1.049100117815589,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2878432622,
 'sample_weights': [17.600087392046678, 2.200095677261055],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.336219271312205,
 'weight_decay_Hydroxylation-K': 9.474768065484625,
 'weight_decay_Hydroxylation-P': 2.6044032262132255}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11304.629
[2,     1] loss: 11313.444
[3,     1] loss: 11236.612
[4,     1] loss: 11243.742
[5,     1] loss: 11376.997
[6,     1] loss: 11252.455
[7,     1] loss: 11299.961
[8,     1] loss: 11296.077
[9,     1] loss: 11289.212
[10,     1] loss: 11224.648
[11,     1] loss: 11201.646
[12,     1] loss: 11120.815
[13,     1] loss: 11171.319
[14,     1] loss: 10920.855
[15,     1] loss: 10807.673
[16,     1] loss: 10595.913
[17,     1] loss: 10104.818
[18,     1] loss: 10246.354
[19,     1] loss: 10007.385
[20,     1] loss: 9979.407
[21,     1] loss: 9666.594
[22,     1] loss: 10168.793
[23,     1] loss: 9102.989
[24,     1] loss: 9265.820
[25,     1] loss: 8924.790
[26,     1] loss: 8993.834
[27,     1] loss: 8878.484
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007110594532613925,
 'learning_rate_Hydroxylation-K': 0.0039627672890502445,
 'learning_rate_Hydroxylation-P': 0.00983126277806278,
 'log_base': 2.611108037204387,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 262362765,
 'sample_weights': [34.82885059131958, 4.353774042313012],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.146910944601878,
 'weight_decay_Hydroxylation-K': 2.8886030020329643,
 'weight_decay_Hydroxylation-P': 1.7740967158259537}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.328
[2,     1] loss: 1283.810
[3,     1] loss: 1274.360
[4,     1] loss: 1277.056
[5,     1] loss: 1278.521
[6,     1] loss: 1274.254
[7,     1] loss: 1272.807
[8,     1] loss: 1274.292
[9,     1] loss: 1274.429
[10,     1] loss: 1273.295
[11,     1] loss: 1275.732
[12,     1] loss: 1276.065
[13,     1] loss: 1273.606
[14,     1] loss: 1272.580
[15,     1] loss: 1272.089
[16,     1] loss: 1271.661
[17,     1] loss: 1269.932
[18,     1] loss: 1265.576
[19,     1] loss: 1266.689
[20,     1] loss: 1248.911
[21,     1] loss: 1236.285
[22,     1] loss: 1213.728
[23,     1] loss: 1173.661
[24,     1] loss: 1162.303
[25,     1] loss: 1115.911
[26,     1] loss: 1106.681
[27,     1] loss: 1065.482
[28,     1] loss: 1044.696
[29,     1] loss: 1050.943
[30,     1] loss: 1128.203
[31,     1] loss: 1101.986
[32,     1] loss: 1062.019
[33,     1] loss: 1069.108
[34,     1] loss: 1030.304
[35,     1] loss: 1050.707
[36,     1] loss: 1029.778
[37,     1] loss: 1013.698
[38,     1] loss: 1040.528
[39,     1] loss: 988.628
[40,     1] loss: 961.751
[41,     1] loss: 963.340
[42,     1] loss: 978.692
[43,     1] loss: 904.351
[44,     1] loss: 917.418
[45,     1] loss: 913.848
[46,     1] loss: 849.649
[47,     1] loss: 882.346
[48,     1] loss: 965.916
[49,     1] loss: 1154.892
[50,     1] loss: 936.784
[51,     1] loss: 1025.604
[52,     1] loss: 917.168
[53,     1] loss: 989.076
[54,     1] loss: 962.795
[55,     1] loss: 899.519
[56,     1] loss: 964.272
[57,     1] loss: 891.071
[58,     1] loss: 906.584
[59,     1] loss: 836.643
[60,     1] loss: 893.962
[61,     1] loss: 858.275
[62,     1] loss: 802.715
[63,     1] loss: 816.293
[64,     1] loss: 934.145
[65,     1] loss: 785.719
[66,     1] loss: 752.112
[67,     1] loss: 749.787
[68,     1] loss: 761.099
[69,     1] loss: 865.608
[70,     1] loss: 778.933
[71,     1] loss: 720.004
[72,     1] loss: 739.879
[73,     1] loss: 689.909
[74,     1] loss: 676.872
[75,     1] loss: 667.865
[76,     1] loss: 716.135
[77,     1] loss: 879.083
[78,     1] loss: 2062.248
[79,     1] loss: 892.855
[80,     1] loss: 1122.396
[81,     1] loss: 988.656
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0004252088046220663,
 'learning_rate_Hydroxylation-K': 0.006675207345126714,
 'learning_rate_Hydroxylation-P': 0.005716895049406942,
 'log_base': 1.5909204540672364,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3010800180,
 'sample_weights': [1.7394115572833413, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.334473995067198,
 'weight_decay_Hydroxylation-K': 8.931171351533463,
 'weight_decay_Hydroxylation-P': 2.9291786353965716}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1673.760
[2,     1] loss: 1666.964
[3,     1] loss: 1666.677
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007573733047503486,
 'learning_rate_Hydroxylation-K': 0.009530840815171169,
 'learning_rate_Hydroxylation-P': 0.0005991333295774642,
 'log_base': 2.618324356823297,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 606018854,
 'sample_weights': [3.595514327450504, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.319795638642279,
 'weight_decay_Hydroxylation-K': 1.5916315131073326,
 'weight_decay_Hydroxylation-P': 4.125899542094311}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1274.142
[2,     1] loss: 1284.872
[3,     1] loss: 1276.872
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031635531161467215,
 'learning_rate_Hydroxylation-K': 0.0057661849466859524,
 'learning_rate_Hydroxylation-P': 0.004042385682272116,
 'log_base': 1.224704181995593,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3489950118,
 'sample_weights': [1.7344241193889485, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.498066362209483,
 'weight_decay_Hydroxylation-K': 9.563190812137808,
 'weight_decay_Hydroxylation-P': 4.4411513357220365}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2700.919
[2,     1] loss: 2672.819
[3,     1] loss: 2672.153
[4,     1] loss: 2669.206
[5,     1] loss: 2660.171
[6,     1] loss: 2674.560
[7,     1] loss: 2656.235
[8,     1] loss: 2666.748
[9,     1] loss: 2668.981
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002173059655612307,
 'learning_rate_Hydroxylation-K': 0.005990508035344592,
 'learning_rate_Hydroxylation-P': 0.0038272357867178745,
 'log_base': 1.193535770960685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 927891741,
 'sample_weights': [8.236056533837006, 1.0295467274759547],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.887368950182796,
 'weight_decay_Hydroxylation-K': 9.972869099947806,
 'weight_decay_Hydroxylation-P': 2.840346387145139}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3065.251
[2,     1] loss: 3056.627
[3,     1] loss: 3067.193
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006369134588188604,
 'learning_rate_Hydroxylation-K': 0.009729831353162783,
 'learning_rate_Hydroxylation-P': 0.007669485253526097,
 'log_base': 1.2277889434856994,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3419577278,
 'sample_weights': [9.436139762992072, 1.1795629101234724],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.725699067914089,
 'weight_decay_Hydroxylation-K': 9.808164704092636,
 'weight_decay_Hydroxylation-P': 1.5259696455237175}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2640.942
[2,     1] loss: 2646.143
[3,     1] loss: 2645.268
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004653724177027611,
 'learning_rate_Hydroxylation-K': 0.004145730179289491,
 'learning_rate_Hydroxylation-P': 0.0003212771760088346,
 'log_base': 2.0590316613270403,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2778185050,
 'sample_weights': [8.135095369508432, 1.0169260957561987],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.278705167881781,
 'weight_decay_Hydroxylation-K': 6.969559931153229,
 'weight_decay_Hydroxylation-P': 9.970619562977488}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1402.295
[2,     1] loss: 1396.202
[3,     1] loss: 1394.272
[4,     1] loss: 1394.792
[5,     1] loss: 1389.885
[6,     1] loss: 1399.764
[7,     1] loss: 1386.363
[8,     1] loss: 1386.518
[9,     1] loss: 1381.233
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0067155682349807466,
 'learning_rate_Hydroxylation-K': 0.007208591161807506,
 'learning_rate_Hydroxylation-P': 0.00941853254045253,
 'log_base': 1.0122791584130426,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1000147034,
 'sample_weights': [2.3114931936904988, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5618274617471655,
 'weight_decay_Hydroxylation-K': 9.020126624372137,
 'weight_decay_Hydroxylation-P': 5.740569368733476}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44355.777
Exploding loss, terminate run (best metric=0.5313292145729065)
Finished Training
Total time taken: 0.2129993438720703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44721.359
Exploding loss, terminate run (best metric=0.5399254560470581)
Finished Training
Total time taken: 0.21900391578674316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44475.590
Exploding loss, terminate run (best metric=0.5275343060493469)
Finished Training
Total time taken: 0.2109994888305664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44677.828
Exploding loss, terminate run (best metric=0.5271295309066772)
Finished Training
Total time taken: 0.21499991416931152
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44309.312
Exploding loss, terminate run (best metric=0.5287473797798157)
Finished Training
Total time taken: 0.2090003490447998
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44348.051
Exploding loss, terminate run (best metric=0.5339375734329224)
Finished Training
Total time taken: 0.2110004425048828
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44256.449
Exploding loss, terminate run (best metric=0.5289636254310608)
Finished Training
Total time taken: 0.21900033950805664
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44889.875
Exploding loss, terminate run (best metric=0.5302368402481079)
Finished Training
Total time taken: 0.205519437789917
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44519.488
Exploding loss, terminate run (best metric=0.5271769165992737)
Finished Training
Total time taken: 0.21200037002563477
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44408.453
Exploding loss, terminate run (best metric=0.5298547744750977)
Finished Training
Total time taken: 0.22099971771240234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44934.547
Exploding loss, terminate run (best metric=0.5545315146446228)
Finished Training
Total time taken: 0.21900010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44325.383
Exploding loss, terminate run (best metric=0.5295131802558899)
Finished Training
Total time taken: 0.20900464057922363
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44419.074
Exploding loss, terminate run (best metric=0.5263051390647888)
Finished Training
Total time taken: 0.21199750900268555
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 44315.578
Exploding loss, terminate run (best metric=0.5273321270942688)
Finished Training
Total time taken: 0.21000170707702637
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 44486.797
Exploding loss, terminate run (best metric=0.5274308323860168)
Finished Training
Total time taken: 0.2129979133605957
{'Hydroxylation-K Validation Accuracy': 0.3571513002364066, 'Hydroxylation-K Validation Sensitivity': 0.7333333333333333, 'Hydroxylation-K Validation Specificity': 0.26666666666666666, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6482261208576998, 'Hydroxylation-K AUC PR': 0.3533607210389852, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.24351395730706077, 'Validation Loss (Hydroxylation-K)': 0.5587445378303528, 'Hydroxylation-P Validation Accuracy': 0.34864666768184355, 'Hydroxylation-P Validation Sensitivity': 0.7371428571428571, 'Hydroxylation-P Validation Specificity': 0.2658536585365854, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5421557640130885, 'Hydroxylation-P AUC PR': 0.24909554171525025, 'Hydroxylation-P MCC': 0.008128776809488364, 'Hydroxylation-P F1': 0.22686356639946517, 'Validation Loss (Hydroxylation-P)': 0.531329894065857, 'Validation Loss (total)': 1.0900744279225667, 'TimeToTrain': 0.21323501269022624}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002574736186317179,
 'learning_rate_Hydroxylation-K': 0.008197557607810341,
 'learning_rate_Hydroxylation-P': 0.0036098401356566053,
 'log_base': 2.0946154418852134,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1202654073,
 'sample_weights': [136.89195129271533, 17.075939250505062],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.29121523034906,
 'weight_decay_Hydroxylation-K': 6.401571751912824,
 'weight_decay_Hydroxylation-P': 4.053841370386525}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1385.971
[2,     1] loss: 1383.364
[3,     1] loss: 1386.374
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00518613625800405,
 'learning_rate_Hydroxylation-K': 0.008517763537477675,
 'learning_rate_Hydroxylation-P': 0.00581577388996678,
 'log_base': 2.6081618995582088,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3076902789,
 'sample_weights': [2.2579266136533303, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.016256126100041,
 'weight_decay_Hydroxylation-K': 7.957063537788818,
 'weight_decay_Hydroxylation-P': 8.75813487752851}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1279.057
[2,     1] loss: 1276.125
[3,     1] loss: 1273.880
[4,     1] loss: 1274.345
[5,     1] loss: 1271.666
[6,     1] loss: 1270.937
[7,     1] loss: 1265.374
[8,     1] loss: 1265.671
[9,     1] loss: 1248.260
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015983604943164173,
 'learning_rate_Hydroxylation-K': 0.004187061979524871,
 'learning_rate_Hydroxylation-P': 0.00876533759646458,
 'log_base': 2.8015095843572277,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2638658855,
 'sample_weights': [1.7414599703155766, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.1122471028776335,
 'weight_decay_Hydroxylation-K': 1.5042277918670017,
 'weight_decay_Hydroxylation-P': 2.3006952657060076}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1250.815
[2,     1] loss: 1250.295
[3,     1] loss: 1248.804
[4,     1] loss: 1248.586
[5,     1] loss: 1246.086
[6,     1] loss: 1238.831
[7,     1] loss: 1238.308
[8,     1] loss: 1227.012
[9,     1] loss: 1210.820
[10,     1] loss: 1201.571
[11,     1] loss: 1160.372
[12,     1] loss: 1140.457
[13,     1] loss: 1124.622
[14,     1] loss: 1092.660
[15,     1] loss: 1089.042
[16,     1] loss: 1059.260
[17,     1] loss: 1034.880
[18,     1] loss: 1041.471
[19,     1] loss: 1041.627
[20,     1] loss: 1008.628
[21,     1] loss: 1032.175
[22,     1] loss: 1040.994
[23,     1] loss: 1009.479
[24,     1] loss: 1028.092
[25,     1] loss: 1010.837
[26,     1] loss: 1003.998
[27,     1] loss: 999.650
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007490375733884816,
 'learning_rate_Hydroxylation-K': 0.006182759796119959,
 'learning_rate_Hydroxylation-P': 0.009575394258878083,
 'log_base': 1.1432191297497472,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2516088575,
 'sample_weights': [1.6205693537655594, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.4586351209660131,
 'weight_decay_Hydroxylation-K': 9.689357742848264,
 'weight_decay_Hydroxylation-P': 6.987583926581401}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4046.845
[2,     1] loss: 4058.137
[3,     1] loss: 4043.332
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002950516646692647,
 'learning_rate_Hydroxylation-K': 0.006470770628015448,
 'learning_rate_Hydroxylation-P': 0.00350844707701433,
 'log_base': 1.6919112747607365,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3486535863,
 'sample_weights': [12.47267151605737, 1.5591440016812237],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.370851698799929,
 'weight_decay_Hydroxylation-K': 1.743394896037965,
 'weight_decay_Hydroxylation-P': 1.0135525461071295}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1580.350
[2,     1] loss: 1585.094
[3,     1] loss: 1581.119
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005970004470614678,
 'learning_rate_Hydroxylation-K': 0.0029532191798483124,
 'learning_rate_Hydroxylation-P': 0.009736549036264358,
 'log_base': 1.0149258859933337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 929793390,
 'sample_weights': [3.174698376170829, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4150548920809056,
 'weight_decay_Hydroxylation-K': 2.56015235186919,
 'weight_decay_Hydroxylation-P': 5.313443328625005}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36417.176
Exploding loss, terminate run (best metric=0.5322079062461853)
Finished Training
Total time taken: 0.19900226593017578
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36640.965
Exploding loss, terminate run (best metric=0.5284698009490967)
Finished Training
Total time taken: 0.23400115966796875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36438.984
Exploding loss, terminate run (best metric=0.5339336395263672)
Finished Training
Total time taken: 0.20100164413452148
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36540.730
Exploding loss, terminate run (best metric=0.5359932780265808)
Finished Training
Total time taken: 0.21699905395507812
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36599.887
Exploding loss, terminate run (best metric=0.5300073623657227)
Finished Training
Total time taken: 0.2069993019104004
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36562.887
Exploding loss, terminate run (best metric=0.5318575501441956)
Finished Training
Total time taken: 0.20800065994262695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36611.430
Exploding loss, terminate run (best metric=0.5449755191802979)
Finished Training
Total time taken: 0.22500085830688477
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36579.453
Exploding loss, terminate run (best metric=0.5301553010940552)
Finished Training
Total time taken: 0.21500015258789062
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36546.059
Exploding loss, terminate run (best metric=0.5280141830444336)
Finished Training
Total time taken: 0.21200060844421387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36418.191
Exploding loss, terminate run (best metric=0.540793776512146)
Finished Training
Total time taken: 0.22199797630310059
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36509.027
Exploding loss, terminate run (best metric=0.5315055251121521)
Finished Training
Total time taken: 0.21600008010864258
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36591.234
Exploding loss, terminate run (best metric=0.5356594324111938)
Finished Training
Total time taken: 0.2199995517730713
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36636.562
Exploding loss, terminate run (best metric=0.527646005153656)
Finished Training
Total time taken: 0.21600055694580078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 36603.012
Exploding loss, terminate run (best metric=0.5361900329589844)
Finished Training
Total time taken: 0.2140026092529297
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 36510.887
Exploding loss, terminate run (best metric=0.528354823589325)
Finished Training
Total time taken: 0.2090005874633789
{'Hydroxylation-K Validation Accuracy': 0.5397458628841607, 'Hydroxylation-K Validation Sensitivity': 0.44666666666666666, 'Hydroxylation-K Validation Specificity': 0.5614035087719298, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5645614035087719, 'Hydroxylation-K AUC PR': 0.28409259515070484, 'Hydroxylation-K MCC': 0.00670191591802112, 'Hydroxylation-K F1': 0.15874278977727255, 'Validation Loss (Hydroxylation-K)': 0.5576981544494629, 'Hydroxylation-P Validation Accuracy': 0.5515220547180346, 'Hydroxylation-P Validation Sensitivity': 0.45714285714285713, 'Hydroxylation-P Validation Specificity': 0.5707317073170731, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5802101379398252, 'Hydroxylation-P AUC PR': 0.26087363627280175, 'Hydroxylation-P MCC': 0.0328572038599683, 'Hydroxylation-P F1': 0.15579312953673422, 'Validation Loss (Hydroxylation-P)': 0.5330509424209595, 'Validation Loss (total)': 1.0907490968704223, 'TimeToTrain': 0.21433380444844563}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005194435162541819,
 'learning_rate_Hydroxylation-K': 0.003025748617138063,
 'learning_rate_Hydroxylation-P': 0.009071145628285777,
 'log_base': 1.3620230211539088,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2659717506,
 'sample_weights': [112.76509497162161, 14.066348628455213],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2846028802390728,
 'weight_decay_Hydroxylation-K': 1.4380159052259986,
 'weight_decay_Hydroxylation-P': 3.6448955316630833}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2054.349
[2,     1] loss: 2056.125
[3,     1] loss: 2045.759
[4,     1] loss: 2043.743
[5,     1] loss: 2046.250
[6,     1] loss: 2048.438
[7,     1] loss: 2043.573
[8,     1] loss: 2041.036
[9,     1] loss: 2032.754
[10,     1] loss: 2028.493
[11,     1] loss: 2031.636
[12,     1] loss: 2018.130
[13,     1] loss: 2009.290
[14,     1] loss: 1982.228
[15,     1] loss: 1956.312
[16,     1] loss: 1912.516
[17,     1] loss: 1923.721
[18,     1] loss: 1856.541
[19,     1] loss: 1746.374
[20,     1] loss: 1876.872
[21,     1] loss: 1720.091
[22,     1] loss: 1712.861
[23,     1] loss: 1760.844
[24,     1] loss: 1821.777
[25,     1] loss: 1640.573
[26,     1] loss: 1663.698
[27,     1] loss: 1643.522
[28,     1] loss: 1633.140
[29,     1] loss: 1639.856
[30,     1] loss: 1595.165
[31,     1] loss: 1723.036
[32,     1] loss: 1601.381
[33,     1] loss: 1612.070
[34,     1] loss: 1523.477
[35,     1] loss: 1671.303
[36,     1] loss: 1533.074
[37,     1] loss: 1528.354
[38,     1] loss: 1527.764
[39,     1] loss: 1422.259
[40,     1] loss: 1447.304
[41,     1] loss: 1451.631
[42,     1] loss: 1392.589
[43,     1] loss: 1460.806
[44,     1] loss: 1453.300
[45,     1] loss: 1385.661
[46,     1] loss: 1373.716
[47,     1] loss: 1345.993
[48,     1] loss: 1250.094
[49,     1] loss: 1332.606
[50,     1] loss: 1237.994
[51,     1] loss: 1325.785
[52,     1] loss: 1234.619
[53,     1] loss: 1189.028
[54,     1] loss: 1195.934
[55,     1] loss: 1387.137
[56,     1] loss: 1300.660
[57,     1] loss: 1266.021
[58,     1] loss: 1305.642
[59,     1] loss: 1122.760
[60,     1] loss: 1291.746
[61,     1] loss: 1233.386
[62,     1] loss: 1341.544
[63,     1] loss: 1059.188
[64,     1] loss: 1188.315
[65,     1] loss: 1055.800
[66,     1] loss: 1160.450
[67,     1] loss: 1045.101
[68,     1] loss: 1102.026
[69,     1] loss: 1079.776
[70,     1] loss: 875.245
[71,     1] loss: 1094.315
[72,     1] loss: 1049.030
[73,     1] loss: 959.693
[74,     1] loss: 986.554
[75,     1] loss: 902.006
[76,     1] loss: 989.394
[77,     1] loss: 980.347
[78,     1] loss: 1021.820
[79,     1] loss: 1087.960
[80,     1] loss: 959.375
[81,     1] loss: 1240.257
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005413062770024302,
 'learning_rate_Hydroxylation-K': 0.0040660180192962515,
 'learning_rate_Hydroxylation-P': 0.009242924435729216,
 'log_base': 1.281428089116017,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3145436130,
 'sample_weights': [5.4032338077495865, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.7859118048178715,
 'weight_decay_Hydroxylation-K': 5.218250604513308,
 'weight_decay_Hydroxylation-P': 2.422684029562192}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2325.586
[2,     1] loss: 2328.497
[3,     1] loss: 2325.938
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004451150937273083,
 'learning_rate_Hydroxylation-K': 0.00989776201911539,
 'learning_rate_Hydroxylation-P': 0.0037148760779970006,
 'log_base': 2.082666093210334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3270239827,
 'sample_weights': [6.732300164718101, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.694579543949492,
 'weight_decay_Hydroxylation-K': 4.7778509001875085,
 'weight_decay_Hydroxylation-P': 4.933706693155786}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1390.733
[2,     1] loss: 1389.207
[3,     1] loss: 1389.349
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0059412334534963635,
 'learning_rate_Hydroxylation-K': 0.0020650912150901226,
 'learning_rate_Hydroxylation-P': 0.007937482516825054,
 'log_base': 1.0451463468503221,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 602538122,
 'sample_weights': [2.275534338787167, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8984162340101105,
 'weight_decay_Hydroxylation-K': 2.383793820535257,
 'weight_decay_Hydroxylation-P': 4.530192084883163}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12293.916
[2,     1] loss: 12388.928
[3,     1] loss: 12229.416
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007409748368920749,
 'learning_rate_Hydroxylation-K': 0.0038347057822021454,
 'learning_rate_Hydroxylation-P': 0.008065541118621967,
 'log_base': 1.4051431562028804,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3983141623,
 'sample_weights': [37.80705562407089, 4.72606401295908],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.580513295703533,
 'weight_decay_Hydroxylation-K': 3.6462040160573306,
 'weight_decay_Hydroxylation-P': 5.084669762978253}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1946.964
[2,     1] loss: 1946.283
[3,     1] loss: 1932.213
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0030599260937447578,
 'learning_rate_Hydroxylation-K': 0.0010529991265916503,
 'learning_rate_Hydroxylation-P': 9.433570276088515e-05,
 'log_base': 2.3458724574232916,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3125114043,
 'sample_weights': [4.908117634420652, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.7704208457246775,
 'weight_decay_Hydroxylation-K': 4.2930371260792635,
 'weight_decay_Hydroxylation-P': 0.9951465569210594}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.512
[2,     1] loss: 1322.846
[3,     1] loss: 1317.831
[4,     1] loss: 1321.669
[5,     1] loss: 1318.934
[6,     1] loss: 1316.934
[7,     1] loss: 1318.352
[8,     1] loss: 1316.713
[9,     1] loss: 1304.560
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0047248639932604706,
 'learning_rate_Hydroxylation-K': 0.007380560869794187,
 'learning_rate_Hydroxylation-P': 2.708147637013776e-05,
 'log_base': 1.9810910772876793,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 881657381,
 'sample_weights': [1.9579296223099882, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.444506907261708,
 'weight_decay_Hydroxylation-K': 6.015039871541024,
 'weight_decay_Hydroxylation-P': 3.4051797955610046}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1424.862
[2,     1] loss: 1419.561
[3,     1] loss: 1421.505
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003961564219200915,
 'learning_rate_Hydroxylation-K': 0.00902181419303662,
 'learning_rate_Hydroxylation-P': 0.0017037811093682,
 'log_base': 1.0553561207332223,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1826742120,
 'sample_weights': [2.441963959932437, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.675821960974844,
 'weight_decay_Hydroxylation-K': 8.877648174605511,
 'weight_decay_Hydroxylation-P': 5.565556818128894}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10064.335
[2,     1] loss: 10037.396
[3,     1] loss: 10032.570
[4,     1] loss: 10039.099
[5,     1] loss: 9996.157
[6,     1] loss: 10070.889
[7,     1] loss: 10053.700
[8,     1] loss: 9941.045
[9,     1] loss: 9911.275
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00638842962394222,
 'learning_rate_Hydroxylation-K': 0.0011559838242269846,
 'learning_rate_Hydroxylation-P': 0.008945328935715684,
 'log_base': 1.171710530304332,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3378452896,
 'sample_weights': [30.985465888069527, 3.873332446580209],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.146087930391961,
 'weight_decay_Hydroxylation-K': 2.8178405325866724,
 'weight_decay_Hydroxylation-P': 8.343486179665803}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3431.981
[2,     1] loss: 3439.287
[3,     1] loss: 3429.449
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0006919087507533577,
 'learning_rate_Hydroxylation-K': 0.0022133322765939185,
 'learning_rate_Hydroxylation-P': 0.00438024013860194,
 'log_base': 2.6273780551152797,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2021577668,
 'sample_weights': [10.535112447229153, 1.3169397877582154],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.245414210103998,
 'weight_decay_Hydroxylation-K': 1.9092051055233061,
 'weight_decay_Hydroxylation-P': 3.5350585709463482}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.095
[2,     1] loss: 1274.386
[3,     1] loss: 1271.820
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034297115677467027,
 'learning_rate_Hydroxylation-K': 0.003955810860706704,
 'learning_rate_Hydroxylation-P': 0.007606591484743805,
 'log_base': 1.4765650194896278,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 599249288,
 'sample_weights': [1.7282263261735875, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0521560503194274,
 'weight_decay_Hydroxylation-K': 4.510875667076993,
 'weight_decay_Hydroxylation-P': 5.964813949880249}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1811.325
[2,     1] loss: 1811.563
[3,     1] loss: 1819.359
[4,     1] loss: 1810.985
[5,     1] loss: 1809.946
[6,     1] loss: 1811.749
[7,     1] loss: 1803.681
[8,     1] loss: 1799.677
[9,     1] loss: 1808.277
[10,     1] loss: 1802.369
[11,     1] loss: 1800.240
[12,     1] loss: 1803.979
[13,     1] loss: 1784.220
[14,     1] loss: 1780.649
[15,     1] loss: 1748.980
[16,     1] loss: 1733.505
[17,     1] loss: 1674.005
[18,     1] loss: 1648.575
[19,     1] loss: 1687.712
[20,     1] loss: 1671.894
[21,     1] loss: 1605.782
[22,     1] loss: 1571.717
[23,     1] loss: 1555.843
[24,     1] loss: 1529.396
[25,     1] loss: 1502.031
[26,     1] loss: 1489.204
[27,     1] loss: 1457.215
[28,     1] loss: 1544.233
[29,     1] loss: 1444.369
[30,     1] loss: 1501.940
[31,     1] loss: 1407.651
[32,     1] loss: 1433.783
[33,     1] loss: 1480.266
[34,     1] loss: 1379.754
[35,     1] loss: 1383.066
[36,     1] loss: 1408.160
[37,     1] loss: 1398.978
[38,     1] loss: 1288.851
[39,     1] loss: 1286.417
[40,     1] loss: 1419.906
[41,     1] loss: 1300.015
[42,     1] loss: 1322.615
[43,     1] loss: 1291.708
[44,     1] loss: 1285.178
[45,     1] loss: 1278.572
[46,     1] loss: 1210.588
[47,     1] loss: 1288.179
[48,     1] loss: 1237.609
[49,     1] loss: 1117.595
[50,     1] loss: 1161.347
[51,     1] loss: 1136.403
[52,     1] loss: 1173.169
[53,     1] loss: 1141.519
[54,     1] loss: 1126.406
[55,     1] loss: 1047.917
[56,     1] loss: 1066.931
[57,     1] loss: 984.217
[58,     1] loss: 1118.740
[59,     1] loss: 1054.757
[60,     1] loss: 1073.201
[61,     1] loss: 980.096
[62,     1] loss: 1004.095
[63,     1] loss: 916.417
[64,     1] loss: 983.877
[65,     1] loss: 948.920
[66,     1] loss: 947.536
[67,     1] loss: 950.147
[68,     1] loss: 960.867
[69,     1] loss: 930.526
[70,     1] loss: 911.061
[71,     1] loss: 913.141
[72,     1] loss: 872.828
[73,     1] loss: 842.618
[74,     1] loss: 779.318
[75,     1] loss: 788.489
[76,     1] loss: 815.934
[77,     1] loss: 881.275
Early stopping applied (best metric=0.35870659351348877)
Finished Training
Total time taken: 11.356569290161133
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1817.972
[2,     1] loss: 1814.028
[3,     1] loss: 1814.286
[4,     1] loss: 1807.990
[5,     1] loss: 1816.719
[6,     1] loss: 1806.744
[7,     1] loss: 1808.761
[8,     1] loss: 1806.802
[9,     1] loss: 1803.327
[10,     1] loss: 1799.488
[11,     1] loss: 1787.277
[12,     1] loss: 1775.991
[13,     1] loss: 1760.296
[14,     1] loss: 1737.612
[15,     1] loss: 1721.983
[16,     1] loss: 1658.897
[17,     1] loss: 1599.049
[18,     1] loss: 1632.922
[19,     1] loss: 1582.305
[20,     1] loss: 1539.395
[21,     1] loss: 1598.031
[22,     1] loss: 1578.171
[23,     1] loss: 1540.360
[24,     1] loss: 1517.143
[25,     1] loss: 1447.686
[26,     1] loss: 1600.418
[27,     1] loss: 1521.683
[28,     1] loss: 1474.934
[29,     1] loss: 1443.425
[30,     1] loss: 1487.688
[31,     1] loss: 1465.552
[32,     1] loss: 1374.553
[33,     1] loss: 1406.046
[34,     1] loss: 1458.559
[35,     1] loss: 1341.477
[36,     1] loss: 1379.182
[37,     1] loss: 1393.288
[38,     1] loss: 1306.039
[39,     1] loss: 1382.798
[40,     1] loss: 1345.421
[41,     1] loss: 1344.581
[42,     1] loss: 1323.267
[43,     1] loss: 1327.834
[44,     1] loss: 1372.700
[45,     1] loss: 1352.666
[46,     1] loss: 1252.219
[47,     1] loss: 1346.976
[48,     1] loss: 1239.502
[49,     1] loss: 1293.251
[50,     1] loss: 1185.383
[51,     1] loss: 1190.846
[52,     1] loss: 1142.791
[53,     1] loss: 1088.276
[54,     1] loss: 1239.269
[55,     1] loss: 1141.705
[56,     1] loss: 1183.073
[57,     1] loss: 1093.201
[58,     1] loss: 1096.162
[59,     1] loss: 1117.252
[60,     1] loss: 1075.752
[61,     1] loss: 1128.793
[62,     1] loss: 1061.209
[63,     1] loss: 1121.885
[64,     1] loss: 990.649
[65,     1] loss: 1065.684
[66,     1] loss: 1006.584
[67,     1] loss: 948.370
[68,     1] loss: 1033.949
[69,     1] loss: 950.830
[70,     1] loss: 1012.927
[71,     1] loss: 956.770
[72,     1] loss: 910.410
[73,     1] loss: 898.744
[74,     1] loss: 927.187
[75,     1] loss: 916.058
[76,     1] loss: 889.037
[77,     1] loss: 869.637
[78,     1] loss: 827.240
[79,     1] loss: 795.164
[80,     1] loss: 824.176
[81,     1] loss: 771.269
[82,     1] loss: 881.069
[83,     1] loss: 782.853
[84,     1] loss: 1017.221
[85,     1] loss: 887.113
[86,     1] loss: 804.519
[87,     1] loss: 840.948
[88,     1] loss: 829.970
[89,     1] loss: 781.334
[90,     1] loss: 777.014
[91,     1] loss: 713.346
[92,     1] loss: 821.955
Early stopping applied (best metric=0.39450666308403015)
Finished Training
Total time taken: 13.755849123001099
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1815.535
[2,     1] loss: 1806.213
[3,     1] loss: 1815.278
[4,     1] loss: 1815.609
[5,     1] loss: 1806.895
[6,     1] loss: 1810.950
[7,     1] loss: 1801.376
[8,     1] loss: 1794.635
[9,     1] loss: 1788.199
[10,     1] loss: 1771.661
[11,     1] loss: 1733.495
[12,     1] loss: 1691.196
[13,     1] loss: 1637.385
[14,     1] loss: 1601.048
[15,     1] loss: 1571.419
[16,     1] loss: 1499.066
[17,     1] loss: 1525.286
[18,     1] loss: 1473.833
[19,     1] loss: 1520.231
[20,     1] loss: 1473.253
[21,     1] loss: 1460.203
[22,     1] loss: 1424.769
[23,     1] loss: 1503.527
[24,     1] loss: 1507.597
[25,     1] loss: 1438.622
[26,     1] loss: 1452.304
[27,     1] loss: 1499.037
[28,     1] loss: 1429.348
[29,     1] loss: 1391.382
[30,     1] loss: 1416.389
[31,     1] loss: 1315.407
[32,     1] loss: 1364.953
[33,     1] loss: 1390.108
[34,     1] loss: 1290.672
[35,     1] loss: 1313.726
[36,     1] loss: 1296.352
[37,     1] loss: 1368.896
[38,     1] loss: 1314.229
[39,     1] loss: 1245.040
[40,     1] loss: 1289.351
[41,     1] loss: 1255.674
[42,     1] loss: 1234.021
[43,     1] loss: 1248.614
[44,     1] loss: 1186.576
[45,     1] loss: 1118.264
[46,     1] loss: 1172.859
[47,     1] loss: 1152.070
[48,     1] loss: 1243.191
[49,     1] loss: 1246.214
[50,     1] loss: 1082.285
[51,     1] loss: 1078.558
[52,     1] loss: 1094.115
[53,     1] loss: 1120.903
[54,     1] loss: 1086.824
[55,     1] loss: 1232.877
[56,     1] loss: 1181.083
[57,     1] loss: 1003.503
[58,     1] loss: 1209.234
[59,     1] loss: 1035.997
[60,     1] loss: 1043.465
[61,     1] loss: 1086.971
[62,     1] loss: 1078.892
[63,     1] loss: 992.892
[64,     1] loss: 1140.420
[65,     1] loss: 929.088
[66,     1] loss: 980.092
[67,     1] loss: 959.997
[68,     1] loss: 940.617
[69,     1] loss: 930.623
[70,     1] loss: 843.122
[71,     1] loss: 857.622
[72,     1] loss: 975.590
[73,     1] loss: 806.010
[74,     1] loss: 890.839
[75,     1] loss: 833.416
[76,     1] loss: 839.805
[77,     1] loss: 796.800
Early stopping applied (best metric=0.45340704917907715)
Finished Training
Total time taken: 11.768512487411499
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1806.611
[2,     1] loss: 1817.228
[3,     1] loss: 1812.904
[4,     1] loss: 1805.698
[5,     1] loss: 1809.337
[6,     1] loss: 1805.209
[7,     1] loss: 1810.620
[8,     1] loss: 1804.942
[9,     1] loss: 1797.880
[10,     1] loss: 1806.696
[11,     1] loss: 1797.307
[12,     1] loss: 1783.724
[13,     1] loss: 1781.630
[14,     1] loss: 1759.024
[15,     1] loss: 1740.061
[16,     1] loss: 1692.710
[17,     1] loss: 1669.582
[18,     1] loss: 1628.812
[19,     1] loss: 1623.339
[20,     1] loss: 1599.992
[21,     1] loss: 1594.164
[22,     1] loss: 1576.911
[23,     1] loss: 1496.846
[24,     1] loss: 1563.229
[25,     1] loss: 1595.235
[26,     1] loss: 1456.126
[27,     1] loss: 1503.520
[28,     1] loss: 1497.723
[29,     1] loss: 1501.456
[30,     1] loss: 1452.153
[31,     1] loss: 1438.652
[32,     1] loss: 1510.974
[33,     1] loss: 1414.460
[34,     1] loss: 1378.979
[35,     1] loss: 1365.311
[36,     1] loss: 1347.580
[37,     1] loss: 1344.970
[38,     1] loss: 1355.449
[39,     1] loss: 1309.727
[40,     1] loss: 1329.966
[41,     1] loss: 1293.594
[42,     1] loss: 1318.972
[43,     1] loss: 1343.039
[44,     1] loss: 1323.058
[45,     1] loss: 1314.800
[46,     1] loss: 1297.891
[47,     1] loss: 1299.902
[48,     1] loss: 1255.938
[49,     1] loss: 1201.507
[50,     1] loss: 1116.723
[51,     1] loss: 1179.038
[52,     1] loss: 1131.869
[53,     1] loss: 1118.240
[54,     1] loss: 1125.312
[55,     1] loss: 1117.042
[56,     1] loss: 1015.782
[57,     1] loss: 1110.263
[58,     1] loss: 1062.233
[59,     1] loss: 1177.964
[60,     1] loss: 991.122
[61,     1] loss: 1059.859
[62,     1] loss: 1081.692
[63,     1] loss: 961.034
[64,     1] loss: 1080.059
[65,     1] loss: 1006.180
[66,     1] loss: 1224.330
[67,     1] loss: 864.273
[68,     1] loss: 1094.510
[69,     1] loss: 845.163
[70,     1] loss: 1077.295
[71,     1] loss: 1019.192
[72,     1] loss: 925.486
[73,     1] loss: 1032.001
[74,     1] loss: 971.434
[75,     1] loss: 804.070
[76,     1] loss: 891.457
[77,     1] loss: 773.013
Early stopping applied (best metric=0.4112031161785126)
Finished Training
Total time taken: 11.387770175933838
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1812.405
[2,     1] loss: 1813.122
[3,     1] loss: 1815.860
[4,     1] loss: 1811.881
[5,     1] loss: 1811.467
[6,     1] loss: 1807.336
[7,     1] loss: 1801.271
[8,     1] loss: 1801.125
[9,     1] loss: 1784.107
[10,     1] loss: 1765.381
[11,     1] loss: 1740.971
[12,     1] loss: 1725.560
[13,     1] loss: 1659.282
[14,     1] loss: 1604.535
[15,     1] loss: 1591.257
[16,     1] loss: 1538.333
[17,     1] loss: 1459.586
[18,     1] loss: 1442.531
[19,     1] loss: 1486.012
[20,     1] loss: 1420.238
[21,     1] loss: 1447.177
[22,     1] loss: 1421.766
[23,     1] loss: 1389.656
[24,     1] loss: 1386.797
[25,     1] loss: 1430.980
[26,     1] loss: 1384.874
[27,     1] loss: 1299.510
[28,     1] loss: 1290.975
[29,     1] loss: 1277.088
[30,     1] loss: 1237.798
[31,     1] loss: 1344.953
[32,     1] loss: 1163.515
[33,     1] loss: 1217.636
[34,     1] loss: 1235.501
[35,     1] loss: 1269.981
[36,     1] loss: 1150.479
[37,     1] loss: 1180.119
[38,     1] loss: 1136.800
[39,     1] loss: 1094.767
[40,     1] loss: 1142.266
[41,     1] loss: 1132.174
[42,     1] loss: 1097.170
[43,     1] loss: 1132.096
[44,     1] loss: 1076.495
[45,     1] loss: 959.109
[46,     1] loss: 1091.407
[47,     1] loss: 992.141
[48,     1] loss: 1088.562
[49,     1] loss: 1083.568
[50,     1] loss: 1003.727
[51,     1] loss: 1011.203
[52,     1] loss: 934.899
[53,     1] loss: 996.485
[54,     1] loss: 889.963
[55,     1] loss: 1027.718
[56,     1] loss: 890.740
[57,     1] loss: 974.995
[58,     1] loss: 928.073
[59,     1] loss: 900.403
[60,     1] loss: 842.878
[61,     1] loss: 919.109
[62,     1] loss: 913.088
[63,     1] loss: 880.693
Early stopping applied (best metric=0.4601280093193054)
Finished Training
Total time taken: 9.36001467704773
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1812.797
[2,     1] loss: 1808.929
[3,     1] loss: 1811.356
[4,     1] loss: 1817.480
[5,     1] loss: 1817.392
[6,     1] loss: 1808.355
[7,     1] loss: 1808.354
[8,     1] loss: 1805.843
[9,     1] loss: 1803.449
[10,     1] loss: 1799.870
[11,     1] loss: 1792.568
[12,     1] loss: 1785.366
[13,     1] loss: 1764.087
[14,     1] loss: 1748.923
[15,     1] loss: 1717.975
[16,     1] loss: 1681.191
[17,     1] loss: 1653.663
[18,     1] loss: 1605.122
[19,     1] loss: 1610.404
[20,     1] loss: 1562.137
[21,     1] loss: 1588.832
[22,     1] loss: 1565.098
[23,     1] loss: 1529.815
[24,     1] loss: 1518.582
[25,     1] loss: 1427.480
[26,     1] loss: 1464.971
[27,     1] loss: 1532.859
[28,     1] loss: 1453.821
[29,     1] loss: 1459.032
[30,     1] loss: 1471.834
[31,     1] loss: 1430.120
[32,     1] loss: 1467.544
[33,     1] loss: 1432.055
[34,     1] loss: 1373.255
[35,     1] loss: 1427.733
[36,     1] loss: 1368.656
[37,     1] loss: 1309.541
[38,     1] loss: 1377.399
[39,     1] loss: 1289.129
[40,     1] loss: 1359.826
[41,     1] loss: 1234.339
[42,     1] loss: 1368.802
[43,     1] loss: 1319.802
[44,     1] loss: 1224.071
[45,     1] loss: 1305.136
[46,     1] loss: 1279.595
[47,     1] loss: 1332.395
[48,     1] loss: 1188.127
[49,     1] loss: 1266.777
[50,     1] loss: 1174.118
[51,     1] loss: 1173.517
[52,     1] loss: 1166.309
[53,     1] loss: 1207.493
[54,     1] loss: 1127.058
[55,     1] loss: 1101.807
[56,     1] loss: 1023.060
[57,     1] loss: 1194.227
[58,     1] loss: 957.572
[59,     1] loss: 1108.026
[60,     1] loss: 981.329
[61,     1] loss: 1096.225
[62,     1] loss: 994.195
[63,     1] loss: 980.587
[64,     1] loss: 917.762
[65,     1] loss: 942.733
[66,     1] loss: 920.624
[67,     1] loss: 891.427
[68,     1] loss: 1040.993
[69,     1] loss: 971.464
[70,     1] loss: 1027.799
[71,     1] loss: 927.651
[72,     1] loss: 1021.993
[73,     1] loss: 829.085
[74,     1] loss: 935.220
[75,     1] loss: 886.493
[76,     1] loss: 893.967
[77,     1] loss: 906.134
[78,     1] loss: 836.101
[79,     1] loss: 915.634
[80,     1] loss: 804.245
[81,     1] loss: 753.247
[82,     1] loss: 834.610
[83,     1] loss: 825.447
[84,     1] loss: 767.624
[85,     1] loss: 857.133
[86,     1] loss: 702.862
[87,     1] loss: 848.299
[88,     1] loss: 812.626
[89,     1] loss: 782.806
[90,     1] loss: 758.456
[91,     1] loss: 767.783
[92,     1] loss: 761.735
[93,     1] loss: 748.850
[94,     1] loss: 720.326
[95,     1] loss: 734.724
[96,     1] loss: 687.992
[97,     1] loss: 708.874
[98,     1] loss: 806.672
[99,     1] loss: 822.396
[100,     1] loss: 730.957
[101,     1] loss: 922.326
[102,     1] loss: 761.181
[103,     1] loss: 929.797
[104,     1] loss: 693.977
[105,     1] loss: 897.704
[106,     1] loss: 708.461
[107,     1] loss: 800.911
[108,     1] loss: 682.013
[109,     1] loss: 655.828
[110,     1] loss: 756.484
[111,     1] loss: 764.471
[112,     1] loss: 655.948
[113,     1] loss: 657.561
[114,     1] loss: 696.435
[115,     1] loss: 598.996
[116,     1] loss: 585.107
[117,     1] loss: 583.651
[118,     1] loss: 591.728
[119,     1] loss: 559.223
[120,     1] loss: 690.380
[121,     1] loss: 544.876
[122,     1] loss: 583.853
[123,     1] loss: 524.059
Early stopping applied (best metric=0.3338603973388672)
Finished Training
Total time taken: 18.12919020652771
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1820.607
[2,     1] loss: 1811.216
[3,     1] loss: 1810.412
[4,     1] loss: 1813.012
[5,     1] loss: 1808.788
[6,     1] loss: 1805.385
[7,     1] loss: 1806.341
[8,     1] loss: 1803.958
[9,     1] loss: 1816.082
[10,     1] loss: 1805.347
[11,     1] loss: 1806.021
[12,     1] loss: 1795.551
[13,     1] loss: 1795.812
[14,     1] loss: 1778.534
[15,     1] loss: 1769.542
[16,     1] loss: 1748.893
[17,     1] loss: 1737.859
[18,     1] loss: 1685.085
[19,     1] loss: 1673.786
[20,     1] loss: 1623.154
[21,     1] loss: 1632.936
[22,     1] loss: 1595.132
[23,     1] loss: 1536.013
[24,     1] loss: 1520.957
[25,     1] loss: 1533.137
[26,     1] loss: 1457.238
[27,     1] loss: 1529.980
[28,     1] loss: 1479.281
[29,     1] loss: 1462.635
[30,     1] loss: 1476.413
[31,     1] loss: 1434.808
[32,     1] loss: 1485.799
[33,     1] loss: 1478.240
[34,     1] loss: 1427.161
[35,     1] loss: 1390.314
[36,     1] loss: 1360.377
[37,     1] loss: 1341.547
[38,     1] loss: 1402.385
[39,     1] loss: 1336.359
[40,     1] loss: 1301.187
[41,     1] loss: 1365.709
[42,     1] loss: 1299.660
[43,     1] loss: 1278.638
[44,     1] loss: 1343.935
[45,     1] loss: 1276.290
[46,     1] loss: 1244.867
[47,     1] loss: 1209.014
[48,     1] loss: 1256.955
[49,     1] loss: 1235.717
[50,     1] loss: 1153.483
[51,     1] loss: 1184.526
[52,     1] loss: 1136.993
[53,     1] loss: 1135.276
[54,     1] loss: 1110.655
[55,     1] loss: 1096.151
[56,     1] loss: 1062.807
[57,     1] loss: 1092.660
[58,     1] loss: 1046.177
[59,     1] loss: 995.583
[60,     1] loss: 1051.032
[61,     1] loss: 988.670
[62,     1] loss: 938.455
[63,     1] loss: 970.811
[64,     1] loss: 1022.683
[65,     1] loss: 908.134
[66,     1] loss: 1037.052
[67,     1] loss: 933.453
[68,     1] loss: 924.335
[69,     1] loss: 1100.352
[70,     1] loss: 898.645
[71,     1] loss: 933.732
[72,     1] loss: 884.963
[73,     1] loss: 927.949
[74,     1] loss: 881.324
[75,     1] loss: 916.171
[76,     1] loss: 853.282
[77,     1] loss: 895.640
[78,     1] loss: 865.843
[79,     1] loss: 849.211
[80,     1] loss: 930.508
[81,     1] loss: 844.622
[82,     1] loss: 890.645
[83,     1] loss: 851.115
[84,     1] loss: 871.740
Early stopping applied (best metric=0.37315675616264343)
Finished Training
Total time taken: 12.425476551055908
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1810.255
[2,     1] loss: 1810.420
[3,     1] loss: 1816.497
[4,     1] loss: 1812.151
[5,     1] loss: 1810.569
[6,     1] loss: 1806.974
[7,     1] loss: 1811.589
[8,     1] loss: 1810.011
[9,     1] loss: 1802.648
[10,     1] loss: 1794.247
[11,     1] loss: 1780.296
[12,     1] loss: 1765.215
[13,     1] loss: 1760.387
[14,     1] loss: 1728.141
[15,     1] loss: 1701.812
[16,     1] loss: 1640.965
[17,     1] loss: 1606.527
[18,     1] loss: 1545.145
[19,     1] loss: 1566.752
[20,     1] loss: 1548.806
[21,     1] loss: 1567.625
[22,     1] loss: 1477.557
[23,     1] loss: 1466.623
[24,     1] loss: 1533.600
[25,     1] loss: 1555.285
[26,     1] loss: 1503.539
[27,     1] loss: 1513.736
[28,     1] loss: 1520.654
[29,     1] loss: 1461.473
[30,     1] loss: 1453.127
[31,     1] loss: 1424.314
[32,     1] loss: 1453.206
[33,     1] loss: 1431.667
[34,     1] loss: 1438.724
[35,     1] loss: 1427.278
[36,     1] loss: 1395.221
[37,     1] loss: 1283.208
[38,     1] loss: 1331.180
[39,     1] loss: 1360.516
[40,     1] loss: 1289.813
[41,     1] loss: 1336.632
[42,     1] loss: 1397.896
[43,     1] loss: 1359.881
[44,     1] loss: 1231.683
[45,     1] loss: 1296.555
[46,     1] loss: 1204.242
[47,     1] loss: 1174.605
[48,     1] loss: 1256.969
[49,     1] loss: 1092.829
[50,     1] loss: 1143.826
[51,     1] loss: 1048.165
[52,     1] loss: 1054.874
[53,     1] loss: 1088.805
[54,     1] loss: 1270.327
[55,     1] loss: 1259.488
[56,     1] loss: 1048.693
[57,     1] loss: 1042.786
[58,     1] loss: 1117.880
[59,     1] loss: 1004.359
[60,     1] loss: 1099.629
[61,     1] loss: 989.106
[62,     1] loss: 1066.163
[63,     1] loss: 1013.837
[64,     1] loss: 1088.445
[65,     1] loss: 1017.245
[66,     1] loss: 904.012
[67,     1] loss: 928.841
[68,     1] loss: 1068.323
[69,     1] loss: 1010.730
[70,     1] loss: 957.450
[71,     1] loss: 957.767
[72,     1] loss: 945.721
[73,     1] loss: 925.983
[74,     1] loss: 953.925
[75,     1] loss: 891.813
[76,     1] loss: 961.567
[77,     1] loss: 863.214
[78,     1] loss: 905.282
[79,     1] loss: 844.284
[80,     1] loss: 969.744
[81,     1] loss: 879.695
[82,     1] loss: 973.354
[83,     1] loss: 833.303
[84,     1] loss: 839.611
[85,     1] loss: 813.773
[86,     1] loss: 760.882
Early stopping applied (best metric=0.3507102131843567)
Finished Training
Total time taken: 12.780582666397095
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1821.540
[2,     1] loss: 1809.071
[3,     1] loss: 1814.866
[4,     1] loss: 1820.918
[5,     1] loss: 1807.314
[6,     1] loss: 1801.440
[7,     1] loss: 1793.835
[8,     1] loss: 1817.821
[9,     1] loss: 1812.326
[10,     1] loss: 1794.667
[11,     1] loss: 1805.197
[12,     1] loss: 1794.460
[13,     1] loss: 1788.473
[14,     1] loss: 1790.987
[15,     1] loss: 1777.237
[16,     1] loss: 1762.711
[17,     1] loss: 1728.614
[18,     1] loss: 1715.089
[19,     1] loss: 1698.520
[20,     1] loss: 1647.850
[21,     1] loss: 1643.210
[22,     1] loss: 1619.900
[23,     1] loss: 1598.955
[24,     1] loss: 1592.519
[25,     1] loss: 1534.825
[26,     1] loss: 1518.958
[27,     1] loss: 1505.974
[28,     1] loss: 1495.549
[29,     1] loss: 1534.541
[30,     1] loss: 1449.907
[31,     1] loss: 1467.222
[32,     1] loss: 1460.124
[33,     1] loss: 1441.427
[34,     1] loss: 1409.733
[35,     1] loss: 1427.461
[36,     1] loss: 1437.258
[37,     1] loss: 1352.144
[38,     1] loss: 1373.822
[39,     1] loss: 1290.913
[40,     1] loss: 1261.990
[41,     1] loss: 1278.225
[42,     1] loss: 1309.407
[43,     1] loss: 1295.199
[44,     1] loss: 1215.156
[45,     1] loss: 1241.045
[46,     1] loss: 1220.082
[47,     1] loss: 1227.781
[48,     1] loss: 1186.246
[49,     1] loss: 1199.535
[50,     1] loss: 1199.279
[51,     1] loss: 1182.398
[52,     1] loss: 1087.146
[53,     1] loss: 1235.823
[54,     1] loss: 1130.453
[55,     1] loss: 1158.213
[56,     1] loss: 1077.519
[57,     1] loss: 1171.828
[58,     1] loss: 1037.762
[59,     1] loss: 1205.035
[60,     1] loss: 1044.188
[61,     1] loss: 1028.243
[62,     1] loss: 1064.013
[63,     1] loss: 1059.432
[64,     1] loss: 1000.895
[65,     1] loss: 990.148
[66,     1] loss: 1013.083
[67,     1] loss: 964.473
[68,     1] loss: 883.369
[69,     1] loss: 948.531
[70,     1] loss: 888.789
[71,     1] loss: 875.101
[72,     1] loss: 850.774
[73,     1] loss: 876.701
[74,     1] loss: 863.888
[75,     1] loss: 772.113
[76,     1] loss: 837.681
[77,     1] loss: 840.628
[78,     1] loss: 855.999
[79,     1] loss: 973.542
[80,     1] loss: 823.909
[81,     1] loss: 888.827
[82,     1] loss: 962.372
Early stopping applied (best metric=0.42158591747283936)
Finished Training
Total time taken: 12.054018259048462
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1814.067
[2,     1] loss: 1810.191
[3,     1] loss: 1810.024
[4,     1] loss: 1806.423
[5,     1] loss: 1815.611
[6,     1] loss: 1811.561
[7,     1] loss: 1807.081
[8,     1] loss: 1801.318
[9,     1] loss: 1799.775
[10,     1] loss: 1792.784
[11,     1] loss: 1778.556
[12,     1] loss: 1749.721
[13,     1] loss: 1732.119
[14,     1] loss: 1689.868
[15,     1] loss: 1656.448
[16,     1] loss: 1630.582
[17,     1] loss: 1612.481
[18,     1] loss: 1578.708
[19,     1] loss: 1524.015
[20,     1] loss: 1527.465
[21,     1] loss: 1465.123
[22,     1] loss: 1499.671
[23,     1] loss: 1490.024
[24,     1] loss: 1447.507
[25,     1] loss: 1438.723
[26,     1] loss: 1400.887
[27,     1] loss: 1388.988
[28,     1] loss: 1371.244
[29,     1] loss: 1383.383
[30,     1] loss: 1312.926
[31,     1] loss: 1263.724
[32,     1] loss: 1201.970
[33,     1] loss: 1231.949
[34,     1] loss: 1311.665
[35,     1] loss: 1262.440
[36,     1] loss: 1154.625
[37,     1] loss: 1383.018
[38,     1] loss: 1065.248
[39,     1] loss: 1134.960
[40,     1] loss: 1169.893
[41,     1] loss: 1136.507
[42,     1] loss: 1197.074
[43,     1] loss: 1114.041
[44,     1] loss: 1042.663
[45,     1] loss: 1061.946
[46,     1] loss: 1000.274
[47,     1] loss: 1115.127
[48,     1] loss: 959.009
[49,     1] loss: 1003.759
[50,     1] loss: 870.118
[51,     1] loss: 936.037
[52,     1] loss: 987.442
[53,     1] loss: 866.361
[54,     1] loss: 899.803
[55,     1] loss: 939.668
[56,     1] loss: 919.333
[57,     1] loss: 878.866
[58,     1] loss: 909.504
[59,     1] loss: 927.480
[60,     1] loss: 904.543
[61,     1] loss: 841.391
[62,     1] loss: 815.179
[63,     1] loss: 854.013
[64,     1] loss: 815.202
[65,     1] loss: 918.002
[66,     1] loss: 959.871
[67,     1] loss: 749.825
[68,     1] loss: 801.563
[69,     1] loss: 766.049
[70,     1] loss: 758.028
[71,     1] loss: 774.705
[72,     1] loss: 709.475
Early stopping applied (best metric=0.4661198556423187)
Finished Training
Total time taken: 10.676976680755615
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1818.266
[2,     1] loss: 1805.985
[3,     1] loss: 1812.886
[4,     1] loss: 1821.132
[5,     1] loss: 1807.966
[6,     1] loss: 1818.534
[7,     1] loss: 1804.933
[8,     1] loss: 1811.910
[9,     1] loss: 1804.104
[10,     1] loss: 1804.963
[11,     1] loss: 1812.645
[12,     1] loss: 1805.717
[13,     1] loss: 1800.676
[14,     1] loss: 1799.783
[15,     1] loss: 1797.850
[16,     1] loss: 1786.846
[17,     1] loss: 1780.713
[18,     1] loss: 1755.520
[19,     1] loss: 1736.900
[20,     1] loss: 1699.219
[21,     1] loss: 1654.388
[22,     1] loss: 1630.076
[23,     1] loss: 1611.448
[24,     1] loss: 1553.526
[25,     1] loss: 1500.672
[26,     1] loss: 1485.342
[27,     1] loss: 1520.675
[28,     1] loss: 1604.594
[29,     1] loss: 1525.091
[30,     1] loss: 1527.894
[31,     1] loss: 1486.890
[32,     1] loss: 1510.676
[33,     1] loss: 1424.432
[34,     1] loss: 1435.193
[35,     1] loss: 1455.014
[36,     1] loss: 1454.133
[37,     1] loss: 1340.419
[38,     1] loss: 1316.591
[39,     1] loss: 1372.665
[40,     1] loss: 1361.193
[41,     1] loss: 1217.582
[42,     1] loss: 1333.439
[43,     1] loss: 1247.317
[44,     1] loss: 1363.086
[45,     1] loss: 1183.430
[46,     1] loss: 1193.093
[47,     1] loss: 1243.296
[48,     1] loss: 1258.274
[49,     1] loss: 1167.448
[50,     1] loss: 1200.236
[51,     1] loss: 1215.895
[52,     1] loss: 1126.604
[53,     1] loss: 1251.464
[54,     1] loss: 1055.456
[55,     1] loss: 1298.981
[56,     1] loss: 1085.852
[57,     1] loss: 1149.678
[58,     1] loss: 1244.732
[59,     1] loss: 1072.813
[60,     1] loss: 1040.349
[61,     1] loss: 1057.423
[62,     1] loss: 971.867
[63,     1] loss: 1072.019
[64,     1] loss: 1040.658
[65,     1] loss: 1031.315
[66,     1] loss: 872.420
[67,     1] loss: 902.274
[68,     1] loss: 914.047
[69,     1] loss: 913.234
[70,     1] loss: 841.656
[71,     1] loss: 897.773
[72,     1] loss: 929.889
[73,     1] loss: 849.333
[74,     1] loss: 795.975
[75,     1] loss: 845.811
[76,     1] loss: 844.651
[77,     1] loss: 783.623
[78,     1] loss: 831.824
[79,     1] loss: 741.762
[80,     1] loss: 810.886
[81,     1] loss: 689.815
[82,     1] loss: 708.581
[83,     1] loss: 780.226
[84,     1] loss: 687.517
[85,     1] loss: 775.945
[86,     1] loss: 664.498
[87,     1] loss: 724.998
[88,     1] loss: 672.298
[89,     1] loss: 755.057
[90,     1] loss: 732.599
Early stopping applied (best metric=0.43826064467430115)
Finished Training
Total time taken: 13.233085870742798
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1809.633
[2,     1] loss: 1818.764
[3,     1] loss: 1817.816
[4,     1] loss: 1811.080
[5,     1] loss: 1809.574
[6,     1] loss: 1808.955
[7,     1] loss: 1809.569
[8,     1] loss: 1797.967
[9,     1] loss: 1804.806
[10,     1] loss: 1791.716
[11,     1] loss: 1790.132
[12,     1] loss: 1768.907
[13,     1] loss: 1766.448
[14,     1] loss: 1732.281
[15,     1] loss: 1684.455
[16,     1] loss: 1693.488
[17,     1] loss: 1650.354
[18,     1] loss: 1585.235
[19,     1] loss: 1616.511
[20,     1] loss: 1575.432
[21,     1] loss: 1601.613
[22,     1] loss: 1530.880
[23,     1] loss: 1504.584
[24,     1] loss: 1563.607
[25,     1] loss: 1547.724
[26,     1] loss: 1587.917
[27,     1] loss: 1431.185
[28,     1] loss: 1475.216
[29,     1] loss: 1460.987
[30,     1] loss: 1458.297
[31,     1] loss: 1454.868
[32,     1] loss: 1476.231
[33,     1] loss: 1468.195
[34,     1] loss: 1416.034
[35,     1] loss: 1394.830
[36,     1] loss: 1389.967
[37,     1] loss: 1350.780
[38,     1] loss: 1374.559
[39,     1] loss: 1380.665
[40,     1] loss: 1279.954
[41,     1] loss: 1250.757
[42,     1] loss: 1302.890
[43,     1] loss: 1196.640
[44,     1] loss: 1289.355
[45,     1] loss: 1244.968
[46,     1] loss: 1187.578
[47,     1] loss: 1152.545
[48,     1] loss: 1066.595
[49,     1] loss: 1165.402
[50,     1] loss: 1114.323
[51,     1] loss: 1004.356
[52,     1] loss: 924.032
[53,     1] loss: 972.267
[54,     1] loss: 1011.672
[55,     1] loss: 1008.843
[56,     1] loss: 968.976
[57,     1] loss: 1023.662
[58,     1] loss: 929.985
[59,     1] loss: 984.550
[60,     1] loss: 939.063
[61,     1] loss: 938.659
[62,     1] loss: 908.329
[63,     1] loss: 903.592
[64,     1] loss: 842.779
[65,     1] loss: 880.887
[66,     1] loss: 824.979
[67,     1] loss: 1023.139
[68,     1] loss: 899.511
[69,     1] loss: 790.681
[70,     1] loss: 862.506
[71,     1] loss: 910.885
[72,     1] loss: 909.387
[73,     1] loss: 769.112
[74,     1] loss: 928.091
[75,     1] loss: 792.942
[76,     1] loss: 829.609
[77,     1] loss: 781.536
[78,     1] loss: 782.496
[79,     1] loss: 780.029
[80,     1] loss: 878.485
[81,     1] loss: 701.591
[82,     1] loss: 855.895
[83,     1] loss: 714.670
[84,     1] loss: 780.130
[85,     1] loss: 764.631
[86,     1] loss: 696.863
[87,     1] loss: 743.240
[88,     1] loss: 678.869
[89,     1] loss: 703.617
[90,     1] loss: 689.096
Early stopping applied (best metric=0.40503668785095215)
Finished Training
Total time taken: 13.257016897201538
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1819.773
[2,     1] loss: 1812.365
[3,     1] loss: 1811.240
[4,     1] loss: 1806.778
[5,     1] loss: 1820.558
[6,     1] loss: 1814.921
[7,     1] loss: 1806.843
[8,     1] loss: 1815.828
[9,     1] loss: 1813.497
[10,     1] loss: 1811.161
[11,     1] loss: 1819.677
[12,     1] loss: 1801.871
[13,     1] loss: 1803.776
[14,     1] loss: 1791.935
[15,     1] loss: 1789.408
[16,     1] loss: 1776.506
[17,     1] loss: 1768.782
[18,     1] loss: 1750.807
[19,     1] loss: 1710.805
[20,     1] loss: 1692.413
[21,     1] loss: 1676.359
[22,     1] loss: 1623.678
[23,     1] loss: 1594.656
[24,     1] loss: 1630.529
[25,     1] loss: 1529.644
[26,     1] loss: 1554.420
[27,     1] loss: 1520.561
[28,     1] loss: 1503.156
[29,     1] loss: 1466.905
[30,     1] loss: 1478.419
[31,     1] loss: 1464.342
[32,     1] loss: 1414.987
[33,     1] loss: 1489.045
[34,     1] loss: 1457.603
[35,     1] loss: 1369.495
[36,     1] loss: 1531.818
[37,     1] loss: 1434.370
[38,     1] loss: 1474.065
[39,     1] loss: 1444.845
[40,     1] loss: 1349.480
[41,     1] loss: 1314.898
[42,     1] loss: 1382.964
[43,     1] loss: 1297.911
[44,     1] loss: 1266.918
[45,     1] loss: 1259.210
[46,     1] loss: 1350.662
[47,     1] loss: 1247.998
[48,     1] loss: 1273.750
[49,     1] loss: 1156.954
[50,     1] loss: 1147.005
[51,     1] loss: 1163.117
[52,     1] loss: 1265.684
[53,     1] loss: 1179.187
[54,     1] loss: 1113.212
[55,     1] loss: 1055.998
[56,     1] loss: 1180.568
[57,     1] loss: 1067.506
[58,     1] loss: 1081.742
[59,     1] loss: 1040.481
[60,     1] loss: 1079.801
[61,     1] loss: 1005.499
[62,     1] loss: 1065.779
[63,     1] loss: 970.842
[64,     1] loss: 1010.933
[65,     1] loss: 822.466
[66,     1] loss: 1039.548
[67,     1] loss: 931.597
[68,     1] loss: 1023.058
[69,     1] loss: 874.519
[70,     1] loss: 952.762
[71,     1] loss: 928.662
[72,     1] loss: 864.890
[73,     1] loss: 822.902
[74,     1] loss: 850.693
[75,     1] loss: 843.643
[76,     1] loss: 934.838
[77,     1] loss: 847.870
[78,     1] loss: 824.654
[79,     1] loss: 824.276
[80,     1] loss: 901.553
[81,     1] loss: 848.627
[82,     1] loss: 812.018
[83,     1] loss: 910.854
[84,     1] loss: 773.637
[85,     1] loss: 887.478
[86,     1] loss: 717.481
[87,     1] loss: 885.350
[88,     1] loss: 762.135
[89,     1] loss: 860.834
[90,     1] loss: 775.036
[91,     1] loss: 762.081
[92,     1] loss: 830.267
[93,     1] loss: 734.139
[94,     1] loss: 843.265
[95,     1] loss: 684.773
Early stopping applied (best metric=0.35732096433639526)
Finished Training
Total time taken: 14.200082302093506
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1819.207
[2,     1] loss: 1817.218
[3,     1] loss: 1816.026
[4,     1] loss: 1813.547
[5,     1] loss: 1807.242
[6,     1] loss: 1813.503
[7,     1] loss: 1802.973
[8,     1] loss: 1809.715
[9,     1] loss: 1804.945
[10,     1] loss: 1809.458
[11,     1] loss: 1802.112
[12,     1] loss: 1801.329
[13,     1] loss: 1798.426
[14,     1] loss: 1798.284
[15,     1] loss: 1782.013
[16,     1] loss: 1774.654
[17,     1] loss: 1760.144
[18,     1] loss: 1708.608
[19,     1] loss: 1657.844
[20,     1] loss: 1666.700
[21,     1] loss: 1640.787
[22,     1] loss: 1632.788
[23,     1] loss: 1589.106
[24,     1] loss: 1516.490
[25,     1] loss: 1543.104
[26,     1] loss: 1500.427
[27,     1] loss: 1598.240
[28,     1] loss: 1468.885
[29,     1] loss: 1439.498
[30,     1] loss: 1484.564
[31,     1] loss: 1413.167
[32,     1] loss: 1393.185
[33,     1] loss: 1418.381
[34,     1] loss: 1516.392
[35,     1] loss: 1459.118
[36,     1] loss: 1443.226
[37,     1] loss: 1401.379
[38,     1] loss: 1330.408
[39,     1] loss: 1351.578
[40,     1] loss: 1325.771
[41,     1] loss: 1268.738
[42,     1] loss: 1306.818
[43,     1] loss: 1294.673
[44,     1] loss: 1243.234
[45,     1] loss: 1297.636
[46,     1] loss: 1231.017
[47,     1] loss: 1264.071
[48,     1] loss: 1282.163
[49,     1] loss: 1288.170
[50,     1] loss: 1209.116
[51,     1] loss: 1203.107
[52,     1] loss: 1241.085
[53,     1] loss: 1193.877
[54,     1] loss: 1143.633
[55,     1] loss: 1140.387
[56,     1] loss: 1032.431
[57,     1] loss: 1037.944
[58,     1] loss: 1182.224
[59,     1] loss: 1128.045
[60,     1] loss: 1262.137
[61,     1] loss: 1086.540
[62,     1] loss: 1076.066
[63,     1] loss: 1025.185
[64,     1] loss: 1030.287
[65,     1] loss: 1014.154
[66,     1] loss: 994.451
[67,     1] loss: 1024.091
[68,     1] loss: 994.879
[69,     1] loss: 933.237
[70,     1] loss: 957.644
[71,     1] loss: 1105.383
[72,     1] loss: 982.279
[73,     1] loss: 1043.293
[74,     1] loss: 1190.775
[75,     1] loss: 932.744
[76,     1] loss: 999.962
[77,     1] loss: 1021.004
[78,     1] loss: 965.574
[79,     1] loss: 1141.128
[80,     1] loss: 881.262
[81,     1] loss: 878.994
Early stopping applied (best metric=0.3996480107307434)
Finished Training
Total time taken: 11.850661277770996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1810.250
[2,     1] loss: 1825.204
[3,     1] loss: 1815.727
[4,     1] loss: 1821.417
[5,     1] loss: 1807.129
[6,     1] loss: 1814.469
[7,     1] loss: 1811.261
[8,     1] loss: 1804.778
[9,     1] loss: 1803.003
[10,     1] loss: 1794.939
[11,     1] loss: 1784.591
[12,     1] loss: 1770.238
[13,     1] loss: 1732.259
[14,     1] loss: 1695.027
[15,     1] loss: 1658.408
[16,     1] loss: 1622.909
[17,     1] loss: 1572.118
[18,     1] loss: 1519.497
[19,     1] loss: 1473.701
[20,     1] loss: 1483.440
[21,     1] loss: 1544.940
[22,     1] loss: 1542.582
[23,     1] loss: 1550.715
[24,     1] loss: 1488.843
[25,     1] loss: 1487.300
[26,     1] loss: 1457.090
[27,     1] loss: 1449.879
[28,     1] loss: 1477.332
[29,     1] loss: 1410.292
[30,     1] loss: 1352.011
[31,     1] loss: 1364.626
[32,     1] loss: 1350.255
[33,     1] loss: 1359.756
[34,     1] loss: 1410.370
[35,     1] loss: 1270.108
[36,     1] loss: 1305.260
[37,     1] loss: 1322.378
[38,     1] loss: 1338.113
[39,     1] loss: 1212.970
[40,     1] loss: 1248.798
[41,     1] loss: 1200.811
[42,     1] loss: 1185.683
[43,     1] loss: 1152.792
[44,     1] loss: 1126.441
[45,     1] loss: 1204.381
[46,     1] loss: 1106.631
[47,     1] loss: 1154.503
[48,     1] loss: 1073.530
[49,     1] loss: 1005.423
[50,     1] loss: 1077.578
[51,     1] loss: 995.358
[52,     1] loss: 976.267
[53,     1] loss: 1137.651
[54,     1] loss: 923.598
[55,     1] loss: 1041.605
[56,     1] loss: 1027.662
[57,     1] loss: 1013.328
[58,     1] loss: 953.350
[59,     1] loss: 1014.833
[60,     1] loss: 956.020
[61,     1] loss: 989.101
[62,     1] loss: 960.614
[63,     1] loss: 890.171
[64,     1] loss: 1047.685
[65,     1] loss: 929.426
[66,     1] loss: 925.372
[67,     1] loss: 902.913
[68,     1] loss: 910.206
[69,     1] loss: 832.162
[70,     1] loss: 854.735
[71,     1] loss: 835.318
[72,     1] loss: 905.088
[73,     1] loss: 861.810
[74,     1] loss: 950.223
[75,     1] loss: 833.739
[76,     1] loss: 940.787
[77,     1] loss: 848.225
[78,     1] loss: 876.126
[79,     1] loss: 787.109
Early stopping applied (best metric=0.38197487592697144)
Finished Training
Total time taken: 11.6675705909729
{'Hydroxylation-K Validation Accuracy': 0.7181737588652483, 'Hydroxylation-K Validation Sensitivity': 0.6844444444444444, 'Hydroxylation-K Validation Specificity': 0.7263157894736842, 'Hydroxylation-K Validation Precision': 0.39602049311969895, 'Hydroxylation-K AUC ROC': 0.7700974658869396, 'Hydroxylation-K AUC PR': 0.5520864100538699, 'Hydroxylation-K MCC': 0.3503428025914975, 'Hydroxylation-K F1': 0.4953559977605455, 'Validation Loss (Hydroxylation-K)': 0.5040982643763224, 'Hydroxylation-P Validation Accuracy': 0.7708535607329577, 'Hydroxylation-P Validation Sensitivity': 0.7496825396825397, 'Hydroxylation-P Validation Specificity': 0.7753528854306948, 'Hydroxylation-P Validation Precision': 0.41952938972821524, 'Hydroxylation-P AUC ROC': 0.8151088642350052, 'Hydroxylation-P AUC PR': 0.5532057795615531, 'Hydroxylation-P MCC': 0.43186245085104, 'Hydroxylation-P F1': 0.5357034077489228, 'Validation Loss (Hydroxylation-P)': 0.4003750503063202, 'Validation Loss (total)': 0.9044733087221781, 'TimeToTrain': 12.526891803741455}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003934312379796397,
 'learning_rate_Hydroxylation-K': 0.0009325163834347744,
 'learning_rate_Hydroxylation-P': 0.006129606841230501,
 'log_base': 2.392163803753456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3892820569,
 'sample_weights': [4.286893539929564, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.08462725505385,
 'weight_decay_Hydroxylation-K': 7.425360927846125,
 'weight_decay_Hydroxylation-P': 2.887822800657177}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1314.113
[2,     1] loss: 1310.916
[3,     1] loss: 1309.947
[4,     1] loss: 1309.474
[5,     1] loss: 1309.744
[6,     1] loss: 1304.650
[7,     1] loss: 1303.041
[8,     1] loss: 1291.864
[9,     1] loss: 1268.513
[10,     1] loss: 1241.915
[11,     1] loss: 1196.122
[12,     1] loss: 1137.965
[13,     1] loss: 1122.335
[14,     1] loss: 1152.593
[15,     1] loss: 1128.472
[16,     1] loss: 1079.788
[17,     1] loss: 1116.938
[18,     1] loss: 1025.973
[19,     1] loss: 1056.314
[20,     1] loss: 1033.759
[21,     1] loss: 1005.213
[22,     1] loss: 1016.452
[23,     1] loss: 979.335
[24,     1] loss: 1014.565
[25,     1] loss: 1017.567
[26,     1] loss: 966.988
[27,     1] loss: 958.480
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004311312761395573,
 'learning_rate_Hydroxylation-K': 0.004448951678998895,
 'learning_rate_Hydroxylation-P': 0.008389649411826934,
 'log_base': 1.617492294767577,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 534583211,
 'sample_weights': [1.9140637185862155, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.9344231954871445,
 'weight_decay_Hydroxylation-K': 3.6467595461531297,
 'weight_decay_Hydroxylation-P': 7.093725676623931}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1643.230
[2,     1] loss: 1640.538
[3,     1] loss: 1640.071
[4,     1] loss: 1658.775
[5,     1] loss: 1644.190
[6,     1] loss: 1636.564
[7,     1] loss: 1647.294
[8,     1] loss: 1634.774
[9,     1] loss: 1639.116
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004969870431087432,
 'learning_rate_Hydroxylation-K': 0.004493014452879448,
 'learning_rate_Hydroxylation-P': 0.007324305248549582,
 'log_base': 1.3199136240353577,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3302626288,
 'sample_weights': [3.4716636561188303, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.811639989141836,
 'weight_decay_Hydroxylation-K': 5.320345616920115,
 'weight_decay_Hydroxylation-P': 5.70526617533145}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2184.939
[2,     1] loss: 2177.647
[3,     1] loss: 2183.843
[4,     1] loss: 2178.702
[5,     1] loss: 2189.260
[6,     1] loss: 2169.198
[7,     1] loss: 2168.583
[8,     1] loss: 2173.482
[9,     1] loss: 2174.651
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005478583830037029,
 'learning_rate_Hydroxylation-K': 0.003970169962500601,
 'learning_rate_Hydroxylation-P': 0.007795132117771882,
 'log_base': 1.0594998079890712,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 520374108,
 'sample_weights': [6.014574387196118, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.313693918239959,
 'weight_decay_Hydroxylation-K': 9.888009064068529,
 'weight_decay_Hydroxylation-P': 5.296707563468342}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9441.694
[2,     1] loss: 9374.598
[3,     1] loss: 9398.062
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008405710668882179,
 'learning_rate_Hydroxylation-K': 0.003125258519065269,
 'learning_rate_Hydroxylation-P': 0.009281912395571204,
 'log_base': 1.0759343440301825,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1735761330,
 'sample_weights': [28.884639868067648, 3.6107190775481706],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.324498011027953,
 'weight_decay_Hydroxylation-K': 2.6047413285234517,
 'weight_decay_Hydroxylation-P': 5.260558606951176}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7413.030
[2,     1] loss: 7406.445
[3,     1] loss: 7373.987
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002171634634347914,
 'learning_rate_Hydroxylation-K': 0.004041196519651513,
 'learning_rate_Hydroxylation-P': 0.005594596427396952,
 'log_base': 1.19981696473377,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 369320223,
 'sample_weights': [22.8098905709229, 2.851346162437969],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.899941731631194,
 'weight_decay_Hydroxylation-K': 5.157698703279253,
 'weight_decay_Hydroxylation-P': 5.935990299295176}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2974.362
[2,     1] loss: 2976.468
[3,     1] loss: 2993.751
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004372948909568138,
 'learning_rate_Hydroxylation-K': 0.004395699521160007,
 'learning_rate_Hydroxylation-P': 0.0007944029512009327,
 'log_base': 2.7513374748440143,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1422099303,
 'sample_weights': [9.164254087623918, 1.1455758914364522],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.359388454213365,
 'weight_decay_Hydroxylation-K': 9.101641057091516,
 'weight_decay_Hydroxylation-P': 9.196353562447985}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.625
[2,     1] loss: 1258.621
[3,     1] loss: 1255.552
[4,     1] loss: 1256.296
[5,     1] loss: 1249.547
[6,     1] loss: 1254.670
[7,     1] loss: 1247.994
[8,     1] loss: 1234.525
[9,     1] loss: 1224.398
[10,     1] loss: 1189.780
[11,     1] loss: 1161.270
[12,     1] loss: 1146.738
[13,     1] loss: 1113.902
[14,     1] loss: 1082.632
[15,     1] loss: 1081.793
[16,     1] loss: 1049.342
[17,     1] loss: 1023.943
[18,     1] loss: 1074.774
[19,     1] loss: 1003.734
[20,     1] loss: 1077.107
[21,     1] loss: 968.665
[22,     1] loss: 1002.577
[23,     1] loss: 973.574
[24,     1] loss: 1017.012
[25,     1] loss: 995.228
[26,     1] loss: 989.043
[27,     1] loss: 938.058
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003641194576871182,
 'learning_rate_Hydroxylation-K': 0.0020556296934863974,
 'learning_rate_Hydroxylation-P': 0.00754712752457703,
 'log_base': 2.2204950785017044,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2890707029,
 'sample_weights': [1.64950533245044, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.873426717929227,
 'weight_decay_Hydroxylation-K': 8.47179251611058,
 'weight_decay_Hydroxylation-P': 4.806387712936513}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.205
[2,     1] loss: 1351.374
[3,     1] loss: 1349.980
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008506861644603259,
 'learning_rate_Hydroxylation-K': 0.0017424572906760477,
 'learning_rate_Hydroxylation-P': 0.0018008163516378176,
 'log_base': 1.4465658610151095,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1448198521,
 'sample_weights': [2.09274161944353, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.493442143832635,
 'weight_decay_Hydroxylation-K': 2.1805440285662128,
 'weight_decay_Hydroxylation-P': 9.974293790436825}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1868.354
[2,     1] loss: 1858.239
[3,     1] loss: 1882.263
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00365678666064384,
 'learning_rate_Hydroxylation-K': 0.0003205968730134618,
 'learning_rate_Hydroxylation-P': 0.00517156250694379,
 'log_base': 2.263153213259692,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1719817921,
 'sample_weights': [4.521878720550037, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.313032783375401,
 'weight_decay_Hydroxylation-K': 5.185239187565517,
 'weight_decay_Hydroxylation-P': 3.1747602281204044}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1340.883
[2,     1] loss: 1341.211
[3,     1] loss: 1338.544
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007871212562666776,
 'learning_rate_Hydroxylation-K': 0.006241392423616735,
 'learning_rate_Hydroxylation-P': 0.009478542794994129,
 'log_base': 1.232239160366427,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 456537598,
 'sample_weights': [2.043984833351829, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.6948127553666055,
 'weight_decay_Hydroxylation-K': 5.591770742821474,
 'weight_decay_Hydroxylation-P': 7.342101502366103}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2600.403
[2,     1] loss: 2590.864
[3,     1] loss: 2596.600
[4,     1] loss: 2582.878
[5,     1] loss: 2586.410
[6,     1] loss: 2589.628
[7,     1] loss: 2580.786
[8,     1] loss: 2585.259
[9,     1] loss: 2583.311
[10,     1] loss: 2602.678
[11,     1] loss: 2579.538
[12,     1] loss: 2581.182
[13,     1] loss: 2569.667
[14,     1] loss: 2580.961
[15,     1] loss: 2570.664
[16,     1] loss: 2567.667
[17,     1] loss: 2555.436
[18,     1] loss: 2547.907
[19,     1] loss: 2522.427
[20,     1] loss: 2510.889
[21,     1] loss: 2526.194
[22,     1] loss: 2502.972
[23,     1] loss: 2462.225
[24,     1] loss: 2412.438
[25,     1] loss: 2442.225
[26,     1] loss: 2391.470
[27,     1] loss: 2348.105
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005079599494704931,
 'learning_rate_Hydroxylation-K': 0.0051125468950555765,
 'learning_rate_Hydroxylation-P': 0.008697127591531198,
 'log_base': 1.1596918990643135,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2778977540,
 'sample_weights': [7.994155079328929, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.461502577862451,
 'weight_decay_Hydroxylation-K': 1.4466607245318328,
 'weight_decay_Hydroxylation-P': 5.532599360191689}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3638.458
[2,     1] loss: 3702.395
[3,     1] loss: 3668.903
[4,     1] loss: 3652.312
[5,     1] loss: 3652.351
[6,     1] loss: 3661.865
[7,     1] loss: 3638.590
[8,     1] loss: 3643.733
[9,     1] loss: 3645.184
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004518847713756999,
 'learning_rate_Hydroxylation-K': 0.00026985967015906356,
 'learning_rate_Hydroxylation-P': 0.005490972337529224,
 'log_base': 2.552362824861924,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 622657011,
 'sample_weights': [11.26826834187076, 1.4085878051020595],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.4387390792215395,
 'weight_decay_Hydroxylation-K': 7.619145383213469,
 'weight_decay_Hydroxylation-P': 3.214170956336904}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1287.217
[2,     1] loss: 1282.288
[3,     1] loss: 1285.544
[4,     1] loss: 1281.113
[5,     1] loss: 1283.579
[6,     1] loss: 1281.128
[7,     1] loss: 1280.272
[8,     1] loss: 1276.585
[9,     1] loss: 1275.540
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007325578835193375,
 'learning_rate_Hydroxylation-K': 0.009512903165476259,
 'learning_rate_Hydroxylation-P': 0.002377972476200289,
 'log_base': 2.663396765444074,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3394715200,
 'sample_weights': [1.7816524600699057, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.145047390986864,
 'weight_decay_Hydroxylation-K': 9.227312315937258,
 'weight_decay_Hydroxylation-P': 9.72535197389187}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.135
[2,     1] loss: 1282.908
[3,     1] loss: 1275.980
[4,     1] loss: 1268.926
[5,     1] loss: 1265.701
[6,     1] loss: 1268.035
[7,     1] loss: 1267.297
[8,     1] loss: 1266.363
[9,     1] loss: 1266.631
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004651812135638276,
 'learning_rate_Hydroxylation-K': 0.002178912453577817,
 'learning_rate_Hydroxylation-P': 0.0012706584133940421,
 'log_base': 1.0523573427437016,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4276734251,
 'sample_weights': [1.7042050314595512, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.508457844764346,
 'weight_decay_Hydroxylation-K': 1.3031989053207993,
 'weight_decay_Hydroxylation-P': 7.227918499239436}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10651.395
[2,     1] loss: 10662.376
[3,     1] loss: 10632.665
[4,     1] loss: 10633.104
[5,     1] loss: 10636.899
[6,     1] loss: 10601.805
[7,     1] loss: 10626.693
[8,     1] loss: 10606.826
[9,     1] loss: 10683.246
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005211522099644111,
 'learning_rate_Hydroxylation-K': 0.008150799146513213,
 'learning_rate_Hydroxylation-P': 0.005512658375839345,
 'log_base': 1.409960045836594,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3287770922,
 'sample_weights': [32.713181314297245, 4.089305194675684],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.72611934847765,
 'weight_decay_Hydroxylation-K': 7.95132157979326,
 'weight_decay_Hydroxylation-P': 7.9749435879437}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1931.637
[2,     1] loss: 1937.378
[3,     1] loss: 1933.853
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009269679753494207,
 'learning_rate_Hydroxylation-K': 0.0008259853014552353,
 'learning_rate_Hydroxylation-P': 0.004575213683194442,
 'log_base': 1.5550666693080537,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2961395197,
 'sample_weights': [4.859228377706675, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.4072608993030284,
 'weight_decay_Hydroxylation-K': 6.075050274458173,
 'weight_decay_Hydroxylation-P': 2.709993180887384}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1704.254
[2,     1] loss: 1711.361
[3,     1] loss: 1708.005
[4,     1] loss: 1709.865
[5,     1] loss: 1702.614
[6,     1] loss: 1704.974
[7,     1] loss: 1699.976
[8,     1] loss: 1704.029
[9,     1] loss: 1709.573
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034541972022071356,
 'learning_rate_Hydroxylation-K': 0.00044340853694593927,
 'learning_rate_Hydroxylation-P': 0.0042741777057060506,
 'log_base': 2.1511384926330823,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1473937732,
 'sample_weights': [3.7811404373897886, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.391878654625087,
 'weight_decay_Hydroxylation-K': 9.184156455478337,
 'weight_decay_Hydroxylation-P': 1.2712371055944505}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1368.425
[2,     1] loss: 1366.931
[3,     1] loss: 1366.543
[4,     1] loss: 1364.867
[5,     1] loss: 1361.475
[6,     1] loss: 1360.836
[7,     1] loss: 1358.626
[8,     1] loss: 1346.261
[9,     1] loss: 1336.135
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007001145919215707,
 'learning_rate_Hydroxylation-K': 0.006950680048058202,
 'learning_rate_Hydroxylation-P': 0.009820800806884123,
 'log_base': 2.372712647489489,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1040245428,
 'sample_weights': [2.1794375678311195, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.895755238885593,
 'weight_decay_Hydroxylation-K': 8.337160612030027,
 'weight_decay_Hydroxylation-P': 8.084503897051171}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1325.734
[2,     1] loss: 1317.093
[3,     1] loss: 1316.134
[4,     1] loss: 1312.079
[5,     1] loss: 1318.100
[6,     1] loss: 1317.228
[7,     1] loss: 1315.699
[8,     1] loss: 1313.767
[9,     1] loss: 1312.189
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007197994050495632,
 'learning_rate_Hydroxylation-K': 0.009541609779558884,
 'learning_rate_Hydroxylation-P': 0.009375895088253747,
 'log_base': 2.2051999663465396,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3237721851,
 'sample_weights': [1.9321501050701024, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.748498330877029,
 'weight_decay_Hydroxylation-K': 9.621464650255485,
 'weight_decay_Hydroxylation-P': 9.994313817008832}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1357.395
[2,     1] loss: 1354.150
[3,     1] loss: 1354.193
[4,     1] loss: 1352.616
[5,     1] loss: 1354.034
[6,     1] loss: 1351.752
[7,     1] loss: 1347.486
[8,     1] loss: 1345.650
[9,     1] loss: 1336.943
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008954640492415495,
 'learning_rate_Hydroxylation-K': 0.0074903413877718044,
 'learning_rate_Hydroxylation-P': 0.005967009242927331,
 'log_base': 1.1355815061582013,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 511988176,
 'sample_weights': [2.1110328054091427, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.647068431801391,
 'weight_decay_Hydroxylation-K': 5.093038948815075,
 'weight_decay_Hydroxylation-P': 6.385054795130574}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4259.641
[2,     1] loss: 4271.429
[3,     1] loss: 4274.736
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025508686726956717,
 'learning_rate_Hydroxylation-K': 0.0065547155234365,
 'learning_rate_Hydroxylation-P': 0.005936719541079922,
 'log_base': 1.5514017694946323,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3042407790,
 'sample_weights': [13.130244887261577, 1.6413438396276034],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.368585423929846,
 'weight_decay_Hydroxylation-K': 2.5034453958108824,
 'weight_decay_Hydroxylation-P': 6.915932080531001}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1727.467
[2,     1] loss: 1712.090
[3,     1] loss: 1712.863
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0048280342504888225,
 'learning_rate_Hydroxylation-K': 0.0009866824357820136,
 'learning_rate_Hydroxylation-P': 0.008014802103864333,
 'log_base': 2.775357704434652,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1950854806,
 'sample_weights': [3.801455889329552, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.76308393554627,
 'weight_decay_Hydroxylation-K': 8.132020940198169,
 'weight_decay_Hydroxylation-P': 5.085057983587491}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.767
[2,     1] loss: 1251.794
[3,     1] loss: 1252.944
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007307514309422971,
 'learning_rate_Hydroxylation-K': 0.0010477752417747434,
 'learning_rate_Hydroxylation-P': 0.0076553722612144005,
 'log_base': 2.4130798827784985,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2873821616,
 'sample_weights': [1.6354588980145859, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.7004976542273536,
 'weight_decay_Hydroxylation-K': 4.242143350773688,
 'weight_decay_Hydroxylation-P': 5.349185107668594}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1311.002
[2,     1] loss: 1312.621
[3,     1] loss: 1307.586
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007980109965298878,
 'learning_rate_Hydroxylation-K': 0.003422129424482134,
 'learning_rate_Hydroxylation-P': 0.007526354730533645,
 'log_base': 1.308043679933614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3091642239,
 'sample_weights': [1.8951478861375661, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2394095603836117,
 'weight_decay_Hydroxylation-K': 5.918812866697339,
 'weight_decay_Hydroxylation-P': 5.207267824538622}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2233.512
[2,     1] loss: 2226.691
[3,     1] loss: 2224.481
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00443635480317051,
 'learning_rate_Hydroxylation-K': 2.1992760104839505e-05,
 'learning_rate_Hydroxylation-P': 0.007801195449531386,
 'log_base': 1.3304619143460525,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 766109165,
 'sample_weights': [6.216909439350478, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.923231455022933,
 'weight_decay_Hydroxylation-K': 1.6491296607133223,
 'weight_decay_Hydroxylation-P': 3.433879084316658}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2152.103
[2,     1] loss: 2163.263
[3,     1] loss: 2140.181
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009310926945342957,
 'learning_rate_Hydroxylation-K': 0.006847747086079311,
 'learning_rate_Hydroxylation-P': 0.006810478716807127,
 'log_base': 1.083816779276295,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1360919110,
 'sample_weights': [5.846900317836679, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.2094344004922615,
 'weight_decay_Hydroxylation-K': 3.2311833421216827,
 'weight_decay_Hydroxylation-P': 5.596987828500012}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6726.843
[2,     1] loss: 6722.160
[3,     1] loss: 6735.730
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007693733123060645,
 'learning_rate_Hydroxylation-K': 0.007696472918070172,
 'learning_rate_Hydroxylation-P': 0.009092932847312623,
 'log_base': 1.7665853746630633,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 270752685,
 'sample_weights': [20.74129295109459, 2.5927614986235503],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.421135128783758,
 'weight_decay_Hydroxylation-K': 9.522968634218497,
 'weight_decay_Hydroxylation-P': 5.62820625434131}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1527.246
[2,     1] loss: 1524.096
[3,     1] loss: 1521.328
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004668076699993709,
 'learning_rate_Hydroxylation-K': 0.002210185355376661,
 'learning_rate_Hydroxylation-P': 0.009869869103756576,
 'log_base': 1.4982986774606397,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2023778054,
 'sample_weights': [2.93374483729833, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.2490860151652345,
 'weight_decay_Hydroxylation-K': 4.953847046918883,
 'weight_decay_Hydroxylation-P': 5.589974525664549}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1779.373
[2,     1] loss: 1799.932
[3,     1] loss: 1777.300
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007232469722155622,
 'learning_rate_Hydroxylation-K': 0.00828205475872758,
 'learning_rate_Hydroxylation-P': 0.009540014543066128,
 'log_base': 1.0799253849557686,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2588785416,
 'sample_weights': [4.128909845383435, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3817158633113635,
 'weight_decay_Hydroxylation-K': 7.773531645034876,
 'weight_decay_Hydroxylation-P': 4.462646239549877}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7079.054
[2,     1] loss: 7056.310
[3,     1] loss: 7077.470
[4,     1] loss: 7031.098
[5,     1] loss: 6992.636
[6,     1] loss: 7074.750
[7,     1] loss: 7064.293
[8,     1] loss: 7026.199
[9,     1] loss: 7052.223
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003446616748615098,
 'learning_rate_Hydroxylation-K': 0.0026290160559122506,
 'learning_rate_Hydroxylation-P': 0.006644837606429424,
 'log_base': 2.923172323744013,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1533556239,
 'sample_weights': [21.71154628328696, 2.7140478374044137],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.383499645027539,
 'weight_decay_Hydroxylation-K': 5.493274875787152,
 'weight_decay_Hydroxylation-P': 3.1850849173462064}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.571
[2,     1] loss: 1236.349
[3,     1] loss: 1237.808
[4,     1] loss: 1234.623
[5,     1] loss: 1233.972
[6,     1] loss: 1232.988
[7,     1] loss: 1229.488
[8,     1] loss: 1225.012
[9,     1] loss: 1220.579
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006282794298516388,
 'learning_rate_Hydroxylation-K': 0.007176533555460513,
 'learning_rate_Hydroxylation-P': 0.009990853937546255,
 'log_base': 1.1255899119209736,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3089991107,
 'sample_weights': [1.5563444683268428, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.175235398957198,
 'weight_decay_Hydroxylation-K': 7.705829398747843,
 'weight_decay_Hydroxylation-P': 7.159449736000627}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4580.932
[2,     1] loss: 4575.235
[3,     1] loss: 4579.500
[4,     1] loss: 4580.762
[5,     1] loss: 4575.983
[6,     1] loss: 4597.161
[7,     1] loss: 4578.990
[8,     1] loss: 4563.240
[9,     1] loss: 4598.188
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019247738830517871,
 'learning_rate_Hydroxylation-K': 0.009702084172819515,
 'learning_rate_Hydroxylation-P': 0.0013668631162889978,
 'log_base': 2.5109845653632794,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3244043427,
 'sample_weights': [14.111078945340758, 1.7639528200958237],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6052257570131565,
 'weight_decay_Hydroxylation-K': 6.98517658241578,
 'weight_decay_Hydroxylation-P': 1.2165747598393502}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1294.453
[2,     1] loss: 1286.564
[3,     1] loss: 1288.735
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0001786550997839708,
 'learning_rate_Hydroxylation-K': 0.00999050110458839,
 'learning_rate_Hydroxylation-P': 0.007017686941317664,
 'log_base': 2.978661067315753,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3891354118,
 'sample_weights': [1.8132818511231965, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.160274735696746,
 'weight_decay_Hydroxylation-K': 8.466009353384893,
 'weight_decay_Hydroxylation-P': 9.22231960867658}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.755
[2,     1] loss: 1239.741
[3,     1] loss: 1234.173
[4,     1] loss: 1228.238
[5,     1] loss: 1231.067
[6,     1] loss: 1232.174
[7,     1] loss: 1229.084
[8,     1] loss: 1232.386
[9,     1] loss: 1231.576
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002549787848552288,
 'learning_rate_Hydroxylation-K': 0.001922340482043072,
 'learning_rate_Hydroxylation-P': 0.0024614667815611754,
 'log_base': 1.5410025966345242,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1550155269,
 'sample_weights': [1.5295309926361589, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.482682112400667,
 'weight_decay_Hydroxylation-K': 7.422988510357552,
 'weight_decay_Hydroxylation-P': 2.2080279549844213}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1737.191
[2,     1] loss: 1729.208
[3,     1] loss: 1724.025
[4,     1] loss: 1722.331
[5,     1] loss: 1726.994
[6,     1] loss: 1719.298
[7,     1] loss: 1724.006
[8,     1] loss: 1720.337
[9,     1] loss: 1717.913
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002696535931029837,
 'learning_rate_Hydroxylation-K': 0.003978127369572048,
 'learning_rate_Hydroxylation-P': 0.002648502855196111,
 'log_base': 2.153551240021964,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2669640142,
 'sample_weights': [3.8605800565648214, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.892825206734457,
 'weight_decay_Hydroxylation-K': 9.221104896021869,
 'weight_decay_Hydroxylation-P': 9.0831088394952}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1373.061
[2,     1] loss: 1367.845
[3,     1] loss: 1371.604
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0008992396203751289,
 'learning_rate_Hydroxylation-K': 0.0024035868088565122,
 'learning_rate_Hydroxylation-P': 0.006437245593205016,
 'log_base': 2.2880307399299675,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 698942531,
 'sample_weights': [2.1762527677447943, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.357698237668534,
 'weight_decay_Hydroxylation-K': 8.896402515390344,
 'weight_decay_Hydroxylation-P': 8.05273215139842}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1339.905
[2,     1] loss: 1330.061
[3,     1] loss: 1332.676
[4,     1] loss: 1331.626
[5,     1] loss: 1334.650
[6,     1] loss: 1333.954
[7,     1] loss: 1332.103
[8,     1] loss: 1331.840
[9,     1] loss: 1331.718
[10,     1] loss: 1332.741
[11,     1] loss: 1325.799
[12,     1] loss: 1324.877
[13,     1] loss: 1327.730
[14,     1] loss: 1322.446
[15,     1] loss: 1319.313
[16,     1] loss: 1312.711
[17,     1] loss: 1312.625
[18,     1] loss: 1295.006
[19,     1] loss: 1292.024
[20,     1] loss: 1277.678
[21,     1] loss: 1266.885
[22,     1] loss: 1252.552
[23,     1] loss: 1243.719
[24,     1] loss: 1212.713
[25,     1] loss: 1210.169
[26,     1] loss: 1191.226
[27,     1] loss: 1182.138
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007947518973597025,
 'learning_rate_Hydroxylation-K': 0.0061504960435818015,
 'learning_rate_Hydroxylation-P': 0.0024687632591182313,
 'log_base': 2.437713275912446,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1547589518,
 'sample_weights': [2.0169871623097975, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.944937844790587,
 'weight_decay_Hydroxylation-K': 9.770492776579811,
 'weight_decay_Hydroxylation-P': 1.7730451838155181}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1304.238
[2,     1] loss: 1305.548
[3,     1] loss: 1305.435
[4,     1] loss: 1313.515
[5,     1] loss: 1302.672
[6,     1] loss: 1306.430
[7,     1] loss: 1302.573
[8,     1] loss: 1300.347
[9,     1] loss: 1298.731
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006732848163844189,
 'learning_rate_Hydroxylation-K': 0.008079483931729676,
 'learning_rate_Hydroxylation-P': 0.0026297605667472647,
 'log_base': 1.0175764124312612,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3391652741,
 'sample_weights': [1.8735465227036459, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.770034475173967,
 'weight_decay_Hydroxylation-K': 9.098991094759121,
 'weight_decay_Hydroxylation-P': 6.11692591324829}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31169.387
Exploding loss, terminate run (best metric=0.5324200987815857)
Finished Training
Total time taken: 0.21000027656555176
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31150.279
Exploding loss, terminate run (best metric=0.528178870677948)
Finished Training
Total time taken: 0.22299885749816895
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31167.424
Exploding loss, terminate run (best metric=0.5333492159843445)
Finished Training
Total time taken: 0.21899867057800293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31131.355
Exploding loss, terminate run (best metric=0.5311292409896851)
Finished Training
Total time taken: 0.21300005912780762
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31247.941
Exploding loss, terminate run (best metric=0.5336458086967468)
Finished Training
Total time taken: 0.22099781036376953
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31149.312
Exploding loss, terminate run (best metric=0.5320718288421631)
Finished Training
Total time taken: 0.2140026092529297
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31107.838
Exploding loss, terminate run (best metric=0.5353353023529053)
Finished Training
Total time taken: 0.21499896049499512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30886.670
Exploding loss, terminate run (best metric=0.5266550779342651)
Finished Training
Total time taken: 0.21400094032287598
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31040.770
Exploding loss, terminate run (best metric=0.5284295082092285)
Finished Training
Total time taken: 0.2180004119873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31161.547
Exploding loss, terminate run (best metric=0.5307072401046753)
Finished Training
Total time taken: 0.21500229835510254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31126.316
Exploding loss, terminate run (best metric=0.5313361883163452)
Finished Training
Total time taken: 0.22499966621398926
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31110.430
Exploding loss, terminate run (best metric=0.5315149426460266)
Finished Training
Total time taken: 0.2279961109161377
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31176.133
Exploding loss, terminate run (best metric=0.5319700241088867)
Finished Training
Total time taken: 0.21599912643432617
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31070.605
Exploding loss, terminate run (best metric=0.528623640537262)
Finished Training
Total time taken: 0.24100232124328613
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31304.334
Exploding loss, terminate run (best metric=0.5339505672454834)
Finished Training
Total time taken: 0.22100019454956055
{'Hydroxylation-K Validation Accuracy': 0.4394208037825059, 'Hydroxylation-K Validation Sensitivity': 0.6, 'Hydroxylation-K Validation Specificity': 0.4, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6391423001949318, 'Hydroxylation-K AUC PR': 0.3698327963920851, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.20065681444991793, 'Validation Loss (Hydroxylation-K)': 0.5587175925572713, 'Hydroxylation-P Validation Accuracy': 0.43594078134781655, 'Hydroxylation-P Validation Sensitivity': 0.6, 'Hydroxylation-P Validation Specificity': 0.4, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6182465083561148, 'Hydroxylation-P AUC PR': 0.2880234993552643, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.18076066117093254, 'Validation Loss (Hydroxylation-P)': 0.5312878370285035, 'Validation Loss (total)': 1.0900054454803467, 'TimeToTrain': 0.21953322092692057}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008037113702273151,
 'learning_rate_Hydroxylation-K': 0.006409216901660058,
 'learning_rate_Hydroxylation-P': 0.008702843882606318,
 'log_base': 2.814056937855682,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 614593577,
 'sample_weights': [95.88539194932439, 11.960769880739221],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5650454967740095,
 'weight_decay_Hydroxylation-K': 0.7628351102020003,
 'weight_decay_Hydroxylation-P': 1.2852316982263767}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.910
[2,     1] loss: 1250.794
[3,     1] loss: 1251.961
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026092183874883348,
 'learning_rate_Hydroxylation-K': 0.009966065651555382,
 'learning_rate_Hydroxylation-P': 0.003181642619014356,
 'log_base': 1.4401181912671925,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2297812715,
 'sample_weights': [1.6135697572046386, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.005293872155249,
 'weight_decay_Hydroxylation-K': 6.7504372705964855,
 'weight_decay_Hydroxylation-P': 1.5213133299531645}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1881.002
[2,     1] loss: 1881.269
[3,     1] loss: 1872.063
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004496801129101744,
 'learning_rate_Hydroxylation-K': 0.00632435440357972,
 'learning_rate_Hydroxylation-P': 0.0028237028229901933,
 'log_base': 1.087948311295394,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3642397224,
 'sample_weights': [4.577263113959406, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.926925791507994,
 'weight_decay_Hydroxylation-K': 7.866108745892288,
 'weight_decay_Hydroxylation-P': 6.332922529311549}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6397.308
[2,     1] loss: 6390.023
[3,     1] loss: 6417.686
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000903819178370946,
 'learning_rate_Hydroxylation-K': 0.008656604221443046,
 'learning_rate_Hydroxylation-P': 0.003208514839295417,
 'log_base': 1.069901495768001,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1009202511,
 'sample_weights': [19.80509041008408, 2.4757316727120715],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.417693205416732,
 'weight_decay_Hydroxylation-K': 8.205897255105791,
 'weight_decay_Hydroxylation-P': 3.0722974307447206}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8004.107
[2,     1] loss: 8018.364
[3,     1] loss: 8023.384
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006838264960367061,
 'learning_rate_Hydroxylation-K': 0.008679744397189467,
 'learning_rate_Hydroxylation-P': 0.008104066213600675,
 'log_base': 1.0398431912719717,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1127712212,
 'sample_weights': [24.70811817762935, 3.0886337541950235],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.86400571023505,
 'weight_decay_Hydroxylation-K': 7.440447299720612,
 'weight_decay_Hydroxylation-P': 5.032198605066464}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13944.457
[2,     1] loss: 13822.530
[3,     1] loss: 13909.196
[4,     1] loss: 13887.148
[5,     1] loss: 13860.952
[6,     1] loss: 13795.939
[7,     1] loss: 13872.719
[8,     1] loss: 13837.395
[9,     1] loss: 13832.369
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034153285741684767,
 'learning_rate_Hydroxylation-K': 0.00040422223421340874,
 'learning_rate_Hydroxylation-P': 0.003457852592511127,
 'log_base': 2.967266936990229,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 356289622,
 'sample_weights': [42.729623443794964, 5.341408695059542],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.6563681901269565,
 'weight_decay_Hydroxylation-K': 9.357760396413049,
 'weight_decay_Hydroxylation-P': 0.9257578582782786}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.303
[2,     1] loss: 1237.640
[3,     1] loss: 1229.303
[4,     1] loss: 1226.991
[5,     1] loss: 1225.658
[6,     1] loss: 1219.665
[7,     1] loss: 1219.518
[8,     1] loss: 1198.239
[9,     1] loss: 1178.941
[10,     1] loss: 1150.736
[11,     1] loss: 1110.934
[12,     1] loss: 1093.445
[13,     1] loss: 1056.846
[14,     1] loss: 1034.360
[15,     1] loss: 985.845
[16,     1] loss: 972.850
[17,     1] loss: 985.569
[18,     1] loss: 966.306
[19,     1] loss: 971.316
[20,     1] loss: 1026.442
[21,     1] loss: 923.368
[22,     1] loss: 948.567
[23,     1] loss: 960.080
[24,     1] loss: 928.586
[25,     1] loss: 942.484
[26,     1] loss: 900.755
[27,     1] loss: 916.959
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008469525178452345,
 'learning_rate_Hydroxylation-K': 0.007748898173079729,
 'learning_rate_Hydroxylation-P': 0.001495989837898496,
 'log_base': 1.019221229469744,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4207070325,
 'sample_weights': [1.5349206933817885, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.063468450304725,
 'weight_decay_Hydroxylation-K': 8.062894174154655,
 'weight_decay_Hydroxylation-P': 5.8095864458805915}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28496.594
Exploding loss, terminate run (best metric=0.5330502390861511)
Finished Training
Total time taken: 0.20300054550170898
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28575.605
Exploding loss, terminate run (best metric=0.5423357486724854)
Finished Training
Total time taken: 0.20800232887268066
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28449.025
Exploding loss, terminate run (best metric=0.526613712310791)
Finished Training
Total time taken: 0.2290036678314209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28590.871
Exploding loss, terminate run (best metric=0.5279027223587036)
Finished Training
Total time taken: 0.2140030860900879
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28515.531
Exploding loss, terminate run (best metric=0.5255013108253479)
Finished Training
Total time taken: 0.2135179042816162
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28455.809
Exploding loss, terminate run (best metric=0.5382186770439148)
Finished Training
Total time taken: 0.2129983901977539
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28342.699
Exploding loss, terminate run (best metric=0.5277586579322815)
Finished Training
Total time taken: 0.21700000762939453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28531.547
Exploding loss, terminate run (best metric=0.5273722410202026)
Finished Training
Total time taken: 0.20899677276611328
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28388.045
Exploding loss, terminate run (best metric=0.5265814065933228)
Finished Training
Total time taken: 0.21600031852722168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28453.816
Exploding loss, terminate run (best metric=0.5379197001457214)
Finished Training
Total time taken: 0.22499990463256836
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28554.316
Exploding loss, terminate run (best metric=0.5409806966781616)
Finished Training
Total time taken: 0.2069988250732422
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28455.928
Exploding loss, terminate run (best metric=0.5282745361328125)
Finished Training
Total time taken: 0.2200028896331787
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28483.154
Exploding loss, terminate run (best metric=0.5285624265670776)
Finished Training
Total time taken: 0.20600295066833496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 28485.229
Exploding loss, terminate run (best metric=0.5275520086288452)
Finished Training
Total time taken: 0.21900010108947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 28441.637
Exploding loss, terminate run (best metric=0.5340939164161682)
Finished Training
Total time taken: 0.21000003814697266
{'Hydroxylation-K Validation Accuracy': 0.40469858156028365, 'Hydroxylation-K Validation Sensitivity': 0.66, 'Hydroxylation-K Validation Specificity': 0.34035087719298246, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.62588693957115, 'Hydroxylation-K AUC PR': 0.3493358603068625, 'Hydroxylation-K MCC': 0.00046647428569421306, 'Hydroxylation-K F1': 0.22329832388388018, 'Validation Loss (Hydroxylation-K)': 0.5571657458941142, 'Hydroxylation-P Validation Accuracy': 0.4002925401417864, 'Hydroxylation-P Validation Sensitivity': 0.6537037037037037, 'Hydroxylation-P Validation Specificity': 0.3467479674796748, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6028982455286136, 'Hydroxylation-P AUC PR': 0.3294182550693338, 'Hydroxylation-P MCC': 0.000433816005246128, 'Hydroxylation-P F1': 0.19930066300391258, 'Validation Loss (Hydroxylation-P)': 0.5315145333607991, 'Validation Loss (total)': 1.0886802593866984, 'TimeToTrain': 0.21396851539611816}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009122761059320306,
 'learning_rate_Hydroxylation-K': 0.008231177328890556,
 'learning_rate_Hydroxylation-P': 0.003018327686492592,
 'log_base': 1.2615913309377902,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2162280601,
 'sample_weights': [87.75124756927104, 10.946114497592387],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.518723234212208,
 'weight_decay_Hydroxylation-K': 8.103860390686174,
 'weight_decay_Hydroxylation-P': 5.552084016458909}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2457.801
[2,     1] loss: 2430.158
[3,     1] loss: 2432.038
[4,     1] loss: 2421.741
[5,     1] loss: 2428.232
[6,     1] loss: 2425.638
[7,     1] loss: 2429.084
[8,     1] loss: 2420.694
[9,     1] loss: 2422.329
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0026487264652699195,
 'learning_rate_Hydroxylation-K': 0.006744963603556035,
 'learning_rate_Hydroxylation-P': 0.008838947596704397,
 'log_base': 2.019705158173357,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2717410573,
 'sample_weights': [7.184297608439772, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8747418077152016,
 'weight_decay_Hydroxylation-K': 7.1145558064774175,
 'weight_decay_Hydroxylation-P': 9.393301605003868}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1418.264
[2,     1] loss: 1411.912
[3,     1] loss: 1411.431
[4,     1] loss: 1411.998
[5,     1] loss: 1406.181
[6,     1] loss: 1410.099
[7,     1] loss: 1406.280
[8,     1] loss: 1403.375
[9,     1] loss: 1403.112
[10,     1] loss: 1395.830
[11,     1] loss: 1389.293
[12,     1] loss: 1373.323
[13,     1] loss: 1355.725
[14,     1] loss: 1341.955
[15,     1] loss: 1303.032
[16,     1] loss: 1277.348
[17,     1] loss: 1249.389
[18,     1] loss: 1228.455
[19,     1] loss: 1196.675
[20,     1] loss: 1182.147
[21,     1] loss: 1206.953
[22,     1] loss: 1176.062
[23,     1] loss: 1203.070
[24,     1] loss: 1188.031
[25,     1] loss: 1151.910
[26,     1] loss: 1151.798
[27,     1] loss: 1132.790
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00038760088005786804,
 'learning_rate_Hydroxylation-K': 0.004798232067974052,
 'learning_rate_Hydroxylation-P': 0.003520433565962039,
 'log_base': 1.3765324063465614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 334005434,
 'sample_weights': [2.374905030733025, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.383054146010863,
 'weight_decay_Hydroxylation-K': 3.087550476471222,
 'weight_decay_Hydroxylation-P': 1.4401346871584813}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2008.484
[2,     1] loss: 2009.615
[3,     1] loss: 2005.607
[4,     1] loss: 2014.525
[5,     1] loss: 2016.289
[6,     1] loss: 2002.539
[7,     1] loss: 2012.475
[8,     1] loss: 2002.205
[9,     1] loss: 2005.121
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0031365142525339837,
 'learning_rate_Hydroxylation-K': 0.009338790095577334,
 'learning_rate_Hydroxylation-P': 0.0033363623462330087,
 'log_base': 2.258061719315859,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2224821119,
 'sample_weights': [5.224069056666983, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.252455759689431,
 'weight_decay_Hydroxylation-K': 8.406760557211909,
 'weight_decay_Hydroxylation-P': 9.544291360660834}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1341.946
[2,     1] loss: 1337.936
[3,     1] loss: 1338.874
[4,     1] loss: 1340.162
[5,     1] loss: 1338.332
[6,     1] loss: 1324.046
[7,     1] loss: 1332.567
[8,     1] loss: 1301.634
[9,     1] loss: 1292.633
[10,     1] loss: 1255.609
[11,     1] loss: 1237.493
[12,     1] loss: 1180.454
[13,     1] loss: 1146.030
[14,     1] loss: 1131.503
[15,     1] loss: 1126.534
[16,     1] loss: 1100.732
[17,     1] loss: 1078.999
[18,     1] loss: 1137.015
[19,     1] loss: 1055.284
[20,     1] loss: 1081.944
[21,     1] loss: 1086.901
[22,     1] loss: 1064.226
[23,     1] loss: 1087.742
[24,     1] loss: 1027.300
[25,     1] loss: 1062.208
[26,     1] loss: 1025.624
[27,     1] loss: 1019.150
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007299506169105772,
 'learning_rate_Hydroxylation-K': 0.008853404044533495,
 'learning_rate_Hydroxylation-P': 0.00030534704846209255,
 'log_base': 1.0128444615426833,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4185848122,
 'sample_weights': [2.0496368471387263, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.197383529945627,
 'weight_decay_Hydroxylation-K': 7.961867148911755,
 'weight_decay_Hydroxylation-P': 6.935048598260145}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42582.160
Exploding loss, terminate run (best metric=0.5425244569778442)
Finished Training
Total time taken: 0.19890165328979492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42679.387
Exploding loss, terminate run (best metric=0.5265201926231384)
Finished Training
Total time taken: 0.20288634300231934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42655.219
Exploding loss, terminate run (best metric=0.5301066040992737)
Finished Training
Total time taken: 0.21199989318847656
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42357.629
Exploding loss, terminate run (best metric=0.5285928845405579)
Finished Training
Total time taken: 0.21402192115783691
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 43010.273
Exploding loss, terminate run (best metric=0.5283589363098145)
Finished Training
Total time taken: 0.20499968528747559
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42411.395
Exploding loss, terminate run (best metric=0.5314079523086548)
Finished Training
Total time taken: 0.2135331630706787
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42473.359
Exploding loss, terminate run (best metric=0.5273427963256836)
Finished Training
Total time taken: 0.21800518035888672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42491.402
Exploding loss, terminate run (best metric=0.5273529291152954)
Finished Training
Total time taken: 0.21400141716003418
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42473.836
Exploding loss, terminate run (best metric=0.5268164277076721)
Finished Training
Total time taken: 0.20900368690490723
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 42685.961
Exploding loss, terminate run (best metric=0.5417025685310364)
Finished Training
Total time taken: 0.21100068092346191
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42499.738
Exploding loss, terminate run (best metric=0.5420004725456238)
Finished Training
Total time taken: 0.2160022258758545
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42547.891
Exploding loss, terminate run (best metric=0.5291785001754761)
Finished Training
Total time taken: 0.21899724006652832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42481.520
Exploding loss, terminate run (best metric=0.5282812118530273)
Finished Training
Total time taken: 0.209000825881958
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 42505.680
Exploding loss, terminate run (best metric=0.5440992712974548)
Finished Training
Total time taken: 0.21199774742126465
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 42714.125
Exploding loss, terminate run (best metric=0.5280720591545105)
Finished Training
Total time taken: 0.2109999656677246
{'Hydroxylation-K Validation Accuracy': 0.413031914893617, 'Hydroxylation-K Validation Sensitivity': 0.6466666666666666, 'Hydroxylation-K Validation Specificity': 0.3543859649122807, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5974269005847953, 'Hydroxylation-K AUC PR': 0.3244489064332428, 'Hydroxylation-K MCC': 0.00488122319537301, 'Hydroxylation-K F1': 0.22220058801767947, 'Validation Loss (Hydroxylation-K)': 0.5584401686986288, 'Hydroxylation-P Validation Accuracy': 0.43295034093024043, 'Hydroxylation-P Validation Sensitivity': 0.6247619047619047, 'Hydroxylation-P Validation Specificity': 0.39227642276422764, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5976137471132235, 'Hydroxylation-P AUC PR': 0.28367348729163444, 'Hydroxylation-P MCC': 0.013173330277222022, 'Hydroxylation-P F1': 0.2026940496893129, 'Validation Loss (Hydroxylation-P)': 0.5321571509043376, 'Validation Loss (total)': 1.0905973275502523, 'TimeToTrain': 0.21102344195048015}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007336467285025432,
 'learning_rate_Hydroxylation-K': 0.009022719963773726,
 'learning_rate_Hydroxylation-P': 0.000753605299305835,
 'log_base': 1.1000393414894267,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 715378287,
 'sample_weights': [130.90375269357446, 16.328969727948987],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.367390273102028,
 'weight_decay_Hydroxylation-K': 6.245181773369837,
 'weight_decay_Hydroxylation-P': 7.729120634406225}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5686.295
[2,     1] loss: 5692.091
[3,     1] loss: 5677.293
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005625214060110191,
 'learning_rate_Hydroxylation-K': 0.007713609854409914,
 'learning_rate_Hydroxylation-P': 0.00044035423481256176,
 'log_base': 1.0226416448668523,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1266602637,
 'sample_weights': [17.509325249398252, 2.188749972364727],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.984423477686233,
 'weight_decay_Hydroxylation-K': 8.354245490988282,
 'weight_decay_Hydroxylation-P': 6.4813992126292685}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24352.004
Exploding loss, terminate run (best metric=0.5349849462509155)
Finished Training
Total time taken: 0.2129971981048584
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24262.793
Exploding loss, terminate run (best metric=0.5322521328926086)
Finished Training
Total time taken: 0.2050004005432129
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24137.789
Exploding loss, terminate run (best metric=0.5297967791557312)
Finished Training
Total time taken: 0.21400070190429688
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24242.539
Exploding loss, terminate run (best metric=0.5330758094787598)
Finished Training
Total time taken: 0.22299885749816895
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24291.988
Exploding loss, terminate run (best metric=0.5279608964920044)
Finished Training
Total time taken: 0.23000049591064453
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24288.527
Exploding loss, terminate run (best metric=0.5321711897850037)
Finished Training
Total time taken: 0.2100050449371338
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24406.754
Exploding loss, terminate run (best metric=0.5268100500106812)
Finished Training
Total time taken: 0.2200000286102295
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24127.250
Exploding loss, terminate run (best metric=0.5350666642189026)
Finished Training
Total time taken: 0.2180004119873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24206.199
Exploding loss, terminate run (best metric=0.5282991528511047)
Finished Training
Total time taken: 0.1999969482421875
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24328.771
Exploding loss, terminate run (best metric=0.5311642289161682)
Finished Training
Total time taken: 0.20799946784973145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24213.795
Exploding loss, terminate run (best metric=0.5387791991233826)
Finished Training
Total time taken: 0.21103239059448242
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24286.191
Exploding loss, terminate run (best metric=0.5391809344291687)
Finished Training
Total time taken: 0.21999859809875488
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24187.816
Exploding loss, terminate run (best metric=0.526404857635498)
Finished Training
Total time taken: 0.2160031795501709
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 24250.227
Exploding loss, terminate run (best metric=0.540255069732666)
Finished Training
Total time taken: 0.21200060844421387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 24315.719
Exploding loss, terminate run (best metric=0.5346246361732483)
Finished Training
Total time taken: 0.21000146865844727
{'Hydroxylation-K Validation Accuracy': 0.5605791962174941, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.6, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6589863547758285, 'Hydroxylation-K AUC PR': 0.3448943580593513, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.13481116584564862, 'Validation Loss (Hydroxylation-K)': 0.5583399891853332, 'Hydroxylation-P Validation Accuracy': 0.5644901950831599, 'Hydroxylation-P Validation Sensitivity': 0.4, 'Hydroxylation-P Validation Specificity': 0.6, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5577309737967376, 'Hydroxylation-P AUC PR': 0.23051724297376563, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.12013967512621766, 'Validation Loss (Hydroxylation-P)': 0.5327217698097229, 'Validation Loss (total)': 1.0910617431004843, 'TimeToTrain': 0.21400238672892252}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0048018383871056315,
 'learning_rate_Hydroxylation-K': 0.0019315901388432847,
 'learning_rate_Hydroxylation-P': 0.0026406185974176175,
 'log_base': 2.766730287960986,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 703800201,
 'sample_weights': [74.62021691202597, 9.308146160650663],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.27832934439997,
 'weight_decay_Hydroxylation-K': 9.074643199226776,
 'weight_decay_Hydroxylation-P': 1.6379309824646815}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1254.641
[2,     1] loss: 1256.021
[3,     1] loss: 1252.724
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061840408624794655,
 'learning_rate_Hydroxylation-K': 0.005666038067393882,
 'learning_rate_Hydroxylation-P': 0.0003273391437663599,
 'log_base': 1.1661111517855838,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1260536966,
 'sample_weights': [1.6404623757016057, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.357289099712565,
 'weight_decay_Hydroxylation-K': 9.7629625184932,
 'weight_decay_Hydroxylation-P': 8.077402159713545}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3535.023
[2,     1] loss: 3469.021
[3,     1] loss: 3653.314
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009396627391197213,
 'learning_rate_Hydroxylation-K': 0.007141057688608531,
 'learning_rate_Hydroxylation-P': 0.0004694372641991422,
 'log_base': 1.3056866872910424,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3960880545,
 'sample_weights': [10.863507714460361, 1.3579907775501676],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.260753399200949,
 'weight_decay_Hydroxylation-K': 4.421422173447612,
 'weight_decay_Hydroxylation-P': 5.130943788442467}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2254.983
[2,     1] loss: 2246.836
[3,     1] loss: 2222.662
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007022102265135597,
 'learning_rate_Hydroxylation-K': 0.002347154481480926,
 'learning_rate_Hydroxylation-P': 0.0072623731670720375,
 'log_base': 2.7551278446248704,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2095110978,
 'sample_weights': [6.258946437815603, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.478609802910087,
 'weight_decay_Hydroxylation-K': 2.927258525098053,
 'weight_decay_Hydroxylation-P': 7.24092373496317}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.817
[2,     1] loss: 1261.440
[3,     1] loss: 1256.050
[4,     1] loss: 1252.610
[5,     1] loss: 1256.013
[6,     1] loss: 1251.364
[7,     1] loss: 1256.899
[8,     1] loss: 1250.897
[9,     1] loss: 1247.337
[10,     1] loss: 1238.655
[11,     1] loss: 1228.217
[12,     1] loss: 1207.209
[13,     1] loss: 1183.835
[14,     1] loss: 1134.554
[15,     1] loss: 1108.376
[16,     1] loss: 1082.411
[17,     1] loss: 1052.127
[18,     1] loss: 1024.905
[19,     1] loss: 974.966
[20,     1] loss: 994.765
[21,     1] loss: 1003.232
[22,     1] loss: 1005.470
[23,     1] loss: 1026.068
[24,     1] loss: 996.862
[25,     1] loss: 985.136
[26,     1] loss: 926.687
[27,     1] loss: 961.660
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007928018110586466,
 'learning_rate_Hydroxylation-K': 0.009336630384656144,
 'learning_rate_Hydroxylation-P': 0.0019253068123430176,
 'log_base': 1.1352820601178815,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2950183692,
 'sample_weights': [1.647264629942448, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.392933840159283,
 'weight_decay_Hydroxylation-K': 9.05099638489511,
 'weight_decay_Hydroxylation-P': 7.408248141634274}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4290.854
[2,     1] loss: 4290.034
[3,     1] loss: 4278.933
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008779828466218002,
 'learning_rate_Hydroxylation-K': 0.009158060342175235,
 'learning_rate_Hydroxylation-P': 0.00633852974680741,
 'log_base': 2.7749221132888593,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3898273353,
 'sample_weights': [13.157536758551894, 1.6447554549629524],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.484041221294103,
 'weight_decay_Hydroxylation-K': 9.999744124540058,
 'weight_decay_Hydroxylation-P': 8.598405537389414}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.361
[2,     1] loss: 1253.880
[3,     1] loss: 1252.122
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032495082075847557,
 'learning_rate_Hydroxylation-K': 0.002384803525161068,
 'learning_rate_Hydroxylation-P': 0.008094669183689857,
 'log_base': 2.9463501544101254,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2656549667,
 'sample_weights': [1.6357104157305555, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.005329023366746,
 'weight_decay_Hydroxylation-K': 0.46433597931277626,
 'weight_decay_Hydroxylation-P': 1.5515200863923218}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.266
[2,     1] loss: 1234.625
[3,     1] loss: 1235.938
[4,     1] loss: 1230.261
[5,     1] loss: 1229.054
[6,     1] loss: 1225.981
[7,     1] loss: 1217.225
[8,     1] loss: 1202.279
[9,     1] loss: 1191.473
[10,     1] loss: 1163.193
[11,     1] loss: 1137.534
[12,     1] loss: 1096.360
[13,     1] loss: 1083.299
[14,     1] loss: 1044.277
[15,     1] loss: 1013.489
[16,     1] loss: 1012.198
[17,     1] loss: 1023.985
[18,     1] loss: 995.637
[19,     1] loss: 1020.418
[20,     1] loss: 962.211
[21,     1] loss: 951.211
[22,     1] loss: 962.227
[23,     1] loss: 931.354
[24,     1] loss: 914.472
[25,     1] loss: 956.617
[26,     1] loss: 906.604
[27,     1] loss: 930.680
[28,     1] loss: 860.141
[29,     1] loss: 861.574
[30,     1] loss: 848.431
[31,     1] loss: 841.922
[32,     1] loss: 781.563
[33,     1] loss: 798.131
[34,     1] loss: 822.912
[35,     1] loss: 813.266
[36,     1] loss: 806.831
[37,     1] loss: 876.124
[38,     1] loss: 904.872
[39,     1] loss: 794.524
[40,     1] loss: 912.185
[41,     1] loss: 790.837
[42,     1] loss: 860.291
[43,     1] loss: 803.015
[44,     1] loss: 803.715
[45,     1] loss: 741.388
[46,     1] loss: 724.557
[47,     1] loss: 719.119
[48,     1] loss: 752.251
[49,     1] loss: 702.350
[50,     1] loss: 744.827
[51,     1] loss: 646.702
[52,     1] loss: 708.046
[53,     1] loss: 656.056
[54,     1] loss: 699.013
[55,     1] loss: 702.399
[56,     1] loss: 971.606
[57,     1] loss: 747.389
[58,     1] loss: 721.884
[59,     1] loss: 748.762
[60,     1] loss: 737.414
[61,     1] loss: 680.015
[62,     1] loss: 766.801
[63,     1] loss: 651.124
[64,     1] loss: 722.016
[65,     1] loss: 679.545
[66,     1] loss: 683.350
[67,     1] loss: 662.313
[68,     1] loss: 621.459
[69,     1] loss: 698.946
[70,     1] loss: 642.950
Early stopping applied (best metric=0.3916264772415161)
Finished Training
Total time taken: 10.526713609695435
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.626
[2,     1] loss: 1232.136
[3,     1] loss: 1233.743
[4,     1] loss: 1229.431
[5,     1] loss: 1229.466
[6,     1] loss: 1221.966
[7,     1] loss: 1213.070
[8,     1] loss: 1190.166
[9,     1] loss: 1157.841
[10,     1] loss: 1136.063
[11,     1] loss: 1099.246
[12,     1] loss: 1095.114
[13,     1] loss: 1061.707
[14,     1] loss: 1027.586
[15,     1] loss: 1078.506
[16,     1] loss: 1024.856
[17,     1] loss: 969.050
[18,     1] loss: 982.555
[19,     1] loss: 1032.836
[20,     1] loss: 1019.968
[21,     1] loss: 956.529
[22,     1] loss: 985.607
[23,     1] loss: 970.991
[24,     1] loss: 968.433
[25,     1] loss: 933.453
[26,     1] loss: 1001.885
[27,     1] loss: 926.967
[28,     1] loss: 957.043
[29,     1] loss: 915.609
[30,     1] loss: 942.139
[31,     1] loss: 937.719
[32,     1] loss: 906.104
[33,     1] loss: 897.834
[34,     1] loss: 911.683
[35,     1] loss: 827.293
[36,     1] loss: 928.186
[37,     1] loss: 909.645
[38,     1] loss: 850.491
[39,     1] loss: 849.307
[40,     1] loss: 875.865
[41,     1] loss: 804.307
[42,     1] loss: 827.345
[43,     1] loss: 795.173
[44,     1] loss: 750.702
[45,     1] loss: 808.793
[46,     1] loss: 998.170
[47,     1] loss: 1108.514
[48,     1] loss: 771.907
[49,     1] loss: 954.069
[50,     1] loss: 921.371
[51,     1] loss: 834.453
[52,     1] loss: 903.840
[53,     1] loss: 915.344
[54,     1] loss: 870.716
[55,     1] loss: 849.538
[56,     1] loss: 904.432
[57,     1] loss: 838.819
[58,     1] loss: 816.834
[59,     1] loss: 812.041
[60,     1] loss: 815.550
[61,     1] loss: 783.412
[62,     1] loss: 807.604
[63,     1] loss: 811.984
[64,     1] loss: 745.301
[65,     1] loss: 781.692
[66,     1] loss: 756.058
[67,     1] loss: 716.040
[68,     1] loss: 726.462
[69,     1] loss: 723.801
[70,     1] loss: 712.985
[71,     1] loss: 709.454
[72,     1] loss: 691.888
[73,     1] loss: 754.547
[74,     1] loss: 873.232
[75,     1] loss: 742.954
[76,     1] loss: 672.228
[77,     1] loss: 718.892
[78,     1] loss: 662.174
[79,     1] loss: 733.910
[80,     1] loss: 653.548
[81,     1] loss: 652.285
[82,     1] loss: 630.186
[83,     1] loss: 653.923
[84,     1] loss: 614.095
[85,     1] loss: 792.242
[86,     1] loss: 861.564
[87,     1] loss: 667.286
[88,     1] loss: 760.775
[89,     1] loss: 678.747
[90,     1] loss: 677.904
[91,     1] loss: 633.243
[92,     1] loss: 698.913
[93,     1] loss: 690.965
[94,     1] loss: 616.549
Early stopping applied (best metric=0.3638964593410492)
Finished Training
Total time taken: 14.012675046920776
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.809
[2,     1] loss: 1234.446
[3,     1] loss: 1232.832
[4,     1] loss: 1231.560
[5,     1] loss: 1228.697
[6,     1] loss: 1221.699
[7,     1] loss: 1213.826
[8,     1] loss: 1189.188
[9,     1] loss: 1146.667
[10,     1] loss: 1117.813
[11,     1] loss: 1068.650
[12,     1] loss: 1058.872
[13,     1] loss: 1036.354
[14,     1] loss: 1025.003
[15,     1] loss: 1019.800
[16,     1] loss: 988.930
[17,     1] loss: 978.703
[18,     1] loss: 985.437
[19,     1] loss: 1024.365
[20,     1] loss: 998.634
[21,     1] loss: 959.070
[22,     1] loss: 971.130
[23,     1] loss: 949.193
[24,     1] loss: 968.127
[25,     1] loss: 877.610
[26,     1] loss: 957.551
[27,     1] loss: 920.686
[28,     1] loss: 922.150
[29,     1] loss: 881.604
[30,     1] loss: 911.418
[31,     1] loss: 878.771
[32,     1] loss: 833.073
[33,     1] loss: 925.246
[34,     1] loss: 856.874
[35,     1] loss: 850.856
[36,     1] loss: 852.040
[37,     1] loss: 865.157
[38,     1] loss: 847.849
[39,     1] loss: 812.927
[40,     1] loss: 816.546
[41,     1] loss: 818.189
[42,     1] loss: 776.021
[43,     1] loss: 761.636
[44,     1] loss: 776.724
[45,     1] loss: 889.690
[46,     1] loss: 899.153
[47,     1] loss: 746.926
[48,     1] loss: 863.216
[49,     1] loss: 761.157
[50,     1] loss: 826.360
[51,     1] loss: 761.549
[52,     1] loss: 753.191
[53,     1] loss: 730.508
[54,     1] loss: 752.429
[55,     1] loss: 725.912
[56,     1] loss: 782.445
[57,     1] loss: 706.665
[58,     1] loss: 691.498
[59,     1] loss: 686.213
[60,     1] loss: 827.162
[61,     1] loss: 700.183
[62,     1] loss: 704.273
[63,     1] loss: 752.118
[64,     1] loss: 658.324
[65,     1] loss: 727.655
[66,     1] loss: 693.635
[67,     1] loss: 640.728
[68,     1] loss: 703.061
[69,     1] loss: 688.611
[70,     1] loss: 633.671
[71,     1] loss: 637.949
[72,     1] loss: 607.423
[73,     1] loss: 574.102
[74,     1] loss: 570.816
[75,     1] loss: 535.858
[76,     1] loss: 651.876
[77,     1] loss: 947.793
[78,     1] loss: 1324.602
[79,     1] loss: 629.327
[80,     1] loss: 861.438
[81,     1] loss: 851.138
[82,     1] loss: 797.897
[83,     1] loss: 845.102
[84,     1] loss: 870.859
[85,     1] loss: 868.471
[86,     1] loss: 841.464
[87,     1] loss: 796.524
[88,     1] loss: 825.318
[89,     1] loss: 851.933
[90,     1] loss: 762.260
[91,     1] loss: 749.460
[92,     1] loss: 780.835
[93,     1] loss: 774.969
[94,     1] loss: 727.931
[95,     1] loss: 764.429
[96,     1] loss: 738.934
[97,     1] loss: 724.685
[98,     1] loss: 692.741
[99,     1] loss: 659.824
[100,     1] loss: 612.117
[101,     1] loss: 627.511
[102,     1] loss: 621.321
[103,     1] loss: 607.783
[104,     1] loss: 627.090
[105,     1] loss: 540.990
[106,     1] loss: 546.408
[107,     1] loss: 567.593
[108,     1] loss: 537.019
[109,     1] loss: 600.954
[110,     1] loss: 537.690
[111,     1] loss: 508.274
[112,     1] loss: 534.026
[113,     1] loss: 456.512
[114,     1] loss: 554.000
[115,     1] loss: 1020.373
[116,     1] loss: 1512.067
[117,     1] loss: 600.094
[118,     1] loss: 886.676
[119,     1] loss: 953.401
[120,     1] loss: 840.732
[121,     1] loss: 883.379
[122,     1] loss: 911.604
[123,     1] loss: 832.152
[124,     1] loss: 794.475
[125,     1] loss: 770.297
[126,     1] loss: 830.684
[127,     1] loss: 798.458
[128,     1] loss: 730.234
Early stopping applied (best metric=0.3644983768463135)
Finished Training
Total time taken: 18.472050189971924
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1235.641
[2,     1] loss: 1232.794
[3,     1] loss: 1233.557
[4,     1] loss: 1232.702
[5,     1] loss: 1228.297
[6,     1] loss: 1223.445
[7,     1] loss: 1219.522
[8,     1] loss: 1198.125
[9,     1] loss: 1161.227
[10,     1] loss: 1152.018
[11,     1] loss: 1108.381
[12,     1] loss: 1101.419
[13,     1] loss: 1052.046
[14,     1] loss: 1066.771
[15,     1] loss: 1046.404
[16,     1] loss: 970.852
[17,     1] loss: 1027.198
[18,     1] loss: 986.685
[19,     1] loss: 980.454
[20,     1] loss: 977.458
[21,     1] loss: 936.310
[22,     1] loss: 968.258
[23,     1] loss: 953.697
[24,     1] loss: 939.567
[25,     1] loss: 899.003
[26,     1] loss: 953.772
[27,     1] loss: 911.419
[28,     1] loss: 922.005
[29,     1] loss: 904.544
[30,     1] loss: 885.088
[31,     1] loss: 871.516
[32,     1] loss: 874.432
[33,     1] loss: 893.304
[34,     1] loss: 844.395
[35,     1] loss: 857.143
[36,     1] loss: 943.776
[37,     1] loss: 821.366
[38,     1] loss: 924.456
[39,     1] loss: 839.332
[40,     1] loss: 844.866
[41,     1] loss: 790.700
[42,     1] loss: 800.387
[43,     1] loss: 780.362
[44,     1] loss: 749.554
[45,     1] loss: 737.203
[46,     1] loss: 766.608
[47,     1] loss: 755.134
[48,     1] loss: 743.431
[49,     1] loss: 723.959
[50,     1] loss: 707.487
[51,     1] loss: 705.704
[52,     1] loss: 682.321
[53,     1] loss: 818.203
[54,     1] loss: 789.944
[55,     1] loss: 737.625
[56,     1] loss: 714.696
[57,     1] loss: 697.227
[58,     1] loss: 699.555
[59,     1] loss: 712.035
[60,     1] loss: 680.560
[61,     1] loss: 624.950
[62,     1] loss: 594.976
[63,     1] loss: 650.316
[64,     1] loss: 759.424
[65,     1] loss: 937.818
[66,     1] loss: 925.141
[67,     1] loss: 692.490
[68,     1] loss: 747.702
[69,     1] loss: 842.179
[70,     1] loss: 776.343
[71,     1] loss: 748.537
[72,     1] loss: 807.003
[73,     1] loss: 783.162
[74,     1] loss: 718.283
[75,     1] loss: 740.641
[76,     1] loss: 683.878
[77,     1] loss: 693.518
[78,     1] loss: 679.342
[79,     1] loss: 576.029
[80,     1] loss: 734.006
[81,     1] loss: 635.034
[82,     1] loss: 655.776
[83,     1] loss: 603.974
[84,     1] loss: 596.900
[85,     1] loss: 605.672
[86,     1] loss: 574.873
[87,     1] loss: 570.668
[88,     1] loss: 595.736
[89,     1] loss: 565.246
[90,     1] loss: 734.459
[91,     1] loss: 775.648
[92,     1] loss: 533.812
[93,     1] loss: 640.526
[94,     1] loss: 548.944
[95,     1] loss: 665.088
[96,     1] loss: 565.972
[97,     1] loss: 590.387
[98,     1] loss: 632.707
[99,     1] loss: 552.877
[100,     1] loss: 576.352
[101,     1] loss: 640.446
[102,     1] loss: 506.274
[103,     1] loss: 633.081
[104,     1] loss: 549.766
[105,     1] loss: 553.031
[106,     1] loss: 696.014
[107,     1] loss: 607.057
[108,     1] loss: 528.134
[109,     1] loss: 577.749
[110,     1] loss: 486.169
[111,     1] loss: 488.240
[112,     1] loss: 482.412
[113,     1] loss: 543.809
[114,     1] loss: 492.616
[115,     1] loss: 455.717
[116,     1] loss: 585.129
[117,     1] loss: 582.433
[118,     1] loss: 450.337
[119,     1] loss: 620.520
[120,     1] loss: 557.520
[121,     1] loss: 502.038
[122,     1] loss: 527.577
[123,     1] loss: 484.037
[124,     1] loss: 551.945
[125,     1] loss: 490.571
[126,     1] loss: 475.233
Early stopping applied (best metric=0.41048580408096313)
Finished Training
Total time taken: 18.032927989959717
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1239.626
[2,     1] loss: 1234.372
[3,     1] loss: 1234.107
[4,     1] loss: 1233.169
[5,     1] loss: 1232.667
[6,     1] loss: 1228.608
[7,     1] loss: 1228.784
[8,     1] loss: 1218.699
[9,     1] loss: 1202.656
[10,     1] loss: 1187.752
[11,     1] loss: 1144.193
[12,     1] loss: 1107.401
[13,     1] loss: 1085.836
[14,     1] loss: 1059.073
[15,     1] loss: 1044.342
[16,     1] loss: 1043.013
[17,     1] loss: 1043.569
[18,     1] loss: 994.817
[19,     1] loss: 997.726
[20,     1] loss: 977.685
[21,     1] loss: 1017.866
[22,     1] loss: 979.547
[23,     1] loss: 969.065
[24,     1] loss: 927.797
[25,     1] loss: 985.958
[26,     1] loss: 896.950
[27,     1] loss: 939.092
[28,     1] loss: 926.557
[29,     1] loss: 912.486
[30,     1] loss: 874.401
[31,     1] loss: 919.515
[32,     1] loss: 889.860
[33,     1] loss: 895.313
[34,     1] loss: 866.954
[35,     1] loss: 960.509
[36,     1] loss: 862.973
[37,     1] loss: 854.220
[38,     1] loss: 831.891
[39,     1] loss: 880.737
[40,     1] loss: 809.017
[41,     1] loss: 765.367
[42,     1] loss: 843.730
[43,     1] loss: 760.907
[44,     1] loss: 791.870
[45,     1] loss: 802.564
[46,     1] loss: 827.504
[47,     1] loss: 837.875
[48,     1] loss: 748.521
[49,     1] loss: 745.241
[50,     1] loss: 802.137
[51,     1] loss: 775.860
[52,     1] loss: 693.169
[53,     1] loss: 854.098
[54,     1] loss: 929.208
[55,     1] loss: 689.643
[56,     1] loss: 855.888
[57,     1] loss: 818.990
[58,     1] loss: 815.693
[59,     1] loss: 862.667
[60,     1] loss: 776.504
[61,     1] loss: 743.485
[62,     1] loss: 838.063
[63,     1] loss: 686.030
[64,     1] loss: 754.222
[65,     1] loss: 701.532
[66,     1] loss: 704.118
[67,     1] loss: 692.993
[68,     1] loss: 689.688
[69,     1] loss: 693.641
[70,     1] loss: 653.196
[71,     1] loss: 702.421
[72,     1] loss: 676.414
[73,     1] loss: 676.203
[74,     1] loss: 700.475
[75,     1] loss: 650.986
[76,     1] loss: 677.844
[77,     1] loss: 715.873
[78,     1] loss: 614.945
[79,     1] loss: 693.280
[80,     1] loss: 652.845
[81,     1] loss: 616.455
[82,     1] loss: 598.355
[83,     1] loss: 578.204
[84,     1] loss: 538.849
[85,     1] loss: 554.442
[86,     1] loss: 646.809
[87,     1] loss: 973.066
[88,     1] loss: 659.818
[89,     1] loss: 640.616
[90,     1] loss: 671.611
[91,     1] loss: 709.921
[92,     1] loss: 614.109
[93,     1] loss: 711.182
[94,     1] loss: 601.954
[95,     1] loss: 634.096
[96,     1] loss: 567.152
[97,     1] loss: 605.888
[98,     1] loss: 569.645
[99,     1] loss: 520.832
[100,     1] loss: 518.322
[101,     1] loss: 516.768
[102,     1] loss: 557.319
[103,     1] loss: 451.253
[104,     1] loss: 488.651
[105,     1] loss: 473.386
[106,     1] loss: 478.670
[107,     1] loss: 537.928
[108,     1] loss: 691.428
[109,     1] loss: 866.585
[110,     1] loss: 937.096
[111,     1] loss: 775.956
[112,     1] loss: 701.838
[113,     1] loss: 831.719
[114,     1] loss: 736.243
[115,     1] loss: 683.692
[116,     1] loss: 780.923
[117,     1] loss: 717.828
[118,     1] loss: 614.328
[119,     1] loss: 774.414
[120,     1] loss: 595.811
[121,     1] loss: 664.035
[122,     1] loss: 654.187
[123,     1] loss: 617.482
[124,     1] loss: 630.700
[125,     1] loss: 535.719
[126,     1] loss: 584.544
[127,     1] loss: 542.277
Early stopping applied (best metric=0.37782567739486694)
Finished Training
Total time taken: 18.117936611175537
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.450
[2,     1] loss: 1233.640
[3,     1] loss: 1229.764
[4,     1] loss: 1231.604
[5,     1] loss: 1230.021
[6,     1] loss: 1222.460
[7,     1] loss: 1212.809
[8,     1] loss: 1190.616
[9,     1] loss: 1172.344
[10,     1] loss: 1118.047
[11,     1] loss: 1102.485
[12,     1] loss: 1083.618
[13,     1] loss: 1060.114
[14,     1] loss: 1013.745
[15,     1] loss: 1017.049
[16,     1] loss: 1063.690
[17,     1] loss: 1010.273
[18,     1] loss: 1031.882
[19,     1] loss: 1040.074
[20,     1] loss: 1033.357
[21,     1] loss: 970.012
[22,     1] loss: 995.480
[23,     1] loss: 964.839
[24,     1] loss: 995.509
[25,     1] loss: 1008.654
[26,     1] loss: 946.971
[27,     1] loss: 907.987
[28,     1] loss: 967.532
[29,     1] loss: 940.924
[30,     1] loss: 945.186
[31,     1] loss: 926.450
[32,     1] loss: 857.552
[33,     1] loss: 898.767
[34,     1] loss: 849.454
[35,     1] loss: 835.065
[36,     1] loss: 862.964
[37,     1] loss: 830.864
[38,     1] loss: 827.473
[39,     1] loss: 802.313
[40,     1] loss: 876.395
[41,     1] loss: 894.266
[42,     1] loss: 836.161
[43,     1] loss: 789.535
[44,     1] loss: 814.461
[45,     1] loss: 768.170
[46,     1] loss: 826.242
[47,     1] loss: 750.904
[48,     1] loss: 774.319
[49,     1] loss: 781.866
[50,     1] loss: 695.694
[51,     1] loss: 716.962
[52,     1] loss: 700.249
[53,     1] loss: 794.472
[54,     1] loss: 859.891
[55,     1] loss: 748.247
[56,     1] loss: 747.994
[57,     1] loss: 699.316
[58,     1] loss: 737.005
[59,     1] loss: 735.960
[60,     1] loss: 681.771
[61,     1] loss: 680.340
[62,     1] loss: 628.940
[63,     1] loss: 720.124
[64,     1] loss: 668.371
[65,     1] loss: 727.363
[66,     1] loss: 629.054
[67,     1] loss: 612.989
[68,     1] loss: 682.326
[69,     1] loss: 604.999
[70,     1] loss: 635.851
[71,     1] loss: 674.871
[72,     1] loss: 554.029
[73,     1] loss: 599.487
[74,     1] loss: 717.077
[75,     1] loss: 697.395
[76,     1] loss: 596.851
[77,     1] loss: 637.080
[78,     1] loss: 588.827
[79,     1] loss: 594.154
[80,     1] loss: 611.136
[81,     1] loss: 575.514
[82,     1] loss: 653.754
[83,     1] loss: 611.944
[84,     1] loss: 499.610
[85,     1] loss: 657.656
[86,     1] loss: 557.101
[87,     1] loss: 518.744
[88,     1] loss: 564.058
[89,     1] loss: 583.324
[90,     1] loss: 522.817
[91,     1] loss: 558.038
[92,     1] loss: 547.452
[93,     1] loss: 485.882
[94,     1] loss: 493.783
[95,     1] loss: 505.830
[96,     1] loss: 435.707
[97,     1] loss: 442.085
[98,     1] loss: 456.009
[99,     1] loss: 391.687
[100,     1] loss: 457.298
[101,     1] loss: 541.482
[102,     1] loss: 1360.134
[103,     1] loss: 1534.779
[104,     1] loss: 1126.117
[105,     1] loss: 923.507
[106,     1] loss: 995.710
[107,     1] loss: 1069.516
[108,     1] loss: 1058.934
[109,     1] loss: 1112.408
[110,     1] loss: 1100.668
[111,     1] loss: 1106.984
[112,     1] loss: 1098.051
[113,     1] loss: 1084.764
[114,     1] loss: 1076.914
[115,     1] loss: 1071.550
[116,     1] loss: 1071.045
[117,     1] loss: 1043.298
[118,     1] loss: 1022.674
[119,     1] loss: 992.106
[120,     1] loss: 964.025
[121,     1] loss: 954.264
[122,     1] loss: 969.118
[123,     1] loss: 945.865
[124,     1] loss: 950.233
[125,     1] loss: 964.302
[126,     1] loss: 951.124
Early stopping applied (best metric=0.3476976454257965)
Finished Training
Total time taken: 18.056930541992188
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.961
[2,     1] loss: 1238.310
[3,     1] loss: 1234.305
[4,     1] loss: 1233.634
[5,     1] loss: 1233.841
[6,     1] loss: 1230.982
[7,     1] loss: 1230.677
[8,     1] loss: 1229.505
[9,     1] loss: 1225.222
[10,     1] loss: 1216.885
[11,     1] loss: 1202.615
[12,     1] loss: 1185.022
[13,     1] loss: 1168.218
[14,     1] loss: 1148.622
[15,     1] loss: 1131.300
[16,     1] loss: 1119.081
[17,     1] loss: 1084.101
[18,     1] loss: 1078.491
[19,     1] loss: 1052.384
[20,     1] loss: 1063.761
[21,     1] loss: 1073.725
[22,     1] loss: 1016.795
[23,     1] loss: 1032.524
[24,     1] loss: 1015.214
[25,     1] loss: 989.465
[26,     1] loss: 994.203
[27,     1] loss: 991.798
[28,     1] loss: 984.261
[29,     1] loss: 970.125
[30,     1] loss: 923.003
[31,     1] loss: 898.805
[32,     1] loss: 937.443
[33,     1] loss: 961.492
[34,     1] loss: 891.815
[35,     1] loss: 880.908
[36,     1] loss: 891.183
[37,     1] loss: 866.785
[38,     1] loss: 903.730
[39,     1] loss: 885.561
[40,     1] loss: 837.777
[41,     1] loss: 875.929
[42,     1] loss: 834.019
[43,     1] loss: 858.902
[44,     1] loss: 835.363
[45,     1] loss: 808.754
[46,     1] loss: 891.907
[47,     1] loss: 954.223
[48,     1] loss: 823.552
[49,     1] loss: 883.449
[50,     1] loss: 802.796
[51,     1] loss: 877.041
[52,     1] loss: 800.527
[53,     1] loss: 856.429
[54,     1] loss: 801.616
[55,     1] loss: 799.284
[56,     1] loss: 770.633
[57,     1] loss: 787.766
[58,     1] loss: 723.817
[59,     1] loss: 732.434
[60,     1] loss: 816.829
[61,     1] loss: 804.195
[62,     1] loss: 668.317
[63,     1] loss: 734.606
[64,     1] loss: 682.589
[65,     1] loss: 705.954
[66,     1] loss: 639.468
[67,     1] loss: 633.020
[68,     1] loss: 680.093
[69,     1] loss: 738.944
[70,     1] loss: 893.403
[71,     1] loss: 749.344
[72,     1] loss: 657.812
[73,     1] loss: 699.599
[74,     1] loss: 741.824
[75,     1] loss: 685.523
[76,     1] loss: 761.438
[77,     1] loss: 650.729
[78,     1] loss: 702.492
[79,     1] loss: 622.969
[80,     1] loss: 676.036
[81,     1] loss: 647.297
[82,     1] loss: 782.229
[83,     1] loss: 621.638
[84,     1] loss: 636.532
[85,     1] loss: 662.458
[86,     1] loss: 571.196
[87,     1] loss: 659.700
[88,     1] loss: 561.971
[89,     1] loss: 595.329
[90,     1] loss: 605.431
[91,     1] loss: 516.714
[92,     1] loss: 599.337
[93,     1] loss: 690.098
[94,     1] loss: 691.544
[95,     1] loss: 648.167
[96,     1] loss: 544.156
[97,     1] loss: 668.433
[98,     1] loss: 553.248
[99,     1] loss: 631.402
[100,     1] loss: 550.082
[101,     1] loss: 544.567
[102,     1] loss: 515.374
[103,     1] loss: 491.935
[104,     1] loss: 517.888
[105,     1] loss: 493.326
[106,     1] loss: 452.813
[107,     1] loss: 489.972
[108,     1] loss: 501.139
[109,     1] loss: 531.115
[110,     1] loss: 688.826
[111,     1] loss: 911.078
[112,     1] loss: 595.688
[113,     1] loss: 706.481
[114,     1] loss: 610.975
[115,     1] loss: 692.730
[116,     1] loss: 612.069
[117,     1] loss: 641.997
[118,     1] loss: 618.429
[119,     1] loss: 639.061
[120,     1] loss: 543.981
[121,     1] loss: 621.804
[122,     1] loss: 541.896
[123,     1] loss: 627.917
[124,     1] loss: 505.722
[125,     1] loss: 547.279
[126,     1] loss: 461.690
[127,     1] loss: 487.316
[128,     1] loss: 531.766
[129,     1] loss: 462.304
[130,     1] loss: 511.175
[131,     1] loss: 624.464
[132,     1] loss: 422.916
[133,     1] loss: 561.570
[134,     1] loss: 594.889
[135,     1] loss: 485.927
[136,     1] loss: 706.411
[137,     1] loss: 492.501
[138,     1] loss: 544.472
[139,     1] loss: 502.668
[140,     1] loss: 427.245
[141,     1] loss: 513.017
[142,     1] loss: 418.511
[143,     1] loss: 472.575
[144,     1] loss: 519.009
[145,     1] loss: 442.681
Early stopping applied (best metric=0.3549658954143524)
Finished Training
Total time taken: 20.614203214645386
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.779
[2,     1] loss: 1226.660
[3,     1] loss: 1237.550
[4,     1] loss: 1233.640
[5,     1] loss: 1229.572
[6,     1] loss: 1230.400
[7,     1] loss: 1217.669
[8,     1] loss: 1201.991
[9,     1] loss: 1179.944
[10,     1] loss: 1157.798
[11,     1] loss: 1128.069
[12,     1] loss: 1103.594
[13,     1] loss: 1075.560
[14,     1] loss: 1070.685
[15,     1] loss: 1027.096
[16,     1] loss: 1004.444
[17,     1] loss: 993.939
[18,     1] loss: 1003.931
[19,     1] loss: 962.147
[20,     1] loss: 966.651
[21,     1] loss: 980.798
[22,     1] loss: 972.322
[23,     1] loss: 913.643
[24,     1] loss: 1004.965
[25,     1] loss: 938.574
[26,     1] loss: 964.481
[27,     1] loss: 937.938
[28,     1] loss: 966.784
[29,     1] loss: 887.148
[30,     1] loss: 899.058
[31,     1] loss: 860.989
[32,     1] loss: 902.223
[33,     1] loss: 823.309
[34,     1] loss: 892.965
[35,     1] loss: 805.948
[36,     1] loss: 835.633
[37,     1] loss: 852.993
[38,     1] loss: 847.204
[39,     1] loss: 838.105
[40,     1] loss: 841.067
[41,     1] loss: 856.642
[42,     1] loss: 779.457
[43,     1] loss: 840.861
[44,     1] loss: 793.928
[45,     1] loss: 855.677
[46,     1] loss: 761.847
[47,     1] loss: 828.747
[48,     1] loss: 745.623
[49,     1] loss: 792.705
[50,     1] loss: 797.633
[51,     1] loss: 724.502
[52,     1] loss: 738.928
[53,     1] loss: 690.929
[54,     1] loss: 766.932
[55,     1] loss: 732.404
[56,     1] loss: 699.403
[57,     1] loss: 640.037
[58,     1] loss: 717.388
[59,     1] loss: 895.548
[60,     1] loss: 722.234
[61,     1] loss: 708.699
[62,     1] loss: 771.988
[63,     1] loss: 720.946
[64,     1] loss: 716.557
[65,     1] loss: 714.856
[66,     1] loss: 694.116
[67,     1] loss: 691.849
[68,     1] loss: 655.290
[69,     1] loss: 630.093
[70,     1] loss: 601.319
[71,     1] loss: 676.437
[72,     1] loss: 699.419
[73,     1] loss: 840.240
[74,     1] loss: 810.328
[75,     1] loss: 692.520
[76,     1] loss: 758.116
[77,     1] loss: 736.761
[78,     1] loss: 700.535
[79,     1] loss: 709.423
[80,     1] loss: 685.743
[81,     1] loss: 647.612
[82,     1] loss: 674.899
[83,     1] loss: 660.041
[84,     1] loss: 668.807
[85,     1] loss: 619.059
[86,     1] loss: 666.075
[87,     1] loss: 609.514
[88,     1] loss: 629.726
[89,     1] loss: 604.897
[90,     1] loss: 638.890
[91,     1] loss: 577.672
[92,     1] loss: 558.341
[93,     1] loss: 567.692
[94,     1] loss: 533.105
[95,     1] loss: 556.057
[96,     1] loss: 625.245
[97,     1] loss: 791.744
[98,     1] loss: 620.251
[99,     1] loss: 549.401
[100,     1] loss: 621.581
[101,     1] loss: 587.376
[102,     1] loss: 528.858
[103,     1] loss: 544.018
[104,     1] loss: 536.627
[105,     1] loss: 519.675
[106,     1] loss: 587.000
[107,     1] loss: 527.545
[108,     1] loss: 510.756
[109,     1] loss: 520.340
[110,     1] loss: 446.818
[111,     1] loss: 492.566
[112,     1] loss: 481.053
[113,     1] loss: 490.040
[114,     1] loss: 640.892
[115,     1] loss: 914.351
[116,     1] loss: 842.943
[117,     1] loss: 698.290
[118,     1] loss: 652.006
[119,     1] loss: 845.949
[120,     1] loss: 673.535
[121,     1] loss: 708.495
[122,     1] loss: 748.756
[123,     1] loss: 681.424
[124,     1] loss: 675.182
Early stopping applied (best metric=0.3746982216835022)
Finished Training
Total time taken: 17.712894678115845
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.917
[2,     1] loss: 1236.829
[3,     1] loss: 1233.103
[4,     1] loss: 1234.974
[5,     1] loss: 1236.336
[6,     1] loss: 1231.094
[7,     1] loss: 1230.996
[8,     1] loss: 1226.385
[9,     1] loss: 1224.786
[10,     1] loss: 1210.602
[11,     1] loss: 1201.369
[12,     1] loss: 1176.653
[13,     1] loss: 1140.315
[14,     1] loss: 1131.181
[15,     1] loss: 1088.660
[16,     1] loss: 1089.618
[17,     1] loss: 1047.793
[18,     1] loss: 1027.441
[19,     1] loss: 1034.956
[20,     1] loss: 998.086
[21,     1] loss: 1075.868
[22,     1] loss: 999.848
[23,     1] loss: 1045.897
[24,     1] loss: 988.315
[25,     1] loss: 1005.162
[26,     1] loss: 977.004
[27,     1] loss: 1005.927
[28,     1] loss: 960.137
[29,     1] loss: 980.237
[30,     1] loss: 949.712
[31,     1] loss: 929.721
[32,     1] loss: 945.862
[33,     1] loss: 946.056
[34,     1] loss: 915.744
[35,     1] loss: 888.467
[36,     1] loss: 900.981
[37,     1] loss: 898.957
[38,     1] loss: 888.224
[39,     1] loss: 942.658
[40,     1] loss: 882.319
[41,     1] loss: 889.235
[42,     1] loss: 899.140
[43,     1] loss: 871.436
[44,     1] loss: 886.513
[45,     1] loss: 891.253
[46,     1] loss: 842.447
[47,     1] loss: 804.030
[48,     1] loss: 816.566
[49,     1] loss: 838.597
[50,     1] loss: 777.381
[51,     1] loss: 875.562
[52,     1] loss: 788.316
[53,     1] loss: 824.097
[54,     1] loss: 828.859
[55,     1] loss: 769.090
[56,     1] loss: 797.442
[57,     1] loss: 795.785
[58,     1] loss: 769.994
[59,     1] loss: 788.969
[60,     1] loss: 753.520
[61,     1] loss: 732.758
[62,     1] loss: 698.220
[63,     1] loss: 692.971
[64,     1] loss: 672.694
[65,     1] loss: 678.727
[66,     1] loss: 779.432
[67,     1] loss: 1155.390
[68,     1] loss: 919.989
[69,     1] loss: 770.534
[70,     1] loss: 797.875
[71,     1] loss: 862.602
[72,     1] loss: 823.379
[73,     1] loss: 785.465
[74,     1] loss: 820.367
[75,     1] loss: 850.073
[76,     1] loss: 793.104
[77,     1] loss: 841.089
[78,     1] loss: 793.116
[79,     1] loss: 737.655
[80,     1] loss: 749.198
[81,     1] loss: 715.752
[82,     1] loss: 730.495
[83,     1] loss: 718.104
[84,     1] loss: 684.241
[85,     1] loss: 689.728
[86,     1] loss: 654.997
[87,     1] loss: 659.662
[88,     1] loss: 740.243
[89,     1] loss: 608.647
[90,     1] loss: 632.462
[91,     1] loss: 632.962
[92,     1] loss: 601.698
[93,     1] loss: 586.551
[94,     1] loss: 564.564
[95,     1] loss: 624.089
[96,     1] loss: 741.728
[97,     1] loss: 1143.470
[98,     1] loss: 588.337
[99,     1] loss: 825.979
[100,     1] loss: 686.984
[101,     1] loss: 760.836
[102,     1] loss: 773.167
[103,     1] loss: 669.983
[104,     1] loss: 683.664
[105,     1] loss: 672.196
[106,     1] loss: 645.847
[107,     1] loss: 610.702
[108,     1] loss: 631.155
[109,     1] loss: 567.608
[110,     1] loss: 616.936
[111,     1] loss: 581.016
[112,     1] loss: 537.101
[113,     1] loss: 571.155
[114,     1] loss: 566.997
[115,     1] loss: 578.618
[116,     1] loss: 520.137
[117,     1] loss: 526.581
[118,     1] loss: 567.425
[119,     1] loss: 679.233
[120,     1] loss: 692.163
[121,     1] loss: 596.615
[122,     1] loss: 533.225
[123,     1] loss: 697.070
[124,     1] loss: 514.206
[125,     1] loss: 693.906
[126,     1] loss: 576.948
[127,     1] loss: 515.132
[128,     1] loss: 667.218
[129,     1] loss: 548.852
[130,     1] loss: 477.051
[131,     1] loss: 548.313
[132,     1] loss: 479.177
[133,     1] loss: 470.510
[134,     1] loss: 475.405
[135,     1] loss: 529.940
[136,     1] loss: 643.607
[137,     1] loss: 912.259
[138,     1] loss: 886.468
[139,     1] loss: 725.409
[140,     1] loss: 794.938
[141,     1] loss: 911.021
[142,     1] loss: 758.518
[143,     1] loss: 743.191
[144,     1] loss: 779.015
[145,     1] loss: 645.863
[146,     1] loss: 774.328
[147,     1] loss: 634.325
Early stopping applied (best metric=0.3423854112625122)
Finished Training
Total time taken: 21.05525279045105
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1238.576
[2,     1] loss: 1237.397
[3,     1] loss: 1236.203
[4,     1] loss: 1234.744
[5,     1] loss: 1233.009
[6,     1] loss: 1233.998
[7,     1] loss: 1228.063
[8,     1] loss: 1226.321
[9,     1] loss: 1213.207
[10,     1] loss: 1195.563
[11,     1] loss: 1169.965
[12,     1] loss: 1143.810
[13,     1] loss: 1113.656
[14,     1] loss: 1109.200
[15,     1] loss: 1052.385
[16,     1] loss: 1045.534
[17,     1] loss: 1048.593
[18,     1] loss: 1030.222
[19,     1] loss: 1073.320
[20,     1] loss: 989.694
[21,     1] loss: 1044.388
[22,     1] loss: 1005.613
[23,     1] loss: 1006.030
[24,     1] loss: 984.960
[25,     1] loss: 999.856
[26,     1] loss: 982.872
[27,     1] loss: 952.729
[28,     1] loss: 1011.771
[29,     1] loss: 961.938
[30,     1] loss: 963.323
[31,     1] loss: 943.751
[32,     1] loss: 914.733
[33,     1] loss: 888.211
[34,     1] loss: 899.511
[35,     1] loss: 942.985
[36,     1] loss: 903.535
[37,     1] loss: 881.723
[38,     1] loss: 909.310
[39,     1] loss: 916.240
[40,     1] loss: 875.978
[41,     1] loss: 831.975
[42,     1] loss: 830.549
[43,     1] loss: 909.625
[44,     1] loss: 1056.539
[45,     1] loss: 845.379
[46,     1] loss: 1020.870
[47,     1] loss: 824.380
[48,     1] loss: 921.666
[49,     1] loss: 931.190
[50,     1] loss: 895.550
[51,     1] loss: 867.523
[52,     1] loss: 878.428
[53,     1] loss: 873.165
[54,     1] loss: 827.764
[55,     1] loss: 825.447
[56,     1] loss: 895.048
[57,     1] loss: 802.983
[58,     1] loss: 796.109
[59,     1] loss: 789.245
[60,     1] loss: 789.602
[61,     1] loss: 831.595
[62,     1] loss: 770.315
[63,     1] loss: 711.587
[64,     1] loss: 682.933
[65,     1] loss: 694.514
[66,     1] loss: 705.333
[67,     1] loss: 689.840
[68,     1] loss: 724.222
[69,     1] loss: 836.241
[70,     1] loss: 1135.682
[71,     1] loss: 702.903
[72,     1] loss: 966.432
[73,     1] loss: 773.215
[74,     1] loss: 808.229
[75,     1] loss: 863.983
[76,     1] loss: 862.811
[77,     1] loss: 832.740
[78,     1] loss: 806.772
[79,     1] loss: 832.203
[80,     1] loss: 760.529
[81,     1] loss: 816.909
[82,     1] loss: 725.765
[83,     1] loss: 731.586
[84,     1] loss: 722.886
[85,     1] loss: 727.651
[86,     1] loss: 659.798
[87,     1] loss: 698.393
[88,     1] loss: 654.828
[89,     1] loss: 694.243
[90,     1] loss: 612.348
[91,     1] loss: 608.249
[92,     1] loss: 743.432
[93,     1] loss: 832.439
[94,     1] loss: 581.231
[95,     1] loss: 832.628
[96,     1] loss: 599.459
[97,     1] loss: 733.622
[98,     1] loss: 591.080
[99,     1] loss: 689.299
[100,     1] loss: 593.836
[101,     1] loss: 630.452
[102,     1] loss: 585.034
[103,     1] loss: 606.410
[104,     1] loss: 537.559
[105,     1] loss: 562.532
[106,     1] loss: 661.643
[107,     1] loss: 535.807
[108,     1] loss: 536.171
[109,     1] loss: 612.152
[110,     1] loss: 578.831
[111,     1] loss: 503.677
[112,     1] loss: 663.159
[113,     1] loss: 617.311
[114,     1] loss: 587.621
[115,     1] loss: 790.234
[116,     1] loss: 574.102
[117,     1] loss: 574.893
[118,     1] loss: 545.052
[119,     1] loss: 474.405
[120,     1] loss: 507.014
[121,     1] loss: 450.006
[122,     1] loss: 607.442
[123,     1] loss: 1126.418
[124,     1] loss: 484.381
[125,     1] loss: 729.273
[126,     1] loss: 645.666
[127,     1] loss: 756.561
[128,     1] loss: 611.808
[129,     1] loss: 757.563
[130,     1] loss: 594.721
[131,     1] loss: 649.203
[132,     1] loss: 632.122
[133,     1] loss: 560.682
[134,     1] loss: 596.691
[135,     1] loss: 547.977
[136,     1] loss: 526.764
[137,     1] loss: 625.164
[138,     1] loss: 467.492
[139,     1] loss: 584.404
[140,     1] loss: 582.203
[141,     1] loss: 432.868
[142,     1] loss: 520.613
[143,     1] loss: 489.515
[144,     1] loss: 530.399
[145,     1] loss: 475.521
[146,     1] loss: 431.948
[147,     1] loss: 609.350
[148,     1] loss: 839.379
[149,     1] loss: 430.754
[150,     1] loss: 821.469
[151,     1] loss: 698.549
[152,     1] loss: 818.044
[153,     1] loss: 593.815
[154,     1] loss: 890.444
[155,     1] loss: 640.808
[156,     1] loss: 679.210
[157,     1] loss: 702.775
[158,     1] loss: 578.746
[159,     1] loss: 594.512
[160,     1] loss: 533.266
[161,     1] loss: 612.553
[162,     1] loss: 505.946
[163,     1] loss: 651.224
[164,     1] loss: 563.934
[165,     1] loss: 573.671
[166,     1] loss: 618.303
[167,     1] loss: 565.445
[168,     1] loss: 545.929
[169,     1] loss: 530.044
[170,     1] loss: 581.170
[171,     1] loss: 486.417
[172,     1] loss: 482.190
[173,     1] loss: 567.422
[174,     1] loss: 462.897
[175,     1] loss: 419.994
[176,     1] loss: 474.338
[177,     1] loss: 463.018
[178,     1] loss: 440.332
[179,     1] loss: 449.788
[180,     1] loss: 439.789
[181,     1] loss: 424.954
[182,     1] loss: 604.389
[183,     1] loss: 1688.270
[184,     1] loss: 589.580
[185,     1] loss: 1168.891
[186,     1] loss: 849.926
[187,     1] loss: 898.654
[188,     1] loss: 966.027
[189,     1] loss: 895.490
[190,     1] loss: 830.893
[191,     1] loss: 973.268
[192,     1] loss: 833.310
[193,     1] loss: 850.023
[194,     1] loss: 874.067
[195,     1] loss: 821.731
[196,     1] loss: 819.226
[197,     1] loss: 872.269
[198,     1] loss: 831.723
[199,     1] loss: 796.610
[200,     1] loss: 800.995
Finished Training
Total time taken: 28.429036140441895
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.667
[2,     1] loss: 1231.319
[3,     1] loss: 1234.889
[4,     1] loss: 1231.637
[5,     1] loss: 1230.059
[6,     1] loss: 1231.071
[7,     1] loss: 1221.145
[8,     1] loss: 1208.692
[9,     1] loss: 1184.697
[10,     1] loss: 1153.234
[11,     1] loss: 1118.948
[12,     1] loss: 1112.944
[13,     1] loss: 1073.609
[14,     1] loss: 1089.435
[15,     1] loss: 1055.508
[16,     1] loss: 1025.093
[17,     1] loss: 1017.122
[18,     1] loss: 1028.286
[19,     1] loss: 970.439
[20,     1] loss: 1075.033
[21,     1] loss: 977.180
[22,     1] loss: 988.686
[23,     1] loss: 949.103
[24,     1] loss: 941.562
[25,     1] loss: 970.062
[26,     1] loss: 950.565
[27,     1] loss: 943.465
[28,     1] loss: 924.616
[29,     1] loss: 905.852
[30,     1] loss: 906.552
[31,     1] loss: 927.232
[32,     1] loss: 887.770
[33,     1] loss: 881.532
[34,     1] loss: 912.187
[35,     1] loss: 961.169
[36,     1] loss: 1001.019
[37,     1] loss: 892.971
[38,     1] loss: 926.861
[39,     1] loss: 862.011
[40,     1] loss: 911.794
[41,     1] loss: 862.363
[42,     1] loss: 828.675
[43,     1] loss: 862.506
[44,     1] loss: 808.127
[45,     1] loss: 833.954
[46,     1] loss: 776.808
[47,     1] loss: 825.144
[48,     1] loss: 811.352
[49,     1] loss: 848.402
[50,     1] loss: 899.660
[51,     1] loss: 847.048
[52,     1] loss: 793.580
[53,     1] loss: 768.454
[54,     1] loss: 728.512
[55,     1] loss: 745.808
[56,     1] loss: 737.847
[57,     1] loss: 761.994
[58,     1] loss: 744.097
[59,     1] loss: 788.250
[60,     1] loss: 824.612
[61,     1] loss: 716.566
[62,     1] loss: 876.626
[63,     1] loss: 781.277
[64,     1] loss: 872.302
[65,     1] loss: 729.508
[66,     1] loss: 821.320
[67,     1] loss: 757.724
[68,     1] loss: 780.621
[69,     1] loss: 794.466
[70,     1] loss: 654.805
[71,     1] loss: 743.290
[72,     1] loss: 667.226
[73,     1] loss: 718.842
[74,     1] loss: 647.216
[75,     1] loss: 671.375
[76,     1] loss: 679.032
[77,     1] loss: 622.056
[78,     1] loss: 674.923
[79,     1] loss: 680.685
[80,     1] loss: 598.391
[81,     1] loss: 726.149
[82,     1] loss: 651.984
[83,     1] loss: 687.737
[84,     1] loss: 747.643
[85,     1] loss: 545.548
[86,     1] loss: 671.035
[87,     1] loss: 542.670
[88,     1] loss: 690.725
[89,     1] loss: 592.374
[90,     1] loss: 635.827
[91,     1] loss: 587.226
[92,     1] loss: 519.266
[93,     1] loss: 577.434
Early stopping applied (best metric=0.3127058744430542)
Finished Training
Total time taken: 13.306423902511597
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.550
[2,     1] loss: 1241.123
[3,     1] loss: 1235.227
[4,     1] loss: 1231.146
[5,     1] loss: 1228.211
[6,     1] loss: 1231.243
[7,     1] loss: 1219.999
[8,     1] loss: 1206.927
[9,     1] loss: 1179.916
[10,     1] loss: 1154.237
[11,     1] loss: 1122.381
[12,     1] loss: 1103.254
[13,     1] loss: 1064.149
[14,     1] loss: 1046.073
[15,     1] loss: 1053.535
[16,     1] loss: 1044.007
[17,     1] loss: 1066.632
[18,     1] loss: 1063.247
[19,     1] loss: 1050.685
[20,     1] loss: 1013.974
[21,     1] loss: 987.875
[22,     1] loss: 989.707
[23,     1] loss: 1006.688
[24,     1] loss: 912.836
[25,     1] loss: 988.267
[26,     1] loss: 955.113
[27,     1] loss: 989.054
[28,     1] loss: 988.589
[29,     1] loss: 935.004
[30,     1] loss: 992.659
[31,     1] loss: 961.218
[32,     1] loss: 923.433
[33,     1] loss: 974.232
[34,     1] loss: 895.037
[35,     1] loss: 953.001
[36,     1] loss: 933.973
[37,     1] loss: 923.560
[38,     1] loss: 949.368
[39,     1] loss: 905.816
[40,     1] loss: 910.020
[41,     1] loss: 876.888
[42,     1] loss: 869.034
[43,     1] loss: 876.478
[44,     1] loss: 837.162
[45,     1] loss: 829.058
[46,     1] loss: 827.165
[47,     1] loss: 797.183
[48,     1] loss: 779.263
[49,     1] loss: 777.853
[50,     1] loss: 784.005
[51,     1] loss: 840.775
[52,     1] loss: 785.037
[53,     1] loss: 809.137
[54,     1] loss: 863.253
[55,     1] loss: 773.330
[56,     1] loss: 845.927
[57,     1] loss: 818.522
[58,     1] loss: 795.819
[59,     1] loss: 897.583
[60,     1] loss: 766.591
[61,     1] loss: 851.993
[62,     1] loss: 774.945
[63,     1] loss: 787.662
[64,     1] loss: 772.339
[65,     1] loss: 732.679
[66,     1] loss: 799.678
[67,     1] loss: 662.563
[68,     1] loss: 795.614
[69,     1] loss: 658.380
[70,     1] loss: 751.340
[71,     1] loss: 640.441
[72,     1] loss: 804.096
[73,     1] loss: 620.472
[74,     1] loss: 748.884
[75,     1] loss: 741.337
[76,     1] loss: 640.782
[77,     1] loss: 699.462
[78,     1] loss: 609.135
[79,     1] loss: 654.873
[80,     1] loss: 591.575
[81,     1] loss: 647.016
[82,     1] loss: 682.572
[83,     1] loss: 572.091
[84,     1] loss: 572.744
[85,     1] loss: 687.621
[86,     1] loss: 611.645
[87,     1] loss: 546.463
[88,     1] loss: 637.791
[89,     1] loss: 543.395
[90,     1] loss: 500.270
[91,     1] loss: 564.063
[92,     1] loss: 484.839
[93,     1] loss: 486.569
[94,     1] loss: 557.890
[95,     1] loss: 683.330
[96,     1] loss: 1056.468
[97,     1] loss: 529.236
[98,     1] loss: 786.850
[99,     1] loss: 622.857
[100,     1] loss: 710.652
[101,     1] loss: 740.957
[102,     1] loss: 643.665
[103,     1] loss: 629.890
[104,     1] loss: 620.657
[105,     1] loss: 647.730
[106,     1] loss: 549.285
[107,     1] loss: 555.102
[108,     1] loss: 507.266
[109,     1] loss: 518.224
[110,     1] loss: 585.995
[111,     1] loss: 507.851
[112,     1] loss: 484.059
[113,     1] loss: 530.940
[114,     1] loss: 510.497
[115,     1] loss: 496.190
[116,     1] loss: 642.916
[117,     1] loss: 626.449
[118,     1] loss: 497.482
[119,     1] loss: 807.893
[120,     1] loss: 722.235
[121,     1] loss: 637.927
[122,     1] loss: 562.425
[123,     1] loss: 684.560
[124,     1] loss: 510.951
[125,     1] loss: 682.596
[126,     1] loss: 456.508
[127,     1] loss: 570.313
[128,     1] loss: 484.676
[129,     1] loss: 603.652
[130,     1] loss: 496.613
[131,     1] loss: 570.586
[132,     1] loss: 518.267
[133,     1] loss: 520.807
[134,     1] loss: 486.622
[135,     1] loss: 462.478
[136,     1] loss: 467.772
[137,     1] loss: 414.712
[138,     1] loss: 388.660
[139,     1] loss: 476.889
[140,     1] loss: 481.220
[141,     1] loss: 630.951
[142,     1] loss: 667.577
[143,     1] loss: 545.567
[144,     1] loss: 488.437
[145,     1] loss: 563.178
[146,     1] loss: 395.420
[147,     1] loss: 556.345
[148,     1] loss: 519.651
[149,     1] loss: 412.598
[150,     1] loss: 428.071
[151,     1] loss: 541.131
[152,     1] loss: 621.744
[153,     1] loss: 393.150
[154,     1] loss: 485.936
[155,     1] loss: 447.583
[156,     1] loss: 460.928
[157,     1] loss: 528.138
[158,     1] loss: 420.500
[159,     1] loss: 408.742
[160,     1] loss: 458.241
[161,     1] loss: 423.165
[162,     1] loss: 377.779
[163,     1] loss: 406.875
[164,     1] loss: 421.289
[165,     1] loss: 496.702
[166,     1] loss: 465.559
[167,     1] loss: 421.455
[168,     1] loss: 396.670
[169,     1] loss: 395.082
[170,     1] loss: 502.970
[171,     1] loss: 984.867
[172,     1] loss: 1016.608
[173,     1] loss: 705.454
[174,     1] loss: 696.375
[175,     1] loss: 619.520
[176,     1] loss: 749.365
[177,     1] loss: 672.138
Early stopping applied (best metric=0.302075058221817)
Finished Training
Total time taken: 25.12168550491333
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.803
[2,     1] loss: 1236.914
[3,     1] loss: 1234.760
[4,     1] loss: 1233.505
[5,     1] loss: 1237.661
[6,     1] loss: 1230.426
[7,     1] loss: 1231.950
[8,     1] loss: 1231.030
[9,     1] loss: 1229.679
[10,     1] loss: 1228.698
[11,     1] loss: 1225.331
[12,     1] loss: 1220.589
[13,     1] loss: 1209.332
[14,     1] loss: 1188.724
[15,     1] loss: 1169.748
[16,     1] loss: 1141.332
[17,     1] loss: 1120.470
[18,     1] loss: 1106.362
[19,     1] loss: 1072.903
[20,     1] loss: 1058.985
[21,     1] loss: 1016.699
[22,     1] loss: 1006.607
[23,     1] loss: 997.193
[24,     1] loss: 1014.400
[25,     1] loss: 975.757
[26,     1] loss: 972.928
[27,     1] loss: 1016.628
[28,     1] loss: 960.093
[29,     1] loss: 944.347
[30,     1] loss: 965.490
[31,     1] loss: 984.213
[32,     1] loss: 920.071
[33,     1] loss: 883.058
[34,     1] loss: 918.909
[35,     1] loss: 882.623
[36,     1] loss: 887.471
[37,     1] loss: 867.525
[38,     1] loss: 929.144
[39,     1] loss: 863.103
[40,     1] loss: 872.203
[41,     1] loss: 909.026
[42,     1] loss: 879.048
[43,     1] loss: 863.045
[44,     1] loss: 852.999
[45,     1] loss: 799.713
[46,     1] loss: 876.525
[47,     1] loss: 835.946
[48,     1] loss: 781.158
[49,     1] loss: 816.401
[50,     1] loss: 753.521
[51,     1] loss: 802.660
[52,     1] loss: 943.510
[53,     1] loss: 794.094
[54,     1] loss: 725.166
[55,     1] loss: 770.642
[56,     1] loss: 756.987
[57,     1] loss: 756.052
[58,     1] loss: 732.884
[59,     1] loss: 689.225
[60,     1] loss: 679.993
[61,     1] loss: 668.931
[62,     1] loss: 707.364
[63,     1] loss: 868.833
[64,     1] loss: 1290.536
[65,     1] loss: 811.179
[66,     1] loss: 908.775
[67,     1] loss: 729.440
[68,     1] loss: 824.092
[69,     1] loss: 879.292
[70,     1] loss: 846.670
[71,     1] loss: 823.677
[72,     1] loss: 837.791
[73,     1] loss: 848.042
[74,     1] loss: 809.608
[75,     1] loss: 766.426
[76,     1] loss: 746.172
[77,     1] loss: 772.606
[78,     1] loss: 743.317
[79,     1] loss: 694.084
[80,     1] loss: 783.050
[81,     1] loss: 733.556
[82,     1] loss: 743.483
[83,     1] loss: 722.126
[84,     1] loss: 702.584
[85,     1] loss: 678.537
[86,     1] loss: 662.863
[87,     1] loss: 660.178
[88,     1] loss: 666.307
[89,     1] loss: 658.916
[90,     1] loss: 645.615
[91,     1] loss: 592.444
[92,     1] loss: 568.461
[93,     1] loss: 641.782
[94,     1] loss: 1013.663
[95,     1] loss: 798.109
[96,     1] loss: 692.477
[97,     1] loss: 710.911
[98,     1] loss: 837.960
[99,     1] loss: 684.640
[100,     1] loss: 775.498
[101,     1] loss: 667.904
[102,     1] loss: 674.342
[103,     1] loss: 648.217
[104,     1] loss: 609.357
[105,     1] loss: 677.332
[106,     1] loss: 605.202
[107,     1] loss: 610.375
[108,     1] loss: 635.247
[109,     1] loss: 581.297
[110,     1] loss: 559.120
[111,     1] loss: 553.381
[112,     1] loss: 545.099
[113,     1] loss: 528.486
[114,     1] loss: 559.622
[115,     1] loss: 645.527
[116,     1] loss: 882.383
[117,     1] loss: 924.075
[118,     1] loss: 653.515
[119,     1] loss: 770.724
[120,     1] loss: 718.104
[121,     1] loss: 680.280
[122,     1] loss: 773.439
[123,     1] loss: 687.545
[124,     1] loss: 644.681
[125,     1] loss: 702.121
[126,     1] loss: 592.640
[127,     1] loss: 601.546
[128,     1] loss: 559.948
[129,     1] loss: 588.185
[130,     1] loss: 587.994
[131,     1] loss: 551.002
[132,     1] loss: 536.909
[133,     1] loss: 562.613
[134,     1] loss: 487.759
[135,     1] loss: 559.970
[136,     1] loss: 670.254
[137,     1] loss: 505.342
[138,     1] loss: 542.737
[139,     1] loss: 627.893
[140,     1] loss: 493.608
[141,     1] loss: 481.446
[142,     1] loss: 537.494
[143,     1] loss: 482.618
[144,     1] loss: 404.746
[145,     1] loss: 414.659
[146,     1] loss: 462.106
[147,     1] loss: 583.245
[148,     1] loss: 864.302
[149,     1] loss: 1376.715
[150,     1] loss: 643.860
[151,     1] loss: 913.124
[152,     1] loss: 960.975
[153,     1] loss: 865.317
[154,     1] loss: 860.702
[155,     1] loss: 886.960
[156,     1] loss: 824.250
[157,     1] loss: 823.733
[158,     1] loss: 798.761
[159,     1] loss: 784.965
[160,     1] loss: 733.464
[161,     1] loss: 778.492
[162,     1] loss: 718.159
[163,     1] loss: 736.320
[164,     1] loss: 707.210
[165,     1] loss: 723.088
[166,     1] loss: 674.010
[167,     1] loss: 651.088
[168,     1] loss: 691.657
[169,     1] loss: 617.521
[170,     1] loss: 606.280
[171,     1] loss: 655.984
[172,     1] loss: 542.486
[173,     1] loss: 573.056
[174,     1] loss: 575.131
[175,     1] loss: 535.599
[176,     1] loss: 504.007
[177,     1] loss: 513.494
[178,     1] loss: 544.338
[179,     1] loss: 635.611
[180,     1] loss: 610.388
[181,     1] loss: 621.549
[182,     1] loss: 552.927
[183,     1] loss: 516.152
[184,     1] loss: 554.208
[185,     1] loss: 533.080
[186,     1] loss: 479.708
[187,     1] loss: 444.078
[188,     1] loss: 524.087
[189,     1] loss: 752.152
[190,     1] loss: 1498.046
[191,     1] loss: 1105.848
[192,     1] loss: 901.074
[193,     1] loss: 878.216
[194,     1] loss: 968.303
[195,     1] loss: 1008.763
[196,     1] loss: 912.748
[197,     1] loss: 936.715
[198,     1] loss: 941.347
[199,     1] loss: 928.445
[200,     1] loss: 885.216
Finished Training
Total time taken: 28.69406819343567
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.380
[2,     1] loss: 1231.048
[3,     1] loss: 1232.888
[4,     1] loss: 1237.538
[5,     1] loss: 1235.429
[6,     1] loss: 1229.394
[7,     1] loss: 1229.980
[8,     1] loss: 1233.424
[9,     1] loss: 1224.996
[10,     1] loss: 1219.738
[11,     1] loss: 1210.781
[12,     1] loss: 1193.961
[13,     1] loss: 1159.236
[14,     1] loss: 1146.046
[15,     1] loss: 1103.729
[16,     1] loss: 1097.534
[17,     1] loss: 1058.760
[18,     1] loss: 1034.691
[19,     1] loss: 1024.206
[20,     1] loss: 1023.865
[21,     1] loss: 1019.856
[22,     1] loss: 988.366
[23,     1] loss: 1009.548
[24,     1] loss: 986.602
[25,     1] loss: 979.555
[26,     1] loss: 974.102
[27,     1] loss: 962.331
[28,     1] loss: 949.007
[29,     1] loss: 958.568
[30,     1] loss: 933.114
[31,     1] loss: 930.378
[32,     1] loss: 926.289
[33,     1] loss: 886.667
[34,     1] loss: 909.007
[35,     1] loss: 881.082
[36,     1] loss: 880.306
[37,     1] loss: 880.271
[38,     1] loss: 824.834
[39,     1] loss: 852.047
[40,     1] loss: 938.845
[41,     1] loss: 983.495
[42,     1] loss: 834.419
[43,     1] loss: 937.403
[44,     1] loss: 851.725
[45,     1] loss: 912.656
[46,     1] loss: 842.655
[47,     1] loss: 849.952
[48,     1] loss: 866.130
[49,     1] loss: 832.543
[50,     1] loss: 825.313
[51,     1] loss: 773.151
[52,     1] loss: 820.577
[53,     1] loss: 816.007
[54,     1] loss: 803.117
[55,     1] loss: 758.257
[56,     1] loss: 792.635
[57,     1] loss: 759.773
[58,     1] loss: 752.977
[59,     1] loss: 742.712
[60,     1] loss: 712.246
[61,     1] loss: 763.055
[62,     1] loss: 695.047
[63,     1] loss: 814.253
[64,     1] loss: 825.059
[65,     1] loss: 642.543
[66,     1] loss: 787.028
[67,     1] loss: 661.507
[68,     1] loss: 732.108
[69,     1] loss: 690.427
[70,     1] loss: 731.480
[71,     1] loss: 646.665
[72,     1] loss: 730.083
[73,     1] loss: 650.649
[74,     1] loss: 595.261
[75,     1] loss: 587.962
[76,     1] loss: 597.104
[77,     1] loss: 664.680
[78,     1] loss: 687.740
[79,     1] loss: 741.471
[80,     1] loss: 702.500
[81,     1] loss: 554.164
[82,     1] loss: 693.832
[83,     1] loss: 618.487
[84,     1] loss: 664.579
[85,     1] loss: 606.050
[86,     1] loss: 625.574
[87,     1] loss: 651.564
[88,     1] loss: 579.237
[89,     1] loss: 746.030
[90,     1] loss: 711.109
[91,     1] loss: 616.969
[92,     1] loss: 802.009
[93,     1] loss: 578.761
[94,     1] loss: 688.752
[95,     1] loss: 572.419
[96,     1] loss: 606.988
[97,     1] loss: 537.324
[98,     1] loss: 648.108
[99,     1] loss: 521.258
[100,     1] loss: 519.894
[101,     1] loss: 544.634
[102,     1] loss: 502.461
[103,     1] loss: 493.265
[104,     1] loss: 544.463
[105,     1] loss: 513.763
[106,     1] loss: 461.877
[107,     1] loss: 462.924
[108,     1] loss: 465.466
[109,     1] loss: 489.630
[110,     1] loss: 618.226
Early stopping applied (best metric=0.35607942938804626)
Finished Training
Total time taken: 15.817692041397095
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1237.400
[2,     1] loss: 1241.255
[3,     1] loss: 1237.902
[4,     1] loss: 1232.705
[5,     1] loss: 1237.446
[6,     1] loss: 1233.982
[7,     1] loss: 1229.947
[8,     1] loss: 1227.778
[9,     1] loss: 1227.461
[10,     1] loss: 1214.245
[11,     1] loss: 1199.822
[12,     1] loss: 1182.271
[13,     1] loss: 1166.491
[14,     1] loss: 1140.195
[15,     1] loss: 1120.229
[16,     1] loss: 1085.505
[17,     1] loss: 1051.459
[18,     1] loss: 1049.541
[19,     1] loss: 1018.481
[20,     1] loss: 1009.392
[21,     1] loss: 1012.890
[22,     1] loss: 973.893
[23,     1] loss: 970.221
[24,     1] loss: 1014.701
[25,     1] loss: 953.240
[26,     1] loss: 983.038
[27,     1] loss: 944.629
[28,     1] loss: 967.389
[29,     1] loss: 972.207
[30,     1] loss: 947.314
[31,     1] loss: 950.022
[32,     1] loss: 938.397
[33,     1] loss: 908.003
[34,     1] loss: 894.991
[35,     1] loss: 900.861
[36,     1] loss: 875.239
[37,     1] loss: 855.440
[38,     1] loss: 826.731
[39,     1] loss: 844.345
[40,     1] loss: 860.782
[41,     1] loss: 833.396
[42,     1] loss: 830.441
[43,     1] loss: 823.083
[44,     1] loss: 779.745
[45,     1] loss: 836.396
[46,     1] loss: 894.821
[47,     1] loss: 807.999
[48,     1] loss: 807.289
[49,     1] loss: 776.770
[50,     1] loss: 762.981
[51,     1] loss: 746.622
[52,     1] loss: 744.119
[53,     1] loss: 739.631
[54,     1] loss: 776.428
[55,     1] loss: 831.836
[56,     1] loss: 727.777
[57,     1] loss: 742.363
[58,     1] loss: 719.957
[59,     1] loss: 700.535
[60,     1] loss: 750.098
[61,     1] loss: 743.952
[62,     1] loss: 729.629
[63,     1] loss: 663.132
[64,     1] loss: 773.566
[65,     1] loss: 748.366
[66,     1] loss: 661.367
[67,     1] loss: 700.667
[68,     1] loss: 783.247
[69,     1] loss: 625.142
[70,     1] loss: 816.821
[71,     1] loss: 798.027
[72,     1] loss: 705.768
[73,     1] loss: 725.296
[74,     1] loss: 606.310
[75,     1] loss: 672.323
[76,     1] loss: 729.911
[77,     1] loss: 623.717
[78,     1] loss: 672.118
[79,     1] loss: 623.224
[80,     1] loss: 627.033
[81,     1] loss: 748.691
[82,     1] loss: 754.494
[83,     1] loss: 644.887
[84,     1] loss: 687.620
[85,     1] loss: 647.550
Early stopping applied (best metric=0.3693816065788269)
Finished Training
Total time taken: 12.168298244476318
{'Hydroxylation-K Validation Accuracy': 0.7351950354609929, 'Hydroxylation-K Validation Sensitivity': 0.6503703703703704, 'Hydroxylation-K Validation Specificity': 0.7578947368421053, 'Hydroxylation-K Validation Precision': 0.4082669028025065, 'Hydroxylation-K AUC ROC': 0.7795711500974659, 'Hydroxylation-K AUC PR': 0.5355027617789693, 'Hydroxylation-K MCC': 0.35249610285910427, 'Hydroxylation-K F1': 0.49246594600980564, 'Validation Loss (Hydroxylation-K)': 0.4844299753506978, 'Hydroxylation-P Validation Accuracy': 0.8164220936331489, 'Hydroxylation-P Validation Sensitivity': 0.8067724867724868, 'Hydroxylation-P Validation Specificity': 0.8184971819043344, 'Hydroxylation-P Validation Precision': 0.4958926091832817, 'Hydroxylation-P AUC ROC': 0.8647815708656649, 'Hydroxylation-P AUC PR': 0.6140656200905892, 'Hydroxylation-P MCC': 0.5286104341470298, 'Hydroxylation-P F1': 0.6122116525500145, 'Validation Loss (Hydroxylation-P)': 0.3486201584339142, 'Validation Loss (total)': 0.8330501357714335, 'TimeToTrain': 18.675919246673583}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007298194302963538,
 'learning_rate_Hydroxylation-K': 0.00908243275849474,
 'learning_rate_Hydroxylation-P': 0.00047979005364264535,
 'log_base': 1.1957244571603618,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1943047590,
 'sample_weights': [1.5461153978201796, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.281934080653553,
 'weight_decay_Hydroxylation-K': 8.806873096384182,
 'weight_decay_Hydroxylation-P': 4.770534668727512}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3047.900
[2,     1] loss: 3034.561
[3,     1] loss: 3052.814
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005757835669146407,
 'learning_rate_Hydroxylation-K': 0.007371832053284962,
 'learning_rate_Hydroxylation-P': 0.0001376855596453134,
 'log_base': 1.0941030174130344,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4061156454,
 'sample_weights': [9.339424942511274, 1.1674730918329654],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.880533114929678,
 'weight_decay_Hydroxylation-K': 8.762832234808752,
 'weight_decay_Hydroxylation-P': 5.637742191112589}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6055.386
[2,     1] loss: 6036.057
[3,     1] loss: 6028.984
[4,     1] loss: 6041.137
[5,     1] loss: 6011.646
[6,     1] loss: 6032.029
[7,     1] loss: 6018.149
[8,     1] loss: 6008.595
[9,     1] loss: 6030.541
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00036544123730686707,
 'learning_rate_Hydroxylation-K': 0.009305506315806072,
 'learning_rate_Hydroxylation-P': 0.008397031837586923,
 'log_base': 2.375265757819025,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 595498930,
 'sample_weights': [18.562802536577326, 2.3204397062840454],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.692316841387736,
 'weight_decay_Hydroxylation-K': 0.7154711023302376,
 'weight_decay_Hydroxylation-P': 6.049092024249559}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1316.033
[2,     1] loss: 1319.448
[3,     1] loss: 1319.009
[4,     1] loss: 1318.933
[5,     1] loss: 1315.761
[6,     1] loss: 1316.986
[7,     1] loss: 1317.011
[8,     1] loss: 1313.569
[9,     1] loss: 1314.585
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038015847680415125,
 'learning_rate_Hydroxylation-K': 0.006340865745702149,
 'learning_rate_Hydroxylation-P': 0.0068988971385871515,
 'log_base': 1.75415306479006,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3241991194,
 'sample_weights': [1.9297481728237433, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.0415263241688244,
 'weight_decay_Hydroxylation-K': 1.7817769301969868,
 'weight_decay_Hydroxylation-P': 0.8229363866773376}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1545.664
[2,     1] loss: 1534.958
[3,     1] loss: 1535.423
[4,     1] loss: 1533.105
[5,     1] loss: 1534.889
[6,     1] loss: 1537.439
[7,     1] loss: 1533.411
[8,     1] loss: 1534.812
[9,     1] loss: 1530.815
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007284179001111713,
 'learning_rate_Hydroxylation-K': 0.009130593005610931,
 'learning_rate_Hydroxylation-P': 0.003299400602340175,
 'log_base': 1.232029738451976,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1550345434,
 'sample_weights': [2.9706125832349293, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.739331015158774,
 'weight_decay_Hydroxylation-K': 9.369564217802369,
 'weight_decay_Hydroxylation-P': 4.89169140704445}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2598.511
[2,     1] loss: 2594.339
[3,     1] loss: 2597.976
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008446862654865227,
 'learning_rate_Hydroxylation-K': 0.005288881470980546,
 'learning_rate_Hydroxylation-P': 0.0009788447039031727,
 'log_base': 1.1379625180356654,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2191349657,
 'sample_weights': [8.000666730711826, 1.0001218685652504],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.536950826233834,
 'weight_decay_Hydroxylation-K': 8.525402524395735,
 'weight_decay_Hydroxylation-P': 5.133406217741724}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4181.561
[2,     1] loss: 4204.063
[3,     1] loss: 4183.120
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005822286165090808,
 'learning_rate_Hydroxylation-K': 0.006349550332811794,
 'learning_rate_Hydroxylation-P': 0.0055098879976328684,
 'log_base': 1.8072783695350623,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 177981217,
 'sample_weights': [12.917447522128596, 1.6147431442598532],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.32198773126306,
 'weight_decay_Hydroxylation-K': 5.257538911505998,
 'weight_decay_Hydroxylation-P': 1.7856467958009141}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1510.097
[2,     1] loss: 1501.771
[3,     1] loss: 1500.281
[4,     1] loss: 1504.206
[5,     1] loss: 1501.874
[6,     1] loss: 1499.966
[7,     1] loss: 1498.989
[8,     1] loss: 1495.730
[9,     1] loss: 1491.048
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019564476974488244,
 'learning_rate_Hydroxylation-K': 0.003936623615683468,
 'learning_rate_Hydroxylation-P': 0.009669614762487353,
 'log_base': 2.442906722544972,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4260013296,
 'sample_weights': [2.820853238387439, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.429526506879607,
 'weight_decay_Hydroxylation-K': 2.7588963431653477,
 'weight_decay_Hydroxylation-P': 3.1864136735223454}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.899
[2,     1] loss: 1305.623
[3,     1] loss: 1303.100
[4,     1] loss: 1299.535
[5,     1] loss: 1299.388
[6,     1] loss: 1294.212
[7,     1] loss: 1287.902
[8,     1] loss: 1279.400
[9,     1] loss: 1265.252
[10,     1] loss: 1236.704
[11,     1] loss: 1220.726
[12,     1] loss: 1185.428
[13,     1] loss: 1161.467
[14,     1] loss: 1146.815
[15,     1] loss: 1129.191
[16,     1] loss: 1086.864
[17,     1] loss: 1101.361
[18,     1] loss: 1066.492
[19,     1] loss: 1060.965
[20,     1] loss: 1095.997
[21,     1] loss: 1078.433
[22,     1] loss: 1084.206
[23,     1] loss: 1111.273
[24,     1] loss: 1053.484
[25,     1] loss: 1018.254
[26,     1] loss: 1030.890
[27,     1] loss: 1034.937
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013700278612273017,
 'learning_rate_Hydroxylation-K': 0.004638574162985552,
 'learning_rate_Hydroxylation-P': 0.002537559902346748,
 'log_base': 1.0529328533188789,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3534093877,
 'sample_weights': [1.8690824409136821, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.869522144494582,
 'weight_decay_Hydroxylation-K': 8.006766525715257,
 'weight_decay_Hydroxylation-P': 2.264688970927754}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10535.092
[2,     1] loss: 10509.100
[3,     1] loss: 10481.230
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020379685704449107,
 'learning_rate_Hydroxylation-K': 0.004352941657594292,
 'learning_rate_Hydroxylation-P': 0.008189634207170334,
 'log_base': 2.7809291988386615,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 791809947,
 'sample_weights': [32.36643064979743, 4.045959691834705],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.665366353847922,
 'weight_decay_Hydroxylation-K': 0.6508283876714449,
 'weight_decay_Hydroxylation-P': 5.614384021777812}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.115
[2,     1] loss: 1252.728
[3,     1] loss: 1251.888
[4,     1] loss: 1248.803
[5,     1] loss: 1248.426
[6,     1] loss: 1246.616
[7,     1] loss: 1239.874
[8,     1] loss: 1232.773
[9,     1] loss: 1216.506
[10,     1] loss: 1195.067
[11,     1] loss: 1158.097
[12,     1] loss: 1149.469
[13,     1] loss: 1133.402
[14,     1] loss: 1099.744
[15,     1] loss: 1074.141
[16,     1] loss: 1045.139
[17,     1] loss: 1030.335
[18,     1] loss: 979.548
[19,     1] loss: 978.347
[20,     1] loss: 985.669
[21,     1] loss: 1008.434
[22,     1] loss: 952.844
[23,     1] loss: 977.993
[24,     1] loss: 965.426
[25,     1] loss: 945.430
[26,     1] loss: 967.207
[27,     1] loss: 924.022
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023510291321927846,
 'learning_rate_Hydroxylation-K': 0.004396712395941633,
 'learning_rate_Hydroxylation-P': 0.005139512400883225,
 'log_base': 2.0651687280287265,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 644640305,
 'sample_weights': [1.6322520940509717, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.177502061699554,
 'weight_decay_Hydroxylation-K': 1.1432162502821044,
 'weight_decay_Hydroxylation-P': 1.2598907294260333}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1393.355
[2,     1] loss: 1393.910
[3,     1] loss: 1392.573
[4,     1] loss: 1395.462
[5,     1] loss: 1390.385
[6,     1] loss: 1388.565
[7,     1] loss: 1386.858
[8,     1] loss: 1377.512
[9,     1] loss: 1367.413
[10,     1] loss: 1347.069
[11,     1] loss: 1321.151
[12,     1] loss: 1285.795
[13,     1] loss: 1279.339
[14,     1] loss: 1226.742
[15,     1] loss: 1226.281
[16,     1] loss: 1221.600
[17,     1] loss: 1184.822
[18,     1] loss: 1161.301
[19,     1] loss: 1224.722
[20,     1] loss: 1196.881
[21,     1] loss: 1172.547
[22,     1] loss: 1163.649
[23,     1] loss: 1178.667
[24,     1] loss: 1166.940
[25,     1] loss: 1139.075
[26,     1] loss: 1111.833
[27,     1] loss: 1115.800
[28,     1] loss: 1123.924
[29,     1] loss: 1081.733
[30,     1] loss: 1055.062
[31,     1] loss: 1051.386
[32,     1] loss: 1056.085
[33,     1] loss: 1075.323
[34,     1] loss: 1036.196
[35,     1] loss: 1042.572
[36,     1] loss: 1107.902
[37,     1] loss: 955.882
[38,     1] loss: 1061.338
[39,     1] loss: 1004.917
[40,     1] loss: 1029.623
[41,     1] loss: 971.889
[42,     1] loss: 1044.334
[43,     1] loss: 942.245
[44,     1] loss: 970.623
[45,     1] loss: 915.429
[46,     1] loss: 972.341
[47,     1] loss: 929.915
[48,     1] loss: 957.004
[49,     1] loss: 871.219
[50,     1] loss: 964.687
[51,     1] loss: 892.215
[52,     1] loss: 959.190
[53,     1] loss: 867.067
[54,     1] loss: 927.226
[55,     1] loss: 793.664
[56,     1] loss: 896.457
[57,     1] loss: 822.560
[58,     1] loss: 903.241
[59,     1] loss: 859.399
[60,     1] loss: 933.222
[61,     1] loss: 811.469
[62,     1] loss: 914.036
[63,     1] loss: 849.240
[64,     1] loss: 761.221
[65,     1] loss: 935.822
[66,     1] loss: 768.549
[67,     1] loss: 793.534
[68,     1] loss: 829.475
[69,     1] loss: 733.762
[70,     1] loss: 802.797
[71,     1] loss: 781.680
[72,     1] loss: 767.958
[73,     1] loss: 723.847
[74,     1] loss: 694.391
[75,     1] loss: 752.857
[76,     1] loss: 643.234
[77,     1] loss: 679.945
[78,     1] loss: 615.109
[79,     1] loss: 641.419
[80,     1] loss: 607.018
[81,     1] loss: 609.001
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032478583617882068,
 'learning_rate_Hydroxylation-K': 0.008338752809409839,
 'learning_rate_Hydroxylation-P': 0.003225381445242448,
 'log_base': 1.0398289674957666,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3140375990,
 'sample_weights': [2.3020072819602158, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.069996135665923,
 'weight_decay_Hydroxylation-K': 6.710217912524735,
 'weight_decay_Hydroxylation-P': 0.1452369138409999}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14104.697
[2,     1] loss: 13864.134
[3,     1] loss: 13885.295
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00450314954713076,
 'learning_rate_Hydroxylation-K': 0.0005692778425112748,
 'learning_rate_Hydroxylation-P': 0.005178613007827187,
 'log_base': 2.8643296194432426,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2255050407,
 'sample_weights': [42.744588853270564, 5.34327944330995],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.782178433461156,
 'weight_decay_Hydroxylation-K': 7.1800909298868065,
 'weight_decay_Hydroxylation-P': 0.5288971281067913}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.736
[2,     1] loss: 1247.811
[3,     1] loss: 1243.742
[4,     1] loss: 1240.629
[5,     1] loss: 1244.653
[6,     1] loss: 1241.602
[7,     1] loss: 1241.318
[8,     1] loss: 1238.763
[9,     1] loss: 1240.746
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017175982049080583,
 'learning_rate_Hydroxylation-K': 0.0012794986780054658,
 'learning_rate_Hydroxylation-P': 0.008446202320229683,
 'log_base': 2.7996691388192043,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 753198008,
 'sample_weights': [1.586418968735292, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.342141146335148,
 'weight_decay_Hydroxylation-K': 0.7745321844395021,
 'weight_decay_Hydroxylation-P': 0.47380046870588677}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.229
[2,     1] loss: 1250.797
[3,     1] loss: 1249.932
[4,     1] loss: 1247.726
[5,     1] loss: 1245.906
[6,     1] loss: 1242.540
[7,     1] loss: 1244.673
[8,     1] loss: 1233.028
[9,     1] loss: 1224.670
[10,     1] loss: 1215.830
[11,     1] loss: 1192.980
[12,     1] loss: 1164.810
[13,     1] loss: 1146.334
[14,     1] loss: 1135.602
[15,     1] loss: 1098.206
[16,     1] loss: 1078.344
[17,     1] loss: 1046.633
[18,     1] loss: 1012.467
[19,     1] loss: 999.969
[20,     1] loss: 1027.625
[21,     1] loss: 1009.964
[22,     1] loss: 1063.340
[23,     1] loss: 1051.484
[24,     1] loss: 958.898
[25,     1] loss: 968.369
[26,     1] loss: 991.481
[27,     1] loss: 966.160
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038276841752142776,
 'learning_rate_Hydroxylation-K': 0.0032752363428253176,
 'learning_rate_Hydroxylation-P': 0.005312896842799976,
 'log_base': 1.8187912260847021,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3760582212,
 'sample_weights': [1.6216038151816687, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.897999433960262,
 'weight_decay_Hydroxylation-K': 7.373324961644264,
 'weight_decay_Hydroxylation-P': 8.374709338023195}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1497.330
[2,     1] loss: 1499.493
[3,     1] loss: 1496.473
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0034941423808856824,
 'learning_rate_Hydroxylation-K': 0.0047571118913496396,
 'learning_rate_Hydroxylation-P': 0.006831570484095275,
 'log_base': 1.0176835232892605,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 352224972,
 'sample_weights': [2.790907657768028, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.46276597013297,
 'weight_decay_Hydroxylation-K': 8.5694835781503,
 'weight_decay_Hydroxylation-P': 4.730620616017384}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31022.465
Exploding loss, terminate run (best metric=0.543799102306366)
Finished Training
Total time taken: 0.20202159881591797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30948.203
Exploding loss, terminate run (best metric=0.5287206768989563)
Finished Training
Total time taken: 0.22602558135986328
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30922.484
Exploding loss, terminate run (best metric=0.526940107345581)
Finished Training
Total time taken: 0.20502018928527832
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30853.385
Exploding loss, terminate run (best metric=0.5361064076423645)
Finished Training
Total time taken: 0.2100226879119873
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31075.416
Exploding loss, terminate run (best metric=0.5287389755249023)
Finished Training
Total time taken: 0.2210252285003662
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31007.695
Exploding loss, terminate run (best metric=0.5326815843582153)
Finished Training
Total time taken: 0.20902466773986816
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30782.199
Exploding loss, terminate run (best metric=0.5263500213623047)
Finished Training
Total time taken: 0.20902395248413086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30939.188
Exploding loss, terminate run (best metric=0.5313137769699097)
Finished Training
Total time taken: 0.22402691841125488
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31026.988
Exploding loss, terminate run (best metric=0.5321604013442993)
Finished Training
Total time taken: 0.20401978492736816
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 31078.199
Exploding loss, terminate run (best metric=0.5287286043167114)
Finished Training
Total time taken: 0.2210235595703125
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31143.787
Exploding loss, terminate run (best metric=0.5358525514602661)
Finished Training
Total time taken: 0.20702242851257324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31010.758
Exploding loss, terminate run (best metric=0.5333062410354614)
Finished Training
Total time taken: 0.22802424430847168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 30962.082
Exploding loss, terminate run (best metric=0.5268637537956238)
Finished Training
Total time taken: 0.2080221176147461
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31061.074
Exploding loss, terminate run (best metric=0.5284926295280457)
Finished Training
Total time taken: 0.22002315521240234
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 30845.785
Exploding loss, terminate run (best metric=0.5416991114616394)
Finished Training
Total time taken: 0.20801949501037598
{'Hydroxylation-K Validation Accuracy': 0.4805555555555555, 'Hydroxylation-K Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-K Validation Specificity': 0.4666666666666667, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6228849902534113, 'Hydroxylation-K AUC PR': 0.34854124220662924, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1792282430213465, 'Validation Loss (Hydroxylation-K)': 0.5572232961654663, 'Hydroxylation-P Validation Accuracy': 0.47817647158350673, 'Hydroxylation-P Validation Sensitivity': 0.5333333333333333, 'Hydroxylation-P Validation Specificity': 0.4666666666666667, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5673271714458307, 'Hydroxylation-P AUC PR': 0.25878089391652814, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.16011130737252802, 'Validation Loss (Hydroxylation-P)': 0.5321169296900431, 'Validation Loss (total)': 1.0893402258555094, 'TimeToTrain': 0.21348970731099445}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004006039040745186,
 'learning_rate_Hydroxylation-K': 0.004046790445798698,
 'learning_rate_Hydroxylation-P': 0.007973546619745236,
 'log_base': 1.284784189136389,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 506214036,
 'sample_weights': [95.30963499603041, 11.88894980173048],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.825026411543913,
 'weight_decay_Hydroxylation-K': 7.0463674863657495,
 'weight_decay_Hydroxylation-P': 2.663093185975109}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2319.783
[2,     1] loss: 2319.375
[3,     1] loss: 2306.135
[4,     1] loss: 2310.209
[5,     1] loss: 2323.478
[6,     1] loss: 2304.347
[7,     1] loss: 2319.546
[8,     1] loss: 2315.111
[9,     1] loss: 2314.733
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002068968547531003,
 'learning_rate_Hydroxylation-K': 0.0020227280288095967,
 'learning_rate_Hydroxylation-P': 0.008604205016528465,
 'log_base': 1.551389757642694,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2062941649,
 'sample_weights': [6.662029998416881, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.1605533270857356,
 'weight_decay_Hydroxylation-K': 2.3310836938248523,
 'weight_decay_Hydroxylation-P': 6.931310895082886}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1712.504
[2,     1] loss: 1709.395
[3,     1] loss: 1713.668
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0005782841574459392,
 'learning_rate_Hydroxylation-K': 0.004350455614969417,
 'learning_rate_Hydroxylation-P': 0.007546296183767979,
 'log_base': 1.0807087574740617,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3374154501,
 'sample_weights': [3.80152291224311, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.802979716954725,
 'weight_decay_Hydroxylation-K': 7.754004577678581,
 'weight_decay_Hydroxylation-P': 5.459886493243665}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7029.967
[2,     1] loss: 7025.399
[3,     1] loss: 7021.595
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003005675580687867,
 'learning_rate_Hydroxylation-K': 0.004559867971131424,
 'learning_rate_Hydroxylation-P': 0.008232106093007794,
 'log_base': 1.1917169236833183,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1443291862,
 'sample_weights': [21.508707708342005, 2.68869203876674],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.5958672503602,
 'weight_decay_Hydroxylation-K': 9.931631063574812,
 'weight_decay_Hydroxylation-P': 5.977406269794072}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3086.875
[2,     1] loss: 3079.164
[3,     1] loss: 3073.429
[4,     1] loss: 3064.964
[5,     1] loss: 3107.602
[6,     1] loss: 3081.865
[7,     1] loss: 3088.600
[8,     1] loss: 3073.420
[9,     1] loss: 3071.198
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011937427174787281,
 'learning_rate_Hydroxylation-K': 0.0017551065688203769,
 'learning_rate_Hydroxylation-P': 0.007077123326929518,
 'log_base': 1.6856905402070863,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4110253147,
 'sample_weights': [9.518187938041446, 1.1898193271077915],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0041743520021793,
 'weight_decay_Hydroxylation-K': 6.363414435864797,
 'weight_decay_Hydroxylation-P': 5.4195210967467515}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1592.109
[2,     1] loss: 1583.092
[3,     1] loss: 1587.872
[4,     1] loss: 1581.799
[5,     1] loss: 1581.812
[6,     1] loss: 1584.517
[7,     1] loss: 1578.886
[8,     1] loss: 1578.416
[9,     1] loss: 1573.045
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003983925320422968,
 'learning_rate_Hydroxylation-K': 0.006060114814947658,
 'learning_rate_Hydroxylation-P': 0.007542561051389608,
 'log_base': 1.075798400363967,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3955624952,
 'sample_weights': [3.19709331370979, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.990249468576595,
 'weight_decay_Hydroxylation-K': 7.9083767630827495,
 'weight_decay_Hydroxylation-P': 5.0501774653993525}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7432.077
[2,     1] loss: 7438.763
[3,     1] loss: 7409.722
[4,     1] loss: 7393.078
[5,     1] loss: 7417.279
[6,     1] loss: 7429.887
[7,     1] loss: 7402.739
[8,     1] loss: 7426.567
[9,     1] loss: 7437.018
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033810525611126063,
 'learning_rate_Hydroxylation-K': 0.0028302514849292016,
 'learning_rate_Hydroxylation-P': 0.005999260897082621,
 'log_base': 1.0785406402478697,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2019566999,
 'sample_weights': [22.84933864887987, 2.8562773621448865],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.39135039247904,
 'weight_decay_Hydroxylation-K': 7.708727251222952,
 'weight_decay_Hydroxylation-P': 4.595643402189931}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7159.500
[2,     1] loss: 7156.891
[3,     1] loss: 7180.534
[4,     1] loss: 7155.255
[5,     1] loss: 7163.087
[6,     1] loss: 7135.575
[7,     1] loss: 7170.234
[8,     1] loss: 7119.353
[9,     1] loss: 7126.889
[10,     1] loss: 7084.963
[11,     1] loss: 7064.296
[12,     1] loss: 6988.850
[13,     1] loss: 6881.738
[14,     1] loss: 6772.881
[15,     1] loss: 6547.138
[16,     1] loss: 6378.179
[17,     1] loss: 6406.676
[18,     1] loss: 6293.390
[19,     1] loss: 6154.703
[20,     1] loss: 5975.373
[21,     1] loss: 5893.985
[22,     1] loss: 5694.558
[23,     1] loss: 6075.405
[24,     1] loss: 6213.549
[25,     1] loss: 6081.206
[26,     1] loss: 5811.111
[27,     1] loss: 5454.855
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003213823284121059,
 'learning_rate_Hydroxylation-K': 0.00036592472438552834,
 'learning_rate_Hydroxylation-P': 0.003102549924717585,
 'log_base': 2.9987193816992583,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 132847730,
 'sample_weights': [22.079991199960862, 2.760105226235867],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.200736650239882,
 'weight_decay_Hydroxylation-K': 9.344711518405498,
 'weight_decay_Hydroxylation-P': 2.624534916152024}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1233.589
[2,     1] loss: 1226.213
[3,     1] loss: 1230.352
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005312422526429594,
 'learning_rate_Hydroxylation-K': 0.0045195273613535735,
 'learning_rate_Hydroxylation-P': 0.005579514586018555,
 'log_base': 1.3420005793949334,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 721075075,
 'sample_weights': [1.520183442426527, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.476789580302029,
 'weight_decay_Hydroxylation-K': 9.398338007128174,
 'weight_decay_Hydroxylation-P': 4.224086411020717}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2106.848
[2,     1] loss: 2112.262
[3,     1] loss: 2110.375
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0055449089649918125,
 'learning_rate_Hydroxylation-K': 0.007754381605945461,
 'learning_rate_Hydroxylation-P': 0.0038225549019215528,
 'log_base': 1.3081506587957858,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2194826637,
 'sample_weights': [5.675261093106074, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.949805932144052,
 'weight_decay_Hydroxylation-K': 9.234188816536475,
 'weight_decay_Hydroxylation-P': 7.683658929827786}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2231.371
[2,     1] loss: 2234.847
[3,     1] loss: 2222.014
[4,     1] loss: 2218.132
[5,     1] loss: 2217.476
[6,     1] loss: 2217.519
[7,     1] loss: 2219.303
[8,     1] loss: 2219.921
[9,     1] loss: 2224.727
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0021745699907657996,
 'learning_rate_Hydroxylation-K': 0.0017643737376434225,
 'learning_rate_Hydroxylation-P': 0.0044821374100979475,
 'log_base': 2.9600447994671466,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1807710510,
 'sample_weights': [6.21501664612775, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.887206709646926,
 'weight_decay_Hydroxylation-K': 9.121169931881878,
 'weight_decay_Hydroxylation-P': 1.1989251277988513}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.915
[2,     1] loss: 1232.329
[3,     1] loss: 1234.260
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009518402959576646,
 'learning_rate_Hydroxylation-K': 0.007018464114968792,
 'learning_rate_Hydroxylation-P': 0.00031715368483925966,
 'log_base': 1.0467161206814448,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3244355117,
 'sample_weights': [1.5383674656469928, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.560917097123916,
 'weight_decay_Hydroxylation-K': 8.69393222395024,
 'weight_decay_Hydroxylation-P': 6.376604140579621}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11933.918
[2,     1] loss: 11902.040
[3,     1] loss: 11839.679
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035625697047737036,
 'learning_rate_Hydroxylation-K': 0.005821671749251464,
 'learning_rate_Hydroxylation-P': 0.00919905380217403,
 'log_base': 1.9486691794003745,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3345720586,
 'sample_weights': [36.564281227860405, 4.570711228850237],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.51474770764134,
 'weight_decay_Hydroxylation-K': 7.018313456607596,
 'weight_decay_Hydroxylation-P': 0.1349762186273109}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1436.657
[2,     1] loss: 1434.737
[3,     1] loss: 1434.448
[4,     1] loss: 1434.963
[5,     1] loss: 1438.975
[6,     1] loss: 1436.743
[7,     1] loss: 1432.545
[8,     1] loss: 1434.353
[9,     1] loss: 1435.650
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014674878188327487,
 'learning_rate_Hydroxylation-K': 0.007682982289684352,
 'learning_rate_Hydroxylation-P': 0.004477125952183678,
 'log_base': 1.0375722562762033,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2748772488,
 'sample_weights': [2.502363016698644, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.508831185613053,
 'weight_decay_Hydroxylation-K': 8.61131821751264,
 'weight_decay_Hydroxylation-P': 1.4939779657940198}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14688.258
[2,     1] loss: 14674.234
[3,     1] loss: 14695.785
[4,     1] loss: 14696.676
[5,     1] loss: 14665.084
[6,     1] loss: 14683.600
[7,     1] loss: 14649.398
[8,     1] loss: 14710.794
[9,     1] loss: 14619.742
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002825462297517842,
 'learning_rate_Hydroxylation-K': 0.007892743361532709,
 'learning_rate_Hydroxylation-P': 0.0038886902671512033,
 'log_base': 2.0059279080305132,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3363176467,
 'sample_weights': [45.26245959866008, 5.658025411295133],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.441836990804664,
 'weight_decay_Hydroxylation-K': 5.0864716195468445,
 'weight_decay_Hydroxylation-P': 2.7292888074785506}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1416.837
[2,     1] loss: 1412.253
[3,     1] loss: 1416.714
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00418079725234662,
 'learning_rate_Hydroxylation-K': 0.0037925966369182886,
 'learning_rate_Hydroxylation-P': 0.00599655691875827,
 'log_base': 1.8659206718156163,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1532732860,
 'sample_weights': [2.398257373151469, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.776369155876118,
 'weight_decay_Hydroxylation-K': 3.6518678559178834,
 'weight_decay_Hydroxylation-P': 5.416653721381306}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1472.436
[2,     1] loss: 1473.218
[3,     1] loss: 1474.628
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008231830655714721,
 'learning_rate_Hydroxylation-K': 0.009482883257233328,
 'learning_rate_Hydroxylation-P': 0.006832754671941162,
 'log_base': 1.1489640154488805,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3513106035,
 'sample_weights': [2.676442268490487, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.2668367679660255,
 'weight_decay_Hydroxylation-K': 2.1157622623092522,
 'weight_decay_Hydroxylation-P': 7.313885220117292}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3898.932
[2,     1] loss: 3894.942
[3,     1] loss: 3894.496
[4,     1] loss: 3895.272
[5,     1] loss: 3908.525
[6,     1] loss: 3931.103
[7,     1] loss: 3928.410
[8,     1] loss: 3873.813
[9,     1] loss: 3879.811
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003186217393798245,
 'learning_rate_Hydroxylation-K': 0.007820908147106924,
 'learning_rate_Hydroxylation-P': 0.004223671414984108,
 'log_base': 1.2043108788514614,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3048574369,
 'sample_weights': [12.022432445746727, 1.5028619497652707],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.7635318703095475,
 'weight_decay_Hydroxylation-K': 7.6893112074677195,
 'weight_decay_Hydroxylation-P': 3.7769746925404655}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2900.981
[2,     1] loss: 2918.884
[3,     1] loss: 2924.955
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014150397719519747,
 'learning_rate_Hydroxylation-K': 0.002041920217393226,
 'learning_rate_Hydroxylation-P': 0.005339328448530385,
 'log_base': 2.449655297313472,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2365834802,
 'sample_weights': [8.979965738573037, 1.1225389603641873],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.78181867406405,
 'weight_decay_Hydroxylation-K': 1.9988035876723576,
 'weight_decay_Hydroxylation-P': 3.6657071691890164}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1303.716
[2,     1] loss: 1301.673
[3,     1] loss: 1300.440
[4,     1] loss: 1301.137
[5,     1] loss: 1299.197
[6,     1] loss: 1300.110
[7,     1] loss: 1295.925
[8,     1] loss: 1297.170
[9,     1] loss: 1293.387
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.000911017902350394,
 'learning_rate_Hydroxylation-K': 0.004811093486928727,
 'learning_rate_Hydroxylation-P': 0.0018575157568081013,
 'log_base': 2.3485548866429924,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 380525472,
 'sample_weights': [1.8633273527871501, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.759633322261343,
 'weight_decay_Hydroxylation-K': 2.8536139909785403,
 'weight_decay_Hydroxylation-P': 0.9275469783349884}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.867
[2,     1] loss: 1322.191
[3,     1] loss: 1320.297
[4,     1] loss: 1320.654
[5,     1] loss: 1322.279
[6,     1] loss: 1319.081
[7,     1] loss: 1318.707
[8,     1] loss: 1317.095
[9,     1] loss: 1317.101
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0019313655187315087,
 'learning_rate_Hydroxylation-K': 0.00017102415940623575,
 'learning_rate_Hydroxylation-P': 0.0008032252826485962,
 'log_base': 2.683818863277243,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3289924698,
 'sample_weights': [1.9553089270777186, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.196216262026033,
 'weight_decay_Hydroxylation-K': 9.282529186670468,
 'weight_decay_Hydroxylation-P': 3.0426859464925173}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.376
[2,     1] loss: 1266.583
[3,     1] loss: 1268.820
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050284715060430335,
 'learning_rate_Hydroxylation-K': 0.00536863076210531,
 'learning_rate_Hydroxylation-P': 0.002094172946020826,
 'log_base': 2.35021887969641,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2414070982,
 'sample_weights': [1.6910193208145339, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.305792100525265,
 'weight_decay_Hydroxylation-K': 2.1164364809842455,
 'weight_decay_Hydroxylation-P': 1.5227044188039331}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1337.261
[2,     1] loss: 1322.386
[3,     1] loss: 1319.329
[4,     1] loss: 1318.990
[5,     1] loss: 1322.693
[6,     1] loss: 1321.726
[7,     1] loss: 1320.804
[8,     1] loss: 1326.753
[9,     1] loss: 1315.945
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00301188146296201,
 'learning_rate_Hydroxylation-K': 0.001839564554425066,
 'learning_rate_Hydroxylation-P': 0.005385192396228043,
 'log_base': 1.7748880576916957,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4265277525,
 'sample_weights': [1.953688251821407, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.240619491569979,
 'weight_decay_Hydroxylation-K': 5.3307048507316885,
 'weight_decay_Hydroxylation-P': 0.6970870249194141}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1515.976
[2,     1] loss: 1524.567
[3,     1] loss: 1529.530
[4,     1] loss: 1514.303
[5,     1] loss: 1514.573
[6,     1] loss: 1512.332
[7,     1] loss: 1508.349
[8,     1] loss: 1499.579
[9,     1] loss: 1482.489
[10,     1] loss: 1450.692
[11,     1] loss: 1410.495
[12,     1] loss: 1372.760
[13,     1] loss: 1354.857
[14,     1] loss: 1326.099
[15,     1] loss: 1299.447
[16,     1] loss: 1247.916
[17,     1] loss: 1242.331
[18,     1] loss: 1304.216
[19,     1] loss: 1267.471
[20,     1] loss: 1259.277
[21,     1] loss: 1250.282
[22,     1] loss: 1217.449
[23,     1] loss: 1219.640
[24,     1] loss: 1180.031
[25,     1] loss: 1264.328
[26,     1] loss: 1171.146
[27,     1] loss: 1225.299
[28,     1] loss: 1189.009
[29,     1] loss: 1240.263
[30,     1] loss: 1188.603
[31,     1] loss: 1212.797
[32,     1] loss: 1141.750
[33,     1] loss: 1088.283
[34,     1] loss: 1119.588
[35,     1] loss: 1161.245
[36,     1] loss: 1087.560
[37,     1] loss: 1081.916
[38,     1] loss: 1053.764
[39,     1] loss: 1010.910
[40,     1] loss: 1056.560
[41,     1] loss: 1045.412
[42,     1] loss: 1107.011
[43,     1] loss: 1091.052
[44,     1] loss: 1104.550
[45,     1] loss: 995.331
[46,     1] loss: 1000.142
[47,     1] loss: 966.635
[48,     1] loss: 951.007
[49,     1] loss: 888.758
[50,     1] loss: 976.922
[51,     1] loss: 1086.512
[52,     1] loss: 979.669
[53,     1] loss: 927.784
[54,     1] loss: 876.418
[55,     1] loss: 896.932
[56,     1] loss: 864.515
[57,     1] loss: 871.118
[58,     1] loss: 793.525
[59,     1] loss: 854.486
[60,     1] loss: 805.801
[61,     1] loss: 817.580
[62,     1] loss: 833.386
[63,     1] loss: 964.186
[64,     1] loss: 1013.567
[65,     1] loss: 859.610
[66,     1] loss: 818.088
[67,     1] loss: 828.388
[68,     1] loss: 812.473
[69,     1] loss: 821.009
[70,     1] loss: 770.771
[71,     1] loss: 776.918
[72,     1] loss: 714.431
[73,     1] loss: 736.897
[74,     1] loss: 732.514
[75,     1] loss: 703.311
[76,     1] loss: 747.003
[77,     1] loss: 761.369
[78,     1] loss: 927.640
[79,     1] loss: 755.170
[80,     1] loss: 705.422
[81,     1] loss: 797.974
[82,     1] loss: 667.474
[83,     1] loss: 810.990
[84,     1] loss: 655.367
[85,     1] loss: 754.091
[86,     1] loss: 614.549
[87,     1] loss: 715.982
[88,     1] loss: 659.701
[89,     1] loss: 739.641
[90,     1] loss: 597.089
[91,     1] loss: 670.088
[92,     1] loss: 687.387
[93,     1] loss: 715.836
[94,     1] loss: 754.390
[95,     1] loss: 643.880
[96,     1] loss: 816.698
Early stopping applied (best metric=0.366452693939209)
Finished Training
Total time taken: 13.783472299575806
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1524.179
[2,     1] loss: 1522.126
[3,     1] loss: 1517.714
[4,     1] loss: 1513.670
[5,     1] loss: 1520.687
[6,     1] loss: 1519.570
[7,     1] loss: 1518.266
[8,     1] loss: 1513.719
[9,     1] loss: 1511.028
[10,     1] loss: 1508.566
[11,     1] loss: 1495.747
[12,     1] loss: 1476.261
[13,     1] loss: 1457.729
[14,     1] loss: 1445.941
[15,     1] loss: 1403.629
[16,     1] loss: 1387.157
[17,     1] loss: 1353.179
[18,     1] loss: 1335.192
[19,     1] loss: 1301.957
[20,     1] loss: 1329.411
[21,     1] loss: 1282.368
[22,     1] loss: 1287.770
[23,     1] loss: 1274.376
[24,     1] loss: 1190.699
[25,     1] loss: 1283.870
[26,     1] loss: 1224.199
[27,     1] loss: 1173.896
[28,     1] loss: 1183.511
[29,     1] loss: 1162.835
[30,     1] loss: 1203.411
[31,     1] loss: 1152.757
[32,     1] loss: 1173.190
[33,     1] loss: 1170.561
[34,     1] loss: 1160.383
[35,     1] loss: 1074.119
[36,     1] loss: 1072.784
[37,     1] loss: 1055.466
[38,     1] loss: 1041.399
[39,     1] loss: 1020.276
[40,     1] loss: 1017.275
[41,     1] loss: 977.766
[42,     1] loss: 985.065
[43,     1] loss: 1035.595
[44,     1] loss: 993.849
[45,     1] loss: 974.941
[46,     1] loss: 973.179
[47,     1] loss: 944.942
[48,     1] loss: 946.854
[49,     1] loss: 942.990
[50,     1] loss: 956.743
[51,     1] loss: 922.103
[52,     1] loss: 850.765
[53,     1] loss: 873.075
[54,     1] loss: 845.806
[55,     1] loss: 946.165
[56,     1] loss: 918.772
[57,     1] loss: 837.240
[58,     1] loss: 832.509
[59,     1] loss: 774.946
[60,     1] loss: 794.597
[61,     1] loss: 804.330
[62,     1] loss: 734.962
[63,     1] loss: 723.020
[64,     1] loss: 766.368
[65,     1] loss: 797.176
[66,     1] loss: 774.476
[67,     1] loss: 729.964
[68,     1] loss: 886.965
[69,     1] loss: 711.092
[70,     1] loss: 657.724
[71,     1] loss: 670.599
[72,     1] loss: 657.457
[73,     1] loss: 690.850
[74,     1] loss: 654.673
[75,     1] loss: 702.803
[76,     1] loss: 624.355
[77,     1] loss: 666.876
[78,     1] loss: 683.610
[79,     1] loss: 581.086
[80,     1] loss: 553.160
[81,     1] loss: 686.014
Early stopping applied (best metric=0.38309404253959656)
Finished Training
Total time taken: 11.592238426208496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1527.021
[2,     1] loss: 1523.059
[3,     1] loss: 1520.913
[4,     1] loss: 1520.837
[5,     1] loss: 1522.353
[6,     1] loss: 1517.097
[7,     1] loss: 1515.633
[8,     1] loss: 1509.909
[9,     1] loss: 1504.936
[10,     1] loss: 1491.185
[11,     1] loss: 1465.125
[12,     1] loss: 1440.921
[13,     1] loss: 1406.936
[14,     1] loss: 1404.187
[15,     1] loss: 1373.154
[16,     1] loss: 1353.366
[17,     1] loss: 1324.633
[18,     1] loss: 1333.017
[19,     1] loss: 1313.547
[20,     1] loss: 1265.116
[21,     1] loss: 1323.047
[22,     1] loss: 1224.019
[23,     1] loss: 1213.233
[24,     1] loss: 1230.636
[25,     1] loss: 1247.491
[26,     1] loss: 1193.235
[27,     1] loss: 1194.502
[28,     1] loss: 1209.070
[29,     1] loss: 1147.476
[30,     1] loss: 1129.703
[31,     1] loss: 1147.427
[32,     1] loss: 1133.225
[33,     1] loss: 1105.340
[34,     1] loss: 1107.674
[35,     1] loss: 1058.628
[36,     1] loss: 1031.131
[37,     1] loss: 1079.726
[38,     1] loss: 1123.576
[39,     1] loss: 1054.723
[40,     1] loss: 1060.531
[41,     1] loss: 1086.112
[42,     1] loss: 1014.917
[43,     1] loss: 952.099
[44,     1] loss: 965.150
[45,     1] loss: 1020.186
[46,     1] loss: 953.162
[47,     1] loss: 1019.132
[48,     1] loss: 910.908
[49,     1] loss: 940.008
[50,     1] loss: 870.157
[51,     1] loss: 871.760
[52,     1] loss: 925.639
[53,     1] loss: 914.155
[54,     1] loss: 908.629
[55,     1] loss: 918.776
[56,     1] loss: 875.537
[57,     1] loss: 845.328
[58,     1] loss: 881.348
[59,     1] loss: 781.727
[60,     1] loss: 799.498
[61,     1] loss: 810.577
[62,     1] loss: 846.998
[63,     1] loss: 901.611
[64,     1] loss: 960.862
[65,     1] loss: 846.969
[66,     1] loss: 832.639
[67,     1] loss: 829.475
[68,     1] loss: 848.255
[69,     1] loss: 872.133
[70,     1] loss: 823.723
[71,     1] loss: 794.902
[72,     1] loss: 824.380
[73,     1] loss: 760.620
[74,     1] loss: 777.214
[75,     1] loss: 776.373
[76,     1] loss: 760.313
[77,     1] loss: 734.607
[78,     1] loss: 756.197
Early stopping applied (best metric=0.36666175723075867)
Finished Training
Total time taken: 11.314208030700684
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1527.814
[2,     1] loss: 1523.297
[3,     1] loss: 1520.705
[4,     1] loss: 1520.208
[5,     1] loss: 1520.784
[6,     1] loss: 1522.312
[7,     1] loss: 1520.768
[8,     1] loss: 1517.906
[9,     1] loss: 1518.025
[10,     1] loss: 1512.104
[11,     1] loss: 1511.785
[12,     1] loss: 1495.059
[13,     1] loss: 1489.461
[14,     1] loss: 1465.586
[15,     1] loss: 1438.900
[16,     1] loss: 1426.554
[17,     1] loss: 1385.695
[18,     1] loss: 1349.772
[19,     1] loss: 1328.682
[20,     1] loss: 1287.040
[21,     1] loss: 1313.184
[22,     1] loss: 1280.766
[23,     1] loss: 1315.189
[24,     1] loss: 1200.317
[25,     1] loss: 1249.267
[26,     1] loss: 1232.714
[27,     1] loss: 1163.446
[28,     1] loss: 1176.891
[29,     1] loss: 1147.338
[30,     1] loss: 1162.577
[31,     1] loss: 1168.244
[32,     1] loss: 1132.211
[33,     1] loss: 1194.793
[34,     1] loss: 1124.073
[35,     1] loss: 1187.091
[36,     1] loss: 1093.356
[37,     1] loss: 1121.988
[38,     1] loss: 1041.478
[39,     1] loss: 1029.951
[40,     1] loss: 1080.309
[41,     1] loss: 993.787
[42,     1] loss: 1012.219
[43,     1] loss: 1004.362
[44,     1] loss: 1015.596
[45,     1] loss: 1007.937
[46,     1] loss: 1104.884
[47,     1] loss: 957.424
[48,     1] loss: 945.107
[49,     1] loss: 909.838
[50,     1] loss: 983.827
[51,     1] loss: 952.538
[52,     1] loss: 898.623
[53,     1] loss: 862.570
[54,     1] loss: 830.206
[55,     1] loss: 847.051
[56,     1] loss: 876.580
[57,     1] loss: 810.557
[58,     1] loss: 852.566
[59,     1] loss: 738.814
[60,     1] loss: 816.413
[61,     1] loss: 735.020
[62,     1] loss: 783.937
[63,     1] loss: 773.370
[64,     1] loss: 743.131
[65,     1] loss: 743.868
[66,     1] loss: 679.627
[67,     1] loss: 695.824
[68,     1] loss: 770.446
[69,     1] loss: 691.917
[70,     1] loss: 636.091
[71,     1] loss: 722.949
[72,     1] loss: 688.494
[73,     1] loss: 699.596
[74,     1] loss: 687.757
[75,     1] loss: 636.032
Early stopping applied (best metric=0.4196738302707672)
Finished Training
Total time taken: 10.825154066085815
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1524.270
[2,     1] loss: 1527.750
[3,     1] loss: 1524.057
[4,     1] loss: 1528.694
[5,     1] loss: 1520.011
[6,     1] loss: 1519.338
[7,     1] loss: 1521.006
[8,     1] loss: 1518.605
[9,     1] loss: 1510.903
[10,     1] loss: 1507.262
[11,     1] loss: 1490.004
[12,     1] loss: 1480.694
[13,     1] loss: 1448.492
[14,     1] loss: 1415.027
[15,     1] loss: 1396.698
[16,     1] loss: 1341.588
[17,     1] loss: 1334.804
[18,     1] loss: 1291.201
[19,     1] loss: 1299.641
[20,     1] loss: 1217.987
[21,     1] loss: 1215.696
[22,     1] loss: 1291.016
[23,     1] loss: 1216.504
[24,     1] loss: 1282.027
[25,     1] loss: 1254.174
[26,     1] loss: 1249.806
[27,     1] loss: 1180.589
[28,     1] loss: 1197.299
[29,     1] loss: 1243.742
[30,     1] loss: 1196.482
[31,     1] loss: 1146.290
[32,     1] loss: 1162.505
[33,     1] loss: 1047.105
[34,     1] loss: 1062.530
[35,     1] loss: 1082.790
[36,     1] loss: 1118.031
[37,     1] loss: 1104.063
[38,     1] loss: 1070.354
[39,     1] loss: 1089.428
[40,     1] loss: 1018.445
[41,     1] loss: 977.945
[42,     1] loss: 1051.498
[43,     1] loss: 1050.980
[44,     1] loss: 985.468
[45,     1] loss: 980.507
[46,     1] loss: 922.783
[47,     1] loss: 999.520
[48,     1] loss: 993.538
[49,     1] loss: 933.610
[50,     1] loss: 1009.055
[51,     1] loss: 951.824
[52,     1] loss: 1091.319
[53,     1] loss: 931.000
[54,     1] loss: 1041.816
[55,     1] loss: 968.488
[56,     1] loss: 1004.008
[57,     1] loss: 1021.796
[58,     1] loss: 960.306
[59,     1] loss: 957.003
[60,     1] loss: 896.110
[61,     1] loss: 940.602
[62,     1] loss: 847.046
[63,     1] loss: 918.347
[64,     1] loss: 783.915
[65,     1] loss: 851.559
[66,     1] loss: 813.222
[67,     1] loss: 768.107
[68,     1] loss: 823.223
[69,     1] loss: 809.893
[70,     1] loss: 770.630
[71,     1] loss: 734.906
[72,     1] loss: 696.260
[73,     1] loss: 785.541
[74,     1] loss: 719.092
[75,     1] loss: 727.052
[76,     1] loss: 715.669
[77,     1] loss: 717.676
[78,     1] loss: 829.179
[79,     1] loss: 736.791
[80,     1] loss: 645.026
[81,     1] loss: 769.838
[82,     1] loss: 683.211
[83,     1] loss: 708.091
[84,     1] loss: 673.274
[85,     1] loss: 636.408
[86,     1] loss: 613.165
[87,     1] loss: 646.788
[88,     1] loss: 646.183
[89,     1] loss: 721.591
[90,     1] loss: 809.534
[91,     1] loss: 782.853
[92,     1] loss: 669.263
[93,     1] loss: 763.230
Early stopping applied (best metric=0.41791248321533203)
Finished Training
Total time taken: 13.382430076599121
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1526.969
[2,     1] loss: 1526.071
[3,     1] loss: 1522.783
[4,     1] loss: 1527.679
[5,     1] loss: 1524.433
[6,     1] loss: 1517.316
[7,     1] loss: 1520.433
[8,     1] loss: 1520.794
[9,     1] loss: 1517.356
[10,     1] loss: 1513.260
[11,     1] loss: 1508.749
[12,     1] loss: 1502.428
[13,     1] loss: 1486.650
[14,     1] loss: 1471.988
[15,     1] loss: 1457.416
[16,     1] loss: 1411.643
[17,     1] loss: 1385.690
[18,     1] loss: 1363.865
[19,     1] loss: 1334.544
[20,     1] loss: 1309.030
[21,     1] loss: 1295.381
[22,     1] loss: 1318.955
[23,     1] loss: 1293.077
[24,     1] loss: 1253.691
[25,     1] loss: 1328.573
[26,     1] loss: 1224.399
[27,     1] loss: 1278.211
[28,     1] loss: 1206.116
[29,     1] loss: 1221.062
[30,     1] loss: 1268.293
[31,     1] loss: 1222.962
[32,     1] loss: 1226.919
[33,     1] loss: 1197.326
[34,     1] loss: 1185.585
[35,     1] loss: 1175.767
[36,     1] loss: 1157.775
[37,     1] loss: 1141.488
[38,     1] loss: 1141.236
[39,     1] loss: 1099.342
[40,     1] loss: 1113.572
[41,     1] loss: 1099.426
[42,     1] loss: 1048.834
[43,     1] loss: 1073.751
[44,     1] loss: 1012.353
[45,     1] loss: 1033.322
[46,     1] loss: 999.326
[47,     1] loss: 1070.493
[48,     1] loss: 1017.196
[49,     1] loss: 993.667
[50,     1] loss: 1167.073
[51,     1] loss: 1173.756
[52,     1] loss: 1017.468
[53,     1] loss: 1121.447
[54,     1] loss: 996.083
[55,     1] loss: 1093.951
[56,     1] loss: 1019.992
[57,     1] loss: 990.052
[58,     1] loss: 1014.745
[59,     1] loss: 899.341
[60,     1] loss: 1063.396
[61,     1] loss: 899.186
[62,     1] loss: 1026.865
[63,     1] loss: 931.265
[64,     1] loss: 901.660
[65,     1] loss: 937.134
[66,     1] loss: 863.709
[67,     1] loss: 904.390
[68,     1] loss: 822.224
[69,     1] loss: 846.379
[70,     1] loss: 795.206
[71,     1] loss: 778.544
[72,     1] loss: 746.437
[73,     1] loss: 758.429
[74,     1] loss: 799.679
[75,     1] loss: 784.954
[76,     1] loss: 767.684
[77,     1] loss: 677.711
[78,     1] loss: 711.936
[79,     1] loss: 675.842
[80,     1] loss: 731.876
[81,     1] loss: 768.575
[82,     1] loss: 642.684
[83,     1] loss: 813.778
[84,     1] loss: 894.117
[85,     1] loss: 649.556
[86,     1] loss: 936.899
[87,     1] loss: 714.573
[88,     1] loss: 897.513
[89,     1] loss: 695.019
[90,     1] loss: 882.934
[91,     1] loss: 651.005
[92,     1] loss: 804.935
[93,     1] loss: 710.833
[94,     1] loss: 736.527
[95,     1] loss: 722.451
[96,     1] loss: 671.558
[97,     1] loss: 650.354
[98,     1] loss: 615.482
[99,     1] loss: 703.199
[100,     1] loss: 634.791
[101,     1] loss: 669.740
[102,     1] loss: 585.355
[103,     1] loss: 585.484
[104,     1] loss: 589.786
[105,     1] loss: 653.275
[106,     1] loss: 545.477
[107,     1] loss: 614.132
[108,     1] loss: 551.342
[109,     1] loss: 636.745
[110,     1] loss: 563.575
[111,     1] loss: 579.500
[112,     1] loss: 671.963
[113,     1] loss: 487.964
[114,     1] loss: 650.164
[115,     1] loss: 531.937
[116,     1] loss: 568.166
[117,     1] loss: 515.402
[118,     1] loss: 532.222
[119,     1] loss: 560.156
[120,     1] loss: 481.646
[121,     1] loss: 530.569
[122,     1] loss: 491.165
[123,     1] loss: 466.135
[124,     1] loss: 507.171
[125,     1] loss: 435.451
[126,     1] loss: 488.711
[127,     1] loss: 500.035
[128,     1] loss: 471.683
[129,     1] loss: 504.282
[130,     1] loss: 504.058
[131,     1] loss: 458.311
[132,     1] loss: 644.369
[133,     1] loss: 693.510
[134,     1] loss: 560.776
[135,     1] loss: 585.562
[136,     1] loss: 644.615
[137,     1] loss: 540.430
[138,     1] loss: 570.238
[139,     1] loss: 546.621
[140,     1] loss: 564.919
[141,     1] loss: 485.357
[142,     1] loss: 528.765
[143,     1] loss: 498.476
[144,     1] loss: 547.951
[145,     1] loss: 484.579
[146,     1] loss: 687.319
[147,     1] loss: 555.493
[148,     1] loss: 523.472
[149,     1] loss: 603.410
[150,     1] loss: 560.958
[151,     1] loss: 514.886
[152,     1] loss: 519.552
[153,     1] loss: 435.205
[154,     1] loss: 441.407
[155,     1] loss: 447.953
[156,     1] loss: 447.183
[157,     1] loss: 385.187
[158,     1] loss: 405.333
[159,     1] loss: 404.894
[160,     1] loss: 436.501
[161,     1] loss: 384.652
[162,     1] loss: 421.733
[163,     1] loss: 424.256
[164,     1] loss: 447.696
[165,     1] loss: 423.637
[166,     1] loss: 494.195
[167,     1] loss: 404.605
[168,     1] loss: 522.320
[169,     1] loss: 554.604
[170,     1] loss: 378.702
[171,     1] loss: 495.795
[172,     1] loss: 552.319
[173,     1] loss: 390.572
[174,     1] loss: 515.332
[175,     1] loss: 549.973
[176,     1] loss: 394.380
[177,     1] loss: 505.582
[178,     1] loss: 414.705
[179,     1] loss: 415.894
[180,     1] loss: 426.856
[181,     1] loss: 398.360
[182,     1] loss: 421.598
[183,     1] loss: 401.406
[184,     1] loss: 464.767
Early stopping applied (best metric=0.29408878087997437)
Finished Training
Total time taken: 26.5168354511261
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1520.353
[2,     1] loss: 1526.781
[3,     1] loss: 1520.414
[4,     1] loss: 1521.578
[5,     1] loss: 1517.815
[6,     1] loss: 1510.639
[7,     1] loss: 1510.699
[8,     1] loss: 1496.342
[9,     1] loss: 1474.844
[10,     1] loss: 1443.601
[11,     1] loss: 1417.311
[12,     1] loss: 1348.024
[13,     1] loss: 1344.836
[14,     1] loss: 1294.572
[15,     1] loss: 1260.224
[16,     1] loss: 1273.108
[17,     1] loss: 1255.154
[18,     1] loss: 1264.743
[19,     1] loss: 1211.153
[20,     1] loss: 1185.253
[21,     1] loss: 1203.489
[22,     1] loss: 1225.979
[23,     1] loss: 1199.548
[24,     1] loss: 1216.342
[25,     1] loss: 1135.216
[26,     1] loss: 1192.462
[27,     1] loss: 1144.346
[28,     1] loss: 1140.290
[29,     1] loss: 1190.461
[30,     1] loss: 1090.812
[31,     1] loss: 1066.304
[32,     1] loss: 1097.405
[33,     1] loss: 1044.470
[34,     1] loss: 1100.276
[35,     1] loss: 1072.356
[36,     1] loss: 1105.447
[37,     1] loss: 1143.647
[38,     1] loss: 1027.339
[39,     1] loss: 1069.218
[40,     1] loss: 999.096
[41,     1] loss: 998.170
[42,     1] loss: 901.138
[43,     1] loss: 1015.897
[44,     1] loss: 941.040
[45,     1] loss: 898.228
[46,     1] loss: 922.409
[47,     1] loss: 817.236
[48,     1] loss: 850.242
[49,     1] loss: 898.215
[50,     1] loss: 891.592
[51,     1] loss: 931.596
[52,     1] loss: 891.352
[53,     1] loss: 841.147
[54,     1] loss: 838.014
[55,     1] loss: 836.999
[56,     1] loss: 823.627
[57,     1] loss: 803.192
[58,     1] loss: 815.762
[59,     1] loss: 803.677
[60,     1] loss: 1077.034
[61,     1] loss: 846.583
[62,     1] loss: 759.445
[63,     1] loss: 822.447
[64,     1] loss: 741.066
[65,     1] loss: 819.710
[66,     1] loss: 748.037
[67,     1] loss: 854.005
[68,     1] loss: 762.856
[69,     1] loss: 797.342
[70,     1] loss: 745.803
[71,     1] loss: 782.246
[72,     1] loss: 686.117
[73,     1] loss: 726.077
[74,     1] loss: 711.764
[75,     1] loss: 771.212
Early stopping applied (best metric=0.38702425360679626)
Finished Training
Total time taken: 10.881161212921143
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1523.784
[2,     1] loss: 1524.949
[3,     1] loss: 1523.289
[4,     1] loss: 1521.837
[5,     1] loss: 1520.202
[6,     1] loss: 1518.770
[7,     1] loss: 1518.237
[8,     1] loss: 1524.772
[9,     1] loss: 1513.146
[10,     1] loss: 1513.380
[11,     1] loss: 1505.192
[12,     1] loss: 1489.589
[13,     1] loss: 1473.512
[14,     1] loss: 1442.064
[15,     1] loss: 1424.212
[16,     1] loss: 1367.155
[17,     1] loss: 1342.618
[18,     1] loss: 1331.313
[19,     1] loss: 1327.616
[20,     1] loss: 1323.123
[21,     1] loss: 1284.559
[22,     1] loss: 1252.081
[23,     1] loss: 1269.729
[24,     1] loss: 1250.586
[25,     1] loss: 1231.870
[26,     1] loss: 1178.902
[27,     1] loss: 1215.639
[28,     1] loss: 1265.463
[29,     1] loss: 1231.167
[30,     1] loss: 1158.214
[31,     1] loss: 1167.072
[32,     1] loss: 1179.829
[33,     1] loss: 1152.973
[34,     1] loss: 1103.627
[35,     1] loss: 1157.908
[36,     1] loss: 1133.339
[37,     1] loss: 1125.854
[38,     1] loss: 1140.539
[39,     1] loss: 1049.686
[40,     1] loss: 1052.144
[41,     1] loss: 1078.009
[42,     1] loss: 1116.651
[43,     1] loss: 1062.322
[44,     1] loss: 1098.966
[45,     1] loss: 1043.705
[46,     1] loss: 979.593
[47,     1] loss: 1056.147
[48,     1] loss: 1021.615
[49,     1] loss: 978.987
[50,     1] loss: 966.347
[51,     1] loss: 922.300
[52,     1] loss: 913.815
[53,     1] loss: 1007.622
[54,     1] loss: 1128.040
[55,     1] loss: 929.970
[56,     1] loss: 957.099
[57,     1] loss: 915.683
[58,     1] loss: 895.482
[59,     1] loss: 889.979
[60,     1] loss: 836.133
[61,     1] loss: 851.077
[62,     1] loss: 875.471
[63,     1] loss: 854.052
[64,     1] loss: 820.779
[65,     1] loss: 739.207
[66,     1] loss: 720.578
[67,     1] loss: 797.866
[68,     1] loss: 767.190
[69,     1] loss: 741.957
[70,     1] loss: 737.973
[71,     1] loss: 839.721
[72,     1] loss: 796.986
[73,     1] loss: 703.667
[74,     1] loss: 881.193
[75,     1] loss: 863.047
[76,     1] loss: 815.708
[77,     1] loss: 800.561
[78,     1] loss: 796.792
[79,     1] loss: 759.045
[80,     1] loss: 808.789
[81,     1] loss: 680.569
[82,     1] loss: 778.495
[83,     1] loss: 694.329
[84,     1] loss: 692.167
[85,     1] loss: 695.523
[86,     1] loss: 709.911
Early stopping applied (best metric=0.37643659114837646)
Finished Training
Total time taken: 12.356320858001709
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1528.006
[2,     1] loss: 1523.661
[3,     1] loss: 1520.518
[4,     1] loss: 1518.449
[5,     1] loss: 1517.516
[6,     1] loss: 1524.238
[7,     1] loss: 1519.119
[8,     1] loss: 1516.126
[9,     1] loss: 1509.853
[10,     1] loss: 1509.410
[11,     1] loss: 1497.256
[12,     1] loss: 1478.406
[13,     1] loss: 1448.872
[14,     1] loss: 1432.031
[15,     1] loss: 1387.628
[16,     1] loss: 1389.422
[17,     1] loss: 1351.264
[18,     1] loss: 1271.898
[19,     1] loss: 1309.332
[20,     1] loss: 1235.371
[21,     1] loss: 1317.675
[22,     1] loss: 1211.417
[23,     1] loss: 1197.303
[24,     1] loss: 1196.037
[25,     1] loss: 1237.236
[26,     1] loss: 1249.782
[27,     1] loss: 1203.161
[28,     1] loss: 1233.418
[29,     1] loss: 1144.994
[30,     1] loss: 1242.365
[31,     1] loss: 1152.432
[32,     1] loss: 1178.068
[33,     1] loss: 1118.345
[34,     1] loss: 1132.741
[35,     1] loss: 1161.187
[36,     1] loss: 1106.141
[37,     1] loss: 1071.299
[38,     1] loss: 1114.413
[39,     1] loss: 1063.269
[40,     1] loss: 1001.276
[41,     1] loss: 937.385
[42,     1] loss: 1040.778
[43,     1] loss: 972.810
[44,     1] loss: 1002.011
[45,     1] loss: 1031.943
[46,     1] loss: 943.224
[47,     1] loss: 966.858
[48,     1] loss: 875.327
[49,     1] loss: 958.931
[50,     1] loss: 951.382
[51,     1] loss: 879.776
[52,     1] loss: 869.224
[53,     1] loss: 842.244
[54,     1] loss: 833.983
[55,     1] loss: 815.508
[56,     1] loss: 787.440
[57,     1] loss: 844.525
[58,     1] loss: 755.765
[59,     1] loss: 826.309
[60,     1] loss: 835.236
[61,     1] loss: 812.358
[62,     1] loss: 782.552
[63,     1] loss: 843.864
[64,     1] loss: 747.667
[65,     1] loss: 774.305
[66,     1] loss: 784.027
[67,     1] loss: 700.303
[68,     1] loss: 697.906
[69,     1] loss: 660.363
[70,     1] loss: 650.115
[71,     1] loss: 681.920
[72,     1] loss: 649.581
[73,     1] loss: 649.073
[74,     1] loss: 661.278
[75,     1] loss: 778.502
[76,     1] loss: 797.788
[77,     1] loss: 621.498
[78,     1] loss: 719.502
[79,     1] loss: 641.487
[80,     1] loss: 720.111
[81,     1] loss: 624.244
[82,     1] loss: 673.384
[83,     1] loss: 608.537
[84,     1] loss: 625.645
[85,     1] loss: 641.436
[86,     1] loss: 603.953
Early stopping applied (best metric=0.38101157546043396)
Finished Training
Total time taken: 12.445331811904907
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1525.107
[2,     1] loss: 1520.316
[3,     1] loss: 1524.328
[4,     1] loss: 1517.708
[5,     1] loss: 1518.203
[6,     1] loss: 1515.104
[7,     1] loss: 1529.614
[8,     1] loss: 1519.818
[9,     1] loss: 1518.220
[10,     1] loss: 1512.826
[11,     1] loss: 1518.796
[12,     1] loss: 1508.560
[13,     1] loss: 1500.604
[14,     1] loss: 1480.209
[15,     1] loss: 1463.500
[16,     1] loss: 1434.489
[17,     1] loss: 1413.968
[18,     1] loss: 1354.730
[19,     1] loss: 1298.604
[20,     1] loss: 1313.857
[21,     1] loss: 1288.442
[22,     1] loss: 1297.075
[23,     1] loss: 1313.931
[24,     1] loss: 1286.866
[25,     1] loss: 1267.194
[26,     1] loss: 1287.715
[27,     1] loss: 1210.327
[28,     1] loss: 1234.051
[29,     1] loss: 1204.304
[30,     1] loss: 1243.516
[31,     1] loss: 1161.117
[32,     1] loss: 1182.688
[33,     1] loss: 1185.042
[34,     1] loss: 1128.883
[35,     1] loss: 1135.240
[36,     1] loss: 1057.432
[37,     1] loss: 1096.675
[38,     1] loss: 1027.736
[39,     1] loss: 1102.808
[40,     1] loss: 1064.964
[41,     1] loss: 1053.671
[42,     1] loss: 1033.210
[43,     1] loss: 954.011
[44,     1] loss: 1028.764
[45,     1] loss: 978.667
[46,     1] loss: 932.075
[47,     1] loss: 1000.845
[48,     1] loss: 944.931
[49,     1] loss: 864.359
[50,     1] loss: 895.242
[51,     1] loss: 937.080
[52,     1] loss: 886.232
[53,     1] loss: 942.884
[54,     1] loss: 998.435
[55,     1] loss: 870.457
[56,     1] loss: 980.398
[57,     1] loss: 808.174
[58,     1] loss: 807.325
[59,     1] loss: 861.041
[60,     1] loss: 804.385
[61,     1] loss: 792.581
[62,     1] loss: 858.246
[63,     1] loss: 813.594
[64,     1] loss: 770.237
[65,     1] loss: 770.666
[66,     1] loss: 725.462
[67,     1] loss: 771.968
[68,     1] loss: 690.526
[69,     1] loss: 738.492
[70,     1] loss: 770.720
[71,     1] loss: 707.323
[72,     1] loss: 649.478
[73,     1] loss: 653.768
[74,     1] loss: 675.947
[75,     1] loss: 609.240
[76,     1] loss: 646.721
[77,     1] loss: 678.441
[78,     1] loss: 652.125
[79,     1] loss: 585.253
[80,     1] loss: 657.098
[81,     1] loss: 770.318
[82,     1] loss: 685.335
[83,     1] loss: 718.958
[84,     1] loss: 871.201
[85,     1] loss: 694.750
[86,     1] loss: 698.958
Early stopping applied (best metric=0.35969701409339905)
Finished Training
Total time taken: 12.439327001571655
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1521.926
[2,     1] loss: 1518.693
[3,     1] loss: 1529.422
[4,     1] loss: 1518.277
[5,     1] loss: 1517.884
[6,     1] loss: 1518.682
[7,     1] loss: 1516.130
[8,     1] loss: 1509.018
[9,     1] loss: 1502.552
[10,     1] loss: 1489.582
[11,     1] loss: 1463.545
[12,     1] loss: 1433.573
[13,     1] loss: 1394.847
[14,     1] loss: 1359.698
[15,     1] loss: 1327.561
[16,     1] loss: 1326.246
[17,     1] loss: 1321.354
[18,     1] loss: 1250.813
[19,     1] loss: 1234.364
[20,     1] loss: 1331.613
[21,     1] loss: 1191.848
[22,     1] loss: 1191.208
[23,     1] loss: 1216.267
[24,     1] loss: 1179.059
[25,     1] loss: 1243.310
[26,     1] loss: 1182.972
[27,     1] loss: 1191.668
[28,     1] loss: 1144.192
[29,     1] loss: 1155.316
[30,     1] loss: 1128.792
[31,     1] loss: 1059.239
[32,     1] loss: 1070.455
[33,     1] loss: 1076.135
[34,     1] loss: 1099.048
[35,     1] loss: 1063.471
[36,     1] loss: 1073.996
[37,     1] loss: 1006.818
[38,     1] loss: 1091.997
[39,     1] loss: 1019.606
[40,     1] loss: 1013.727
[41,     1] loss: 932.272
[42,     1] loss: 1004.403
[43,     1] loss: 945.999
[44,     1] loss: 983.878
[45,     1] loss: 1007.158
[46,     1] loss: 942.096
[47,     1] loss: 1040.306
[48,     1] loss: 999.711
[49,     1] loss: 1001.509
[50,     1] loss: 943.288
[51,     1] loss: 998.916
[52,     1] loss: 902.825
[53,     1] loss: 939.112
[54,     1] loss: 870.514
[55,     1] loss: 1011.581
[56,     1] loss: 876.719
[57,     1] loss: 853.309
[58,     1] loss: 824.743
[59,     1] loss: 835.371
[60,     1] loss: 823.878
[61,     1] loss: 732.958
[62,     1] loss: 756.472
[63,     1] loss: 782.659
[64,     1] loss: 809.549
[65,     1] loss: 751.577
[66,     1] loss: 715.864
[67,     1] loss: 694.016
[68,     1] loss: 763.459
[69,     1] loss: 679.292
[70,     1] loss: 669.310
[71,     1] loss: 680.025
[72,     1] loss: 662.764
[73,     1] loss: 742.729
[74,     1] loss: 830.610
[75,     1] loss: 956.569
[76,     1] loss: 702.783
[77,     1] loss: 788.221
[78,     1] loss: 700.674
[79,     1] loss: 711.557
[80,     1] loss: 716.519
[81,     1] loss: 767.425
[82,     1] loss: 664.420
[83,     1] loss: 837.428
[84,     1] loss: 618.091
[85,     1] loss: 768.127
[86,     1] loss: 659.499
[87,     1] loss: 731.110
[88,     1] loss: 625.021
[89,     1] loss: 588.359
[90,     1] loss: 676.801
[91,     1] loss: 589.041
[92,     1] loss: 565.613
[93,     1] loss: 585.592
[94,     1] loss: 572.182
[95,     1] loss: 576.671
[96,     1] loss: 618.659
[97,     1] loss: 601.036
[98,     1] loss: 543.768
[99,     1] loss: 571.503
Early stopping applied (best metric=0.3796803653240204)
Finished Training
Total time taken: 14.298526763916016
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1525.448
[2,     1] loss: 1526.412
[3,     1] loss: 1521.163
[4,     1] loss: 1515.370
[5,     1] loss: 1518.384
[6,     1] loss: 1510.927
[7,     1] loss: 1518.398
[8,     1] loss: 1516.693
[9,     1] loss: 1511.787
[10,     1] loss: 1505.407
[11,     1] loss: 1492.343
[12,     1] loss: 1470.720
[13,     1] loss: 1441.408
[14,     1] loss: 1392.289
[15,     1] loss: 1371.348
[16,     1] loss: 1359.025
[17,     1] loss: 1321.421
[18,     1] loss: 1339.228
[19,     1] loss: 1288.077
[20,     1] loss: 1244.736
[21,     1] loss: 1286.412
[22,     1] loss: 1275.245
[23,     1] loss: 1267.420
[24,     1] loss: 1240.094
[25,     1] loss: 1224.604
[26,     1] loss: 1221.684
[27,     1] loss: 1162.931
[28,     1] loss: 1172.889
[29,     1] loss: 1176.821
[30,     1] loss: 1182.427
[31,     1] loss: 1148.685
[32,     1] loss: 1102.832
[33,     1] loss: 1135.047
[34,     1] loss: 1179.607
[35,     1] loss: 1116.225
[36,     1] loss: 1257.228
[37,     1] loss: 1111.790
[38,     1] loss: 1123.861
[39,     1] loss: 1138.796
[40,     1] loss: 1075.542
[41,     1] loss: 1039.601
[42,     1] loss: 1200.832
[43,     1] loss: 1059.740
[44,     1] loss: 1024.146
[45,     1] loss: 1018.081
[46,     1] loss: 998.359
[47,     1] loss: 981.935
[48,     1] loss: 945.011
[49,     1] loss: 955.405
[50,     1] loss: 980.234
[51,     1] loss: 926.171
[52,     1] loss: 944.074
[53,     1] loss: 849.013
[54,     1] loss: 914.416
[55,     1] loss: 978.260
[56,     1] loss: 905.220
[57,     1] loss: 872.600
[58,     1] loss: 938.836
[59,     1] loss: 1024.981
[60,     1] loss: 871.511
[61,     1] loss: 811.465
[62,     1] loss: 833.761
[63,     1] loss: 755.395
[64,     1] loss: 807.337
[65,     1] loss: 804.456
[66,     1] loss: 900.754
[67,     1] loss: 903.530
[68,     1] loss: 721.659
[69,     1] loss: 822.236
[70,     1] loss: 830.623
[71,     1] loss: 767.355
[72,     1] loss: 797.762
[73,     1] loss: 735.336
[74,     1] loss: 720.109
[75,     1] loss: 693.226
[76,     1] loss: 820.469
[77,     1] loss: 709.840
[78,     1] loss: 685.907
[79,     1] loss: 635.543
Early stopping applied (best metric=0.366378515958786)
Finished Training
Total time taken: 11.44022512435913
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1526.793
[2,     1] loss: 1523.410
[3,     1] loss: 1523.710
[4,     1] loss: 1520.465
[5,     1] loss: 1519.262
[6,     1] loss: 1519.599
[7,     1] loss: 1516.913
[8,     1] loss: 1514.714
[9,     1] loss: 1503.096
[10,     1] loss: 1489.680
[11,     1] loss: 1470.510
[12,     1] loss: 1431.123
[13,     1] loss: 1407.996
[14,     1] loss: 1390.875
[15,     1] loss: 1368.810
[16,     1] loss: 1329.289
[17,     1] loss: 1361.316
[18,     1] loss: 1318.952
[19,     1] loss: 1339.842
[20,     1] loss: 1314.142
[21,     1] loss: 1289.538
[22,     1] loss: 1255.545
[23,     1] loss: 1190.942
[24,     1] loss: 1274.135
[25,     1] loss: 1243.694
[26,     1] loss: 1224.365
[27,     1] loss: 1205.391
[28,     1] loss: 1241.978
[29,     1] loss: 1269.753
[30,     1] loss: 1198.515
[31,     1] loss: 1229.126
[32,     1] loss: 1238.649
[33,     1] loss: 1174.859
[34,     1] loss: 1232.818
[35,     1] loss: 1131.567
[36,     1] loss: 1104.281
[37,     1] loss: 1179.035
[38,     1] loss: 1148.253
[39,     1] loss: 1143.283
[40,     1] loss: 1126.447
[41,     1] loss: 1085.135
[42,     1] loss: 1140.805
[43,     1] loss: 1085.375
[44,     1] loss: 1141.981
[45,     1] loss: 1093.286
[46,     1] loss: 1078.661
[47,     1] loss: 1039.047
[48,     1] loss: 997.723
[49,     1] loss: 1072.110
[50,     1] loss: 1035.704
[51,     1] loss: 988.679
[52,     1] loss: 1013.036
[53,     1] loss: 973.847
[54,     1] loss: 1013.926
[55,     1] loss: 934.134
[56,     1] loss: 895.525
[57,     1] loss: 954.051
[58,     1] loss: 900.757
[59,     1] loss: 884.040
[60,     1] loss: 919.406
[61,     1] loss: 866.472
[62,     1] loss: 814.391
[63,     1] loss: 788.302
[64,     1] loss: 875.356
[65,     1] loss: 891.743
[66,     1] loss: 851.472
[67,     1] loss: 743.323
[68,     1] loss: 744.692
[69,     1] loss: 805.016
[70,     1] loss: 783.721
[71,     1] loss: 694.039
[72,     1] loss: 753.958
[73,     1] loss: 718.670
Early stopping applied (best metric=0.3771933615207672)
Finished Training
Total time taken: 10.740147113800049
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1519.631
[2,     1] loss: 1523.400
[3,     1] loss: 1523.159
[4,     1] loss: 1523.367
[5,     1] loss: 1518.727
[6,     1] loss: 1520.189
[7,     1] loss: 1518.611
[8,     1] loss: 1518.636
[9,     1] loss: 1511.506
[10,     1] loss: 1506.279
[11,     1] loss: 1492.885
[12,     1] loss: 1469.541
[13,     1] loss: 1441.661
[14,     1] loss: 1407.996
[15,     1] loss: 1357.730
[16,     1] loss: 1337.389
[17,     1] loss: 1303.086
[18,     1] loss: 1329.576
[19,     1] loss: 1310.868
[20,     1] loss: 1318.020
[21,     1] loss: 1277.622
[22,     1] loss: 1302.955
[23,     1] loss: 1271.819
[24,     1] loss: 1235.637
[25,     1] loss: 1243.448
[26,     1] loss: 1212.821
[27,     1] loss: 1209.253
[28,     1] loss: 1277.207
[29,     1] loss: 1214.688
[30,     1] loss: 1246.165
[31,     1] loss: 1186.136
[32,     1] loss: 1171.919
[33,     1] loss: 1122.389
[34,     1] loss: 1135.643
[35,     1] loss: 1197.339
[36,     1] loss: 1122.002
[37,     1] loss: 1202.765
[38,     1] loss: 1098.441
[39,     1] loss: 1118.090
[40,     1] loss: 1029.246
[41,     1] loss: 1020.094
[42,     1] loss: 1038.339
[43,     1] loss: 1016.794
[44,     1] loss: 1043.431
[45,     1] loss: 1067.761
[46,     1] loss: 979.228
[47,     1] loss: 951.742
[48,     1] loss: 943.555
[49,     1] loss: 929.777
[50,     1] loss: 997.574
[51,     1] loss: 864.684
[52,     1] loss: 861.401
[53,     1] loss: 956.418
[54,     1] loss: 853.054
[55,     1] loss: 923.302
[56,     1] loss: 915.253
[57,     1] loss: 810.975
[58,     1] loss: 857.404
[59,     1] loss: 924.604
[60,     1] loss: 857.840
[61,     1] loss: 777.472
[62,     1] loss: 906.877
[63,     1] loss: 979.166
[64,     1] loss: 835.118
[65,     1] loss: 748.344
[66,     1] loss: 845.532
[67,     1] loss: 770.049
[68,     1] loss: 774.757
[69,     1] loss: 765.990
[70,     1] loss: 695.855
[71,     1] loss: 684.077
[72,     1] loss: 721.757
[73,     1] loss: 704.828
[74,     1] loss: 749.303
[75,     1] loss: 681.037
[76,     1] loss: 663.865
[77,     1] loss: 1008.771
[78,     1] loss: 831.146
[79,     1] loss: 796.973
[80,     1] loss: 740.084
[81,     1] loss: 796.658
[82,     1] loss: 650.949
[83,     1] loss: 797.305
[84,     1] loss: 685.573
[85,     1] loss: 727.881
[86,     1] loss: 810.683
Early stopping applied (best metric=0.3833102583885193)
Finished Training
Total time taken: 12.40332293510437
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1524.515
[2,     1] loss: 1521.044
[3,     1] loss: 1523.682
[4,     1] loss: 1522.815
[5,     1] loss: 1522.368
[6,     1] loss: 1516.471
[7,     1] loss: 1514.980
[8,     1] loss: 1508.992
[9,     1] loss: 1494.583
[10,     1] loss: 1465.452
[11,     1] loss: 1430.427
[12,     1] loss: 1395.741
[13,     1] loss: 1343.526
[14,     1] loss: 1301.611
[15,     1] loss: 1247.816
[16,     1] loss: 1244.014
[17,     1] loss: 1219.581
[18,     1] loss: 1302.621
[19,     1] loss: 1203.841
[20,     1] loss: 1165.963
[21,     1] loss: 1213.023
[22,     1] loss: 1219.635
[23,     1] loss: 1155.998
[24,     1] loss: 1201.863
[25,     1] loss: 1153.757
[26,     1] loss: 1178.480
[27,     1] loss: 1128.653
[28,     1] loss: 1141.183
[29,     1] loss: 1115.122
[30,     1] loss: 1092.045
[31,     1] loss: 1071.063
[32,     1] loss: 1112.588
[33,     1] loss: 1086.012
[34,     1] loss: 1045.393
[35,     1] loss: 1029.735
[36,     1] loss: 1025.272
[37,     1] loss: 1028.460
[38,     1] loss: 1044.533
[39,     1] loss: 945.969
[40,     1] loss: 961.329
[41,     1] loss: 1010.778
[42,     1] loss: 923.881
[43,     1] loss: 988.629
[44,     1] loss: 918.097
[45,     1] loss: 934.502
[46,     1] loss: 946.595
[47,     1] loss: 870.680
[48,     1] loss: 817.947
[49,     1] loss: 871.013
[50,     1] loss: 908.473
[51,     1] loss: 811.326
[52,     1] loss: 833.917
[53,     1] loss: 920.103
[54,     1] loss: 818.577
[55,     1] loss: 822.125
[56,     1] loss: 803.451
[57,     1] loss: 853.846
[58,     1] loss: 823.321
[59,     1] loss: 779.886
[60,     1] loss: 798.551
[61,     1] loss: 759.683
[62,     1] loss: 765.650
[63,     1] loss: 727.761
[64,     1] loss: 703.863
[65,     1] loss: 830.642
[66,     1] loss: 982.103
[67,     1] loss: 942.654
[68,     1] loss: 769.105
[69,     1] loss: 869.178
[70,     1] loss: 775.550
[71,     1] loss: 837.690
[72,     1] loss: 831.479
[73,     1] loss: 767.626
[74,     1] loss: 803.571
[75,     1] loss: 694.168
[76,     1] loss: 807.118
[77,     1] loss: 749.769
[78,     1] loss: 689.970
[79,     1] loss: 747.966
[80,     1] loss: 680.578
[81,     1] loss: 706.148
[82,     1] loss: 627.060
[83,     1] loss: 685.260
[84,     1] loss: 632.479
[85,     1] loss: 672.911
[86,     1] loss: 543.324
[87,     1] loss: 629.922
[88,     1] loss: 619.820
[89,     1] loss: 715.439
[90,     1] loss: 595.136
[91,     1] loss: 683.266
[92,     1] loss: 591.135
[93,     1] loss: 575.791
[94,     1] loss: 599.722
[95,     1] loss: 588.937
[96,     1] loss: 571.570
[97,     1] loss: 557.532
[98,     1] loss: 560.476
[99,     1] loss: 597.310
[100,     1] loss: 624.561
[101,     1] loss: 532.753
[102,     1] loss: 514.181
[103,     1] loss: 524.483
[104,     1] loss: 534.666
[105,     1] loss: 550.938
[106,     1] loss: 510.228
[107,     1] loss: 528.114
[108,     1] loss: 509.950
[109,     1] loss: 616.214
[110,     1] loss: 588.675
[111,     1] loss: 506.419
[112,     1] loss: 626.624
[113,     1] loss: 545.807
[114,     1] loss: 503.364
[115,     1] loss: 614.825
[116,     1] loss: 469.533
[117,     1] loss: 513.669
[118,     1] loss: 492.806
[119,     1] loss: 449.946
[120,     1] loss: 516.252
[121,     1] loss: 486.520
[122,     1] loss: 533.524
[123,     1] loss: 562.214
[124,     1] loss: 503.868
[125,     1] loss: 482.004
Early stopping applied (best metric=0.408887654542923)
Finished Training
Total time taken: 17.948916912078857
{'Hydroxylation-K Validation Accuracy': 0.758983451536643, 'Hydroxylation-K Validation Sensitivity': 0.6437037037037037, 'Hydroxylation-K Validation Specificity': 0.787719298245614, 'Hydroxylation-K Validation Precision': 0.4392711645652822, 'Hydroxylation-K AUC ROC': 0.7776803118908382, 'Hydroxylation-K AUC PR': 0.5523292149826303, 'Hydroxylation-K MCC': 0.381395091873883, 'Hydroxylation-K F1': 0.5166908566756011, 'Validation Loss (Hydroxylation-K)': 0.48248350421587627, 'Hydroxylation-P Validation Accuracy': 0.775188755223931, 'Hydroxylation-P Validation Sensitivity': 0.7820634920634921, 'Hydroxylation-P Validation Specificity': 0.7737293630605018, 'Hydroxylation-P Validation Precision': 0.43273458324936487, 'Hydroxylation-P AUC ROC': 0.8389355899474857, 'Hydroxylation-P AUC PR': 0.5745783244122509, 'Hydroxylation-P MCC': 0.4567480629109296, 'Hydroxylation-P F1': 0.5546531242643041, 'Validation Loss (Hydroxylation-P)': 0.3778335452079773, 'Validation Loss (total)': 0.860317063331604, 'TimeToTrain': 13.491174538930258}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005007127894183188,
 'learning_rate_Hydroxylation-K': 0.0014718495441970341,
 'learning_rate_Hydroxylation-P': 0.0062098744602884295,
 'log_base': 1.5659877629189765,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 380886698,
 'sample_weights': [2.9119274238527253, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.4659703422096735,
 'weight_decay_Hydroxylation-K': 5.946617522719889,
 'weight_decay_Hydroxylation-P': 1.5854103462747888}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1690.295
[2,     1] loss: 1693.878
[3,     1] loss: 1698.282
[4,     1] loss: 1696.833
[5,     1] loss: 1686.225
[6,     1] loss: 1695.441
[7,     1] loss: 1677.914
[8,     1] loss: 1681.491
[9,     1] loss: 1678.220
[10,     1] loss: 1660.995
[11,     1] loss: 1647.830
[12,     1] loss: 1606.400
[13,     1] loss: 1556.475
[14,     1] loss: 1543.829
[15,     1] loss: 1455.950
[16,     1] loss: 1496.529
[17,     1] loss: 1443.793
[18,     1] loss: 1426.525
[19,     1] loss: 1418.633
[20,     1] loss: 1432.770
[21,     1] loss: 1388.725
[22,     1] loss: 1391.722
[23,     1] loss: 1370.215
[24,     1] loss: 1374.540
[25,     1] loss: 1332.536
[26,     1] loss: 1371.746
[27,     1] loss: 1364.498
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0033645539614016984,
 'learning_rate_Hydroxylation-K': 0.0032438725111613945,
 'learning_rate_Hydroxylation-P': 0.007156944546734586,
 'log_base': 2.452414313819982,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2692540408,
 'sample_weights': [3.722141978953443, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.92211062231417,
 'weight_decay_Hydroxylation-K': 9.516930534961485,
 'weight_decay_Hydroxylation-P': 1.1868493979087422}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1307.465
[2,     1] loss: 1301.848
[3,     1] loss: 1300.643
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004606439727434517,
 'learning_rate_Hydroxylation-K': 0.00730447251760184,
 'learning_rate_Hydroxylation-P': 0.0011992182557187107,
 'log_base': 1.7352277313056201,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3309954703,
 'sample_weights': [1.8609892357164017, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.743423051301951,
 'weight_decay_Hydroxylation-K': 6.525213065418118,
 'weight_decay_Hydroxylation-P': 2.6516910430456955}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1554.327
[2,     1] loss: 1556.603
[3,     1] loss: 1553.266
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0002443732118969791,
 'learning_rate_Hydroxylation-K': 0.007647891340135383,
 'learning_rate_Hydroxylation-P': 0.0027653038885249964,
 'log_base': 1.1288647053642984,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4191215210,
 'sample_weights': [3.029080089026654, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.9666076302976148,
 'weight_decay_Hydroxylation-K': 9.8328374284986,
 'weight_decay_Hydroxylation-P': 2.9802802593021527}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4486.601
[2,     1] loss: 4475.623
[3,     1] loss: 4487.069
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008043124428126575,
 'learning_rate_Hydroxylation-K': 0.0016635001894121976,
 'learning_rate_Hydroxylation-P': 0.005625572216868184,
 'log_base': 2.429424133680926,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2804784953,
 'sample_weights': [13.77286949690041, 1.7216750103925207],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.066997266149249,
 'weight_decay_Hydroxylation-K': 6.902275859021287,
 'weight_decay_Hydroxylation-P': 3.33058247171562}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1317.010
[2,     1] loss: 1310.938
[3,     1] loss: 1305.495
[4,     1] loss: 1306.740
[5,     1] loss: 1308.806
[6,     1] loss: 1301.669
[7,     1] loss: 1301.194
[8,     1] loss: 1294.801
[9,     1] loss: 1289.639
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027647320899362063,
 'learning_rate_Hydroxylation-K': 0.0011223917935388114,
 'learning_rate_Hydroxylation-P': 0.0025730778731068004,
 'log_base': 1.721560747301056,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4282236196,
 'sample_weights': [1.8807358299788206, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.024903849319088,
 'weight_decay_Hydroxylation-K': 9.472779445950659,
 'weight_decay_Hydroxylation-P': 2.361797072059923}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1558.344
[2,     1] loss: 1558.291
[3,     1] loss: 1555.652
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004810273246544067,
 'learning_rate_Hydroxylation-K': 0.00809377122774793,
 'learning_rate_Hydroxylation-P': 0.0024513766392755374,
 'log_base': 1.1816433801591353,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 527030708,
 'sample_weights': [3.0731719183282302, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.95847233303002,
 'weight_decay_Hydroxylation-K': 9.564955898785175,
 'weight_decay_Hydroxylation-P': 6.595556525377496}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3248.121
[2,     1] loss: 3244.626
[3,     1] loss: 3259.486
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006058076512469423,
 'learning_rate_Hydroxylation-K': 0.00023397229739394087,
 'learning_rate_Hydroxylation-P': 0.009109780520939649,
 'log_base': 2.10224949168465,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 111293914,
 'sample_weights': [10.002285722341748, 1.2503338813191551],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.58142683433797,
 'weight_decay_Hydroxylation-K': 8.43029562609081,
 'weight_decay_Hydroxylation-P': 1.2492483010359303}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1389.670
[2,     1] loss: 1383.518
[3,     1] loss: 1381.058
[4,     1] loss: 1378.431
[5,     1] loss: 1383.507
[6,     1] loss: 1380.329
[7,     1] loss: 1376.235
[8,     1] loss: 1374.414
[9,     1] loss: 1358.973
[10,     1] loss: 1345.736
[11,     1] loss: 1320.900
[12,     1] loss: 1288.221
[13,     1] loss: 1236.793
[14,     1] loss: 1229.804
[15,     1] loss: 1181.623
[16,     1] loss: 1156.870
[17,     1] loss: 1131.449
[18,     1] loss: 1135.418
[19,     1] loss: 1101.583
[20,     1] loss: 1221.077
[21,     1] loss: 1124.591
[22,     1] loss: 1132.338
[23,     1] loss: 1182.000
[24,     1] loss: 1097.326
[25,     1] loss: 1105.983
[26,     1] loss: 1119.849
[27,     1] loss: 1102.379
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008288112572293112,
 'learning_rate_Hydroxylation-K': 8.337549056940466e-05,
 'learning_rate_Hydroxylation-P': 0.004605420422083307,
 'log_base': 2.9438591453498444,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 54296174,
 'sample_weights': [2.24687115339048, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.227031132413845,
 'weight_decay_Hydroxylation-K': 5.2019440408857225,
 'weight_decay_Hydroxylation-P': 7.0976293502981305}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.488
[2,     1] loss: 1246.102
[3,     1] loss: 1236.073
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009624584342729706,
 'learning_rate_Hydroxylation-K': 0.009914229327630393,
 'learning_rate_Hydroxylation-P': 0.006588554720794007,
 'log_base': 1.252334838164351,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 318397288,
 'sample_weights': [1.5461796134546035, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4967752538092833,
 'weight_decay_Hydroxylation-K': 5.628775305667467,
 'weight_decay_Hydroxylation-P': 5.989259039076947}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2479.060
[2,     1] loss: 2486.698
[3,     1] loss: 2479.924
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007758343885651539,
 'learning_rate_Hydroxylation-K': 0.002198028975590218,
 'learning_rate_Hydroxylation-P': 0.008840998604221946,
 'log_base': 2.3976153691362394,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1790647358,
 'sample_weights': [7.419428136607688, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.678280281867655,
 'weight_decay_Hydroxylation-K': 1.5118283961688626,
 'weight_decay_Hydroxylation-P': 5.254672098532961}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1314.167
[2,     1] loss: 1315.129
[3,     1] loss: 1308.388
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004449110036890107,
 'learning_rate_Hydroxylation-K': 0.009911359177950036,
 'learning_rate_Hydroxylation-P': 0.004032387334452307,
 'log_base': 1.0477037959894588,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 751609962,
 'sample_weights': [1.9090812440785039, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.7137937334286906,
 'weight_decay_Hydroxylation-K': 7.347979277874272,
 'weight_decay_Hydroxylation-P': 3.9157740146230737}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11663.488
[2,     1] loss: 11620.000
[3,     1] loss: 11646.255
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010304634770798402,
 'learning_rate_Hydroxylation-K': 0.004019518938964858,
 'learning_rate_Hydroxylation-P': 0.008050040411712074,
 'log_base': 1.3269100116317831,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1906718808,
 'sample_weights': [35.82426179909109, 4.4782052366890674],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.8444131798131242,
 'weight_decay_Hydroxylation-K': 1.9690241155851367,
 'weight_decay_Hydroxylation-P': 3.253460559215299}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2163.661
[2,     1] loss: 2157.193
[3,     1] loss: 2156.417
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0027281006787088728,
 'learning_rate_Hydroxylation-K': 0.005034461050571836,
 'learning_rate_Hydroxylation-P': 0.006774836375227829,
 'log_base': 2.5422341478976476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4269979403,
 'sample_weights': [5.902159438135042, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.512459146911902,
 'weight_decay_Hydroxylation-K': 9.620925340020133,
 'weight_decay_Hydroxylation-P': 4.101152358552916}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.456
[2,     1] loss: 1289.646
[3,     1] loss: 1283.686
[4,     1] loss: 1286.492
[5,     1] loss: 1284.502
[6,     1] loss: 1279.234
[7,     1] loss: 1273.694
[8,     1] loss: 1260.790
[9,     1] loss: 1249.407
[10,     1] loss: 1216.630
[11,     1] loss: 1196.353
[12,     1] loss: 1162.030
[13,     1] loss: 1135.218
[14,     1] loss: 1072.629
[15,     1] loss: 1064.037
[16,     1] loss: 1091.095
[17,     1] loss: 1060.433
[18,     1] loss: 1068.700
[19,     1] loss: 1059.550
[20,     1] loss: 1038.038
[21,     1] loss: 1012.657
[22,     1] loss: 998.979
[23,     1] loss: 976.493
[24,     1] loss: 1026.130
[25,     1] loss: 979.924
[26,     1] loss: 966.386
[27,     1] loss: 956.691
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050194548952253015,
 'learning_rate_Hydroxylation-K': 0.008283970400377389,
 'learning_rate_Hydroxylation-P': 0.005424255711297472,
 'log_base': 1.0987294873849685,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1001416482,
 'sample_weights': [1.789245132392418, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.002777776211619,
 'weight_decay_Hydroxylation-K': 8.725779564176053,
 'weight_decay_Hydroxylation-P': 4.265081029229034}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5805.810
[2,     1] loss: 5798.716
[3,     1] loss: 5761.799
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004182999292300547,
 'learning_rate_Hydroxylation-K': 0.0028764352299664644,
 'learning_rate_Hydroxylation-P': 0.002707695384461835,
 'log_base': 2.940009542843694,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 170323951,
 'sample_weights': [17.730890539532552, 2.216446700579602],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.802322586350306,
 'weight_decay_Hydroxylation-K': 1.5962487943335417,
 'weight_decay_Hydroxylation-P': 0.23845361416202393}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.929
[2,     1] loss: 1236.849
[3,     1] loss: 1231.430
[4,     1] loss: 1232.027
[5,     1] loss: 1233.557
[6,     1] loss: 1232.782
[7,     1] loss: 1229.272
[8,     1] loss: 1222.239
[9,     1] loss: 1208.742
[10,     1] loss: 1181.403
[11,     1] loss: 1155.683
[12,     1] loss: 1136.560
[13,     1] loss: 1102.571
[14,     1] loss: 1067.738
[15,     1] loss: 1040.470
[16,     1] loss: 1040.973
[17,     1] loss: 1020.282
[18,     1] loss: 1005.369
[19,     1] loss: 1018.408
[20,     1] loss: 987.602
[21,     1] loss: 998.657
[22,     1] loss: 985.300
[23,     1] loss: 988.735
[24,     1] loss: 960.570
[25,     1] loss: 970.213
[26,     1] loss: 980.373
[27,     1] loss: 955.322
[28,     1] loss: 952.080
[29,     1] loss: 913.890
[30,     1] loss: 905.603
[31,     1] loss: 901.015
[32,     1] loss: 909.403
[33,     1] loss: 854.931
[34,     1] loss: 795.948
[35,     1] loss: 908.219
[36,     1] loss: 952.044
[37,     1] loss: 927.926
[38,     1] loss: 835.967
[39,     1] loss: 914.380
[40,     1] loss: 863.646
[41,     1] loss: 871.353
[42,     1] loss: 849.024
[43,     1] loss: 808.500
[44,     1] loss: 838.080
[45,     1] loss: 845.953
[46,     1] loss: 795.620
[47,     1] loss: 783.717
[48,     1] loss: 785.495
[49,     1] loss: 774.591
[50,     1] loss: 765.613
[51,     1] loss: 760.401
[52,     1] loss: 708.798
[53,     1] loss: 738.911
[54,     1] loss: 853.090
[55,     1] loss: 864.887
[56,     1] loss: 810.634
[57,     1] loss: 690.838
[58,     1] loss: 716.883
[59,     1] loss: 748.660
[60,     1] loss: 732.671
[61,     1] loss: 732.551
[62,     1] loss: 685.809
[63,     1] loss: 717.790
[64,     1] loss: 648.750
[65,     1] loss: 700.475
[66,     1] loss: 664.916
[67,     1] loss: 621.776
[68,     1] loss: 603.725
[69,     1] loss: 618.147
[70,     1] loss: 629.209
[71,     1] loss: 749.349
[72,     1] loss: 1208.389
[73,     1] loss: 1007.485
[74,     1] loss: 872.228
[75,     1] loss: 774.518
[76,     1] loss: 875.863
[77,     1] loss: 922.821
[78,     1] loss: 898.525
[79,     1] loss: 855.120
[80,     1] loss: 811.755
[81,     1] loss: 830.101
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008011810596974292,
 'learning_rate_Hydroxylation-K': 0.007206841843813466,
 'learning_rate_Hydroxylation-P': 0.0013452768438141241,
 'log_base': 1.0696748550188044,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1877274967,
 'sample_weights': [1.5480557216125999, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.727496883209456,
 'weight_decay_Hydroxylation-K': 6.847921540929619,
 'weight_decay_Hydroxylation-P': 6.588187138683525}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8095.946
[2,     1] loss: 8087.109
[3,     1] loss: 8054.122
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002248945915406178,
 'learning_rate_Hydroxylation-K': 0.004916617634503271,
 'learning_rate_Hydroxylation-P': 0.003127305623388429,
 'log_base': 1.8462180301498652,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1370565250,
 'sample_weights': [24.785834418503455, 3.0983486585469966],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0168648842368153,
 'weight_decay_Hydroxylation-K': 9.574492262804055,
 'weight_decay_Hydroxylation-P': 7.229653255741174}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1486.695
[2,     1] loss: 1483.119
[3,     1] loss: 1483.441
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003750165305673231,
 'learning_rate_Hydroxylation-K': 0.0032946521589969054,
 'learning_rate_Hydroxylation-P': 0.006896500195728137,
 'log_base': 1.2972920715290632,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3493514793,
 'sample_weights': [2.722779822735672, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.250469383435043,
 'weight_decay_Hydroxylation-K': 3.0583534296962864,
 'weight_decay_Hydroxylation-P': 7.3870265050779595}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2263.972
[2,     1] loss: 2248.377
[3,     1] loss: 2265.912
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005528181337263292,
 'learning_rate_Hydroxylation-K': 0.00836169452094778,
 'learning_rate_Hydroxylation-P': 0.008782354908321136,
 'log_base': 1.1387256500573215,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2268461596,
 'sample_weights': [6.414050684305583, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.393573637847741,
 'weight_decay_Hydroxylation-K': 9.99845222962946,
 'weight_decay_Hydroxylation-P': 4.328897138480919}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4182.830
[2,     1] loss: 4204.685
[3,     1] loss: 4161.261
[4,     1] loss: 4176.064
[5,     1] loss: 4169.431
[6,     1] loss: 4154.117
[7,     1] loss: 4180.074
[8,     1] loss: 4151.287
[9,     1] loss: 4170.073
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00378409560665584,
 'learning_rate_Hydroxylation-K': 0.009343879098759302,
 'learning_rate_Hydroxylation-P': 0.003526820474537342,
 'log_base': 1.1527233861655344,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1704618502,
 'sample_weights': [12.850788187226572, 1.6064104063989426],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.9165022838759,
 'weight_decay_Hydroxylation-K': 0.8892748302113436,
 'weight_decay_Hydroxylation-P': 5.703253356685516}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3824.595
[2,     1] loss: 3809.048
[3,     1] loss: 3803.575
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003488347834107771,
 'learning_rate_Hydroxylation-K': 0.00858369075589442,
 'learning_rate_Hydroxylation-P': 0.002459396259880531,
 'log_base': 1.1297519266314533,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 224043600,
 'sample_weights': [11.746111363560779, 1.468320483867316],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.053622736598957,
 'weight_decay_Hydroxylation-K': 9.476539173141088,
 'weight_decay_Hydroxylation-P': 9.33441267837651}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4467.324
[2,     1] loss: 4436.695
[3,     1] loss: 4429.286
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0029441369494143054,
 'learning_rate_Hydroxylation-K': 0.004902589375732149,
 'learning_rate_Hydroxylation-P': 0.0033098968421526927,
 'log_base': 1.6022151748554274,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 241031525,
 'sample_weights': [13.684176180300824, 1.710587918714733],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.664146772371536,
 'weight_decay_Hydroxylation-K': 3.577557990669429,
 'weight_decay_Hydroxylation-P': 0.1152719217214242}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1660.976
[2,     1] loss: 1657.814
[3,     1] loss: 1655.429
[4,     1] loss: 1651.887
[5,     1] loss: 1652.168
[6,     1] loss: 1653.313
[7,     1] loss: 1651.081
[8,     1] loss: 1648.461
[9,     1] loss: 1648.881
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0047191412726734615,
 'learning_rate_Hydroxylation-K': 0.0007620849569562684,
 'learning_rate_Hydroxylation-P': 0.008487032064899331,
 'log_base': 2.3301141466767885,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1982060358,
 'sample_weights': [3.5415541686250878, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.753966415061858,
 'weight_decay_Hydroxylation-K': 8.608973792399294,
 'weight_decay_Hydroxylation-P': 0.3427171005277895}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1324.258
[2,     1] loss: 1327.423
[3,     1] loss: 1326.035
[4,     1] loss: 1317.640
[5,     1] loss: 1321.864
[6,     1] loss: 1319.515
[7,     1] loss: 1322.818
[8,     1] loss: 1299.289
[9,     1] loss: 1276.423
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0010299958047830064,
 'learning_rate_Hydroxylation-K': 0.00949219420307576,
 'learning_rate_Hydroxylation-P': 0.0008851585342040122,
 'log_base': 1.936084919535082,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 281383764,
 'sample_weights': [1.9735300762064738, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5080799887989826,
 'weight_decay_Hydroxylation-K': 8.86920254717885,
 'weight_decay_Hydroxylation-P': 6.885202483667055}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1444.898
[2,     1] loss: 1440.990
[3,     1] loss: 1442.248
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005313935327631438,
 'learning_rate_Hydroxylation-K': 0.0030051221300673578,
 'learning_rate_Hydroxylation-P': 0.008999435062412325,
 'log_base': 1.550626683977954,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1369186299,
 'sample_weights': [2.5269023521689733, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.675918644980406,
 'weight_decay_Hydroxylation-K': 7.655196032558501,
 'weight_decay_Hydroxylation-P': 0.691302319124449}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1715.750
[2,     1] loss: 1715.047
[3,     1] loss: 1714.703
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005352845016413107,
 'learning_rate_Hydroxylation-K': 0.007682837839187718,
 'learning_rate_Hydroxylation-P': 0.0003115670762781388,
 'log_base': 1.0503123740632705,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2229781579,
 'sample_weights': [3.805786574379232, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.12149214186702,
 'weight_decay_Hydroxylation-K': 8.15645082082577,
 'weight_decay_Hydroxylation-P': 8.345589634955711}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11120.646
[2,     1] loss: 11130.899
[3,     1] loss: 11046.387
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0015377218563815125,
 'learning_rate_Hydroxylation-K': 0.002171432177608593,
 'learning_rate_Hydroxylation-P': 0.006018566427798919,
 'log_base': 1.8076248864794535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2949516460,
 'sample_weights': [34.009454528518596, 4.251345588659548],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.147519281825273,
 'weight_decay_Hydroxylation-K': 4.377817023550305,
 'weight_decay_Hydroxylation-P': 1.4004043083471684}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1508.600
[2,     1] loss: 1503.557
[3,     1] loss: 1502.067
[4,     1] loss: 1495.615
[5,     1] loss: 1506.899
[6,     1] loss: 1496.978
[7,     1] loss: 1495.428
[8,     1] loss: 1497.566
[9,     1] loss: 1491.307
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005668810282790315,
 'learning_rate_Hydroxylation-K': 0.0004802232524789555,
 'learning_rate_Hydroxylation-P': 0.0066462561916858865,
 'log_base': 1.7849368863077508,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3831632387,
 'sample_weights': [2.819939742744738, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.5840200278726595,
 'weight_decay_Hydroxylation-K': 9.127134416567745,
 'weight_decay_Hydroxylation-P': 2.6133368461199415}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1519.982
[2,     1] loss: 1515.736
[3,     1] loss: 1518.986
[4,     1] loss: 1518.840
[5,     1] loss: 1517.012
[6,     1] loss: 1508.736
[7,     1] loss: 1520.166
[8,     1] loss: 1512.990
[9,     1] loss: 1512.765
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00683570689633714,
 'learning_rate_Hydroxylation-K': 0.008528028262185421,
 'learning_rate_Hydroxylation-P': 0.001935848884666774,
 'log_base': 1.3052714920869597,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3654704189,
 'sample_weights': [2.88141520163973, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.695622846040413,
 'weight_decay_Hydroxylation-K': 7.8787343472117595,
 'weight_decay_Hydroxylation-P': 7.535601778442534}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2229.781
[2,     1] loss: 2220.972
[3,     1] loss: 2236.065
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005925546546273951,
 'learning_rate_Hydroxylation-K': 0.0001210758045984036,
 'learning_rate_Hydroxylation-P': 0.008344983234862812,
 'log_base': 2.255012933646957,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3083188496,
 'sample_weights': [6.2664183437886205, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.070546872422942,
 'weight_decay_Hydroxylation-K': 9.826426857712514,
 'weight_decay_Hydroxylation-P': 2.0836699762303725}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1344.337
[2,     1] loss: 1349.950
[3,     1] loss: 1342.672
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004025989205496091,
 'learning_rate_Hydroxylation-K': 0.008024468375284578,
 'learning_rate_Hydroxylation-P': 0.005328057561535777,
 'log_base': 2.8842402487498378,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 427542047,
 'sample_weights': [2.053042400137767, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.68440774971355,
 'weight_decay_Hydroxylation-K': 9.202095149861487,
 'weight_decay_Hydroxylation-P': 9.100470905860025}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.592
[2,     1] loss: 1243.204
[3,     1] loss: 1238.693
[4,     1] loss: 1236.718
[5,     1] loss: 1242.778
[6,     1] loss: 1238.302
[7,     1] loss: 1235.711
[8,     1] loss: 1229.312
[9,     1] loss: 1217.000
[10,     1] loss: 1202.564
[11,     1] loss: 1170.385
[12,     1] loss: 1128.861
[13,     1] loss: 1114.330
[14,     1] loss: 1049.876
[15,     1] loss: 1061.827
[16,     1] loss: 1061.416
[17,     1] loss: 1025.418
[18,     1] loss: 1015.316
[19,     1] loss: 995.433
[20,     1] loss: 1020.084
[21,     1] loss: 980.501
[22,     1] loss: 1009.514
[23,     1] loss: 953.427
[24,     1] loss: 960.437
[25,     1] loss: 944.136
[26,     1] loss: 936.237
[27,     1] loss: 965.665
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00670793548304861,
 'learning_rate_Hydroxylation-K': 0.009060468230791836,
 'learning_rate_Hydroxylation-P': 0.005568386908104812,
 'log_base': 1.1027531614175337,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2813758022,
 'sample_weights': [1.5760443629539196, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.465054930443573,
 'weight_decay_Hydroxylation-K': 9.238714400622333,
 'weight_decay_Hydroxylation-P': 0.6041852574673989}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5544.888
[2,     1] loss: 5531.041
[3,     1] loss: 5586.662
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006444332418968542,
 'learning_rate_Hydroxylation-K': 0.007376822095433992,
 'learning_rate_Hydroxylation-P': 0.007563645131998696,
 'log_base': 1.1465023892497233,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1550626871,
 'sample_weights': [17.068238380594288, 2.1336119897095527],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.053945222657115544,
 'weight_decay_Hydroxylation-K': 4.671990350823376,
 'weight_decay_Hydroxylation-P': 5.235855849782672}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3967.528
[2,     1] loss: 3958.395
[3,     1] loss: 3971.507
[4,     1] loss: 3966.337
[5,     1] loss: 3948.363
[6,     1] loss: 3962.556
[7,     1] loss: 3965.931
[8,     1] loss: 3958.792
[9,     1] loss: 3963.022
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00717087006086547,
 'learning_rate_Hydroxylation-K': 0.007755322154296722,
 'learning_rate_Hydroxylation-P': 0.0029767372311659507,
 'log_base': 1.0483176198751878,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3168726925,
 'sample_weights': [12.211038062988967, 1.526438560151269],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.664151768315186,
 'weight_decay_Hydroxylation-K': 9.480764377080746,
 'weight_decay_Hydroxylation-P': 5.437817686720306}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11503.526
[2,     1] loss: 11501.360
[3,     1] loss: 11537.098
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009170523235545028,
 'learning_rate_Hydroxylation-K': 0.008941788347693406,
 'learning_rate_Hydroxylation-P': 0.006331728945124794,
 'log_base': 2.1201654052540535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1421058076,
 'sample_weights': [35.379593133749964, 4.4226195122186],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.357481331792991,
 'weight_decay_Hydroxylation-K': 2.5079218563154897,
 'weight_decay_Hydroxylation-P': 7.811851889571634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1376.936
[2,     1] loss: 1383.617
[3,     1] loss: 1377.083
[4,     1] loss: 1382.008
[5,     1] loss: 1380.874
[6,     1] loss: 1375.998
[7,     1] loss: 1374.333
[8,     1] loss: 1376.367
[9,     1] loss: 1375.480
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002215502431687248,
 'learning_rate_Hydroxylation-K': 0.004841381923993089,
 'learning_rate_Hydroxylation-P': 0.004402694173342494,
 'log_base': 1.1186655963169458,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3967377692,
 'sample_weights': [2.221498654351117, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.476788731182826,
 'weight_decay_Hydroxylation-K': 8.845805219644465,
 'weight_decay_Hydroxylation-P': 5.454966770353321}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4836.701
[2,     1] loss: 4823.959
[3,     1] loss: 4817.640
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009750623548916471,
 'learning_rate_Hydroxylation-K': 0.004739798327026164,
 'learning_rate_Hydroxylation-P': 0.009925115801047794,
 'log_base': 1.0991619296674708,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1652008771,
 'sample_weights': [14.88759240169316, 1.8610207414419402],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.072681342779722,
 'weight_decay_Hydroxylation-K': 7.258614811955592,
 'weight_decay_Hydroxylation-P': 4.093793892900211}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5726.151
[2,     1] loss: 5727.139
[3,     1] loss: 5744.953
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006746411261017038,
 'learning_rate_Hydroxylation-K': 0.006404746847454169,
 'learning_rate_Hydroxylation-P': 0.0075571536078000625,
 'log_base': 1.152189010226143,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 553090425,
 'sample_weights': [17.657095004160812, 2.2072219032955895],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4336909490086212,
 'weight_decay_Hydroxylation-K': 7.627780396159663,
 'weight_decay_Hydroxylation-P': 4.134843169909343}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3802.906
[2,     1] loss: 3897.525
[3,     1] loss: 3828.744
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0024416025753670107,
 'learning_rate_Hydroxylation-K': 0.00871332922525771,
 'learning_rate_Hydroxylation-P': 0.003115725276398642,
 'log_base': 1.1179929025182462,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3052233737,
 'sample_weights': [11.784557991368752, 1.4731264974831242],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.192845044543898,
 'weight_decay_Hydroxylation-K': 8.514191657637836,
 'weight_decay_Hydroxylation-P': 5.36608475714992}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4855.196
[2,     1] loss: 4857.677
[3,     1] loss: 4854.538
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0060187011916469696,
 'learning_rate_Hydroxylation-K': 0.003328176664874937,
 'learning_rate_Hydroxylation-P': 0.009771750676863103,
 'log_base': 1.089523626801808,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1767600003,
 'sample_weights': [14.967882307645157, 1.8710573663221388],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.3668556143386836,
 'weight_decay_Hydroxylation-K': 1.2705623926994187,
 'weight_decay_Hydroxylation-P': 7.020415012161153}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6340.532
[2,     1] loss: 6306.856
[3,     1] loss: 6340.622
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009856750480829502,
 'learning_rate_Hydroxylation-K': 0.006000496784705043,
 'learning_rate_Hydroxylation-P': 0.0005106590022167113,
 'log_base': 2.4135583693347895,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3954198784,
 'sample_weights': [19.470868004530917, 2.4339522625692793],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.000843090921876,
 'weight_decay_Hydroxylation-K': 8.457844843772701,
 'weight_decay_Hydroxylation-P': 2.0184400816863746}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1307.555
[2,     1] loss: 1312.563
[3,     1] loss: 1316.034
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003744652963142183,
 'learning_rate_Hydroxylation-K': 0.0028730774584510466,
 'learning_rate_Hydroxylation-P': 0.007406696525995475,
 'log_base': 1.3677559586694377,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 611172,
 'sample_weights': [1.8947214324970865, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8581111744883527,
 'weight_decay_Hydroxylation-K': 5.067325380033829,
 'weight_decay_Hydroxylation-P': 6.759858005544877}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2038.049
[2,     1] loss: 2047.322
[3,     1] loss: 2030.661
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009569549538807237,
 'learning_rate_Hydroxylation-K': 0.006323063396866048,
 'learning_rate_Hydroxylation-P': 0.007445869642595012,
 'log_base': 1.0197980487593248,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3686227450,
 'sample_weights': [5.330764850958444, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0070568318520425,
 'weight_decay_Hydroxylation-K': 5.045821273051179,
 'weight_decay_Hydroxylation-P': 6.074624876269475}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27400.664
Exploding loss, terminate run (best metric=0.5559990406036377)
Finished Training
Total time taken: 0.22002458572387695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27492.328
Exploding loss, terminate run (best metric=0.5407864451408386)
Finished Training
Total time taken: 0.20702314376831055
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27711.596
Exploding loss, terminate run (best metric=0.5412001013755798)
Finished Training
Total time taken: 0.20702648162841797
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27586.582
Exploding loss, terminate run (best metric=0.5587437152862549)
Finished Training
Total time taken: 0.21602296829223633
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 27721.742
Exploding loss, terminate run (best metric=0.5280652642250061)
Finished Training
Total time taken: 0.22202277183532715
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27746.941
Exploding loss, terminate run (best metric=0.5316861867904663)
Finished Training
Total time taken: 0.21102523803710938
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27670.949
Exploding loss, terminate run (best metric=0.5320961475372314)
Finished Training
Total time taken: 0.2320249080657959
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27607.699
Exploding loss, terminate run (best metric=0.5275171995162964)
Finished Training
Total time taken: 0.20502042770385742
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27627.613
Exploding loss, terminate run (best metric=0.5263170599937439)
Finished Training
Total time taken: 0.2250208854675293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 27631.221
Exploding loss, terminate run (best metric=0.5523819327354431)
Finished Training
Total time taken: 0.2030184268951416
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27823.969
Exploding loss, terminate run (best metric=0.5320095419883728)
Finished Training
Total time taken: 0.20902299880981445
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27651.404
Exploding loss, terminate run (best metric=0.5270795226097107)
Finished Training
Total time taken: 0.21502232551574707
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27729.383
Exploding loss, terminate run (best metric=0.526672899723053)
Finished Training
Total time taken: 0.22802376747131348
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 27654.430
Exploding loss, terminate run (best metric=0.5271018743515015)
Finished Training
Total time taken: 0.2080233097076416
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 27690.123
Exploding loss, terminate run (best metric=0.5277162790298462)
Finished Training
Total time taken: 0.22702312469482422
{'Hydroxylation-K Validation Accuracy': 0.400531914893617, 'Hydroxylation-K Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-K Validation Specificity': 0.3333333333333333, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6877192982456141, 'Hydroxylation-K AUC PR': 0.3532509239607132, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.22364532019704436, 'Validation Loss (Hydroxylation-K)': 0.5626489718755087, 'Hydroxylation-P Validation Accuracy': 0.3928431382501734, 'Hydroxylation-P Validation Sensitivity': 0.6666666666666666, 'Hydroxylation-P Validation Specificity': 0.3333333333333333, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5810741223684512, 'Hydroxylation-P AUC PR': 0.26877638791102143, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.20078927347422293, 'Validation Loss (Hydroxylation-P)': 0.5356915473937989, 'Validation Loss (total)': 1.0983405113220215, 'TimeToTrain': 0.21568969090779622}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007741692499343573,
 'learning_rate_Hydroxylation-K': 0.0019140951752868972,
 'learning_rate_Hydroxylation-P': 0.005056756229089429,
 'log_base': 2.6230078852174037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1380885540,
 'sample_weights': [85.21878284888497, 10.630214159325668],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.436726670756659,
 'weight_decay_Hydroxylation-K': 3.946244245441463,
 'weight_decay_Hydroxylation-P': 9.597889568465328}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1283.229
[2,     1] loss: 1273.837
[3,     1] loss: 1272.967
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008942048568764755,
 'learning_rate_Hydroxylation-K': 0.006334150810692368,
 'learning_rate_Hydroxylation-P': 0.007764421050597389,
 'log_base': 1.0428465413166863,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 552620320,
 'sample_weights': [1.7312097562422997, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.4953763927886723,
 'weight_decay_Hydroxylation-K': 3.9788617145533105,
 'weight_decay_Hydroxylation-P': 6.191549945591331}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 12969.572
[2,     1] loss: 12867.469
[3,     1] loss: 12977.104
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016221063022021289,
 'learning_rate_Hydroxylation-K': 0.008910017573975373,
 'learning_rate_Hydroxylation-P': 0.0006947021244607357,
 'log_base': 2.3568343584357923,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1406007597,
 'sample_weights': [39.792196854985356, 4.974216226274864],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.263417598468607,
 'weight_decay_Hydroxylation-K': 2.370968533333808,
 'weight_decay_Hydroxylation-P': 5.038217729881234}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.536
[2,     1] loss: 1316.890
[3,     1] loss: 1321.547
[4,     1] loss: 1318.330
[5,     1] loss: 1323.507
[6,     1] loss: 1316.320
[7,     1] loss: 1317.110
[8,     1] loss: 1313.954
[9,     1] loss: 1308.938
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004477789932322,
 'learning_rate_Hydroxylation-K': 0.0023619946936196162,
 'learning_rate_Hydroxylation-P': 0.007270470023581888,
 'log_base': 2.207487192626751,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1797170726,
 'sample_weights': [1.9472827215079505, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.3587248403084144,
 'weight_decay_Hydroxylation-K': 2.169861539515807,
 'weight_decay_Hydroxylation-P': 0.8209807410282455}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1359.446
[2,     1] loss: 1359.801
[3,     1] loss: 1352.517
[4,     1] loss: 1354.372
[5,     1] loss: 1353.655
[6,     1] loss: 1345.986
[7,     1] loss: 1352.713
[8,     1] loss: 1350.282
[9,     1] loss: 1350.912
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00843402238403277,
 'learning_rate_Hydroxylation-K': 0.00938764210088543,
 'learning_rate_Hydroxylation-P': 0.008558193697574313,
 'log_base': 1.0511554984416827,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2301997471,
 'sample_weights': [2.10826914021911, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.271807841886824,
 'weight_decay_Hydroxylation-K': 5.922959908024953,
 'weight_decay_Hydroxylation-P': 3.798747658122716}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10829.177
[2,     1] loss: 11066.787
[3,     1] loss: 10867.172
[4,     1] loss: 10863.759
[5,     1] loss: 10866.514
[6,     1] loss: 10865.538
[7,     1] loss: 10874.449
[8,     1] loss: 10842.365
[9,     1] loss: 10850.266
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016046735054082552,
 'learning_rate_Hydroxylation-K': 0.0040364739512586895,
 'learning_rate_Hydroxylation-P': 0.005913484153948211,
 'log_base': 1.4910416942606717,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3480328264,
 'sample_weights': [33.46245773662486, 4.182968355608712],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.0263812919258655,
 'weight_decay_Hydroxylation-K': 0.5686333404286582,
 'weight_decay_Hydroxylation-P': 3.912784026111058}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1790.961
[2,     1] loss: 1799.839
[3,     1] loss: 1786.237
[4,     1] loss: 1790.457
[5,     1] loss: 1785.890
[6,     1] loss: 1782.631
[7,     1] loss: 1792.399
[8,     1] loss: 1786.667
[9,     1] loss: 1780.487
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0062631321019098055,
 'learning_rate_Hydroxylation-K': 0.0009105311132903602,
 'learning_rate_Hydroxylation-P': 0.0037465836758955747,
 'log_base': 1.8904803056758812,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3495884189,
 'sample_weights': [4.179092934836128, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.012869550020165,
 'weight_decay_Hydroxylation-K': 6.754658118484455,
 'weight_decay_Hydroxylation-P': 0.7362182683171278}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1468.653
[2,     1] loss: 1461.090
[3,     1] loss: 1457.671
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009197122451706253,
 'learning_rate_Hydroxylation-K': 0.0038474475235100573,
 'learning_rate_Hydroxylation-P': 0.006181485586575576,
 'log_base': 1.0402885932751191,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3383141593,
 'sample_weights': [2.621485667919101, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7764370667775295,
 'weight_decay_Hydroxylation-K': 5.185267728865924,
 'weight_decay_Hydroxylation-P': 6.233894276485575}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13779.885
[2,     1] loss: 13775.826
[3,     1] loss: 13864.122
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007111720362314972,
 'learning_rate_Hydroxylation-K': 0.003480202743378077,
 'learning_rate_Hydroxylation-P': 0.0010077724107196793,
 'log_base': 2.825120024392776,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1400094864,
 'sample_weights': [42.26634357504946, 5.283496480541241],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.547389677115927,
 'weight_decay_Hydroxylation-K': 8.700975006410854,
 'weight_decay_Hydroxylation-P': 7.506421841188905}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.472
[2,     1] loss: 1270.390
[3,     1] loss: 1248.064
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009172204714162776,
 'learning_rate_Hydroxylation-K': 0.009623700323143662,
 'learning_rate_Hydroxylation-P': 0.004310909076987919,
 'log_base': 1.0673192915953456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3012555623,
 'sample_weights': [1.6074736707322286, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.047777144854837,
 'weight_decay_Hydroxylation-K': 8.71121883257772,
 'weight_decay_Hydroxylation-P': 9.060928208673287}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8312.974
[2,     1] loss: 8300.274
[3,     1] loss: 8340.199
[4,     1] loss: 8307.915
[5,     1] loss: 8323.033
[6,     1] loss: 8289.295
[7,     1] loss: 8292.392
[8,     1] loss: 8306.977
[9,     1] loss: 8287.928
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008716860747233048,
 'learning_rate_Hydroxylation-K': 0.007737191788683881,
 'learning_rate_Hydroxylation-P': 0.009846394341205869,
 'log_base': 1.0661816111504951,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1988223693,
 'sample_weights': [25.62454019159492, 3.2031909189727186],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6027663285814895,
 'weight_decay_Hydroxylation-K': 5.3876805758105,
 'weight_decay_Hydroxylation-P': 4.804008404415124}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 8458.530
[2,     1] loss: 8490.673
[3,     1] loss: 8440.795
[4,     1] loss: 8452.876
[5,     1] loss: 8459.342
[6,     1] loss: 8379.823
[7,     1] loss: 8498.764
[8,     1] loss: 8473.139
[9,     1] loss: 8481.745
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003475491809634363,
 'learning_rate_Hydroxylation-K': 0.0011243269249763695,
 'learning_rate_Hydroxylation-P': 0.0004037708897279141,
 'log_base': 2.7919577363381607,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1321544688,
 'sample_weights': [26.0509882579619, 3.256498980830088],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6288616345556766,
 'weight_decay_Hydroxylation-K': 3.0267531169205366,
 'weight_decay_Hydroxylation-P': 0.5672028427831064}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.016
[2,     1] loss: 1248.811
[3,     1] loss: 1250.866
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006071698658366406,
 'learning_rate_Hydroxylation-K': 0.009140123861969163,
 'learning_rate_Hydroxylation-P': 0.0012867419154421833,
 'log_base': 1.1461611787975063,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4190486344,
 'sample_weights': [1.6259600215443937, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.384302232610148,
 'weight_decay_Hydroxylation-K': 7.286735198500917,
 'weight_decay_Hydroxylation-P': 6.256760077260284}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3971.255
[2,     1] loss: 3986.480
[3,     1] loss: 3968.462
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009776637838926452,
 'learning_rate_Hydroxylation-K': 0.004750628142545256,
 'learning_rate_Hydroxylation-P': 0.006330176697164207,
 'log_base': 1.1036577565474797,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2551248204,
 'sample_weights': [12.237681613394088, 1.5297691322539781],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.1146174972801965,
 'weight_decay_Hydroxylation-K': 3.4797960491322844,
 'weight_decay_Hydroxylation-P': 3.389455225356067}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 5478.244
[2,     1] loss: 5542.530
[3,     1] loss: 5500.635
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009296633692456447,
 'learning_rate_Hydroxylation-K': 0.004930955347997508,
 'learning_rate_Hydroxylation-P': 0.008670238891656721,
 'log_base': 1.443613742611332,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2929133959,
 'sample_weights': [16.92633982031749, 2.1158739863621587],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.569564800814091,
 'weight_decay_Hydroxylation-K': 4.0535455334379105,
 'weight_decay_Hydroxylation-P': 4.555698237197196}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1876.073
[2,     1] loss: 1880.891
[3,     1] loss: 1867.255
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009049939981013829,
 'learning_rate_Hydroxylation-K': 0.0078025653427502145,
 'learning_rate_Hydroxylation-P': 0.008123439003792081,
 'log_base': 1.3084699104434652,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3968178853,
 'sample_weights': [4.5470389756569976, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.9645503153972164,
 'weight_decay_Hydroxylation-K': 6.48455706527822,
 'weight_decay_Hydroxylation-P': 6.3618624017780325}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2219.322
[2,     1] loss: 2230.573
[3,     1] loss: 2238.136
[4,     1] loss: 2212.815
[5,     1] loss: 2228.846
[6,     1] loss: 2221.342
[7,     1] loss: 2221.904
[8,     1] loss: 2219.734
[9,     1] loss: 2220.772
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009738600647850764,
 'learning_rate_Hydroxylation-K': 0.004902596626479595,
 'learning_rate_Hydroxylation-P': 0.009123816011674335,
 'log_base': 1.2413970878207339,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 833468434,
 'sample_weights': [6.209375842198628, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5785865416239813,
 'weight_decay_Hydroxylation-K': 5.107416224003168,
 'weight_decay_Hydroxylation-P': 6.563817894117554}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2532.006
[2,     1] loss: 2546.846
[3,     1] loss: 2581.401
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004059931398903243,
 'learning_rate_Hydroxylation-K': 0.008119751952235762,
 'learning_rate_Hydroxylation-P': 0.00014720696473287338,
 'log_base': 1.1428685494810873,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1896996620,
 'sample_weights': [7.720417109483232, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.975722217580955,
 'weight_decay_Hydroxylation-K': 9.66775970829953,
 'weight_decay_Hydroxylation-P': 7.1963595952888015}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 4066.009
[2,     1] loss: 4044.110
[3,     1] loss: 4063.167
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0011477021426618218,
 'learning_rate_Hydroxylation-K': 0.0019657842076554457,
 'learning_rate_Hydroxylation-P': 0.006272956222520227,
 'log_base': 1.2545887567041056,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1731453683,
 'sample_weights': [12.50131779602009, 1.5627249246227808],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.163762292766239,
 'weight_decay_Hydroxylation-K': 7.708621464874508,
 'weight_decay_Hydroxylation-P': 0.43587687313179707}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2479.155
[2,     1] loss: 2463.145
[3,     1] loss: 2469.177
[4,     1] loss: 2461.243
[5,     1] loss: 2462.777
[6,     1] loss: 2473.654
[7,     1] loss: 2456.185
[8,     1] loss: 2457.942
[9,     1] loss: 2459.186
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009578803986165136,
 'learning_rate_Hydroxylation-K': 0.005017404696243904,
 'learning_rate_Hydroxylation-P': 0.007234066798768087,
 'log_base': 1.171328042859941,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 427413226,
 'sample_weights': [7.360606160506862, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.619987970598005,
 'weight_decay_Hydroxylation-K': 5.235291001326837,
 'weight_decay_Hydroxylation-P': 8.449818079031063}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3429.881
[2,     1] loss: 3414.177
[3,     1] loss: 3426.739
[4,     1] loss: 3429.462
[5,     1] loss: 3451.087
[6,     1] loss: 3409.243
[7,     1] loss: 3434.392
[8,     1] loss: 3439.562
[9,     1] loss: 3416.542
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009657857030056918,
 'learning_rate_Hydroxylation-K': 0.005944045834751018,
 'learning_rate_Hydroxylation-P': 0.007913374089564263,
 'log_base': 1.171836425538911,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3785405625,
 'sample_weights': [10.556862993636306, 1.3196587107989084],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.4820014672943036,
 'weight_decay_Hydroxylation-K': 5.6473498296962115,
 'weight_decay_Hydroxylation-P': 5.895585443742805}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3417.223
[2,     1] loss: 3406.738
[3,     1] loss: 3430.781
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006836543022419119,
 'learning_rate_Hydroxylation-K': 0.009288335391313282,
 'learning_rate_Hydroxylation-P': 0.00865335728517498,
 'log_base': 1.1683091817043803,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3912711592,
 'sample_weights': [10.527974423444848, 1.3160475004120098],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6181103907500195,
 'weight_decay_Hydroxylation-K': 5.471531433539915,
 'weight_decay_Hydroxylation-P': 1.2665289932898975}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3500.748
[2,     1] loss: 3479.227
[3,     1] loss: 3490.654
[4,     1] loss: 3511.166
[5,     1] loss: 3492.789
[6,     1] loss: 3474.052
[7,     1] loss: 3466.137
[8,     1] loss: 3482.791
[9,     1] loss: 3470.671
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006522274880994091,
 'learning_rate_Hydroxylation-K': 0.007629962580062498,
 'learning_rate_Hydroxylation-P': 0.009621178298517831,
 'log_base': 1.0911946435049626,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2751788371,
 'sample_weights': [10.731996247668587, 1.3415512109074703],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.0284696140062375,
 'weight_decay_Hydroxylation-K': 7.231813443454054,
 'weight_decay_Hydroxylation-P': 5.786943876848669}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 6255.148
[2,     1] loss: 6196.576
[3,     1] loss: 6262.969
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0023923656764517956,
 'learning_rate_Hydroxylation-K': 0.001874326351076494,
 'learning_rate_Hydroxylation-P': 0.0034570127687733377,
 'log_base': 2.248773340145494,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 190785741,
 'sample_weights': [19.12895452468263, 2.3912114311032133],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.191048016284668,
 'weight_decay_Hydroxylation-K': 6.7037661880956145,
 'weight_decay_Hydroxylation-P': 2.400721462986167}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1351.728
[2,     1] loss: 1345.930
[3,     1] loss: 1340.861
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005577030493091208,
 'learning_rate_Hydroxylation-K': 0.0016299383833193607,
 'learning_rate_Hydroxylation-P': 0.007295547313463624,
 'log_base': 2.0119286994177332,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2590180248,
 'sample_weights': [2.0600620489289936, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.72249099404205,
 'weight_decay_Hydroxylation-K': 5.820320165995,
 'weight_decay_Hydroxylation-P': 1.956667890584962}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1409.850
[2,     1] loss: 1415.289
[3,     1] loss: 1411.673
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005313146933231387,
 'learning_rate_Hydroxylation-K': 0.003413452525316567,
 'learning_rate_Hydroxylation-P': 0.007400664979207084,
 'log_base': 1.0346105980337672,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4291117880,
 'sample_weights': [2.3880101842188446, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.571055595386689,
 'weight_decay_Hydroxylation-K': 3.0591567374690305,
 'weight_decay_Hydroxylation-P': 4.891917300778856}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15922.065
Exploding loss, terminate run (best metric=0.5317288637161255)
Finished Training
Total time taken: 0.19802117347717285
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15931.222
Exploding loss, terminate run (best metric=0.5293288230895996)
Finished Training
Total time taken: 0.21802473068237305
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15991.381
Exploding loss, terminate run (best metric=0.5299797058105469)
Finished Training
Total time taken: 0.2100224494934082
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15940.849
Exploding loss, terminate run (best metric=0.5270261764526367)
Finished Training
Total time taken: 0.22002577781677246
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15923.379
Exploding loss, terminate run (best metric=0.5296606421470642)
Finished Training
Total time taken: 0.21302485466003418
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15948.754
Exploding loss, terminate run (best metric=0.5377854704856873)
Finished Training
Total time taken: 0.2280254364013672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15931.811
Exploding loss, terminate run (best metric=0.5268653631210327)
Finished Training
Total time taken: 0.21802353858947754
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15947.438
Exploding loss, terminate run (best metric=0.5336079001426697)
Finished Training
Total time taken: 0.22502470016479492
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 16015.188
Exploding loss, terminate run (best metric=0.534030020236969)
Finished Training
Total time taken: 0.2120225429534912
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 16038.338
Exploding loss, terminate run (best metric=0.5294926762580872)
Finished Training
Total time taken: 0.2210242748260498
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15890.584
Exploding loss, terminate run (best metric=0.5410873293876648)
Finished Training
Total time taken: 0.20502448081970215
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15891.789
Exploding loss, terminate run (best metric=0.5289446711540222)
Finished Training
Total time taken: 0.22802448272705078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15936.416
Exploding loss, terminate run (best metric=0.5303182601928711)
Finished Training
Total time taken: 0.21302270889282227
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 15999.721
Exploding loss, terminate run (best metric=0.5262256860733032)
Finished Training
Total time taken: 0.22202444076538086
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 15883.820
Exploding loss, terminate run (best metric=0.5420925617218018)
Finished Training
Total time taken: 0.20601844787597656
{'Hydroxylation-K Validation Accuracy': 0.5560874704491725, 'Hydroxylation-K Validation Sensitivity': 0.4, 'Hydroxylation-K Validation Specificity': 0.6, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.5850292397660819, 'Hydroxylation-K AUC PR': 0.32037746987306154, 'Hydroxylation-K MCC': 0.0, 'Hydroxylation-K F1': 0.1316912972085386, 'Validation Loss (Hydroxylation-K)': 0.5568660060564677, 'Hydroxylation-P Validation Accuracy': 0.5646086323875269, 'Hydroxylation-P Validation Sensitivity': 0.4, 'Hydroxylation-P Validation Specificity': 0.6, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6118493183763272, 'Hydroxylation-P AUC PR': 0.2973189490995427, 'Hydroxylation-P MCC': 0.0, 'Hydroxylation-P F1': 0.12022526748648812, 'Validation Loss (Hydroxylation-P)': 0.5318782766660054, 'Validation Loss (total)': 1.0887442827224731, 'TimeToTrain': 0.2158236026763916}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00809912130907888,
 'learning_rate_Hydroxylation-K': 0.0036059766710647445,
 'learning_rate_Hydroxylation-P': 0.004127327745426687,
 'log_base': 2.852661338147535,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2489022416,
 'sample_weights': [49.10141191582546, 6.124923482139587],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.911628420584148,
 'weight_decay_Hydroxylation-K': 1.9830370660697005,
 'weight_decay_Hydroxylation-P': 7.515894326744126}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.337
[2,     1] loss: 1244.641
[3,     1] loss: 1244.000
[4,     1] loss: 1242.862
[5,     1] loss: 1234.868
[6,     1] loss: 1226.040
[7,     1] loss: 1205.631
[8,     1] loss: 1165.218
[9,     1] loss: 1121.822
[10,     1] loss: 1070.370
[11,     1] loss: 1082.050
[12,     1] loss: 1130.189
[13,     1] loss: 1027.851
[14,     1] loss: 1106.168
[15,     1] loss: 1032.204
[16,     1] loss: 1076.637
[17,     1] loss: 1039.082
[18,     1] loss: 985.133
[19,     1] loss: 1057.555
[20,     1] loss: 949.091
[21,     1] loss: 1003.272
[22,     1] loss: 954.279
[23,     1] loss: 970.289
[24,     1] loss: 961.348
[25,     1] loss: 930.100
[26,     1] loss: 972.069
[27,     1] loss: 923.254
[28,     1] loss: 909.596
[29,     1] loss: 881.931
[30,     1] loss: 882.688
[31,     1] loss: 902.229
[32,     1] loss: 897.378
[33,     1] loss: 900.351
[34,     1] loss: 854.122
[35,     1] loss: 870.205
[36,     1] loss: 839.305
[37,     1] loss: 817.370
[38,     1] loss: 1025.932
[39,     1] loss: 992.004
[40,     1] loss: 869.249
[41,     1] loss: 902.061
[42,     1] loss: 947.720
[43,     1] loss: 898.146
[44,     1] loss: 887.072
[45,     1] loss: 869.184
[46,     1] loss: 844.328
[47,     1] loss: 930.353
[48,     1] loss: 816.725
[49,     1] loss: 871.463
[50,     1] loss: 735.969
[51,     1] loss: 799.958
[52,     1] loss: 842.864
[53,     1] loss: 737.230
[54,     1] loss: 842.643
[55,     1] loss: 705.257
[56,     1] loss: 765.430
[57,     1] loss: 770.893
[58,     1] loss: 676.020
[59,     1] loss: 663.299
[60,     1] loss: 653.128
[61,     1] loss: 665.421
[62,     1] loss: 722.691
[63,     1] loss: 1466.928
[64,     1] loss: 1284.789
[65,     1] loss: 1029.481
[66,     1] loss: 1081.020
[67,     1] loss: 1129.538
[68,     1] loss: 1130.143
[69,     1] loss: 1108.990
[70,     1] loss: 1155.430
[71,     1] loss: 1113.131
[72,     1] loss: 1104.582
[73,     1] loss: 1086.896
[74,     1] loss: 1056.656
[75,     1] loss: 1041.545
[76,     1] loss: 998.259
[77,     1] loss: 1001.195
[78,     1] loss: 1001.445
[79,     1] loss: 946.372
[80,     1] loss: 964.558
[81,     1] loss: 958.451
[82,     1] loss: 956.471
[83,     1] loss: 911.469
[84,     1] loss: 959.477
[85,     1] loss: 939.856
[86,     1] loss: 906.301
[87,     1] loss: 919.083
[88,     1] loss: 917.735
[89,     1] loss: 892.803
[90,     1] loss: 865.625
[91,     1] loss: 844.929
[92,     1] loss: 827.805
[93,     1] loss: 868.184
[94,     1] loss: 816.938
[95,     1] loss: 927.428
[96,     1] loss: 1572.581
[97,     1] loss: 965.294
Early stopping applied (best metric=0.38068678975105286)
Finished Training
Total time taken: 13.99849534034729
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.667
[2,     1] loss: 1242.385
[3,     1] loss: 1244.899
[4,     1] loss: 1246.906
[5,     1] loss: 1244.031
[6,     1] loss: 1236.753
[7,     1] loss: 1229.969
[8,     1] loss: 1220.933
[9,     1] loss: 1209.034
[10,     1] loss: 1164.070
[11,     1] loss: 1101.321
[12,     1] loss: 1053.296
[13,     1] loss: 1080.034
[14,     1] loss: 1076.563
[15,     1] loss: 1004.389
[16,     1] loss: 973.518
[17,     1] loss: 987.937
[18,     1] loss: 925.162
[19,     1] loss: 928.589
[20,     1] loss: 985.242
[21,     1] loss: 961.413
[22,     1] loss: 945.796
[23,     1] loss: 905.654
[24,     1] loss: 937.070
[25,     1] loss: 889.011
[26,     1] loss: 860.890
[27,     1] loss: 899.851
[28,     1] loss: 858.391
[29,     1] loss: 841.427
[30,     1] loss: 946.333
[31,     1] loss: 1222.327
[32,     1] loss: 848.706
[33,     1] loss: 964.759
[34,     1] loss: 902.946
[35,     1] loss: 902.807
[36,     1] loss: 944.949
[37,     1] loss: 885.739
[38,     1] loss: 837.412
[39,     1] loss: 846.275
[40,     1] loss: 808.780
[41,     1] loss: 885.453
[42,     1] loss: 806.518
[43,     1] loss: 888.093
[44,     1] loss: 704.472
[45,     1] loss: 805.299
[46,     1] loss: 785.905
[47,     1] loss: 785.813
[48,     1] loss: 789.682
[49,     1] loss: 768.143
[50,     1] loss: 735.135
[51,     1] loss: 663.779
[52,     1] loss: 756.069
[53,     1] loss: 718.358
[54,     1] loss: 695.775
[55,     1] loss: 713.022
[56,     1] loss: 716.718
[57,     1] loss: 672.302
[58,     1] loss: 713.440
[59,     1] loss: 767.828
[60,     1] loss: 607.863
[61,     1] loss: 926.120
[62,     1] loss: 1368.767
[63,     1] loss: 1111.676
[64,     1] loss: 853.333
[65,     1] loss: 1097.843
[66,     1] loss: 1003.446
[67,     1] loss: 1044.437
[68,     1] loss: 1068.171
[69,     1] loss: 1041.680
[70,     1] loss: 970.438
[71,     1] loss: 954.814
[72,     1] loss: 965.289
[73,     1] loss: 928.882
[74,     1] loss: 906.940
[75,     1] loss: 925.171
[76,     1] loss: 881.526
[77,     1] loss: 867.071
[78,     1] loss: 849.413
[79,     1] loss: 855.839
[80,     1] loss: 797.056
[81,     1] loss: 781.747
[82,     1] loss: 731.711
[83,     1] loss: 750.316
[84,     1] loss: 759.990
[85,     1] loss: 856.928
[86,     1] loss: 857.659
[87,     1] loss: 879.245
[88,     1] loss: 785.225
[89,     1] loss: 767.218
[90,     1] loss: 786.954
[91,     1] loss: 791.738
[92,     1] loss: 705.869
[93,     1] loss: 832.676
[94,     1] loss: 709.514
[95,     1] loss: 755.099
Early stopping applied (best metric=0.36463651061058044)
Finished Training
Total time taken: 13.66645860671997
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.186
[2,     1] loss: 1242.261
[3,     1] loss: 1242.520
[4,     1] loss: 1248.949
[5,     1] loss: 1244.801
[6,     1] loss: 1240.133
[7,     1] loss: 1242.515
[8,     1] loss: 1243.064
[9,     1] loss: 1234.937
[10,     1] loss: 1232.781
[11,     1] loss: 1213.541
[12,     1] loss: 1197.467
[13,     1] loss: 1181.495
[14,     1] loss: 1137.456
[15,     1] loss: 1093.955
[16,     1] loss: 1066.412
[17,     1] loss: 1021.956
[18,     1] loss: 1059.307
[19,     1] loss: 1112.137
[20,     1] loss: 1017.552
[21,     1] loss: 1005.766
[22,     1] loss: 994.288
[23,     1] loss: 1022.369
[24,     1] loss: 992.911
[25,     1] loss: 985.850
[26,     1] loss: 934.320
[27,     1] loss: 971.422
[28,     1] loss: 921.101
[29,     1] loss: 920.683
[30,     1] loss: 852.631
[31,     1] loss: 900.588
[32,     1] loss: 904.763
[33,     1] loss: 916.741
[34,     1] loss: 866.473
[35,     1] loss: 874.011
[36,     1] loss: 861.771
[37,     1] loss: 842.630
[38,     1] loss: 819.417
[39,     1] loss: 828.583
[40,     1] loss: 787.864
[41,     1] loss: 839.685
[42,     1] loss: 1229.108
[43,     1] loss: 896.251
[44,     1] loss: 971.591
[45,     1] loss: 934.802
[46,     1] loss: 959.382
[47,     1] loss: 965.101
[48,     1] loss: 892.828
[49,     1] loss: 879.391
[50,     1] loss: 959.023
[51,     1] loss: 887.702
[52,     1] loss: 805.697
[53,     1] loss: 822.566
[54,     1] loss: 811.732
[55,     1] loss: 828.546
[56,     1] loss: 775.145
[57,     1] loss: 785.682
[58,     1] loss: 754.312
[59,     1] loss: 714.123
[60,     1] loss: 719.241
[61,     1] loss: 713.749
[62,     1] loss: 738.080
[63,     1] loss: 697.873
[64,     1] loss: 772.418
[65,     1] loss: 995.401
[66,     1] loss: 747.305
[67,     1] loss: 963.202
[68,     1] loss: 835.497
[69,     1] loss: 932.088
[70,     1] loss: 820.713
[71,     1] loss: 839.822
[72,     1] loss: 791.503
[73,     1] loss: 695.491
[74,     1] loss: 756.984
[75,     1] loss: 708.436
[76,     1] loss: 701.508
[77,     1] loss: 729.748
[78,     1] loss: 594.600
[79,     1] loss: 726.618
[80,     1] loss: 798.469
[81,     1] loss: 656.973
[82,     1] loss: 695.372
[83,     1] loss: 852.884
[84,     1] loss: 618.505
Early stopping applied (best metric=0.3619101345539093)
Finished Training
Total time taken: 12.063289403915405
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.938
[2,     1] loss: 1263.323
[3,     1] loss: 1243.720
[4,     1] loss: 1245.531
[5,     1] loss: 1245.904
[6,     1] loss: 1242.898
[7,     1] loss: 1244.302
[8,     1] loss: 1243.228
[9,     1] loss: 1244.007
[10,     1] loss: 1244.484
[11,     1] loss: 1243.792
[12,     1] loss: 1243.341
[13,     1] loss: 1243.209
[14,     1] loss: 1243.019
[15,     1] loss: 1244.133
[16,     1] loss: 1242.143
[17,     1] loss: 1244.721
[18,     1] loss: 1243.792
[19,     1] loss: 1242.949
[20,     1] loss: 1243.282
[21,     1] loss: 1242.024
[22,     1] loss: 1242.684
[23,     1] loss: 1241.834
[24,     1] loss: 1240.728
[25,     1] loss: 1238.129
[26,     1] loss: 1232.188
[27,     1] loss: 1222.548
[28,     1] loss: 1202.060
[29,     1] loss: 1184.926
[30,     1] loss: 1158.682
[31,     1] loss: 1147.246
[32,     1] loss: 1099.178
[33,     1] loss: 1093.007
[34,     1] loss: 1020.709
[35,     1] loss: 1062.171
[36,     1] loss: 1100.836
[37,     1] loss: 1019.670
[38,     1] loss: 1081.745
[39,     1] loss: 954.761
[40,     1] loss: 1044.232
[41,     1] loss: 970.625
[42,     1] loss: 970.305
[43,     1] loss: 944.611
[44,     1] loss: 950.434
[45,     1] loss: 976.112
[46,     1] loss: 968.854
[47,     1] loss: 924.813
[48,     1] loss: 939.733
[49,     1] loss: 924.619
[50,     1] loss: 911.207
[51,     1] loss: 914.263
[52,     1] loss: 900.911
[53,     1] loss: 879.395
[54,     1] loss: 834.172
[55,     1] loss: 844.554
[56,     1] loss: 812.126
[57,     1] loss: 893.963
[58,     1] loss: 765.011
[59,     1] loss: 866.503
[60,     1] loss: 878.259
[61,     1] loss: 773.167
[62,     1] loss: 833.564
[63,     1] loss: 799.640
[64,     1] loss: 744.491
[65,     1] loss: 840.472
[66,     1] loss: 810.880
[67,     1] loss: 708.819
[68,     1] loss: 798.998
[69,     1] loss: 677.486
[70,     1] loss: 720.943
[71,     1] loss: 697.449
[72,     1] loss: 647.948
[73,     1] loss: 648.497
[74,     1] loss: 1031.773
[75,     1] loss: 1090.375
[76,     1] loss: 784.978
[77,     1] loss: 991.029
[78,     1] loss: 897.325
[79,     1] loss: 819.831
[80,     1] loss: 925.315
[81,     1] loss: 820.632
[82,     1] loss: 785.761
[83,     1] loss: 830.568
[84,     1] loss: 820.160
[85,     1] loss: 840.229
[86,     1] loss: 786.475
[87,     1] loss: 738.143
[88,     1] loss: 759.725
[89,     1] loss: 705.413
[90,     1] loss: 678.922
[91,     1] loss: 696.557
[92,     1] loss: 752.064
[93,     1] loss: 867.629
[94,     1] loss: 700.047
[95,     1] loss: 775.283
[96,     1] loss: 755.925
[97,     1] loss: 747.063
[98,     1] loss: 739.040
[99,     1] loss: 639.586
[100,     1] loss: 664.701
[101,     1] loss: 628.013
[102,     1] loss: 599.113
[103,     1] loss: 595.949
[104,     1] loss: 624.019
[105,     1] loss: 1030.366
[106,     1] loss: 1087.476
[107,     1] loss: 863.844
[108,     1] loss: 998.319
[109,     1] loss: 916.855
[110,     1] loss: 838.857
[111,     1] loss: 927.489
[112,     1] loss: 899.866
[113,     1] loss: 901.452
[114,     1] loss: 932.008
[115,     1] loss: 828.109
[116,     1] loss: 874.913
[117,     1] loss: 892.571
[118,     1] loss: 785.268
[119,     1] loss: 824.710
[120,     1] loss: 836.890
[121,     1] loss: 773.292
[122,     1] loss: 744.743
[123,     1] loss: 773.603
[124,     1] loss: 718.049
[125,     1] loss: 676.565
[126,     1] loss: 688.305
[127,     1] loss: 728.690
[128,     1] loss: 577.816
[129,     1] loss: 690.478
[130,     1] loss: 854.435
[131,     1] loss: 593.734
[132,     1] loss: 717.986
[133,     1] loss: 846.557
[134,     1] loss: 721.547
[135,     1] loss: 881.774
[136,     1] loss: 683.683
Early stopping applied (best metric=0.41231173276901245)
Finished Training
Total time taken: 19.593093872070312
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1248.227
[2,     1] loss: 1246.762
[3,     1] loss: 1249.631
[4,     1] loss: 1242.394
[5,     1] loss: 1244.847
[6,     1] loss: 1235.772
[7,     1] loss: 1225.786
[8,     1] loss: 1215.204
[9,     1] loss: 1177.751
[10,     1] loss: 1138.981
[11,     1] loss: 1098.031
[12,     1] loss: 1035.052
[13,     1] loss: 1000.520
[14,     1] loss: 1084.279
[15,     1] loss: 1193.135
[16,     1] loss: 947.622
[17,     1] loss: 1109.962
[18,     1] loss: 990.361
[19,     1] loss: 1049.191
[20,     1] loss: 1076.154
[21,     1] loss: 1005.317
[22,     1] loss: 992.398
[23,     1] loss: 1002.642
[24,     1] loss: 992.434
[25,     1] loss: 954.554
[26,     1] loss: 1005.038
[27,     1] loss: 985.906
[28,     1] loss: 907.832
[29,     1] loss: 945.374
[30,     1] loss: 938.151
[31,     1] loss: 943.295
[32,     1] loss: 898.493
[33,     1] loss: 896.196
[34,     1] loss: 884.451
[35,     1] loss: 842.228
[36,     1] loss: 918.865
[37,     1] loss: 872.187
[38,     1] loss: 892.155
[39,     1] loss: 860.837
[40,     1] loss: 840.668
[41,     1] loss: 816.961
[42,     1] loss: 790.289
[43,     1] loss: 764.650
[44,     1] loss: 818.560
[45,     1] loss: 1229.853
[46,     1] loss: 1083.398
[47,     1] loss: 893.766
[48,     1] loss: 929.049
[49,     1] loss: 919.989
[50,     1] loss: 969.349
[51,     1] loss: 915.263
[52,     1] loss: 878.190
[53,     1] loss: 942.483
[54,     1] loss: 881.971
[55,     1] loss: 865.538
[56,     1] loss: 841.155
[57,     1] loss: 830.427
[58,     1] loss: 840.059
[59,     1] loss: 899.580
[60,     1] loss: 768.943
[61,     1] loss: 804.428
[62,     1] loss: 815.301
[63,     1] loss: 822.251
[64,     1] loss: 742.781
[65,     1] loss: 788.990
[66,     1] loss: 705.795
[67,     1] loss: 733.463
[68,     1] loss: 949.503
[69,     1] loss: 974.739
[70,     1] loss: 774.606
[71,     1] loss: 872.910
[72,     1] loss: 814.373
[73,     1] loss: 904.954
[74,     1] loss: 727.100
[75,     1] loss: 835.653
[76,     1] loss: 725.525
[77,     1] loss: 800.694
[78,     1] loss: 681.841
[79,     1] loss: 767.501
[80,     1] loss: 711.168
[81,     1] loss: 670.848
[82,     1] loss: 714.484
[83,     1] loss: 691.296
[84,     1] loss: 742.923
[85,     1] loss: 885.636
[86,     1] loss: 923.201
[87,     1] loss: 796.787
[88,     1] loss: 846.625
[89,     1] loss: 1017.121
Early stopping applied (best metric=0.42061418294906616)
Finished Training
Total time taken: 12.824371814727783
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.812
[2,     1] loss: 1242.094
[3,     1] loss: 1254.565
[4,     1] loss: 1243.985
[5,     1] loss: 1241.019
[6,     1] loss: 1245.692
[7,     1] loss: 1238.457
[8,     1] loss: 1237.929
[9,     1] loss: 1245.079
[10,     1] loss: 1244.778
[11,     1] loss: 1234.046
[12,     1] loss: 1233.331
[13,     1] loss: 1228.403
[14,     1] loss: 1213.544
[15,     1] loss: 1182.425
[16,     1] loss: 1153.035
[17,     1] loss: 1114.303
[18,     1] loss: 1094.277
[19,     1] loss: 1049.100
[20,     1] loss: 1053.240
[21,     1] loss: 1039.806
[22,     1] loss: 1105.665
[23,     1] loss: 1010.516
[24,     1] loss: 1020.568
[25,     1] loss: 983.981
[26,     1] loss: 1048.378
[27,     1] loss: 991.988
[28,     1] loss: 970.355
[29,     1] loss: 1034.268
[30,     1] loss: 968.096
[31,     1] loss: 983.984
[32,     1] loss: 971.661
[33,     1] loss: 933.096
[34,     1] loss: 918.718
[35,     1] loss: 942.927
[36,     1] loss: 941.220
[37,     1] loss: 899.650
[38,     1] loss: 923.744
[39,     1] loss: 910.398
[40,     1] loss: 850.728
[41,     1] loss: 873.459
[42,     1] loss: 876.562
[43,     1] loss: 847.379
[44,     1] loss: 815.906
[45,     1] loss: 887.269
[46,     1] loss: 1375.344
[47,     1] loss: 870.800
[48,     1] loss: 1049.613
[49,     1] loss: 981.222
[50,     1] loss: 1012.580
[51,     1] loss: 1013.635
[52,     1] loss: 975.101
[53,     1] loss: 934.386
[54,     1] loss: 928.062
[55,     1] loss: 921.994
[56,     1] loss: 928.921
[57,     1] loss: 946.871
[58,     1] loss: 913.417
[59,     1] loss: 870.698
[60,     1] loss: 947.430
[61,     1] loss: 868.046
[62,     1] loss: 905.073
[63,     1] loss: 843.851
[64,     1] loss: 825.011
[65,     1] loss: 864.834
[66,     1] loss: 791.676
[67,     1] loss: 805.562
[68,     1] loss: 738.792
[69,     1] loss: 753.332
[70,     1] loss: 1044.188
[71,     1] loss: 1131.357
[72,     1] loss: 821.208
[73,     1] loss: 906.888
[74,     1] loss: 902.836
[75,     1] loss: 834.436
[76,     1] loss: 883.139
[77,     1] loss: 824.261
[78,     1] loss: 913.863
[79,     1] loss: 836.519
[80,     1] loss: 833.028
[81,     1] loss: 763.263
[82,     1] loss: 817.077
[83,     1] loss: 763.689
[84,     1] loss: 811.174
[85,     1] loss: 701.469
[86,     1] loss: 651.731
[87,     1] loss: 673.212
[88,     1] loss: 572.954
[89,     1] loss: 672.988
[90,     1] loss: 1464.230
[91,     1] loss: 2253.272
[92,     1] loss: 1573.965
[93,     1] loss: 1140.815
[94,     1] loss: 1229.243
[95,     1] loss: 1241.463
[96,     1] loss: 1242.701
[97,     1] loss: 1242.206
[98,     1] loss: 1242.988
[99,     1] loss: 1243.009
[100,     1] loss: 1245.189
[101,     1] loss: 1242.521
[102,     1] loss: 1241.626
[103,     1] loss: 1238.417
[104,     1] loss: 1227.791
[105,     1] loss: 1222.357
[106,     1] loss: 1213.345
[107,     1] loss: 1208.195
[108,     1] loss: 1196.432
[109,     1] loss: 1190.375
[110,     1] loss: 1164.066
[111,     1] loss: 1154.652
[112,     1] loss: 1138.966
[113,     1] loss: 1151.840
[114,     1] loss: 1150.068
[115,     1] loss: 1075.241
[116,     1] loss: 1223.216
[117,     1] loss: 1205.847
[118,     1] loss: 1250.357
[119,     1] loss: 1250.681
[120,     1] loss: 1246.665
[121,     1] loss: 1246.953
[122,     1] loss: 1247.703
[123,     1] loss: 1246.931
[124,     1] loss: 1238.758
[125,     1] loss: 1238.677
[126,     1] loss: 1240.697
[127,     1] loss: 1238.259
[128,     1] loss: 1238.293
[129,     1] loss: 1236.198
[130,     1] loss: 1229.824
[131,     1] loss: 1216.359
[132,     1] loss: 1190.510
[133,     1] loss: 1145.421
[134,     1] loss: 1099.896
[135,     1] loss: 1059.566
[136,     1] loss: 1071.809
[137,     1] loss: 1098.592
Early stopping applied (best metric=0.3677651584148407)
Finished Training
Total time taken: 19.551090478897095
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.106
[2,     1] loss: 1247.585
[3,     1] loss: 1242.210
[4,     1] loss: 1246.997
[5,     1] loss: 1237.740
[6,     1] loss: 1242.611
[7,     1] loss: 1237.369
[8,     1] loss: 1227.901
[9,     1] loss: 1205.682
[10,     1] loss: 1184.569
[11,     1] loss: 1147.878
[12,     1] loss: 1083.768
[13,     1] loss: 1046.427
[14,     1] loss: 1092.434
[15,     1] loss: 1088.788
[16,     1] loss: 1043.598
[17,     1] loss: 1011.061
[18,     1] loss: 1008.934
[19,     1] loss: 1027.407
[20,     1] loss: 979.815
[21,     1] loss: 983.570
[22,     1] loss: 923.388
[23,     1] loss: 909.369
[24,     1] loss: 972.042
[25,     1] loss: 938.795
[26,     1] loss: 938.050
[27,     1] loss: 903.529
[28,     1] loss: 896.795
[29,     1] loss: 957.299
[30,     1] loss: 930.204
[31,     1] loss: 874.089
[32,     1] loss: 936.694
[33,     1] loss: 872.407
[34,     1] loss: 863.713
[35,     1] loss: 924.696
[36,     1] loss: 869.882
[37,     1] loss: 816.416
[38,     1] loss: 892.478
[39,     1] loss: 841.756
[40,     1] loss: 799.240
[41,     1] loss: 772.470
[42,     1] loss: 795.070
[43,     1] loss: 758.247
[44,     1] loss: 794.829
[45,     1] loss: 1193.259
[46,     1] loss: 1216.129
[47,     1] loss: 933.841
[48,     1] loss: 974.180
[49,     1] loss: 1019.408
[50,     1] loss: 1077.670
[51,     1] loss: 994.645
[52,     1] loss: 982.593
[53,     1] loss: 910.827
[54,     1] loss: 964.476
[55,     1] loss: 932.452
[56,     1] loss: 875.606
[57,     1] loss: 903.031
[58,     1] loss: 861.802
[59,     1] loss: 855.148
[60,     1] loss: 869.572
[61,     1] loss: 825.719
[62,     1] loss: 817.167
[63,     1] loss: 788.806
[64,     1] loss: 804.448
[65,     1] loss: 775.518
[66,     1] loss: 768.934
[67,     1] loss: 793.387
[68,     1] loss: 765.840
[69,     1] loss: 713.009
[70,     1] loss: 690.631
[71,     1] loss: 769.489
[72,     1] loss: 1528.435
[73,     1] loss: 958.591
[74,     1] loss: 1025.098
[75,     1] loss: 1068.332
[76,     1] loss: 1079.581
[77,     1] loss: 1020.185
[78,     1] loss: 972.363
[79,     1] loss: 999.060
[80,     1] loss: 970.566
[81,     1] loss: 952.939
[82,     1] loss: 941.295
[83,     1] loss: 941.289
[84,     1] loss: 895.758
[85,     1] loss: 1028.533
[86,     1] loss: 905.078
[87,     1] loss: 906.409
[88,     1] loss: 878.773
[89,     1] loss: 944.745
[90,     1] loss: 877.914
[91,     1] loss: 912.841
[92,     1] loss: 900.400
[93,     1] loss: 883.677
[94,     1] loss: 883.462
[95,     1] loss: 904.809
[96,     1] loss: 807.711
[97,     1] loss: 845.524
[98,     1] loss: 859.015
[99,     1] loss: 805.051
[100,     1] loss: 759.689
[101,     1] loss: 790.039
[102,     1] loss: 777.559
[103,     1] loss: 811.116
[104,     1] loss: 1333.676
[105,     1] loss: 914.316
[106,     1] loss: 993.797
[107,     1] loss: 1015.799
[108,     1] loss: 968.793
[109,     1] loss: 893.871
[110,     1] loss: 890.685
[111,     1] loss: 908.421
[112,     1] loss: 871.398
[113,     1] loss: 876.497
[114,     1] loss: 917.077
[115,     1] loss: 799.735
[116,     1] loss: 858.010
[117,     1] loss: 824.547
[118,     1] loss: 850.740
[119,     1] loss: 821.995
[120,     1] loss: 869.796
[121,     1] loss: 796.795
[122,     1] loss: 787.452
[123,     1] loss: 760.029
[124,     1] loss: 732.237
[125,     1] loss: 800.199
[126,     1] loss: 928.148
[127,     1] loss: 1156.738
[128,     1] loss: 776.950
[129,     1] loss: 982.263
[130,     1] loss: 889.444
[131,     1] loss: 934.828
[132,     1] loss: 854.126
[133,     1] loss: 805.042
[134,     1] loss: 797.162
[135,     1] loss: 826.221
[136,     1] loss: 742.888
[137,     1] loss: 844.116
[138,     1] loss: 835.380
[139,     1] loss: 759.664
[140,     1] loss: 786.430
[141,     1] loss: 800.655
Early stopping applied (best metric=0.3374154567718506)
Finished Training
Total time taken: 20.20015788078308
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.035
[2,     1] loss: 1246.285
[3,     1] loss: 1249.945
[4,     1] loss: 1239.581
[5,     1] loss: 1240.496
[6,     1] loss: 1237.959
[7,     1] loss: 1233.945
[8,     1] loss: 1216.456
[9,     1] loss: 1197.728
[10,     1] loss: 1153.818
[11,     1] loss: 1109.621
[12,     1] loss: 1075.620
[13,     1] loss: 1024.205
[14,     1] loss: 1057.628
[15,     1] loss: 993.711
[16,     1] loss: 1033.362
[17,     1] loss: 1033.069
[18,     1] loss: 1017.022
[19,     1] loss: 1031.462
[20,     1] loss: 1015.907
[21,     1] loss: 957.406
[22,     1] loss: 931.882
[23,     1] loss: 918.713
[24,     1] loss: 878.517
[25,     1] loss: 892.694
[26,     1] loss: 894.383
[27,     1] loss: 904.069
[28,     1] loss: 873.873
[29,     1] loss: 891.097
[30,     1] loss: 906.210
[31,     1] loss: 857.142
[32,     1] loss: 824.214
[33,     1] loss: 845.704
[34,     1] loss: 831.149
[35,     1] loss: 865.199
[36,     1] loss: 809.310
[37,     1] loss: 822.601
[38,     1] loss: 842.307
[39,     1] loss: 823.272
[40,     1] loss: 780.582
[41,     1] loss: 815.822
[42,     1] loss: 897.441
[43,     1] loss: 911.588
[44,     1] loss: 793.810
[45,     1] loss: 810.865
[46,     1] loss: 761.335
[47,     1] loss: 811.938
[48,     1] loss: 745.701
[49,     1] loss: 705.755
[50,     1] loss: 742.436
[51,     1] loss: 1151.562
[52,     1] loss: 877.721
[53,     1] loss: 826.687
[54,     1] loss: 810.186
[55,     1] loss: 894.062
[56,     1] loss: 867.753
[57,     1] loss: 791.880
[58,     1] loss: 899.368
[59,     1] loss: 744.976
[60,     1] loss: 767.707
[61,     1] loss: 796.979
[62,     1] loss: 704.404
[63,     1] loss: 800.490
[64,     1] loss: 702.104
[65,     1] loss: 678.431
[66,     1] loss: 634.604
[67,     1] loss: 603.237
[68,     1] loss: 716.844
[69,     1] loss: 1353.560
[70,     1] loss: 1039.830
[71,     1] loss: 859.138
[72,     1] loss: 901.706
[73,     1] loss: 989.430
[74,     1] loss: 1031.158
[75,     1] loss: 941.194
[76,     1] loss: 882.588
[77,     1] loss: 910.477
[78,     1] loss: 910.666
[79,     1] loss: 894.726
[80,     1] loss: 934.723
[81,     1] loss: 903.286
[82,     1] loss: 848.055
[83,     1] loss: 876.413
[84,     1] loss: 874.130
[85,     1] loss: 849.081
[86,     1] loss: 863.762
[87,     1] loss: 791.356
[88,     1] loss: 771.490
[89,     1] loss: 766.703
[90,     1] loss: 790.216
[91,     1] loss: 715.801
[92,     1] loss: 697.298
[93,     1] loss: 688.044
[94,     1] loss: 679.962
[95,     1] loss: 744.252
[96,     1] loss: 1296.342
[97,     1] loss: 1277.794
[98,     1] loss: 1035.593
[99,     1] loss: 1139.490
[100,     1] loss: 1162.932
[101,     1] loss: 1131.626
[102,     1] loss: 1152.858
[103,     1] loss: 1117.470
[104,     1] loss: 1096.103
[105,     1] loss: 1057.189
[106,     1] loss: 991.319
[107,     1] loss: 974.540
[108,     1] loss: 964.550
[109,     1] loss: 932.271
[110,     1] loss: 922.909
[111,     1] loss: 913.983
[112,     1] loss: 945.805
[113,     1] loss: 881.233
[114,     1] loss: 912.896
Early stopping applied (best metric=0.38107606768608093)
Finished Training
Total time taken: 16.29674243927002
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.085
[2,     1] loss: 1255.221
[3,     1] loss: 1253.301
[4,     1] loss: 1247.332
[5,     1] loss: 1246.614
[6,     1] loss: 1243.023
[7,     1] loss: 1246.627
[8,     1] loss: 1245.990
[9,     1] loss: 1241.050
[10,     1] loss: 1244.816
[11,     1] loss: 1243.336
[12,     1] loss: 1245.704
[13,     1] loss: 1242.703
[14,     1] loss: 1241.415
[15,     1] loss: 1242.807
[16,     1] loss: 1242.599
[17,     1] loss: 1241.575
[18,     1] loss: 1240.656
[19,     1] loss: 1241.105
[20,     1] loss: 1233.523
[21,     1] loss: 1226.422
[22,     1] loss: 1211.551
[23,     1] loss: 1176.254
[24,     1] loss: 1157.762
[25,     1] loss: 1141.569
[26,     1] loss: 1108.086
[27,     1] loss: 1108.467
[28,     1] loss: 1127.285
[29,     1] loss: 1051.104
[30,     1] loss: 1141.005
[31,     1] loss: 1021.781
[32,     1] loss: 1080.967
[33,     1] loss: 1035.334
[34,     1] loss: 1024.035
[35,     1] loss: 1066.207
[36,     1] loss: 1000.862
[37,     1] loss: 1010.255
[38,     1] loss: 1007.981
[39,     1] loss: 979.176
[40,     1] loss: 984.263
[41,     1] loss: 946.933
[42,     1] loss: 1028.828
[43,     1] loss: 914.910
[44,     1] loss: 946.050
[45,     1] loss: 867.438
[46,     1] loss: 900.645
[47,     1] loss: 923.818
[48,     1] loss: 931.140
[49,     1] loss: 878.602
[50,     1] loss: 933.379
[51,     1] loss: 885.832
[52,     1] loss: 845.326
[53,     1] loss: 885.536
[54,     1] loss: 925.497
[55,     1] loss: 852.225
[56,     1] loss: 832.486
[57,     1] loss: 888.394
[58,     1] loss: 826.459
[59,     1] loss: 802.302
[60,     1] loss: 803.937
[61,     1] loss: 859.920
[62,     1] loss: 836.392
[63,     1] loss: 842.876
[64,     1] loss: 781.004
[65,     1] loss: 795.516
[66,     1] loss: 772.733
[67,     1] loss: 797.867
[68,     1] loss: 872.566
[69,     1] loss: 1040.279
[70,     1] loss: 815.486
[71,     1] loss: 925.702
[72,     1] loss: 837.980
[73,     1] loss: 809.431
[74,     1] loss: 893.536
[75,     1] loss: 889.110
[76,     1] loss: 760.647
[77,     1] loss: 787.614
[78,     1] loss: 730.534
[79,     1] loss: 746.680
[80,     1] loss: 679.465
[81,     1] loss: 717.275
[82,     1] loss: 972.198
[83,     1] loss: 1612.688
[84,     1] loss: 890.877
[85,     1] loss: 956.636
[86,     1] loss: 1138.815
[87,     1] loss: 1026.386
[88,     1] loss: 1069.078
[89,     1] loss: 1067.258
[90,     1] loss: 1016.806
[91,     1] loss: 1001.810
[92,     1] loss: 981.452
[93,     1] loss: 997.792
[94,     1] loss: 988.749
[95,     1] loss: 983.255
[96,     1] loss: 932.762
[97,     1] loss: 940.556
[98,     1] loss: 909.327
[99,     1] loss: 943.723
[100,     1] loss: 890.885
[101,     1] loss: 851.977
[102,     1] loss: 828.150
[103,     1] loss: 818.024
[104,     1] loss: 881.560
[105,     1] loss: 869.931
[106,     1] loss: 752.890
[107,     1] loss: 837.101
[108,     1] loss: 916.693
[109,     1] loss: 762.092
[110,     1] loss: 1120.985
[111,     1] loss: 1020.431
[112,     1] loss: 1092.029
[113,     1] loss: 1073.697
[114,     1] loss: 1095.648
[115,     1] loss: 1037.682
[116,     1] loss: 942.322
Early stopping applied (best metric=0.34847110509872437)
Finished Training
Total time taken: 16.62677788734436
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1253.462
[2,     1] loss: 1246.106
[3,     1] loss: 1245.867
[4,     1] loss: 1245.694
[5,     1] loss: 1245.097
[6,     1] loss: 1244.330
[7,     1] loss: 1247.028
[8,     1] loss: 1245.579
[9,     1] loss: 1244.426
[10,     1] loss: 1243.845
[11,     1] loss: 1242.149
[12,     1] loss: 1242.392
[13,     1] loss: 1236.951
[14,     1] loss: 1234.669
[15,     1] loss: 1227.945
[16,     1] loss: 1210.908
[17,     1] loss: 1181.187
[18,     1] loss: 1146.611
[19,     1] loss: 1122.915
[20,     1] loss: 1091.227
[21,     1] loss: 1071.919
[22,     1] loss: 1066.220
[23,     1] loss: 1077.833
[24,     1] loss: 1015.582
[25,     1] loss: 1066.645
[26,     1] loss: 1016.069
[27,     1] loss: 1033.859
[28,     1] loss: 989.921
[29,     1] loss: 1025.610
[30,     1] loss: 976.427
[31,     1] loss: 997.036
[32,     1] loss: 964.503
[33,     1] loss: 952.773
[34,     1] loss: 928.723
[35,     1] loss: 936.526
[36,     1] loss: 976.207
[37,     1] loss: 976.566
[38,     1] loss: 960.602
[39,     1] loss: 970.079
[40,     1] loss: 918.752
[41,     1] loss: 936.683
[42,     1] loss: 892.548
[43,     1] loss: 903.484
[44,     1] loss: 899.024
[45,     1] loss: 883.383
[46,     1] loss: 952.436
[47,     1] loss: 832.936
[48,     1] loss: 858.030
[49,     1] loss: 901.108
[50,     1] loss: 792.098
[51,     1] loss: 854.136
[52,     1] loss: 844.894
[53,     1] loss: 768.604
[54,     1] loss: 957.453
[55,     1] loss: 1230.113
[56,     1] loss: 834.767
[57,     1] loss: 950.248
[58,     1] loss: 925.729
[59,     1] loss: 895.378
[60,     1] loss: 899.039
[61,     1] loss: 889.483
[62,     1] loss: 828.171
[63,     1] loss: 896.484
[64,     1] loss: 848.445
[65,     1] loss: 852.558
[66,     1] loss: 803.238
[67,     1] loss: 833.017
[68,     1] loss: 823.836
[69,     1] loss: 791.673
[70,     1] loss: 849.980
[71,     1] loss: 786.974
[72,     1] loss: 773.104
[73,     1] loss: 859.903
[74,     1] loss: 842.944
[75,     1] loss: 740.517
[76,     1] loss: 829.142
[77,     1] loss: 729.872
[78,     1] loss: 735.650
[79,     1] loss: 853.347
[80,     1] loss: 838.841
[81,     1] loss: 745.604
[82,     1] loss: 750.949
[83,     1] loss: 827.725
[84,     1] loss: 657.232
[85,     1] loss: 669.903
[86,     1] loss: 759.041
[87,     1] loss: 772.714
[88,     1] loss: 685.022
[89,     1] loss: 641.319
[90,     1] loss: 731.971
[91,     1] loss: 909.795
[92,     1] loss: 1008.115
[93,     1] loss: 847.859
[94,     1] loss: 862.656
[95,     1] loss: 764.928
[96,     1] loss: 822.924
[97,     1] loss: 799.795
[98,     1] loss: 812.798
[99,     1] loss: 751.738
[100,     1] loss: 761.519
[101,     1] loss: 747.422
[102,     1] loss: 736.145
[103,     1] loss: 671.657
[104,     1] loss: 624.014
[105,     1] loss: 691.307
[106,     1] loss: 1370.040
[107,     1] loss: 1075.285
[108,     1] loss: 784.188
[109,     1] loss: 903.810
[110,     1] loss: 991.622
[111,     1] loss: 930.512
[112,     1] loss: 820.575
[113,     1] loss: 929.855
[114,     1] loss: 868.767
[115,     1] loss: 905.616
[116,     1] loss: 910.103
[117,     1] loss: 832.153
[118,     1] loss: 839.322
[119,     1] loss: 776.322
[120,     1] loss: 782.395
[121,     1] loss: 789.773
[122,     1] loss: 741.035
[123,     1] loss: 769.949
[124,     1] loss: 675.589
[125,     1] loss: 773.563
[126,     1] loss: 699.056
[127,     1] loss: 641.787
[128,     1] loss: 719.841
[129,     1] loss: 661.529
[130,     1] loss: 645.467
[131,     1] loss: 540.214
[132,     1] loss: 623.404
[133,     1] loss: 1396.633
[134,     1] loss: 768.175
Early stopping applied (best metric=0.334658682346344)
Finished Training
Total time taken: 19.21405339241028
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.889
[2,     1] loss: 1255.578
[3,     1] loss: 1244.538
[4,     1] loss: 1242.437
[5,     1] loss: 1243.743
[6,     1] loss: 1244.874
[7,     1] loss: 1244.258
[8,     1] loss: 1245.152
[9,     1] loss: 1245.629
[10,     1] loss: 1243.048
[11,     1] loss: 1243.132
[12,     1] loss: 1243.711
[13,     1] loss: 1241.894
[14,     1] loss: 1242.980
[15,     1] loss: 1243.097
[16,     1] loss: 1242.234
[17,     1] loss: 1242.964
[18,     1] loss: 1242.975
[19,     1] loss: 1241.283
[20,     1] loss: 1241.582
[21,     1] loss: 1237.812
[22,     1] loss: 1235.089
[23,     1] loss: 1228.304
[24,     1] loss: 1216.200
[25,     1] loss: 1195.046
[26,     1] loss: 1163.557
[27,     1] loss: 1157.757
[28,     1] loss: 1117.318
[29,     1] loss: 1066.935
[30,     1] loss: 1021.687
[31,     1] loss: 1044.450
[32,     1] loss: 999.881
[33,     1] loss: 981.945
[34,     1] loss: 989.526
[35,     1] loss: 988.919
[36,     1] loss: 1017.231
[37,     1] loss: 972.975
[38,     1] loss: 960.961
[39,     1] loss: 920.381
[40,     1] loss: 917.061
[41,     1] loss: 930.084
[42,     1] loss: 922.376
[43,     1] loss: 911.921
[44,     1] loss: 928.733
[45,     1] loss: 863.399
[46,     1] loss: 874.605
[47,     1] loss: 832.807
[48,     1] loss: 878.098
[49,     1] loss: 836.918
[50,     1] loss: 822.674
[51,     1] loss: 873.727
[52,     1] loss: 884.457
[53,     1] loss: 810.749
[54,     1] loss: 798.393
[55,     1] loss: 756.658
[56,     1] loss: 763.568
[57,     1] loss: 829.521
[58,     1] loss: 925.214
[59,     1] loss: 990.211
[60,     1] loss: 806.894
[61,     1] loss: 870.953
[62,     1] loss: 811.901
[63,     1] loss: 924.691
[64,     1] loss: 807.309
[65,     1] loss: 810.404
[66,     1] loss: 780.453
[67,     1] loss: 791.711
[68,     1] loss: 733.613
[69,     1] loss: 763.533
[70,     1] loss: 732.121
[71,     1] loss: 711.413
[72,     1] loss: 656.343
[73,     1] loss: 641.150
[74,     1] loss: 1021.900
[75,     1] loss: 1702.359
[76,     1] loss: 782.964
[77,     1] loss: 1000.034
[78,     1] loss: 1068.180
[79,     1] loss: 995.434
[80,     1] loss: 1025.757
[81,     1] loss: 1031.450
[82,     1] loss: 994.115
[83,     1] loss: 941.114
[84,     1] loss: 957.275
[85,     1] loss: 965.599
[86,     1] loss: 936.206
[87,     1] loss: 998.492
[88,     1] loss: 923.681
[89,     1] loss: 926.918
[90,     1] loss: 930.321
[91,     1] loss: 895.016
[92,     1] loss: 948.758
[93,     1] loss: 935.726
[94,     1] loss: 872.684
[95,     1] loss: 894.846
[96,     1] loss: 851.917
[97,     1] loss: 880.267
[98,     1] loss: 833.822
[99,     1] loss: 837.635
[100,     1] loss: 815.974
[101,     1] loss: 878.330
[102,     1] loss: 778.430
[103,     1] loss: 832.070
[104,     1] loss: 787.322
[105,     1] loss: 745.273
[106,     1] loss: 852.156
[107,     1] loss: 1047.755
[108,     1] loss: 772.639
[109,     1] loss: 942.847
[110,     1] loss: 803.926
[111,     1] loss: 844.719
[112,     1] loss: 807.411
[113,     1] loss: 835.860
[114,     1] loss: 767.771
[115,     1] loss: 730.065
[116,     1] loss: 744.254
[117,     1] loss: 693.284
[118,     1] loss: 730.757
[119,     1] loss: 689.269
[120,     1] loss: 753.441
[121,     1] loss: 1105.594
[122,     1] loss: 1742.445
Early stopping applied (best metric=0.36786630749702454)
Finished Training
Total time taken: 17.451864004135132
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.698
[2,     1] loss: 1244.337
[3,     1] loss: 1244.138
[4,     1] loss: 1244.358
[5,     1] loss: 1239.699
[6,     1] loss: 1236.242
[7,     1] loss: 1227.755
[8,     1] loss: 1205.195
[9,     1] loss: 1176.298
[10,     1] loss: 1120.992
[11,     1] loss: 1126.618
[12,     1] loss: 1147.012
[13,     1] loss: 1030.098
[14,     1] loss: 1104.567
[15,     1] loss: 1031.125
[16,     1] loss: 1067.081
[17,     1] loss: 983.715
[18,     1] loss: 988.742
[19,     1] loss: 1023.333
[20,     1] loss: 987.470
[21,     1] loss: 941.515
[22,     1] loss: 980.122
[23,     1] loss: 961.152
[24,     1] loss: 917.595
[25,     1] loss: 929.108
[26,     1] loss: 959.339
[27,     1] loss: 957.351
[28,     1] loss: 906.033
[29,     1] loss: 983.383
[30,     1] loss: 908.858
[31,     1] loss: 963.546
[32,     1] loss: 863.260
[33,     1] loss: 940.519
[34,     1] loss: 872.153
[35,     1] loss: 870.342
[36,     1] loss: 851.561
[37,     1] loss: 815.649
[38,     1] loss: 838.262
[39,     1] loss: 807.623
[40,     1] loss: 839.124
[41,     1] loss: 867.541
[42,     1] loss: 829.839
[43,     1] loss: 787.568
[44,     1] loss: 868.163
[45,     1] loss: 795.031
[46,     1] loss: 783.862
[47,     1] loss: 830.557
[48,     1] loss: 677.835
[49,     1] loss: 748.974
[50,     1] loss: 775.095
[51,     1] loss: 722.634
[52,     1] loss: 797.091
[53,     1] loss: 997.693
[54,     1] loss: 840.117
[55,     1] loss: 852.912
[56,     1] loss: 744.200
[57,     1] loss: 858.535
[58,     1] loss: 749.701
[59,     1] loss: 763.106
[60,     1] loss: 707.888
[61,     1] loss: 769.671
[62,     1] loss: 758.846
[63,     1] loss: 710.712
[64,     1] loss: 746.949
[65,     1] loss: 714.635
[66,     1] loss: 676.196
[67,     1] loss: 617.210
[68,     1] loss: 643.675
[69,     1] loss: 592.800
[70,     1] loss: 631.540
[71,     1] loss: 979.083
[72,     1] loss: 1667.950
[73,     1] loss: 940.358
[74,     1] loss: 1079.114
[75,     1] loss: 1103.568
[76,     1] loss: 1105.302
[77,     1] loss: 1091.024
[78,     1] loss: 1085.296
[79,     1] loss: 1059.409
[80,     1] loss: 1062.211
[81,     1] loss: 1066.425
[82,     1] loss: 1004.633
[83,     1] loss: 1051.285
[84,     1] loss: 1001.549
[85,     1] loss: 1028.777
[86,     1] loss: 984.221
[87,     1] loss: 994.536
[88,     1] loss: 970.856
[89,     1] loss: 959.347
[90,     1] loss: 955.341
[91,     1] loss: 981.761
[92,     1] loss: 913.539
[93,     1] loss: 984.070
[94,     1] loss: 966.160
[95,     1] loss: 929.403
[96,     1] loss: 877.457
[97,     1] loss: 798.201
[98,     1] loss: 902.185
[99,     1] loss: 823.845
[100,     1] loss: 815.800
[101,     1] loss: 797.952
[102,     1] loss: 773.982
[103,     1] loss: 858.010
[104,     1] loss: 1137.573
[105,     1] loss: 1005.459
[106,     1] loss: 899.175
[107,     1] loss: 1018.252
[108,     1] loss: 950.854
[109,     1] loss: 897.335
[110,     1] loss: 812.202
[111,     1] loss: 886.629
[112,     1] loss: 854.366
[113,     1] loss: 879.041
[114,     1] loss: 811.818
[115,     1] loss: 880.395
[116,     1] loss: 853.979
[117,     1] loss: 835.874
[118,     1] loss: 841.106
[119,     1] loss: 773.098
[120,     1] loss: 806.828
[121,     1] loss: 784.160
[122,     1] loss: 758.641
[123,     1] loss: 754.940
[124,     1] loss: 753.779
[125,     1] loss: 722.500
[126,     1] loss: 685.941
[127,     1] loss: 727.132
[128,     1] loss: 795.216
[129,     1] loss: 1719.426
[130,     1] loss: 905.692
[131,     1] loss: 1003.011
[132,     1] loss: 1007.332
[133,     1] loss: 1037.786
[134,     1] loss: 979.279
[135,     1] loss: 989.848
[136,     1] loss: 1054.152
[137,     1] loss: 988.491
[138,     1] loss: 1106.579
[139,     1] loss: 1141.940
[140,     1] loss: 1185.094
[141,     1] loss: 1104.964
[142,     1] loss: 1019.929
[143,     1] loss: 1054.891
[144,     1] loss: 1027.782
[145,     1] loss: 1007.482
[146,     1] loss: 1032.959
[147,     1] loss: 1004.480
[148,     1] loss: 961.006
[149,     1] loss: 964.779
[150,     1] loss: 985.176
[151,     1] loss: 902.917
[152,     1] loss: 898.728
[153,     1] loss: 869.011
[154,     1] loss: 855.995
[155,     1] loss: 856.001
[156,     1] loss: 839.050
[157,     1] loss: 830.780
[158,     1] loss: 817.438
[159,     1] loss: 838.189
[160,     1] loss: 799.579
[161,     1] loss: 763.189
[162,     1] loss: 748.563
[163,     1] loss: 777.280
[164,     1] loss: 911.060
[165,     1] loss: 1592.382
[166,     1] loss: 952.974
[167,     1] loss: 1085.661
[168,     1] loss: 1080.027
[169,     1] loss: 1086.489
[170,     1] loss: 1093.925
[171,     1] loss: 1081.560
[172,     1] loss: 1061.351
Early stopping applied (best metric=0.3904026746749878)
Finished Training
Total time taken: 24.701640605926514
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.880
[2,     1] loss: 1244.938
[3,     1] loss: 1266.082
[4,     1] loss: 1243.019
[5,     1] loss: 1246.352
[6,     1] loss: 1243.653
[7,     1] loss: 1242.307
[8,     1] loss: 1245.944
[9,     1] loss: 1242.145
[10,     1] loss: 1241.133
[11,     1] loss: 1241.795
[12,     1] loss: 1245.077
[13,     1] loss: 1239.852
[14,     1] loss: 1237.017
[15,     1] loss: 1232.006
[16,     1] loss: 1218.974
[17,     1] loss: 1197.718
[18,     1] loss: 1175.049
[19,     1] loss: 1126.261
[20,     1] loss: 1100.667
[21,     1] loss: 1056.272
[22,     1] loss: 1045.926
[23,     1] loss: 1159.676
[24,     1] loss: 1070.484
[25,     1] loss: 1043.896
[26,     1] loss: 1021.547
[27,     1] loss: 1054.466
[28,     1] loss: 993.514
[29,     1] loss: 1012.849
[30,     1] loss: 1015.153
[31,     1] loss: 966.986
[32,     1] loss: 945.674
[33,     1] loss: 957.600
[34,     1] loss: 967.853
[35,     1] loss: 960.407
[36,     1] loss: 952.087
[37,     1] loss: 868.090
[38,     1] loss: 907.441
[39,     1] loss: 868.031
[40,     1] loss: 852.231
[41,     1] loss: 909.112
[42,     1] loss: 958.663
[43,     1] loss: 855.530
[44,     1] loss: 901.474
[45,     1] loss: 887.992
[46,     1] loss: 814.446
[47,     1] loss: 870.514
[48,     1] loss: 829.925
[49,     1] loss: 813.214
[50,     1] loss: 829.454
[51,     1] loss: 830.893
[52,     1] loss: 782.436
[53,     1] loss: 700.244
[54,     1] loss: 916.414
[55,     1] loss: 1115.210
[56,     1] loss: 747.598
[57,     1] loss: 877.296
[58,     1] loss: 900.818
[59,     1] loss: 851.496
[60,     1] loss: 910.062
[61,     1] loss: 850.147
[62,     1] loss: 804.951
[63,     1] loss: 882.121
[64,     1] loss: 751.285
[65,     1] loss: 788.029
[66,     1] loss: 719.405
[67,     1] loss: 770.025
[68,     1] loss: 720.830
[69,     1] loss: 670.403
[70,     1] loss: 806.355
[71,     1] loss: 897.303
[72,     1] loss: 629.872
[73,     1] loss: 838.159
[74,     1] loss: 862.683
[75,     1] loss: 759.752
[76,     1] loss: 847.514
[77,     1] loss: 660.320
[78,     1] loss: 765.494
[79,     1] loss: 640.659
[80,     1] loss: 813.696
[81,     1] loss: 717.206
[82,     1] loss: 664.055
[83,     1] loss: 691.855
[84,     1] loss: 622.418
[85,     1] loss: 618.553
[86,     1] loss: 742.049
[87,     1] loss: 867.312
[88,     1] loss: 682.910
[89,     1] loss: 792.372
[90,     1] loss: 643.484
[91,     1] loss: 657.355
[92,     1] loss: 754.889
[93,     1] loss: 685.760
[94,     1] loss: 651.070
[95,     1] loss: 721.975
[96,     1] loss: 728.144
[97,     1] loss: 553.975
[98,     1] loss: 721.316
[99,     1] loss: 778.182
[100,     1] loss: 552.262
[101,     1] loss: 770.116
[102,     1] loss: 662.164
[103,     1] loss: 649.593
[104,     1] loss: 835.796
[105,     1] loss: 704.262
[106,     1] loss: 634.965
[107,     1] loss: 659.418
[108,     1] loss: 611.473
[109,     1] loss: 649.350
[110,     1] loss: 510.248
[111,     1] loss: 637.157
[112,     1] loss: 852.622
[113,     1] loss: 577.783
[114,     1] loss: 599.865
[115,     1] loss: 545.096
[116,     1] loss: 521.640
[117,     1] loss: 694.850
[118,     1] loss: 841.098
[119,     1] loss: 589.972
[120,     1] loss: 593.905
[121,     1] loss: 619.135
[122,     1] loss: 560.972
[123,     1] loss: 698.650
[124,     1] loss: 878.940
[125,     1] loss: 553.745
[126,     1] loss: 855.940
[127,     1] loss: 693.963
[128,     1] loss: 873.495
[129,     1] loss: 581.257
[130,     1] loss: 796.296
[131,     1] loss: 599.664
[132,     1] loss: 717.916
[133,     1] loss: 572.480
[134,     1] loss: 596.438
[135,     1] loss: 536.979
[136,     1] loss: 542.959
[137,     1] loss: 481.697
[138,     1] loss: 417.300
[139,     1] loss: 389.831
Early stopping applied (best metric=0.32130947709083557)
Finished Training
Total time taken: 19.95513415336609
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.897
[2,     1] loss: 1246.327
[3,     1] loss: 1244.679
[4,     1] loss: 1249.473
[5,     1] loss: 1240.458
[6,     1] loss: 1238.030
[7,     1] loss: 1234.078
[8,     1] loss: 1221.280
[9,     1] loss: 1195.322
[10,     1] loss: 1154.445
[11,     1] loss: 1130.951
[12,     1] loss: 1078.828
[13,     1] loss: 1047.819
[14,     1] loss: 1081.772
[15,     1] loss: 1048.467
[16,     1] loss: 1018.143
[17,     1] loss: 1016.541
[18,     1] loss: 1038.507
[19,     1] loss: 1066.854
[20,     1] loss: 1012.837
[21,     1] loss: 1036.229
[22,     1] loss: 958.688
[23,     1] loss: 961.230
[24,     1] loss: 980.761
[25,     1] loss: 983.965
[26,     1] loss: 975.353
[27,     1] loss: 965.177
[28,     1] loss: 939.089
[29,     1] loss: 908.414
[30,     1] loss: 902.115
[31,     1] loss: 898.135
[32,     1] loss: 909.069
[33,     1] loss: 863.386
[34,     1] loss: 1017.048
[35,     1] loss: 1164.047
[36,     1] loss: 885.719
[37,     1] loss: 985.057
[38,     1] loss: 928.598
[39,     1] loss: 925.286
[40,     1] loss: 934.320
[41,     1] loss: 915.221
[42,     1] loss: 925.274
[43,     1] loss: 903.859
[44,     1] loss: 814.041
[45,     1] loss: 899.824
[46,     1] loss: 837.611
[47,     1] loss: 935.465
[48,     1] loss: 835.840
[49,     1] loss: 902.694
[50,     1] loss: 854.705
[51,     1] loss: 831.054
[52,     1] loss: 863.969
[53,     1] loss: 781.583
[54,     1] loss: 805.317
[55,     1] loss: 785.435
[56,     1] loss: 740.042
[57,     1] loss: 698.245
[58,     1] loss: 709.691
[59,     1] loss: 883.347
[60,     1] loss: 1639.660
[61,     1] loss: 1041.050
[62,     1] loss: 1023.931
[63,     1] loss: 1006.712
[64,     1] loss: 1055.950
[65,     1] loss: 1058.267
[66,     1] loss: 1069.288
[67,     1] loss: 1069.680
[68,     1] loss: 1035.924
[69,     1] loss: 992.810
[70,     1] loss: 995.049
[71,     1] loss: 996.318
[72,     1] loss: 973.724
[73,     1] loss: 929.739
[74,     1] loss: 967.168
[75,     1] loss: 967.665
[76,     1] loss: 900.192
[77,     1] loss: 975.447
[78,     1] loss: 914.622
[79,     1] loss: 938.952
[80,     1] loss: 933.676
[81,     1] loss: 905.695
[82,     1] loss: 890.176
[83,     1] loss: 878.738
[84,     1] loss: 886.579
[85,     1] loss: 852.007
[86,     1] loss: 872.490
[87,     1] loss: 843.879
[88,     1] loss: 802.493
[89,     1] loss: 852.256
[90,     1] loss: 910.086
[91,     1] loss: 922.143
[92,     1] loss: 847.249
[93,     1] loss: 905.060
[94,     1] loss: 808.694
[95,     1] loss: 801.126
[96,     1] loss: 802.855
[97,     1] loss: 721.042
[98,     1] loss: 728.628
[99,     1] loss: 738.904
[100,     1] loss: 955.059
[101,     1] loss: 1448.339
[102,     1] loss: 901.173
[103,     1] loss: 1005.800
[104,     1] loss: 1056.323
[105,     1] loss: 996.752
Early stopping applied (best metric=0.32866305112838745)
Finished Training
Total time taken: 15.038607358932495
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1245.933
[2,     1] loss: 1245.158
[3,     1] loss: 1249.120
[4,     1] loss: 1241.770
[5,     1] loss: 1251.069
[6,     1] loss: 1244.964
[7,     1] loss: 1240.021
[8,     1] loss: 1234.148
[9,     1] loss: 1218.150
[10,     1] loss: 1203.958
[11,     1] loss: 1165.460
[12,     1] loss: 1127.054
[13,     1] loss: 1083.561
[14,     1] loss: 1068.060
[15,     1] loss: 1063.754
[16,     1] loss: 1069.124
[17,     1] loss: 981.353
[18,     1] loss: 999.241
[19,     1] loss: 995.209
[20,     1] loss: 970.547
[21,     1] loss: 942.330
[22,     1] loss: 943.326
[23,     1] loss: 946.859
[24,     1] loss: 906.833
[25,     1] loss: 879.121
[26,     1] loss: 903.269
[27,     1] loss: 919.772
[28,     1] loss: 999.178
[29,     1] loss: 982.604
[30,     1] loss: 900.635
[31,     1] loss: 936.022
[32,     1] loss: 877.278
[33,     1] loss: 921.495
[34,     1] loss: 843.826
[35,     1] loss: 863.513
[36,     1] loss: 829.033
[37,     1] loss: 807.802
[38,     1] loss: 810.049
[39,     1] loss: 807.526
[40,     1] loss: 777.440
[41,     1] loss: 843.375
[42,     1] loss: 920.762
[43,     1] loss: 1177.265
[44,     1] loss: 818.703
[45,     1] loss: 959.823
[46,     1] loss: 916.701
[47,     1] loss: 864.518
[48,     1] loss: 860.314
[49,     1] loss: 897.821
[50,     1] loss: 855.463
[51,     1] loss: 847.986
[52,     1] loss: 877.337
[53,     1] loss: 808.855
[54,     1] loss: 803.823
[55,     1] loss: 785.087
[56,     1] loss: 803.936
[57,     1] loss: 764.753
[58,     1] loss: 741.788
[59,     1] loss: 746.251
[60,     1] loss: 755.341
[61,     1] loss: 709.823
[62,     1] loss: 621.917
[63,     1] loss: 636.707
[64,     1] loss: 603.309
[65,     1] loss: 618.489
[66,     1] loss: 587.929
[67,     1] loss: 712.487
[68,     1] loss: 3008.481
[69,     1] loss: 1615.524
[70,     1] loss: 1066.165
[71,     1] loss: 1192.146
[72,     1] loss: 1156.927
[73,     1] loss: 1174.187
[74,     1] loss: 1161.470
[75,     1] loss: 1104.812
[76,     1] loss: 1133.833
[77,     1] loss: 1157.122
[78,     1] loss: 1108.275
[79,     1] loss: 1114.644
[80,     1] loss: 1093.275
[81,     1] loss: 1109.390
[82,     1] loss: 1097.855
[83,     1] loss: 1079.672
[84,     1] loss: 1103.950
[85,     1] loss: 1091.050
[86,     1] loss: 1045.396
[87,     1] loss: 1106.410
[88,     1] loss: 1023.534
Early stopping applied (best metric=0.4004559814929962)
Finished Training
Total time taken: 12.730360507965088
{'Hydroxylation-K Validation Accuracy': 0.7552009456264775, 'Hydroxylation-K Validation Sensitivity': 0.5933333333333334, 'Hydroxylation-K Validation Specificity': 0.7964912280701755, 'Hydroxylation-K Validation Precision': 0.431483728975989, 'Hydroxylation-K AUC ROC': 0.7788304093567251, 'Hydroxylation-K AUC PR': 0.5434623018148048, 'Hydroxylation-K MCC': 0.3514266179260675, 'Hydroxylation-K F1': 0.4930780941442979, 'Validation Loss (Hydroxylation-K)': 0.4648288349310557, 'Hydroxylation-P Validation Accuracy': 0.7912723042823545, 'Hydroxylation-P Validation Sensitivity': 0.7935978835978836, 'Hydroxylation-P Validation Specificity': 0.7907551498827872, 'Hydroxylation-P Validation Precision': 0.45540702030943303, 'Hydroxylation-P AUC ROC': 0.847289145561628, 'Hydroxylation-P AUC PR': 0.5855644264961382, 'Hydroxylation-P MCC': 0.4846070508546735, 'Hydroxylation-P F1': 0.5760897397073402, 'Validation Loss (Hydroxylation-P)': 0.3678828875223796, 'Validation Loss (total)': 0.8327117323875427, 'TimeToTrain': 16.927475849787395}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008979338313047858,
 'learning_rate_Hydroxylation-K': 0.008126358208841,
 'learning_rate_Hydroxylation-P': 0.00783546131535243,
 'log_base': 1.3006998147954207,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1826775179,
 'sample_weights': [1.5937779862636674, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.6620178284328647,
 'weight_decay_Hydroxylation-K': 4.24128152188044,
 'weight_decay_Hydroxylation-P': 7.830478915000336}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2265.258
[2,     1] loss: 2271.190
[3,     1] loss: 2246.669
[4,     1] loss: 2235.363
[5,     1] loss: 2255.759
[6,     1] loss: 2260.276
[7,     1] loss: 2246.588
[8,     1] loss: 2245.397
[9,     1] loss: 2249.827
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008198161334791981,
 'learning_rate_Hydroxylation-K': 0.0008258669743257699,
 'learning_rate_Hydroxylation-P': 0.009484208460825776,
 'log_base': 2.019876215146007,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1887853743,
 'sample_weights': [6.350048161150898, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.8421834334492235,
 'weight_decay_Hydroxylation-K': 7.994506071123432,
 'weight_decay_Hydroxylation-P': 1.8703819525132679}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1409.875
[2,     1] loss: 1413.491
[3,     1] loss: 1413.426
[4,     1] loss: 1408.413
[5,     1] loss: 1407.163
[6,     1] loss: 1407.251
[7,     1] loss: 1404.073
[8,     1] loss: 1402.526
[9,     1] loss: 1397.110
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003475085455518375,
 'learning_rate_Hydroxylation-K': 0.005197687750257656,
 'learning_rate_Hydroxylation-P': 0.006675593450751188,
 'log_base': 1.0315254883083447,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2794587612,
 'sample_weights': [2.37461894054856, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.51362167894043,
 'weight_decay_Hydroxylation-K': 9.24544759625077,
 'weight_decay_Hydroxylation-P': 2.597530934142638}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17521.348
Exploding loss, terminate run (best metric=0.5316292643547058)
Finished Training
Total time taken: 0.20902180671691895
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17450.742
Exploding loss, terminate run (best metric=0.5267855525016785)
Finished Training
Total time taken: 0.2120203971862793
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17439.168
Exploding loss, terminate run (best metric=0.5288517475128174)
Finished Training
Total time taken: 0.21402215957641602
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17512.576
Exploding loss, terminate run (best metric=0.5316058397293091)
Finished Training
Total time taken: 0.21202373504638672
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17522.801
Exploding loss, terminate run (best metric=0.5283536314964294)
Finished Training
Total time taken: 0.2230231761932373
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17569.174
Exploding loss, terminate run (best metric=0.5322647094726562)
Finished Training
Total time taken: 0.21402454376220703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17418.902
Exploding loss, terminate run (best metric=0.5276933908462524)
Finished Training
Total time taken: 0.21802139282226562
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17502.059
Exploding loss, terminate run (best metric=0.5293741822242737)
Finished Training
Total time taken: 0.2170238494873047
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17459.012
Exploding loss, terminate run (best metric=0.5329374670982361)
Finished Training
Total time taken: 0.2230222225189209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17445.379
Exploding loss, terminate run (best metric=0.5295692682266235)
Finished Training
Total time taken: 0.20702123641967773
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17468.146
Exploding loss, terminate run (best metric=0.5350229740142822)
Finished Training
Total time taken: 0.2320246696472168
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17438.893
Exploding loss, terminate run (best metric=0.529682457447052)
Finished Training
Total time taken: 0.2100207805633545
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17435.145
Exploding loss, terminate run (best metric=0.5352710485458374)
Finished Training
Total time taken: 0.22102618217468262
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17424.771
Exploding loss, terminate run (best metric=0.5347912311553955)
Finished Training
Total time taken: 0.21502351760864258
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17496.232
Exploding loss, terminate run (best metric=0.5338422060012817)
Finished Training
Total time taken: 0.22402215003967285
{'Hydroxylation-K Validation Accuracy': 0.4925236406619385, 'Hydroxylation-K Validation Sensitivity': 0.5133333333333333, 'Hydroxylation-K Validation Specificity': 0.4842105263157895, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6760038986354776, 'Hydroxylation-K AUC PR': 0.36566472978570724, 'Hydroxylation-K MCC': 0.0010394849350702131, 'Hydroxylation-K F1': 0.17849283378360944, 'Validation Loss (Hydroxylation-K)': 0.5563652793566386, 'Hydroxylation-P Validation Accuracy': 0.4948062873289004, 'Hydroxylation-P Validation Sensitivity': 0.5142857142857142, 'Hydroxylation-P Validation Specificity': 0.4902439024390244, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6053222343354021, 'Hydroxylation-P AUC PR': 0.27961668065645656, 'Hydroxylation-P MCC': 0.00677762449557289, 'Hydroxylation-P F1': 0.16051892880403432, 'Validation Loss (Hydroxylation-P)': 0.531178331375122, 'Validation Loss (total)': 1.0875436385472617, 'TimeToTrain': 0.21675612131754557}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0032463721636918756,
 'learning_rate_Hydroxylation-K': 0.00015725021282155433,
 'learning_rate_Hydroxylation-P': 0.008204149046066898,
 'log_base': 1.302062946062271,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3019591574,
 'sample_weights': [53.82564790293067, 6.714225964565019],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.04588568323877107,
 'weight_decay_Hydroxylation-K': 2.044100499809849,
 'weight_decay_Hydroxylation-P': 4.032335657896403}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2247.056
[2,     1] loss: 2234.594
[3,     1] loss: 2266.841
[4,     1] loss: 2240.154
[5,     1] loss: 2236.738
[6,     1] loss: 2244.146
[7,     1] loss: 2239.447
[8,     1] loss: 2243.173
[9,     1] loss: 2234.241
[10,     1] loss: 2235.879
[11,     1] loss: 2242.597
[12,     1] loss: 2227.568
[13,     1] loss: 2234.140
[14,     1] loss: 2221.010
[15,     1] loss: 2214.338
[16,     1] loss: 2224.832
[17,     1] loss: 2201.028
[18,     1] loss: 2181.064
[19,     1] loss: 2180.202
[20,     1] loss: 2137.244
[21,     1] loss: 2092.049
[22,     1] loss: 2060.358
[23,     1] loss: 2035.833
[24,     1] loss: 2026.470
[25,     1] loss: 1942.408
[26,     1] loss: 1905.400
[27,     1] loss: 1936.700
[28,     1] loss: 1878.419
[29,     1] loss: 1799.948
[30,     1] loss: 1765.393
[31,     1] loss: 1838.375
[32,     1] loss: 1822.790
[33,     1] loss: 1898.342
[34,     1] loss: 1784.561
[35,     1] loss: 1847.094
[36,     1] loss: 1878.749
[37,     1] loss: 1742.728
[38,     1] loss: 1876.026
[39,     1] loss: 1765.361
[40,     1] loss: 1752.614
[41,     1] loss: 1668.815
[42,     1] loss: 1630.216
[43,     1] loss: 1794.161
[44,     1] loss: 1623.694
[45,     1] loss: 1647.907
[46,     1] loss: 1649.998
[47,     1] loss: 1591.593
[48,     1] loss: 1549.182
[49,     1] loss: 1636.271
[50,     1] loss: 1462.779
[51,     1] loss: 1662.549
[52,     1] loss: 1501.542
[53,     1] loss: 1481.610
[54,     1] loss: 1514.635
[55,     1] loss: 1587.342
[56,     1] loss: 1569.960
[57,     1] loss: 1548.080
[58,     1] loss: 1567.140
[59,     1] loss: 1554.344
[60,     1] loss: 1572.355
[61,     1] loss: 1425.579
[62,     1] loss: 1479.034
[63,     1] loss: 1431.019
[64,     1] loss: 1570.800
[65,     1] loss: 1475.117
[66,     1] loss: 1372.537
[67,     1] loss: 1386.396
[68,     1] loss: 1433.186
[69,     1] loss: 1376.633
[70,     1] loss: 1416.051
Early stopping applied (best metric=0.5077730417251587)
Finished Training
Total time taken: 10.028071641921997
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2244.228
[2,     1] loss: 2252.168
[3,     1] loss: 2255.225
[4,     1] loss: 2238.772
[5,     1] loss: 2234.943
[6,     1] loss: 2237.682
[7,     1] loss: 2242.635
[8,     1] loss: 2238.329
[9,     1] loss: 2248.936
[10,     1] loss: 2236.384
[11,     1] loss: 2233.924
[12,     1] loss: 2214.786
[13,     1] loss: 2218.921
[14,     1] loss: 2196.244
[15,     1] loss: 2186.350
[16,     1] loss: 2156.784
[17,     1] loss: 2090.792
[18,     1] loss: 2096.259
[19,     1] loss: 2035.210
[20,     1] loss: 2047.503
[21,     1] loss: 1986.720
[22,     1] loss: 1974.523
[23,     1] loss: 1963.948
[24,     1] loss: 1916.272
[25,     1] loss: 1792.485
[26,     1] loss: 1977.551
[27,     1] loss: 1911.887
[28,     1] loss: 1763.706
[29,     1] loss: 1828.649
[30,     1] loss: 1892.378
[31,     1] loss: 1785.813
[32,     1] loss: 1887.357
[33,     1] loss: 1750.938
[34,     1] loss: 1778.615
[35,     1] loss: 1904.496
[36,     1] loss: 1844.301
[37,     1] loss: 1659.234
[38,     1] loss: 1707.138
[39,     1] loss: 1691.140
[40,     1] loss: 1577.423
[41,     1] loss: 1768.551
[42,     1] loss: 1635.749
[43,     1] loss: 1544.146
[44,     1] loss: 1575.709
[45,     1] loss: 1605.062
[46,     1] loss: 1776.840
[47,     1] loss: 1618.169
[48,     1] loss: 1527.401
[49,     1] loss: 1602.557
[50,     1] loss: 1565.387
[51,     1] loss: 1548.689
[52,     1] loss: 1644.966
[53,     1] loss: 1459.261
[54,     1] loss: 1533.643
[55,     1] loss: 1501.074
[56,     1] loss: 1488.381
[57,     1] loss: 1511.379
[58,     1] loss: 1412.715
[59,     1] loss: 1573.270
[60,     1] loss: 1548.816
[61,     1] loss: 1406.605
[62,     1] loss: 1460.912
[63,     1] loss: 1468.451
[64,     1] loss: 1569.397
[65,     1] loss: 1357.310
[66,     1] loss: 1512.319
[67,     1] loss: 1368.721
[68,     1] loss: 1490.330
[69,     1] loss: 1500.994
[70,     1] loss: 1400.379
[71,     1] loss: 1423.367
[72,     1] loss: 1454.842
[73,     1] loss: 1310.072
[74,     1] loss: 1233.255
[75,     1] loss: 1467.497
[76,     1] loss: 1373.573
[77,     1] loss: 1371.307
[78,     1] loss: 1224.557
[79,     1] loss: 1451.399
[80,     1] loss: 1345.367
[81,     1] loss: 1245.620
[82,     1] loss: 1322.837
[83,     1] loss: 1224.361
[84,     1] loss: 1355.772
[85,     1] loss: 1388.155
Early stopping applied (best metric=0.4243811070919037)
Finished Training
Total time taken: 12.180302381515503
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2250.408
[2,     1] loss: 2250.374
[3,     1] loss: 2233.799
[4,     1] loss: 2241.103
[5,     1] loss: 2241.990
[6,     1] loss: 2242.380
[7,     1] loss: 2229.744
[8,     1] loss: 2236.518
[9,     1] loss: 2251.301
[10,     1] loss: 2236.195
[11,     1] loss: 2228.842
[12,     1] loss: 2225.850
[13,     1] loss: 2223.412
[14,     1] loss: 2220.593
[15,     1] loss: 2212.257
[16,     1] loss: 2188.295
[17,     1] loss: 2181.419
[18,     1] loss: 2178.682
[19,     1] loss: 2107.728
[20,     1] loss: 2110.786
[21,     1] loss: 2043.743
[22,     1] loss: 1997.746
[23,     1] loss: 1945.986
[24,     1] loss: 1995.542
[25,     1] loss: 1956.031
[26,     1] loss: 1863.525
[27,     1] loss: 1958.517
[28,     1] loss: 1928.820
[29,     1] loss: 1927.329
[30,     1] loss: 2001.610
[31,     1] loss: 1780.416
[32,     1] loss: 1898.374
[33,     1] loss: 1998.225
[34,     1] loss: 1809.541
[35,     1] loss: 1839.538
[36,     1] loss: 1845.874
[37,     1] loss: 1796.189
[38,     1] loss: 1815.268
[39,     1] loss: 1735.351
[40,     1] loss: 1775.476
[41,     1] loss: 1770.313
[42,     1] loss: 1689.481
[43,     1] loss: 1686.629
[44,     1] loss: 1731.866
[45,     1] loss: 1668.994
[46,     1] loss: 1613.522
[47,     1] loss: 1600.069
[48,     1] loss: 1768.546
[49,     1] loss: 1688.808
[50,     1] loss: 1618.397
[51,     1] loss: 1644.923
[52,     1] loss: 1742.929
[53,     1] loss: 1600.067
[54,     1] loss: 1667.391
[55,     1] loss: 1609.317
[56,     1] loss: 1563.639
[57,     1] loss: 1609.862
[58,     1] loss: 1603.869
[59,     1] loss: 1678.189
[60,     1] loss: 1421.629
[61,     1] loss: 1428.035
[62,     1] loss: 1596.170
[63,     1] loss: 1531.950
[64,     1] loss: 1423.730
[65,     1] loss: 1518.225
[66,     1] loss: 1377.068
[67,     1] loss: 1400.855
[68,     1] loss: 1343.978
[69,     1] loss: 1467.841
[70,     1] loss: 1398.072
[71,     1] loss: 1418.320
[72,     1] loss: 1436.975
[73,     1] loss: 1348.667
[74,     1] loss: 1398.955
[75,     1] loss: 1418.043
[76,     1] loss: 1388.777
[77,     1] loss: 1465.631
[78,     1] loss: 1232.795
[79,     1] loss: 1357.427
[80,     1] loss: 1331.084
[81,     1] loss: 1381.729
[82,     1] loss: 1237.292
[83,     1] loss: 1298.235
[84,     1] loss: 1384.486
[85,     1] loss: 1342.950
[86,     1] loss: 1336.021
[87,     1] loss: 1218.971
Early stopping applied (best metric=0.372440367937088)
Finished Training
Total time taken: 12.354320049285889
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2241.570
[2,     1] loss: 2240.678
[3,     1] loss: 2234.511
[4,     1] loss: 2247.038
[5,     1] loss: 2242.099
[6,     1] loss: 2227.649
[7,     1] loss: 2226.162
[8,     1] loss: 2217.148
[9,     1] loss: 2205.232
[10,     1] loss: 2179.875
[11,     1] loss: 2142.884
[12,     1] loss: 2057.633
[13,     1] loss: 2015.208
[14,     1] loss: 1908.565
[15,     1] loss: 1954.046
[16,     1] loss: 1911.494
[17,     1] loss: 1863.488
[18,     1] loss: 1876.066
[19,     1] loss: 1791.433
[20,     1] loss: 1878.516
[21,     1] loss: 1669.131
[22,     1] loss: 1765.180
[23,     1] loss: 1625.714
[24,     1] loss: 1785.699
[25,     1] loss: 1764.661
[26,     1] loss: 1733.719
[27,     1] loss: 1737.156
[28,     1] loss: 1729.160
[29,     1] loss: 1743.354
[30,     1] loss: 1638.793
[31,     1] loss: 1635.404
[32,     1] loss: 1602.568
[33,     1] loss: 1596.008
[34,     1] loss: 1674.619
[35,     1] loss: 1659.622
[36,     1] loss: 1598.527
[37,     1] loss: 1601.348
[38,     1] loss: 1503.313
[39,     1] loss: 1625.708
[40,     1] loss: 1627.766
[41,     1] loss: 1548.034
[42,     1] loss: 1378.165
[43,     1] loss: 1588.945
[44,     1] loss: 1547.643
[45,     1] loss: 1584.407
[46,     1] loss: 1545.732
[47,     1] loss: 1502.372
[48,     1] loss: 1473.931
[49,     1] loss: 1368.835
[50,     1] loss: 1461.650
[51,     1] loss: 1531.490
[52,     1] loss: 1336.422
[53,     1] loss: 1439.477
[54,     1] loss: 1426.604
[55,     1] loss: 1416.804
[56,     1] loss: 1351.062
[57,     1] loss: 1433.502
[58,     1] loss: 1347.500
[59,     1] loss: 1364.919
[60,     1] loss: 1405.092
[61,     1] loss: 1395.377
Early stopping applied (best metric=0.48093679547309875)
Finished Training
Total time taken: 8.675928115844727
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2234.387
[2,     1] loss: 2271.829
[3,     1] loss: 2242.109
[4,     1] loss: 2238.848
[5,     1] loss: 2244.616
[6,     1] loss: 2248.075
[7,     1] loss: 2239.038
[8,     1] loss: 2240.042
[9,     1] loss: 2249.329
[10,     1] loss: 2246.007
[11,     1] loss: 2243.228
[12,     1] loss: 2240.793
[13,     1] loss: 2239.189
[14,     1] loss: 2241.488
[15,     1] loss: 2246.555
[16,     1] loss: 2238.704
[17,     1] loss: 2234.094
[18,     1] loss: 2237.985
[19,     1] loss: 2228.702
[20,     1] loss: 2235.242
[21,     1] loss: 2236.420
[22,     1] loss: 2236.062
[23,     1] loss: 2224.741
[24,     1] loss: 2236.995
[25,     1] loss: 2213.519
[26,     1] loss: 2216.232
[27,     1] loss: 2207.471
[28,     1] loss: 2205.256
[29,     1] loss: 2175.396
[30,     1] loss: 2175.166
[31,     1] loss: 2164.343
[32,     1] loss: 2108.580
[33,     1] loss: 2104.850
[34,     1] loss: 2061.051
[35,     1] loss: 2012.815
[36,     1] loss: 2030.965
[37,     1] loss: 1915.357
[38,     1] loss: 1947.071
[39,     1] loss: 1923.955
[40,     1] loss: 1907.422
[41,     1] loss: 1947.988
[42,     1] loss: 1927.814
[43,     1] loss: 1912.943
[44,     1] loss: 1811.154
[45,     1] loss: 1863.930
[46,     1] loss: 1841.697
[47,     1] loss: 1832.302
[48,     1] loss: 1813.683
[49,     1] loss: 1824.161
[50,     1] loss: 1712.112
[51,     1] loss: 1848.999
[52,     1] loss: 1693.708
[53,     1] loss: 1792.213
[54,     1] loss: 1780.270
[55,     1] loss: 1646.912
[56,     1] loss: 1810.074
[57,     1] loss: 1741.390
[58,     1] loss: 1756.213
[59,     1] loss: 1721.720
[60,     1] loss: 1612.417
[61,     1] loss: 1633.465
[62,     1] loss: 1741.174
[63,     1] loss: 1681.595
[64,     1] loss: 1670.150
[65,     1] loss: 1503.291
[66,     1] loss: 1709.219
[67,     1] loss: 1596.354
[68,     1] loss: 1574.886
[69,     1] loss: 1546.484
[70,     1] loss: 1645.529
[71,     1] loss: 1584.877
[72,     1] loss: 1412.406
[73,     1] loss: 1604.160
[74,     1] loss: 1575.732
[75,     1] loss: 1564.897
[76,     1] loss: 1654.840
[77,     1] loss: 1570.029
[78,     1] loss: 1493.408
[79,     1] loss: 1517.252
[80,     1] loss: 1591.940
[81,     1] loss: 1530.245
[82,     1] loss: 1671.516
[83,     1] loss: 1556.612
[84,     1] loss: 1447.030
[85,     1] loss: 1540.699
[86,     1] loss: 1440.174
[87,     1] loss: 1480.731
[88,     1] loss: 1452.842
[89,     1] loss: 1404.214
[90,     1] loss: 1493.912
[91,     1] loss: 1488.884
[92,     1] loss: 1495.648
[93,     1] loss: 1429.758
[94,     1] loss: 1338.703
[95,     1] loss: 1456.348
[96,     1] loss: 1430.558
[97,     1] loss: 1340.521
[98,     1] loss: 1430.128
[99,     1] loss: 1361.938
[100,     1] loss: 1341.291
[101,     1] loss: 1486.840
[102,     1] loss: 1385.926
[103,     1] loss: 1379.529
[104,     1] loss: 1257.483
[105,     1] loss: 1312.252
[106,     1] loss: 1202.373
Early stopping applied (best metric=0.4014686346054077)
Finished Training
Total time taken: 15.086610317230225
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2244.359
[2,     1] loss: 2239.288
[3,     1] loss: 2231.053
[4,     1] loss: 2258.414
[5,     1] loss: 2236.140
[6,     1] loss: 2254.063
[7,     1] loss: 2232.113
[8,     1] loss: 2229.260
[9,     1] loss: 2234.553
[10,     1] loss: 2231.356
[11,     1] loss: 2206.589
[12,     1] loss: 2209.879
[13,     1] loss: 2188.918
[14,     1] loss: 2171.810
[15,     1] loss: 2133.435
[16,     1] loss: 2106.947
[17,     1] loss: 2044.317
[18,     1] loss: 2056.940
[19,     1] loss: 2040.012
[20,     1] loss: 1979.753
[21,     1] loss: 1912.132
[22,     1] loss: 1944.452
[23,     1] loss: 1932.315
[24,     1] loss: 1942.904
[25,     1] loss: 1893.706
[26,     1] loss: 1860.889
[27,     1] loss: 1961.444
[28,     1] loss: 1975.674
[29,     1] loss: 1922.058
[30,     1] loss: 1887.754
[31,     1] loss: 1939.712
[32,     1] loss: 1896.046
[33,     1] loss: 1872.777
[34,     1] loss: 1844.644
[35,     1] loss: 1841.914
[36,     1] loss: 1791.509
[37,     1] loss: 1856.796
[38,     1] loss: 1894.178
[39,     1] loss: 1862.367
[40,     1] loss: 1766.560
[41,     1] loss: 1792.668
[42,     1] loss: 1907.109
[43,     1] loss: 1912.839
[44,     1] loss: 1726.632
[45,     1] loss: 1745.499
[46,     1] loss: 1760.430
[47,     1] loss: 1752.294
[48,     1] loss: 1828.838
[49,     1] loss: 1856.479
[50,     1] loss: 1810.985
[51,     1] loss: 1649.785
[52,     1] loss: 1766.571
[53,     1] loss: 1587.631
[54,     1] loss: 1719.396
[55,     1] loss: 1680.833
[56,     1] loss: 1729.479
[57,     1] loss: 1692.917
[58,     1] loss: 1692.134
[59,     1] loss: 1710.006
[60,     1] loss: 1631.520
[61,     1] loss: 1748.344
[62,     1] loss: 1657.678
[63,     1] loss: 1618.280
[64,     1] loss: 1588.730
[65,     1] loss: 1658.907
[66,     1] loss: 1738.870
[67,     1] loss: 1576.652
[68,     1] loss: 1667.687
[69,     1] loss: 1596.222
[70,     1] loss: 1562.364
[71,     1] loss: 1608.661
[72,     1] loss: 1611.631
[73,     1] loss: 1641.293
[74,     1] loss: 1465.048
[75,     1] loss: 1515.770
[76,     1] loss: 1605.562
[77,     1] loss: 1398.237
[78,     1] loss: 1488.645
[79,     1] loss: 1444.717
[80,     1] loss: 1470.116
[81,     1] loss: 1471.656
[82,     1] loss: 1313.492
[83,     1] loss: 1642.028
[84,     1] loss: 1380.544
[85,     1] loss: 1314.822
[86,     1] loss: 1345.000
[87,     1] loss: 1360.997
[88,     1] loss: 1343.914
[89,     1] loss: 1299.239
[90,     1] loss: 1454.214
[91,     1] loss: 1237.268
[92,     1] loss: 1261.451
[93,     1] loss: 1304.231
[94,     1] loss: 1320.297
[95,     1] loss: 1225.050
[96,     1] loss: 1300.534
[97,     1] loss: 1369.756
[98,     1] loss: 1104.859
[99,     1] loss: 1376.719
[100,     1] loss: 1465.231
[101,     1] loss: 1164.837
[102,     1] loss: 1311.014
[103,     1] loss: 1448.996
[104,     1] loss: 1295.645
[105,     1] loss: 1105.516
[106,     1] loss: 1157.225
[107,     1] loss: 1205.424
[108,     1] loss: 1070.209
[109,     1] loss: 1185.275
[110,     1] loss: 1212.671
[111,     1] loss: 1156.382
[112,     1] loss: 1122.630
[113,     1] loss: 1139.665
[114,     1] loss: 1067.643
[115,     1] loss: 1154.739
Early stopping applied (best metric=0.3482727110385895)
Finished Training
Total time taken: 16.551770448684692
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2237.221
[2,     1] loss: 2253.944
[3,     1] loss: 2246.706
[4,     1] loss: 2244.391
[5,     1] loss: 2239.511
[6,     1] loss: 2245.167
[7,     1] loss: 2247.364
[8,     1] loss: 2229.118
[9,     1] loss: 2235.258
[10,     1] loss: 2236.689
[11,     1] loss: 2230.354
[12,     1] loss: 2233.857
[13,     1] loss: 2215.701
[14,     1] loss: 2217.429
[15,     1] loss: 2206.123
[16,     1] loss: 2204.801
[17,     1] loss: 2175.638
[18,     1] loss: 2147.740
[19,     1] loss: 2162.658
[20,     1] loss: 2083.089
[21,     1] loss: 2059.194
[22,     1] loss: 2066.772
[23,     1] loss: 1951.592
[24,     1] loss: 1998.491
[25,     1] loss: 1953.148
[26,     1] loss: 1907.583
[27,     1] loss: 1833.747
[28,     1] loss: 1879.734
[29,     1] loss: 1919.391
[30,     1] loss: 1984.818
[31,     1] loss: 1977.073
[32,     1] loss: 1905.871
[33,     1] loss: 1895.291
[34,     1] loss: 1902.811
[35,     1] loss: 1809.163
[36,     1] loss: 1898.234
[37,     1] loss: 1889.384
[38,     1] loss: 1850.621
[39,     1] loss: 1806.008
[40,     1] loss: 1847.901
[41,     1] loss: 1869.923
[42,     1] loss: 1841.191
[43,     1] loss: 1835.032
[44,     1] loss: 1872.278
[45,     1] loss: 1702.275
[46,     1] loss: 1841.837
[47,     1] loss: 1765.012
[48,     1] loss: 1766.584
[49,     1] loss: 1827.158
[50,     1] loss: 1647.622
[51,     1] loss: 1720.510
[52,     1] loss: 1735.287
[53,     1] loss: 1683.879
[54,     1] loss: 1718.187
[55,     1] loss: 1626.651
[56,     1] loss: 1690.311
[57,     1] loss: 1667.089
[58,     1] loss: 1638.647
[59,     1] loss: 1548.961
[60,     1] loss: 1571.205
[61,     1] loss: 1667.793
[62,     1] loss: 1557.105
[63,     1] loss: 1626.681
[64,     1] loss: 1513.386
[65,     1] loss: 1680.790
[66,     1] loss: 1699.216
[67,     1] loss: 1542.106
[68,     1] loss: 1545.944
[69,     1] loss: 1624.396
[70,     1] loss: 1421.595
[71,     1] loss: 1580.132
[72,     1] loss: 1462.233
[73,     1] loss: 1568.107
[74,     1] loss: 1464.802
[75,     1] loss: 1682.669
[76,     1] loss: 1592.491
[77,     1] loss: 1417.819
[78,     1] loss: 1486.413
[79,     1] loss: 1416.315
[80,     1] loss: 1616.546
[81,     1] loss: 1390.279
[82,     1] loss: 1482.209
[83,     1] loss: 1437.120
[84,     1] loss: 1293.913
[85,     1] loss: 1359.070
[86,     1] loss: 1432.427
[87,     1] loss: 1285.342
[88,     1] loss: 1476.775
[89,     1] loss: 1381.568
[90,     1] loss: 1428.166
[91,     1] loss: 1353.329
[92,     1] loss: 1326.417
[93,     1] loss: 1398.783
[94,     1] loss: 1262.890
[95,     1] loss: 1362.284
[96,     1] loss: 1247.052
[97,     1] loss: 1256.833
[98,     1] loss: 1255.973
[99,     1] loss: 1221.312
Early stopping applied (best metric=0.4117855131626129)
Finished Training
Total time taken: 14.262521266937256
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2235.556
[2,     1] loss: 2249.845
[3,     1] loss: 2249.586
[4,     1] loss: 2241.545
[5,     1] loss: 2237.581
[6,     1] loss: 2237.452
[7,     1] loss: 2240.292
[8,     1] loss: 2226.203
[9,     1] loss: 2219.189
[10,     1] loss: 2218.302
[11,     1] loss: 2219.359
[12,     1] loss: 2170.416
[13,     1] loss: 2142.951
[14,     1] loss: 2106.010
[15,     1] loss: 2065.124
[16,     1] loss: 2042.117
[17,     1] loss: 1938.316
[18,     1] loss: 1992.137
[19,     1] loss: 1898.684
[20,     1] loss: 1821.003
[21,     1] loss: 1972.453
[22,     1] loss: 1810.256
[23,     1] loss: 1877.131
[24,     1] loss: 1846.588
[25,     1] loss: 1831.342
[26,     1] loss: 1954.661
[27,     1] loss: 1823.543
[28,     1] loss: 1809.983
[29,     1] loss: 1773.117
[30,     1] loss: 1731.715
[31,     1] loss: 1835.127
[32,     1] loss: 1773.392
[33,     1] loss: 1804.364
[34,     1] loss: 1900.821
[35,     1] loss: 1820.285
[36,     1] loss: 1788.421
[37,     1] loss: 1773.184
[38,     1] loss: 1767.983
[39,     1] loss: 1759.711
[40,     1] loss: 1610.110
[41,     1] loss: 1775.337
[42,     1] loss: 1699.604
[43,     1] loss: 1546.851
[44,     1] loss: 1768.979
[45,     1] loss: 1648.839
[46,     1] loss: 1622.800
[47,     1] loss: 1751.931
[48,     1] loss: 1628.167
[49,     1] loss: 1617.375
[50,     1] loss: 1621.978
[51,     1] loss: 1506.065
[52,     1] loss: 1544.602
[53,     1] loss: 1528.227
[54,     1] loss: 1411.099
[55,     1] loss: 1540.173
[56,     1] loss: 1414.220
[57,     1] loss: 1549.827
[58,     1] loss: 1615.468
[59,     1] loss: 1625.442
[60,     1] loss: 1423.085
[61,     1] loss: 1495.196
[62,     1] loss: 1360.551
[63,     1] loss: 1344.523
[64,     1] loss: 1589.816
[65,     1] loss: 1428.082
[66,     1] loss: 1326.545
[67,     1] loss: 1314.659
[68,     1] loss: 1225.199
[69,     1] loss: 1338.407
[70,     1] loss: 1310.105
[71,     1] loss: 1269.978
[72,     1] loss: 1234.120
[73,     1] loss: 1129.791
[74,     1] loss: 1270.260
[75,     1] loss: 1162.087
[76,     1] loss: 1285.881
[77,     1] loss: 1279.022
[78,     1] loss: 1154.547
[79,     1] loss: 1112.063
[80,     1] loss: 1240.970
[81,     1] loss: 1285.892
[82,     1] loss: 1384.632
[83,     1] loss: 1160.297
[84,     1] loss: 1165.362
[85,     1] loss: 1100.789
Early stopping applied (best metric=0.4733525216579437)
Finished Training
Total time taken: 12.09429121017456
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2244.313
[2,     1] loss: 2235.924
[3,     1] loss: 2246.325
[4,     1] loss: 2237.719
[5,     1] loss: 2250.304
[6,     1] loss: 2242.645
[7,     1] loss: 2240.735
[8,     1] loss: 2230.867
[9,     1] loss: 2238.183
[10,     1] loss: 2229.373
[11,     1] loss: 2230.832
[12,     1] loss: 2214.343
[13,     1] loss: 2217.128
[14,     1] loss: 2180.674
[15,     1] loss: 2157.506
[16,     1] loss: 2149.068
[17,     1] loss: 2108.948
[18,     1] loss: 2089.588
[19,     1] loss: 2023.175
[20,     1] loss: 2019.413
[21,     1] loss: 1958.016
[22,     1] loss: 1907.329
[23,     1] loss: 1909.573
[24,     1] loss: 1909.718
[25,     1] loss: 1930.654
[26,     1] loss: 1845.578
[27,     1] loss: 1833.503
[28,     1] loss: 1854.609
[29,     1] loss: 1934.849
[30,     1] loss: 1732.811
[31,     1] loss: 1861.938
[32,     1] loss: 1844.076
[33,     1] loss: 1851.912
[34,     1] loss: 1744.034
[35,     1] loss: 1742.229
[36,     1] loss: 1735.423
[37,     1] loss: 1774.413
[38,     1] loss: 1765.751
[39,     1] loss: 1778.581
[40,     1] loss: 1727.659
[41,     1] loss: 1574.139
[42,     1] loss: 1638.232
[43,     1] loss: 1637.015
[44,     1] loss: 1541.324
[45,     1] loss: 1481.533
[46,     1] loss: 1516.828
[47,     1] loss: 1665.784
[48,     1] loss: 1555.124
[49,     1] loss: 1649.282
[50,     1] loss: 1439.024
[51,     1] loss: 1702.456
[52,     1] loss: 1586.094
[53,     1] loss: 1664.185
[54,     1] loss: 1543.279
[55,     1] loss: 1556.562
[56,     1] loss: 1492.374
[57,     1] loss: 1508.380
[58,     1] loss: 1532.052
[59,     1] loss: 1464.595
[60,     1] loss: 1539.119
[61,     1] loss: 1510.654
[62,     1] loss: 1472.290
[63,     1] loss: 1472.822
[64,     1] loss: 1353.124
[65,     1] loss: 1378.079
[66,     1] loss: 1331.104
[67,     1] loss: 1491.978
[68,     1] loss: 1316.416
[69,     1] loss: 1332.620
[70,     1] loss: 1376.468
[71,     1] loss: 1249.707
[72,     1] loss: 1400.490
[73,     1] loss: 1318.177
[74,     1] loss: 1425.891
[75,     1] loss: 1291.242
[76,     1] loss: 1334.987
[77,     1] loss: 1394.547
[78,     1] loss: 1369.507
[79,     1] loss: 1190.041
[80,     1] loss: 1396.705
[81,     1] loss: 1310.811
[82,     1] loss: 1337.778
[83,     1] loss: 1216.545
[84,     1] loss: 1332.378
Early stopping applied (best metric=0.37560349702835083)
Finished Training
Total time taken: 12.064290046691895
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2237.064
[2,     1] loss: 2257.036
[3,     1] loss: 2243.610
[4,     1] loss: 2241.244
[5,     1] loss: 2237.422
[6,     1] loss: 2250.623
[7,     1] loss: 2236.171
[8,     1] loss: 2244.874
[9,     1] loss: 2244.543
[10,     1] loss: 2239.546
[11,     1] loss: 2250.706
[12,     1] loss: 2255.719
[13,     1] loss: 2230.587
[14,     1] loss: 2222.990
[15,     1] loss: 2222.967
[16,     1] loss: 2201.014
[17,     1] loss: 2199.963
[18,     1] loss: 2173.799
[19,     1] loss: 2161.013
[20,     1] loss: 2175.575
[21,     1] loss: 2120.122
[22,     1] loss: 2103.656
[23,     1] loss: 2011.194
[24,     1] loss: 2008.865
[25,     1] loss: 2027.979
[26,     1] loss: 2017.015
[27,     1] loss: 1858.812
[28,     1] loss: 2033.736
[29,     1] loss: 1972.479
[30,     1] loss: 1947.650
[31,     1] loss: 1849.865
[32,     1] loss: 2014.815
[33,     1] loss: 1774.733
[34,     1] loss: 1891.370
[35,     1] loss: 1843.496
[36,     1] loss: 1808.317
[37,     1] loss: 1823.301
[38,     1] loss: 1841.611
[39,     1] loss: 1787.621
[40,     1] loss: 1762.535
[41,     1] loss: 1654.185
[42,     1] loss: 1722.233
[43,     1] loss: 1756.425
[44,     1] loss: 1703.033
[45,     1] loss: 1752.341
[46,     1] loss: 1759.079
[47,     1] loss: 1660.508
[48,     1] loss: 1622.405
[49,     1] loss: 1547.061
[50,     1] loss: 1665.157
[51,     1] loss: 1532.824
[52,     1] loss: 1468.409
[53,     1] loss: 1575.729
[54,     1] loss: 1594.603
[55,     1] loss: 1592.115
[56,     1] loss: 1501.274
[57,     1] loss: 1487.288
[58,     1] loss: 1457.776
[59,     1] loss: 1487.991
[60,     1] loss: 1395.351
[61,     1] loss: 1512.006
[62,     1] loss: 1446.633
[63,     1] loss: 1498.897
[64,     1] loss: 1414.213
[65,     1] loss: 1395.496
[66,     1] loss: 1373.173
[67,     1] loss: 1549.220
[68,     1] loss: 1396.474
[69,     1] loss: 1408.159
[70,     1] loss: 1432.783
[71,     1] loss: 1465.643
[72,     1] loss: 1334.164
[73,     1] loss: 1506.227
[74,     1] loss: 1416.869
[75,     1] loss: 1395.316
[76,     1] loss: 1383.575
[77,     1] loss: 1382.675
[78,     1] loss: 1237.432
[79,     1] loss: 1281.021
[80,     1] loss: 1335.958
[81,     1] loss: 1376.467
[82,     1] loss: 1258.408
[83,     1] loss: 1250.949
[84,     1] loss: 1303.780
[85,     1] loss: 1340.902
[86,     1] loss: 1134.435
[87,     1] loss: 1255.705
[88,     1] loss: 1281.713
[89,     1] loss: 1256.875
[90,     1] loss: 1244.327
Early stopping applied (best metric=0.460972398519516)
Finished Training
Total time taken: 12.92438268661499
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2243.572
[2,     1] loss: 2240.115
[3,     1] loss: 2247.073
[4,     1] loss: 2255.232
[5,     1] loss: 2239.118
[6,     1] loss: 2247.791
[7,     1] loss: 2239.700
[8,     1] loss: 2230.089
[9,     1] loss: 2238.236
[10,     1] loss: 2241.687
[11,     1] loss: 2239.300
[12,     1] loss: 2238.556
[13,     1] loss: 2235.464
[14,     1] loss: 2225.989
[15,     1] loss: 2218.677
[16,     1] loss: 2215.527
[17,     1] loss: 2205.146
[18,     1] loss: 2195.098
[19,     1] loss: 2168.469
[20,     1] loss: 2129.658
[21,     1] loss: 2085.201
[22,     1] loss: 2072.216
[23,     1] loss: 2002.511
[24,     1] loss: 2009.806
[25,     1] loss: 1956.465
[26,     1] loss: 1932.094
[27,     1] loss: 1907.219
[28,     1] loss: 1849.856
[29,     1] loss: 1850.081
[30,     1] loss: 1933.478
[31,     1] loss: 1890.064
[32,     1] loss: 1894.970
[33,     1] loss: 1883.848
[34,     1] loss: 1755.575
[35,     1] loss: 1904.710
[36,     1] loss: 1805.854
[37,     1] loss: 1785.769
[38,     1] loss: 1785.497
[39,     1] loss: 1813.564
[40,     1] loss: 1900.366
[41,     1] loss: 1820.355
[42,     1] loss: 1783.986
[43,     1] loss: 1676.593
[44,     1] loss: 1698.757
[45,     1] loss: 1848.611
[46,     1] loss: 1710.262
[47,     1] loss: 1803.780
[48,     1] loss: 1683.274
[49,     1] loss: 1594.564
[50,     1] loss: 1837.756
[51,     1] loss: 1647.673
[52,     1] loss: 1584.838
[53,     1] loss: 1583.638
[54,     1] loss: 1571.342
[55,     1] loss: 1562.556
[56,     1] loss: 1481.552
[57,     1] loss: 1528.391
[58,     1] loss: 1483.341
[59,     1] loss: 1525.219
[60,     1] loss: 1443.252
[61,     1] loss: 1496.534
[62,     1] loss: 1601.059
[63,     1] loss: 1540.584
[64,     1] loss: 1418.695
[65,     1] loss: 1505.593
[66,     1] loss: 1538.471
[67,     1] loss: 1514.821
[68,     1] loss: 1438.951
[69,     1] loss: 1327.639
[70,     1] loss: 1444.633
[71,     1] loss: 1396.174
[72,     1] loss: 1369.445
[73,     1] loss: 1396.543
[74,     1] loss: 1478.169
Early stopping applied (best metric=0.4593924582004547)
Finished Training
Total time taken: 10.65113615989685
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2249.117
[2,     1] loss: 2238.093
[3,     1] loss: 2244.294
[4,     1] loss: 2264.946
[5,     1] loss: 2239.869
[6,     1] loss: 2246.610
[7,     1] loss: 2244.844
[8,     1] loss: 2244.709
[9,     1] loss: 2242.344
[10,     1] loss: 2231.813
[11,     1] loss: 2234.292
[12,     1] loss: 2234.234
[13,     1] loss: 2237.824
[14,     1] loss: 2229.456
[15,     1] loss: 2242.066
[16,     1] loss: 2222.143
[17,     1] loss: 2226.552
[18,     1] loss: 2223.738
[19,     1] loss: 2221.900
[20,     1] loss: 2216.191
[21,     1] loss: 2212.933
[22,     1] loss: 2178.973
[23,     1] loss: 2187.237
[24,     1] loss: 2130.118
[25,     1] loss: 2123.211
[26,     1] loss: 2053.653
[27,     1] loss: 2086.174
[28,     1] loss: 2059.618
[29,     1] loss: 2019.458
[30,     1] loss: 1938.640
[31,     1] loss: 1910.933
[32,     1] loss: 1925.024
[33,     1] loss: 1919.517
[34,     1] loss: 1953.364
[35,     1] loss: 1842.646
[36,     1] loss: 1939.864
[37,     1] loss: 1847.130
[38,     1] loss: 1988.848
[39,     1] loss: 1998.977
[40,     1] loss: 1955.619
[41,     1] loss: 1782.989
[42,     1] loss: 1813.851
[43,     1] loss: 1838.690
[44,     1] loss: 1765.643
[45,     1] loss: 1869.226
[46,     1] loss: 1827.406
[47,     1] loss: 1767.228
[48,     1] loss: 1814.895
[49,     1] loss: 1731.471
[50,     1] loss: 1760.936
[51,     1] loss: 1761.837
[52,     1] loss: 1725.672
[53,     1] loss: 1622.815
[54,     1] loss: 1553.997
[55,     1] loss: 1707.521
[56,     1] loss: 1582.821
[57,     1] loss: 1586.143
[58,     1] loss: 1621.208
[59,     1] loss: 1485.251
[60,     1] loss: 1525.030
[61,     1] loss: 1581.677
[62,     1] loss: 1460.046
[63,     1] loss: 1519.137
[64,     1] loss: 1422.967
[65,     1] loss: 1514.570
[66,     1] loss: 1493.610
[67,     1] loss: 1336.065
[68,     1] loss: 1500.519
[69,     1] loss: 1487.937
[70,     1] loss: 1331.466
[71,     1] loss: 1488.049
[72,     1] loss: 1348.807
[73,     1] loss: 1512.581
[74,     1] loss: 1357.583
[75,     1] loss: 1433.133
[76,     1] loss: 1313.961
[77,     1] loss: 1188.101
[78,     1] loss: 1336.964
[79,     1] loss: 1361.407
[80,     1] loss: 1353.234
Early stopping applied (best metric=0.45348086953163147)
Finished Training
Total time taken: 11.488227605819702
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2228.039
[2,     1] loss: 2259.027
[3,     1] loss: 2253.966
[4,     1] loss: 2247.834
[5,     1] loss: 2242.147
[6,     1] loss: 2231.438
[7,     1] loss: 2250.598
[8,     1] loss: 2235.021
[9,     1] loss: 2258.675
[10,     1] loss: 2234.845
[11,     1] loss: 2230.449
[12,     1] loss: 2230.837
[13,     1] loss: 2223.135
[14,     1] loss: 2225.998
[15,     1] loss: 2209.354
[16,     1] loss: 2210.190
[17,     1] loss: 2207.025
[18,     1] loss: 2170.910
[19,     1] loss: 2148.003
[20,     1] loss: 2094.621
[21,     1] loss: 2116.653
[22,     1] loss: 2085.345
[23,     1] loss: 1989.396
[24,     1] loss: 2072.959
[25,     1] loss: 1973.746
[26,     1] loss: 1870.313
[27,     1] loss: 1986.837
[28,     1] loss: 1907.268
[29,     1] loss: 1944.561
[30,     1] loss: 1914.088
[31,     1] loss: 1993.705
[32,     1] loss: 1892.884
[33,     1] loss: 1920.485
[34,     1] loss: 1909.997
[35,     1] loss: 1887.506
[36,     1] loss: 1880.563
[37,     1] loss: 1830.380
[38,     1] loss: 1875.535
[39,     1] loss: 1817.146
[40,     1] loss: 1810.409
[41,     1] loss: 1878.986
[42,     1] loss: 1775.229
[43,     1] loss: 1787.758
[44,     1] loss: 1857.448
[45,     1] loss: 1831.622
[46,     1] loss: 1822.671
[47,     1] loss: 1839.247
[48,     1] loss: 1814.737
[49,     1] loss: 1844.290
[50,     1] loss: 1734.269
[51,     1] loss: 1689.504
[52,     1] loss: 1602.760
[53,     1] loss: 1621.960
[54,     1] loss: 1778.601
[55,     1] loss: 1686.073
[56,     1] loss: 1678.369
[57,     1] loss: 1662.998
[58,     1] loss: 1633.896
[59,     1] loss: 1658.568
[60,     1] loss: 1700.592
[61,     1] loss: 1669.454
[62,     1] loss: 1674.801
[63,     1] loss: 1640.827
[64,     1] loss: 1560.735
[65,     1] loss: 1536.811
[66,     1] loss: 1521.645
[67,     1] loss: 1555.741
[68,     1] loss: 1601.617
[69,     1] loss: 1534.941
[70,     1] loss: 1514.655
[71,     1] loss: 1602.626
[72,     1] loss: 1474.289
[73,     1] loss: 1556.665
[74,     1] loss: 1421.955
[75,     1] loss: 1365.906
[76,     1] loss: 1370.221
[77,     1] loss: 1462.209
[78,     1] loss: 1394.072
[79,     1] loss: 1410.641
[80,     1] loss: 1326.729
[81,     1] loss: 1296.352
[82,     1] loss: 1363.378
[83,     1] loss: 1438.448
[84,     1] loss: 1480.788
[85,     1] loss: 1285.516
[86,     1] loss: 1414.795
[87,     1] loss: 1450.440
[88,     1] loss: 1198.416
[89,     1] loss: 1201.180
[90,     1] loss: 1237.161
[91,     1] loss: 1262.056
[92,     1] loss: 1278.732
[93,     1] loss: 1249.075
[94,     1] loss: 1254.363
[95,     1] loss: 1118.271
[96,     1] loss: 1157.899
[97,     1] loss: 1137.143
[98,     1] loss: 1137.834
[99,     1] loss: 1089.015
Early stopping applied (best metric=0.4278033375740051)
Finished Training
Total time taken: 14.211519002914429
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2252.551
[2,     1] loss: 2245.480
[3,     1] loss: 2235.594
[4,     1] loss: 2240.465
[5,     1] loss: 2245.989
[6,     1] loss: 2233.081
[7,     1] loss: 2254.348
[8,     1] loss: 2249.036
[9,     1] loss: 2243.949
[10,     1] loss: 2236.114
[11,     1] loss: 2238.178
[12,     1] loss: 2239.066
[13,     1] loss: 2249.193
[14,     1] loss: 2224.006
[15,     1] loss: 2227.666
[16,     1] loss: 2222.711
[17,     1] loss: 2223.670
[18,     1] loss: 2204.919
[19,     1] loss: 2184.993
[20,     1] loss: 2163.683
[21,     1] loss: 2160.937
[22,     1] loss: 2116.751
[23,     1] loss: 2088.192
[24,     1] loss: 2077.530
[25,     1] loss: 2007.559
[26,     1] loss: 1979.138
[27,     1] loss: 1920.344
[28,     1] loss: 1941.838
[29,     1] loss: 2003.153
[30,     1] loss: 1906.203
[31,     1] loss: 1914.322
[32,     1] loss: 1841.492
[33,     1] loss: 1887.897
[34,     1] loss: 1793.183
[35,     1] loss: 1884.308
[36,     1] loss: 1793.553
[37,     1] loss: 1841.869
[38,     1] loss: 1859.147
[39,     1] loss: 1947.949
[40,     1] loss: 1813.675
[41,     1] loss: 1823.983
[42,     1] loss: 1800.159
[43,     1] loss: 1771.918
[44,     1] loss: 1793.249
[45,     1] loss: 1711.758
[46,     1] loss: 1735.383
[47,     1] loss: 1792.726
[48,     1] loss: 1674.796
[49,     1] loss: 1676.724
[50,     1] loss: 1677.368
[51,     1] loss: 1809.939
[52,     1] loss: 1657.462
[53,     1] loss: 1598.363
[54,     1] loss: 1678.350
[55,     1] loss: 1654.361
[56,     1] loss: 1699.804
[57,     1] loss: 1640.125
[58,     1] loss: 1659.697
[59,     1] loss: 1630.099
[60,     1] loss: 1478.420
[61,     1] loss: 1595.093
[62,     1] loss: 1503.287
[63,     1] loss: 1528.350
[64,     1] loss: 1527.646
[65,     1] loss: 1417.003
[66,     1] loss: 1440.864
[67,     1] loss: 1448.257
[68,     1] loss: 1527.146
[69,     1] loss: 1638.135
[70,     1] loss: 1513.580
[71,     1] loss: 1678.456
[72,     1] loss: 1473.603
[73,     1] loss: 1601.848
[74,     1] loss: 1529.111
[75,     1] loss: 1458.068
[76,     1] loss: 1435.302
[77,     1] loss: 1610.487
[78,     1] loss: 1467.539
[79,     1] loss: 1419.285
[80,     1] loss: 1456.255
[81,     1] loss: 1423.821
[82,     1] loss: 1336.955
[83,     1] loss: 1348.409
[84,     1] loss: 1475.968
[85,     1] loss: 1481.344
[86,     1] loss: 1339.692
[87,     1] loss: 1426.805
[88,     1] loss: 1317.021
[89,     1] loss: 1351.394
[90,     1] loss: 1397.788
[91,     1] loss: 1453.843
Early stopping applied (best metric=0.42099326848983765)
Finished Training
Total time taken: 13.086402177810669
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 2245.390
[2,     1] loss: 2247.471
[3,     1] loss: 2241.557
[4,     1] loss: 2246.357
[5,     1] loss: 2247.526
[6,     1] loss: 2250.496
[7,     1] loss: 2240.010
[8,     1] loss: 2233.254
[9,     1] loss: 2237.429
[10,     1] loss: 2236.429
[11,     1] loss: 2230.805
[12,     1] loss: 2218.533
[13,     1] loss: 2201.893
[14,     1] loss: 2202.688
[15,     1] loss: 2167.451
[16,     1] loss: 2152.287
[17,     1] loss: 2122.431
[18,     1] loss: 2090.512
[19,     1] loss: 2066.962
[20,     1] loss: 1985.694
[21,     1] loss: 1967.640
[22,     1] loss: 1991.923
[23,     1] loss: 1928.267
[24,     1] loss: 2000.065
[25,     1] loss: 1865.136
[26,     1] loss: 1883.874
[27,     1] loss: 1975.899
[28,     1] loss: 1816.940
[29,     1] loss: 1858.167
[30,     1] loss: 1886.081
[31,     1] loss: 1878.516
[32,     1] loss: 1897.336
[33,     1] loss: 1886.889
[34,     1] loss: 1825.630
[35,     1] loss: 1833.740
[36,     1] loss: 1844.507
[37,     1] loss: 1892.017
[38,     1] loss: 1771.228
[39,     1] loss: 1937.166
[40,     1] loss: 1840.825
[41,     1] loss: 1715.589
[42,     1] loss: 1841.063
[43,     1] loss: 1789.545
[44,     1] loss: 1723.440
[45,     1] loss: 1726.613
[46,     1] loss: 1733.064
[47,     1] loss: 1725.995
[48,     1] loss: 1717.668
[49,     1] loss: 1728.314
[50,     1] loss: 1683.357
[51,     1] loss: 1718.583
[52,     1] loss: 1617.580
[53,     1] loss: 1703.040
[54,     1] loss: 1594.372
[55,     1] loss: 1652.613
[56,     1] loss: 1559.044
[57,     1] loss: 1552.625
[58,     1] loss: 1571.101
[59,     1] loss: 1584.546
[60,     1] loss: 1586.702
[61,     1] loss: 1516.244
[62,     1] loss: 1621.686
[63,     1] loss: 1558.568
[64,     1] loss: 1426.898
[65,     1] loss: 1530.734
[66,     1] loss: 1619.719
[67,     1] loss: 1503.182
[68,     1] loss: 1507.290
[69,     1] loss: 1570.763
[70,     1] loss: 1502.690
[71,     1] loss: 1353.638
[72,     1] loss: 1466.740
[73,     1] loss: 1397.485
[74,     1] loss: 1448.047
Early stopping applied (best metric=0.44010454416275024)
Finished Training
Total time taken: 10.647139310836792
{'Hydroxylation-K Validation Accuracy': 0.7153664302600473, 'Hydroxylation-K Validation Sensitivity': 0.7925925925925926, 'Hydroxylation-K Validation Specificity': 0.6964912280701754, 'Hydroxylation-K Validation Precision': 0.4161790279763646, 'Hydroxylation-K AUC ROC': 0.8145614035087719, 'Hydroxylation-K AUC PR': 0.5686026837379631, 'Hydroxylation-K MCC': 0.41151472268930966, 'Hydroxylation-K F1': 0.5377993247964546, 'Validation Loss (Hydroxylation-K)': 0.436652867992719, 'Hydroxylation-P Validation Accuracy': 0.730340693365819, 'Hydroxylation-P Validation Sensitivity': 0.7293650793650794, 'Hydroxylation-P Validation Specificity': 0.7305701032470447, 'Hydroxylation-P Validation Precision': 0.3687949815400723, 'Hydroxylation-P AUC ROC': 0.787009087655878, 'Hydroxylation-P AUC PR': 0.48979748595652656, 'Hydroxylation-P MCC': 0.36862738335095246, 'Hydroxylation-P F1': 0.4886842794734486, 'Validation Loss (Hydroxylation-P)': 0.43058407107988994, 'Validation Loss (total)': 0.8672369400660197, 'TimeToTrain': 12.420460828145345}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002019490378503429,
 'learning_rate_Hydroxylation-K': 0.0005077530160314833,
 'learning_rate_Hydroxylation-P': 0.006428422232753811,
 'log_base': 2.6104234116934903,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2141484573,
 'sample_weights': [6.329540612852032, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.544235076948325,
 'weight_decay_Hydroxylation-K': 7.82917378858528,
 'weight_decay_Hydroxylation-P': 3.9413530217668056}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.868
[2,     1] loss: 1276.671
[3,     1] loss: 1274.557
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035264865937867504,
 'learning_rate_Hydroxylation-K': 0.0009342656818099029,
 'learning_rate_Hydroxylation-P': 0.005881822756347687,
 'log_base': 1.2750481176265134,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3237230954,
 'sample_weights': [1.7398869329222149, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.10641730617065,
 'weight_decay_Hydroxylation-K': 4.699493271278618,
 'weight_decay_Hydroxylation-P': 2.838923793589545}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2365.471
[2,     1] loss: 2357.006
[3,     1] loss: 2360.294
[4,     1] loss: 2353.612
[5,     1] loss: 2351.578
[6,     1] loss: 2355.670
[7,     1] loss: 2353.725
[8,     1] loss: 2346.216
[9,     1] loss: 2350.700
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0013609112751709796,
 'learning_rate_Hydroxylation-K': 0.0005438163567481756,
 'learning_rate_Hydroxylation-P': 0.003792477071910323,
 'log_base': 2.123116114568048,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2789237250,
 'sample_weights': [6.870591134469686, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.2126278631118996,
 'weight_decay_Hydroxylation-K': 1.2483699204415257,
 'weight_decay_Hydroxylation-P': 0.2635685515587549}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1381.599
[2,     1] loss: 1374.781
[3,     1] loss: 1375.104
[4,     1] loss: 1371.995
[5,     1] loss: 1373.803
[6,     1] loss: 1376.097
[7,     1] loss: 1370.775
[8,     1] loss: 1374.860
[9,     1] loss: 1373.893
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003518486314683845,
 'learning_rate_Hydroxylation-K': 0.00620822132450912,
 'learning_rate_Hydroxylation-P': 0.007144830469069031,
 'log_base': 1.2354531603075949,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2334358130,
 'sample_weights': [2.217394987185729, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.302500253827594,
 'weight_decay_Hydroxylation-K': 9.415897130643238,
 'weight_decay_Hydroxylation-P': 1.2194876033337916}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2586.821
[2,     1] loss: 2563.443
[3,     1] loss: 2575.901
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003092335584820172,
 'learning_rate_Hydroxylation-K': 0.003058701592681921,
 'learning_rate_Hydroxylation-P': 0.006352984287090916,
 'log_base': 1.268617618204463,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 789517913,
 'sample_weights': [7.8956689739962975, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.6732281867018415,
 'weight_decay_Hydroxylation-K': 9.467950494763711,
 'weight_decay_Hydroxylation-P': 2.2098268557140686}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2387.646
[2,     1] loss: 2389.755
[3,     1] loss: 2388.488
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0038662374920557753,
 'learning_rate_Hydroxylation-K': 0.006795249942441952,
 'learning_rate_Hydroxylation-P': 0.009693151743945408,
 'log_base': 2.3648116732936058,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 764589134,
 'sample_weights': [7.016595038056578, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.648922964704582,
 'weight_decay_Hydroxylation-K': 4.146016438981525,
 'weight_decay_Hydroxylation-P': 2.841317818212607}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1322.895
[2,     1] loss: 1318.764
[3,     1] loss: 1319.146
[4,     1] loss: 1319.915
[5,     1] loss: 1315.777
[6,     1] loss: 1315.718
[7,     1] loss: 1315.501
[8,     1] loss: 1311.784
[9,     1] loss: 1309.752
[10,     1] loss: 1305.229
[11,     1] loss: 1295.454
[12,     1] loss: 1278.511
[13,     1] loss: 1253.029
[14,     1] loss: 1222.319
[15,     1] loss: 1212.174
[16,     1] loss: 1167.190
[17,     1] loss: 1127.926
[18,     1] loss: 1113.289
[19,     1] loss: 1090.801
[20,     1] loss: 1066.893
[21,     1] loss: 1129.846
[22,     1] loss: 1125.312
[23,     1] loss: 1114.426
[24,     1] loss: 1070.025
[25,     1] loss: 1095.747
[26,     1] loss: 1059.198
[27,     1] loss: 1088.432
[28,     1] loss: 1002.137
[29,     1] loss: 1002.312
[30,     1] loss: 1025.715
[31,     1] loss: 983.594
[32,     1] loss: 1009.170
[33,     1] loss: 968.976
[34,     1] loss: 1006.851
[35,     1] loss: 1000.605
[36,     1] loss: 974.119
[37,     1] loss: 920.588
[38,     1] loss: 955.761
[39,     1] loss: 962.284
[40,     1] loss: 952.125
[41,     1] loss: 984.860
[42,     1] loss: 1034.650
[43,     1] loss: 958.231
[44,     1] loss: 956.942
[45,     1] loss: 899.954
[46,     1] loss: 903.893
[47,     1] loss: 875.095
[48,     1] loss: 974.359
[49,     1] loss: 931.789
[50,     1] loss: 914.646
[51,     1] loss: 879.085
[52,     1] loss: 835.538
[53,     1] loss: 929.926
[54,     1] loss: 924.517
[55,     1] loss: 871.423
[56,     1] loss: 895.331
[57,     1] loss: 876.876
[58,     1] loss: 904.009
[59,     1] loss: 807.399
[60,     1] loss: 825.336
[61,     1] loss: 797.298
[62,     1] loss: 809.509
[63,     1] loss: 808.700
[64,     1] loss: 762.726
[65,     1] loss: 878.153
[66,     1] loss: 875.755
[67,     1] loss: 786.303
[68,     1] loss: 873.311
[69,     1] loss: 785.060
[70,     1] loss: 852.602
[71,     1] loss: 816.102
[72,     1] loss: 823.452
[73,     1] loss: 814.519
[74,     1] loss: 799.539
[75,     1] loss: 766.232
[76,     1] loss: 781.097
[77,     1] loss: 714.917
[78,     1] loss: 865.130
[79,     1] loss: 718.848
[80,     1] loss: 742.708
[81,     1] loss: 836.847
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006174230060622576,
 'learning_rate_Hydroxylation-K': 0.005888040833502738,
 'learning_rate_Hydroxylation-P': 0.009287947561335825,
 'log_base': 2.9607248424433523,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 156301944,
 'sample_weights': [1.9396378227728688, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1452570527882062,
 'weight_decay_Hydroxylation-K': 5.322067984546772,
 'weight_decay_Hydroxylation-P': 3.804250956835963}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.882
[2,     1] loss: 1233.250
[3,     1] loss: 1229.535
[4,     1] loss: 1229.552
[5,     1] loss: 1230.999
[6,     1] loss: 1224.586
[7,     1] loss: 1222.503
[8,     1] loss: 1214.513
[9,     1] loss: 1215.781
[10,     1] loss: 1184.651
[11,     1] loss: 1162.999
[12,     1] loss: 1110.910
[13,     1] loss: 1111.814
[14,     1] loss: 1083.818
[15,     1] loss: 999.744
[16,     1] loss: 1053.921
[17,     1] loss: 1065.364
[18,     1] loss: 1037.304
[19,     1] loss: 1011.318
[20,     1] loss: 1011.216
[21,     1] loss: 978.145
[22,     1] loss: 1005.668
[23,     1] loss: 1008.280
[24,     1] loss: 984.090
[25,     1] loss: 969.108
[26,     1] loss: 967.270
[27,     1] loss: 941.728
[28,     1] loss: 1028.686
[29,     1] loss: 966.974
[30,     1] loss: 962.906
[31,     1] loss: 925.475
[32,     1] loss: 935.053
[33,     1] loss: 992.252
[34,     1] loss: 950.079
[35,     1] loss: 936.179
[36,     1] loss: 932.128
[37,     1] loss: 941.657
[38,     1] loss: 969.983
[39,     1] loss: 918.314
[40,     1] loss: 869.297
[41,     1] loss: 941.372
[42,     1] loss: 930.423
[43,     1] loss: 903.446
[44,     1] loss: 918.403
[45,     1] loss: 932.514
[46,     1] loss: 898.828
[47,     1] loss: 866.387
[48,     1] loss: 865.115
[49,     1] loss: 826.364
[50,     1] loss: 811.481
[51,     1] loss: 853.808
[52,     1] loss: 860.764
[53,     1] loss: 874.166
[54,     1] loss: 833.616
[55,     1] loss: 863.265
[56,     1] loss: 760.125
[57,     1] loss: 823.865
[58,     1] loss: 832.660
[59,     1] loss: 783.132
[60,     1] loss: 790.423
[61,     1] loss: 741.342
[62,     1] loss: 833.956
[63,     1] loss: 862.081
[64,     1] loss: 755.965
[65,     1] loss: 814.089
[66,     1] loss: 747.338
[67,     1] loss: 806.167
[68,     1] loss: 770.654
[69,     1] loss: 822.592
[70,     1] loss: 729.270
[71,     1] loss: 816.937
[72,     1] loss: 694.972
[73,     1] loss: 757.151
[74,     1] loss: 703.261
[75,     1] loss: 782.186
[76,     1] loss: 724.351
[77,     1] loss: 757.396
[78,     1] loss: 661.295
[79,     1] loss: 648.206
[80,     1] loss: 655.642
[81,     1] loss: 679.197
[82,     1] loss: 635.971
[83,     1] loss: 624.098
[84,     1] loss: 615.788
[85,     1] loss: 574.827
[86,     1] loss: 601.540
[87,     1] loss: 647.940
[88,     1] loss: 560.641
[89,     1] loss: 582.794
[90,     1] loss: 529.039
[91,     1] loss: 618.348
[92,     1] loss: 576.948
[93,     1] loss: 578.874
[94,     1] loss: 559.448
[95,     1] loss: 639.286
[96,     1] loss: 551.477
[97,     1] loss: 538.556
[98,     1] loss: 530.722
[99,     1] loss: 531.808
[100,     1] loss: 526.282
[101,     1] loss: 498.962
[102,     1] loss: 564.494
[103,     1] loss: 618.816
[104,     1] loss: 549.448
[105,     1] loss: 472.802
[106,     1] loss: 549.224
[107,     1] loss: 528.760
[108,     1] loss: 533.689
[109,     1] loss: 586.966
[110,     1] loss: 481.136
[111,     1] loss: 494.048
[112,     1] loss: 469.228
[113,     1] loss: 467.091
[114,     1] loss: 485.147
[115,     1] loss: 530.204
[116,     1] loss: 471.614
[117,     1] loss: 507.393
[118,     1] loss: 450.846
[119,     1] loss: 489.643
[120,     1] loss: 431.771
[121,     1] loss: 471.605
[122,     1] loss: 471.469
[123,     1] loss: 525.341
[124,     1] loss: 510.698
[125,     1] loss: 473.655
[126,     1] loss: 423.379
[127,     1] loss: 507.265
[128,     1] loss: 442.613
[129,     1] loss: 461.960
[130,     1] loss: 426.999
Early stopping applied (best metric=0.2736532688140869)
Finished Training
Total time taken: 18.81200933456421
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1236.672
[2,     1] loss: 1231.414
[3,     1] loss: 1230.520
[4,     1] loss: 1239.759
[5,     1] loss: 1231.702
[6,     1] loss: 1234.238
[7,     1] loss: 1232.318
[8,     1] loss: 1228.682
[9,     1] loss: 1224.903
[10,     1] loss: 1220.065
[11,     1] loss: 1209.220
[12,     1] loss: 1197.196
[13,     1] loss: 1178.444
[14,     1] loss: 1141.366
[15,     1] loss: 1117.883
[16,     1] loss: 1063.966
[17,     1] loss: 1058.230
[18,     1] loss: 991.890
[19,     1] loss: 1032.203
[20,     1] loss: 983.407
[21,     1] loss: 1017.142
[22,     1] loss: 1002.065
[23,     1] loss: 989.747
[24,     1] loss: 987.968
[25,     1] loss: 956.435
[26,     1] loss: 1000.077
[27,     1] loss: 969.049
[28,     1] loss: 991.028
[29,     1] loss: 940.768
[30,     1] loss: 933.735
[31,     1] loss: 918.472
[32,     1] loss: 941.125
[33,     1] loss: 972.460
[34,     1] loss: 951.270
[35,     1] loss: 906.887
[36,     1] loss: 948.542
[37,     1] loss: 877.404
[38,     1] loss: 902.721
[39,     1] loss: 898.399
[40,     1] loss: 848.565
[41,     1] loss: 855.355
[42,     1] loss: 875.934
[43,     1] loss: 918.693
[44,     1] loss: 903.783
[45,     1] loss: 857.676
[46,     1] loss: 838.864
[47,     1] loss: 892.500
[48,     1] loss: 890.616
[49,     1] loss: 865.809
[50,     1] loss: 860.410
[51,     1] loss: 852.626
[52,     1] loss: 865.411
[53,     1] loss: 838.057
[54,     1] loss: 878.118
[55,     1] loss: 849.955
[56,     1] loss: 831.322
[57,     1] loss: 836.309
[58,     1] loss: 853.627
[59,     1] loss: 865.690
[60,     1] loss: 800.163
[61,     1] loss: 830.107
[62,     1] loss: 792.784
[63,     1] loss: 823.640
[64,     1] loss: 762.009
[65,     1] loss: 772.787
[66,     1] loss: 766.786
[67,     1] loss: 809.735
[68,     1] loss: 806.674
[69,     1] loss: 754.832
[70,     1] loss: 816.365
[71,     1] loss: 763.035
[72,     1] loss: 822.722
[73,     1] loss: 755.374
[74,     1] loss: 738.794
[75,     1] loss: 751.014
[76,     1] loss: 784.467
[77,     1] loss: 681.465
[78,     1] loss: 769.050
[79,     1] loss: 739.490
[80,     1] loss: 694.762
[81,     1] loss: 665.851
[82,     1] loss: 699.990
[83,     1] loss: 677.895
[84,     1] loss: 663.060
[85,     1] loss: 663.305
[86,     1] loss: 667.628
[87,     1] loss: 655.290
[88,     1] loss: 628.731
[89,     1] loss: 640.937
[90,     1] loss: 624.601
[91,     1] loss: 635.060
[92,     1] loss: 586.317
[93,     1] loss: 620.234
[94,     1] loss: 650.804
Early stopping applied (best metric=0.4139865040779114)
Finished Training
Total time taken: 13.40743088722229
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1231.675
[2,     1] loss: 1232.299
[3,     1] loss: 1231.821
[4,     1] loss: 1231.307
[5,     1] loss: 1229.333
[6,     1] loss: 1222.460
[7,     1] loss: 1212.619
[8,     1] loss: 1184.041
[9,     1] loss: 1152.925
[10,     1] loss: 1105.049
[11,     1] loss: 1072.432
[12,     1] loss: 1055.063
[13,     1] loss: 1028.803
[14,     1] loss: 1057.060
[15,     1] loss: 1036.664
[16,     1] loss: 1048.838
[17,     1] loss: 1028.149
[18,     1] loss: 1012.842
[19,     1] loss: 963.921
[20,     1] loss: 996.200
[21,     1] loss: 985.658
[22,     1] loss: 1002.161
[23,     1] loss: 996.195
[24,     1] loss: 951.958
[25,     1] loss: 923.915
[26,     1] loss: 950.996
[27,     1] loss: 951.557
[28,     1] loss: 901.810
[29,     1] loss: 931.879
[30,     1] loss: 890.057
[31,     1] loss: 873.703
[32,     1] loss: 931.682
[33,     1] loss: 925.532
[34,     1] loss: 853.930
[35,     1] loss: 885.614
[36,     1] loss: 864.422
[37,     1] loss: 856.231
[38,     1] loss: 835.866
[39,     1] loss: 855.532
[40,     1] loss: 846.446
[41,     1] loss: 853.021
[42,     1] loss: 855.752
[43,     1] loss: 817.460
[44,     1] loss: 812.715
[45,     1] loss: 827.139
[46,     1] loss: 823.335
[47,     1] loss: 799.442
[48,     1] loss: 759.406
[49,     1] loss: 804.591
[50,     1] loss: 773.640
[51,     1] loss: 737.001
[52,     1] loss: 768.684
[53,     1] loss: 694.031
[54,     1] loss: 790.294
[55,     1] loss: 687.748
[56,     1] loss: 720.131
[57,     1] loss: 693.932
[58,     1] loss: 699.806
[59,     1] loss: 644.689
[60,     1] loss: 711.995
[61,     1] loss: 663.418
[62,     1] loss: 717.719
[63,     1] loss: 681.232
[64,     1] loss: 683.580
[65,     1] loss: 684.202
[66,     1] loss: 682.209
[67,     1] loss: 721.966
[68,     1] loss: 678.875
[69,     1] loss: 649.398
[70,     1] loss: 604.528
[71,     1] loss: 631.493
[72,     1] loss: 605.096
Early stopping applied (best metric=0.36982110142707825)
Finished Training
Total time taken: 10.374109506607056
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.003
[2,     1] loss: 1239.533
[3,     1] loss: 1240.919
[4,     1] loss: 1233.733
[5,     1] loss: 1238.674
[6,     1] loss: 1230.243
[7,     1] loss: 1229.969
[8,     1] loss: 1231.265
[9,     1] loss: 1230.413
[10,     1] loss: 1226.058
[11,     1] loss: 1224.926
[12,     1] loss: 1216.309
[13,     1] loss: 1201.661
[14,     1] loss: 1191.342
[15,     1] loss: 1166.496
[16,     1] loss: 1154.093
[17,     1] loss: 1089.130
[18,     1] loss: 1073.629
[19,     1] loss: 1014.858
[20,     1] loss: 1030.769
[21,     1] loss: 1003.095
[22,     1] loss: 1054.095
[23,     1] loss: 1031.176
[24,     1] loss: 988.442
[25,     1] loss: 997.157
[26,     1] loss: 978.690
[27,     1] loss: 983.653
[28,     1] loss: 1015.285
[29,     1] loss: 987.152
[30,     1] loss: 970.846
[31,     1] loss: 980.377
[32,     1] loss: 956.871
[33,     1] loss: 958.868
[34,     1] loss: 960.275
[35,     1] loss: 906.103
[36,     1] loss: 908.479
[37,     1] loss: 943.195
[38,     1] loss: 912.916
[39,     1] loss: 899.298
[40,     1] loss: 911.648
[41,     1] loss: 883.065
[42,     1] loss: 900.493
[43,     1] loss: 898.460
[44,     1] loss: 938.146
[45,     1] loss: 849.181
[46,     1] loss: 874.473
[47,     1] loss: 839.953
[48,     1] loss: 847.746
[49,     1] loss: 829.028
[50,     1] loss: 820.658
[51,     1] loss: 804.098
[52,     1] loss: 835.691
[53,     1] loss: 777.874
[54,     1] loss: 796.232
[55,     1] loss: 778.722
[56,     1] loss: 747.941
[57,     1] loss: 756.861
[58,     1] loss: 796.641
[59,     1] loss: 751.660
[60,     1] loss: 822.577
[61,     1] loss: 704.337
[62,     1] loss: 690.552
[63,     1] loss: 749.679
[64,     1] loss: 707.453
[65,     1] loss: 745.558
[66,     1] loss: 714.266
[67,     1] loss: 740.595
[68,     1] loss: 707.431
[69,     1] loss: 652.039
[70,     1] loss: 684.536
[71,     1] loss: 686.209
[72,     1] loss: 632.210
[73,     1] loss: 624.940
[74,     1] loss: 604.544
[75,     1] loss: 605.048
[76,     1] loss: 608.589
[77,     1] loss: 671.798
[78,     1] loss: 604.230
[79,     1] loss: 663.662
[80,     1] loss: 727.318
[81,     1] loss: 568.027
[82,     1] loss: 612.985
[83,     1] loss: 612.662
Early stopping applied (best metric=0.41654014587402344)
Finished Training
Total time taken: 12.046286582946777
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1240.992
[2,     1] loss: 1237.589
[3,     1] loss: 1238.308
[4,     1] loss: 1236.528
[5,     1] loss: 1235.070
[6,     1] loss: 1234.430
[7,     1] loss: 1234.623
[8,     1] loss: 1233.702
[9,     1] loss: 1235.202
[10,     1] loss: 1229.879
[11,     1] loss: 1229.390
[12,     1] loss: 1226.383
[13,     1] loss: 1229.170
[14,     1] loss: 1224.200
[15,     1] loss: 1214.743
[16,     1] loss: 1210.365
[17,     1] loss: 1190.764
[18,     1] loss: 1175.075
[19,     1] loss: 1141.578
[20,     1] loss: 1105.559
[21,     1] loss: 1065.751
[22,     1] loss: 1057.987
[23,     1] loss: 1068.755
[24,     1] loss: 1049.710
[25,     1] loss: 1023.739
[26,     1] loss: 1034.687
[27,     1] loss: 983.933
[28,     1] loss: 961.510
[29,     1] loss: 1007.447
[30,     1] loss: 958.162
[31,     1] loss: 996.784
[32,     1] loss: 965.229
[33,     1] loss: 950.698
[34,     1] loss: 962.902
[35,     1] loss: 983.198
[36,     1] loss: 960.938
[37,     1] loss: 936.625
[38,     1] loss: 955.740
[39,     1] loss: 944.479
[40,     1] loss: 914.037
[41,     1] loss: 904.912
[42,     1] loss: 921.644
[43,     1] loss: 924.410
[44,     1] loss: 917.369
[45,     1] loss: 901.380
[46,     1] loss: 886.014
[47,     1] loss: 912.143
[48,     1] loss: 900.412
[49,     1] loss: 865.376
[50,     1] loss: 876.683
[51,     1] loss: 880.899
[52,     1] loss: 878.022
[53,     1] loss: 855.445
[54,     1] loss: 827.202
[55,     1] loss: 837.209
[56,     1] loss: 843.040
[57,     1] loss: 797.963
[58,     1] loss: 778.150
[59,     1] loss: 768.280
[60,     1] loss: 797.510
[61,     1] loss: 793.482
[62,     1] loss: 850.781
[63,     1] loss: 820.199
[64,     1] loss: 802.588
[65,     1] loss: 781.489
[66,     1] loss: 744.102
[67,     1] loss: 735.673
[68,     1] loss: 742.143
[69,     1] loss: 735.223
[70,     1] loss: 769.595
[71,     1] loss: 701.675
[72,     1] loss: 679.010
[73,     1] loss: 728.832
[74,     1] loss: 669.844
[75,     1] loss: 705.803
[76,     1] loss: 669.078
[77,     1] loss: 674.102
[78,     1] loss: 707.810
[79,     1] loss: 625.845
[80,     1] loss: 670.731
[81,     1] loss: 602.492
[82,     1] loss: 630.795
[83,     1] loss: 646.860
[84,     1] loss: 604.363
[85,     1] loss: 631.755
[86,     1] loss: 665.338
[87,     1] loss: 652.637
[88,     1] loss: 645.576
[89,     1] loss: 705.310
[90,     1] loss: 576.179
[91,     1] loss: 666.714
[92,     1] loss: 631.901
[93,     1] loss: 654.137
[94,     1] loss: 590.942
[95,     1] loss: 550.977
[96,     1] loss: 558.593
[97,     1] loss: 552.913
[98,     1] loss: 582.204
[99,     1] loss: 587.677
[100,     1] loss: 591.310
[101,     1] loss: 493.491
Early stopping applied (best metric=0.36118102073669434)
Finished Training
Total time taken: 14.511549234390259
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.053
[2,     1] loss: 1236.862
[3,     1] loss: 1233.535
[4,     1] loss: 1232.141
[5,     1] loss: 1232.873
[6,     1] loss: 1233.434
[7,     1] loss: 1232.069
[8,     1] loss: 1230.414
[9,     1] loss: 1227.424
[10,     1] loss: 1228.205
[11,     1] loss: 1224.177
[12,     1] loss: 1223.911
[13,     1] loss: 1218.786
[14,     1] loss: 1212.505
[15,     1] loss: 1205.124
[16,     1] loss: 1183.340
[17,     1] loss: 1162.921
[18,     1] loss: 1144.945
[19,     1] loss: 1127.529
[20,     1] loss: 1083.821
[21,     1] loss: 1064.442
[22,     1] loss: 1059.080
[23,     1] loss: 1050.365
[24,     1] loss: 1046.358
[25,     1] loss: 1030.643
[26,     1] loss: 1007.783
[27,     1] loss: 1005.968
[28,     1] loss: 984.184
[29,     1] loss: 978.914
[30,     1] loss: 979.294
[31,     1] loss: 954.792
[32,     1] loss: 956.427
[33,     1] loss: 962.482
[34,     1] loss: 963.295
[35,     1] loss: 984.133
[36,     1] loss: 918.265
[37,     1] loss: 932.002
[38,     1] loss: 910.268
[39,     1] loss: 955.097
[40,     1] loss: 941.463
[41,     1] loss: 923.546
[42,     1] loss: 923.290
[43,     1] loss: 894.746
[44,     1] loss: 848.230
[45,     1] loss: 880.084
[46,     1] loss: 870.191
[47,     1] loss: 860.563
[48,     1] loss: 866.291
[49,     1] loss: 883.222
[50,     1] loss: 835.976
[51,     1] loss: 871.527
[52,     1] loss: 852.977
[53,     1] loss: 815.420
[54,     1] loss: 871.920
[55,     1] loss: 822.517
[56,     1] loss: 854.098
[57,     1] loss: 808.416
[58,     1] loss: 822.343
[59,     1] loss: 834.106
[60,     1] loss: 830.825
[61,     1] loss: 802.952
[62,     1] loss: 771.519
[63,     1] loss: 797.875
[64,     1] loss: 752.764
[65,     1] loss: 776.353
[66,     1] loss: 749.329
[67,     1] loss: 855.443
[68,     1] loss: 729.958
[69,     1] loss: 770.639
[70,     1] loss: 771.506
[71,     1] loss: 754.833
[72,     1] loss: 672.890
[73,     1] loss: 727.173
[74,     1] loss: 684.750
[75,     1] loss: 660.825
[76,     1] loss: 717.470
[77,     1] loss: 732.395
[78,     1] loss: 704.050
[79,     1] loss: 691.047
[80,     1] loss: 685.099
[81,     1] loss: 714.110
[82,     1] loss: 664.777
[83,     1] loss: 683.073
[84,     1] loss: 656.529
[85,     1] loss: 655.310
[86,     1] loss: 684.781
[87,     1] loss: 608.966
Early stopping applied (best metric=0.3758864998817444)
Finished Training
Total time taken: 12.522342205047607
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.925
[2,     1] loss: 1232.800
[3,     1] loss: 1231.115
[4,     1] loss: 1237.079
[5,     1] loss: 1231.491
[6,     1] loss: 1229.043
[7,     1] loss: 1230.172
[8,     1] loss: 1226.701
[9,     1] loss: 1219.169
[10,     1] loss: 1212.048
[11,     1] loss: 1195.342
[12,     1] loss: 1163.576
[13,     1] loss: 1133.935
[14,     1] loss: 1116.205
[15,     1] loss: 1069.911
[16,     1] loss: 997.245
[17,     1] loss: 1030.840
[18,     1] loss: 1025.226
[19,     1] loss: 1039.089
[20,     1] loss: 1025.935
[21,     1] loss: 1019.635
[22,     1] loss: 985.093
[23,     1] loss: 969.816
[24,     1] loss: 983.823
[25,     1] loss: 989.826
[26,     1] loss: 930.378
[27,     1] loss: 936.557
[28,     1] loss: 907.620
[29,     1] loss: 926.414
[30,     1] loss: 967.139
[31,     1] loss: 924.262
[32,     1] loss: 941.380
[33,     1] loss: 929.063
[34,     1] loss: 919.881
[35,     1] loss: 909.901
[36,     1] loss: 894.747
[37,     1] loss: 898.535
[38,     1] loss: 874.883
[39,     1] loss: 864.705
[40,     1] loss: 856.955
[41,     1] loss: 832.708
[42,     1] loss: 852.075
[43,     1] loss: 838.067
[44,     1] loss: 840.704
[45,     1] loss: 867.205
[46,     1] loss: 848.655
[47,     1] loss: 818.435
[48,     1] loss: 774.616
[49,     1] loss: 814.066
[50,     1] loss: 777.095
[51,     1] loss: 804.000
[52,     1] loss: 825.831
[53,     1] loss: 779.962
[54,     1] loss: 746.198
[55,     1] loss: 770.629
[56,     1] loss: 767.038
[57,     1] loss: 790.406
[58,     1] loss: 800.987
[59,     1] loss: 776.232
[60,     1] loss: 787.849
[61,     1] loss: 710.136
[62,     1] loss: 747.362
[63,     1] loss: 686.278
[64,     1] loss: 761.789
[65,     1] loss: 696.383
[66,     1] loss: 726.020
[67,     1] loss: 693.508
[68,     1] loss: 698.929
[69,     1] loss: 704.402
[70,     1] loss: 699.356
[71,     1] loss: 672.713
[72,     1] loss: 685.062
[73,     1] loss: 662.298
[74,     1] loss: 700.195
[75,     1] loss: 705.977
[76,     1] loss: 705.257
[77,     1] loss: 639.431
[78,     1] loss: 686.263
Early stopping applied (best metric=0.3926560878753662)
Finished Training
Total time taken: 11.299205303192139
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.945
[2,     1] loss: 1234.840
[3,     1] loss: 1230.518
[4,     1] loss: 1231.217
[5,     1] loss: 1221.299
[6,     1] loss: 1206.962
[7,     1] loss: 1195.135
[8,     1] loss: 1159.751
[9,     1] loss: 1124.489
[10,     1] loss: 1079.673
[11,     1] loss: 1076.517
[12,     1] loss: 1077.605
[13,     1] loss: 1057.945
[14,     1] loss: 1009.869
[15,     1] loss: 1018.961
[16,     1] loss: 1014.683
[17,     1] loss: 1018.865
[18,     1] loss: 987.404
[19,     1] loss: 979.600
[20,     1] loss: 966.443
[21,     1] loss: 997.327
[22,     1] loss: 969.080
[23,     1] loss: 941.283
[24,     1] loss: 950.996
[25,     1] loss: 964.079
[26,     1] loss: 982.376
[27,     1] loss: 972.962
[28,     1] loss: 934.723
[29,     1] loss: 922.047
[30,     1] loss: 958.114
[31,     1] loss: 926.670
[32,     1] loss: 968.369
[33,     1] loss: 923.132
[34,     1] loss: 930.564
[35,     1] loss: 888.275
[36,     1] loss: 894.204
[37,     1] loss: 893.211
[38,     1] loss: 901.907
[39,     1] loss: 909.941
[40,     1] loss: 936.016
[41,     1] loss: 850.303
[42,     1] loss: 824.594
[43,     1] loss: 846.632
[44,     1] loss: 882.579
[45,     1] loss: 909.281
[46,     1] loss: 873.047
[47,     1] loss: 846.725
[48,     1] loss: 794.116
[49,     1] loss: 824.588
[50,     1] loss: 843.038
[51,     1] loss: 880.103
[52,     1] loss: 799.607
[53,     1] loss: 852.013
[54,     1] loss: 772.927
[55,     1] loss: 813.723
[56,     1] loss: 746.825
[57,     1] loss: 886.653
[58,     1] loss: 753.117
[59,     1] loss: 841.119
[60,     1] loss: 755.507
[61,     1] loss: 821.414
[62,     1] loss: 769.805
[63,     1] loss: 777.970
[64,     1] loss: 791.768
[65,     1] loss: 713.258
[66,     1] loss: 748.780
[67,     1] loss: 674.140
[68,     1] loss: 709.555
[69,     1] loss: 713.815
[70,     1] loss: 740.034
[71,     1] loss: 654.162
[72,     1] loss: 678.958
[73,     1] loss: 686.864
[74,     1] loss: 657.476
[75,     1] loss: 662.837
[76,     1] loss: 633.018
[77,     1] loss: 634.013
[78,     1] loss: 618.252
[79,     1] loss: 596.506
[80,     1] loss: 625.687
[81,     1] loss: 611.143
[82,     1] loss: 578.333
[83,     1] loss: 601.918
[84,     1] loss: 572.137
[85,     1] loss: 573.064
[86,     1] loss: 564.630
[87,     1] loss: 571.381
[88,     1] loss: 585.320
[89,     1] loss: 568.250
[90,     1] loss: 524.554
[91,     1] loss: 623.441
[92,     1] loss: 565.989
[93,     1] loss: 510.984
[94,     1] loss: 610.608
[95,     1] loss: 524.174
[96,     1] loss: 491.656
[97,     1] loss: 510.536
[98,     1] loss: 483.500
[99,     1] loss: 500.405
[100,     1] loss: 536.482
Early stopping applied (best metric=0.3543527126312256)
Finished Training
Total time taken: 14.412540435791016
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.982
[2,     1] loss: 1233.840
[3,     1] loss: 1229.874
[4,     1] loss: 1233.660
[5,     1] loss: 1228.880
[6,     1] loss: 1226.417
[7,     1] loss: 1216.223
[8,     1] loss: 1203.614
[9,     1] loss: 1179.493
[10,     1] loss: 1134.379
[11,     1] loss: 1119.448
[12,     1] loss: 1081.893
[13,     1] loss: 1100.812
[14,     1] loss: 1045.591
[15,     1] loss: 980.923
[16,     1] loss: 1088.232
[17,     1] loss: 1023.996
[18,     1] loss: 1062.622
[19,     1] loss: 1045.419
[20,     1] loss: 1000.156
[21,     1] loss: 1022.670
[22,     1] loss: 994.054
[23,     1] loss: 989.654
[24,     1] loss: 963.647
[25,     1] loss: 996.818
[26,     1] loss: 985.925
[27,     1] loss: 937.976
[28,     1] loss: 986.306
[29,     1] loss: 913.645
[30,     1] loss: 950.590
[31,     1] loss: 916.225
[32,     1] loss: 896.245
[33,     1] loss: 895.741
[34,     1] loss: 935.569
[35,     1] loss: 897.561
[36,     1] loss: 900.211
[37,     1] loss: 926.450
[38,     1] loss: 894.960
[39,     1] loss: 868.109
[40,     1] loss: 877.180
[41,     1] loss: 865.273
[42,     1] loss: 875.842
[43,     1] loss: 847.307
[44,     1] loss: 881.786
[45,     1] loss: 862.019
[46,     1] loss: 894.398
[47,     1] loss: 866.698
[48,     1] loss: 870.176
[49,     1] loss: 813.294
[50,     1] loss: 821.405
[51,     1] loss: 810.806
[52,     1] loss: 845.620
[53,     1] loss: 790.239
[54,     1] loss: 778.954
[55,     1] loss: 805.066
[56,     1] loss: 743.580
[57,     1] loss: 767.298
[58,     1] loss: 785.897
[59,     1] loss: 777.064
[60,     1] loss: 721.384
[61,     1] loss: 781.249
[62,     1] loss: 689.409
[63,     1] loss: 774.814
[64,     1] loss: 687.962
[65,     1] loss: 790.102
[66,     1] loss: 699.962
[67,     1] loss: 751.314
[68,     1] loss: 681.764
[69,     1] loss: 738.647
[70,     1] loss: 680.971
[71,     1] loss: 680.271
[72,     1] loss: 629.776
[73,     1] loss: 609.806
[74,     1] loss: 626.142
[75,     1] loss: 599.713
[76,     1] loss: 565.461
[77,     1] loss: 551.079
[78,     1] loss: 593.397
[79,     1] loss: 632.722
[80,     1] loss: 580.124
[81,     1] loss: 587.083
[82,     1] loss: 555.872
[83,     1] loss: 577.504
[84,     1] loss: 564.154
[85,     1] loss: 606.991
[86,     1] loss: 563.149
[87,     1] loss: 566.542
[88,     1] loss: 540.680
[89,     1] loss: 556.180
[90,     1] loss: 552.763
[91,     1] loss: 580.295
[92,     1] loss: 508.818
[93,     1] loss: 527.428
[94,     1] loss: 502.469
[95,     1] loss: 474.590
[96,     1] loss: 524.486
[97,     1] loss: 533.177
[98,     1] loss: 486.691
[99,     1] loss: 508.755
[100,     1] loss: 473.908
[101,     1] loss: 510.624
[102,     1] loss: 493.822
Early stopping applied (best metric=0.3597973585128784)
Finished Training
Total time taken: 14.668568134307861
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1234.536
[2,     1] loss: 1242.060
[3,     1] loss: 1233.531
[4,     1] loss: 1232.970
[5,     1] loss: 1232.573
[6,     1] loss: 1225.858
[7,     1] loss: 1235.576
[8,     1] loss: 1230.908
[9,     1] loss: 1215.428
[10,     1] loss: 1217.689
[11,     1] loss: 1206.473
[12,     1] loss: 1184.887
[13,     1] loss: 1171.070
[14,     1] loss: 1128.708
[15,     1] loss: 1073.378
[16,     1] loss: 1072.833
[17,     1] loss: 1023.818
[18,     1] loss: 1091.829
[19,     1] loss: 1008.660
[20,     1] loss: 1012.853
[21,     1] loss: 982.183
[22,     1] loss: 987.583
[23,     1] loss: 1015.567
[24,     1] loss: 1022.437
[25,     1] loss: 990.972
[26,     1] loss: 1006.663
[27,     1] loss: 1000.617
[28,     1] loss: 980.363
[29,     1] loss: 950.789
[30,     1] loss: 968.134
[31,     1] loss: 963.572
[32,     1] loss: 959.462
[33,     1] loss: 973.604
[34,     1] loss: 970.391
[35,     1] loss: 944.398
[36,     1] loss: 976.595
[37,     1] loss: 957.095
[38,     1] loss: 991.138
[39,     1] loss: 941.304
[40,     1] loss: 925.627
[41,     1] loss: 917.266
[42,     1] loss: 948.342
[43,     1] loss: 886.969
[44,     1] loss: 897.429
[45,     1] loss: 885.175
[46,     1] loss: 900.731
[47,     1] loss: 922.348
[48,     1] loss: 902.056
[49,     1] loss: 856.630
[50,     1] loss: 856.368
[51,     1] loss: 888.063
[52,     1] loss: 848.183
[53,     1] loss: 844.309
[54,     1] loss: 870.707
[55,     1] loss: 900.776
[56,     1] loss: 805.837
[57,     1] loss: 824.493
[58,     1] loss: 799.715
[59,     1] loss: 817.855
[60,     1] loss: 809.908
[61,     1] loss: 784.228
[62,     1] loss: 759.783
[63,     1] loss: 796.763
[64,     1] loss: 759.248
[65,     1] loss: 780.853
[66,     1] loss: 803.004
[67,     1] loss: 760.641
[68,     1] loss: 815.189
[69,     1] loss: 785.576
[70,     1] loss: 726.268
[71,     1] loss: 737.267
[72,     1] loss: 723.394
[73,     1] loss: 789.379
[74,     1] loss: 785.714
[75,     1] loss: 803.385
[76,     1] loss: 756.859
[77,     1] loss: 788.718
[78,     1] loss: 705.408
[79,     1] loss: 738.337
[80,     1] loss: 668.860
[81,     1] loss: 754.228
[82,     1] loss: 665.307
[83,     1] loss: 767.755
[84,     1] loss: 657.047
[85,     1] loss: 620.601
[86,     1] loss: 627.218
[87,     1] loss: 624.281
[88,     1] loss: 616.820
[89,     1] loss: 634.525
[90,     1] loss: 587.958
[91,     1] loss: 633.856
[92,     1] loss: 598.864
[93,     1] loss: 594.818
[94,     1] loss: 574.751
[95,     1] loss: 588.180
[96,     1] loss: 563.596
[97,     1] loss: 537.218
[98,     1] loss: 576.296
[99,     1] loss: 536.498
[100,     1] loss: 507.367
[101,     1] loss: 502.169
[102,     1] loss: 513.434
[103,     1] loss: 465.356
[104,     1] loss: 527.987
[105,     1] loss: 488.054
[106,     1] loss: 496.673
[107,     1] loss: 492.658
[108,     1] loss: 502.223
[109,     1] loss: 485.405
Early stopping applied (best metric=0.3693554103374481)
Finished Training
Total time taken: 15.65067434310913
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1232.743
[2,     1] loss: 1245.992
[3,     1] loss: 1234.770
[4,     1] loss: 1230.724
[5,     1] loss: 1232.306
[6,     1] loss: 1232.394
[7,     1] loss: 1233.368
[8,     1] loss: 1229.955
[9,     1] loss: 1221.982
[10,     1] loss: 1223.951
[11,     1] loss: 1221.267
[12,     1] loss: 1211.843
[13,     1] loss: 1206.236
[14,     1] loss: 1176.070
[15,     1] loss: 1142.760
[16,     1] loss: 1113.538
[17,     1] loss: 1063.934
[18,     1] loss: 1064.145
[19,     1] loss: 1018.532
[20,     1] loss: 1005.510
[21,     1] loss: 1025.543
[22,     1] loss: 980.252
[23,     1] loss: 1002.403
[24,     1] loss: 991.995
[25,     1] loss: 971.152
[26,     1] loss: 1013.480
[27,     1] loss: 993.666
[28,     1] loss: 965.595
[29,     1] loss: 1013.533
[30,     1] loss: 943.186
[31,     1] loss: 965.194
[32,     1] loss: 925.234
[33,     1] loss: 969.383
[34,     1] loss: 938.788
[35,     1] loss: 928.151
[36,     1] loss: 949.323
[37,     1] loss: 953.847
[38,     1] loss: 906.705
[39,     1] loss: 914.917
[40,     1] loss: 900.917
[41,     1] loss: 884.266
[42,     1] loss: 868.769
[43,     1] loss: 899.646
[44,     1] loss: 917.988
[45,     1] loss: 861.894
[46,     1] loss: 836.016
[47,     1] loss: 892.775
[48,     1] loss: 902.641
[49,     1] loss: 872.576
[50,     1] loss: 827.290
[51,     1] loss: 832.811
[52,     1] loss: 834.978
[53,     1] loss: 793.628
[54,     1] loss: 778.665
[55,     1] loss: 846.852
[56,     1] loss: 830.364
[57,     1] loss: 804.594
[58,     1] loss: 756.083
[59,     1] loss: 821.014
[60,     1] loss: 781.047
[61,     1] loss: 722.041
[62,     1] loss: 760.267
[63,     1] loss: 779.897
[64,     1] loss: 762.306
[65,     1] loss: 724.686
[66,     1] loss: 724.199
[67,     1] loss: 771.490
[68,     1] loss: 714.659
[69,     1] loss: 717.587
[70,     1] loss: 805.873
[71,     1] loss: 754.840
[72,     1] loss: 754.071
[73,     1] loss: 719.227
[74,     1] loss: 778.644
[75,     1] loss: 706.794
[76,     1] loss: 649.002
[77,     1] loss: 741.693
[78,     1] loss: 652.357
[79,     1] loss: 699.878
[80,     1] loss: 614.208
[81,     1] loss: 671.911
[82,     1] loss: 674.944
[83,     1] loss: 664.761
[84,     1] loss: 656.281
[85,     1] loss: 616.454
[86,     1] loss: 671.136
[87,     1] loss: 581.710
[88,     1] loss: 552.936
[89,     1] loss: 574.788
[90,     1] loss: 601.698
[91,     1] loss: 512.864
[92,     1] loss: 622.432
[93,     1] loss: 552.479
[94,     1] loss: 562.231
Early stopping applied (best metric=0.4281722903251648)
Finished Training
Total time taken: 13.471441984176636
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.737
[2,     1] loss: 1233.964
[3,     1] loss: 1232.436
[4,     1] loss: 1225.508
[5,     1] loss: 1221.775
[6,     1] loss: 1210.678
[7,     1] loss: 1184.447
[8,     1] loss: 1155.826
[9,     1] loss: 1110.712
[10,     1] loss: 1054.604
[11,     1] loss: 1055.458
[12,     1] loss: 1064.619
[13,     1] loss: 1018.401
[14,     1] loss: 1030.461
[15,     1] loss: 1025.010
[16,     1] loss: 1028.271
[17,     1] loss: 999.250
[18,     1] loss: 1012.035
[19,     1] loss: 1012.796
[20,     1] loss: 1003.776
[21,     1] loss: 950.692
[22,     1] loss: 952.800
[23,     1] loss: 937.603
[24,     1] loss: 984.079
[25,     1] loss: 959.737
[26,     1] loss: 931.557
[27,     1] loss: 963.981
[28,     1] loss: 979.612
[29,     1] loss: 938.265
[30,     1] loss: 936.716
[31,     1] loss: 935.615
[32,     1] loss: 919.427
[33,     1] loss: 954.579
[34,     1] loss: 879.818
[35,     1] loss: 904.000
[36,     1] loss: 870.169
[37,     1] loss: 936.781
[38,     1] loss: 954.507
[39,     1] loss: 930.501
[40,     1] loss: 936.243
[41,     1] loss: 880.732
[42,     1] loss: 895.822
[43,     1] loss: 886.423
[44,     1] loss: 857.392
[45,     1] loss: 856.884
[46,     1] loss: 817.572
[47,     1] loss: 867.628
[48,     1] loss: 831.648
[49,     1] loss: 808.512
[50,     1] loss: 821.468
[51,     1] loss: 798.213
[52,     1] loss: 745.851
[53,     1] loss: 782.802
[54,     1] loss: 777.340
[55,     1] loss: 747.204
[56,     1] loss: 772.853
[57,     1] loss: 811.107
[58,     1] loss: 798.989
[59,     1] loss: 713.813
[60,     1] loss: 753.980
[61,     1] loss: 707.376
[62,     1] loss: 748.180
[63,     1] loss: 727.494
[64,     1] loss: 715.475
[65,     1] loss: 751.104
[66,     1] loss: 638.996
[67,     1] loss: 651.374
[68,     1] loss: 681.225
[69,     1] loss: 641.611
[70,     1] loss: 672.537
[71,     1] loss: 642.985
[72,     1] loss: 667.979
[73,     1] loss: 709.413
[74,     1] loss: 651.794
[75,     1] loss: 583.212
[76,     1] loss: 609.205
[77,     1] loss: 593.278
[78,     1] loss: 615.228
[79,     1] loss: 606.322
[80,     1] loss: 625.079
[81,     1] loss: 604.253
[82,     1] loss: 677.816
[83,     1] loss: 616.534
[84,     1] loss: 569.269
[85,     1] loss: 633.773
[86,     1] loss: 567.802
[87,     1] loss: 638.907
[88,     1] loss: 558.734
[89,     1] loss: 521.519
[90,     1] loss: 559.972
[91,     1] loss: 612.844
[92,     1] loss: 502.296
[93,     1] loss: 534.701
[94,     1] loss: 531.430
[95,     1] loss: 501.273
[96,     1] loss: 528.606
[97,     1] loss: 553.852
[98,     1] loss: 515.502
[99,     1] loss: 500.553
[100,     1] loss: 439.215
Early stopping applied (best metric=0.2657143175601959)
Finished Training
Total time taken: 14.246520042419434
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1234.823
[2,     1] loss: 1240.013
[3,     1] loss: 1233.688
[4,     1] loss: 1235.733
[5,     1] loss: 1230.275
[6,     1] loss: 1233.738
[7,     1] loss: 1230.837
[8,     1] loss: 1228.084
[9,     1] loss: 1224.464
[10,     1] loss: 1223.057
[11,     1] loss: 1214.621
[12,     1] loss: 1211.086
[13,     1] loss: 1193.442
[14,     1] loss: 1181.252
[15,     1] loss: 1146.903
[16,     1] loss: 1112.584
[17,     1] loss: 1088.764
[18,     1] loss: 1046.938
[19,     1] loss: 1063.240
[20,     1] loss: 1026.180
[21,     1] loss: 1005.713
[22,     1] loss: 1032.612
[23,     1] loss: 975.474
[24,     1] loss: 1020.107
[25,     1] loss: 1011.960
[26,     1] loss: 988.388
[27,     1] loss: 994.626
[28,     1] loss: 981.397
[29,     1] loss: 968.068
[30,     1] loss: 1003.158
[31,     1] loss: 945.999
[32,     1] loss: 947.979
[33,     1] loss: 970.080
[34,     1] loss: 908.145
[35,     1] loss: 940.772
[36,     1] loss: 922.701
[37,     1] loss: 904.416
[38,     1] loss: 936.627
[39,     1] loss: 894.474
[40,     1] loss: 896.129
[41,     1] loss: 883.950
[42,     1] loss: 863.700
[43,     1] loss: 889.292
[44,     1] loss: 887.769
[45,     1] loss: 912.344
[46,     1] loss: 898.011
[47,     1] loss: 886.047
[48,     1] loss: 868.972
[49,     1] loss: 855.497
[50,     1] loss: 827.506
[51,     1] loss: 864.842
[52,     1] loss: 791.227
[53,     1] loss: 839.537
[54,     1] loss: 841.976
[55,     1] loss: 891.607
[56,     1] loss: 784.543
[57,     1] loss: 799.582
[58,     1] loss: 772.480
[59,     1] loss: 801.035
[60,     1] loss: 760.288
[61,     1] loss: 796.148
[62,     1] loss: 758.908
[63,     1] loss: 762.801
[64,     1] loss: 751.316
[65,     1] loss: 767.361
[66,     1] loss: 781.248
[67,     1] loss: 723.917
[68,     1] loss: 761.174
[69,     1] loss: 690.987
[70,     1] loss: 707.162
[71,     1] loss: 715.569
[72,     1] loss: 685.505
[73,     1] loss: 671.359
[74,     1] loss: 669.666
[75,     1] loss: 774.054
[76,     1] loss: 693.404
[77,     1] loss: 726.874
[78,     1] loss: 717.038
[79,     1] loss: 641.932
[80,     1] loss: 658.951
[81,     1] loss: 616.793
[82,     1] loss: 661.233
[83,     1] loss: 671.590
[84,     1] loss: 629.714
[85,     1] loss: 613.619
[86,     1] loss: 603.765
[87,     1] loss: 603.659
[88,     1] loss: 526.182
[89,     1] loss: 642.567
[90,     1] loss: 593.392
[91,     1] loss: 573.583
[92,     1] loss: 562.951
Early stopping applied (best metric=0.38468796014785767)
Finished Training
Total time taken: 13.230413675308228
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.206
[2,     1] loss: 1232.172
[3,     1] loss: 1233.745
[4,     1] loss: 1231.782
[5,     1] loss: 1232.094
[6,     1] loss: 1231.696
[7,     1] loss: 1234.286
[8,     1] loss: 1236.426
[9,     1] loss: 1235.257
[10,     1] loss: 1229.692
[11,     1] loss: 1236.965
[12,     1] loss: 1232.069
[13,     1] loss: 1227.557
[14,     1] loss: 1226.629
[15,     1] loss: 1225.322
[16,     1] loss: 1221.635
[17,     1] loss: 1211.121
[18,     1] loss: 1211.472
[19,     1] loss: 1201.494
[20,     1] loss: 1181.200
[21,     1] loss: 1159.852
[22,     1] loss: 1138.875
[23,     1] loss: 1113.606
[24,     1] loss: 1081.072
[25,     1] loss: 1056.576
[26,     1] loss: 1048.621
[27,     1] loss: 1069.368
[28,     1] loss: 1010.812
[29,     1] loss: 1018.220
[30,     1] loss: 1044.011
[31,     1] loss: 1010.486
[32,     1] loss: 1003.192
[33,     1] loss: 981.075
[34,     1] loss: 982.180
[35,     1] loss: 991.992
[36,     1] loss: 1013.281
[37,     1] loss: 950.348
[38,     1] loss: 939.216
[39,     1] loss: 977.419
[40,     1] loss: 946.609
[41,     1] loss: 1003.049
[42,     1] loss: 951.262
[43,     1] loss: 949.792
[44,     1] loss: 971.878
[45,     1] loss: 901.366
[46,     1] loss: 945.528
[47,     1] loss: 923.705
[48,     1] loss: 924.523
[49,     1] loss: 898.915
[50,     1] loss: 872.722
[51,     1] loss: 939.433
[52,     1] loss: 876.837
[53,     1] loss: 838.876
[54,     1] loss: 878.356
[55,     1] loss: 892.444
[56,     1] loss: 876.343
[57,     1] loss: 878.486
[58,     1] loss: 870.656
[59,     1] loss: 879.764
[60,     1] loss: 845.360
[61,     1] loss: 859.811
[62,     1] loss: 847.034
[63,     1] loss: 844.847
[64,     1] loss: 846.642
[65,     1] loss: 848.893
[66,     1] loss: 843.023
[67,     1] loss: 851.781
[68,     1] loss: 806.940
[69,     1] loss: 825.317
[70,     1] loss: 793.792
[71,     1] loss: 842.720
[72,     1] loss: 774.422
[73,     1] loss: 815.402
[74,     1] loss: 745.570
[75,     1] loss: 772.664
[76,     1] loss: 788.946
[77,     1] loss: 747.394
[78,     1] loss: 780.116
[79,     1] loss: 748.344
[80,     1] loss: 692.937
[81,     1] loss: 724.230
[82,     1] loss: 726.503
[83,     1] loss: 675.789
[84,     1] loss: 708.131
[85,     1] loss: 675.006
[86,     1] loss: 692.537
[87,     1] loss: 655.925
[88,     1] loss: 668.112
[89,     1] loss: 699.185
[90,     1] loss: 694.297
[91,     1] loss: 653.212
[92,     1] loss: 650.235
[93,     1] loss: 644.658
[94,     1] loss: 612.374
[95,     1] loss: 621.329
[96,     1] loss: 635.023
[97,     1] loss: 607.309
[98,     1] loss: 591.582
[99,     1] loss: 624.198
[100,     1] loss: 646.055
[101,     1] loss: 640.644
[102,     1] loss: 549.943
[103,     1] loss: 585.128
[104,     1] loss: 542.539
[105,     1] loss: 628.959
[106,     1] loss: 565.725
[107,     1] loss: 588.204
[108,     1] loss: 531.154
[109,     1] loss: 635.196
[110,     1] loss: 591.917
[111,     1] loss: 601.837
[112,     1] loss: 545.804
[113,     1] loss: 565.175
[114,     1] loss: 616.414
[115,     1] loss: 570.304
[116,     1] loss: 614.800
Early stopping applied (best metric=0.33658626675605774)
Finished Training
Total time taken: 16.645779371261597
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1234.398
[2,     1] loss: 1244.006
[3,     1] loss: 1233.771
[4,     1] loss: 1231.949
[5,     1] loss: 1227.958
[6,     1] loss: 1227.919
[7,     1] loss: 1219.832
[8,     1] loss: 1212.394
[9,     1] loss: 1204.492
[10,     1] loss: 1177.710
[11,     1] loss: 1161.484
[12,     1] loss: 1119.775
[13,     1] loss: 1084.939
[14,     1] loss: 1052.426
[15,     1] loss: 1074.126
[16,     1] loss: 1055.591
[17,     1] loss: 1012.407
[18,     1] loss: 1015.121
[19,     1] loss: 992.464
[20,     1] loss: 1004.931
[21,     1] loss: 990.045
[22,     1] loss: 999.202
[23,     1] loss: 985.417
[24,     1] loss: 961.888
[25,     1] loss: 957.541
[26,     1] loss: 980.384
[27,     1] loss: 978.344
[28,     1] loss: 900.288
[29,     1] loss: 912.001
[30,     1] loss: 899.756
[31,     1] loss: 971.543
[32,     1] loss: 957.646
[33,     1] loss: 918.561
[34,     1] loss: 916.197
[35,     1] loss: 934.932
[36,     1] loss: 866.424
[37,     1] loss: 898.031
[38,     1] loss: 895.672
[39,     1] loss: 883.299
[40,     1] loss: 884.035
[41,     1] loss: 892.257
[42,     1] loss: 856.849
[43,     1] loss: 849.970
[44,     1] loss: 859.972
[45,     1] loss: 819.891
[46,     1] loss: 893.245
[47,     1] loss: 844.365
[48,     1] loss: 819.625
[49,     1] loss: 849.773
[50,     1] loss: 869.701
[51,     1] loss: 845.504
[52,     1] loss: 818.783
[53,     1] loss: 830.113
[54,     1] loss: 797.371
[55,     1] loss: 816.803
[56,     1] loss: 773.256
[57,     1] loss: 762.665
[58,     1] loss: 812.618
[59,     1] loss: 839.483
[60,     1] loss: 794.334
[61,     1] loss: 744.037
[62,     1] loss: 731.756
[63,     1] loss: 721.838
[64,     1] loss: 797.534
[65,     1] loss: 700.399
[66,     1] loss: 724.467
[67,     1] loss: 721.887
[68,     1] loss: 792.539
[69,     1] loss: 732.640
[70,     1] loss: 743.398
[71,     1] loss: 684.229
[72,     1] loss: 709.584
[73,     1] loss: 667.168
[74,     1] loss: 649.367
[75,     1] loss: 696.928
[76,     1] loss: 596.006
[77,     1] loss: 688.089
[78,     1] loss: 662.918
[79,     1] loss: 586.498
[80,     1] loss: 648.670
[81,     1] loss: 542.470
[82,     1] loss: 648.504
[83,     1] loss: 610.783
[84,     1] loss: 600.233
[85,     1] loss: 680.050
[86,     1] loss: 588.992
[87,     1] loss: 643.184
[88,     1] loss: 577.002
[89,     1] loss: 596.264
[90,     1] loss: 580.116
[91,     1] loss: 575.023
[92,     1] loss: 539.977
Early stopping applied (best metric=0.355590283870697)
Finished Training
Total time taken: 13.229413986206055
{'Hydroxylation-K Validation Accuracy': 0.7552600472813239, 'Hydroxylation-K Validation Sensitivity': 0.6274074074074074, 'Hydroxylation-K Validation Specificity': 0.787719298245614, 'Hydroxylation-K Validation Precision': 0.44014753764753767, 'Hydroxylation-K AUC ROC': 0.788401559454191, 'Hydroxylation-K AUC PR': 0.5792365098204351, 'Hydroxylation-K MCC': 0.37097935473675775, 'Hydroxylation-K F1': 0.5117577298538397, 'Validation Loss (Hydroxylation-K)': 0.48632702231407166, 'Hydroxylation-P Validation Accuracy': 0.8016982048288581, 'Hydroxylation-P Validation Sensitivity': 0.7915343915343915, 'Hydroxylation-P Validation Specificity': 0.8038356027732056, 'Hydroxylation-P Validation Precision': 0.4675935625969019, 'Hydroxylation-P AUC ROC': 0.8515730646424198, 'Hydroxylation-P AUC PR': 0.5881230716518899, 'Hydroxylation-P MCC': 0.4968350477114252, 'Hydroxylation-P F1': 0.5867878769751004, 'Validation Loss (Hydroxylation-P)': 0.3638654152552287, 'Validation Loss (total)': 0.8501924316088358, 'TimeToTrain': 13.901885668436686}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007397154931534703,
 'learning_rate_Hydroxylation-K': 0.004427828199366015,
 'learning_rate_Hydroxylation-P': 0.009566591273760042,
 'log_base': 2.794894475068008,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1501368679,
 'sample_weights': [1.5391828123365758, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.3679492084551972,
 'weight_decay_Hydroxylation-K': 5.764316066444175,
 'weight_decay_Hydroxylation-P': 3.53622099253696}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.010
[2,     1] loss: 1250.434
[3,     1] loss: 1256.015
[4,     1] loss: 1254.838
[5,     1] loss: 1255.458
[6,     1] loss: 1251.885
[7,     1] loss: 1250.194
[8,     1] loss: 1250.990
[9,     1] loss: 1246.856
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009253640997220704,
 'learning_rate_Hydroxylation-K': 0.0017888121311825243,
 'learning_rate_Hydroxylation-P': 0.006018457064456273,
 'log_base': 2.338205944050899,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3878179446,
 'sample_weights': [1.624296870089564, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.771809995912879,
 'weight_decay_Hydroxylation-K': 1.6153896075492509,
 'weight_decay_Hydroxylation-P': 3.669426741002411}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1323.927
[2,     1] loss: 1330.506
[3,     1] loss: 1327.922
[4,     1] loss: 1321.414
[5,     1] loss: 1321.358
[6,     1] loss: 1322.288
[7,     1] loss: 1317.787
[8,     1] loss: 1324.046
[9,     1] loss: 1324.625
[10,     1] loss: 1323.646
[11,     1] loss: 1318.789
[12,     1] loss: 1316.933
[13,     1] loss: 1315.100
[14,     1] loss: 1306.975
[15,     1] loss: 1295.227
[16,     1] loss: 1284.701
[17,     1] loss: 1257.459
[18,     1] loss: 1230.523
[19,     1] loss: 1184.321
[20,     1] loss: 1186.606
[21,     1] loss: 1121.156
[22,     1] loss: 1093.658
[23,     1] loss: 1100.119
[24,     1] loss: 1089.113
[25,     1] loss: 1122.772
[26,     1] loss: 1083.729
[27,     1] loss: 1072.722
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005735964897570765,
 'learning_rate_Hydroxylation-K': 0.0019926672872028622,
 'learning_rate_Hydroxylation-P': 0.00779089599575466,
 'log_base': 1.0791699259746501,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2746156869,
 'sample_weights': [1.9654752809619687, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.797211033491207,
 'weight_decay_Hydroxylation-K': 3.359424518424095,
 'weight_decay_Hydroxylation-P': 7.376643805204369}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 7134.102
[2,     1] loss: 7125.942
[3,     1] loss: 7104.105
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0068669398493601,
 'learning_rate_Hydroxylation-K': 0.008443344987804777,
 'learning_rate_Hydroxylation-P': 0.0021189174175401757,
 'log_base': 1.0170147973927597,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3608458497,
 'sample_weights': [21.9109574915599, 2.738975198703594],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.605681528367732,
 'weight_decay_Hydroxylation-K': 7.972276287937388,
 'weight_decay_Hydroxylation-P': 6.9365149610800945}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32117.994
Exploding loss, terminate run (best metric=0.5314674377441406)
Finished Training
Total time taken: 0.2140207290649414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32067.070
Exploding loss, terminate run (best metric=0.5410791635513306)
Finished Training
Total time taken: 0.21502161026000977
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32208.867
Exploding loss, terminate run (best metric=0.5286145806312561)
Finished Training
Total time taken: 0.21301937103271484
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32139.324
Exploding loss, terminate run (best metric=0.5319166779518127)
Finished Training
Total time taken: 0.21402454376220703
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32170.035
Exploding loss, terminate run (best metric=0.5453722476959229)
Finished Training
Total time taken: 0.21102237701416016
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32156.996
Exploding loss, terminate run (best metric=0.5318508148193359)
Finished Training
Total time taken: 0.20601916313171387
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32271.359
Exploding loss, terminate run (best metric=0.5264467597007751)
Finished Training
Total time taken: 0.21302247047424316
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32094.771
Exploding loss, terminate run (best metric=0.5266886353492737)
Finished Training
Total time taken: 0.2230210304260254
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32038.127
Exploding loss, terminate run (best metric=0.5325810313224792)
Finished Training
Total time taken: 0.2320241928100586
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32250.195
Exploding loss, terminate run (best metric=0.5285271406173706)
Finished Training
Total time taken: 0.2230226993560791
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32129.715
Exploding loss, terminate run (best metric=0.5340104699134827)
Finished Training
Total time taken: 0.21002554893493652
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32073.154
Exploding loss, terminate run (best metric=0.5367439985275269)
Finished Training
Total time taken: 0.2230226993560791
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 31996.328
Exploding loss, terminate run (best metric=0.5266081690788269)
Finished Training
Total time taken: 0.2080221176147461
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 32117.572
Exploding loss, terminate run (best metric=0.5382915735244751)
Finished Training
Total time taken: 0.22002387046813965
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 32134.797
Exploding loss, terminate run (best metric=0.5273407101631165)
Finished Training
Total time taken: 0.20502018928527832
{'Hydroxylation-K Validation Accuracy': 0.5203014184397163, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.531578947368421, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6319103313840156, 'Hydroxylation-K AUC PR': 0.3399858667757335, 'Hydroxylation-K MCC': -0.004988477453464162, 'Hydroxylation-K F1': 0.15779967159277505, 'Validation Loss (Hydroxylation-K)': 0.558964196840922, 'Hydroxylation-P Validation Accuracy': 0.5212719489704414, 'Hydroxylation-P Validation Sensitivity': 0.4819047619047619, 'Hydroxylation-P Validation Specificity': 0.5292682926829269, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6359832656024487, 'Hydroxylation-P AUC PR': 0.3230932446197213, 'Hydroxylation-P MCC': 0.014830381627643234, 'Hydroxylation-P F1': 0.1606900367008576, 'Validation Loss (Hydroxylation-P)': 0.5325026273727417, 'Validation Loss (total)': 1.0914668162663779, 'TimeToTrain': 0.21535550753275554}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00505832703624041,
 'learning_rate_Hydroxylation-K': 0.0033405339015600472,
 'learning_rate_Hydroxylation-P': 0.008343078188532322,
 'log_base': 1.0592822895097511,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1283775474,
 'sample_weights': [99.0229084900548, 12.35214454770124],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.265824912126787,
 'weight_decay_Hydroxylation-K': 2.926622240823349,
 'weight_decay_Hydroxylation-P': 4.60327661979736}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 9433.448
[2,     1] loss: 9448.981
[3,     1] loss: 9414.764
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006492135327200901,
 'learning_rate_Hydroxylation-K': 0.009903004377931677,
 'learning_rate_Hydroxylation-P': 0.007079840575446071,
 'log_base': 2.734917298908244,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2593932098,
 'sample_weights': [28.987618643559227, 3.6235919203791127],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.323424711750666,
 'weight_decay_Hydroxylation-K': 8.328913281683077,
 'weight_decay_Hydroxylation-P': 9.674841333343524}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1257.931
[2,     1] loss: 1257.854
[3,     1] loss: 1260.708
[4,     1] loss: 1263.749
[5,     1] loss: 1260.241
[6,     1] loss: 1260.387
[7,     1] loss: 1261.943
[8,     1] loss: 1259.015
[9,     1] loss: 1253.616
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005320042649532749,
 'learning_rate_Hydroxylation-K': 0.0034375515306303336,
 'learning_rate_Hydroxylation-P': 0.0059754469659432635,
 'log_base': 1.8307495093852866,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2121816022,
 'sample_weights': [1.6593193128168624, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.302582908755905,
 'weight_decay_Hydroxylation-K': 3.4836269050269317,
 'weight_decay_Hydroxylation-P': 3.840857334955942}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1488.201
[2,     1] loss: 1492.911
[3,     1] loss: 1494.746
[4,     1] loss: 1484.513
[5,     1] loss: 1484.015
[6,     1] loss: 1486.094
[7,     1] loss: 1482.776
[8,     1] loss: 1468.002
[9,     1] loss: 1439.341
[10,     1] loss: 1396.239
[11,     1] loss: 1325.088
[12,     1] loss: 1283.453
[13,     1] loss: 1306.502
[14,     1] loss: 1281.605
[15,     1] loss: 1271.446
[16,     1] loss: 1347.495
[17,     1] loss: 1214.771
[18,     1] loss: 1286.642
[19,     1] loss: 1221.147
[20,     1] loss: 1263.329
[21,     1] loss: 1234.751
[22,     1] loss: 1225.925
[23,     1] loss: 1214.524
[24,     1] loss: 1245.952
[25,     1] loss: 1234.666
[26,     1] loss: 1177.592
[27,     1] loss: 1134.967
[28,     1] loss: 1162.898
[29,     1] loss: 1118.839
[30,     1] loss: 1083.009
[31,     1] loss: 1136.264
[32,     1] loss: 1081.082
[33,     1] loss: 1127.474
[34,     1] loss: 1195.503
[35,     1] loss: 1140.883
[36,     1] loss: 1031.953
[37,     1] loss: 1104.206
[38,     1] loss: 1015.031
[39,     1] loss: 1052.684
[40,     1] loss: 1010.282
[41,     1] loss: 1012.789
[42,     1] loss: 923.663
[43,     1] loss: 960.440
[44,     1] loss: 997.561
[45,     1] loss: 866.567
[46,     1] loss: 894.899
[47,     1] loss: 901.726
[48,     1] loss: 972.028
[49,     1] loss: 1153.194
[50,     1] loss: 1278.105
[51,     1] loss: 957.239
[52,     1] loss: 1128.378
[53,     1] loss: 1109.739
[54,     1] loss: 997.544
[55,     1] loss: 1081.017
[56,     1] loss: 1044.620
[57,     1] loss: 925.348
[58,     1] loss: 934.196
[59,     1] loss: 895.405
[60,     1] loss: 1045.732
[61,     1] loss: 943.440
[62,     1] loss: 1023.683
[63,     1] loss: 910.517
[64,     1] loss: 889.320
[65,     1] loss: 896.320
[66,     1] loss: 822.895
[67,     1] loss: 984.445
[68,     1] loss: 828.914
[69,     1] loss: 888.745
[70,     1] loss: 801.902
[71,     1] loss: 803.510
[72,     1] loss: 787.706
[73,     1] loss: 720.318
[74,     1] loss: 917.992
[75,     1] loss: 1154.865
[76,     1] loss: 722.655
[77,     1] loss: 1004.385
[78,     1] loss: 844.441
[79,     1] loss: 960.424
[80,     1] loss: 750.728
[81,     1] loss: 941.990
[82,     1] loss: 762.772
[83,     1] loss: 862.657
[84,     1] loss: 820.685
[85,     1] loss: 709.587
[86,     1] loss: 807.322
[87,     1] loss: 711.872
[88,     1] loss: 643.163
[89,     1] loss: 675.392
[90,     1] loss: 691.066
[91,     1] loss: 765.384
[92,     1] loss: 1077.817
[93,     1] loss: 1188.746
[94,     1] loss: 839.734
[95,     1] loss: 1067.981
[96,     1] loss: 1092.787
Early stopping applied (best metric=0.3319748342037201)
Finished Training
Total time taken: 13.708465337753296
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1494.865
[2,     1] loss: 1496.898
[3,     1] loss: 1482.952
[4,     1] loss: 1488.173
[5,     1] loss: 1489.635
[6,     1] loss: 1491.652
[7,     1] loss: 1485.800
[8,     1] loss: 1489.101
[9,     1] loss: 1486.780
[10,     1] loss: 1482.781
[11,     1] loss: 1472.004
[12,     1] loss: 1460.916
[13,     1] loss: 1438.136
[14,     1] loss: 1419.698
[15,     1] loss: 1366.839
[16,     1] loss: 1359.713
[17,     1] loss: 1301.335
[18,     1] loss: 1265.233
[19,     1] loss: 1267.820
[20,     1] loss: 1274.060
[21,     1] loss: 1220.645
[22,     1] loss: 1188.328
[23,     1] loss: 1232.006
[24,     1] loss: 1207.068
[25,     1] loss: 1203.376
[26,     1] loss: 1174.528
[27,     1] loss: 1179.128
[28,     1] loss: 1144.560
[29,     1] loss: 1113.179
[30,     1] loss: 1134.722
[31,     1] loss: 1113.016
[32,     1] loss: 1066.461
[33,     1] loss: 1068.343
[34,     1] loss: 1082.676
[35,     1] loss: 1091.782
[36,     1] loss: 1018.494
[37,     1] loss: 1105.168
[38,     1] loss: 1047.342
[39,     1] loss: 1040.114
[40,     1] loss: 1117.098
[41,     1] loss: 1000.784
[42,     1] loss: 1052.689
[43,     1] loss: 1030.504
[44,     1] loss: 1043.865
[45,     1] loss: 1005.150
[46,     1] loss: 1088.844
[47,     1] loss: 944.311
[48,     1] loss: 956.595
[49,     1] loss: 959.434
[50,     1] loss: 949.042
[51,     1] loss: 999.735
[52,     1] loss: 907.416
[53,     1] loss: 880.010
[54,     1] loss: 852.377
[55,     1] loss: 879.079
[56,     1] loss: 859.190
[57,     1] loss: 821.269
[58,     1] loss: 883.738
[59,     1] loss: 1607.324
[60,     1] loss: 905.505
[61,     1] loss: 1085.043
[62,     1] loss: 919.065
[63,     1] loss: 1136.399
[64,     1] loss: 1086.875
[65,     1] loss: 959.146
[66,     1] loss: 967.823
[67,     1] loss: 1064.920
[68,     1] loss: 921.358
[69,     1] loss: 1013.021
[70,     1] loss: 1039.814
[71,     1] loss: 880.753
[72,     1] loss: 994.604
[73,     1] loss: 878.898
[74,     1] loss: 935.906
[75,     1] loss: 806.974
[76,     1] loss: 873.446
[77,     1] loss: 831.534
[78,     1] loss: 793.273
Early stopping applied (best metric=0.3987575173377991)
Finished Training
Total time taken: 11.241200923919678
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1492.510
[2,     1] loss: 1497.375
[3,     1] loss: 1488.285
[4,     1] loss: 1501.532
[5,     1] loss: 1482.447
[6,     1] loss: 1490.391
[7,     1] loss: 1486.121
[8,     1] loss: 1488.992
[9,     1] loss: 1483.157
[10,     1] loss: 1478.743
[11,     1] loss: 1470.624
[12,     1] loss: 1449.717
[13,     1] loss: 1435.414
[14,     1] loss: 1387.046
[15,     1] loss: 1365.452
[16,     1] loss: 1305.282
[17,     1] loss: 1301.319
[18,     1] loss: 1230.221
[19,     1] loss: 1207.433
[20,     1] loss: 1257.206
[21,     1] loss: 1178.425
[22,     1] loss: 1299.521
[23,     1] loss: 1172.101
[24,     1] loss: 1204.229
[25,     1] loss: 1199.182
[26,     1] loss: 1186.854
[27,     1] loss: 1152.899
[28,     1] loss: 1164.425
[29,     1] loss: 1140.245
[30,     1] loss: 1064.413
[31,     1] loss: 1190.617
[32,     1] loss: 1102.198
[33,     1] loss: 1198.563
[34,     1] loss: 974.904
[35,     1] loss: 1117.701
[36,     1] loss: 1008.812
[37,     1] loss: 1093.032
[38,     1] loss: 1123.535
[39,     1] loss: 1030.084
[40,     1] loss: 1029.795
[41,     1] loss: 1015.434
[42,     1] loss: 949.648
[43,     1] loss: 995.450
[44,     1] loss: 942.804
[45,     1] loss: 917.339
[46,     1] loss: 952.618
[47,     1] loss: 926.550
[48,     1] loss: 933.728
[49,     1] loss: 1155.877
[50,     1] loss: 1395.270
[51,     1] loss: 997.032
[52,     1] loss: 1070.792
[53,     1] loss: 1130.569
[54,     1] loss: 1088.119
[55,     1] loss: 1106.354
[56,     1] loss: 1042.849
[57,     1] loss: 1067.324
[58,     1] loss: 1009.295
[59,     1] loss: 917.628
[60,     1] loss: 1118.439
[61,     1] loss: 921.131
[62,     1] loss: 965.958
[63,     1] loss: 916.451
[64,     1] loss: 940.706
[65,     1] loss: 885.013
[66,     1] loss: 902.367
[67,     1] loss: 868.178
[68,     1] loss: 924.553
[69,     1] loss: 808.683
[70,     1] loss: 860.380
[71,     1] loss: 755.534
[72,     1] loss: 866.773
[73,     1] loss: 815.170
[74,     1] loss: 767.655
[75,     1] loss: 740.463
[76,     1] loss: 693.568
[77,     1] loss: 672.735
[78,     1] loss: 792.152
[79,     1] loss: 1204.344
[80,     1] loss: 1925.630
[81,     1] loss: 901.474
[82,     1] loss: 1017.125
[83,     1] loss: 1246.859
[84,     1] loss: 1190.866
[85,     1] loss: 1189.544
[86,     1] loss: 1231.926
[87,     1] loss: 1204.293
[88,     1] loss: 1157.231
[89,     1] loss: 1098.605
[90,     1] loss: 1136.772
[91,     1] loss: 1120.062
[92,     1] loss: 1111.865
[93,     1] loss: 1095.820
[94,     1] loss: 1067.711
Early stopping applied (best metric=0.28008830547332764)
Finished Training
Total time taken: 13.493443250656128
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1493.567
[2,     1] loss: 1491.928
[3,     1] loss: 1491.427
[4,     1] loss: 1490.131
[5,     1] loss: 1487.735
[6,     1] loss: 1500.802
[7,     1] loss: 1487.961
[8,     1] loss: 1486.427
[9,     1] loss: 1481.976
[10,     1] loss: 1479.119
[11,     1] loss: 1477.326
[12,     1] loss: 1461.864
[13,     1] loss: 1427.760
[14,     1] loss: 1402.278
[15,     1] loss: 1380.972
[16,     1] loss: 1309.377
[17,     1] loss: 1285.596
[18,     1] loss: 1315.650
[19,     1] loss: 1227.058
[20,     1] loss: 1274.255
[21,     1] loss: 1257.022
[22,     1] loss: 1249.372
[23,     1] loss: 1246.512
[24,     1] loss: 1229.639
[25,     1] loss: 1217.614
[26,     1] loss: 1203.044
[27,     1] loss: 1209.466
[28,     1] loss: 1223.360
[29,     1] loss: 1208.728
[30,     1] loss: 1169.303
[31,     1] loss: 1205.152
[32,     1] loss: 1160.725
[33,     1] loss: 1176.535
[34,     1] loss: 1130.527
[35,     1] loss: 1110.240
[36,     1] loss: 1148.402
[37,     1] loss: 1101.339
[38,     1] loss: 1117.275
[39,     1] loss: 1079.746
[40,     1] loss: 1093.807
[41,     1] loss: 1091.251
[42,     1] loss: 1024.801
[43,     1] loss: 1078.801
[44,     1] loss: 1087.069
[45,     1] loss: 1028.871
[46,     1] loss: 1083.295
[47,     1] loss: 1123.326
[48,     1] loss: 990.970
[49,     1] loss: 1078.415
[50,     1] loss: 974.423
[51,     1] loss: 1119.764
[52,     1] loss: 1064.552
[53,     1] loss: 954.045
[54,     1] loss: 1031.215
[55,     1] loss: 913.384
[56,     1] loss: 1043.998
[57,     1] loss: 891.807
[58,     1] loss: 923.417
[59,     1] loss: 869.753
[60,     1] loss: 937.623
[61,     1] loss: 1117.885
[62,     1] loss: 877.276
[63,     1] loss: 1007.725
[64,     1] loss: 887.388
[65,     1] loss: 896.911
[66,     1] loss: 832.377
[67,     1] loss: 838.882
[68,     1] loss: 855.307
[69,     1] loss: 797.672
[70,     1] loss: 698.541
[71,     1] loss: 835.077
[72,     1] loss: 1287.711
[73,     1] loss: 1354.719
[74,     1] loss: 1007.738
[75,     1] loss: 1030.292
[76,     1] loss: 1213.875
[77,     1] loss: 1222.139
[78,     1] loss: 1144.383
[79,     1] loss: 1127.762
[80,     1] loss: 1097.486
[81,     1] loss: 1031.588
[82,     1] loss: 1022.500
[83,     1] loss: 983.496
[84,     1] loss: 963.014
[85,     1] loss: 901.867
[86,     1] loss: 926.719
[87,     1] loss: 928.841
[88,     1] loss: 913.797
[89,     1] loss: 861.756
[90,     1] loss: 871.463
[91,     1] loss: 850.740
[92,     1] loss: 859.718
[93,     1] loss: 780.801
[94,     1] loss: 750.871
Early stopping applied (best metric=0.3551294207572937)
Finished Training
Total time taken: 13.383427858352661
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1492.156
[2,     1] loss: 1490.816
[3,     1] loss: 1498.032
[4,     1] loss: 1491.829
[5,     1] loss: 1490.427
[6,     1] loss: 1485.074
[7,     1] loss: 1484.946
[8,     1] loss: 1479.210
[9,     1] loss: 1470.877
[10,     1] loss: 1455.242
[11,     1] loss: 1430.459
[12,     1] loss: 1399.902
[13,     1] loss: 1365.857
[14,     1] loss: 1326.197
[15,     1] loss: 1326.185
[16,     1] loss: 1300.438
[17,     1] loss: 1277.779
[18,     1] loss: 1245.333
[19,     1] loss: 1226.868
[20,     1] loss: 1217.821
[21,     1] loss: 1188.210
[22,     1] loss: 1163.593
[23,     1] loss: 1188.881
[24,     1] loss: 1178.724
[25,     1] loss: 1141.563
[26,     1] loss: 1182.562
[27,     1] loss: 1104.450
[28,     1] loss: 1232.574
[29,     1] loss: 1210.373
[30,     1] loss: 1098.025
[31,     1] loss: 1212.702
[32,     1] loss: 1095.001
[33,     1] loss: 1076.116
[34,     1] loss: 1038.117
[35,     1] loss: 1070.139
[36,     1] loss: 1054.750
[37,     1] loss: 1058.930
[38,     1] loss: 1020.128
[39,     1] loss: 976.573
[40,     1] loss: 963.029
[41,     1] loss: 946.555
[42,     1] loss: 1153.630
[43,     1] loss: 1378.305
[44,     1] loss: 1003.374
[45,     1] loss: 1191.829
[46,     1] loss: 1099.447
[47,     1] loss: 1028.229
[48,     1] loss: 1110.537
[49,     1] loss: 1094.947
[50,     1] loss: 1007.550
[51,     1] loss: 1047.918
[52,     1] loss: 1046.630
[53,     1] loss: 1014.604
[54,     1] loss: 972.649
[55,     1] loss: 948.359
[56,     1] loss: 896.073
[57,     1] loss: 882.436
[58,     1] loss: 899.160
[59,     1] loss: 862.540
[60,     1] loss: 874.645
[61,     1] loss: 940.290
[62,     1] loss: 853.050
[63,     1] loss: 756.809
[64,     1] loss: 826.163
[65,     1] loss: 813.903
[66,     1] loss: 750.241
[67,     1] loss: 782.486
[68,     1] loss: 848.835
[69,     1] loss: 1197.216
[70,     1] loss: 1129.888
[71,     1] loss: 834.126
[72,     1] loss: 988.361
[73,     1] loss: 935.979
[74,     1] loss: 793.866
[75,     1] loss: 865.319
Early stopping applied (best metric=0.39588847756385803)
Finished Training
Total time taken: 10.649141550064087
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1488.881
[2,     1] loss: 1496.246
[3,     1] loss: 1483.514
[4,     1] loss: 1489.452
[5,     1] loss: 1486.095
[6,     1] loss: 1482.774
[7,     1] loss: 1473.320
[8,     1] loss: 1462.014
[9,     1] loss: 1426.137
[10,     1] loss: 1385.801
[11,     1] loss: 1358.777
[12,     1] loss: 1332.673
[13,     1] loss: 1289.624
[14,     1] loss: 1253.089
[15,     1] loss: 1214.187
[16,     1] loss: 1281.106
[17,     1] loss: 1180.883
[18,     1] loss: 1174.029
[19,     1] loss: 1256.123
[20,     1] loss: 1163.075
[21,     1] loss: 1156.662
[22,     1] loss: 1162.786
[23,     1] loss: 1161.047
[24,     1] loss: 1075.884
[25,     1] loss: 1090.172
[26,     1] loss: 1084.560
[27,     1] loss: 1171.452
[28,     1] loss: 1092.894
[29,     1] loss: 1108.052
[30,     1] loss: 1094.826
[31,     1] loss: 1060.883
[32,     1] loss: 1059.016
[33,     1] loss: 1056.578
[34,     1] loss: 1007.388
[35,     1] loss: 1010.692
[36,     1] loss: 966.246
[37,     1] loss: 955.835
[38,     1] loss: 965.054
[39,     1] loss: 946.062
[40,     1] loss: 889.945
[41,     1] loss: 893.850
[42,     1] loss: 909.478
[43,     1] loss: 1015.864
[44,     1] loss: 914.673
[45,     1] loss: 871.554
[46,     1] loss: 980.508
[47,     1] loss: 862.631
[48,     1] loss: 882.055
[49,     1] loss: 916.624
[50,     1] loss: 830.753
[51,     1] loss: 883.981
[52,     1] loss: 813.426
[53,     1] loss: 819.213
[54,     1] loss: 861.545
[55,     1] loss: 786.452
[56,     1] loss: 836.214
[57,     1] loss: 881.333
[58,     1] loss: 750.948
[59,     1] loss: 875.519
[60,     1] loss: 710.774
[61,     1] loss: 713.609
[62,     1] loss: 734.183
[63,     1] loss: 773.196
[64,     1] loss: 783.176
[65,     1] loss: 882.693
[66,     1] loss: 761.153
[67,     1] loss: 702.107
[68,     1] loss: 729.266
Early stopping applied (best metric=0.37714120745658875)
Finished Training
Total time taken: 9.772042751312256
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1495.020
[2,     1] loss: 1486.468
[3,     1] loss: 1498.859
[4,     1] loss: 1491.508
[5,     1] loss: 1488.998
[6,     1] loss: 1490.520
[7,     1] loss: 1486.043
[8,     1] loss: 1487.857
[9,     1] loss: 1489.748
[10,     1] loss: 1484.722
[11,     1] loss: 1487.186
[12,     1] loss: 1474.536
[13,     1] loss: 1466.210
[14,     1] loss: 1446.334
[15,     1] loss: 1422.693
[16,     1] loss: 1371.554
[17,     1] loss: 1335.026
[18,     1] loss: 1311.673
[19,     1] loss: 1301.102
[20,     1] loss: 1206.370
[21,     1] loss: 1167.546
[22,     1] loss: 1270.631
[23,     1] loss: 1216.159
[24,     1] loss: 1215.685
[25,     1] loss: 1210.460
[26,     1] loss: 1185.082
[27,     1] loss: 1200.378
[28,     1] loss: 1145.683
[29,     1] loss: 1117.703
[30,     1] loss: 1109.380
[31,     1] loss: 1150.713
[32,     1] loss: 1106.253
[33,     1] loss: 1015.728
[34,     1] loss: 1048.032
[35,     1] loss: 1048.879
[36,     1] loss: 1030.939
[37,     1] loss: 1044.647
[38,     1] loss: 1132.031
[39,     1] loss: 1169.016
[40,     1] loss: 966.414
[41,     1] loss: 1120.238
[42,     1] loss: 1010.767
[43,     1] loss: 1044.992
[44,     1] loss: 952.223
[45,     1] loss: 1130.020
[46,     1] loss: 941.598
[47,     1] loss: 1019.865
[48,     1] loss: 910.105
[49,     1] loss: 920.817
[50,     1] loss: 939.775
[51,     1] loss: 893.785
[52,     1] loss: 1006.853
[53,     1] loss: 998.975
[54,     1] loss: 853.740
[55,     1] loss: 870.819
[56,     1] loss: 975.027
[57,     1] loss: 1039.811
[58,     1] loss: 813.991
[59,     1] loss: 934.822
[60,     1] loss: 1035.132
[61,     1] loss: 820.927
[62,     1] loss: 945.425
[63,     1] loss: 804.412
[64,     1] loss: 933.395
[65,     1] loss: 791.549
[66,     1] loss: 774.822
[67,     1] loss: 987.803
[68,     1] loss: 757.299
[69,     1] loss: 976.562
[70,     1] loss: 1087.253
[71,     1] loss: 859.802
[72,     1] loss: 982.302
[73,     1] loss: 970.306
[74,     1] loss: 840.444
[75,     1] loss: 990.675
[76,     1] loss: 800.355
[77,     1] loss: 882.579
[78,     1] loss: 789.277
[79,     1] loss: 744.864
[80,     1] loss: 789.469
[81,     1] loss: 725.483
[82,     1] loss: 720.681
[83,     1] loss: 818.989
[84,     1] loss: 678.240
[85,     1] loss: 909.363
[86,     1] loss: 851.910
[87,     1] loss: 751.003
[88,     1] loss: 780.172
[89,     1] loss: 621.839
Early stopping applied (best metric=0.4229864776134491)
Finished Training
Total time taken: 12.882376909255981
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1493.423
[2,     1] loss: 1493.941
[3,     1] loss: 1490.590
[4,     1] loss: 1486.272
[5,     1] loss: 1487.133
[6,     1] loss: 1488.000
[7,     1] loss: 1480.784
[8,     1] loss: 1480.666
[9,     1] loss: 1467.438
[10,     1] loss: 1454.508
[11,     1] loss: 1416.647
[12,     1] loss: 1389.392
[13,     1] loss: 1356.234
[14,     1] loss: 1304.025
[15,     1] loss: 1274.311
[16,     1] loss: 1295.537
[17,     1] loss: 1231.525
[18,     1] loss: 1200.708
[19,     1] loss: 1183.868
[20,     1] loss: 1232.308
[21,     1] loss: 1226.464
[22,     1] loss: 1173.092
[23,     1] loss: 1171.752
[24,     1] loss: 1123.328
[25,     1] loss: 1132.793
[26,     1] loss: 1140.510
[27,     1] loss: 1032.062
[28,     1] loss: 1054.269
[29,     1] loss: 1077.463
[30,     1] loss: 1116.576
[31,     1] loss: 1101.911
[32,     1] loss: 1175.859
[33,     1] loss: 1087.291
[34,     1] loss: 1036.963
[35,     1] loss: 1027.323
[36,     1] loss: 1001.532
[37,     1] loss: 1067.225
[38,     1] loss: 1056.332
[39,     1] loss: 1068.153
[40,     1] loss: 1048.900
[41,     1] loss: 1028.825
[42,     1] loss: 1028.952
[43,     1] loss: 1005.707
[44,     1] loss: 968.761
[45,     1] loss: 980.977
[46,     1] loss: 959.492
[47,     1] loss: 900.467
[48,     1] loss: 993.978
[49,     1] loss: 861.004
[50,     1] loss: 939.315
[51,     1] loss: 942.902
[52,     1] loss: 876.693
[53,     1] loss: 873.620
[54,     1] loss: 963.867
[55,     1] loss: 850.297
[56,     1] loss: 807.695
[57,     1] loss: 858.450
[58,     1] loss: 816.672
[59,     1] loss: 837.911
[60,     1] loss: 936.027
[61,     1] loss: 980.661
[62,     1] loss: 809.535
[63,     1] loss: 782.338
[64,     1] loss: 782.707
[65,     1] loss: 761.352
[66,     1] loss: 798.209
[67,     1] loss: 658.696
[68,     1] loss: 799.894
[69,     1] loss: 826.997
[70,     1] loss: 773.182
[71,     1] loss: 674.052
[72,     1] loss: 777.178
[73,     1] loss: 922.321
[74,     1] loss: 821.965
[75,     1] loss: 711.450
[76,     1] loss: 791.596
[77,     1] loss: 645.691
[78,     1] loss: 760.058
[79,     1] loss: 911.439
Early stopping applied (best metric=0.36736950278282166)
Finished Training
Total time taken: 11.291204690933228
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1497.853
[2,     1] loss: 1490.880
[3,     1] loss: 1485.395
[4,     1] loss: 1491.697
[5,     1] loss: 1494.309
[6,     1] loss: 1487.140
[7,     1] loss: 1488.419
[8,     1] loss: 1485.795
[9,     1] loss: 1485.785
[10,     1] loss: 1477.953
[11,     1] loss: 1463.818
[12,     1] loss: 1448.558
[13,     1] loss: 1423.427
[14,     1] loss: 1383.081
[15,     1] loss: 1354.497
[16,     1] loss: 1304.042
[17,     1] loss: 1288.099
[18,     1] loss: 1266.978
[19,     1] loss: 1295.847
[20,     1] loss: 1298.699
[21,     1] loss: 1234.765
[22,     1] loss: 1253.273
[23,     1] loss: 1232.695
[24,     1] loss: 1259.277
[25,     1] loss: 1174.823
[26,     1] loss: 1203.969
[27,     1] loss: 1184.325
[28,     1] loss: 1153.611
[29,     1] loss: 1146.190
[30,     1] loss: 1166.486
[31,     1] loss: 1119.233
[32,     1] loss: 1105.267
[33,     1] loss: 1159.928
[34,     1] loss: 1145.188
[35,     1] loss: 1138.131
[36,     1] loss: 1139.058
[37,     1] loss: 1073.116
[38,     1] loss: 1079.979
[39,     1] loss: 1010.407
[40,     1] loss: 1023.131
[41,     1] loss: 1031.360
[42,     1] loss: 1031.321
[43,     1] loss: 996.620
[44,     1] loss: 1152.435
[45,     1] loss: 1451.180
[46,     1] loss: 975.652
[47,     1] loss: 1272.549
[48,     1] loss: 1098.312
[49,     1] loss: 1106.737
[50,     1] loss: 1150.906
[51,     1] loss: 1132.069
[52,     1] loss: 1068.932
[53,     1] loss: 1025.491
[54,     1] loss: 1028.574
[55,     1] loss: 1013.450
[56,     1] loss: 1054.720
[57,     1] loss: 942.880
[58,     1] loss: 1018.218
[59,     1] loss: 928.042
[60,     1] loss: 980.650
[61,     1] loss: 873.834
[62,     1] loss: 935.017
[63,     1] loss: 851.665
[64,     1] loss: 865.885
[65,     1] loss: 841.060
[66,     1] loss: 811.471
[67,     1] loss: 853.586
[68,     1] loss: 838.767
[69,     1] loss: 1149.178
[70,     1] loss: 1460.804
[71,     1] loss: 1001.849
[72,     1] loss: 1006.451
[73,     1] loss: 1187.080
[74,     1] loss: 1129.102
[75,     1] loss: 1098.352
[76,     1] loss: 1096.757
[77,     1] loss: 1111.148
[78,     1] loss: 1028.295
[79,     1] loss: 996.984
[80,     1] loss: 1162.052
[81,     1] loss: 1004.083
[82,     1] loss: 1052.193
[83,     1] loss: 939.501
[84,     1] loss: 1032.949
[85,     1] loss: 952.825
[86,     1] loss: 893.894
[87,     1] loss: 978.234
[88,     1] loss: 889.479
[89,     1] loss: 872.542
[90,     1] loss: 781.753
[91,     1] loss: 895.487
[92,     1] loss: 1030.962
[93,     1] loss: 859.194
[94,     1] loss: 800.034
[95,     1] loss: 905.178
[96,     1] loss: 844.504
[97,     1] loss: 798.535
[98,     1] loss: 777.329
[99,     1] loss: 783.909
[100,     1] loss: 797.707
[101,     1] loss: 634.598
[102,     1] loss: 665.517
[103,     1] loss: 722.410
[104,     1] loss: 1065.718
[105,     1] loss: 2635.185
[106,     1] loss: 2359.342
[107,     1] loss: 1534.609
[108,     1] loss: 1446.009
[109,     1] loss: 1484.637
[110,     1] loss: 1490.931
[111,     1] loss: 1490.187
[112,     1] loss: 1489.702
[113,     1] loss: 1490.407
[114,     1] loss: 1489.040
[115,     1] loss: 1490.300
[116,     1] loss: 1489.684
[117,     1] loss: 1489.705
[118,     1] loss: 1490.029
[119,     1] loss: 1490.034
[120,     1] loss: 1489.784
[121,     1] loss: 1488.959
[122,     1] loss: 1489.684
[123,     1] loss: 1489.179
[124,     1] loss: 1489.359
[125,     1] loss: 1490.649
[126,     1] loss: 1489.912
[127,     1] loss: 1489.750
Early stopping applied (best metric=0.3630657196044922)
Finished Training
Total time taken: 18.197944402694702
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1497.282
[2,     1] loss: 1494.875
[3,     1] loss: 1497.595
[4,     1] loss: 1489.720
[5,     1] loss: 1487.191
[6,     1] loss: 1493.025
[7,     1] loss: 1489.367
[8,     1] loss: 1488.271
[9,     1] loss: 1484.508
[10,     1] loss: 1473.645
[11,     1] loss: 1482.871
[12,     1] loss: 1449.811
[13,     1] loss: 1437.780
[14,     1] loss: 1404.796
[15,     1] loss: 1406.327
[16,     1] loss: 1344.947
[17,     1] loss: 1319.523
[18,     1] loss: 1267.680
[19,     1] loss: 1289.913
[20,     1] loss: 1221.673
[21,     1] loss: 1187.745
[22,     1] loss: 1216.578
[23,     1] loss: 1171.157
[24,     1] loss: 1211.046
[25,     1] loss: 1269.908
[26,     1] loss: 1221.327
[27,     1] loss: 1155.252
[28,     1] loss: 1151.200
[29,     1] loss: 1121.279
[30,     1] loss: 1147.670
[31,     1] loss: 1085.340
[32,     1] loss: 1119.521
[33,     1] loss: 1196.565
[34,     1] loss: 1077.984
[35,     1] loss: 1088.725
[36,     1] loss: 1080.665
[37,     1] loss: 1081.542
[38,     1] loss: 1068.371
[39,     1] loss: 1026.985
[40,     1] loss: 1082.173
[41,     1] loss: 1031.479
[42,     1] loss: 960.807
[43,     1] loss: 894.221
[44,     1] loss: 918.641
[45,     1] loss: 909.127
[46,     1] loss: 957.120
[47,     1] loss: 1189.073
[48,     1] loss: 1517.259
[49,     1] loss: 970.836
[50,     1] loss: 1142.618
[51,     1] loss: 1212.024
[52,     1] loss: 1127.608
[53,     1] loss: 1133.301
[54,     1] loss: 1118.439
[55,     1] loss: 1136.255
[56,     1] loss: 1083.070
[57,     1] loss: 1050.748
[58,     1] loss: 1031.221
[59,     1] loss: 1015.544
[60,     1] loss: 1025.185
[61,     1] loss: 970.040
[62,     1] loss: 944.423
[63,     1] loss: 986.842
[64,     1] loss: 885.748
[65,     1] loss: 962.067
[66,     1] loss: 923.509
[67,     1] loss: 901.995
[68,     1] loss: 820.514
[69,     1] loss: 789.996
[70,     1] loss: 851.204
[71,     1] loss: 1192.177
[72,     1] loss: 1138.912
[73,     1] loss: 816.943
[74,     1] loss: 1010.414
Early stopping applied (best metric=0.3888087272644043)
Finished Training
Total time taken: 10.613134384155273
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1493.666
[2,     1] loss: 1485.354
[3,     1] loss: 1493.765
[4,     1] loss: 1488.075
[5,     1] loss: 1487.091
[6,     1] loss: 1485.876
[7,     1] loss: 1484.163
[8,     1] loss: 1476.041
[9,     1] loss: 1458.164
[10,     1] loss: 1443.595
[11,     1] loss: 1394.735
[12,     1] loss: 1351.533
[13,     1] loss: 1338.764
[14,     1] loss: 1335.338
[15,     1] loss: 1279.831
[16,     1] loss: 1283.586
[17,     1] loss: 1242.864
[18,     1] loss: 1232.491
[19,     1] loss: 1283.870
[20,     1] loss: 1229.722
[21,     1] loss: 1218.504
[22,     1] loss: 1248.870
[23,     1] loss: 1227.001
[24,     1] loss: 1240.104
[25,     1] loss: 1229.138
[26,     1] loss: 1141.567
[27,     1] loss: 1179.436
[28,     1] loss: 1106.341
[29,     1] loss: 1106.568
[30,     1] loss: 1114.409
[31,     1] loss: 1092.272
[32,     1] loss: 1106.877
[33,     1] loss: 1068.781
[34,     1] loss: 1039.270
[35,     1] loss: 1057.757
[36,     1] loss: 1052.164
[37,     1] loss: 1049.030
[38,     1] loss: 1143.557
[39,     1] loss: 1148.773
[40,     1] loss: 1037.497
[41,     1] loss: 1080.371
[42,     1] loss: 1026.568
[43,     1] loss: 1087.376
[44,     1] loss: 1081.492
[45,     1] loss: 973.318
[46,     1] loss: 1042.010
[47,     1] loss: 935.063
[48,     1] loss: 1019.223
[49,     1] loss: 1048.087
[50,     1] loss: 945.361
[51,     1] loss: 1094.504
[52,     1] loss: 888.480
[53,     1] loss: 997.065
[54,     1] loss: 885.539
[55,     1] loss: 972.792
[56,     1] loss: 851.824
[57,     1] loss: 1055.462
[58,     1] loss: 1032.554
[59,     1] loss: 808.357
[60,     1] loss: 902.692
[61,     1] loss: 868.563
[62,     1] loss: 913.000
[63,     1] loss: 810.000
[64,     1] loss: 852.221
[65,     1] loss: 748.899
[66,     1] loss: 748.674
[67,     1] loss: 1173.646
[68,     1] loss: 971.000
[69,     1] loss: 797.643
[70,     1] loss: 906.296
[71,     1] loss: 874.501
[72,     1] loss: 844.804
[73,     1] loss: 745.742
[74,     1] loss: 791.544
[75,     1] loss: 687.655
[76,     1] loss: 747.054
[77,     1] loss: 665.988
[78,     1] loss: 747.033
[79,     1] loss: 629.061
[80,     1] loss: 908.143
[81,     1] loss: 917.261
[82,     1] loss: 1020.289
[83,     1] loss: 772.301
[84,     1] loss: 871.064
[85,     1] loss: 748.639
[86,     1] loss: 773.588
[87,     1] loss: 670.149
[88,     1] loss: 811.000
[89,     1] loss: 696.478
[90,     1] loss: 627.500
[91,     1] loss: 691.737
[92,     1] loss: 670.368
[93,     1] loss: 693.789
[94,     1] loss: 636.864
[95,     1] loss: 645.988
[96,     1] loss: 651.478
[97,     1] loss: 529.360
[98,     1] loss: 632.965
[99,     1] loss: 1564.662
[100,     1] loss: 1586.140
[101,     1] loss: 1073.707
[102,     1] loss: 1208.744
[103,     1] loss: 1209.866
Early stopping applied (best metric=0.3931710720062256)
Finished Training
Total time taken: 14.776579141616821
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1492.552
[2,     1] loss: 1492.461
[3,     1] loss: 1491.289
[4,     1] loss: 1489.433
[5,     1] loss: 1483.393
[6,     1] loss: 1479.390
[7,     1] loss: 1470.116
[8,     1] loss: 1445.943
[9,     1] loss: 1401.005
[10,     1] loss: 1382.409
[11,     1] loss: 1325.437
[12,     1] loss: 1297.149
[13,     1] loss: 1246.198
[14,     1] loss: 1264.216
[15,     1] loss: 1244.845
[16,     1] loss: 1217.572
[17,     1] loss: 1250.828
[18,     1] loss: 1280.214
[19,     1] loss: 1182.868
[20,     1] loss: 1205.382
[21,     1] loss: 1164.548
[22,     1] loss: 1124.092
[23,     1] loss: 1173.730
[24,     1] loss: 1165.319
[25,     1] loss: 1081.005
[26,     1] loss: 1139.154
[27,     1] loss: 1120.168
[28,     1] loss: 1122.360
[29,     1] loss: 1055.212
[30,     1] loss: 1086.946
[31,     1] loss: 1044.361
[32,     1] loss: 1094.903
[33,     1] loss: 1038.841
[34,     1] loss: 1090.070
[35,     1] loss: 1032.190
[36,     1] loss: 1045.508
[37,     1] loss: 1069.282
[38,     1] loss: 947.811
[39,     1] loss: 1031.599
[40,     1] loss: 887.946
[41,     1] loss: 889.139
[42,     1] loss: 895.245
[43,     1] loss: 964.334
[44,     1] loss: 1097.881
[45,     1] loss: 1192.453
[46,     1] loss: 910.061
[47,     1] loss: 1103.059
[48,     1] loss: 977.463
[49,     1] loss: 1011.736
[50,     1] loss: 1010.948
[51,     1] loss: 887.006
[52,     1] loss: 967.424
[53,     1] loss: 951.793
[54,     1] loss: 872.345
[55,     1] loss: 1033.481
[56,     1] loss: 935.304
[57,     1] loss: 933.754
[58,     1] loss: 909.289
[59,     1] loss: 889.445
[60,     1] loss: 869.199
[61,     1] loss: 842.259
[62,     1] loss: 832.443
[63,     1] loss: 771.705
[64,     1] loss: 715.968
[65,     1] loss: 801.898
[66,     1] loss: 1007.221
[67,     1] loss: 1772.811
[68,     1] loss: 800.189
[69,     1] loss: 1146.216
[70,     1] loss: 1120.293
[71,     1] loss: 1134.647
[72,     1] loss: 1130.791
[73,     1] loss: 1147.507
[74,     1] loss: 1127.747
[75,     1] loss: 1079.662
[76,     1] loss: 1079.260
[77,     1] loss: 1016.116
[78,     1] loss: 1036.013
[79,     1] loss: 979.957
[80,     1] loss: 1033.445
[81,     1] loss: 996.948
[82,     1] loss: 935.094
[83,     1] loss: 919.693
[84,     1] loss: 874.476
[85,     1] loss: 899.522
[86,     1] loss: 845.524
[87,     1] loss: 757.912
[88,     1] loss: 728.184
[89,     1] loss: 746.124
[90,     1] loss: 805.928
[91,     1] loss: 868.679
[92,     1] loss: 845.489
[93,     1] loss: 668.935
[94,     1] loss: 887.334
[95,     1] loss: 1097.009
[96,     1] loss: 760.400
[97,     1] loss: 986.235
[98,     1] loss: 836.736
[99,     1] loss: 848.385
[100,     1] loss: 794.004
[101,     1] loss: 686.449
[102,     1] loss: 927.125
[103,     1] loss: 810.106
[104,     1] loss: 668.389
[105,     1] loss: 772.831
[106,     1] loss: 636.784
[107,     1] loss: 663.043
[108,     1] loss: 643.196
[109,     1] loss: 729.979
[110,     1] loss: 1059.079
[111,     1] loss: 1083.954
[112,     1] loss: 949.942
[113,     1] loss: 1041.047
[114,     1] loss: 884.566
[115,     1] loss: 921.209
[116,     1] loss: 915.895
[117,     1] loss: 885.108
Early stopping applied (best metric=0.3623548746109009)
Finished Training
Total time taken: 17.087825298309326
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1490.167
[2,     1] loss: 1497.376
[3,     1] loss: 1483.994
[4,     1] loss: 1488.656
[5,     1] loss: 1497.717
[6,     1] loss: 1494.475
[7,     1] loss: 1484.726
[8,     1] loss: 1481.900
[9,     1] loss: 1482.550
[10,     1] loss: 1469.933
[11,     1] loss: 1462.507
[12,     1] loss: 1438.441
[13,     1] loss: 1411.577
[14,     1] loss: 1371.537
[15,     1] loss: 1333.379
[16,     1] loss: 1326.078
[17,     1] loss: 1312.107
[18,     1] loss: 1260.458
[19,     1] loss: 1309.262
[20,     1] loss: 1266.911
[21,     1] loss: 1261.555
[22,     1] loss: 1269.502
[23,     1] loss: 1210.074
[24,     1] loss: 1245.338
[25,     1] loss: 1144.490
[26,     1] loss: 1249.361
[27,     1] loss: 1152.710
[28,     1] loss: 1215.677
[29,     1] loss: 1143.221
[30,     1] loss: 1092.388
[31,     1] loss: 1096.922
[32,     1] loss: 1100.795
[33,     1] loss: 1104.259
[34,     1] loss: 1100.051
[35,     1] loss: 1077.432
[36,     1] loss: 1146.755
[37,     1] loss: 1057.167
[38,     1] loss: 1014.505
[39,     1] loss: 1137.470
[40,     1] loss: 1035.781
[41,     1] loss: 1038.682
[42,     1] loss: 993.060
[43,     1] loss: 1014.536
[44,     1] loss: 966.128
[45,     1] loss: 917.853
[46,     1] loss: 910.932
[47,     1] loss: 1085.172
[48,     1] loss: 1476.133
[49,     1] loss: 864.169
[50,     1] loss: 1272.739
[51,     1] loss: 1006.854
[52,     1] loss: 1038.818
[53,     1] loss: 1131.911
[54,     1] loss: 1143.245
[55,     1] loss: 1093.448
[56,     1] loss: 1037.271
[57,     1] loss: 1045.451
[58,     1] loss: 1136.580
[59,     1] loss: 972.855
[60,     1] loss: 1008.840
[61,     1] loss: 997.563
[62,     1] loss: 913.278
[63,     1] loss: 983.785
[64,     1] loss: 883.157
[65,     1] loss: 905.550
[66,     1] loss: 847.520
[67,     1] loss: 960.747
[68,     1] loss: 888.874
[69,     1] loss: 786.741
[70,     1] loss: 811.807
[71,     1] loss: 732.291
[72,     1] loss: 758.644
[73,     1] loss: 855.680
[74,     1] loss: 940.830
[75,     1] loss: 797.119
[76,     1] loss: 759.415
[77,     1] loss: 784.231
[78,     1] loss: 776.536
[79,     1] loss: 700.464
[80,     1] loss: 689.295
[81,     1] loss: 802.103
[82,     1] loss: 1030.436
[83,     1] loss: 1265.642
[84,     1] loss: 1021.793
[85,     1] loss: 1116.320
[86,     1] loss: 941.293
[87,     1] loss: 1066.802
[88,     1] loss: 1223.470
[89,     1] loss: 1013.443
[90,     1] loss: 958.813
[91,     1] loss: 1041.812
[92,     1] loss: 934.566
[93,     1] loss: 906.694
[94,     1] loss: 1091.051
Early stopping applied (best metric=0.35946109890937805)
Finished Training
Total time taken: 13.503443241119385
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1491.077
[2,     1] loss: 1495.201
[3,     1] loss: 1491.393
[4,     1] loss: 1489.347
[5,     1] loss: 1487.657
[6,     1] loss: 1486.826
[7,     1] loss: 1487.023
[8,     1] loss: 1484.734
[9,     1] loss: 1483.698
[10,     1] loss: 1477.606
[11,     1] loss: 1473.139
[12,     1] loss: 1447.977
[13,     1] loss: 1416.667
[14,     1] loss: 1378.051
[15,     1] loss: 1353.779
[16,     1] loss: 1265.079
[17,     1] loss: 1324.887
[18,     1] loss: 1238.377
[19,     1] loss: 1344.093
[20,     1] loss: 1227.295
[21,     1] loss: 1247.670
[22,     1] loss: 1253.030
[23,     1] loss: 1224.432
[24,     1] loss: 1186.358
[25,     1] loss: 1254.736
[26,     1] loss: 1215.010
[27,     1] loss: 1214.291
[28,     1] loss: 1207.064
[29,     1] loss: 1146.001
[30,     1] loss: 1177.004
[31,     1] loss: 1155.847
[32,     1] loss: 1120.933
[33,     1] loss: 1095.411
[34,     1] loss: 1145.008
[35,     1] loss: 1134.158
[36,     1] loss: 1065.373
[37,     1] loss: 1060.328
[38,     1] loss: 1040.061
[39,     1] loss: 1005.045
[40,     1] loss: 988.964
[41,     1] loss: 983.918
[42,     1] loss: 947.555
[43,     1] loss: 938.237
[44,     1] loss: 1089.815
[45,     1] loss: 946.895
[46,     1] loss: 1004.950
[47,     1] loss: 944.716
[48,     1] loss: 1013.925
[49,     1] loss: 870.874
[50,     1] loss: 1112.388
[51,     1] loss: 944.152
[52,     1] loss: 1104.954
[53,     1] loss: 910.066
[54,     1] loss: 1059.948
[55,     1] loss: 872.705
[56,     1] loss: 1059.378
[57,     1] loss: 904.211
[58,     1] loss: 918.784
[59,     1] loss: 925.104
[60,     1] loss: 957.687
[61,     1] loss: 882.726
[62,     1] loss: 823.421
[63,     1] loss: 921.043
[64,     1] loss: 837.025
[65,     1] loss: 835.244
[66,     1] loss: 802.036
[67,     1] loss: 767.923
[68,     1] loss: 790.886
[69,     1] loss: 771.420
[70,     1] loss: 695.395
[71,     1] loss: 681.636
[72,     1] loss: 678.757
[73,     1] loss: 898.216
[74,     1] loss: 1677.901
[75,     1] loss: 1210.826
[76,     1] loss: 1117.031
[77,     1] loss: 1050.387
[78,     1] loss: 1157.130
[79,     1] loss: 1237.480
[80,     1] loss: 1270.694
[81,     1] loss: 1224.161
[82,     1] loss: 1195.054
[83,     1] loss: 1171.801
[84,     1] loss: 1127.048
[85,     1] loss: 1116.833
[86,     1] loss: 1137.025
[87,     1] loss: 1071.852
[88,     1] loss: 1059.314
[89,     1] loss: 1073.971
[90,     1] loss: 1064.979
[91,     1] loss: 1027.488
[92,     1] loss: 1038.229
[93,     1] loss: 1018.453
[94,     1] loss: 1023.883
[95,     1] loss: 1025.245
[96,     1] loss: 1009.478
[97,     1] loss: 971.839
[98,     1] loss: 971.852
[99,     1] loss: 956.292
[100,     1] loss: 889.349
[101,     1] loss: 915.581
[102,     1] loss: 891.182
[103,     1] loss: 892.103
[104,     1] loss: 1167.133
[105,     1] loss: 1423.046
[106,     1] loss: 993.540
[107,     1] loss: 1089.765
[108,     1] loss: 1192.897
[109,     1] loss: 1175.236
[110,     1] loss: 1111.259
[111,     1] loss: 1064.383
[112,     1] loss: 1106.173
[113,     1] loss: 970.661
[114,     1] loss: 986.059
[115,     1] loss: 976.019
[116,     1] loss: 964.356
[117,     1] loss: 900.727
[118,     1] loss: 899.121
[119,     1] loss: 883.106
Early stopping applied (best metric=0.33218398690223694)
Finished Training
Total time taken: 17.09982681274414
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1492.973
[2,     1] loss: 1496.445
[3,     1] loss: 1487.340
[4,     1] loss: 1495.634
[5,     1] loss: 1490.427
[6,     1] loss: 1489.434
[7,     1] loss: 1488.591
[8,     1] loss: 1492.198
[9,     1] loss: 1484.959
[10,     1] loss: 1482.347
[11,     1] loss: 1479.025
[12,     1] loss: 1458.542
[13,     1] loss: 1438.079
[14,     1] loss: 1382.416
[15,     1] loss: 1356.170
[16,     1] loss: 1333.711
[17,     1] loss: 1285.782
[18,     1] loss: 1207.278
[19,     1] loss: 1194.968
[20,     1] loss: 1254.674
[21,     1] loss: 1175.190
[22,     1] loss: 1258.677
[23,     1] loss: 1213.637
[24,     1] loss: 1208.070
[25,     1] loss: 1188.107
[26,     1] loss: 1198.049
[27,     1] loss: 1166.518
[28,     1] loss: 1192.509
[29,     1] loss: 1104.823
[30,     1] loss: 1100.093
[31,     1] loss: 1082.428
[32,     1] loss: 1124.695
[33,     1] loss: 1210.881
[34,     1] loss: 995.353
[35,     1] loss: 1059.661
[36,     1] loss: 1065.652
[37,     1] loss: 1004.299
[38,     1] loss: 1002.034
[39,     1] loss: 1070.729
[40,     1] loss: 1173.524
[41,     1] loss: 1195.039
[42,     1] loss: 981.157
[43,     1] loss: 1112.906
[44,     1] loss: 1040.353
[45,     1] loss: 1048.381
[46,     1] loss: 1030.303
[47,     1] loss: 1012.948
[48,     1] loss: 992.630
[49,     1] loss: 958.228
[50,     1] loss: 875.378
[51,     1] loss: 912.517
[52,     1] loss: 937.810
[53,     1] loss: 880.036
[54,     1] loss: 860.220
[55,     1] loss: 1096.858
[56,     1] loss: 1043.129
[57,     1] loss: 901.420
[58,     1] loss: 975.118
[59,     1] loss: 857.876
[60,     1] loss: 930.122
[61,     1] loss: 830.799
[62,     1] loss: 859.684
[63,     1] loss: 776.737
[64,     1] loss: 878.325
[65,     1] loss: 1172.637
[66,     1] loss: 819.885
[67,     1] loss: 865.566
[68,     1] loss: 846.093
[69,     1] loss: 856.964
[70,     1] loss: 779.853
[71,     1] loss: 807.702
[72,     1] loss: 703.716
[73,     1] loss: 670.280
[74,     1] loss: 645.537
[75,     1] loss: 758.417
[76,     1] loss: 1320.961
[77,     1] loss: 1555.214
[78,     1] loss: 1157.172
[79,     1] loss: 1024.053
[80,     1] loss: 1122.561
[81,     1] loss: 1294.092
[82,     1] loss: 1193.942
[83,     1] loss: 1199.314
[84,     1] loss: 1211.022
[85,     1] loss: 1185.762
[86,     1] loss: 1100.289
[87,     1] loss: 1117.170
[88,     1] loss: 1090.553
[89,     1] loss: 1067.488
[90,     1] loss: 1047.534
[91,     1] loss: 1078.071
[92,     1] loss: 1079.047
[93,     1] loss: 1085.665
[94,     1] loss: 1047.988
[95,     1] loss: 1026.266
[96,     1] loss: 1057.348
[97,     1] loss: 969.527
[98,     1] loss: 1046.111
[99,     1] loss: 928.630
[100,     1] loss: 972.244
[101,     1] loss: 888.180
[102,     1] loss: 924.669
[103,     1] loss: 894.839
[104,     1] loss: 828.156
[105,     1] loss: 834.810
[106,     1] loss: 818.813
[107,     1] loss: 849.165
[108,     1] loss: 885.779
[109,     1] loss: 1292.151
[110,     1] loss: 856.161
[111,     1] loss: 964.264
Early stopping applied (best metric=0.33801668882369995)
Finished Training
Total time taken: 15.965704679489136
{'Hydroxylation-K Validation Accuracy': 0.7576241134751773, 'Hydroxylation-K Validation Sensitivity': 0.66, 'Hydroxylation-K Validation Specificity': 0.7824561403508772, 'Hydroxylation-K Validation Precision': 0.46418210931368825, 'Hydroxylation-K AUC ROC': 0.7860233918128655, 'Hydroxylation-K AUC PR': 0.5835144167191636, 'Hydroxylation-K MCC': 0.40002536047032466, 'Hydroxylation-K F1': 0.5354168843860314, 'Validation Loss (Hydroxylation-K)': 0.47083946466445925, 'Hydroxylation-P Validation Accuracy': 0.8056544168654721, 'Hydroxylation-P Validation Sensitivity': 0.7916402116402117, 'Hydroxylation-P Validation Specificity': 0.8086712554242107, 'Hydroxylation-P Validation Precision': 0.4794208179799375, 'Hydroxylation-P AUC ROC': 0.8479351679654688, 'Hydroxylation-P AUC PR': 0.5844816547959891, 'Hydroxylation-P MCC': 0.5060374564275537, 'Hydroxylation-P F1': 0.5926182931228787, 'Validation Loss (Hydroxylation-P)': 0.3644265274206797, 'Validation Loss (total)': 0.8352659940719604, 'TimeToTrain': 13.57771741549174}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004177917331050973,
 'learning_rate_Hydroxylation-K': 0.005063718162959047,
 'learning_rate_Hydroxylation-P': 0.008067795074835918,
 'log_base': 2.999599001613023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2079304052,
 'sample_weights': [2.7627108053509146, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.5362877864513651,
 'weight_decay_Hydroxylation-K': 1.9880173224782238,
 'weight_decay_Hydroxylation-P': 4.004858184473348}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1229.123
[2,     1] loss: 1225.576
[3,     1] loss: 1234.363
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006390304319046848,
 'learning_rate_Hydroxylation-K': 0.003527324433041091,
 'learning_rate_Hydroxylation-P': 0.005879333381426198,
 'log_base': 1.1863381046583097,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1788273294,
 'sample_weights': [1.5197775602975718, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.882528162237161,
 'weight_decay_Hydroxylation-K': 3.681894415623543,
 'weight_decay_Hydroxylation-P': 5.122305636648231}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3164.308
[2,     1] loss: 3172.720
[3,     1] loss: 3187.326
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009455035692690829,
 'learning_rate_Hydroxylation-K': 0.003875978342094131,
 'learning_rate_Hydroxylation-P': 0.0041812245207801515,
 'log_base': 1.642755392232099,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1091615248,
 'sample_weights': [9.770176495121653, 1.2213191101942062],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.17482282605953,
 'weight_decay_Hydroxylation-K': 9.300216604436988,
 'weight_decay_Hydroxylation-P': 6.915873487021832}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1622.188
[2,     1] loss: 1625.194
[3,     1] loss: 1616.390
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0018720288176197,
 'learning_rate_Hydroxylation-K': 0.005580330208793693,
 'learning_rate_Hydroxylation-P': 0.00604162325970417,
 'log_base': 1.243639693000929,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 559337735,
 'sample_weights': [3.3632703463462645, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.44412568728938,
 'weight_decay_Hydroxylation-K': 9.135472404077772,
 'weight_decay_Hydroxylation-P': 2.6955199753251504}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2525.489
[2,     1] loss: 2524.966
[3,     1] loss: 2521.992
[4,     1] loss: 2517.962
[5,     1] loss: 2525.593
[6,     1] loss: 2515.844
[7,     1] loss: 2516.346
[8,     1] loss: 2512.549
[9,     1] loss: 2507.638
[10,     1] loss: 2501.127
[11,     1] loss: 2502.994
[12,     1] loss: 2486.410
[13,     1] loss: 2479.479
[14,     1] loss: 2451.043
[15,     1] loss: 2415.123
[16,     1] loss: 2357.839
[17,     1] loss: 2321.459
[18,     1] loss: 2330.272
[19,     1] loss: 2272.544
[20,     1] loss: 2236.786
[21,     1] loss: 2221.845
[22,     1] loss: 2195.746
[23,     1] loss: 2135.304
[24,     1] loss: 2127.490
[25,     1] loss: 2056.157
[26,     1] loss: 2111.480
[27,     1] loss: 2027.956
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004856869044581142,
 'learning_rate_Hydroxylation-K': 0.006302614868311586,
 'learning_rate_Hydroxylation-P': 0.008598949263331962,
 'log_base': 2.7165044412575465,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2042110709,
 'sample_weights': [7.65650986524683, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3474381721712776,
 'weight_decay_Hydroxylation-K': 4.185728894728988,
 'weight_decay_Hydroxylation-P': 3.646925877623759}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.892
[2,     1] loss: 1259.922
[3,     1] loss: 1269.658
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009676651153976772,
 'learning_rate_Hydroxylation-K': 0.009831448574373135,
 'learning_rate_Hydroxylation-P': 0.0002894648000794476,
 'log_base': 1.3593396487129117,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1404494399,
 'sample_weights': [1.6705358082657953, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.509242530302119,
 'weight_decay_Hydroxylation-K': 8.124215021256623,
 'weight_decay_Hydroxylation-P': 5.888185037807637}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2069.641
[2,     1] loss: 2068.843
[3,     1] loss: 2051.115
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004821244519056964,
 'learning_rate_Hydroxylation-K': 0.004647455913758895,
 'learning_rate_Hydroxylation-P': 0.007299398500028162,
 'log_base': 1.1618762225649104,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3159139287,
 'sample_weights': [5.437942753825292, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.027314233450934,
 'weight_decay_Hydroxylation-K': 9.69439358661182,
 'weight_decay_Hydroxylation-P': 3.495847702109496}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3604.573
[2,     1] loss: 3613.271
[3,     1] loss: 3591.831
[4,     1] loss: 3593.953
[5,     1] loss: 3641.725
[6,     1] loss: 3609.845
[7,     1] loss: 3587.103
[8,     1] loss: 3598.568
[9,     1] loss: 3588.667
[10,     1] loss: 3579.520
[11,     1] loss: 3555.848
[12,     1] loss: 3541.757
[13,     1] loss: 3516.170
[14,     1] loss: 3429.846
[15,     1] loss: 3445.110
[16,     1] loss: 3373.771
[17,     1] loss: 3299.592
[18,     1] loss: 3238.030
[19,     1] loss: 3133.023
[20,     1] loss: 3243.823
[21,     1] loss: 3132.429
[22,     1] loss: 3043.106
[23,     1] loss: 3108.756
[24,     1] loss: 3124.091
[25,     1] loss: 3036.740
[26,     1] loss: 3062.849
[27,     1] loss: 3108.308
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005563823461084792,
 'learning_rate_Hydroxylation-K': 0.0063785358597048085,
 'learning_rate_Hydroxylation-P': 0.009145408728485727,
 'log_base': 2.321348334601397,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1843684494,
 'sample_weights': [11.12694074555346, 1.3909211749990806],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8429686691109683,
 'weight_decay_Hydroxylation-K': 6.719389758352748,
 'weight_decay_Hydroxylation-P': 3.322441816781725}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1329.646
[2,     1] loss: 1333.382
[3,     1] loss: 1329.365
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00748969367210729,
 'learning_rate_Hydroxylation-K': 0.009568091576097858,
 'learning_rate_Hydroxylation-P': 0.0013536792731421224,
 'log_base': 1.0230735645009026,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2957928135,
 'sample_weights': [1.9823626722849084, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.55026002098709,
 'weight_decay_Hydroxylation-K': 8.472627624362877,
 'weight_decay_Hydroxylation-P': 6.073683213299366}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23737.484
Exploding loss, terminate run (best metric=0.5331806540489197)
Finished Training
Total time taken: 0.22002458572387695
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23767.652
Exploding loss, terminate run (best metric=0.5278763175010681)
Finished Training
Total time taken: 0.21102142333984375
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23733.941
Exploding loss, terminate run (best metric=0.5326206088066101)
Finished Training
Total time taken: 0.22902154922485352
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23806.000
Exploding loss, terminate run (best metric=0.5405060648918152)
Finished Training
Total time taken: 0.21302461624145508
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23847.039
Exploding loss, terminate run (best metric=0.531434178352356)
Finished Training
Total time taken: 0.22902202606201172
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23705.656
Exploding loss, terminate run (best metric=0.5323202610015869)
Finished Training
Total time taken: 0.20701861381530762
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23913.059
Exploding loss, terminate run (best metric=0.5335055589675903)
Finished Training
Total time taken: 0.22602486610412598
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23706.645
Exploding loss, terminate run (best metric=0.5268287658691406)
Finished Training
Total time taken: 0.20502257347106934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23856.258
Exploding loss, terminate run (best metric=0.5327813029289246)
Finished Training
Total time taken: 0.199021577835083
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23810.338
Exploding loss, terminate run (best metric=0.5283839106559753)
Finished Training
Total time taken: 0.20002055168151855
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23836.219
Exploding loss, terminate run (best metric=0.5323416590690613)
Finished Training
Total time taken: 0.21302294731140137
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23733.918
Exploding loss, terminate run (best metric=0.5283356308937073)
Finished Training
Total time taken: 0.2130262851715088
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23880.807
Exploding loss, terminate run (best metric=0.5407307147979736)
Finished Training
Total time taken: 0.21802306175231934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 23803.619
Exploding loss, terminate run (best metric=0.5415570139884949)
Finished Training
Total time taken: 0.21802306175231934
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 23895.885
Exploding loss, terminate run (best metric=0.5508132576942444)
Finished Training
Total time taken: 0.22302460670471191
{'Hydroxylation-K Validation Accuracy': 0.5697695035460992, 'Hydroxylation-K Validation Sensitivity': 0.44, 'Hydroxylation-K Validation Specificity': 0.5982456140350877, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6745321637426901, 'Hydroxylation-K AUC PR': 0.37561686423314783, 'Hydroxylation-K MCC': 0.04400808673350896, 'Hydroxylation-K F1': 0.18342992369361538, 'Validation Loss (Hydroxylation-K)': 0.559959348042806, 'Hydroxylation-P Validation Accuracy': 0.5694133461922407, 'Hydroxylation-P Validation Sensitivity': 0.4203703703703704, 'Hydroxylation-P Validation Specificity': 0.6008429348097162, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.6317097436676967, 'Hydroxylation-P AUC PR': 0.3163439941414066, 'Hydroxylation-P MCC': 0.028281250323293906, 'Hydroxylation-P F1': 0.14735415884599912, 'Validation Loss (Hydroxylation-P)': 0.5342143932978313, 'Validation Loss (total)': 1.094173749287923, 'TimeToTrain': 0.21495615641276042}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003028440479982201,
 'learning_rate_Hydroxylation-K': 0.00544930637679217,
 'learning_rate_Hydroxylation-P': 0.006795329050983842,
 'log_base': 1.6345020152359757,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2837436232,
 'sample_weights': [73.23890283405123, 9.1358406667292],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.6592239267888695,
 'weight_decay_Hydroxylation-K': 4.697827689250337,
 'weight_decay_Hydroxylation-P': 5.749292489524567}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1625.465
[2,     1] loss: 1629.958
[3,     1] loss: 1625.646
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 9.666302177250511e-05,
 'learning_rate_Hydroxylation-K': 0.006684637388882576,
 'learning_rate_Hydroxylation-P': 0.0024769261979797966,
 'log_base': 1.7400932627241175,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 9410067,
 'sample_weights': [3.39774765116066, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.942831623264134,
 'weight_decay_Hydroxylation-K': 0.9965421715163894,
 'weight_decay_Hydroxylation-P': 1.619600742827878}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1552.902
[2,     1] loss: 1549.608
[3,     1] loss: 1556.460
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003325694835713345,
 'learning_rate_Hydroxylation-K': 0.001154886224822903,
 'learning_rate_Hydroxylation-P': 0.0033795842030365786,
 'log_base': 1.8935011368604984,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1834986668,
 'sample_weights': [3.013768696971501, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.692008353589873,
 'weight_decay_Hydroxylation-K': 7.983672238169282,
 'weight_decay_Hydroxylation-P': 0.3929287370856487}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1461.025
[2,     1] loss: 1461.148
[3,     1] loss: 1457.686
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035545367858984965,
 'learning_rate_Hydroxylation-K': 0.007437668008901143,
 'learning_rate_Hydroxylation-P': 0.006478931178140248,
 'log_base': 1.1611597222976338,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1517990667,
 'sample_weights': [2.614929600648558, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.676205963225438,
 'weight_decay_Hydroxylation-K': 7.950995095983003,
 'weight_decay_Hydroxylation-P': 2.3179486326268126}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3624.832
[2,     1] loss: 3637.160
[3,     1] loss: 3649.221
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007509934592109926,
 'learning_rate_Hydroxylation-K': 0.005154535667265175,
 'learning_rate_Hydroxylation-P': 0.006118061205128428,
 'log_base': 1.452647438705853,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 182633671,
 'sample_weights': [11.172877426411354, 1.3966634813144938],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.006403508443864,
 'weight_decay_Hydroxylation-K': 4.349604403925893,
 'weight_decay_Hydroxylation-P': 6.82671036239061}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1854.338
[2,     1] loss: 1867.471
[3,     1] loss: 1851.240
[4,     1] loss: 1855.641
[5,     1] loss: 1855.187
[6,     1] loss: 1847.443
[7,     1] loss: 1847.647
[8,     1] loss: 1853.445
[9,     1] loss: 1852.907
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003351140855778013,
 'learning_rate_Hydroxylation-K': 0.0022063295892401783,
 'learning_rate_Hydroxylation-P': 0.0045130444427366,
 'log_base': 2.4482851376311827,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2115269323,
 'sample_weights': [4.471071479442033, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.825786755433866,
 'weight_decay_Hydroxylation-K': 9.275045372539394,
 'weight_decay_Hydroxylation-P': 2.826400919007724}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1302.819
[2,     1] loss: 1304.397
[3,     1] loss: 1301.293
[4,     1] loss: 1301.586
[5,     1] loss: 1303.376
[6,     1] loss: 1298.553
[7,     1] loss: 1292.879
[8,     1] loss: 1287.159
[9,     1] loss: 1275.101
[10,     1] loss: 1252.893
[11,     1] loss: 1218.570
[12,     1] loss: 1206.736
[13,     1] loss: 1170.596
[14,     1] loss: 1130.212
[15,     1] loss: 1099.334
[16,     1] loss: 1139.912
[17,     1] loss: 1068.806
[18,     1] loss: 1081.285
[19,     1] loss: 1079.195
[20,     1] loss: 1050.933
[21,     1] loss: 1078.044
[22,     1] loss: 1078.681
[23,     1] loss: 1038.497
[24,     1] loss: 994.624
[25,     1] loss: 1008.698
[26,     1] loss: 1065.767
[27,     1] loss: 1042.814
[28,     1] loss: 986.275
[29,     1] loss: 1005.474
[30,     1] loss: 970.731
[31,     1] loss: 1019.213
[32,     1] loss: 974.844
[33,     1] loss: 954.369
[34,     1] loss: 982.149
[35,     1] loss: 966.935
[36,     1] loss: 925.873
[37,     1] loss: 988.393
[38,     1] loss: 994.207
[39,     1] loss: 958.884
[40,     1] loss: 970.079
[41,     1] loss: 946.831
[42,     1] loss: 972.566
[43,     1] loss: 911.807
[44,     1] loss: 955.039
[45,     1] loss: 922.395
[46,     1] loss: 897.500
[47,     1] loss: 820.526
[48,     1] loss: 843.772
[49,     1] loss: 865.937
[50,     1] loss: 925.214
[51,     1] loss: 810.067
[52,     1] loss: 852.997
[53,     1] loss: 790.025
[54,     1] loss: 809.137
[55,     1] loss: 860.882
[56,     1] loss: 771.837
[57,     1] loss: 753.144
[58,     1] loss: 765.850
[59,     1] loss: 757.143
[60,     1] loss: 697.894
[61,     1] loss: 678.939
[62,     1] loss: 718.593
[63,     1] loss: 922.232
[64,     1] loss: 1397.451
[65,     1] loss: 763.158
[66,     1] loss: 1090.013
[67,     1] loss: 949.247
[68,     1] loss: 936.288
[69,     1] loss: 985.110
[70,     1] loss: 1003.086
[71,     1] loss: 981.319
[72,     1] loss: 959.158
[73,     1] loss: 874.310
[74,     1] loss: 924.974
[75,     1] loss: 946.854
[76,     1] loss: 855.218
[77,     1] loss: 849.586
[78,     1] loss: 829.427
[79,     1] loss: 852.460
[80,     1] loss: 795.020
[81,     1] loss: 832.494
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004284103829735876,
 'learning_rate_Hydroxylation-K': 0.002739970591955417,
 'learning_rate_Hydroxylation-P': 0.008596770411913913,
 'log_base': 2.854304870087504,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 582105846,
 'sample_weights': [1.8644916548342563, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.7791183149726586,
 'weight_decay_Hydroxylation-K': 9.007076429285629,
 'weight_decay_Hydroxylation-P': 5.208368481867427}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1242.805
[2,     1] loss: 1244.677
[3,     1] loss: 1244.932
[4,     1] loss: 1242.083
[5,     1] loss: 1238.077
[6,     1] loss: 1239.495
[7,     1] loss: 1234.459
[8,     1] loss: 1228.854
[9,     1] loss: 1210.568
[10,     1] loss: 1187.178
[11,     1] loss: 1157.407
[12,     1] loss: 1121.242
[13,     1] loss: 1084.206
[14,     1] loss: 1091.334
[15,     1] loss: 1104.615
[16,     1] loss: 1084.635
[17,     1] loss: 1088.854
[18,     1] loss: 1108.906
[19,     1] loss: 1018.297
[20,     1] loss: 1062.121
[21,     1] loss: 1022.693
[22,     1] loss: 1014.516
[23,     1] loss: 1021.870
[24,     1] loss: 1030.215
[25,     1] loss: 1026.772
[26,     1] loss: 1000.319
[27,     1] loss: 966.345
[28,     1] loss: 971.087
[29,     1] loss: 990.053
[30,     1] loss: 926.904
[31,     1] loss: 973.163
[32,     1] loss: 950.726
[33,     1] loss: 958.309
[34,     1] loss: 969.005
[35,     1] loss: 934.486
[36,     1] loss: 905.532
[37,     1] loss: 908.726
[38,     1] loss: 913.471
[39,     1] loss: 912.783
[40,     1] loss: 941.512
[41,     1] loss: 934.682
[42,     1] loss: 861.437
[43,     1] loss: 864.243
[44,     1] loss: 871.859
[45,     1] loss: 879.221
[46,     1] loss: 873.940
[47,     1] loss: 897.761
[48,     1] loss: 988.584
[49,     1] loss: 849.305
[50,     1] loss: 915.839
[51,     1] loss: 895.523
[52,     1] loss: 858.137
[53,     1] loss: 817.179
[54,     1] loss: 850.919
[55,     1] loss: 841.482
[56,     1] loss: 819.224
[57,     1] loss: 829.619
[58,     1] loss: 763.170
[59,     1] loss: 848.323
[60,     1] loss: 831.822
[61,     1] loss: 821.641
[62,     1] loss: 813.000
[63,     1] loss: 774.288
[64,     1] loss: 783.602
[65,     1] loss: 804.570
[66,     1] loss: 770.554
[67,     1] loss: 768.872
[68,     1] loss: 777.683
[69,     1] loss: 774.608
[70,     1] loss: 710.358
[71,     1] loss: 714.568
[72,     1] loss: 804.233
[73,     1] loss: 739.245
[74,     1] loss: 752.375
[75,     1] loss: 731.931
[76,     1] loss: 763.945
[77,     1] loss: 695.085
[78,     1] loss: 723.994
[79,     1] loss: 695.039
[80,     1] loss: 724.426
[81,     1] loss: 671.929
[82,     1] loss: 732.331
[83,     1] loss: 692.930
[84,     1] loss: 626.626
[85,     1] loss: 646.196
[86,     1] loss: 629.880
[87,     1] loss: 644.488
[88,     1] loss: 638.394
[89,     1] loss: 646.548
[90,     1] loss: 629.452
[91,     1] loss: 604.559
[92,     1] loss: 624.693
[93,     1] loss: 582.726
[94,     1] loss: 589.696
[95,     1] loss: 580.180
[96,     1] loss: 606.722
[97,     1] loss: 557.442
[98,     1] loss: 580.432
[99,     1] loss: 557.841
[100,     1] loss: 630.170
[101,     1] loss: 535.937
[102,     1] loss: 560.832
[103,     1] loss: 531.748
[104,     1] loss: 547.494
[105,     1] loss: 566.634
Early stopping applied (best metric=0.28937816619873047)
Finished Training
Total time taken: 15.09461522102356
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1243.683
[2,     1] loss: 1243.882
[3,     1] loss: 1241.890
[4,     1] loss: 1245.648
[5,     1] loss: 1242.055
[6,     1] loss: 1242.209
[7,     1] loss: 1237.310
[8,     1] loss: 1236.536
[9,     1] loss: 1231.239
[10,     1] loss: 1222.317
[11,     1] loss: 1205.889
[12,     1] loss: 1172.244
[13,     1] loss: 1137.899
[14,     1] loss: 1113.909
[15,     1] loss: 1080.426
[16,     1] loss: 1040.920
[17,     1] loss: 1036.565
[18,     1] loss: 1034.882
[19,     1] loss: 1007.346
[20,     1] loss: 1058.809
[21,     1] loss: 1041.072
[22,     1] loss: 991.759
[23,     1] loss: 1006.659
[24,     1] loss: 950.636
[25,     1] loss: 979.701
[26,     1] loss: 974.435
[27,     1] loss: 898.811
[28,     1] loss: 955.417
[29,     1] loss: 932.681
[30,     1] loss: 882.610
[31,     1] loss: 924.906
[32,     1] loss: 900.775
[33,     1] loss: 882.760
[34,     1] loss: 911.831
[35,     1] loss: 908.999
[36,     1] loss: 861.955
[37,     1] loss: 886.393
[38,     1] loss: 872.347
[39,     1] loss: 930.314
[40,     1] loss: 881.133
[41,     1] loss: 875.515
[42,     1] loss: 866.877
[43,     1] loss: 840.557
[44,     1] loss: 851.646
[45,     1] loss: 863.866
[46,     1] loss: 823.818
[47,     1] loss: 823.673
[48,     1] loss: 818.096
[49,     1] loss: 795.935
[50,     1] loss: 792.488
[51,     1] loss: 852.114
[52,     1] loss: 787.704
[53,     1] loss: 811.732
[54,     1] loss: 767.927
[55,     1] loss: 770.905
[56,     1] loss: 739.179
[57,     1] loss: 814.810
[58,     1] loss: 742.233
[59,     1] loss: 757.809
[60,     1] loss: 770.095
[61,     1] loss: 762.424
[62,     1] loss: 729.826
[63,     1] loss: 770.697
[64,     1] loss: 717.706
[65,     1] loss: 738.207
[66,     1] loss: 691.779
[67,     1] loss: 740.662
[68,     1] loss: 717.662
[69,     1] loss: 705.767
[70,     1] loss: 687.628
[71,     1] loss: 662.556
[72,     1] loss: 628.079
[73,     1] loss: 605.263
[74,     1] loss: 673.295
[75,     1] loss: 643.977
[76,     1] loss: 628.957
[77,     1] loss: 608.037
[78,     1] loss: 691.397
[79,     1] loss: 599.772
[80,     1] loss: 567.925
[81,     1] loss: 543.164
[82,     1] loss: 604.732
[83,     1] loss: 566.688
[84,     1] loss: 659.068
[85,     1] loss: 586.657
[86,     1] loss: 631.678
[87,     1] loss: 544.588
[88,     1] loss: 594.980
[89,     1] loss: 574.383
[90,     1] loss: 574.711
[91,     1] loss: 541.861
[92,     1] loss: 505.790
[93,     1] loss: 573.087
[94,     1] loss: 526.050
[95,     1] loss: 574.073
[96,     1] loss: 523.910
[97,     1] loss: 571.681
[98,     1] loss: 462.017
[99,     1] loss: 503.830
[100,     1] loss: 522.359
[101,     1] loss: 513.515
[102,     1] loss: 484.952
[103,     1] loss: 546.903
[104,     1] loss: 547.025
[105,     1] loss: 486.738
[106,     1] loss: 477.053
[107,     1] loss: 493.553
[108,     1] loss: 472.759
Early stopping applied (best metric=0.40513572096824646)
Finished Training
Total time taken: 15.777686834335327
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.072
[2,     1] loss: 1243.396
[3,     1] loss: 1246.220
[4,     1] loss: 1247.215
[5,     1] loss: 1242.511
[6,     1] loss: 1240.770
[7,     1] loss: 1240.531
[8,     1] loss: 1239.518
[9,     1] loss: 1238.219
[10,     1] loss: 1236.302
[11,     1] loss: 1229.261
[12,     1] loss: 1213.936
[13,     1] loss: 1203.176
[14,     1] loss: 1177.391
[15,     1] loss: 1164.699
[16,     1] loss: 1130.045
[17,     1] loss: 1087.015
[18,     1] loss: 1085.912
[19,     1] loss: 1061.619
[20,     1] loss: 1049.566
[21,     1] loss: 1036.192
[22,     1] loss: 1020.161
[23,     1] loss: 990.845
[24,     1] loss: 1057.000
[25,     1] loss: 1008.664
[26,     1] loss: 1018.025
[27,     1] loss: 1022.128
[28,     1] loss: 1010.825
[29,     1] loss: 1019.362
[30,     1] loss: 971.161
[31,     1] loss: 959.898
[32,     1] loss: 972.060
[33,     1] loss: 972.520
[34,     1] loss: 955.172
[35,     1] loss: 923.011
[36,     1] loss: 893.021
[37,     1] loss: 928.294
[38,     1] loss: 916.481
[39,     1] loss: 928.453
[40,     1] loss: 885.692
[41,     1] loss: 914.034
[42,     1] loss: 957.077
[43,     1] loss: 902.589
[44,     1] loss: 885.433
[45,     1] loss: 880.756
[46,     1] loss: 925.536
[47,     1] loss: 881.751
[48,     1] loss: 854.522
[49,     1] loss: 875.513
[50,     1] loss: 833.287
[51,     1] loss: 866.655
[52,     1] loss: 873.108
[53,     1] loss: 860.769
[54,     1] loss: 818.047
[55,     1] loss: 872.101
[56,     1] loss: 826.187
[57,     1] loss: 824.610
[58,     1] loss: 858.833
[59,     1] loss: 810.311
[60,     1] loss: 835.750
[61,     1] loss: 803.572
[62,     1] loss: 802.838
[63,     1] loss: 826.585
[64,     1] loss: 761.335
[65,     1] loss: 754.536
[66,     1] loss: 760.899
[67,     1] loss: 784.995
[68,     1] loss: 723.196
[69,     1] loss: 859.800
[70,     1] loss: 734.060
[71,     1] loss: 768.955
[72,     1] loss: 762.075
[73,     1] loss: 726.982
[74,     1] loss: 800.127
[75,     1] loss: 667.350
[76,     1] loss: 758.665
[77,     1] loss: 737.129
[78,     1] loss: 700.077
[79,     1] loss: 704.654
[80,     1] loss: 675.535
[81,     1] loss: 692.445
[82,     1] loss: 689.707
[83,     1] loss: 702.761
[84,     1] loss: 666.468
[85,     1] loss: 706.201
[86,     1] loss: 636.358
[87,     1] loss: 693.642
[88,     1] loss: 666.047
[89,     1] loss: 717.155
[90,     1] loss: 681.989
[91,     1] loss: 639.126
[92,     1] loss: 659.089
[93,     1] loss: 594.570
[94,     1] loss: 620.990
[95,     1] loss: 658.288
Early stopping applied (best metric=0.3309386670589447)
Finished Training
Total time taken: 13.816477060317993
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1244.464
[2,     1] loss: 1241.611
[3,     1] loss: 1247.242
[4,     1] loss: 1242.863
[5,     1] loss: 1241.948
[6,     1] loss: 1238.801
[7,     1] loss: 1234.688
[8,     1] loss: 1228.349
[9,     1] loss: 1217.453
[10,     1] loss: 1195.314
[11,     1] loss: 1170.206
[12,     1] loss: 1130.459
[13,     1] loss: 1104.281
[14,     1] loss: 1064.587
[15,     1] loss: 1032.956
[16,     1] loss: 1041.237
[17,     1] loss: 1013.882
[18,     1] loss: 1026.356
[19,     1] loss: 1000.981
[20,     1] loss: 980.359
[21,     1] loss: 966.487
[22,     1] loss: 994.276
[23,     1] loss: 993.858
[24,     1] loss: 1014.044
[25,     1] loss: 971.593
[26,     1] loss: 951.362
[27,     1] loss: 958.131
[28,     1] loss: 964.324
[29,     1] loss: 964.205
[30,     1] loss: 980.031
[31,     1] loss: 906.550
[32,     1] loss: 929.964
[33,     1] loss: 923.852
[34,     1] loss: 905.087
[35,     1] loss: 890.673
[36,     1] loss: 919.790
[37,     1] loss: 884.456
[38,     1] loss: 921.316
[39,     1] loss: 905.504
[40,     1] loss: 879.845
[41,     1] loss: 879.884
[42,     1] loss: 868.413
[43,     1] loss: 863.133
[44,     1] loss: 872.595
[45,     1] loss: 848.675
[46,     1] loss: 876.535
[47,     1] loss: 845.415
[48,     1] loss: 844.263
[49,     1] loss: 813.398
[50,     1] loss: 789.949
[51,     1] loss: 847.635
[52,     1] loss: 849.669
[53,     1] loss: 852.430
[54,     1] loss: 812.937
[55,     1] loss: 787.217
[56,     1] loss: 756.719
[57,     1] loss: 836.290
[58,     1] loss: 736.058
[59,     1] loss: 836.129
[60,     1] loss: 758.918
[61,     1] loss: 788.652
[62,     1] loss: 774.305
[63,     1] loss: 800.547
[64,     1] loss: 734.057
[65,     1] loss: 741.638
[66,     1] loss: 750.430
[67,     1] loss: 694.146
[68,     1] loss: 782.236
[69,     1] loss: 735.031
[70,     1] loss: 747.828
[71,     1] loss: 708.310
[72,     1] loss: 708.659
[73,     1] loss: 657.463
[74,     1] loss: 724.887
[75,     1] loss: 709.796
[76,     1] loss: 675.716
[77,     1] loss: 659.922
[78,     1] loss: 671.760
[79,     1] loss: 630.538
[80,     1] loss: 638.480
[81,     1] loss: 563.890
[82,     1] loss: 634.967
[83,     1] loss: 560.117
[84,     1] loss: 640.184
[85,     1] loss: 570.451
[86,     1] loss: 626.878
[87,     1] loss: 580.901
Early stopping applied (best metric=0.3688289523124695)
Finished Training
Total time taken: 12.656352519989014
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1244.981
[2,     1] loss: 1245.559
[3,     1] loss: 1251.874
[4,     1] loss: 1243.287
[5,     1] loss: 1242.213
[6,     1] loss: 1239.478
[7,     1] loss: 1238.815
[8,     1] loss: 1227.932
[9,     1] loss: 1210.870
[10,     1] loss: 1183.251
[11,     1] loss: 1141.521
[12,     1] loss: 1115.272
[13,     1] loss: 1080.160
[14,     1] loss: 1066.292
[15,     1] loss: 1050.303
[16,     1] loss: 1054.726
[17,     1] loss: 1017.121
[18,     1] loss: 990.575
[19,     1] loss: 986.415
[20,     1] loss: 1010.366
[21,     1] loss: 978.338
[22,     1] loss: 979.836
[23,     1] loss: 983.066
[24,     1] loss: 912.233
[25,     1] loss: 955.420
[26,     1] loss: 911.899
[27,     1] loss: 953.606
[28,     1] loss: 923.069
[29,     1] loss: 946.340
[30,     1] loss: 909.261
[31,     1] loss: 893.807
[32,     1] loss: 916.835
[33,     1] loss: 889.439
[34,     1] loss: 912.572
[35,     1] loss: 917.160
[36,     1] loss: 875.813
[37,     1] loss: 878.131
[38,     1] loss: 860.728
[39,     1] loss: 933.651
[40,     1] loss: 852.585
[41,     1] loss: 839.894
[42,     1] loss: 836.310
[43,     1] loss: 837.476
[44,     1] loss: 825.349
[45,     1] loss: 873.052
[46,     1] loss: 786.567
[47,     1] loss: 865.221
[48,     1] loss: 852.030
[49,     1] loss: 791.649
[50,     1] loss: 830.092
[51,     1] loss: 810.449
[52,     1] loss: 776.427
[53,     1] loss: 760.850
[54,     1] loss: 765.469
[55,     1] loss: 767.709
[56,     1] loss: 727.951
[57,     1] loss: 779.065
[58,     1] loss: 787.273
[59,     1] loss: 742.915
[60,     1] loss: 780.980
[61,     1] loss: 733.148
[62,     1] loss: 696.974
[63,     1] loss: 648.540
[64,     1] loss: 722.593
[65,     1] loss: 693.713
[66,     1] loss: 725.493
[67,     1] loss: 685.707
[68,     1] loss: 697.078
[69,     1] loss: 738.475
[70,     1] loss: 698.588
[71,     1] loss: 689.495
[72,     1] loss: 664.332
[73,     1] loss: 676.897
[74,     1] loss: 653.177
[75,     1] loss: 671.124
[76,     1] loss: 599.030
Early stopping applied (best metric=0.41635921597480774)
Finished Training
Total time taken: 11.058180093765259
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.403
[2,     1] loss: 1246.519
[3,     1] loss: 1242.397
[4,     1] loss: 1247.500
[5,     1] loss: 1251.055
[6,     1] loss: 1240.316
[7,     1] loss: 1241.445
[8,     1] loss: 1247.367
[9,     1] loss: 1241.749
[10,     1] loss: 1241.765
[11,     1] loss: 1235.136
[12,     1] loss: 1226.946
[13,     1] loss: 1222.706
[14,     1] loss: 1211.456
[15,     1] loss: 1204.400
[16,     1] loss: 1181.633
[17,     1] loss: 1158.029
[18,     1] loss: 1131.241
[19,     1] loss: 1090.811
[20,     1] loss: 1079.374
[21,     1] loss: 1067.813
[22,     1] loss: 1096.768
[23,     1] loss: 1052.198
[24,     1] loss: 992.902
[25,     1] loss: 996.130
[26,     1] loss: 1048.972
[27,     1] loss: 1014.386
[28,     1] loss: 1041.817
[29,     1] loss: 1007.844
[30,     1] loss: 975.318
[31,     1] loss: 1018.926
[32,     1] loss: 981.940
[33,     1] loss: 985.982
[34,     1] loss: 996.692
[35,     1] loss: 929.435
[36,     1] loss: 965.048
[37,     1] loss: 957.948
[38,     1] loss: 949.927
[39,     1] loss: 955.271
[40,     1] loss: 940.466
[41,     1] loss: 924.079
[42,     1] loss: 901.826
[43,     1] loss: 957.222
[44,     1] loss: 951.535
[45,     1] loss: 881.666
[46,     1] loss: 953.921
[47,     1] loss: 930.436
[48,     1] loss: 887.053
[49,     1] loss: 908.366
[50,     1] loss: 900.745
[51,     1] loss: 892.467
[52,     1] loss: 897.945
[53,     1] loss: 916.569
[54,     1] loss: 911.419
[55,     1] loss: 868.522
[56,     1] loss: 879.390
[57,     1] loss: 869.152
[58,     1] loss: 849.005
[59,     1] loss: 886.620
[60,     1] loss: 847.008
[61,     1] loss: 879.523
[62,     1] loss: 892.975
[63,     1] loss: 828.364
[64,     1] loss: 832.464
[65,     1] loss: 810.955
[66,     1] loss: 805.128
[67,     1] loss: 791.814
[68,     1] loss: 854.092
[69,     1] loss: 844.881
[70,     1] loss: 810.126
[71,     1] loss: 789.120
[72,     1] loss: 775.173
[73,     1] loss: 723.819
[74,     1] loss: 724.906
[75,     1] loss: 761.653
[76,     1] loss: 720.779
[77,     1] loss: 764.700
[78,     1] loss: 738.682
[79,     1] loss: 847.904
[80,     1] loss: 692.973
[81,     1] loss: 789.188
[82,     1] loss: 757.151
[83,     1] loss: 739.535
[84,     1] loss: 778.263
[85,     1] loss: 696.414
[86,     1] loss: 768.573
[87,     1] loss: 656.882
[88,     1] loss: 695.805
[89,     1] loss: 698.049
[90,     1] loss: 635.601
[91,     1] loss: 638.436
[92,     1] loss: 615.641
[93,     1] loss: 711.473
[94,     1] loss: 632.383
[95,     1] loss: 694.260
[96,     1] loss: 603.072
[97,     1] loss: 638.147
[98,     1] loss: 616.605
[99,     1] loss: 602.594
[100,     1] loss: 602.699
[101,     1] loss: 597.916
[102,     1] loss: 570.581
[103,     1] loss: 587.153
[104,     1] loss: 640.021
[105,     1] loss: 565.593
[106,     1] loss: 561.804
[107,     1] loss: 566.838
[108,     1] loss: 552.401
[109,     1] loss: 536.380
[110,     1] loss: 532.980
[111,     1] loss: 525.939
[112,     1] loss: 512.793
[113,     1] loss: 505.204
[114,     1] loss: 501.219
[115,     1] loss: 475.255
[116,     1] loss: 517.094
[117,     1] loss: 514.706
[118,     1] loss: 510.382
[119,     1] loss: 491.722
[120,     1] loss: 498.575
[121,     1] loss: 496.651
Early stopping applied (best metric=0.32567694783210754)
Finished Training
Total time taken: 17.438862800598145
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1248.162
[2,     1] loss: 1244.035
[3,     1] loss: 1249.112
[4,     1] loss: 1241.861
[5,     1] loss: 1244.427
[6,     1] loss: 1241.035
[7,     1] loss: 1244.060
[8,     1] loss: 1243.067
[9,     1] loss: 1237.363
[10,     1] loss: 1232.332
[11,     1] loss: 1229.701
[12,     1] loss: 1221.589
[13,     1] loss: 1197.288
[14,     1] loss: 1178.264
[15,     1] loss: 1154.541
[16,     1] loss: 1114.115
[17,     1] loss: 1074.536
[18,     1] loss: 1038.344
[19,     1] loss: 1068.866
[20,     1] loss: 1069.625
[21,     1] loss: 1010.897
[22,     1] loss: 1016.272
[23,     1] loss: 1046.580
[24,     1] loss: 1057.375
[25,     1] loss: 980.026
[26,     1] loss: 1011.167
[27,     1] loss: 1017.067
[28,     1] loss: 1010.659
[29,     1] loss: 985.008
[30,     1] loss: 968.651
[31,     1] loss: 993.170
[32,     1] loss: 977.038
[33,     1] loss: 975.906
[34,     1] loss: 955.865
[35,     1] loss: 889.536
[36,     1] loss: 929.644
[37,     1] loss: 935.456
[38,     1] loss: 901.552
[39,     1] loss: 916.204
[40,     1] loss: 953.139
[41,     1] loss: 901.675
[42,     1] loss: 915.316
[43,     1] loss: 903.624
[44,     1] loss: 855.329
[45,     1] loss: 897.847
[46,     1] loss: 849.340
[47,     1] loss: 846.276
[48,     1] loss: 872.499
[49,     1] loss: 868.336
[50,     1] loss: 823.029
[51,     1] loss: 813.339
[52,     1] loss: 817.426
[53,     1] loss: 870.036
[54,     1] loss: 790.542
[55,     1] loss: 813.285
[56,     1] loss: 775.596
[57,     1] loss: 816.772
[58,     1] loss: 790.751
[59,     1] loss: 775.090
[60,     1] loss: 777.575
[61,     1] loss: 757.728
[62,     1] loss: 792.495
[63,     1] loss: 789.040
[64,     1] loss: 729.333
[65,     1] loss: 748.190
[66,     1] loss: 735.362
[67,     1] loss: 724.194
[68,     1] loss: 710.784
[69,     1] loss: 716.535
[70,     1] loss: 684.617
[71,     1] loss: 710.153
[72,     1] loss: 714.336
[73,     1] loss: 655.974
[74,     1] loss: 711.265
[75,     1] loss: 682.779
[76,     1] loss: 670.686
[77,     1] loss: 609.636
[78,     1] loss: 679.604
[79,     1] loss: 694.474
[80,     1] loss: 653.383
[81,     1] loss: 637.510
[82,     1] loss: 648.016
[83,     1] loss: 620.142
[84,     1] loss: 573.875
[85,     1] loss: 611.208
[86,     1] loss: 597.226
[87,     1] loss: 613.461
[88,     1] loss: 549.871
[89,     1] loss: 589.396
[90,     1] loss: 509.421
[91,     1] loss: 568.857
[92,     1] loss: 574.516
Early stopping applied (best metric=0.37277528643608093)
Finished Training
Total time taken: 13.325418710708618
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.927
[2,     1] loss: 1243.344
[3,     1] loss: 1245.292
[4,     1] loss: 1246.280
[5,     1] loss: 1241.033
[6,     1] loss: 1244.126
[7,     1] loss: 1243.558
[8,     1] loss: 1240.783
[9,     1] loss: 1239.627
[10,     1] loss: 1233.118
[11,     1] loss: 1223.661
[12,     1] loss: 1217.559
[13,     1] loss: 1199.824
[14,     1] loss: 1178.191
[15,     1] loss: 1134.389
[16,     1] loss: 1144.102
[17,     1] loss: 1127.535
[18,     1] loss: 1121.366
[19,     1] loss: 1078.361
[20,     1] loss: 1054.658
[21,     1] loss: 1022.830
[22,     1] loss: 1032.322
[23,     1] loss: 986.675
[24,     1] loss: 1009.224
[25,     1] loss: 926.771
[26,     1] loss: 989.487
[27,     1] loss: 1004.195
[28,     1] loss: 956.876
[29,     1] loss: 1029.347
[30,     1] loss: 938.290
[31,     1] loss: 938.050
[32,     1] loss: 941.214
[33,     1] loss: 959.714
[34,     1] loss: 913.211
[35,     1] loss: 924.175
[36,     1] loss: 939.158
[37,     1] loss: 904.219
[38,     1] loss: 917.001
[39,     1] loss: 844.524
[40,     1] loss: 852.804
[41,     1] loss: 932.950
[42,     1] loss: 858.290
[43,     1] loss: 863.330
[44,     1] loss: 829.719
[45,     1] loss: 864.645
[46,     1] loss: 909.803
[47,     1] loss: 841.374
[48,     1] loss: 829.196
[49,     1] loss: 783.002
[50,     1] loss: 781.824
[51,     1] loss: 840.825
[52,     1] loss: 787.137
[53,     1] loss: 750.582
[54,     1] loss: 789.698
[55,     1] loss: 796.574
[56,     1] loss: 792.528
[57,     1] loss: 815.628
[58,     1] loss: 776.381
[59,     1] loss: 754.163
[60,     1] loss: 720.651
[61,     1] loss: 795.881
[62,     1] loss: 747.948
[63,     1] loss: 698.827
[64,     1] loss: 726.562
[65,     1] loss: 708.877
[66,     1] loss: 710.961
[67,     1] loss: 659.502
[68,     1] loss: 668.486
[69,     1] loss: 648.266
[70,     1] loss: 675.926
[71,     1] loss: 612.289
[72,     1] loss: 686.077
[73,     1] loss: 650.928
[74,     1] loss: 626.248
[75,     1] loss: 655.931
[76,     1] loss: 622.593
[77,     1] loss: 602.921
[78,     1] loss: 641.280
[79,     1] loss: 595.393
[80,     1] loss: 588.523
[81,     1] loss: 617.139
Early stopping applied (best metric=0.36833474040031433)
Finished Training
Total time taken: 11.59223747253418
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1251.210
[2,     1] loss: 1240.229
[3,     1] loss: 1248.320
[4,     1] loss: 1239.647
[5,     1] loss: 1242.379
[6,     1] loss: 1249.237
[7,     1] loss: 1243.201
[8,     1] loss: 1240.968
[9,     1] loss: 1238.150
[10,     1] loss: 1233.152
[11,     1] loss: 1218.658
[12,     1] loss: 1209.734
[13,     1] loss: 1193.380
[14,     1] loss: 1175.749
[15,     1] loss: 1152.080
[16,     1] loss: 1132.277
[17,     1] loss: 1108.026
[18,     1] loss: 1090.872
[19,     1] loss: 1080.936
[20,     1] loss: 1039.464
[21,     1] loss: 1044.721
[22,     1] loss: 1060.071
[23,     1] loss: 1051.586
[24,     1] loss: 1050.653
[25,     1] loss: 1003.981
[26,     1] loss: 1061.899
[27,     1] loss: 992.348
[28,     1] loss: 1043.374
[29,     1] loss: 1033.059
[30,     1] loss: 1009.982
[31,     1] loss: 960.302
[32,     1] loss: 1014.444
[33,     1] loss: 987.802
[34,     1] loss: 947.771
[35,     1] loss: 972.937
[36,     1] loss: 964.383
[37,     1] loss: 932.830
[38,     1] loss: 932.667
[39,     1] loss: 909.951
[40,     1] loss: 918.780
[41,     1] loss: 939.705
[42,     1] loss: 849.640
[43,     1] loss: 870.640
[44,     1] loss: 901.003
[45,     1] loss: 902.010
[46,     1] loss: 932.242
[47,     1] loss: 894.600
[48,     1] loss: 893.101
[49,     1] loss: 861.877
[50,     1] loss: 861.732
[51,     1] loss: 865.533
[52,     1] loss: 875.437
[53,     1] loss: 835.976
[54,     1] loss: 814.068
[55,     1] loss: 808.204
[56,     1] loss: 815.735
[57,     1] loss: 847.248
[58,     1] loss: 833.830
[59,     1] loss: 890.434
[60,     1] loss: 782.142
[61,     1] loss: 801.362
[62,     1] loss: 747.440
[63,     1] loss: 751.680
[64,     1] loss: 802.819
[65,     1] loss: 777.483
[66,     1] loss: 758.528
[67,     1] loss: 791.510
[68,     1] loss: 791.212
[69,     1] loss: 747.629
[70,     1] loss: 733.588
[71,     1] loss: 767.329
[72,     1] loss: 753.223
[73,     1] loss: 689.394
[74,     1] loss: 768.044
[75,     1] loss: 688.489
[76,     1] loss: 755.320
[77,     1] loss: 737.552
[78,     1] loss: 691.091
[79,     1] loss: 699.218
[80,     1] loss: 692.292
[81,     1] loss: 661.907
[82,     1] loss: 685.659
[83,     1] loss: 679.423
[84,     1] loss: 661.425
[85,     1] loss: 719.353
[86,     1] loss: 675.323
[87,     1] loss: 727.201
[88,     1] loss: 710.561
[89,     1] loss: 650.651
[90,     1] loss: 702.248
[91,     1] loss: 684.819
[92,     1] loss: 648.652
[93,     1] loss: 650.621
[94,     1] loss: 657.076
[95,     1] loss: 610.396
[96,     1] loss: 590.344
[97,     1] loss: 658.517
[98,     1] loss: 568.797
[99,     1] loss: 632.341
Early stopping applied (best metric=0.34629344940185547)
Finished Training
Total time taken: 14.286528825759888
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1252.955
[2,     1] loss: 1247.630
[3,     1] loss: 1245.371
[4,     1] loss: 1246.175
[5,     1] loss: 1244.105
[6,     1] loss: 1247.230
[7,     1] loss: 1244.870
[8,     1] loss: 1241.806
[9,     1] loss: 1244.751
[10,     1] loss: 1243.318
[11,     1] loss: 1243.005
[12,     1] loss: 1244.509
[13,     1] loss: 1244.162
[14,     1] loss: 1239.826
[15,     1] loss: 1237.926
[16,     1] loss: 1237.409
[17,     1] loss: 1225.815
[18,     1] loss: 1223.834
[19,     1] loss: 1203.651
[20,     1] loss: 1205.310
[21,     1] loss: 1182.221
[22,     1] loss: 1143.863
[23,     1] loss: 1126.476
[24,     1] loss: 1128.374
[25,     1] loss: 1101.816
[26,     1] loss: 1063.243
[27,     1] loss: 1081.969
[28,     1] loss: 1030.078
[29,     1] loss: 1053.977
[30,     1] loss: 1029.950
[31,     1] loss: 995.276
[32,     1] loss: 994.966
[33,     1] loss: 1012.150
[34,     1] loss: 989.824
[35,     1] loss: 981.249
[36,     1] loss: 988.064
[37,     1] loss: 966.010
[38,     1] loss: 967.828
[39,     1] loss: 1000.975
[40,     1] loss: 919.109
[41,     1] loss: 948.165
[42,     1] loss: 969.557
[43,     1] loss: 908.350
[44,     1] loss: 936.865
[45,     1] loss: 891.446
[46,     1] loss: 944.082
[47,     1] loss: 937.286
[48,     1] loss: 902.470
[49,     1] loss: 973.555
[50,     1] loss: 865.094
[51,     1] loss: 908.501
[52,     1] loss: 916.606
[53,     1] loss: 868.138
[54,     1] loss: 859.260
[55,     1] loss: 902.449
[56,     1] loss: 887.907
[57,     1] loss: 871.653
[58,     1] loss: 849.319
[59,     1] loss: 901.316
[60,     1] loss: 869.651
[61,     1] loss: 889.423
[62,     1] loss: 867.108
[63,     1] loss: 789.832
[64,     1] loss: 848.297
[65,     1] loss: 783.395
[66,     1] loss: 813.727
[67,     1] loss: 790.793
[68,     1] loss: 882.632
[69,     1] loss: 871.311
[70,     1] loss: 830.386
[71,     1] loss: 788.865
[72,     1] loss: 817.085
[73,     1] loss: 778.633
[74,     1] loss: 768.457
[75,     1] loss: 832.127
[76,     1] loss: 774.133
[77,     1] loss: 833.398
[78,     1] loss: 752.835
[79,     1] loss: 785.515
[80,     1] loss: 741.381
[81,     1] loss: 748.580
[82,     1] loss: 759.662
[83,     1] loss: 690.641
[84,     1] loss: 776.627
[85,     1] loss: 737.007
[86,     1] loss: 793.287
[87,     1] loss: 724.000
[88,     1] loss: 732.155
[89,     1] loss: 703.310
[90,     1] loss: 652.844
[91,     1] loss: 688.390
[92,     1] loss: 614.589
[93,     1] loss: 662.219
[94,     1] loss: 666.552
[95,     1] loss: 620.696
[96,     1] loss: 685.599
[97,     1] loss: 642.307
[98,     1] loss: 624.723
[99,     1] loss: 677.312
[100,     1] loss: 613.030
[101,     1] loss: 609.569
[102,     1] loss: 597.202
[103,     1] loss: 613.738
[104,     1] loss: 594.270
[105,     1] loss: 576.956
[106,     1] loss: 593.444
[107,     1] loss: 582.274
[108,     1] loss: 620.437
[109,     1] loss: 591.702
[110,     1] loss: 548.367
[111,     1] loss: 556.980
[112,     1] loss: 496.243
[113,     1] loss: 528.445
[114,     1] loss: 549.188
[115,     1] loss: 576.602
Early stopping applied (best metric=0.3539336919784546)
Finished Training
Total time taken: 16.565772533416748
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.045
[2,     1] loss: 1244.568
[3,     1] loss: 1246.992
[4,     1] loss: 1240.997
[5,     1] loss: 1245.143
[6,     1] loss: 1241.494
[7,     1] loss: 1241.891
[8,     1] loss: 1242.413
[9,     1] loss: 1243.574
[10,     1] loss: 1239.588
[11,     1] loss: 1236.208
[12,     1] loss: 1230.604
[13,     1] loss: 1223.090
[14,     1] loss: 1205.061
[15,     1] loss: 1192.907
[16,     1] loss: 1159.499
[17,     1] loss: 1135.692
[18,     1] loss: 1107.495
[19,     1] loss: 1083.003
[20,     1] loss: 1089.083
[21,     1] loss: 1054.832
[22,     1] loss: 1049.958
[23,     1] loss: 999.502
[24,     1] loss: 1067.933
[25,     1] loss: 1036.266
[26,     1] loss: 1021.258
[27,     1] loss: 1061.621
[28,     1] loss: 1033.156
[29,     1] loss: 979.518
[30,     1] loss: 988.355
[31,     1] loss: 946.790
[32,     1] loss: 960.842
[33,     1] loss: 941.346
[34,     1] loss: 934.488
[35,     1] loss: 942.680
[36,     1] loss: 905.897
[37,     1] loss: 948.131
[38,     1] loss: 896.261
[39,     1] loss: 962.608
[40,     1] loss: 917.170
[41,     1] loss: 875.483
[42,     1] loss: 905.600
[43,     1] loss: 897.435
[44,     1] loss: 914.336
[45,     1] loss: 896.173
[46,     1] loss: 832.443
[47,     1] loss: 843.776
[48,     1] loss: 859.300
[49,     1] loss: 850.567
[50,     1] loss: 857.950
[51,     1] loss: 852.069
[52,     1] loss: 811.507
[53,     1] loss: 831.798
[54,     1] loss: 835.720
[55,     1] loss: 825.167
[56,     1] loss: 793.056
[57,     1] loss: 777.594
[58,     1] loss: 765.195
[59,     1] loss: 782.515
[60,     1] loss: 774.705
[61,     1] loss: 783.858
[62,     1] loss: 792.045
[63,     1] loss: 767.902
[64,     1] loss: 698.228
[65,     1] loss: 738.033
[66,     1] loss: 745.616
[67,     1] loss: 740.835
[68,     1] loss: 755.203
[69,     1] loss: 714.714
[70,     1] loss: 705.555
[71,     1] loss: 718.999
[72,     1] loss: 643.702
[73,     1] loss: 730.158
[74,     1] loss: 705.915
[75,     1] loss: 650.388
[76,     1] loss: 680.023
Early stopping applied (best metric=0.36375144124031067)
Finished Training
Total time taken: 11.133187770843506
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1241.284
[2,     1] loss: 1248.724
[3,     1] loss: 1243.537
[4,     1] loss: 1246.038
[5,     1] loss: 1234.763
[6,     1] loss: 1231.431
[7,     1] loss: 1218.710
[8,     1] loss: 1195.238
[9,     1] loss: 1163.331
[10,     1] loss: 1123.715
[11,     1] loss: 1083.898
[12,     1] loss: 1068.763
[13,     1] loss: 1032.500
[14,     1] loss: 1028.307
[15,     1] loss: 1011.218
[16,     1] loss: 1005.905
[17,     1] loss: 1039.095
[18,     1] loss: 1043.451
[19,     1] loss: 963.533
[20,     1] loss: 995.696
[21,     1] loss: 976.611
[22,     1] loss: 987.570
[23,     1] loss: 955.142
[24,     1] loss: 937.026
[25,     1] loss: 943.627
[26,     1] loss: 952.590
[27,     1] loss: 979.840
[28,     1] loss: 893.412
[29,     1] loss: 888.635
[30,     1] loss: 886.238
[31,     1] loss: 907.982
[32,     1] loss: 891.666
[33,     1] loss: 934.483
[34,     1] loss: 893.769
[35,     1] loss: 879.214
[36,     1] loss: 882.606
[37,     1] loss: 822.961
[38,     1] loss: 929.199
[39,     1] loss: 829.576
[40,     1] loss: 857.176
[41,     1] loss: 856.973
[42,     1] loss: 877.034
[43,     1] loss: 825.991
[44,     1] loss: 834.974
[45,     1] loss: 820.851
[46,     1] loss: 820.622
[47,     1] loss: 835.476
[48,     1] loss: 775.613
[49,     1] loss: 793.587
[50,     1] loss: 793.836
[51,     1] loss: 801.405
[52,     1] loss: 785.662
[53,     1] loss: 803.994
[54,     1] loss: 739.714
[55,     1] loss: 750.155
[56,     1] loss: 757.876
[57,     1] loss: 750.915
[58,     1] loss: 742.371
[59,     1] loss: 806.661
[60,     1] loss: 750.700
[61,     1] loss: 775.452
[62,     1] loss: 757.909
[63,     1] loss: 758.410
[64,     1] loss: 740.973
[65,     1] loss: 740.008
[66,     1] loss: 735.065
[67,     1] loss: 709.613
[68,     1] loss: 703.158
[69,     1] loss: 690.288
[70,     1] loss: 721.004
[71,     1] loss: 645.249
[72,     1] loss: 701.474
[73,     1] loss: 701.663
[74,     1] loss: 659.430
[75,     1] loss: 757.105
[76,     1] loss: 663.279
[77,     1] loss: 641.746
[78,     1] loss: 630.752
[79,     1] loss: 636.569
[80,     1] loss: 658.367
[81,     1] loss: 638.605
[82,     1] loss: 648.936
[83,     1] loss: 656.145
[84,     1] loss: 565.779
[85,     1] loss: 611.408
[86,     1] loss: 574.406
[87,     1] loss: 625.276
[88,     1] loss: 639.276
[89,     1] loss: 593.710
[90,     1] loss: 588.086
[91,     1] loss: 529.057
[92,     1] loss: 575.727
[93,     1] loss: 557.588
[94,     1] loss: 534.231
[95,     1] loss: 585.962
[96,     1] loss: 537.657
[97,     1] loss: 538.129
[98,     1] loss: 523.136
[99,     1] loss: 573.066
[100,     1] loss: 563.155
[101,     1] loss: 544.787
[102,     1] loss: 515.037
[103,     1] loss: 497.377
[104,     1] loss: 538.303
[105,     1] loss: 539.297
[106,     1] loss: 487.570
[107,     1] loss: 537.743
[108,     1] loss: 518.496
[109,     1] loss: 469.259
[110,     1] loss: 438.164
[111,     1] loss: 503.546
[112,     1] loss: 477.120
Early stopping applied (best metric=0.40393978357315063)
Finished Training
Total time taken: 16.548768997192383
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1249.639
[2,     1] loss: 1245.541
[3,     1] loss: 1246.092
[4,     1] loss: 1241.489
[5,     1] loss: 1241.250
[6,     1] loss: 1240.978
[7,     1] loss: 1237.849
[8,     1] loss: 1233.327
[9,     1] loss: 1228.695
[10,     1] loss: 1221.314
[11,     1] loss: 1211.518
[12,     1] loss: 1177.369
[13,     1] loss: 1143.226
[14,     1] loss: 1124.380
[15,     1] loss: 1073.586
[16,     1] loss: 1078.528
[17,     1] loss: 1033.146
[18,     1] loss: 1032.059
[19,     1] loss: 1039.448
[20,     1] loss: 1032.394
[21,     1] loss: 1006.635
[22,     1] loss: 982.819
[23,     1] loss: 1026.761
[24,     1] loss: 1002.922
[25,     1] loss: 958.726
[26,     1] loss: 1010.637
[27,     1] loss: 1028.596
[28,     1] loss: 1014.553
[29,     1] loss: 996.635
[30,     1] loss: 951.266
[31,     1] loss: 974.625
[32,     1] loss: 971.832
[33,     1] loss: 956.123
[34,     1] loss: 925.150
[35,     1] loss: 950.245
[36,     1] loss: 918.303
[37,     1] loss: 915.326
[38,     1] loss: 913.992
[39,     1] loss: 906.934
[40,     1] loss: 922.678
[41,     1] loss: 878.602
[42,     1] loss: 903.504
[43,     1] loss: 914.361
[44,     1] loss: 889.892
[45,     1] loss: 875.752
[46,     1] loss: 924.191
[47,     1] loss: 893.046
[48,     1] loss: 846.260
[49,     1] loss: 901.691
[50,     1] loss: 848.990
[51,     1] loss: 880.661
[52,     1] loss: 899.389
[53,     1] loss: 888.831
[54,     1] loss: 850.875
[55,     1] loss: 841.404
[56,     1] loss: 856.624
[57,     1] loss: 864.505
[58,     1] loss: 825.069
[59,     1] loss: 769.028
[60,     1] loss: 849.039
[61,     1] loss: 831.594
[62,     1] loss: 760.869
[63,     1] loss: 809.543
[64,     1] loss: 769.459
[65,     1] loss: 752.948
[66,     1] loss: 718.708
[67,     1] loss: 767.602
[68,     1] loss: 774.533
[69,     1] loss: 764.407
[70,     1] loss: 745.019
[71,     1] loss: 800.937
[72,     1] loss: 714.696
[73,     1] loss: 716.700
[74,     1] loss: 698.558
[75,     1] loss: 676.855
[76,     1] loss: 676.485
[77,     1] loss: 678.027
[78,     1] loss: 664.856
[79,     1] loss: 697.906
[80,     1] loss: 697.358
[81,     1] loss: 691.526
[82,     1] loss: 634.413
[83,     1] loss: 627.477
[84,     1] loss: 680.943
[85,     1] loss: 681.494
[86,     1] loss: 600.575
[87,     1] loss: 659.939
[88,     1] loss: 699.644
[89,     1] loss: 646.507
[90,     1] loss: 594.494
[91,     1] loss: 610.069
[92,     1] loss: 595.401
[93,     1] loss: 583.987
[94,     1] loss: 620.076
[95,     1] loss: 631.028
[96,     1] loss: 575.635
[97,     1] loss: 617.655
[98,     1] loss: 546.915
[99,     1] loss: 547.801
[100,     1] loss: 582.528
[101,     1] loss: 562.342
[102,     1] loss: 523.100
[103,     1] loss: 555.539
[104,     1] loss: 570.682
[105,     1] loss: 555.861
[106,     1] loss: 487.919
[107,     1] loss: 534.126
[108,     1] loss: 534.713
[109,     1] loss: 588.850
[110,     1] loss: 538.472
[111,     1] loss: 558.213
[112,     1] loss: 445.944
[113,     1] loss: 457.555
[114,     1] loss: 508.527
[115,     1] loss: 520.088
Early stopping applied (best metric=0.3314460515975952)
Finished Training
Total time taken: 16.590775966644287
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.016
[2,     1] loss: 1241.196
[3,     1] loss: 1242.092
[4,     1] loss: 1242.497
[5,     1] loss: 1243.451
[6,     1] loss: 1239.378
[7,     1] loss: 1240.964
[8,     1] loss: 1221.561
[9,     1] loss: 1212.028
[10,     1] loss: 1182.110
[11,     1] loss: 1152.323
[12,     1] loss: 1100.964
[13,     1] loss: 1078.032
[14,     1] loss: 1065.724
[15,     1] loss: 1079.439
[16,     1] loss: 1016.648
[17,     1] loss: 971.141
[18,     1] loss: 1043.103
[19,     1] loss: 1059.989
[20,     1] loss: 1005.176
[21,     1] loss: 1019.566
[22,     1] loss: 1019.958
[23,     1] loss: 963.868
[24,     1] loss: 1009.951
[25,     1] loss: 927.876
[26,     1] loss: 988.405
[27,     1] loss: 995.131
[28,     1] loss: 981.220
[29,     1] loss: 939.475
[30,     1] loss: 953.520
[31,     1] loss: 932.318
[32,     1] loss: 966.769
[33,     1] loss: 878.817
[34,     1] loss: 896.061
[35,     1] loss: 880.238
[36,     1] loss: 872.135
[37,     1] loss: 886.252
[38,     1] loss: 900.593
[39,     1] loss: 898.067
[40,     1] loss: 813.939
[41,     1] loss: 861.630
[42,     1] loss: 839.128
[43,     1] loss: 907.032
[44,     1] loss: 817.176
[45,     1] loss: 813.526
[46,     1] loss: 837.465
[47,     1] loss: 836.956
[48,     1] loss: 802.481
[49,     1] loss: 808.911
[50,     1] loss: 779.294
[51,     1] loss: 748.877
[52,     1] loss: 756.165
[53,     1] loss: 776.766
[54,     1] loss: 788.553
[55,     1] loss: 755.963
[56,     1] loss: 749.905
[57,     1] loss: 706.304
[58,     1] loss: 673.660
[59,     1] loss: 769.451
[60,     1] loss: 739.361
[61,     1] loss: 719.901
[62,     1] loss: 727.289
[63,     1] loss: 689.234
[64,     1] loss: 738.567
[65,     1] loss: 639.959
[66,     1] loss: 697.166
[67,     1] loss: 681.247
[68,     1] loss: 677.784
[69,     1] loss: 648.842
[70,     1] loss: 644.696
[71,     1] loss: 705.081
[72,     1] loss: 635.461
[73,     1] loss: 686.171
[74,     1] loss: 620.796
[75,     1] loss: 646.647
[76,     1] loss: 651.318
[77,     1] loss: 634.117
Early stopping applied (best metric=0.36844849586486816)
Finished Training
Total time taken: 11.087185144424438
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1249.716
[2,     1] loss: 1249.966
[3,     1] loss: 1244.818
[4,     1] loss: 1243.348
[5,     1] loss: 1244.083
[6,     1] loss: 1243.042
[7,     1] loss: 1242.823
[8,     1] loss: 1235.263
[9,     1] loss: 1228.605
[10,     1] loss: 1211.152
[11,     1] loss: 1189.942
[12,     1] loss: 1154.725
[13,     1] loss: 1123.388
[14,     1] loss: 1091.905
[15,     1] loss: 1083.798
[16,     1] loss: 1081.313
[17,     1] loss: 1027.792
[18,     1] loss: 1031.122
[19,     1] loss: 999.294
[20,     1] loss: 1055.409
[21,     1] loss: 1023.832
[22,     1] loss: 1046.469
[23,     1] loss: 1034.428
[24,     1] loss: 1029.756
[25,     1] loss: 994.510
[26,     1] loss: 1003.836
[27,     1] loss: 993.221
[28,     1] loss: 981.572
[29,     1] loss: 974.626
[30,     1] loss: 947.596
[31,     1] loss: 941.177
[32,     1] loss: 975.210
[33,     1] loss: 968.334
[34,     1] loss: 956.812
[35,     1] loss: 929.757
[36,     1] loss: 961.862
[37,     1] loss: 906.637
[38,     1] loss: 911.068
[39,     1] loss: 892.474
[40,     1] loss: 876.805
[41,     1] loss: 902.326
[42,     1] loss: 876.422
[43,     1] loss: 862.654
[44,     1] loss: 876.949
[45,     1] loss: 880.392
[46,     1] loss: 886.800
[47,     1] loss: 851.268
[48,     1] loss: 882.180
[49,     1] loss: 847.297
[50,     1] loss: 863.878
[51,     1] loss: 809.503
[52,     1] loss: 881.528
[53,     1] loss: 870.797
[54,     1] loss: 859.392
[55,     1] loss: 851.685
[56,     1] loss: 891.105
[57,     1] loss: 815.785
[58,     1] loss: 828.891
[59,     1] loss: 747.501
[60,     1] loss: 796.349
[61,     1] loss: 805.900
[62,     1] loss: 815.452
[63,     1] loss: 822.808
[64,     1] loss: 775.815
[65,     1] loss: 764.637
[66,     1] loss: 765.156
[67,     1] loss: 772.222
[68,     1] loss: 774.187
[69,     1] loss: 783.133
[70,     1] loss: 765.070
[71,     1] loss: 733.158
[72,     1] loss: 775.650
[73,     1] loss: 718.154
[74,     1] loss: 661.802
[75,     1] loss: 729.893
[76,     1] loss: 709.458
[77,     1] loss: 702.519
[78,     1] loss: 709.340
[79,     1] loss: 695.805
[80,     1] loss: 714.729
[81,     1] loss: 639.143
[82,     1] loss: 641.310
[83,     1] loss: 647.671
Early stopping applied (best metric=0.3984202444553375)
Finished Training
Total time taken: 11.9602792263031
{'Hydroxylation-K Validation Accuracy': 0.758096926713948, 'Hydroxylation-K Validation Sensitivity': 0.6118518518518519, 'Hydroxylation-K Validation Specificity': 0.7947368421052632, 'Hydroxylation-K Validation Precision': 0.4418222907696592, 'Hydroxylation-K AUC ROC': 0.7832163742690058, 'Hydroxylation-K AUC PR': 0.5385472591321474, 'Hydroxylation-K MCC': 0.36668563747674954, 'Hydroxylation-K F1': 0.5070216504325147, 'Validation Loss (Hydroxylation-K)': 0.4992097278436025, 'Hydroxylation-P Validation Accuracy': 0.7986496286144528, 'Hydroxylation-P Validation Sensitivity': 0.7932804232804234, 'Hydroxylation-P Validation Specificity': 0.7997780437927079, 'Hydroxylation-P Validation Precision': 0.46453281222797654, 'Hydroxylation-P AUC ROC': 0.8558842116576293, 'Hydroxylation-P AUC PR': 0.5982745611527996, 'Hydroxylation-P MCC': 0.4942432376700534, 'Hydroxylation-P F1': 0.5839988999494153, 'Validation Loss (Hydroxylation-P)': 0.36291072368621824, 'Validation Loss (total)': 0.8621204535166422, 'TimeToTrain': 13.92882194519043}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009683740170889549,
 'learning_rate_Hydroxylation-K': 0.0070389318132917395,
 'learning_rate_Hydroxylation-P': 0.00940680570724688,
 'log_base': 1.5297710501331434,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 426103489,
 'sample_weights': [1.5929027480578504, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.10455675463093,
 'weight_decay_Hydroxylation-K': 0.18224723943683596,
 'weight_decay_Hydroxylation-P': 7.886676409561685}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1742.996
[2,     1] loss: 1739.377
[3,     1] loss: 1742.004
[4,     1] loss: 1734.393
[5,     1] loss: 1740.165
[6,     1] loss: 1736.182
[7,     1] loss: 1729.352
[8,     1] loss: 1736.610
[9,     1] loss: 1727.438
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002331777985716598,
 'learning_rate_Hydroxylation-K': 0.0024935229256960794,
 'learning_rate_Hydroxylation-P': 0.0038249885377333735,
 'log_base': 2.2367188207581385,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2532666754,
 'sample_weights': [3.9270104264066155, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.705226496077303,
 'weight_decay_Hydroxylation-K': 9.962020259391567,
 'weight_decay_Hydroxylation-P': 3.638851146849494}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1347.680
[2,     1] loss: 1344.908
[3,     1] loss: 1349.413
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006136850966764653,
 'learning_rate_Hydroxylation-K': 0.004144748307556298,
 'learning_rate_Hydroxylation-P': 0.006872637769776304,
 'log_base': 2.9815588989613726,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1964929343,
 'sample_weights': [2.073816709450636, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.8341308361812843,
 'weight_decay_Hydroxylation-K': 4.582400542815198,
 'weight_decay_Hydroxylation-P': 5.45037814128975}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1237.726
[2,     1] loss: 1235.622
[3,     1] loss: 1230.103
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009157847278068704,
 'learning_rate_Hydroxylation-K': 0.00662855661877301,
 'learning_rate_Hydroxylation-P': 0.0032418384620483695,
 'log_base': 1.0523313032295365,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 358792179,
 'sample_weights': [1.5281695508304587, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.387175002644115,
 'weight_decay_Hydroxylation-K': 8.201133710649113,
 'weight_decay_Hydroxylation-P': 6.545809559586663}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10651.807
[2,     1] loss: 10602.547
[3,     1] loss: 10596.920
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003447502108700099,
 'learning_rate_Hydroxylation-K': 0.0037199688631563066,
 'learning_rate_Hydroxylation-P': 0.00045353716066470854,
 'log_base': 2.0178473878474374,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2428948053,
 'sample_weights': [32.729050680061924, 4.091288941815231],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7133076295500214,
 'weight_decay_Hydroxylation-K': 9.673139848710605,
 'weight_decay_Hydroxylation-P': 9.451355794754308}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1413.671
[2,     1] loss: 1411.620
[3,     1] loss: 1409.642
[4,     1] loss: 1408.587
[5,     1] loss: 1407.405
[6,     1] loss: 1402.480
[7,     1] loss: 1402.711
[8,     1] loss: 1395.092
[9,     1] loss: 1386.701
[10,     1] loss: 1368.598
[11,     1] loss: 1347.465
[12,     1] loss: 1321.885
[13,     1] loss: 1275.810
[14,     1] loss: 1252.457
[15,     1] loss: 1249.607
[16,     1] loss: 1205.799
[17,     1] loss: 1152.201
[18,     1] loss: 1165.201
[19,     1] loss: 1170.374
[20,     1] loss: 1115.976
[21,     1] loss: 1204.513
[22,     1] loss: 1163.781
[23,     1] loss: 1132.890
[24,     1] loss: 1156.541
[25,     1] loss: 1142.176
[26,     1] loss: 1132.964
[27,     1] loss: 1141.019
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007237214547722198,
 'learning_rate_Hydroxylation-K': 0.009966561269320558,
 'learning_rate_Hydroxylation-P': 0.0028039850435998366,
 'log_base': 1.0425698726648691,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2731810376,
 'sample_weights': [2.3780181347825002, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.607551356051625,
 'weight_decay_Hydroxylation-K': 9.889329258139876,
 'weight_decay_Hydroxylation-P': 7.657121798924301}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 13046.477
[2,     1] loss: 13077.574
[3,     1] loss: 12966.965
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006643217236316052,
 'learning_rate_Hydroxylation-K': 0.009442487216516544,
 'learning_rate_Hydroxylation-P': 0.008732986420980707,
 'log_base': 2.7378173886050226,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1090112071,
 'sample_weights': [40.04546278076237, 5.005875686599551],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.7703573243769801,
 'weight_decay_Hydroxylation-K': 7.344270869329084,
 'weight_decay_Hydroxylation-P': 2.762861741267958}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.750
[2,     1] loss: 1258.131
[3,     1] loss: 1252.570
[4,     1] loss: 1254.610
[5,     1] loss: 1249.754
[6,     1] loss: 1239.444
[7,     1] loss: 1232.422
[8,     1] loss: 1193.028
[9,     1] loss: 1137.207
[10,     1] loss: 1122.975
[11,     1] loss: 1090.068
[12,     1] loss: 1082.222
[13,     1] loss: 1045.716
[14,     1] loss: 1031.483
[15,     1] loss: 1072.484
[16,     1] loss: 992.720
[17,     1] loss: 986.516
[18,     1] loss: 990.397
[19,     1] loss: 996.266
[20,     1] loss: 943.099
[21,     1] loss: 945.661
[22,     1] loss: 962.595
[23,     1] loss: 958.679
[24,     1] loss: 975.148
[25,     1] loss: 964.097
[26,     1] loss: 975.875
[27,     1] loss: 966.466
[28,     1] loss: 909.815
[29,     1] loss: 910.980
[30,     1] loss: 912.540
[31,     1] loss: 920.231
[32,     1] loss: 955.523
[33,     1] loss: 874.599
[34,     1] loss: 872.971
[35,     1] loss: 931.414
[36,     1] loss: 876.064
[37,     1] loss: 861.960
[38,     1] loss: 875.364
[39,     1] loss: 837.900
[40,     1] loss: 884.876
[41,     1] loss: 783.224
[42,     1] loss: 830.121
[43,     1] loss: 840.695
[44,     1] loss: 883.920
[45,     1] loss: 824.785
[46,     1] loss: 837.429
[47,     1] loss: 786.180
[48,     1] loss: 921.587
[49,     1] loss: 750.307
[50,     1] loss: 818.588
[51,     1] loss: 772.012
[52,     1] loss: 875.983
[53,     1] loss: 729.821
[54,     1] loss: 765.633
[55,     1] loss: 691.778
[56,     1] loss: 722.237
[57,     1] loss: 730.346
[58,     1] loss: 770.416
[59,     1] loss: 656.620
[60,     1] loss: 709.385
[61,     1] loss: 749.977
[62,     1] loss: 715.685
[63,     1] loss: 638.660
[64,     1] loss: 677.995
[65,     1] loss: 702.145
[66,     1] loss: 621.641
[67,     1] loss: 632.593
[68,     1] loss: 612.211
[69,     1] loss: 696.535
[70,     1] loss: 617.680
[71,     1] loss: 593.640
[72,     1] loss: 578.036
[73,     1] loss: 567.754
[74,     1] loss: 566.790
[75,     1] loss: 639.829
[76,     1] loss: 815.138
[77,     1] loss: 1022.279
[78,     1] loss: 566.379
[79,     1] loss: 749.619
[80,     1] loss: 610.156
[81,     1] loss: 645.112
[82,     1] loss: 653.465
[83,     1] loss: 601.793
[84,     1] loss: 584.196
[85,     1] loss: 602.065
[86,     1] loss: 559.233
[87,     1] loss: 579.256
[88,     1] loss: 459.749
[89,     1] loss: 551.231
[90,     1] loss: 478.845
[91,     1] loss: 538.114
[92,     1] loss: 421.936
[93,     1] loss: 480.051
[94,     1] loss: 471.156
[95,     1] loss: 473.643
[96,     1] loss: 530.945
[97,     1] loss: 402.059
[98,     1] loss: 509.665
[99,     1] loss: 400.588
Early stopping applied (best metric=0.3397117257118225)
Finished Training
Total time taken: 14.200515508651733
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.873
[2,     1] loss: 1256.354
[3,     1] loss: 1258.455
[4,     1] loss: 1258.203
[5,     1] loss: 1250.622
[6,     1] loss: 1246.702
[7,     1] loss: 1242.975
[8,     1] loss: 1225.251
[9,     1] loss: 1190.701
[10,     1] loss: 1160.627
[11,     1] loss: 1107.036
[12,     1] loss: 1090.521
[13,     1] loss: 1080.939
[14,     1] loss: 1048.816
[15,     1] loss: 1051.684
[16,     1] loss: 1041.784
[17,     1] loss: 1051.323
[18,     1] loss: 1022.368
[19,     1] loss: 1022.739
[20,     1] loss: 992.951
[21,     1] loss: 979.589
[22,     1] loss: 991.159
[23,     1] loss: 957.949
[24,     1] loss: 972.938
[25,     1] loss: 942.458
[26,     1] loss: 970.646
[27,     1] loss: 952.159
[28,     1] loss: 941.299
[29,     1] loss: 960.880
[30,     1] loss: 953.907
[31,     1] loss: 914.543
[32,     1] loss: 933.784
[33,     1] loss: 960.471
[34,     1] loss: 919.749
[35,     1] loss: 902.552
[36,     1] loss: 921.167
[37,     1] loss: 957.323
[38,     1] loss: 900.625
[39,     1] loss: 827.628
[40,     1] loss: 933.071
[41,     1] loss: 848.187
[42,     1] loss: 844.786
[43,     1] loss: 864.692
[44,     1] loss: 828.716
[45,     1] loss: 860.544
[46,     1] loss: 876.379
[47,     1] loss: 838.929
[48,     1] loss: 812.018
[49,     1] loss: 785.497
[50,     1] loss: 820.052
[51,     1] loss: 790.261
[52,     1] loss: 859.188
[53,     1] loss: 840.718
[54,     1] loss: 730.548
[55,     1] loss: 784.709
[56,     1] loss: 764.933
[57,     1] loss: 713.113
[58,     1] loss: 711.349
[59,     1] loss: 661.518
[60,     1] loss: 658.525
[61,     1] loss: 680.694
[62,     1] loss: 658.032
[63,     1] loss: 649.046
[64,     1] loss: 686.819
[65,     1] loss: 837.044
[66,     1] loss: 895.195
[67,     1] loss: 657.857
[68,     1] loss: 785.750
[69,     1] loss: 676.884
[70,     1] loss: 749.289
[71,     1] loss: 718.302
[72,     1] loss: 643.348
[73,     1] loss: 674.302
[74,     1] loss: 602.635
[75,     1] loss: 639.510
[76,     1] loss: 615.356
[77,     1] loss: 640.224
[78,     1] loss: 565.948
[79,     1] loss: 556.255
[80,     1] loss: 561.293
[81,     1] loss: 542.552
[82,     1] loss: 522.503
[83,     1] loss: 523.914
Early stopping applied (best metric=0.3705260753631592)
Finished Training
Total time taken: 11.936278104782104
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1256.046
[2,     1] loss: 1262.805
[3,     1] loss: 1261.428
[4,     1] loss: 1254.753
[5,     1] loss: 1255.518
[6,     1] loss: 1249.440
[7,     1] loss: 1242.475
[8,     1] loss: 1232.918
[9,     1] loss: 1200.052
[10,     1] loss: 1158.922
[11,     1] loss: 1109.901
[12,     1] loss: 1080.689
[13,     1] loss: 1070.937
[14,     1] loss: 1025.042
[15,     1] loss: 1055.597
[16,     1] loss: 1024.334
[17,     1] loss: 1029.364
[18,     1] loss: 1038.417
[19,     1] loss: 1000.612
[20,     1] loss: 985.100
[21,     1] loss: 996.653
[22,     1] loss: 944.842
[23,     1] loss: 948.108
[24,     1] loss: 934.420
[25,     1] loss: 929.863
[26,     1] loss: 968.501
[27,     1] loss: 894.184
[28,     1] loss: 890.106
[29,     1] loss: 933.564
[30,     1] loss: 936.423
[31,     1] loss: 899.573
[32,     1] loss: 929.815
[33,     1] loss: 898.604
[34,     1] loss: 924.947
[35,     1] loss: 890.077
[36,     1] loss: 908.383
[37,     1] loss: 867.829
[38,     1] loss: 853.120
[39,     1] loss: 841.228
[40,     1] loss: 843.963
[41,     1] loss: 778.054
[42,     1] loss: 859.874
[43,     1] loss: 831.404
[44,     1] loss: 793.641
[45,     1] loss: 803.755
[46,     1] loss: 796.788
[47,     1] loss: 795.740
[48,     1] loss: 839.807
[49,     1] loss: 807.871
[50,     1] loss: 746.324
[51,     1] loss: 758.533
[52,     1] loss: 739.806
[53,     1] loss: 730.180
[54,     1] loss: 710.324
[55,     1] loss: 749.570
[56,     1] loss: 729.145
[57,     1] loss: 771.635
[58,     1] loss: 762.999
[59,     1] loss: 785.802
[60,     1] loss: 695.722
[61,     1] loss: 760.727
[62,     1] loss: 672.002
[63,     1] loss: 700.672
[64,     1] loss: 637.429
[65,     1] loss: 678.598
[66,     1] loss: 649.334
[67,     1] loss: 698.254
[68,     1] loss: 667.008
[69,     1] loss: 719.849
[70,     1] loss: 664.462
[71,     1] loss: 602.107
[72,     1] loss: 654.829
[73,     1] loss: 741.406
[74,     1] loss: 618.790
[75,     1] loss: 628.058
[76,     1] loss: 620.387
[77,     1] loss: 604.132
[78,     1] loss: 543.257
[79,     1] loss: 616.985
[80,     1] loss: 532.036
[81,     1] loss: 549.847
Early stopping applied (best metric=0.4183042347431183)
Finished Training
Total time taken: 11.621241092681885
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1260.925
[2,     1] loss: 1263.861
[3,     1] loss: 1259.820
[4,     1] loss: 1260.787
[5,     1] loss: 1258.324
[6,     1] loss: 1258.127
[7,     1] loss: 1256.149
[8,     1] loss: 1254.781
[9,     1] loss: 1256.239
[10,     1] loss: 1254.359
[11,     1] loss: 1249.228
[12,     1] loss: 1246.677
[13,     1] loss: 1244.371
[14,     1] loss: 1236.076
[15,     1] loss: 1217.138
[16,     1] loss: 1203.389
[17,     1] loss: 1173.128
[18,     1] loss: 1126.814
[19,     1] loss: 1093.016
[20,     1] loss: 1032.914
[21,     1] loss: 1026.886
[22,     1] loss: 1052.405
[23,     1] loss: 1012.512
[24,     1] loss: 1023.556
[25,     1] loss: 988.490
[26,     1] loss: 1000.655
[27,     1] loss: 970.170
[28,     1] loss: 960.614
[29,     1] loss: 978.798
[30,     1] loss: 973.640
[31,     1] loss: 961.968
[32,     1] loss: 986.408
[33,     1] loss: 944.913
[34,     1] loss: 941.905
[35,     1] loss: 894.781
[36,     1] loss: 937.863
[37,     1] loss: 914.617
[38,     1] loss: 915.581
[39,     1] loss: 849.406
[40,     1] loss: 870.564
[41,     1] loss: 881.556
[42,     1] loss: 849.508
[43,     1] loss: 862.668
[44,     1] loss: 839.308
[45,     1] loss: 871.501
[46,     1] loss: 861.807
[47,     1] loss: 815.194
[48,     1] loss: 820.059
[49,     1] loss: 808.027
[50,     1] loss: 816.565
[51,     1] loss: 765.503
[52,     1] loss: 834.397
[53,     1] loss: 804.005
[54,     1] loss: 799.282
[55,     1] loss: 756.461
[56,     1] loss: 763.441
[57,     1] loss: 745.638
[58,     1] loss: 730.908
[59,     1] loss: 741.804
[60,     1] loss: 694.689
[61,     1] loss: 760.405
[62,     1] loss: 766.728
[63,     1] loss: 733.637
[64,     1] loss: 712.361
[65,     1] loss: 681.393
[66,     1] loss: 682.493
[67,     1] loss: 646.914
[68,     1] loss: 640.779
[69,     1] loss: 646.961
[70,     1] loss: 683.735
[71,     1] loss: 741.974
[72,     1] loss: 919.707
[73,     1] loss: 705.364
[74,     1] loss: 680.624
[75,     1] loss: 658.538
[76,     1] loss: 696.668
[77,     1] loss: 647.611
[78,     1] loss: 678.290
[79,     1] loss: 648.555
[80,     1] loss: 644.894
[81,     1] loss: 549.565
[82,     1] loss: 590.862
[83,     1] loss: 576.126
[84,     1] loss: 573.457
[85,     1] loss: 474.346
[86,     1] loss: 505.654
[87,     1] loss: 581.638
[88,     1] loss: 560.934
[89,     1] loss: 518.783
[90,     1] loss: 578.124
[91,     1] loss: 527.448
[92,     1] loss: 493.912
[93,     1] loss: 572.103
[94,     1] loss: 539.381
[95,     1] loss: 591.269
[96,     1] loss: 534.881
Early stopping applied (best metric=0.4283148944377899)
Finished Training
Total time taken: 13.819478750228882
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1263.269
[2,     1] loss: 1263.781
[3,     1] loss: 1259.972
[4,     1] loss: 1257.762
[5,     1] loss: 1257.775
[6,     1] loss: 1259.244
[7,     1] loss: 1253.499
[8,     1] loss: 1250.275
[9,     1] loss: 1240.385
[10,     1] loss: 1241.426
[11,     1] loss: 1216.463
[12,     1] loss: 1191.841
[13,     1] loss: 1173.793
[14,     1] loss: 1139.740
[15,     1] loss: 1106.897
[16,     1] loss: 1083.856
[17,     1] loss: 1107.439
[18,     1] loss: 1031.327
[19,     1] loss: 1032.083
[20,     1] loss: 1049.004
[21,     1] loss: 1012.770
[22,     1] loss: 1020.474
[23,     1] loss: 1053.060
[24,     1] loss: 1072.787
[25,     1] loss: 1011.441
[26,     1] loss: 1018.536
[27,     1] loss: 986.228
[28,     1] loss: 966.410
[29,     1] loss: 1011.092
[30,     1] loss: 945.188
[31,     1] loss: 921.464
[32,     1] loss: 972.037
[33,     1] loss: 974.117
[34,     1] loss: 937.027
[35,     1] loss: 993.047
[36,     1] loss: 947.616
[37,     1] loss: 914.511
[38,     1] loss: 903.948
[39,     1] loss: 950.221
[40,     1] loss: 888.565
[41,     1] loss: 915.332
[42,     1] loss: 872.146
[43,     1] loss: 907.983
[44,     1] loss: 841.182
[45,     1] loss: 858.509
[46,     1] loss: 855.201
[47,     1] loss: 847.453
[48,     1] loss: 892.738
[49,     1] loss: 833.607
[50,     1] loss: 825.604
[51,     1] loss: 808.176
[52,     1] loss: 801.428
[53,     1] loss: 813.938
[54,     1] loss: 773.690
[55,     1] loss: 804.094
[56,     1] loss: 763.564
[57,     1] loss: 738.712
[58,     1] loss: 771.404
[59,     1] loss: 756.386
[60,     1] loss: 724.918
[61,     1] loss: 713.579
[62,     1] loss: 728.122
[63,     1] loss: 732.514
[64,     1] loss: 682.119
[65,     1] loss: 670.550
[66,     1] loss: 783.563
[67,     1] loss: 830.410
[68,     1] loss: 657.817
[69,     1] loss: 811.948
[70,     1] loss: 715.072
[71,     1] loss: 745.752
[72,     1] loss: 655.589
[73,     1] loss: 707.844
[74,     1] loss: 624.760
[75,     1] loss: 674.976
[76,     1] loss: 604.275
[77,     1] loss: 661.282
[78,     1] loss: 581.822
[79,     1] loss: 659.675
[80,     1] loss: 578.868
[81,     1] loss: 667.468
[82,     1] loss: 650.632
[83,     1] loss: 583.609
[84,     1] loss: 555.919
[85,     1] loss: 541.148
[86,     1] loss: 579.503
[87,     1] loss: 453.126
[88,     1] loss: 553.077
[89,     1] loss: 519.036
[90,     1] loss: 500.639
[91,     1] loss: 530.924
[92,     1] loss: 497.287
[93,     1] loss: 594.679
[94,     1] loss: 467.375
[95,     1] loss: 537.158
[96,     1] loss: 485.736
[97,     1] loss: 579.793
[98,     1] loss: 444.051
Early stopping applied (best metric=0.3522995114326477)
Finished Training
Total time taken: 14.125506401062012
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.573
[2,     1] loss: 1256.698
[3,     1] loss: 1257.912
[4,     1] loss: 1256.827
[5,     1] loss: 1252.900
[6,     1] loss: 1252.947
[7,     1] loss: 1251.648
[8,     1] loss: 1245.379
[9,     1] loss: 1241.642
[10,     1] loss: 1226.283
[11,     1] loss: 1213.863
[12,     1] loss: 1173.423
[13,     1] loss: 1142.496
[14,     1] loss: 1118.940
[15,     1] loss: 1131.957
[16,     1] loss: 1085.235
[17,     1] loss: 1046.143
[18,     1] loss: 1086.903
[19,     1] loss: 1058.783
[20,     1] loss: 1054.441
[21,     1] loss: 1032.539
[22,     1] loss: 1016.883
[23,     1] loss: 1076.151
[24,     1] loss: 1012.801
[25,     1] loss: 1001.803
[26,     1] loss: 998.362
[27,     1] loss: 974.516
[28,     1] loss: 986.766
[29,     1] loss: 935.274
[30,     1] loss: 972.759
[31,     1] loss: 937.081
[32,     1] loss: 909.618
[33,     1] loss: 963.842
[34,     1] loss: 918.591
[35,     1] loss: 927.274
[36,     1] loss: 929.466
[37,     1] loss: 946.791
[38,     1] loss: 877.577
[39,     1] loss: 929.763
[40,     1] loss: 904.716
[41,     1] loss: 921.513
[42,     1] loss: 834.301
[43,     1] loss: 874.505
[44,     1] loss: 823.089
[45,     1] loss: 855.509
[46,     1] loss: 866.093
[47,     1] loss: 839.285
[48,     1] loss: 805.674
[49,     1] loss: 853.835
[50,     1] loss: 843.880
[51,     1] loss: 838.126
[52,     1] loss: 795.382
[53,     1] loss: 768.315
[54,     1] loss: 824.011
[55,     1] loss: 763.134
[56,     1] loss: 850.943
[57,     1] loss: 730.576
[58,     1] loss: 779.775
[59,     1] loss: 812.092
[60,     1] loss: 744.194
[61,     1] loss: 788.147
[62,     1] loss: 705.426
[63,     1] loss: 725.195
[64,     1] loss: 705.559
[65,     1] loss: 694.168
[66,     1] loss: 726.538
[67,     1] loss: 768.328
[68,     1] loss: 659.587
[69,     1] loss: 700.669
[70,     1] loss: 635.224
[71,     1] loss: 717.168
[72,     1] loss: 759.161
[73,     1] loss: 678.657
[74,     1] loss: 799.776
[75,     1] loss: 609.094
[76,     1] loss: 737.058
[77,     1] loss: 560.802
[78,     1] loss: 645.651
[79,     1] loss: 544.805
[80,     1] loss: 593.060
[81,     1] loss: 520.658
[82,     1] loss: 568.410
[83,     1] loss: 555.868
[84,     1] loss: 635.937
[85,     1] loss: 646.930
[86,     1] loss: 470.659
[87,     1] loss: 532.100
[88,     1] loss: 535.853
[89,     1] loss: 534.628
[90,     1] loss: 516.389
[91,     1] loss: 488.407
Early stopping applied (best metric=0.33084461092948914)
Finished Training
Total time taken: 13.039393663406372
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.122
[2,     1] loss: 1257.635
[3,     1] loss: 1256.477
[4,     1] loss: 1257.111
[5,     1] loss: 1259.512
[6,     1] loss: 1256.799
[7,     1] loss: 1248.832
[8,     1] loss: 1244.938
[9,     1] loss: 1232.834
[10,     1] loss: 1219.440
[11,     1] loss: 1186.658
[12,     1] loss: 1146.089
[13,     1] loss: 1112.479
[14,     1] loss: 1070.822
[15,     1] loss: 1071.458
[16,     1] loss: 1087.477
[17,     1] loss: 1042.502
[18,     1] loss: 1010.148
[19,     1] loss: 1013.700
[20,     1] loss: 1000.049
[21,     1] loss: 987.528
[22,     1] loss: 1012.672
[23,     1] loss: 1020.104
[24,     1] loss: 1000.286
[25,     1] loss: 986.030
[26,     1] loss: 956.749
[27,     1] loss: 942.955
[28,     1] loss: 912.510
[29,     1] loss: 910.857
[30,     1] loss: 988.379
[31,     1] loss: 965.652
[32,     1] loss: 945.131
[33,     1] loss: 952.902
[34,     1] loss: 931.878
[35,     1] loss: 894.304
[36,     1] loss: 918.143
[37,     1] loss: 893.431
[38,     1] loss: 882.715
[39,     1] loss: 860.291
[40,     1] loss: 901.985
[41,     1] loss: 847.919
[42,     1] loss: 847.767
[43,     1] loss: 861.950
[44,     1] loss: 875.667
[45,     1] loss: 910.503
[46,     1] loss: 875.254
[47,     1] loss: 789.419
[48,     1] loss: 824.111
[49,     1] loss: 773.746
[50,     1] loss: 794.576
[51,     1] loss: 823.938
[52,     1] loss: 825.812
[53,     1] loss: 778.354
[54,     1] loss: 825.932
[55,     1] loss: 739.193
[56,     1] loss: 776.769
[57,     1] loss: 740.105
[58,     1] loss: 789.994
[59,     1] loss: 710.215
[60,     1] loss: 757.934
[61,     1] loss: 768.449
[62,     1] loss: 764.064
[63,     1] loss: 709.389
[64,     1] loss: 724.774
[65,     1] loss: 762.337
[66,     1] loss: 729.596
[67,     1] loss: 656.636
[68,     1] loss: 685.145
[69,     1] loss: 636.964
[70,     1] loss: 683.961
[71,     1] loss: 759.838
[72,     1] loss: 623.757
[73,     1] loss: 684.423
[74,     1] loss: 737.959
[75,     1] loss: 594.333
[76,     1] loss: 582.138
[77,     1] loss: 606.732
[78,     1] loss: 541.378
[79,     1] loss: 615.595
[80,     1] loss: 531.847
[81,     1] loss: 598.203
[82,     1] loss: 543.684
[83,     1] loss: 580.898
[84,     1] loss: 658.588
[85,     1] loss: 1118.123
[86,     1] loss: 596.218
[87,     1] loss: 803.378
[88,     1] loss: 608.757
[89,     1] loss: 770.370
[90,     1] loss: 750.120
[91,     1] loss: 638.035
[92,     1] loss: 691.499
[93,     1] loss: 695.441
[94,     1] loss: 569.675
[95,     1] loss: 598.486
[96,     1] loss: 562.184
[97,     1] loss: 506.845
[98,     1] loss: 616.879
[99,     1] loss: 513.657
[100,     1] loss: 592.364
[101,     1] loss: 503.678
[102,     1] loss: 521.433
[103,     1] loss: 460.241
[104,     1] loss: 465.689
[105,     1] loss: 430.388
[106,     1] loss: 498.941
[107,     1] loss: 407.598
[108,     1] loss: 419.580
[109,     1] loss: 447.547
[110,     1] loss: 364.295
[111,     1] loss: 377.853
[112,     1] loss: 374.016
[113,     1] loss: 390.328
[114,     1] loss: 425.294
[115,     1] loss: 408.470
[116,     1] loss: 352.610
[117,     1] loss: 412.491
[118,     1] loss: 351.869
[119,     1] loss: 369.974
[120,     1] loss: 416.116
[121,     1] loss: 349.425
[122,     1] loss: 365.583
[123,     1] loss: 358.140
[124,     1] loss: 332.488
[125,     1] loss: 358.455
[126,     1] loss: 286.276
[127,     1] loss: 404.054
[128,     1] loss: 369.585
[129,     1] loss: 314.753
[130,     1] loss: 325.593
[131,     1] loss: 324.085
[132,     1] loss: 372.782
[133,     1] loss: 344.036
[134,     1] loss: 263.764
[135,     1] loss: 354.794
[136,     1] loss: 296.723
[137,     1] loss: 310.020
[138,     1] loss: 260.259
[139,     1] loss: 290.439
[140,     1] loss: 340.372
[141,     1] loss: 366.846
[142,     1] loss: 298.009
[143,     1] loss: 338.184
[144,     1] loss: 345.336
Early stopping applied (best metric=0.37112700939178467)
Finished Training
Total time taken: 20.664211750030518
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.726
[2,     1] loss: 1256.090
[3,     1] loss: 1265.573
[4,     1] loss: 1257.172
[5,     1] loss: 1254.846
[6,     1] loss: 1250.391
[7,     1] loss: 1249.563
[8,     1] loss: 1240.364
[9,     1] loss: 1230.855
[10,     1] loss: 1204.095
[11,     1] loss: 1189.456
[12,     1] loss: 1111.816
[13,     1] loss: 1104.571
[14,     1] loss: 1084.954
[15,     1] loss: 1029.070
[16,     1] loss: 1180.339
[17,     1] loss: 1051.174
[18,     1] loss: 1052.529
[19,     1] loss: 992.729
[20,     1] loss: 1076.684
[21,     1] loss: 1049.849
[22,     1] loss: 1021.955
[23,     1] loss: 1038.227
[24,     1] loss: 1004.153
[25,     1] loss: 960.503
[26,     1] loss: 1002.058
[27,     1] loss: 993.566
[28,     1] loss: 992.527
[29,     1] loss: 967.446
[30,     1] loss: 975.843
[31,     1] loss: 961.448
[32,     1] loss: 972.574
[33,     1] loss: 964.007
[34,     1] loss: 953.076
[35,     1] loss: 920.561
[36,     1] loss: 940.739
[37,     1] loss: 926.771
[38,     1] loss: 909.765
[39,     1] loss: 869.762
[40,     1] loss: 929.106
[41,     1] loss: 874.525
[42,     1] loss: 896.204
[43,     1] loss: 840.758
[44,     1] loss: 871.536
[45,     1] loss: 822.251
[46,     1] loss: 894.035
[47,     1] loss: 771.757
[48,     1] loss: 883.606
[49,     1] loss: 818.580
[50,     1] loss: 896.848
[51,     1] loss: 837.820
[52,     1] loss: 841.356
[53,     1] loss: 902.627
[54,     1] loss: 787.200
[55,     1] loss: 788.804
[56,     1] loss: 799.206
[57,     1] loss: 776.998
[58,     1] loss: 762.466
[59,     1] loss: 748.491
[60,     1] loss: 752.456
[61,     1] loss: 705.351
[62,     1] loss: 725.735
[63,     1] loss: 701.408
[64,     1] loss: 712.208
[65,     1] loss: 667.564
[66,     1] loss: 656.376
[67,     1] loss: 847.763
[68,     1] loss: 721.353
[69,     1] loss: 713.905
[70,     1] loss: 614.058
[71,     1] loss: 718.699
[72,     1] loss: 663.424
[73,     1] loss: 611.443
[74,     1] loss: 631.964
[75,     1] loss: 689.571
[76,     1] loss: 640.663
[77,     1] loss: 641.645
[78,     1] loss: 654.804
[79,     1] loss: 541.084
[80,     1] loss: 575.007
[81,     1] loss: 534.568
[82,     1] loss: 585.602
[83,     1] loss: 517.156
[84,     1] loss: 486.738
[85,     1] loss: 467.151
[86,     1] loss: 513.238
[87,     1] loss: 481.054
[88,     1] loss: 475.289
Early stopping applied (best metric=0.35965946316719055)
Finished Training
Total time taken: 12.694355249404907
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.925
[2,     1] loss: 1256.390
[3,     1] loss: 1265.642
[4,     1] loss: 1250.694
[5,     1] loss: 1252.295
[6,     1] loss: 1251.658
[7,     1] loss: 1230.970
[8,     1] loss: 1210.732
[9,     1] loss: 1163.041
[10,     1] loss: 1129.344
[11,     1] loss: 1076.446
[12,     1] loss: 1051.551
[13,     1] loss: 1068.297
[14,     1] loss: 1051.927
[15,     1] loss: 1017.399
[16,     1] loss: 1024.814
[17,     1] loss: 980.601
[18,     1] loss: 997.399
[19,     1] loss: 1004.218
[20,     1] loss: 974.209
[21,     1] loss: 971.403
[22,     1] loss: 944.312
[23,     1] loss: 972.141
[24,     1] loss: 989.440
[25,     1] loss: 931.437
[26,     1] loss: 943.841
[27,     1] loss: 912.266
[28,     1] loss: 914.417
[29,     1] loss: 898.373
[30,     1] loss: 904.895
[31,     1] loss: 891.588
[32,     1] loss: 908.875
[33,     1] loss: 888.583
[34,     1] loss: 903.240
[35,     1] loss: 884.000
[36,     1] loss: 884.814
[37,     1] loss: 870.228
[38,     1] loss: 882.182
[39,     1] loss: 854.821
[40,     1] loss: 823.818
[41,     1] loss: 833.474
[42,     1] loss: 838.863
[43,     1] loss: 828.982
[44,     1] loss: 837.325
[45,     1] loss: 857.261
[46,     1] loss: 802.103
[47,     1] loss: 827.652
[48,     1] loss: 795.911
[49,     1] loss: 872.406
[50,     1] loss: 778.671
[51,     1] loss: 795.367
[52,     1] loss: 749.778
[53,     1] loss: 716.620
[54,     1] loss: 764.890
[55,     1] loss: 700.245
[56,     1] loss: 714.400
[57,     1] loss: 709.623
[58,     1] loss: 732.710
[59,     1] loss: 736.199
[60,     1] loss: 700.932
[61,     1] loss: 721.274
[62,     1] loss: 706.614
[63,     1] loss: 670.800
[64,     1] loss: 680.378
[65,     1] loss: 695.939
[66,     1] loss: 739.643
[67,     1] loss: 790.562
[68,     1] loss: 761.846
[69,     1] loss: 699.312
[70,     1] loss: 698.191
[71,     1] loss: 689.146
[72,     1] loss: 641.447
[73,     1] loss: 622.817
[74,     1] loss: 607.302
[75,     1] loss: 604.381
[76,     1] loss: 642.994
[77,     1] loss: 571.573
[78,     1] loss: 555.048
[79,     1] loss: 603.058
[80,     1] loss: 562.127
[81,     1] loss: 536.411
[82,     1] loss: 547.396
[83,     1] loss: 594.955
[84,     1] loss: 547.411
[85,     1] loss: 496.225
[86,     1] loss: 493.371
[87,     1] loss: 552.813
[88,     1] loss: 476.647
[89,     1] loss: 502.179
[90,     1] loss: 549.709
[91,     1] loss: 501.154
[92,     1] loss: 490.231
[93,     1] loss: 539.147
[94,     1] loss: 606.379
[95,     1] loss: 540.693
[96,     1] loss: 477.934
[97,     1] loss: 594.977
[98,     1] loss: 492.365
[99,     1] loss: 525.808
[100,     1] loss: 477.474
[101,     1] loss: 453.471
[102,     1] loss: 455.210
[103,     1] loss: 400.056
[104,     1] loss: 521.098
[105,     1] loss: 488.433
[106,     1] loss: 443.554
[107,     1] loss: 529.223
[108,     1] loss: 376.537
[109,     1] loss: 509.367
[110,     1] loss: 454.304
[111,     1] loss: 428.856
[112,     1] loss: 434.180
[113,     1] loss: 426.190
[114,     1] loss: 412.126
[115,     1] loss: 373.605
[116,     1] loss: 445.307
[117,     1] loss: 357.948
[118,     1] loss: 408.397
[119,     1] loss: 392.175
[120,     1] loss: 386.837
[121,     1] loss: 451.237
[122,     1] loss: 361.704
[123,     1] loss: 394.188
[124,     1] loss: 362.024
[125,     1] loss: 381.419
[126,     1] loss: 354.849
[127,     1] loss: 346.166
[128,     1] loss: 333.209
[129,     1] loss: 320.962
[130,     1] loss: 387.366
[131,     1] loss: 305.579
[132,     1] loss: 363.031
[133,     1] loss: 393.960
[134,     1] loss: 336.857
[135,     1] loss: 353.792
[136,     1] loss: 328.042
[137,     1] loss: 337.410
[138,     1] loss: 283.562
[139,     1] loss: 270.402
[140,     1] loss: 331.331
[141,     1] loss: 291.088
[142,     1] loss: 284.224
[143,     1] loss: 348.630
[144,     1] loss: 302.176
[145,     1] loss: 275.143
[146,     1] loss: 318.429
[147,     1] loss: 331.110
[148,     1] loss: 347.025
[149,     1] loss: 330.665
[150,     1] loss: 299.445
[151,     1] loss: 325.363
[152,     1] loss: 280.051
[153,     1] loss: 271.685
[154,     1] loss: 321.525
[155,     1] loss: 296.321
[156,     1] loss: 297.515
[157,     1] loss: 362.585
[158,     1] loss: 281.636
Early stopping applied (best metric=0.3740062713623047)
Finished Training
Total time taken: 22.66042160987854
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1263.357
[2,     1] loss: 1258.915
[3,     1] loss: 1259.695
[4,     1] loss: 1261.930
[5,     1] loss: 1258.407
[6,     1] loss: 1253.166
[7,     1] loss: 1249.805
[8,     1] loss: 1229.684
[9,     1] loss: 1220.590
[10,     1] loss: 1184.636
[11,     1] loss: 1133.127
[12,     1] loss: 1083.036
[13,     1] loss: 1069.508
[14,     1] loss: 1088.181
[15,     1] loss: 1065.032
[16,     1] loss: 1077.972
[17,     1] loss: 1002.836
[18,     1] loss: 1035.075
[19,     1] loss: 972.491
[20,     1] loss: 1009.945
[21,     1] loss: 1071.058
[22,     1] loss: 966.969
[23,     1] loss: 1036.006
[24,     1] loss: 969.023
[25,     1] loss: 956.458
[26,     1] loss: 955.603
[27,     1] loss: 995.624
[28,     1] loss: 948.484
[29,     1] loss: 915.110
[30,     1] loss: 964.124
[31,     1] loss: 891.699
[32,     1] loss: 971.655
[33,     1] loss: 903.850
[34,     1] loss: 929.834
[35,     1] loss: 927.430
[36,     1] loss: 855.888
[37,     1] loss: 849.674
[38,     1] loss: 865.964
[39,     1] loss: 822.343
[40,     1] loss: 853.285
[41,     1] loss: 863.979
[42,     1] loss: 847.103
[43,     1] loss: 814.307
[44,     1] loss: 826.422
[45,     1] loss: 825.576
[46,     1] loss: 859.336
[47,     1] loss: 877.083
[48,     1] loss: 851.680
[49,     1] loss: 735.321
[50,     1] loss: 812.073
[51,     1] loss: 753.623
[52,     1] loss: 764.745
[53,     1] loss: 769.374
[54,     1] loss: 718.825
[55,     1] loss: 757.179
[56,     1] loss: 748.465
[57,     1] loss: 758.917
[58,     1] loss: 765.575
[59,     1] loss: 777.464
[60,     1] loss: 712.097
[61,     1] loss: 717.560
[62,     1] loss: 702.135
[63,     1] loss: 680.238
[64,     1] loss: 673.188
[65,     1] loss: 642.624
[66,     1] loss: 684.329
[67,     1] loss: 661.054
[68,     1] loss: 630.620
[69,     1] loss: 654.294
[70,     1] loss: 657.102
[71,     1] loss: 600.454
[72,     1] loss: 657.760
[73,     1] loss: 577.716
[74,     1] loss: 645.281
[75,     1] loss: 658.687
[76,     1] loss: 599.866
[77,     1] loss: 607.422
[78,     1] loss: 654.798
[79,     1] loss: 541.818
[80,     1] loss: 552.107
[81,     1] loss: 639.580
[82,     1] loss: 545.493
[83,     1] loss: 554.472
[84,     1] loss: 628.822
[85,     1] loss: 526.415
Early stopping applied (best metric=0.3995652496814728)
Finished Training
Total time taken: 12.213308334350586
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.513
[2,     1] loss: 1256.513
[3,     1] loss: 1261.516
[4,     1] loss: 1259.191
[5,     1] loss: 1258.312
[6,     1] loss: 1265.492
[7,     1] loss: 1257.889
[8,     1] loss: 1253.907
[9,     1] loss: 1254.253
[10,     1] loss: 1255.337
[11,     1] loss: 1252.825
[12,     1] loss: 1254.437
[13,     1] loss: 1248.319
[14,     1] loss: 1239.015
[15,     1] loss: 1224.507
[16,     1] loss: 1214.149
[17,     1] loss: 1189.737
[18,     1] loss: 1169.754
[19,     1] loss: 1140.091
[20,     1] loss: 1094.987
[21,     1] loss: 1068.662
[22,     1] loss: 1062.906
[23,     1] loss: 1055.298
[24,     1] loss: 1052.972
[25,     1] loss: 1088.975
[26,     1] loss: 1054.475
[27,     1] loss: 1090.067
[28,     1] loss: 1011.481
[29,     1] loss: 1017.297
[30,     1] loss: 1008.849
[31,     1] loss: 1013.141
[32,     1] loss: 988.256
[33,     1] loss: 984.297
[34,     1] loss: 956.232
[35,     1] loss: 979.771
[36,     1] loss: 890.405
[37,     1] loss: 955.055
[38,     1] loss: 909.396
[39,     1] loss: 926.125
[40,     1] loss: 920.095
[41,     1] loss: 911.732
[42,     1] loss: 942.262
[43,     1] loss: 945.282
[44,     1] loss: 894.575
[45,     1] loss: 913.568
[46,     1] loss: 906.341
[47,     1] loss: 884.643
[48,     1] loss: 847.054
[49,     1] loss: 869.264
[50,     1] loss: 850.746
[51,     1] loss: 817.165
[52,     1] loss: 853.638
[53,     1] loss: 843.787
[54,     1] loss: 891.777
[55,     1] loss: 817.346
[56,     1] loss: 843.408
[57,     1] loss: 835.201
[58,     1] loss: 863.432
[59,     1] loss: 831.686
[60,     1] loss: 828.871
[61,     1] loss: 767.320
[62,     1] loss: 777.116
[63,     1] loss: 795.398
[64,     1] loss: 740.845
[65,     1] loss: 756.409
[66,     1] loss: 783.891
[67,     1] loss: 758.148
[68,     1] loss: 729.224
[69,     1] loss: 698.803
[70,     1] loss: 713.184
[71,     1] loss: 677.215
[72,     1] loss: 715.704
[73,     1] loss: 690.028
[74,     1] loss: 677.205
[75,     1] loss: 686.169
[76,     1] loss: 745.058
[77,     1] loss: 614.010
[78,     1] loss: 642.311
[79,     1] loss: 695.252
[80,     1] loss: 650.078
[81,     1] loss: 595.061
[82,     1] loss: 630.267
[83,     1] loss: 594.623
[84,     1] loss: 659.878
[85,     1] loss: 606.515
[86,     1] loss: 528.713
[87,     1] loss: 524.671
[88,     1] loss: 568.890
[89,     1] loss: 531.105
[90,     1] loss: 576.969
[91,     1] loss: 602.947
[92,     1] loss: 473.137
[93,     1] loss: 597.131
[94,     1] loss: 608.461
[95,     1] loss: 513.345
[96,     1] loss: 541.670
[97,     1] loss: 518.678
[98,     1] loss: 552.596
[99,     1] loss: 473.975
[100,     1] loss: 526.631
[101,     1] loss: 464.576
[102,     1] loss: 539.659
[103,     1] loss: 442.539
[104,     1] loss: 583.658
[105,     1] loss: 526.529
[106,     1] loss: 428.799
[107,     1] loss: 578.933
[108,     1] loss: 419.984
Early stopping applied (best metric=0.32731321454048157)
Finished Training
Total time taken: 15.501656770706177
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1255.951
[2,     1] loss: 1262.932
[3,     1] loss: 1260.039
[4,     1] loss: 1256.508
[5,     1] loss: 1249.502
[6,     1] loss: 1251.288
[7,     1] loss: 1241.612
[8,     1] loss: 1227.971
[9,     1] loss: 1200.790
[10,     1] loss: 1157.821
[11,     1] loss: 1112.820
[12,     1] loss: 1073.445
[13,     1] loss: 1081.614
[14,     1] loss: 1058.291
[15,     1] loss: 1059.015
[16,     1] loss: 1035.349
[17,     1] loss: 954.283
[18,     1] loss: 1026.589
[19,     1] loss: 1000.962
[20,     1] loss: 973.660
[21,     1] loss: 1002.912
[22,     1] loss: 952.653
[23,     1] loss: 967.919
[24,     1] loss: 954.861
[25,     1] loss: 1006.686
[26,     1] loss: 989.555
[27,     1] loss: 946.480
[28,     1] loss: 928.245
[29,     1] loss: 919.016
[30,     1] loss: 977.974
[31,     1] loss: 914.397
[32,     1] loss: 943.874
[33,     1] loss: 878.049
[34,     1] loss: 882.495
[35,     1] loss: 885.678
[36,     1] loss: 889.404
[37,     1] loss: 879.175
[38,     1] loss: 852.354
[39,     1] loss: 897.167
[40,     1] loss: 854.553
[41,     1] loss: 848.493
[42,     1] loss: 898.447
[43,     1] loss: 867.562
[44,     1] loss: 874.318
[45,     1] loss: 819.839
[46,     1] loss: 820.925
[47,     1] loss: 866.843
[48,     1] loss: 811.480
[49,     1] loss: 846.293
[50,     1] loss: 860.849
[51,     1] loss: 759.049
[52,     1] loss: 811.216
[53,     1] loss: 834.763
[54,     1] loss: 783.148
[55,     1] loss: 763.573
[56,     1] loss: 719.854
[57,     1] loss: 732.801
[58,     1] loss: 730.200
[59,     1] loss: 818.762
[60,     1] loss: 765.431
[61,     1] loss: 846.570
[62,     1] loss: 700.739
[63,     1] loss: 749.842
[64,     1] loss: 718.962
[65,     1] loss: 709.522
[66,     1] loss: 721.396
[67,     1] loss: 754.605
[68,     1] loss: 696.945
[69,     1] loss: 661.069
[70,     1] loss: 650.229
[71,     1] loss: 687.244
[72,     1] loss: 727.723
[73,     1] loss: 652.430
[74,     1] loss: 599.107
[75,     1] loss: 615.193
[76,     1] loss: 578.105
[77,     1] loss: 562.945
[78,     1] loss: 517.387
[79,     1] loss: 523.748
[80,     1] loss: 554.561
[81,     1] loss: 527.753
[82,     1] loss: 568.685
[83,     1] loss: 504.898
[84,     1] loss: 529.750
[85,     1] loss: 581.244
[86,     1] loss: 496.477
[87,     1] loss: 519.072
[88,     1] loss: 483.566
[89,     1] loss: 486.885
[90,     1] loss: 515.533
[91,     1] loss: 596.391
[92,     1] loss: 752.969
[93,     1] loss: 483.252
[94,     1] loss: 556.272
[95,     1] loss: 555.835
[96,     1] loss: 560.443
[97,     1] loss: 562.469
[98,     1] loss: 463.855
[99,     1] loss: 480.952
[100,     1] loss: 446.968
[101,     1] loss: 433.388
[102,     1] loss: 473.946
[103,     1] loss: 379.406
[104,     1] loss: 379.032
[105,     1] loss: 374.774
Early stopping applied (best metric=0.3746441602706909)
Finished Training
Total time taken: 15.019606590270996
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1258.144
[2,     1] loss: 1261.951
[3,     1] loss: 1256.844
[4,     1] loss: 1258.458
[5,     1] loss: 1259.450
[6,     1] loss: 1256.363
[7,     1] loss: 1257.988
[8,     1] loss: 1257.203
[9,     1] loss: 1255.884
[10,     1] loss: 1256.639
[11,     1] loss: 1253.700
[12,     1] loss: 1247.868
[13,     1] loss: 1248.612
[14,     1] loss: 1242.902
[15,     1] loss: 1235.004
[16,     1] loss: 1226.626
[17,     1] loss: 1206.228
[18,     1] loss: 1173.027
[19,     1] loss: 1137.626
[20,     1] loss: 1097.099
[21,     1] loss: 1077.376
[22,     1] loss: 1037.131
[23,     1] loss: 1081.157
[24,     1] loss: 1102.990
[25,     1] loss: 1031.655
[26,     1] loss: 983.729
[27,     1] loss: 985.027
[28,     1] loss: 1052.332
[29,     1] loss: 993.482
[30,     1] loss: 1010.807
[31,     1] loss: 975.741
[32,     1] loss: 989.345
[33,     1] loss: 972.520
[34,     1] loss: 962.679
[35,     1] loss: 940.680
[36,     1] loss: 954.425
[37,     1] loss: 918.207
[38,     1] loss: 947.959
[39,     1] loss: 934.518
[40,     1] loss: 916.772
[41,     1] loss: 903.333
[42,     1] loss: 956.132
[43,     1] loss: 873.043
[44,     1] loss: 859.764
[45,     1] loss: 901.013
[46,     1] loss: 923.048
[47,     1] loss: 870.894
[48,     1] loss: 882.042
[49,     1] loss: 867.101
[50,     1] loss: 846.733
[51,     1] loss: 855.727
[52,     1] loss: 863.770
[53,     1] loss: 852.763
[54,     1] loss: 849.950
[55,     1] loss: 820.170
[56,     1] loss: 809.514
[57,     1] loss: 821.345
[58,     1] loss: 823.928
[59,     1] loss: 809.016
[60,     1] loss: 789.528
[61,     1] loss: 852.181
[62,     1] loss: 773.671
[63,     1] loss: 789.425
[64,     1] loss: 737.258
[65,     1] loss: 753.879
[66,     1] loss: 808.864
[67,     1] loss: 745.323
[68,     1] loss: 800.757
[69,     1] loss: 764.594
[70,     1] loss: 801.781
[71,     1] loss: 771.066
[72,     1] loss: 685.492
[73,     1] loss: 709.199
[74,     1] loss: 661.271
[75,     1] loss: 654.355
[76,     1] loss: 657.758
[77,     1] loss: 665.720
[78,     1] loss: 646.625
[79,     1] loss: 631.179
[80,     1] loss: 641.500
[81,     1] loss: 613.100
[82,     1] loss: 573.223
[83,     1] loss: 576.380
[84,     1] loss: 604.779
[85,     1] loss: 604.671
[86,     1] loss: 576.983
[87,     1] loss: 560.706
[88,     1] loss: 629.857
[89,     1] loss: 638.437
[90,     1] loss: 619.628
[91,     1] loss: 599.187
[92,     1] loss: 557.089
[93,     1] loss: 658.955
[94,     1] loss: 576.461
[95,     1] loss: 529.013
[96,     1] loss: 528.107
[97,     1] loss: 513.498
[98,     1] loss: 489.866
[99,     1] loss: 512.046
[100,     1] loss: 479.962
[101,     1] loss: 432.228
[102,     1] loss: 584.151
[103,     1] loss: 549.695
[104,     1] loss: 458.652
[105,     1] loss: 607.747
[106,     1] loss: 520.813
[107,     1] loss: 588.612
[108,     1] loss: 496.971
[109,     1] loss: 516.903
[110,     1] loss: 450.313
[111,     1] loss: 505.896
[112,     1] loss: 442.161
[113,     1] loss: 502.564
[114,     1] loss: 443.348
Early stopping applied (best metric=0.4016817808151245)
Finished Training
Total time taken: 16.316744327545166
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1265.274
[2,     1] loss: 1259.940
[3,     1] loss: 1259.088
[4,     1] loss: 1257.140
[5,     1] loss: 1256.628
[6,     1] loss: 1251.869
[7,     1] loss: 1255.174
[8,     1] loss: 1258.934
[9,     1] loss: 1255.568
[10,     1] loss: 1251.085
[11,     1] loss: 1250.609
[12,     1] loss: 1242.142
[13,     1] loss: 1228.383
[14,     1] loss: 1219.269
[15,     1] loss: 1200.467
[16,     1] loss: 1164.963
[17,     1] loss: 1132.966
[18,     1] loss: 1129.274
[19,     1] loss: 1071.507
[20,     1] loss: 1139.491
[21,     1] loss: 1038.441
[22,     1] loss: 1040.520
[23,     1] loss: 1052.564
[24,     1] loss: 1049.459
[25,     1] loss: 1040.166
[26,     1] loss: 1054.573
[27,     1] loss: 1014.422
[28,     1] loss: 1036.550
[29,     1] loss: 1009.859
[30,     1] loss: 1001.367
[31,     1] loss: 1039.950
[32,     1] loss: 990.438
[33,     1] loss: 981.675
[34,     1] loss: 985.351
[35,     1] loss: 960.709
[36,     1] loss: 941.365
[37,     1] loss: 934.194
[38,     1] loss: 961.263
[39,     1] loss: 884.723
[40,     1] loss: 905.982
[41,     1] loss: 916.354
[42,     1] loss: 882.411
[43,     1] loss: 922.724
[44,     1] loss: 887.112
[45,     1] loss: 888.357
[46,     1] loss: 905.764
[47,     1] loss: 851.905
[48,     1] loss: 914.296
[49,     1] loss: 849.114
[50,     1] loss: 866.279
[51,     1] loss: 845.832
[52,     1] loss: 852.432
[53,     1] loss: 851.633
[54,     1] loss: 822.236
[55,     1] loss: 801.964
[56,     1] loss: 809.660
[57,     1] loss: 814.504
[58,     1] loss: 778.783
[59,     1] loss: 815.256
[60,     1] loss: 778.029
[61,     1] loss: 815.137
[62,     1] loss: 770.104
[63,     1] loss: 746.938
[64,     1] loss: 710.937
[65,     1] loss: 690.816
[66,     1] loss: 757.737
[67,     1] loss: 803.985
[68,     1] loss: 708.174
[69,     1] loss: 718.743
[70,     1] loss: 722.321
[71,     1] loss: 666.233
[72,     1] loss: 764.464
[73,     1] loss: 660.719
[74,     1] loss: 675.168
[75,     1] loss: 615.846
[76,     1] loss: 655.721
[77,     1] loss: 575.889
[78,     1] loss: 606.775
[79,     1] loss: 561.678
[80,     1] loss: 562.689
[81,     1] loss: 649.222
[82,     1] loss: 646.472
[83,     1] loss: 629.944
[84,     1] loss: 498.958
[85,     1] loss: 647.640
[86,     1] loss: 542.844
[87,     1] loss: 613.777
[88,     1] loss: 575.493
[89,     1] loss: 547.249
[90,     1] loss: 537.786
[91,     1] loss: 579.576
[92,     1] loss: 524.902
[93,     1] loss: 475.790
[94,     1] loss: 553.476
[95,     1] loss: 475.164
[96,     1] loss: 566.073
[97,     1] loss: 459.174
[98,     1] loss: 510.110
[99,     1] loss: 477.082
[100,     1] loss: 463.340
[101,     1] loss: 516.299
[102,     1] loss: 420.552
[103,     1] loss: 524.553
[104,     1] loss: 493.281
[105,     1] loss: 503.366
[106,     1] loss: 519.353
[107,     1] loss: 447.842
[108,     1] loss: 506.427
[109,     1] loss: 404.912
Early stopping applied (best metric=0.2870430052280426)
Finished Training
Total time taken: 15.509658336639404
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1269.820
[2,     1] loss: 1264.419
[3,     1] loss: 1262.738
[4,     1] loss: 1260.453
[5,     1] loss: 1260.791
[6,     1] loss: 1255.628
[7,     1] loss: 1257.159
[8,     1] loss: 1257.484
[9,     1] loss: 1254.968
[10,     1] loss: 1252.605
[11,     1] loss: 1252.044
[12,     1] loss: 1245.199
[13,     1] loss: 1239.957
[14,     1] loss: 1226.308
[15,     1] loss: 1213.937
[16,     1] loss: 1183.427
[17,     1] loss: 1165.412
[18,     1] loss: 1138.026
[19,     1] loss: 1116.717
[20,     1] loss: 1083.332
[21,     1] loss: 1084.776
[22,     1] loss: 1079.256
[23,     1] loss: 1087.041
[24,     1] loss: 1070.343
[25,     1] loss: 1030.237
[26,     1] loss: 1036.512
[27,     1] loss: 1044.800
[28,     1] loss: 997.687
[29,     1] loss: 1028.481
[30,     1] loss: 1003.962
[31,     1] loss: 1001.080
[32,     1] loss: 974.524
[33,     1] loss: 973.921
[34,     1] loss: 976.820
[35,     1] loss: 1004.143
[36,     1] loss: 980.055
[37,     1] loss: 959.763
[38,     1] loss: 933.970
[39,     1] loss: 919.164
[40,     1] loss: 938.750
[41,     1] loss: 965.509
[42,     1] loss: 955.738
[43,     1] loss: 878.818
[44,     1] loss: 887.575
[45,     1] loss: 889.382
[46,     1] loss: 854.861
[47,     1] loss: 921.088
[48,     1] loss: 888.413
[49,     1] loss: 823.049
[50,     1] loss: 898.717
[51,     1] loss: 806.931
[52,     1] loss: 848.599
[53,     1] loss: 825.881
[54,     1] loss: 852.962
[55,     1] loss: 810.304
[56,     1] loss: 748.211
[57,     1] loss: 785.280
[58,     1] loss: 784.644
[59,     1] loss: 770.925
[60,     1] loss: 792.787
[61,     1] loss: 729.098
[62,     1] loss: 753.655
[63,     1] loss: 820.957
[64,     1] loss: 816.181
[65,     1] loss: 743.502
[66,     1] loss: 669.327
[67,     1] loss: 723.197
[68,     1] loss: 720.031
[69,     1] loss: 669.159
[70,     1] loss: 665.720
[71,     1] loss: 611.656
[72,     1] loss: 647.770
[73,     1] loss: 589.909
[74,     1] loss: 576.671
[75,     1] loss: 702.521
[76,     1] loss: 676.539
[77,     1] loss: 688.944
[78,     1] loss: 612.339
[79,     1] loss: 624.324
[80,     1] loss: 575.195
[81,     1] loss: 603.559
[82,     1] loss: 597.043
[83,     1] loss: 582.037
[84,     1] loss: 510.266
[85,     1] loss: 555.591
[86,     1] loss: 514.256
[87,     1] loss: 593.020
[88,     1] loss: 583.041
[89,     1] loss: 479.232
[90,     1] loss: 611.746
[91,     1] loss: 677.411
Early stopping applied (best metric=0.35544633865356445)
Finished Training
Total time taken: 13.0823974609375
{'Hydroxylation-K Validation Accuracy': 0.7155437352245863, 'Hydroxylation-K Validation Sensitivity': 0.5325925925925926, 'Hydroxylation-K Validation Specificity': 0.7614035087719299, 'Hydroxylation-K Validation Precision': 0.37584099136730714, 'Hydroxylation-K AUC ROC': 0.7287719298245614, 'Hydroxylation-K AUC PR': 0.5190286807875807, 'Hydroxylation-K MCC': 0.2657671481522563, 'Hydroxylation-K F1': 0.4349289724950718, 'Validation Loss (Hydroxylation-K)': 0.5708056370417277, 'Hydroxylation-P Validation Accuracy': 0.7999998308038508, 'Hydroxylation-P Validation Sensitivity': 0.7893121693121693, 'Hydroxylation-P Validation Specificity': 0.8022370193027084, 'Hydroxylation-P Validation Precision': 0.46481647074906124, 'Hydroxylation-P AUC ROC': 0.8532485945148671, 'Hydroxylation-P AUC PR': 0.565248907374463, 'Hydroxylation-P MCC': 0.4932777647285242, 'Hydroxylation-P F1': 0.583520869383135, 'Validation Loss (Hydroxylation-P)': 0.36603250304857887, 'Validation Loss (total)': 0.9368381420771281, 'TimeToTrain': 14.826984930038453}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0044725862434018765,
 'learning_rate_Hydroxylation-K': 0.006896722284491472,
 'learning_rate_Hydroxylation-P': 0.005331465360527629,
 'log_base': 2.7778615145872476,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 170816577,
 'sample_weights': [1.6588028019337022, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.648274064424154,
 'weight_decay_Hydroxylation-K': 8.450727837380747,
 'weight_decay_Hydroxylation-P': 0.044060231807607764}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1259.953
[2,     1] loss: 1251.027
[3,     1] loss: 1252.466
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0050680085837731,
 'learning_rate_Hydroxylation-K': 0.0033142938857729865,
 'learning_rate_Hydroxylation-P': 0.005548753092746051,
 'log_base': 2.1230674749620624,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1775127015,
 'sample_weights': [1.6340154180727506, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.999904409145492,
 'weight_decay_Hydroxylation-K': 1.5676211074756203,
 'weight_decay_Hydroxylation-P': 4.5407702932820255}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1377.643
[2,     1] loss: 1380.098
[3,     1] loss: 1377.167
[4,     1] loss: 1372.597
[5,     1] loss: 1375.377
[6,     1] loss: 1371.579
[7,     1] loss: 1367.767
[8,     1] loss: 1361.228
[9,     1] loss: 1347.938
[10,     1] loss: 1323.117
[11,     1] loss: 1299.331
[12,     1] loss: 1248.502
[13,     1] loss: 1214.398
[14,     1] loss: 1196.561
[15,     1] loss: 1158.277
[16,     1] loss: 1130.938
[17,     1] loss: 1074.649
[18,     1] loss: 1181.071
[19,     1] loss: 1128.938
[20,     1] loss: 1087.830
[21,     1] loss: 1146.985
[22,     1] loss: 1098.751
[23,     1] loss: 1098.008
[24,     1] loss: 1082.376
[25,     1] loss: 1126.149
[26,     1] loss: 1082.238
[27,     1] loss: 1076.067
[28,     1] loss: 1060.792
[29,     1] loss: 1113.694
[30,     1] loss: 1031.314
[31,     1] loss: 979.615
[32,     1] loss: 1074.598
[33,     1] loss: 1008.986
[34,     1] loss: 947.636
[35,     1] loss: 960.999
[36,     1] loss: 974.101
[37,     1] loss: 944.430
[38,     1] loss: 982.238
[39,     1] loss: 919.055
[40,     1] loss: 957.785
[41,     1] loss: 1200.376
[42,     1] loss: 1217.283
[43,     1] loss: 1073.288
[44,     1] loss: 1069.031
[45,     1] loss: 1102.593
[46,     1] loss: 1096.023
[47,     1] loss: 1056.331
[48,     1] loss: 992.319
[49,     1] loss: 1020.806
[50,     1] loss: 1037.634
[51,     1] loss: 986.808
[52,     1] loss: 986.949
[53,     1] loss: 916.196
[54,     1] loss: 923.099
[55,     1] loss: 942.836
[56,     1] loss: 923.986
[57,     1] loss: 945.140
[58,     1] loss: 927.353
[59,     1] loss: 913.900
[60,     1] loss: 905.340
[61,     1] loss: 890.699
[62,     1] loss: 856.596
[63,     1] loss: 883.719
[64,     1] loss: 831.152
[65,     1] loss: 803.023
[66,     1] loss: 812.095
[67,     1] loss: 1080.441
[68,     1] loss: 1244.558
[69,     1] loss: 844.990
[70,     1] loss: 1024.292
[71,     1] loss: 946.170
[72,     1] loss: 885.454
[73,     1] loss: 953.351
[74,     1] loss: 905.211
[75,     1] loss: 961.658
[76,     1] loss: 927.247
[77,     1] loss: 864.426
[78,     1] loss: 823.775
[79,     1] loss: 845.965
[80,     1] loss: 800.993
[81,     1] loss: 798.487
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0035617968813539905,
 'learning_rate_Hydroxylation-K': 0.003970020792093407,
 'learning_rate_Hydroxylation-P': 0.0011279230864816242,
 'log_base': 1.5180238667748829,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 837097674,
 'sample_weights': [2.2174624631315414, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.442372946746156,
 'weight_decay_Hydroxylation-K': 1.2512990306618974,
 'weight_decay_Hydroxylation-P': 2.745343565369855}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1752.866
[2,     1] loss: 1748.887
[3,     1] loss: 1746.136
[4,     1] loss: 1754.928
[5,     1] loss: 1749.335
[6,     1] loss: 1751.552
[7,     1] loss: 1749.484
[8,     1] loss: 1751.224
[9,     1] loss: 1739.516
[10,     1] loss: 1738.896
[11,     1] loss: 1726.664
[12,     1] loss: 1708.931
[13,     1] loss: 1680.208
[14,     1] loss: 1652.650
[15,     1] loss: 1597.715
[16,     1] loss: 1582.744
[17,     1] loss: 1557.141
[18,     1] loss: 1527.286
[19,     1] loss: 1465.138
[20,     1] loss: 1446.867
[21,     1] loss: 1404.184
[22,     1] loss: 1386.860
[23,     1] loss: 1454.207
[24,     1] loss: 1464.424
[25,     1] loss: 1386.809
[26,     1] loss: 1384.866
[27,     1] loss: 1403.010
[28,     1] loss: 1372.549
[29,     1] loss: 1345.635
[30,     1] loss: 1313.601
[31,     1] loss: 1313.912
[32,     1] loss: 1266.347
[33,     1] loss: 1289.565
[34,     1] loss: 1229.568
[35,     1] loss: 1295.629
[36,     1] loss: 1321.004
[37,     1] loss: 1322.031
[38,     1] loss: 1177.335
[39,     1] loss: 1250.654
[40,     1] loss: 1221.130
[41,     1] loss: 1140.324
[42,     1] loss: 1124.015
[43,     1] loss: 1090.434
[44,     1] loss: 1158.227
[45,     1] loss: 1151.010
[46,     1] loss: 1121.161
[47,     1] loss: 1070.332
[48,     1] loss: 1099.021
[49,     1] loss: 1377.041
[50,     1] loss: 1061.167
[51,     1] loss: 1192.411
[52,     1] loss: 1076.332
[53,     1] loss: 1188.781
[54,     1] loss: 1174.117
[55,     1] loss: 1038.850
[56,     1] loss: 1168.166
[57,     1] loss: 1062.138
[58,     1] loss: 1159.385
[59,     1] loss: 945.521
[60,     1] loss: 930.705
[61,     1] loss: 1018.480
[62,     1] loss: 988.266
[63,     1] loss: 891.870
[64,     1] loss: 962.165
[65,     1] loss: 810.286
[66,     1] loss: 864.324
[67,     1] loss: 771.699
[68,     1] loss: 902.779
[69,     1] loss: 925.324
[70,     1] loss: 776.090
[71,     1] loss: 1027.611
[72,     1] loss: 1054.212
[73,     1] loss: 804.271
[74,     1] loss: 1004.096
[75,     1] loss: 791.572
[76,     1] loss: 977.282
[77,     1] loss: 857.443
[78,     1] loss: 929.200
[79,     1] loss: 854.293
[80,     1] loss: 845.980
[81,     1] loss: 892.707
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00647394256191507,
 'learning_rate_Hydroxylation-K': 0.009341391360944197,
 'learning_rate_Hydroxylation-P': 0.009540324971867119,
 'log_base': 2.6861819003503924,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 86199998,
 'sample_weights': [3.9995341311732604, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.804210268863772,
 'weight_decay_Hydroxylation-K': 7.339844393328073,
 'weight_decay_Hydroxylation-P': 3.3759061086283557}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.022
[2,     1] loss: 1266.896
[3,     1] loss: 1265.475
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0057204295340400665,
 'learning_rate_Hydroxylation-K': 0.0017403637062098126,
 'learning_rate_Hydroxylation-P': 0.004564616535346797,
 'log_base': 2.016811213148083,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1502954077,
 'sample_weights': [1.6895131829317172, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.885743297220353,
 'weight_decay_Hydroxylation-K': 1.8454829407733975,
 'weight_decay_Hydroxylation-P': 4.903838951022369}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1408.879
[2,     1] loss: 1409.017
[3,     1] loss: 1405.801
[4,     1] loss: 1409.677
[5,     1] loss: 1408.004
[6,     1] loss: 1403.735
[7,     1] loss: 1398.083
[8,     1] loss: 1384.693
[9,     1] loss: 1352.457
[10,     1] loss: 1324.679
[11,     1] loss: 1291.801
[12,     1] loss: 1214.961
[13,     1] loss: 1196.605
[14,     1] loss: 1144.064
[15,     1] loss: 1156.154
[16,     1] loss: 1194.109
[17,     1] loss: 1166.261
[18,     1] loss: 1099.672
[19,     1] loss: 1071.138
[20,     1] loss: 1088.093
[21,     1] loss: 1067.115
[22,     1] loss: 1085.552
[23,     1] loss: 1078.582
[24,     1] loss: 1041.170
[25,     1] loss: 1079.874
[26,     1] loss: 1050.928
[27,     1] loss: 998.095
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003954693201669537,
 'learning_rate_Hydroxylation-K': 0.0057812472710256305,
 'learning_rate_Hydroxylation-P': 0.0009619380401649812,
 'log_base': 2.5334027844761997,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4213859975,
 'sample_weights': [2.379759271013157, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3443553736214144,
 'weight_decay_Hydroxylation-K': 8.087965294227887,
 'weight_decay_Hydroxylation-P': 7.968466160161897}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1288.855
[2,     1] loss: 1286.601
[3,     1] loss: 1289.698
[4,     1] loss: 1286.262
[5,     1] loss: 1284.407
[6,     1] loss: 1285.124
[7,     1] loss: 1281.612
[8,     1] loss: 1278.554
[9,     1] loss: 1275.202
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004226809843544311,
 'learning_rate_Hydroxylation-K': 0.009351592801625487,
 'learning_rate_Hydroxylation-P': 0.009639891459534219,
 'log_base': 2.383144365554076,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 802308489,
 'sample_weights': [1.7959433381338687, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.65843085008162,
 'weight_decay_Hydroxylation-K': 5.2365593574489395,
 'weight_decay_Hydroxylation-P': 1.8470330187055441}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1314.564
[2,     1] loss: 1308.327
[3,     1] loss: 1310.049
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004825757731033292,
 'learning_rate_Hydroxylation-K': 0.008953358364770427,
 'learning_rate_Hydroxylation-P': 0.007538053236949312,
 'log_base': 2.9608364454450653,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 811858666,
 'sample_weights': [1.9223896874251105, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 3.0768552162593634,
 'weight_decay_Hydroxylation-K': 9.102276636511686,
 'weight_decay_Hydroxylation-P': 6.171164987430742}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.288
[2,     1] loss: 1233.535
[3,     1] loss: 1234.778
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008027979294292891,
 'learning_rate_Hydroxylation-K': 0.008638316070412088,
 'learning_rate_Hydroxylation-P': 0.0077045825646536465,
 'log_base': 2.7542577127412673,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3655133704,
 'sample_weights': [1.537988485731851, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.3816881254398634,
 'weight_decay_Hydroxylation-K': 5.891419219005655,
 'weight_decay_Hydroxylation-P': 0.683944589588894}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.414
[2,     1] loss: 1256.849
[3,     1] loss: 1256.220
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008251463801901332,
 'learning_rate_Hydroxylation-K': 0.009247732750592317,
 'learning_rate_Hydroxylation-P': 0.009280299218713577,
 'log_base': 2.8617479551096765,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4204940511,
 'sample_weights': [1.6477782032088197, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3916274459635676,
 'weight_decay_Hydroxylation-K': 5.8499454341512,
 'weight_decay_Hydroxylation-P': 3.4751761506593297}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1239.937
[2,     1] loss: 1250.200
[3,     1] loss: 1245.043
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005054950619116888,
 'learning_rate_Hydroxylation-K': 0.003999442301736832,
 'learning_rate_Hydroxylation-P': 0.0015801193716916777,
 'log_base': 1.9426586086235784,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2269055860,
 'sample_weights': [1.5877795015584877, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.3316347135970448,
 'weight_decay_Hydroxylation-K': 8.638074894176379,
 'weight_decay_Hydroxylation-P': 9.632437117053927}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1441.522
[2,     1] loss: 1447.879
[3,     1] loss: 1437.015
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0017044245318123658,
 'learning_rate_Hydroxylation-K': 0.0022114331155790486,
 'learning_rate_Hydroxylation-P': 0.00010468174387177068,
 'log_base': 2.2866794062467695,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1353173194,
 'sample_weights': [2.5140040866659357, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 1.1638985340714298,
 'weight_decay_Hydroxylation-K': 9.497009835280059,
 'weight_decay_Hydroxylation-P': 9.995956858125066}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1332.502
[2,     1] loss: 1336.175
[3,     1] loss: 1331.906
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00636024156036603,
 'learning_rate_Hydroxylation-K': 0.005139954835328171,
 'learning_rate_Hydroxylation-P': 0.009896566535599716,
 'log_base': 2.8791994894175623,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 844332253,
 'sample_weights': [2.0184278629889643, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.6795876664208382,
 'weight_decay_Hydroxylation-K': 4.828995200124865,
 'weight_decay_Hydroxylation-P': 4.509576217195804}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.692
[2,     1] loss: 1242.567
[3,     1] loss: 1239.832
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003209151491344099,
 'learning_rate_Hydroxylation-K': 0.005325250396435105,
 'learning_rate_Hydroxylation-P': 0.0052912642307762,
 'log_base': 1.558403167237765,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2093496154,
 'sample_weights': [1.5786512806675528, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.806866508687191,
 'weight_decay_Hydroxylation-K': 6.540205761528432,
 'weight_decay_Hydroxylation-P': 4.2812250635444755}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1699.988
[2,     1] loss: 1705.800
[3,     1] loss: 1699.259
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00427039715730285,
 'learning_rate_Hydroxylation-K': 0.0064509354153695075,
 'learning_rate_Hydroxylation-P': 0.001010072921919821,
 'log_base': 1.5386426003938967,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 847351468,
 'sample_weights': [3.7628742780616005, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.915720996736876,
 'weight_decay_Hydroxylation-K': 6.445608996044782,
 'weight_decay_Hydroxylation-P': 7.164089764786451}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1725.331
[2,     1] loss: 1722.382
[3,     1] loss: 1722.979
[4,     1] loss: 1728.417
[5,     1] loss: 1717.540
[6,     1] loss: 1724.756
[7,     1] loss: 1720.616
[8,     1] loss: 1711.813
[9,     1] loss: 1703.307
[10,     1] loss: 1697.226
[11,     1] loss: 1675.481
[12,     1] loss: 1646.684
[13,     1] loss: 1616.542
[14,     1] loss: 1578.402
[15,     1] loss: 1546.876
[16,     1] loss: 1519.540
[17,     1] loss: 1524.242
[18,     1] loss: 1481.089
[19,     1] loss: 1458.199
[20,     1] loss: 1433.437
[21,     1] loss: 1406.117
[22,     1] loss: 1452.587
[23,     1] loss: 1420.057
[24,     1] loss: 1367.928
[25,     1] loss: 1382.117
[26,     1] loss: 1336.447
[27,     1] loss: 1312.546
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006681659523962115,
 'learning_rate_Hydroxylation-K': 0.00816551944131658,
 'learning_rate_Hydroxylation-P': 0.003215478511504654,
 'log_base': 1.311289579384729,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1433725511,
 'sample_weights': [3.8743114993506342, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.465277576378414,
 'weight_decay_Hydroxylation-K': 6.793910160837925,
 'weight_decay_Hydroxylation-P': 2.7781487530640896}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2206.836
[2,     1] loss: 2216.916
[3,     1] loss: 2200.581
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005832600581565686,
 'learning_rate_Hydroxylation-K': 0.0037194404672848446,
 'learning_rate_Hydroxylation-P': 0.006268380965772132,
 'log_base': 2.166594151913056,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3061134268,
 'sample_weights': [6.160055306284958, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.460723189162863,
 'weight_decay_Hydroxylation-K': 3.847873035566932,
 'weight_decay_Hydroxylation-P': 1.0451568886073903}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1365.429
[2,     1] loss: 1368.007
[3,     1] loss: 1364.048
[4,     1] loss: 1365.298
[5,     1] loss: 1361.901
[6,     1] loss: 1360.456
[7,     1] loss: 1365.786
[8,     1] loss: 1362.550
[9,     1] loss: 1363.538
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00401044801918348,
 'learning_rate_Hydroxylation-K': 0.006282911800161481,
 'learning_rate_Hydroxylation-P': 0.004962060017732138,
 'log_base': 1.3540462383360228,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3876836936,
 'sample_weights': [2.1592566597065965, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.813834303792967,
 'weight_decay_Hydroxylation-K': 8.960689295376682,
 'weight_decay_Hydroxylation-P': 6.0639416795458265}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2085.605
[2,     1] loss: 2071.610
[3,     1] loss: 2070.336
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006317733761063253,
 'learning_rate_Hydroxylation-K': 0.002882523995358362,
 'learning_rate_Hydroxylation-P': 0.009944424047710655,
 'log_base': 1.4390176066234126,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3697055833,
 'sample_weights': [5.507944210829368, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.04525476466904,
 'weight_decay_Hydroxylation-K': 4.121281081067252,
 'weight_decay_Hydroxylation-P': 3.9756715945197634}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1879.707
[2,     1] loss: 1878.288
[3,     1] loss: 1880.585
[4,     1] loss: 1873.693
[5,     1] loss: 1879.476
[6,     1] loss: 1877.391
[7,     1] loss: 1871.741
[8,     1] loss: 1872.156
[9,     1] loss: 1866.899
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008293396493354161,
 'learning_rate_Hydroxylation-K': 0.0042740039393350145,
 'learning_rate_Hydroxylation-P': 0.004094786256605971,
 'log_base': 1.9856561384696203,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4266282250,
 'sample_weights': [4.586877969118571, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.303359522744054,
 'weight_decay_Hydroxylation-K': 1.5713435036146886,
 'weight_decay_Hydroxylation-P': 4.04856894211392}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1424.504
[2,     1] loss: 1420.222
[3,     1] loss: 1425.288
[4,     1] loss: 1422.924
[5,     1] loss: 1425.057
[6,     1] loss: 1429.678
[7,     1] loss: 1423.899
[8,     1] loss: 1420.812
[9,     1] loss: 1421.940
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0061101746158055215,
 'learning_rate_Hydroxylation-K': 0.0009597994996102446,
 'learning_rate_Hydroxylation-P': 0.009135862703228876,
 'log_base': 1.0529135770658085,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4188737529,
 'sample_weights': [2.4337700831518845, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.136041636077133,
 'weight_decay_Hydroxylation-K': 3.0954700153560815,
 'weight_decay_Hydroxylation-P': 6.1817750096866515}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 10584.432
[2,     1] loss: 10529.055
[3,     1] loss: 10483.497
[4,     1] loss: 10571.471
[5,     1] loss: 10485.689
[6,     1] loss: 10503.413
[7,     1] loss: 10482.885
[8,     1] loss: 10482.529
[9,     1] loss: 10515.489
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0025784863917225767,
 'learning_rate_Hydroxylation-K': 0.0026361625063324526,
 'learning_rate_Hydroxylation-P': 0.00861849853846536,
 'log_base': 2.7791907241206966,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1267792240,
 'sample_weights': [32.37792271482243, 4.047396255302892],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.32739408794268376,
 'weight_decay_Hydroxylation-K': 8.692313377323275,
 'weight_decay_Hydroxylation-P': 3.4063042264250045}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1252.471
[2,     1] loss: 1251.459
[3,     1] loss: 1251.384
[4,     1] loss: 1249.493
[5,     1] loss: 1248.526
[6,     1] loss: 1245.238
[7,     1] loss: 1244.397
[8,     1] loss: 1234.788
[9,     1] loss: 1222.127
[10,     1] loss: 1203.058
[11,     1] loss: 1180.194
[12,     1] loss: 1137.181
[13,     1] loss: 1125.054
[14,     1] loss: 1090.608
[15,     1] loss: 1104.317
[16,     1] loss: 1030.399
[17,     1] loss: 1036.576
[18,     1] loss: 1023.181
[19,     1] loss: 1046.730
[20,     1] loss: 1022.792
[21,     1] loss: 1066.431
[22,     1] loss: 996.499
[23,     1] loss: 1009.953
[24,     1] loss: 985.664
[25,     1] loss: 996.813
[26,     1] loss: 988.310
[27,     1] loss: 1024.677
[28,     1] loss: 978.473
[29,     1] loss: 963.902
[30,     1] loss: 1022.814
[31,     1] loss: 958.703
[32,     1] loss: 976.231
[33,     1] loss: 942.684
[34,     1] loss: 944.285
[35,     1] loss: 926.490
[36,     1] loss: 908.551
[37,     1] loss: 946.414
[38,     1] loss: 873.583
[39,     1] loss: 873.610
[40,     1] loss: 875.973
[41,     1] loss: 900.457
[42,     1] loss: 877.548
[43,     1] loss: 900.273
[44,     1] loss: 901.756
[45,     1] loss: 819.402
[46,     1] loss: 864.993
[47,     1] loss: 919.452
[48,     1] loss: 857.988
[49,     1] loss: 903.060
[50,     1] loss: 806.687
[51,     1] loss: 840.974
[52,     1] loss: 812.713
[53,     1] loss: 815.786
[54,     1] loss: 841.743
[55,     1] loss: 818.048
[56,     1] loss: 852.042
[57,     1] loss: 796.710
[58,     1] loss: 786.213
[59,     1] loss: 716.120
[60,     1] loss: 724.048
[61,     1] loss: 781.923
[62,     1] loss: 739.704
[63,     1] loss: 730.350
[64,     1] loss: 728.615
[65,     1] loss: 709.675
[66,     1] loss: 716.366
[67,     1] loss: 705.320
[68,     1] loss: 705.083
[69,     1] loss: 715.365
[70,     1] loss: 678.579
[71,     1] loss: 703.897
[72,     1] loss: 695.850
[73,     1] loss: 597.724
[74,     1] loss: 682.220
[75,     1] loss: 677.515
[76,     1] loss: 619.472
[77,     1] loss: 628.257
[78,     1] loss: 635.776
[79,     1] loss: 600.433
[80,     1] loss: 552.068
[81,     1] loss: 609.251
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0016648334349824048,
 'learning_rate_Hydroxylation-K': 0.0064963478772561345,
 'learning_rate_Hydroxylation-P': 0.002848818421933598,
 'log_base': 1.29490217386271,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 464750801,
 'sample_weights': [1.633250673621916, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.965302288731548,
 'weight_decay_Hydroxylation-K': 9.402233889272253,
 'weight_decay_Hydroxylation-P': 3.900264863402368}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2270.434
[2,     1] loss: 2270.528
[3,     1] loss: 2275.220
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.009801756932284855,
 'learning_rate_Hydroxylation-K': 0.001010438187367616,
 'learning_rate_Hydroxylation-P': 0.0029434136583907046,
 'log_base': 2.6007189070661023,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2240646801,
 'sample_weights': [6.4598145482270874, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 5.704316101786185,
 'weight_decay_Hydroxylation-K': 0.21032607674482762,
 'weight_decay_Hydroxylation-P': 6.2175143975790395}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1282.646
[2,     1] loss: 1289.222
[3,     1] loss: 1280.806
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008332369654891982,
 'learning_rate_Hydroxylation-K': 0.006641549794767799,
 'learning_rate_Hydroxylation-P': 0.0020192121101222827,
 'log_base': 1.0207257627955708,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 4213283743,
 'sample_weights': [1.7466669443985237, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.882732056063187,
 'weight_decay_Hydroxylation-K': 7.8185684240356235,
 'weight_decay_Hydroxylation-P': 4.156137610398524}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26389.016
Exploding loss, terminate run (best metric=0.5341472625732422)
Finished Training
Total time taken: 0.20302176475524902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26396.484
Exploding loss, terminate run (best metric=0.5275917053222656)
Finished Training
Total time taken: 0.20702242851257324
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26462.711
Exploding loss, terminate run (best metric=0.5267378687858582)
Finished Training
Total time taken: 0.2060256004333496
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26433.893
Exploding loss, terminate run (best metric=0.5305227637290955)
Finished Training
Total time taken: 0.21101832389831543
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 26402.090
Exploding loss, terminate run (best metric=0.5321762561798096)
Finished Training
Total time taken: 0.20602130889892578
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26544.641
Exploding loss, terminate run (best metric=0.5351802706718445)
Finished Training
Total time taken: 0.21502447128295898
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26562.387
Exploding loss, terminate run (best metric=0.5286862254142761)
Finished Training
Total time taken: 0.2080214023590088
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26490.182
Exploding loss, terminate run (best metric=0.5268629789352417)
Finished Training
Total time taken: 0.20502161979675293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26515.391
Exploding loss, terminate run (best metric=0.5272300243377686)
Finished Training
Total time taken: 0.21102666854858398
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 26508.734
Exploding loss, terminate run (best metric=0.5381318926811218)
Finished Training
Total time taken: 0.22101902961730957
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26560.609
Exploding loss, terminate run (best metric=0.5326105356216431)
Finished Training
Total time taken: 0.21602320671081543
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26536.164
Exploding loss, terminate run (best metric=0.5768329501152039)
Finished Training
Total time taken: 0.21702241897583008
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26461.182
Exploding loss, terminate run (best metric=0.546451210975647)
Finished Training
Total time taken: 0.20702004432678223
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 26480.629
Exploding loss, terminate run (best metric=0.5324364304542542)
Finished Training
Total time taken: 0.21301960945129395
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 26454.008
Exploding loss, terminate run (best metric=0.5271740555763245)
Finished Training
Total time taken: 0.21102285385131836
{'Hydroxylation-K Validation Accuracy': 0.5189125295508275, 'Hydroxylation-K Validation Sensitivity': 0.4666666666666667, 'Hydroxylation-K Validation Specificity': 0.5298245614035088, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.580233918128655, 'Hydroxylation-K AUC PR': 0.30642972247220723, 'Hydroxylation-K MCC': -0.003225591172108271, 'Hydroxylation-K F1': 0.16535812147000362, 'Validation Loss (Hydroxylation-K)': 0.5594803134600321, 'Hydroxylation-P Validation Accuracy': 0.5043126406442989, 'Hydroxylation-P Validation Sensitivity': 0.4795238095238095, 'Hydroxylation-P Validation Specificity': 0.5097560975609756, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5734192112261467, 'Hydroxylation-P AUC PR': 0.2750204878788305, 'Hydroxylation-P MCC': -0.0060976998734525, 'Hydroxylation-P F1': 0.15077071377356066, 'Validation Loss (Hydroxylation-P)': 0.5348514954249064, 'Validation Loss (total)': 1.0943317890167237, 'TimeToTrain': 0.21048871676127115}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0055018619750259,
 'learning_rate_Hydroxylation-K': 0.009650155293957813,
 'learning_rate_Hydroxylation-P': 0.007417009857173638,
 'log_base': 2.860886301622456,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2477658026,
 'sample_weights': [81.4414135795732, 10.159024088906056],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.031107124622713256,
 'weight_decay_Hydroxylation-K': 7.185571929140003,
 'weight_decay_Hydroxylation-P': 2.4768669432843806}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.654
[2,     1] loss: 1242.470
[3,     1] loss: 1240.267
[4,     1] loss: 1243.197
[5,     1] loss: 1236.141
[6,     1] loss: 1233.609
[7,     1] loss: 1220.032
[8,     1] loss: 1204.984
[9,     1] loss: 1176.600
[10,     1] loss: 1134.853
[11,     1] loss: 1091.299
[12,     1] loss: 1057.991
[13,     1] loss: 1121.171
[14,     1] loss: 1021.552
[15,     1] loss: 1029.416
[16,     1] loss: 990.031
[17,     1] loss: 1018.982
[18,     1] loss: 1006.985
[19,     1] loss: 962.280
[20,     1] loss: 1021.051
[21,     1] loss: 992.137
[22,     1] loss: 1038.629
[23,     1] loss: 966.239
[24,     1] loss: 946.231
[25,     1] loss: 944.694
[26,     1] loss: 965.181
[27,     1] loss: 1061.209
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003638921711805192,
 'learning_rate_Hydroxylation-K': 0.0006844122214323398,
 'learning_rate_Hydroxylation-P': 0.004300303191730833,
 'log_base': 2.8539499023017236,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2656085530,
 'sample_weights': [1.58823438466657, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 6.1902261184491785,
 'weight_decay_Hydroxylation-K': 9.720278637738064,
 'weight_decay_Hydroxylation-P': 0.11623022296825469}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1245.434
[2,     1] loss: 1243.839
[3,     1] loss: 1244.117
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005686593208359105,
 'learning_rate_Hydroxylation-K': 0.006652387572474678,
 'learning_rate_Hydroxylation-P': 0.006012338887645063,
 'log_base': 2.4408332555359933,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2887114340,
 'sample_weights': [1.5919107798845396, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.032968888894464,
 'weight_decay_Hydroxylation-K': 1.8640027902197862,
 'weight_decay_Hydroxylation-P': 5.9724197671579535}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1306.833
[2,     1] loss: 1305.689
[3,     1] loss: 1304.874
[4,     1] loss: 1301.802
[5,     1] loss: 1307.361
[6,     1] loss: 1299.441
[7,     1] loss: 1299.658
[8,     1] loss: 1301.936
[9,     1] loss: 1298.805
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005816889803650596,
 'learning_rate_Hydroxylation-K': 0.007267471430268455,
 'learning_rate_Hydroxylation-P': 0.009970412498065295,
 'log_base': 2.5916793953640576,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1450918276,
 'sample_weights': [1.8708610191278612, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.38232812792448645,
 'weight_decay_Hydroxylation-K': 6.3422354936469345,
 'weight_decay_Hydroxylation-P': 4.11301706965515}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1276.782
[2,     1] loss: 1276.677
[3,     1] loss: 1282.240
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007724613544146457,
 'learning_rate_Hydroxylation-K': 0.005283629711401085,
 'learning_rate_Hydroxylation-P': 0.008379841513536282,
 'log_base': 2.9888147650427417,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 1078024702,
 'sample_weights': [1.7530531213839975, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.46808264585026,
 'weight_decay_Hydroxylation-K': 3.9851756781522045,
 'weight_decay_Hydroxylation-P': 2.1098869003209684}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1230.403
[2,     1] loss: 1232.018
[3,     1] loss: 1231.523
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.007042938228300957,
 'learning_rate_Hydroxylation-K': 0.006963467828154589,
 'learning_rate_Hydroxylation-P': 0.0009463777724283085,
 'log_base': 1.1780109897692375,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 473281494,
 'sample_weights': [1.5247770168959347, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.510943428295702,
 'weight_decay_Hydroxylation-K': 8.081925720329629,
 'weight_decay_Hydroxylation-P': 6.085812426490798}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 3303.391
[2,     1] loss: 3319.476
[3,     1] loss: 3305.747
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.005206391045292918,
 'learning_rate_Hydroxylation-K': 0.0028839333165940045,
 'learning_rate_Hydroxylation-P': 0.0034347929676292405,
 'log_base': 2.8878998287325377,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 874781686,
 'sample_weights': [10.190255116502184, 1.2738309607561946],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.621546398472173,
 'weight_decay_Hydroxylation-K': 2.6601440961492164,
 'weight_decay_Hydroxylation-P': 7.558452985546701}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1240.853
[2,     1] loss: 1239.951
[3,     1] loss: 1242.325
[4,     1] loss: 1239.235
[5,     1] loss: 1238.061
[6,     1] loss: 1233.594
[7,     1] loss: 1231.081
[8,     1] loss: 1230.933
[9,     1] loss: 1215.709
[10,     1] loss: 1202.191
[11,     1] loss: 1163.959
[12,     1] loss: 1126.961
[13,     1] loss: 1082.623
[14,     1] loss: 1052.703
[15,     1] loss: 1072.806
[16,     1] loss: 1031.669
[17,     1] loss: 1038.876
[18,     1] loss: 1002.802
[19,     1] loss: 1001.415
[20,     1] loss: 991.751
[21,     1] loss: 978.071
[22,     1] loss: 996.010
[23,     1] loss: 974.439
[24,     1] loss: 985.787
[25,     1] loss: 991.528
[26,     1] loss: 931.900
[27,     1] loss: 951.008
[28,     1] loss: 923.882
[29,     1] loss: 925.865
[30,     1] loss: 932.499
[31,     1] loss: 893.242
[32,     1] loss: 899.106
[33,     1] loss: 879.220
[34,     1] loss: 869.454
[35,     1] loss: 865.330
[36,     1] loss: 831.179
[37,     1] loss: 831.427
[38,     1] loss: 817.415
[39,     1] loss: 868.057
[40,     1] loss: 1029.312
[41,     1] loss: 845.484
[42,     1] loss: 870.666
[43,     1] loss: 861.616
[44,     1] loss: 902.821
[45,     1] loss: 828.581
[46,     1] loss: 836.037
[47,     1] loss: 902.326
[48,     1] loss: 832.748
[49,     1] loss: 875.345
[50,     1] loss: 790.118
[51,     1] loss: 816.540
[52,     1] loss: 756.354
[53,     1] loss: 741.532
[54,     1] loss: 722.186
[55,     1] loss: 841.442
[56,     1] loss: 731.740
[57,     1] loss: 767.530
[58,     1] loss: 755.718
[59,     1] loss: 691.965
[60,     1] loss: 750.424
[61,     1] loss: 685.646
[62,     1] loss: 698.870
[63,     1] loss: 666.716
[64,     1] loss: 669.091
[65,     1] loss: 747.238
[66,     1] loss: 674.785
[67,     1] loss: 584.751
[68,     1] loss: 586.689
[69,     1] loss: 642.121
[70,     1] loss: 524.475
[71,     1] loss: 591.553
[72,     1] loss: 669.901
[73,     1] loss: 947.830
[74,     1] loss: 916.069
[75,     1] loss: 666.814
[76,     1] loss: 745.479
[77,     1] loss: 793.153
[78,     1] loss: 708.422
[79,     1] loss: 744.145
[80,     1] loss: 788.538
[81,     1] loss: 690.091
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0007303671796877282,
 'learning_rate_Hydroxylation-K': 0.0018687690854483439,
 'learning_rate_Hydroxylation-P': 0.0022819650943395817,
 'log_base': 2.401765827113277,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3055226895,
 'sample_weights': [1.5741599759280132, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.359698234974378,
 'weight_decay_Hydroxylation-K': 2.195251433616006,
 'weight_decay_Hydroxylation-P': 9.064271215995781}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1309.578
[2,     1] loss: 1313.449
[3,     1] loss: 1309.748
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0020972393133985223,
 'learning_rate_Hydroxylation-K': 0.008992692439407467,
 'learning_rate_Hydroxylation-P': 0.005348735294336179,
 'log_base': 1.038192243833952,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3516085456,
 'sample_weights': [1.9053128184760675, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.775345860510129,
 'weight_decay_Hydroxylation-K': 9.71708695621104,
 'weight_decay_Hydroxylation-P': 1.3013903486750438}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 14450.168
[2,     1] loss: 14507.879
[3,     1] loss: 14426.258
[4,     1] loss: 14441.184
[5,     1] loss: 14421.211
[6,     1] loss: 14428.242
[7,     1] loss: 14360.470
[8,     1] loss: 14416.881
[9,     1] loss: 14307.818
[10,     1] loss: 14276.799
[11,     1] loss: 14241.131
[12,     1] loss: 14068.751
[13,     1] loss: 13812.865
[14,     1] loss: 13664.852
[15,     1] loss: 13391.379
[16,     1] loss: 13318.461
[17,     1] loss: 13017.436
[18,     1] loss: 12797.978
[19,     1] loss: 12278.410
[20,     1] loss: 12106.809
[21,     1] loss: 12155.427
[22,     1] loss: 11698.979
[23,     1] loss: 11352.089
[24,     1] loss: 11341.736
[25,     1] loss: 10946.418
[26,     1] loss: 11106.931
[27,     1] loss: 11179.760
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.003178759401145934,
 'learning_rate_Hydroxylation-K': 0.0049990346631482874,
 'learning_rate_Hydroxylation-P': 0.002223402751995739,
 'log_base': 1.0317191690027037,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2773334091,
 'sample_weights': [44.541082769068595, 5.567849833804666],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 4.474561992141326,
 'weight_decay_Hydroxylation-K': 9.467945204027378,
 'weight_decay_Hydroxylation-P': 3.9782092186120126}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17482.762
Exploding loss, terminate run (best metric=0.5360593795776367)
Finished Training
Total time taken: 0.19201946258544922
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17363.004
Exploding loss, terminate run (best metric=0.5321457982063293)
Finished Training
Total time taken: 0.22902536392211914
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17368.693
Exploding loss, terminate run (best metric=0.5283152461051941)
Finished Training
Total time taken: 0.2100222110748291
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17333.711
Exploding loss, terminate run (best metric=0.5302612781524658)
Finished Training
Total time taken: 0.20002007484436035
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17411.676
Exploding loss, terminate run (best metric=0.5333399772644043)
Finished Training
Total time taken: 0.2080223560333252
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17392.344
Exploding loss, terminate run (best metric=0.5341389179229736)
Finished Training
Total time taken: 0.19201922416687012
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17364.340
Exploding loss, terminate run (best metric=0.5293567776679993)
Finished Training
Total time taken: 0.20801949501037598
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17377.086
Exploding loss, terminate run (best metric=0.5267202854156494)
Finished Training
Total time taken: 0.1950209140777588
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17346.492
Exploding loss, terminate run (best metric=0.5311300754547119)
Finished Training
Total time taken: 0.21202325820922852
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17331.281
Exploding loss, terminate run (best metric=0.5308067798614502)
Finished Training
Total time taken: 0.20501971244812012
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17367.299
Exploding loss, terminate run (best metric=0.5349239110946655)
Finished Training
Total time taken: 0.20802044868469238
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17386.094
Exploding loss, terminate run (best metric=0.5271081924438477)
Finished Training
Total time taken: 0.2060253620147705
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17401.445
Exploding loss, terminate run (best metric=0.5339744091033936)
Finished Training
Total time taken: 0.21102237701416016
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 17574.395
Exploding loss, terminate run (best metric=0.5289994478225708)
Finished Training
Total time taken: 0.21802210807800293
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 17279.240
Exploding loss, terminate run (best metric=0.5276160836219788)
Finished Training
Total time taken: 0.21402287483215332
{'Hydroxylation-K Validation Accuracy': 0.5966903073286052, 'Hydroxylation-K Validation Sensitivity': 0.35333333333333333, 'Hydroxylation-K Validation Specificity': 0.6578947368421053, 'Hydroxylation-K Validation Precision': nan, 'Hydroxylation-K AUC ROC': 0.6017933723196881, 'Hydroxylation-K AUC PR': 0.30735871806708726, 'Hydroxylation-K MCC': 0.01223550580642996, 'Hydroxylation-K F1': 0.13404488232074438, 'Validation Loss (Hydroxylation-K)': 0.5562256614367167, 'Hydroxylation-P Validation Accuracy': 0.6044767947481515, 'Hydroxylation-P Validation Sensitivity': 0.3504761904761905, 'Hydroxylation-P Validation Specificity': 0.6597560975609756, 'Hydroxylation-P Validation Precision': nan, 'Hydroxylation-P AUC ROC': 0.5816987408626643, 'Hydroxylation-P AUC PR': 0.27496760118985447, 'Hydroxylation-P MCC': 0.011558965769084056, 'Hydroxylation-P F1': 0.11947282322291118, 'Validation Loss (Hydroxylation-P)': 0.5309931039810181, 'Validation Loss (total)': 1.0872187614440918, 'TimeToTrain': 0.20722168286641438}
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 4,
 'current_CV_Repeat': 3,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006164349767402395,
 'learning_rate_Hydroxylation-K': 0.002854055484736687,
 'learning_rate_Hydroxylation-P': 0.004568111588720888,
 'log_base': 1.7836899704848372,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 168118974,
 'sample_weights': [53.502030803471, 6.673857879527049],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.875923936531152,
 'weight_decay_Hydroxylation-K': 1.8011757339418693,
 'weight_decay_Hydroxylation-P': 2.0429493103004486}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1518.874
[2,     1] loss: 1525.284
[3,     1] loss: 1516.205
[4,     1] loss: 1514.302
[5,     1] loss: 1515.965
[6,     1] loss: 1512.997
[7,     1] loss: 1506.988
[8,     1] loss: 1506.303
[9,     1] loss: 1498.280
[10,     1] loss: 1484.662
[11,     1] loss: 1456.711
[12,     1] loss: 1427.402
[13,     1] loss: 1407.975
[14,     1] loss: 1354.059
[15,     1] loss: 1320.681
[16,     1] loss: 1332.252
[17,     1] loss: 1245.269
[18,     1] loss: 1323.012
[19,     1] loss: 1296.283
[20,     1] loss: 1244.803
[21,     1] loss: 1230.204
[22,     1] loss: 1200.911
[23,     1] loss: 1237.573
[24,     1] loss: 1224.952
[25,     1] loss: 1179.388
[26,     1] loss: 1189.492
[27,     1] loss: 1130.349
[28,     1] loss: 1094.475
[29,     1] loss: 1147.338
[30,     1] loss: 1170.037
[31,     1] loss: 1146.162
[32,     1] loss: 1112.023
[33,     1] loss: 1164.347
[34,     1] loss: 1100.112
[35,     1] loss: 1131.452
[36,     1] loss: 1037.231
[37,     1] loss: 1192.692
[38,     1] loss: 1105.665
[39,     1] loss: 1036.401
[40,     1] loss: 1093.690
[41,     1] loss: 1036.996
[42,     1] loss: 994.956
[43,     1] loss: 1064.065
[44,     1] loss: 965.707
[45,     1] loss: 1176.284
[46,     1] loss: 1398.959
[47,     1] loss: 1006.093
[48,     1] loss: 1140.722
[49,     1] loss: 1131.738
[50,     1] loss: 1114.021
[51,     1] loss: 1173.103
[52,     1] loss: 1122.084
[53,     1] loss: 1028.736
[54,     1] loss: 1073.337
[55,     1] loss: 1022.870
[56,     1] loss: 1073.343
[57,     1] loss: 972.751
[58,     1] loss: 1088.486
[59,     1] loss: 1165.478
[60,     1] loss: 1080.754
[61,     1] loss: 1028.090
[62,     1] loss: 1157.087
[63,     1] loss: 1016.907
[64,     1] loss: 1122.575
[65,     1] loss: 938.266
[66,     1] loss: 1208.416
[67,     1] loss: 991.467
[68,     1] loss: 1033.407
[69,     1] loss: 925.311
[70,     1] loss: 1046.461
[71,     1] loss: 1085.379
[72,     1] loss: 873.939
[73,     1] loss: 1009.022
[74,     1] loss: 965.250
[75,     1] loss: 841.004
[76,     1] loss: 921.103
[77,     1] loss: 1235.986
[78,     1] loss: 1442.318
[79,     1] loss: 964.522
[80,     1] loss: 1143.009
[81,     1] loss: 1167.351
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.00625841418307755,
 'learning_rate_Hydroxylation-K': 0.0021723164245860643,
 'learning_rate_Hydroxylation-P': 0.009899740913822782,
 'log_base': 1.3768835642882458,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3605920173,
 'sample_weights': [2.8848948086827155, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 8.85545694143007,
 'weight_decay_Hydroxylation-K': 7.551964887075295,
 'weight_decay_Hydroxylation-P': 5.290091725588943}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 2033.055
[2,     1] loss: 2009.892
[3,     1] loss: 2019.171
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.004376193167777384,
 'learning_rate_Hydroxylation-K': 0.004767950633072691,
 'learning_rate_Hydroxylation-P': 0.004070056911327235,
 'log_base': 1.0501809776393154,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 2977092323,
 'sample_weights': [5.219902662585125, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.3938077137268134,
 'weight_decay_Hydroxylation-K': 8.047501715843605,
 'weight_decay_Hydroxylation-P': 2.588979015550468}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 11078.802
[2,     1] loss: 11113.065
[3,     1] loss: 11042.671
[4,     1] loss: 11078.648
[5,     1] loss: 11037.146
[6,     1] loss: 11030.954
[7,     1] loss: 11022.447
[8,     1] loss: 11096.463
[9,     1] loss: 11048.015
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.006048057857996947,
 'learning_rate_Hydroxylation-K': 0.0036405808813956796,
 'learning_rate_Hydroxylation-P': 0.0062800539518432725,
 'log_base': 2.450280983804256,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 3842581198,
 'sample_weights': [34.096356220639656, 4.262208718649949],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 9.846464630827377,
 'weight_decay_Hydroxylation-K': 8.602172608233001,
 'weight_decay_Hydroxylation-P': 4.1142380078245475}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1303.658
[2,     1] loss: 1302.597
[3,     1] loss: 1299.722
[4,     1] loss: 1306.889
[5,     1] loss: 1299.106
[6,     1] loss: 1297.098
[7,     1] loss: 1301.320
[8,     1] loss: 1299.730
[9,     1] loss: 1295.212
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.008577310554296495,
 'learning_rate_Hydroxylation-K': 0.009761827800151158,
 'learning_rate_Hydroxylation-P': 0.005738410009871963,
 'log_base': 2.8718331819005116,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 665548866,
 'sample_weights': [1.8627963714553277, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 2.011297162997663,
 'weight_decay_Hydroxylation-K': 9.767166119747593,
 'weight_decay_Hydroxylation-P': 1.0890059177985287}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1246.905
[2,     1] loss: 1248.905
[3,     1] loss: 1238.392
{'CNNType': 'Musite',
 'CV_Repeats': 3,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.002916264622737568,
 'learning_rate_Hydroxylation-K': 0.0038635741256458035,
 'learning_rate_Hydroxylation-P': 0.004879165387006923,
 'log_base': 1.9701375895829714,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 656522344,
 'sample_weights': [1.5824847177584622, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 7.686416578012805,
 'weight_decay_Hydroxylation-K': 0.9700692227805954,
 'weight_decay_Hydroxylation-P': 1.6260979472384172}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1429.253
[2,     1] loss: 1430.326
[3,     1] loss: 1426.342
[4,     1] loss: 1425.168
[5,     1] loss: 1422.067
[6,     1] loss: 1419.485
[7,     1] loss: 1417.040
[8,     1] loss: 1408.794
[9,     1] loss: 1400.513
[10,     1] loss: 1381.981
[11,     1] loss: 1360.680
[12,     1] loss: 1335.161
[13,     1] loss: 1324.334
[14,     1] loss: 1287.627
[15,     1] loss: 1261.904
[16,     1] loss: 1218.568
[17,     1] loss: 1228.458
[18,     1] loss: 1179.829
[19,     1] loss: 1169.310
[20,     1] loss: 1153.870
[21,     1] loss: 1144.971
[22,     1] loss: 1184.320
[23,     1] loss: 1183.590
[24,     1] loss: 1133.007
[25,     1] loss: 1084.817
[26,     1] loss: 1114.586
[27,     1] loss: 1120.472
{'CNNType': 'Musite',
 'CV_Repeats': 5,
 'CreateFigures': False,
 'Experiment Name': 'Model architecture - added max, ranges, bceloss',
 'FCType': 'Adapt',
 'FloatsToTune': {'learning_rate': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-K': [1e-05, 0.01],
                  'learning_rate_Hydroxylation-P': [1e-05, 0.01],
                  'log_base': [1.01, 3],
                  'weight_decay': [0, 10],
                  'weight_decay_Hydroxylation-K': [0, 10],
                  'weight_decay_Hydroxylation-P': [0, 10]},
 'IntsToTune': {},
 'LSTM_dropout': 0,
 'LSTM_hidden_size': 64,
 'LSTM_layers': 1,
 'MultiTask': True,
 'MultiTask_sample_method': 'balanced',
 'SeperateTuningLRandWD': True,
 'UseUncertaintyBasedLoss': False,
 'ValidationMetric': 'Validation Loss (Hydroxylation-P)',
 'WeightDecayWeights': [],
 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'],
 'batch_size': 2048,
 'crossValidation': True,
 'currentFold': 0,
 'current_CV_Repeat': 1,
 'data_sample_mode': ['oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample',
                      'oversample'],
 'dontAverageLoss': False,
 'earlyStopping': True,
 'earlyStoppingPatience': 50,
 'embeddingType': 'adaptiveEmbedding',
 'epochs': 200,
 'folds': 5,
 'gpu_mode': True,
 'layerToSplitOn': 'FC',
 'learning_rate': 0.0014784941977442072,
 'learning_rate_Hydroxylation-K': 0.00230545299195537,
 'learning_rate_Hydroxylation-P': 0.007241029954947504,
 'log_base': 2.7004697798865878,
 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>,
 'n_trials': 500,
 'optimizer': <class 'torch.optim.adamw.AdamW'>,
 'random_state': 525940863,
 'sample_weights': [2.4619301273511005, 1],
 'test_data_ratio': 0.2,
 'useLrWeight': True,
 'useWeightDecayWeight': False,
 'weight_decay': 0.24071392247151047,
 'weight_decay_Hydroxylation-K': 9.94639752906256,
 'weight_decay_Hydroxylation-P': 4.4382413728625}
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.569
[2,     1] loss: 1262.752
[3,     1] loss: 1261.848
[4,     1] loss: 1260.602
[5,     1] loss: 1259.797
[6,     1] loss: 1260.809
[7,     1] loss: 1258.461
[8,     1] loss: 1257.266
[9,     1] loss: 1249.308
[10,     1] loss: 1243.922
[11,     1] loss: 1238.450
[12,     1] loss: 1223.909
[13,     1] loss: 1207.180
[14,     1] loss: 1183.849
[15,     1] loss: 1172.943
[16,     1] loss: 1138.246
[17,     1] loss: 1138.569
[18,     1] loss: 1121.535
[19,     1] loss: 1121.308
[20,     1] loss: 1099.703
[21,     1] loss: 1077.381
[22,     1] loss: 1072.087
[23,     1] loss: 1054.335
[24,     1] loss: 1024.000
[25,     1] loss: 1018.907
[26,     1] loss: 1027.188
[27,     1] loss: 1011.435
[28,     1] loss: 1072.898
[29,     1] loss: 1064.069
[30,     1] loss: 1049.807
[31,     1] loss: 1033.932
[32,     1] loss: 1052.114
[33,     1] loss: 1018.309
[34,     1] loss: 1050.390
[35,     1] loss: 987.662
[36,     1] loss: 1022.813
[37,     1] loss: 1003.572
[38,     1] loss: 1017.203
[39,     1] loss: 1005.465
[40,     1] loss: 1000.557
[41,     1] loss: 985.308
[42,     1] loss: 964.534
[43,     1] loss: 956.701
[44,     1] loss: 934.558
[45,     1] loss: 964.038
[46,     1] loss: 995.601
[47,     1] loss: 900.393
[48,     1] loss: 1013.105
[49,     1] loss: 908.595
[50,     1] loss: 949.050
[51,     1] loss: 925.357
[52,     1] loss: 957.061
[53,     1] loss: 918.688
[54,     1] loss: 901.605
[55,     1] loss: 902.832
[56,     1] loss: 900.059
[57,     1] loss: 921.721
[58,     1] loss: 921.438
[59,     1] loss: 902.641
[60,     1] loss: 897.140
[61,     1] loss: 895.896
[62,     1] loss: 836.972
[63,     1] loss: 861.207
[64,     1] loss: 877.703
[65,     1] loss: 880.758
[66,     1] loss: 793.625
[67,     1] loss: 834.031
[68,     1] loss: 844.844
[69,     1] loss: 846.511
[70,     1] loss: 859.745
[71,     1] loss: 792.588
[72,     1] loss: 804.335
[73,     1] loss: 812.490
[74,     1] loss: 786.217
[75,     1] loss: 802.431
[76,     1] loss: 758.408
[77,     1] loss: 786.986
[78,     1] loss: 762.712
[79,     1] loss: 775.287
[80,     1] loss: 730.977
[81,     1] loss: 811.728
[82,     1] loss: 753.557
[83,     1] loss: 706.834
[84,     1] loss: 752.749
[85,     1] loss: 768.488
[86,     1] loss: 768.723
[87,     1] loss: 728.292
[88,     1] loss: 786.263
[89,     1] loss: 677.446
[90,     1] loss: 749.698
[91,     1] loss: 734.621
[92,     1] loss: 706.725
[93,     1] loss: 705.779
[94,     1] loss: 692.391
[95,     1] loss: 638.615
[96,     1] loss: 689.855
[97,     1] loss: 647.954
[98,     1] loss: 681.315
[99,     1] loss: 676.819
[100,     1] loss: 655.755
[101,     1] loss: 608.269
[102,     1] loss: 653.598
[103,     1] loss: 634.770
[104,     1] loss: 641.007
[105,     1] loss: 664.156
[106,     1] loss: 595.323
[107,     1] loss: 625.273
[108,     1] loss: 607.203
[109,     1] loss: 603.932
[110,     1] loss: 611.337
[111,     1] loss: 551.718
[112,     1] loss: 557.987
[113,     1] loss: 583.686
[114,     1] loss: 606.102
[115,     1] loss: 516.437
[116,     1] loss: 530.784
[117,     1] loss: 555.956
[118,     1] loss: 567.489
[119,     1] loss: 565.902
[120,     1] loss: 534.046
[121,     1] loss: 567.235
[122,     1] loss: 527.631
[123,     1] loss: 595.024
[124,     1] loss: 487.003
Early stopping applied (best metric=0.2859150767326355)
Finished Training
Total time taken: 18.784009218215942
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.314
[2,     1] loss: 1261.764
[3,     1] loss: 1262.490
[4,     1] loss: 1259.745
[5,     1] loss: 1260.254
[6,     1] loss: 1260.291
[7,     1] loss: 1257.421
[8,     1] loss: 1255.913
[9,     1] loss: 1252.727
[10,     1] loss: 1256.086
[11,     1] loss: 1249.416
[12,     1] loss: 1241.570
[13,     1] loss: 1227.418
[14,     1] loss: 1210.209
[15,     1] loss: 1190.891
[16,     1] loss: 1167.943
[17,     1] loss: 1163.653
[18,     1] loss: 1130.546
[19,     1] loss: 1098.379
[20,     1] loss: 1080.553
[21,     1] loss: 1040.721
[22,     1] loss: 1055.698
[23,     1] loss: 1047.561
[24,     1] loss: 1026.589
[25,     1] loss: 1052.875
[26,     1] loss: 982.603
[27,     1] loss: 1027.147
[28,     1] loss: 1033.711
[29,     1] loss: 1007.593
[30,     1] loss: 998.359
[31,     1] loss: 1014.936
[32,     1] loss: 1057.979
[33,     1] loss: 1005.788
[34,     1] loss: 984.561
[35,     1] loss: 993.949
[36,     1] loss: 974.349
[37,     1] loss: 970.542
[38,     1] loss: 989.430
[39,     1] loss: 931.474
[40,     1] loss: 952.534
[41,     1] loss: 976.477
[42,     1] loss: 927.379
[43,     1] loss: 961.530
[44,     1] loss: 951.566
[45,     1] loss: 917.206
[46,     1] loss: 925.424
[47,     1] loss: 931.181
[48,     1] loss: 932.344
[49,     1] loss: 940.043
[50,     1] loss: 894.414
[51,     1] loss: 913.600
[52,     1] loss: 846.670
[53,     1] loss: 868.396
[54,     1] loss: 885.448
[55,     1] loss: 887.777
[56,     1] loss: 864.699
[57,     1] loss: 869.750
[58,     1] loss: 838.849
[59,     1] loss: 819.513
[60,     1] loss: 802.807
[61,     1] loss: 807.025
[62,     1] loss: 804.898
[63,     1] loss: 856.011
[64,     1] loss: 814.580
[65,     1] loss: 746.265
[66,     1] loss: 812.773
[67,     1] loss: 833.754
[68,     1] loss: 839.299
[69,     1] loss: 793.666
[70,     1] loss: 789.616
[71,     1] loss: 718.014
[72,     1] loss: 774.814
[73,     1] loss: 788.213
[74,     1] loss: 751.123
[75,     1] loss: 774.047
[76,     1] loss: 713.400
[77,     1] loss: 718.255
[78,     1] loss: 695.540
[79,     1] loss: 739.795
[80,     1] loss: 767.948
[81,     1] loss: 679.098
[82,     1] loss: 701.472
[83,     1] loss: 735.634
[84,     1] loss: 643.023
[85,     1] loss: 721.194
[86,     1] loss: 748.652
[87,     1] loss: 684.931
[88,     1] loss: 709.896
[89,     1] loss: 632.368
[90,     1] loss: 658.504
[91,     1] loss: 652.525
[92,     1] loss: 619.626
[93,     1] loss: 640.381
[94,     1] loss: 655.887
[95,     1] loss: 665.008
[96,     1] loss: 590.288
[97,     1] loss: 622.126
[98,     1] loss: 610.281
[99,     1] loss: 657.352
[100,     1] loss: 578.564
[101,     1] loss: 596.273
Early stopping applied (best metric=0.4226202666759491)
Finished Training
Total time taken: 15.476652145385742
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.365
[2,     1] loss: 1264.552
[3,     1] loss: 1261.929
[4,     1] loss: 1259.676
[5,     1] loss: 1259.279
[6,     1] loss: 1260.275
[7,     1] loss: 1263.400
[8,     1] loss: 1259.518
[9,     1] loss: 1261.003
[10,     1] loss: 1255.980
[11,     1] loss: 1254.325
[12,     1] loss: 1251.329
[13,     1] loss: 1244.703
[14,     1] loss: 1238.188
[15,     1] loss: 1228.022
[16,     1] loss: 1207.350
[17,     1] loss: 1203.819
[18,     1] loss: 1185.104
[19,     1] loss: 1153.038
[20,     1] loss: 1142.740
[21,     1] loss: 1123.870
[22,     1] loss: 1097.210
[23,     1] loss: 1076.939
[24,     1] loss: 1073.595
[25,     1] loss: 1057.766
[26,     1] loss: 1067.987
[27,     1] loss: 1036.304
[28,     1] loss: 1057.833
[29,     1] loss: 1057.347
[30,     1] loss: 1076.360
[31,     1] loss: 1005.882
[32,     1] loss: 1019.407
[33,     1] loss: 1042.567
[34,     1] loss: 1034.407
[35,     1] loss: 1014.487
[36,     1] loss: 1007.378
[37,     1] loss: 976.705
[38,     1] loss: 986.124
[39,     1] loss: 985.832
[40,     1] loss: 983.796
[41,     1] loss: 977.797
[42,     1] loss: 954.860
[43,     1] loss: 926.508
[44,     1] loss: 942.470
[45,     1] loss: 955.755
[46,     1] loss: 943.318
[47,     1] loss: 912.437
[48,     1] loss: 974.377
[49,     1] loss: 917.181
[50,     1] loss: 915.784
[51,     1] loss: 950.915
[52,     1] loss: 917.268
[53,     1] loss: 895.105
[54,     1] loss: 905.877
[55,     1] loss: 911.018
[56,     1] loss: 897.379
[57,     1] loss: 900.951
[58,     1] loss: 902.362
[59,     1] loss: 872.382
[60,     1] loss: 839.439
[61,     1] loss: 810.692
[62,     1] loss: 888.698
[63,     1] loss: 843.385
[64,     1] loss: 824.262
[65,     1] loss: 813.059
[66,     1] loss: 801.799
[67,     1] loss: 845.260
[68,     1] loss: 793.163
[69,     1] loss: 784.372
[70,     1] loss: 840.988
[71,     1] loss: 782.529
[72,     1] loss: 804.547
[73,     1] loss: 778.103
[74,     1] loss: 800.737
[75,     1] loss: 818.752
[76,     1] loss: 743.013
[77,     1] loss: 814.289
[78,     1] loss: 776.158
[79,     1] loss: 783.388
[80,     1] loss: 773.055
[81,     1] loss: 762.292
[82,     1] loss: 748.236
[83,     1] loss: 756.437
[84,     1] loss: 739.774
[85,     1] loss: 739.531
[86,     1] loss: 706.476
[87,     1] loss: 679.920
[88,     1] loss: 723.014
[89,     1] loss: 711.743
[90,     1] loss: 678.744
[91,     1] loss: 695.747
[92,     1] loss: 719.546
[93,     1] loss: 714.140
[94,     1] loss: 681.462
[95,     1] loss: 681.410
[96,     1] loss: 720.070
[97,     1] loss: 616.592
[98,     1] loss: 693.135
[99,     1] loss: 623.101
[100,     1] loss: 590.069
[101,     1] loss: 681.347
Early stopping applied (best metric=0.35655733942985535)
Finished Training
Total time taken: 14.891588687896729
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1270.257
[2,     1] loss: 1265.604
[3,     1] loss: 1266.022
[4,     1] loss: 1261.170
[5,     1] loss: 1261.936
[6,     1] loss: 1261.370
[7,     1] loss: 1259.533
[8,     1] loss: 1258.175
[9,     1] loss: 1258.831
[10,     1] loss: 1257.428
[11,     1] loss: 1253.916
[12,     1] loss: 1251.401
[13,     1] loss: 1240.822
[14,     1] loss: 1236.875
[15,     1] loss: 1218.366
[16,     1] loss: 1209.561
[17,     1] loss: 1189.282
[18,     1] loss: 1170.442
[19,     1] loss: 1159.409
[20,     1] loss: 1142.407
[21,     1] loss: 1103.020
[22,     1] loss: 1093.551
[23,     1] loss: 1075.176
[24,     1] loss: 1069.742
[25,     1] loss: 1048.042
[26,     1] loss: 1080.933
[27,     1] loss: 1044.668
[28,     1] loss: 1067.699
[29,     1] loss: 1049.618
[30,     1] loss: 1018.015
[31,     1] loss: 1041.849
[32,     1] loss: 1057.323
[33,     1] loss: 1025.124
[34,     1] loss: 1021.290
[35,     1] loss: 983.286
[36,     1] loss: 1011.058
[37,     1] loss: 992.747
[38,     1] loss: 956.070
[39,     1] loss: 962.213
[40,     1] loss: 995.399
[41,     1] loss: 914.520
[42,     1] loss: 921.653
[43,     1] loss: 944.599
[44,     1] loss: 945.607
[45,     1] loss: 976.310
[46,     1] loss: 897.288
[47,     1] loss: 887.179
[48,     1] loss: 968.672
[49,     1] loss: 916.588
[50,     1] loss: 903.457
[51,     1] loss: 890.787
[52,     1] loss: 872.121
[53,     1] loss: 906.551
[54,     1] loss: 862.832
[55,     1] loss: 858.434
[56,     1] loss: 906.133
[57,     1] loss: 899.402
[58,     1] loss: 833.377
[59,     1] loss: 854.290
[60,     1] loss: 793.696
[61,     1] loss: 861.378
[62,     1] loss: 803.232
[63,     1] loss: 815.666
[64,     1] loss: 860.808
[65,     1] loss: 848.146
[66,     1] loss: 800.141
[67,     1] loss: 853.012
[68,     1] loss: 807.955
[69,     1] loss: 754.297
[70,     1] loss: 778.663
[71,     1] loss: 835.600
[72,     1] loss: 771.657
[73,     1] loss: 752.122
[74,     1] loss: 766.032
[75,     1] loss: 739.748
[76,     1] loss: 759.459
[77,     1] loss: 772.754
Early stopping applied (best metric=0.37888795137405396)
Finished Training
Total time taken: 11.109187602996826
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1279.942
[2,     1] loss: 1267.664
[3,     1] loss: 1263.405
[4,     1] loss: 1265.420
[5,     1] loss: 1263.615
[6,     1] loss: 1258.610
[7,     1] loss: 1264.158
[8,     1] loss: 1258.634
[9,     1] loss: 1257.303
[10,     1] loss: 1254.601
[11,     1] loss: 1250.150
[12,     1] loss: 1246.761
[13,     1] loss: 1240.817
[14,     1] loss: 1232.782
[15,     1] loss: 1217.359
[16,     1] loss: 1206.823
[17,     1] loss: 1181.826
[18,     1] loss: 1163.304
[19,     1] loss: 1146.083
[20,     1] loss: 1134.544
[21,     1] loss: 1099.282
[22,     1] loss: 1100.518
[23,     1] loss: 1103.246
[24,     1] loss: 1096.646
[25,     1] loss: 1059.116
[26,     1] loss: 1103.783
[27,     1] loss: 1048.707
[28,     1] loss: 1035.931
[29,     1] loss: 1035.874
[30,     1] loss: 1015.229
[31,     1] loss: 1015.656
[32,     1] loss: 1049.369
[33,     1] loss: 1002.093
[34,     1] loss: 965.276
[35,     1] loss: 1057.751
[36,     1] loss: 989.404
[37,     1] loss: 979.089
[38,     1] loss: 978.138
[39,     1] loss: 969.198
[40,     1] loss: 946.440
[41,     1] loss: 956.177
[42,     1] loss: 962.507
[43,     1] loss: 937.091
[44,     1] loss: 947.598
[45,     1] loss: 952.194
[46,     1] loss: 947.455
[47,     1] loss: 901.662
[48,     1] loss: 930.703
[49,     1] loss: 930.170
[50,     1] loss: 960.156
[51,     1] loss: 891.598
[52,     1] loss: 890.757
[53,     1] loss: 930.028
[54,     1] loss: 906.894
[55,     1] loss: 911.909
[56,     1] loss: 914.569
[57,     1] loss: 894.906
[58,     1] loss: 880.551
[59,     1] loss: 907.412
[60,     1] loss: 855.939
[61,     1] loss: 857.463
[62,     1] loss: 815.857
[63,     1] loss: 826.901
[64,     1] loss: 834.914
[65,     1] loss: 862.915
[66,     1] loss: 814.900
[67,     1] loss: 786.748
[68,     1] loss: 866.113
[69,     1] loss: 814.459
[70,     1] loss: 823.647
[71,     1] loss: 768.019
[72,     1] loss: 822.819
[73,     1] loss: 796.196
[74,     1] loss: 792.498
[75,     1] loss: 761.974
[76,     1] loss: 785.233
[77,     1] loss: 753.963
[78,     1] loss: 738.719
[79,     1] loss: 734.208
[80,     1] loss: 756.938
[81,     1] loss: 709.621
[82,     1] loss: 729.854
[83,     1] loss: 736.159
[84,     1] loss: 725.672
[85,     1] loss: 713.496
[86,     1] loss: 764.541
[87,     1] loss: 695.573
[88,     1] loss: 686.631
[89,     1] loss: 664.557
[90,     1] loss: 690.906
[91,     1] loss: 696.053
[92,     1] loss: 657.033
[93,     1] loss: 694.946
[94,     1] loss: 688.435
[95,     1] loss: 671.822
[96,     1] loss: 654.973
[97,     1] loss: 644.074
[98,     1] loss: 657.560
[99,     1] loss: 643.657
[100,     1] loss: 611.679
[101,     1] loss: 647.735
[102,     1] loss: 629.866
[103,     1] loss: 610.490
[104,     1] loss: 617.478
[105,     1] loss: 653.501
[106,     1] loss: 556.062
[107,     1] loss: 650.067
[108,     1] loss: 593.886
[109,     1] loss: 641.344
[110,     1] loss: 608.853
[111,     1] loss: 624.352
[112,     1] loss: 635.743
[113,     1] loss: 525.035
[114,     1] loss: 617.824
[115,     1] loss: 554.642
[116,     1] loss: 547.912
[117,     1] loss: 516.397
[118,     1] loss: 552.495
[119,     1] loss: 595.695
Early stopping applied (best metric=0.35659894347190857)
Finished Training
Total time taken: 18.03994083404541
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.643
[2,     1] loss: 1262.124
[3,     1] loss: 1260.742
[4,     1] loss: 1261.379
[5,     1] loss: 1262.674
[6,     1] loss: 1262.531
[7,     1] loss: 1260.743
[8,     1] loss: 1258.962
[9,     1] loss: 1261.458
[10,     1] loss: 1255.486
[11,     1] loss: 1252.498
[12,     1] loss: 1249.034
[13,     1] loss: 1236.217
[14,     1] loss: 1228.197
[15,     1] loss: 1213.320
[16,     1] loss: 1200.295
[17,     1] loss: 1182.977
[18,     1] loss: 1160.099
[19,     1] loss: 1132.441
[20,     1] loss: 1106.306
[21,     1] loss: 1109.763
[22,     1] loss: 1051.979
[23,     1] loss: 1103.251
[24,     1] loss: 1092.306
[25,     1] loss: 1038.583
[26,     1] loss: 1040.125
[27,     1] loss: 1025.843
[28,     1] loss: 978.509
[29,     1] loss: 1027.130
[30,     1] loss: 988.487
[31,     1] loss: 997.743
[32,     1] loss: 1017.671
[33,     1] loss: 984.787
[34,     1] loss: 967.916
[35,     1] loss: 959.945
[36,     1] loss: 971.772
[37,     1] loss: 974.769
[38,     1] loss: 901.399
[39,     1] loss: 968.117
[40,     1] loss: 923.693
[41,     1] loss: 890.480
[42,     1] loss: 950.037
[43,     1] loss: 902.102
[44,     1] loss: 903.630
[45,     1] loss: 959.738
[46,     1] loss: 900.464
[47,     1] loss: 917.587
[48,     1] loss: 902.381
[49,     1] loss: 915.919
[50,     1] loss: 928.463
[51,     1] loss: 912.273
[52,     1] loss: 892.068
[53,     1] loss: 873.575
[54,     1] loss: 910.044
[55,     1] loss: 884.675
[56,     1] loss: 856.308
[57,     1] loss: 877.415
[58,     1] loss: 833.604
[59,     1] loss: 883.162
[60,     1] loss: 826.277
[61,     1] loss: 835.275
[62,     1] loss: 799.318
[63,     1] loss: 789.422
[64,     1] loss: 854.831
[65,     1] loss: 811.888
[66,     1] loss: 778.457
[67,     1] loss: 786.155
[68,     1] loss: 775.423
[69,     1] loss: 783.559
[70,     1] loss: 811.241
[71,     1] loss: 801.120
[72,     1] loss: 748.883
[73,     1] loss: 791.293
[74,     1] loss: 817.017
[75,     1] loss: 765.408
[76,     1] loss: 807.314
[77,     1] loss: 767.392
[78,     1] loss: 765.390
[79,     1] loss: 765.617
[80,     1] loss: 736.444
[81,     1] loss: 737.813
[82,     1] loss: 734.705
[83,     1] loss: 706.943
[84,     1] loss: 705.052
[85,     1] loss: 708.386
[86,     1] loss: 695.402
[87,     1] loss: 673.312
[88,     1] loss: 627.185
Early stopping applied (best metric=0.4112829566001892)
Finished Training
Total time taken: 13.552448272705078
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1269.092
[2,     1] loss: 1262.157
[3,     1] loss: 1263.825
[4,     1] loss: 1262.327
[5,     1] loss: 1264.015
[6,     1] loss: 1261.302
[7,     1] loss: 1259.814
[8,     1] loss: 1260.375
[9,     1] loss: 1256.176
[10,     1] loss: 1255.663
[11,     1] loss: 1250.570
[12,     1] loss: 1245.462
[13,     1] loss: 1237.369
[14,     1] loss: 1223.783
[15,     1] loss: 1208.234
[16,     1] loss: 1190.385
[17,     1] loss: 1167.260
[18,     1] loss: 1153.067
[19,     1] loss: 1129.622
[20,     1] loss: 1123.775
[21,     1] loss: 1083.410
[22,     1] loss: 1083.453
[23,     1] loss: 1055.691
[24,     1] loss: 1061.124
[25,     1] loss: 1066.370
[26,     1] loss: 1020.483
[27,     1] loss: 1042.070
[28,     1] loss: 1026.292
[29,     1] loss: 988.558
[30,     1] loss: 1013.343
[31,     1] loss: 1017.097
[32,     1] loss: 1017.787
[33,     1] loss: 1018.826
[34,     1] loss: 976.952
[35,     1] loss: 986.707
[36,     1] loss: 970.016
[37,     1] loss: 964.998
[38,     1] loss: 1002.562
[39,     1] loss: 972.336
[40,     1] loss: 964.987
[41,     1] loss: 1005.033
[42,     1] loss: 965.093
[43,     1] loss: 968.991
[44,     1] loss: 936.919
[45,     1] loss: 928.502
[46,     1] loss: 941.413
[47,     1] loss: 924.831
[48,     1] loss: 884.310
[49,     1] loss: 942.475
[50,     1] loss: 899.756
[51,     1] loss: 942.522
[52,     1] loss: 901.457
[53,     1] loss: 864.481
[54,     1] loss: 907.636
[55,     1] loss: 906.044
[56,     1] loss: 827.172
[57,     1] loss: 872.860
[58,     1] loss: 860.193
[59,     1] loss: 839.497
[60,     1] loss: 895.708
[61,     1] loss: 855.412
[62,     1] loss: 786.570
[63,     1] loss: 853.291
[64,     1] loss: 803.774
[65,     1] loss: 788.529
[66,     1] loss: 821.255
[67,     1] loss: 791.620
[68,     1] loss: 776.664
[69,     1] loss: 778.121
[70,     1] loss: 754.477
[71,     1] loss: 737.964
[72,     1] loss: 760.058
[73,     1] loss: 717.450
[74,     1] loss: 732.162
[75,     1] loss: 702.179
[76,     1] loss: 745.605
[77,     1] loss: 746.097
[78,     1] loss: 717.038
[79,     1] loss: 731.648
[80,     1] loss: 720.545
[81,     1] loss: 650.346
[82,     1] loss: 647.397
[83,     1] loss: 693.956
[84,     1] loss: 657.874
[85,     1] loss: 704.753
Early stopping applied (best metric=0.37356483936309814)
Finished Training
Total time taken: 13.188406944274902
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.702
[2,     1] loss: 1262.229
[3,     1] loss: 1262.040
[4,     1] loss: 1259.957
[5,     1] loss: 1259.914
[6,     1] loss: 1263.481
[7,     1] loss: 1257.155
[8,     1] loss: 1259.473
[9,     1] loss: 1246.829
[10,     1] loss: 1239.406
[11,     1] loss: 1232.965
[12,     1] loss: 1209.328
[13,     1] loss: 1187.494
[14,     1] loss: 1164.642
[15,     1] loss: 1136.063
[16,     1] loss: 1113.493
[17,     1] loss: 1105.050
[18,     1] loss: 1079.783
[19,     1] loss: 1079.019
[20,     1] loss: 1048.463
[21,     1] loss: 1090.585
[22,     1] loss: 1040.289
[23,     1] loss: 1047.721
[24,     1] loss: 1042.116
[25,     1] loss: 1057.269
[26,     1] loss: 1047.530
[27,     1] loss: 1018.949
[28,     1] loss: 1012.562
[29,     1] loss: 1003.809
[30,     1] loss: 1027.583
[31,     1] loss: 1002.116
[32,     1] loss: 1042.469
[33,     1] loss: 1035.688
[34,     1] loss: 989.155
[35,     1] loss: 978.102
[36,     1] loss: 989.068
[37,     1] loss: 985.014
[38,     1] loss: 951.738
[39,     1] loss: 967.374
[40,     1] loss: 988.216
[41,     1] loss: 980.277
[42,     1] loss: 967.004
[43,     1] loss: 968.897
[44,     1] loss: 947.991
[45,     1] loss: 948.776
[46,     1] loss: 941.286
[47,     1] loss: 938.989
[48,     1] loss: 920.407
[49,     1] loss: 925.480
[50,     1] loss: 892.423
[51,     1] loss: 922.105
[52,     1] loss: 907.185
[53,     1] loss: 907.436
[54,     1] loss: 861.890
[55,     1] loss: 850.730
[56,     1] loss: 891.560
[57,     1] loss: 873.605
[58,     1] loss: 918.949
[59,     1] loss: 866.466
[60,     1] loss: 865.092
[61,     1] loss: 854.450
[62,     1] loss: 893.855
[63,     1] loss: 863.991
[64,     1] loss: 872.523
[65,     1] loss: 871.614
[66,     1] loss: 798.765
[67,     1] loss: 780.560
[68,     1] loss: 832.780
[69,     1] loss: 816.867
[70,     1] loss: 813.482
[71,     1] loss: 863.173
[72,     1] loss: 799.558
[73,     1] loss: 814.957
[74,     1] loss: 802.875
[75,     1] loss: 763.933
[76,     1] loss: 795.966
[77,     1] loss: 743.077
[78,     1] loss: 768.708
[79,     1] loss: 750.232
[80,     1] loss: 748.054
[81,     1] loss: 720.473
[82,     1] loss: 737.586
[83,     1] loss: 723.774
[84,     1] loss: 728.873
[85,     1] loss: 785.769
[86,     1] loss: 717.098
[87,     1] loss: 794.048
[88,     1] loss: 683.839
[89,     1] loss: 698.226
[90,     1] loss: 702.596
[91,     1] loss: 633.767
[92,     1] loss: 700.321
[93,     1] loss: 724.380
[94,     1] loss: 664.763
[95,     1] loss: 712.837
[96,     1] loss: 625.607
[97,     1] loss: 655.710
[98,     1] loss: 621.398
[99,     1] loss: 619.207
[100,     1] loss: 626.686
[101,     1] loss: 646.795
[102,     1] loss: 670.544
[103,     1] loss: 644.179
[104,     1] loss: 636.043
Early stopping applied (best metric=0.3434026539325714)
Finished Training
Total time taken: 15.533657550811768
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.050
[2,     1] loss: 1263.835
[3,     1] loss: 1264.514
[4,     1] loss: 1261.587
[5,     1] loss: 1261.088
[6,     1] loss: 1257.651
[7,     1] loss: 1256.104
[8,     1] loss: 1257.250
[9,     1] loss: 1249.417
[10,     1] loss: 1243.974
[11,     1] loss: 1235.381
[12,     1] loss: 1222.693
[13,     1] loss: 1204.798
[14,     1] loss: 1175.029
[15,     1] loss: 1158.942
[16,     1] loss: 1139.159
[17,     1] loss: 1108.630
[18,     1] loss: 1098.689
[19,     1] loss: 1074.159
[20,     1] loss: 1064.455
[21,     1] loss: 1044.239
[22,     1] loss: 1044.064
[23,     1] loss: 1014.196
[24,     1] loss: 1048.111
[25,     1] loss: 976.227
[26,     1] loss: 1057.740
[27,     1] loss: 1032.388
[28,     1] loss: 1017.537
[29,     1] loss: 1012.101
[30,     1] loss: 993.086
[31,     1] loss: 996.181
[32,     1] loss: 962.576
[33,     1] loss: 967.235
[34,     1] loss: 964.264
[35,     1] loss: 969.869
[36,     1] loss: 972.462
[37,     1] loss: 919.126
[38,     1] loss: 919.524
[39,     1] loss: 910.657
[40,     1] loss: 912.196
[41,     1] loss: 916.437
[42,     1] loss: 884.042
[43,     1] loss: 907.904
[44,     1] loss: 877.810
[45,     1] loss: 910.133
[46,     1] loss: 889.900
[47,     1] loss: 856.234
[48,     1] loss: 875.443
[49,     1] loss: 898.450
[50,     1] loss: 830.560
[51,     1] loss: 860.117
[52,     1] loss: 863.963
[53,     1] loss: 847.934
[54,     1] loss: 848.449
[55,     1] loss: 842.806
[56,     1] loss: 818.237
[57,     1] loss: 863.984
[58,     1] loss: 838.065
[59,     1] loss: 806.216
[60,     1] loss: 779.875
[61,     1] loss: 844.320
[62,     1] loss: 782.750
[63,     1] loss: 774.323
[64,     1] loss: 766.054
[65,     1] loss: 805.032
[66,     1] loss: 774.951
[67,     1] loss: 766.841
[68,     1] loss: 826.514
[69,     1] loss: 794.636
[70,     1] loss: 804.510
[71,     1] loss: 805.136
[72,     1] loss: 798.440
[73,     1] loss: 787.486
[74,     1] loss: 768.248
[75,     1] loss: 792.988
[76,     1] loss: 765.007
[77,     1] loss: 741.900
[78,     1] loss: 809.417
[79,     1] loss: 745.140
[80,     1] loss: 768.193
[81,     1] loss: 736.692
[82,     1] loss: 726.852
[83,     1] loss: 743.086
[84,     1] loss: 664.676
[85,     1] loss: 691.872
[86,     1] loss: 719.056
[87,     1] loss: 693.455
[88,     1] loss: 654.940
[89,     1] loss: 689.940
[90,     1] loss: 680.754
[91,     1] loss: 689.260
[92,     1] loss: 678.309
[93,     1] loss: 676.620
[94,     1] loss: 666.993
[95,     1] loss: 636.745
[96,     1] loss: 664.867
[97,     1] loss: 623.987
[98,     1] loss: 610.087
[99,     1] loss: 630.544
[100,     1] loss: 616.223
[101,     1] loss: 576.952
[102,     1] loss: 598.130
[103,     1] loss: 585.065
[104,     1] loss: 553.044
[105,     1] loss: 601.430
[106,     1] loss: 595.238
[107,     1] loss: 611.097
[108,     1] loss: 554.137
[109,     1] loss: 584.790
[110,     1] loss: 528.774
[111,     1] loss: 557.472
[112,     1] loss: 602.715
[113,     1] loss: 529.962
[114,     1] loss: 532.936
[115,     1] loss: 504.171
[116,     1] loss: 591.779
[117,     1] loss: 538.040
[118,     1] loss: 563.590
[119,     1] loss: 489.219
[120,     1] loss: 536.177
[121,     1] loss: 568.120
[122,     1] loss: 538.515
[123,     1] loss: 512.314
[124,     1] loss: 468.651
[125,     1] loss: 491.873
[126,     1] loss: 502.283
[127,     1] loss: 505.546
Early stopping applied (best metric=0.43007850646972656)
Finished Training
Total time taken: 19.386068105697632
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1266.467
[2,     1] loss: 1263.158
[3,     1] loss: 1263.899
[4,     1] loss: 1265.609
[5,     1] loss: 1264.845
[6,     1] loss: 1262.416
[7,     1] loss: 1259.995
[8,     1] loss: 1258.647
[9,     1] loss: 1260.890
[10,     1] loss: 1253.206
[11,     1] loss: 1252.829
[12,     1] loss: 1249.023
[13,     1] loss: 1243.816
[14,     1] loss: 1227.971
[15,     1] loss: 1211.439
[16,     1] loss: 1193.704
[17,     1] loss: 1165.242
[18,     1] loss: 1148.742
[19,     1] loss: 1136.496
[20,     1] loss: 1096.355
[21,     1] loss: 1078.468
[22,     1] loss: 1073.970
[23,     1] loss: 1047.703
[24,     1] loss: 1079.501
[25,     1] loss: 1035.695
[26,     1] loss: 1029.944
[27,     1] loss: 1018.998
[28,     1] loss: 1064.296
[29,     1] loss: 1035.642
[30,     1] loss: 1029.764
[31,     1] loss: 1039.641
[32,     1] loss: 1023.295
[33,     1] loss: 997.413
[34,     1] loss: 997.086
[35,     1] loss: 1013.513
[36,     1] loss: 1030.889
[37,     1] loss: 1003.724
[38,     1] loss: 1019.117
[39,     1] loss: 968.523
[40,     1] loss: 990.129
[41,     1] loss: 995.130
[42,     1] loss: 946.854
[43,     1] loss: 996.803
[44,     1] loss: 1002.059
[45,     1] loss: 959.105
[46,     1] loss: 941.848
[47,     1] loss: 930.380
[48,     1] loss: 933.042
[49,     1] loss: 944.301
[50,     1] loss: 912.325
[51,     1] loss: 938.511
[52,     1] loss: 894.256
[53,     1] loss: 914.111
[54,     1] loss: 932.097
[55,     1] loss: 948.728
[56,     1] loss: 885.895
[57,     1] loss: 874.394
[58,     1] loss: 853.120
[59,     1] loss: 906.274
[60,     1] loss: 869.740
[61,     1] loss: 937.977
[62,     1] loss: 826.558
[63,     1] loss: 829.841
[64,     1] loss: 859.165
[65,     1] loss: 875.679
[66,     1] loss: 847.602
[67,     1] loss: 844.868
[68,     1] loss: 866.968
[69,     1] loss: 807.502
[70,     1] loss: 786.814
[71,     1] loss: 779.597
[72,     1] loss: 843.170
[73,     1] loss: 802.934
[74,     1] loss: 796.658
[75,     1] loss: 772.144
[76,     1] loss: 769.435
[77,     1] loss: 780.350
[78,     1] loss: 781.011
[79,     1] loss: 762.342
[80,     1] loss: 800.269
[81,     1] loss: 788.510
[82,     1] loss: 745.997
[83,     1] loss: 771.387
[84,     1] loss: 770.959
[85,     1] loss: 755.014
[86,     1] loss: 755.451
[87,     1] loss: 735.420
[88,     1] loss: 775.947
[89,     1] loss: 717.147
[90,     1] loss: 699.091
[91,     1] loss: 653.623
[92,     1] loss: 689.826
[93,     1] loss: 666.776
[94,     1] loss: 645.531
[95,     1] loss: 646.532
[96,     1] loss: 657.313
[97,     1] loss: 657.048
[98,     1] loss: 693.138
Early stopping applied (best metric=0.35703304409980774)
Finished Training
Total time taken: 14.675568103790283
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.250
[2,     1] loss: 1265.765
[3,     1] loss: 1262.332
[4,     1] loss: 1261.497
[5,     1] loss: 1260.372
[6,     1] loss: 1261.736
[7,     1] loss: 1260.204
[8,     1] loss: 1257.891
[9,     1] loss: 1252.340
[10,     1] loss: 1250.496
[11,     1] loss: 1248.231
[12,     1] loss: 1243.271
[13,     1] loss: 1225.455
[14,     1] loss: 1223.162
[15,     1] loss: 1201.134
[16,     1] loss: 1186.471
[17,     1] loss: 1166.713
[18,     1] loss: 1159.684
[19,     1] loss: 1107.933
[20,     1] loss: 1121.477
[21,     1] loss: 1094.640
[22,     1] loss: 1094.812
[23,     1] loss: 1060.957
[24,     1] loss: 1039.300
[25,     1] loss: 1060.812
[26,     1] loss: 1066.835
[27,     1] loss: 1080.197
[28,     1] loss: 1084.778
[29,     1] loss: 1028.885
[30,     1] loss: 1064.569
[31,     1] loss: 1035.768
[32,     1] loss: 1008.956
[33,     1] loss: 1046.919
[34,     1] loss: 1007.393
[35,     1] loss: 1000.484
[36,     1] loss: 984.453
[37,     1] loss: 1020.285
[38,     1] loss: 1005.052
[39,     1] loss: 1005.910
[40,     1] loss: 975.849
[41,     1] loss: 967.971
[42,     1] loss: 984.978
[43,     1] loss: 952.438
[44,     1] loss: 936.444
[45,     1] loss: 925.741
[46,     1] loss: 934.765
[47,     1] loss: 956.252
[48,     1] loss: 1002.889
[49,     1] loss: 922.942
[50,     1] loss: 944.721
[51,     1] loss: 871.594
[52,     1] loss: 908.234
[53,     1] loss: 908.041
[54,     1] loss: 885.814
[55,     1] loss: 876.987
[56,     1] loss: 874.895
[57,     1] loss: 861.219
[58,     1] loss: 902.399
[59,     1] loss: 846.182
[60,     1] loss: 877.309
[61,     1] loss: 858.858
[62,     1] loss: 877.135
[63,     1] loss: 857.615
[64,     1] loss: 855.535
[65,     1] loss: 819.955
[66,     1] loss: 806.557
[67,     1] loss: 778.202
[68,     1] loss: 816.999
[69,     1] loss: 838.498
[70,     1] loss: 821.372
[71,     1] loss: 802.528
[72,     1] loss: 770.687
[73,     1] loss: 753.943
[74,     1] loss: 801.129
[75,     1] loss: 776.619
[76,     1] loss: 765.973
[77,     1] loss: 746.088
[78,     1] loss: 771.898
[79,     1] loss: 748.997
[80,     1] loss: 754.141
[81,     1] loss: 743.638
[82,     1] loss: 723.750
[83,     1] loss: 778.185
[84,     1] loss: 691.144
[85,     1] loss: 708.079
[86,     1] loss: 709.365
[87,     1] loss: 710.660
[88,     1] loss: 649.377
[89,     1] loss: 680.074
[90,     1] loss: 673.334
[91,     1] loss: 664.377
[92,     1] loss: 634.380
[93,     1] loss: 632.949
[94,     1] loss: 711.786
[95,     1] loss: 663.655
[96,     1] loss: 617.322
[97,     1] loss: 670.392
[98,     1] loss: 587.225
[99,     1] loss: 681.641
Early stopping applied (best metric=0.4006296694278717)
Finished Training
Total time taken: 15.38164210319519
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.665
[2,     1] loss: 1260.406
[3,     1] loss: 1262.222
[4,     1] loss: 1263.190
[5,     1] loss: 1259.948
[6,     1] loss: 1257.322
[7,     1] loss: 1259.121
[8,     1] loss: 1256.129
[9,     1] loss: 1258.775
[10,     1] loss: 1247.073
[11,     1] loss: 1245.689
[12,     1] loss: 1238.899
[13,     1] loss: 1232.512
[14,     1] loss: 1217.758
[15,     1] loss: 1196.167
[16,     1] loss: 1173.391
[17,     1] loss: 1156.382
[18,     1] loss: 1132.026
[19,     1] loss: 1130.270
[20,     1] loss: 1109.579
[21,     1] loss: 1104.214
[22,     1] loss: 1110.664
[23,     1] loss: 1056.431
[24,     1] loss: 1016.216
[25,     1] loss: 1064.827
[26,     1] loss: 1047.516
[27,     1] loss: 1005.182
[28,     1] loss: 1017.033
[29,     1] loss: 1032.924
[30,     1] loss: 1017.200
[31,     1] loss: 1004.555
[32,     1] loss: 975.484
[33,     1] loss: 1001.027
[34,     1] loss: 929.289
[35,     1] loss: 1009.772
[36,     1] loss: 998.396
[37,     1] loss: 990.267
[38,     1] loss: 974.865
[39,     1] loss: 943.614
[40,     1] loss: 948.872
[41,     1] loss: 961.143
[42,     1] loss: 942.885
[43,     1] loss: 969.926
[44,     1] loss: 967.768
[45,     1] loss: 945.240
[46,     1] loss: 946.419
[47,     1] loss: 859.425
[48,     1] loss: 898.546
[49,     1] loss: 951.895
[50,     1] loss: 974.199
[51,     1] loss: 892.835
[52,     1] loss: 899.829
[53,     1] loss: 908.667
[54,     1] loss: 886.024
[55,     1] loss: 890.869
[56,     1] loss: 851.237
[57,     1] loss: 903.276
[58,     1] loss: 842.595
[59,     1] loss: 857.457
[60,     1] loss: 802.565
[61,     1] loss: 841.214
[62,     1] loss: 805.515
[63,     1] loss: 866.679
[64,     1] loss: 867.900
[65,     1] loss: 822.711
[66,     1] loss: 847.056
[67,     1] loss: 795.168
[68,     1] loss: 825.214
[69,     1] loss: 816.370
[70,     1] loss: 741.648
[71,     1] loss: 775.116
[72,     1] loss: 753.573
[73,     1] loss: 763.261
[74,     1] loss: 723.827
[75,     1] loss: 760.066
[76,     1] loss: 757.322
[77,     1] loss: 766.856
[78,     1] loss: 787.996
[79,     1] loss: 735.898
Early stopping applied (best metric=0.37771913409233093)
Finished Training
Total time taken: 12.611344337463379
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1263.666
[2,     1] loss: 1266.298
[3,     1] loss: 1262.380
[4,     1] loss: 1262.074
[5,     1] loss: 1259.664
[6,     1] loss: 1260.599
[7,     1] loss: 1262.128
[8,     1] loss: 1256.927
[9,     1] loss: 1253.711
[10,     1] loss: 1251.209
[11,     1] loss: 1245.427
[12,     1] loss: 1238.775
[13,     1] loss: 1229.005
[14,     1] loss: 1209.784
[15,     1] loss: 1189.013
[16,     1] loss: 1163.362
[17,     1] loss: 1144.423
[18,     1] loss: 1139.641
[19,     1] loss: 1102.096
[20,     1] loss: 1103.599
[21,     1] loss: 1104.733
[22,     1] loss: 1053.825
[23,     1] loss: 1053.162
[24,     1] loss: 1077.150
[25,     1] loss: 1035.225
[26,     1] loss: 1032.644
[27,     1] loss: 1002.967
[28,     1] loss: 1004.323
[29,     1] loss: 1001.421
[30,     1] loss: 1006.282
[31,     1] loss: 1001.497
[32,     1] loss: 1018.465
[33,     1] loss: 990.851
[34,     1] loss: 998.821
[35,     1] loss: 971.888
[36,     1] loss: 996.335
[37,     1] loss: 961.201
[38,     1] loss: 1004.275
[39,     1] loss: 980.522
[40,     1] loss: 993.247
[41,     1] loss: 981.133
[42,     1] loss: 990.049
[43,     1] loss: 957.156
[44,     1] loss: 976.329
[45,     1] loss: 980.521
[46,     1] loss: 936.733
[47,     1] loss: 914.207
[48,     1] loss: 900.994
[49,     1] loss: 947.744
[50,     1] loss: 932.806
[51,     1] loss: 935.209
[52,     1] loss: 933.107
[53,     1] loss: 936.295
[54,     1] loss: 851.448
[55,     1] loss: 834.247
[56,     1] loss: 848.341
[57,     1] loss: 852.222
[58,     1] loss: 868.776
[59,     1] loss: 870.469
[60,     1] loss: 821.378
[61,     1] loss: 830.936
[62,     1] loss: 860.229
[63,     1] loss: 813.114
[64,     1] loss: 813.906
[65,     1] loss: 789.732
[66,     1] loss: 818.085
[67,     1] loss: 794.446
[68,     1] loss: 787.986
[69,     1] loss: 800.207
[70,     1] loss: 855.591
[71,     1] loss: 738.349
[72,     1] loss: 758.472
[73,     1] loss: 733.059
[74,     1] loss: 763.541
[75,     1] loss: 761.369
[76,     1] loss: 734.347
[77,     1] loss: 771.907
[78,     1] loss: 780.842
[79,     1] loss: 767.806
[80,     1] loss: 737.398
[81,     1] loss: 690.738
[82,     1] loss: 728.865
[83,     1] loss: 707.408
[84,     1] loss: 691.685
[85,     1] loss: 702.570
Early stopping applied (best metric=0.38853272795677185)
Finished Training
Total time taken: 12.99439024925232
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.104
[2,     1] loss: 1260.049
[3,     1] loss: 1263.911
[4,     1] loss: 1261.140
[5,     1] loss: 1260.243
[6,     1] loss: 1258.110
[7,     1] loss: 1256.537
[8,     1] loss: 1255.703
[9,     1] loss: 1252.030
[10,     1] loss: 1244.242
[11,     1] loss: 1237.540
[12,     1] loss: 1223.946
[13,     1] loss: 1199.891
[14,     1] loss: 1185.130
[15,     1] loss: 1155.700
[16,     1] loss: 1147.968
[17,     1] loss: 1144.549
[18,     1] loss: 1103.664
[19,     1] loss: 1066.492
[20,     1] loss: 1055.366
[21,     1] loss: 1050.561
[22,     1] loss: 1072.436
[23,     1] loss: 1032.156
[24,     1] loss: 1075.998
[25,     1] loss: 1037.424
[26,     1] loss: 999.611
[27,     1] loss: 1025.342
[28,     1] loss: 1029.484
[29,     1] loss: 1008.997
[30,     1] loss: 1009.599
[31,     1] loss: 995.080
[32,     1] loss: 995.494
[33,     1] loss: 996.767
[34,     1] loss: 979.471
[35,     1] loss: 952.389
[36,     1] loss: 963.917
[37,     1] loss: 988.164
[38,     1] loss: 1002.384
[39,     1] loss: 967.908
[40,     1] loss: 966.256
[41,     1] loss: 982.068
[42,     1] loss: 962.907
[43,     1] loss: 1000.456
[44,     1] loss: 926.878
[45,     1] loss: 929.353
[46,     1] loss: 959.279
[47,     1] loss: 921.962
[48,     1] loss: 924.149
[49,     1] loss: 912.792
[50,     1] loss: 907.064
[51,     1] loss: 877.843
[52,     1] loss: 894.414
[53,     1] loss: 888.005
[54,     1] loss: 905.600
[55,     1] loss: 889.986
[56,     1] loss: 918.103
[57,     1] loss: 851.718
[58,     1] loss: 897.427
[59,     1] loss: 833.848
[60,     1] loss: 889.495
[61,     1] loss: 883.870
[62,     1] loss: 841.982
[63,     1] loss: 855.484
[64,     1] loss: 853.653
[65,     1] loss: 810.432
[66,     1] loss: 858.662
[67,     1] loss: 790.240
[68,     1] loss: 784.997
[69,     1] loss: 796.152
[70,     1] loss: 766.419
[71,     1] loss: 816.577
[72,     1] loss: 829.708
[73,     1] loss: 779.698
[74,     1] loss: 798.594
[75,     1] loss: 799.665
[76,     1] loss: 753.391
[77,     1] loss: 732.378
[78,     1] loss: 742.269
[79,     1] loss: 785.760
[80,     1] loss: 748.181
[81,     1] loss: 709.716
[82,     1] loss: 675.268
[83,     1] loss: 655.453
[84,     1] loss: 729.344
[85,     1] loss: 684.861
[86,     1] loss: 705.407
[87,     1] loss: 668.355
[88,     1] loss: 651.520
[89,     1] loss: 687.025
[90,     1] loss: 690.799
[91,     1] loss: 652.737
[92,     1] loss: 684.696
[93,     1] loss: 654.648
[94,     1] loss: 680.493
[95,     1] loss: 626.647
[96,     1] loss: 660.221
[97,     1] loss: 646.512
[98,     1] loss: 590.782
[99,     1] loss: 655.963
[100,     1] loss: 600.078
[101,     1] loss: 582.802
[102,     1] loss: 614.454
[103,     1] loss: 615.831
[104,     1] loss: 615.880
[105,     1] loss: 579.323
[106,     1] loss: 572.632
[107,     1] loss: 538.962
[108,     1] loss: 561.251
[109,     1] loss: 562.396
[110,     1] loss: 505.257
[111,     1] loss: 539.090
[112,     1] loss: 479.530
[113,     1] loss: 552.178
[114,     1] loss: 536.170
[115,     1] loss: 581.208
[116,     1] loss: 534.392
[117,     1] loss: 510.136
[118,     1] loss: 518.075
[119,     1] loss: 527.866
[120,     1] loss: 509.914
[121,     1] loss: 492.213
[122,     1] loss: 533.207
[123,     1] loss: 558.649
[124,     1] loss: 457.871
[125,     1] loss: 484.451
Early stopping applied (best metric=0.42476266622543335)
Finished Training
Total time taken: 19.8621244430542
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1268.630
[2,     1] loss: 1265.956
[3,     1] loss: 1267.231
[4,     1] loss: 1263.795
[5,     1] loss: 1266.910
[6,     1] loss: 1260.952
[7,     1] loss: 1257.731
[8,     1] loss: 1253.442
[9,     1] loss: 1255.402
[10,     1] loss: 1240.290
[11,     1] loss: 1240.294
[12,     1] loss: 1218.315
[13,     1] loss: 1212.927
[14,     1] loss: 1191.479
[15,     1] loss: 1167.247
[16,     1] loss: 1148.903
[17,     1] loss: 1145.943
[18,     1] loss: 1104.330
[19,     1] loss: 1092.013
[20,     1] loss: 1082.409
[21,     1] loss: 1074.742
[22,     1] loss: 1093.421
[23,     1] loss: 1129.433
[24,     1] loss: 1076.642
[25,     1] loss: 1044.037
[26,     1] loss: 1042.140
[27,     1] loss: 1049.037
[28,     1] loss: 1049.609
[29,     1] loss: 1042.502
[30,     1] loss: 1037.326
[31,     1] loss: 1012.268
[32,     1] loss: 1016.460
[33,     1] loss: 1031.389
[34,     1] loss: 1033.522
[35,     1] loss: 1009.074
[36,     1] loss: 988.541
[37,     1] loss: 950.546
[38,     1] loss: 1015.289
[39,     1] loss: 976.062
[40,     1] loss: 976.589
[41,     1] loss: 1015.452
[42,     1] loss: 968.454
[43,     1] loss: 960.099
[44,     1] loss: 960.664
[45,     1] loss: 971.368
[46,     1] loss: 914.954
[47,     1] loss: 953.042
[48,     1] loss: 913.068
[49,     1] loss: 943.373
[50,     1] loss: 893.659
[51,     1] loss: 906.885
[52,     1] loss: 910.777
[53,     1] loss: 891.636
[54,     1] loss: 826.173
[55,     1] loss: 845.675
[56,     1] loss: 804.494
[57,     1] loss: 853.496
[58,     1] loss: 829.840
[59,     1] loss: 857.194
[60,     1] loss: 851.772
[61,     1] loss: 824.638
[62,     1] loss: 850.468
[63,     1] loss: 840.829
[64,     1] loss: 844.549
[65,     1] loss: 841.590
[66,     1] loss: 859.089
[67,     1] loss: 835.723
[68,     1] loss: 781.754
[69,     1] loss: 797.089
[70,     1] loss: 787.618
[71,     1] loss: 792.719
[72,     1] loss: 770.666
[73,     1] loss: 768.808
[74,     1] loss: 804.555
[75,     1] loss: 736.362
[76,     1] loss: 744.152
[77,     1] loss: 802.102
[78,     1] loss: 761.861
[79,     1] loss: 720.075
[80,     1] loss: 751.429
[81,     1] loss: 714.724
[82,     1] loss: 676.933
[83,     1] loss: 724.475
[84,     1] loss: 653.885
[85,     1] loss: 717.596
[86,     1] loss: 686.892
[87,     1] loss: 658.906
[88,     1] loss: 647.032
[89,     1] loss: 686.236
[90,     1] loss: 642.310
[91,     1] loss: 614.710
[92,     1] loss: 646.402
[93,     1] loss: 652.809
[94,     1] loss: 668.007
[95,     1] loss: 610.657
[96,     1] loss: 610.721
[97,     1] loss: 673.760
[98,     1] loss: 614.830
[99,     1] loss: 614.223
[100,     1] loss: 591.828
[101,     1] loss: 575.559
[102,     1] loss: 573.103
[103,     1] loss: 606.272
[104,     1] loss: 573.216
[105,     1] loss: 596.203
Early stopping applied (best metric=0.3215291202068329)
Finished Training
Total time taken: 16.641780614852905
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1268.056
[2,     1] loss: 1260.294
[3,     1] loss: 1262.694
[4,     1] loss: 1259.023
[5,     1] loss: 1263.917
[6,     1] loss: 1260.715
[7,     1] loss: 1263.515
[8,     1] loss: 1260.869
[9,     1] loss: 1260.527
[10,     1] loss: 1256.286
[11,     1] loss: 1252.985
[12,     1] loss: 1252.498
[13,     1] loss: 1244.489
[14,     1] loss: 1240.180
[15,     1] loss: 1236.860
[16,     1] loss: 1218.300
[17,     1] loss: 1199.103
[18,     1] loss: 1180.377
[19,     1] loss: 1145.109
[20,     1] loss: 1141.099
[21,     1] loss: 1122.852
[22,     1] loss: 1073.882
[23,     1] loss: 1089.665
[24,     1] loss: 1083.185
[25,     1] loss: 1060.835
[26,     1] loss: 1038.120
[27,     1] loss: 1049.650
[28,     1] loss: 1079.186
[29,     1] loss: 1022.234
[30,     1] loss: 996.922
[31,     1] loss: 976.984
[32,     1] loss: 991.284
[33,     1] loss: 1017.768
[34,     1] loss: 1009.244
[35,     1] loss: 997.016
[36,     1] loss: 1014.099
[37,     1] loss: 1013.537
[38,     1] loss: 955.803
[39,     1] loss: 957.059
[40,     1] loss: 934.797
[41,     1] loss: 964.480
[42,     1] loss: 971.615
[43,     1] loss: 961.695
[44,     1] loss: 906.514
[45,     1] loss: 995.232
[46,     1] loss: 957.797
[47,     1] loss: 927.880
[48,     1] loss: 903.501
[49,     1] loss: 904.508
[50,     1] loss: 909.920
[51,     1] loss: 917.926
[52,     1] loss: 945.639
[53,     1] loss: 930.521
[54,     1] loss: 919.728
[55,     1] loss: 889.008
[56,     1] loss: 889.117
[57,     1] loss: 918.084
[58,     1] loss: 875.480
[59,     1] loss: 895.962
[60,     1] loss: 836.190
[61,     1] loss: 925.614
[62,     1] loss: 864.795
[63,     1] loss: 862.736
[64,     1] loss: 877.673
[65,     1] loss: 903.358
[66,     1] loss: 882.520
[67,     1] loss: 880.018
[68,     1] loss: 849.703
[69,     1] loss: 836.760
[70,     1] loss: 869.387
[71,     1] loss: 841.859
[72,     1] loss: 804.836
[73,     1] loss: 814.508
[74,     1] loss: 832.488
[75,     1] loss: 824.700
[76,     1] loss: 780.106
[77,     1] loss: 800.028
[78,     1] loss: 762.700
[79,     1] loss: 793.263
[80,     1] loss: 763.125
[81,     1] loss: 775.406
[82,     1] loss: 787.131
[83,     1] loss: 759.539
[84,     1] loss: 787.628
[85,     1] loss: 713.425
[86,     1] loss: 747.576
Early stopping applied (best metric=0.39509180188179016)
Finished Training
Total time taken: 13.501441240310669
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1266.743
[2,     1] loss: 1263.505
[3,     1] loss: 1264.803
[4,     1] loss: 1260.585
[5,     1] loss: 1259.206
[6,     1] loss: 1264.074
[7,     1] loss: 1258.797
[8,     1] loss: 1259.416
[9,     1] loss: 1253.974
[10,     1] loss: 1255.174
[11,     1] loss: 1246.389
[12,     1] loss: 1241.840
[13,     1] loss: 1233.106
[14,     1] loss: 1219.003
[15,     1] loss: 1203.188
[16,     1] loss: 1174.227
[17,     1] loss: 1160.522
[18,     1] loss: 1136.927
[19,     1] loss: 1110.758
[20,     1] loss: 1087.412
[21,     1] loss: 1071.559
[22,     1] loss: 1027.359
[23,     1] loss: 1032.826
[24,     1] loss: 1006.279
[25,     1] loss: 1017.336
[26,     1] loss: 1007.085
[27,     1] loss: 1019.782
[28,     1] loss: 982.360
[29,     1] loss: 1021.260
[30,     1] loss: 1007.809
[31,     1] loss: 986.390
[32,     1] loss: 971.496
[33,     1] loss: 1009.152
[34,     1] loss: 992.735
[35,     1] loss: 990.724
[36,     1] loss: 995.290
[37,     1] loss: 988.056
[38,     1] loss: 960.597
[39,     1] loss: 963.287
[40,     1] loss: 919.963
[41,     1] loss: 944.172
[42,     1] loss: 940.426
[43,     1] loss: 936.998
[44,     1] loss: 924.074
[45,     1] loss: 958.785
[46,     1] loss: 894.891
[47,     1] loss: 879.988
[48,     1] loss: 892.001
[49,     1] loss: 900.262
[50,     1] loss: 896.302
[51,     1] loss: 915.440
[52,     1] loss: 906.980
[53,     1] loss: 865.651
[54,     1] loss: 823.283
[55,     1] loss: 813.841
[56,     1] loss: 824.261
[57,     1] loss: 844.611
[58,     1] loss: 849.562
[59,     1] loss: 864.899
[60,     1] loss: 857.268
[61,     1] loss: 783.098
[62,     1] loss: 821.400
[63,     1] loss: 820.820
[64,     1] loss: 775.636
[65,     1] loss: 791.849
[66,     1] loss: 832.730
[67,     1] loss: 762.763
[68,     1] loss: 792.417
[69,     1] loss: 744.586
[70,     1] loss: 734.317
[71,     1] loss: 733.158
[72,     1] loss: 693.233
[73,     1] loss: 715.311
[74,     1] loss: 770.564
[75,     1] loss: 723.483
[76,     1] loss: 755.116
[77,     1] loss: 755.605
[78,     1] loss: 699.103
[79,     1] loss: 660.778
[80,     1] loss: 690.718
[81,     1] loss: 663.191
[82,     1] loss: 694.566
[83,     1] loss: 659.471
Early stopping applied (best metric=0.45440438389778137)
Finished Training
Total time taken: 13.350438356399536
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1273.120
[2,     1] loss: 1266.512
[3,     1] loss: 1261.885
[4,     1] loss: 1262.233
[5,     1] loss: 1260.843
[6,     1] loss: 1259.480
[7,     1] loss: 1259.607
[8,     1] loss: 1259.706
[9,     1] loss: 1259.416
[10,     1] loss: 1258.942
[11,     1] loss: 1253.783
[12,     1] loss: 1258.124
[13,     1] loss: 1245.965
[14,     1] loss: 1238.065
[15,     1] loss: 1227.219
[16,     1] loss: 1216.262
[17,     1] loss: 1208.095
[18,     1] loss: 1180.522
[19,     1] loss: 1167.195
[20,     1] loss: 1143.641
[21,     1] loss: 1114.466
[22,     1] loss: 1084.933
[23,     1] loss: 1092.163
[24,     1] loss: 1050.294
[25,     1] loss: 1053.870
[26,     1] loss: 1052.422
[27,     1] loss: 1047.138
[28,     1] loss: 1004.196
[29,     1] loss: 1028.093
[30,     1] loss: 975.815
[31,     1] loss: 983.999
[32,     1] loss: 1011.512
[33,     1] loss: 1028.433
[34,     1] loss: 1029.369
[35,     1] loss: 953.232
[36,     1] loss: 954.255
[37,     1] loss: 961.146
[38,     1] loss: 975.581
[39,     1] loss: 992.194
[40,     1] loss: 954.501
[41,     1] loss: 991.629
[42,     1] loss: 943.036
[43,     1] loss: 988.188
[44,     1] loss: 967.711
[45,     1] loss: 961.652
[46,     1] loss: 938.831
[47,     1] loss: 935.795
[48,     1] loss: 943.855
[49,     1] loss: 893.160
[50,     1] loss: 928.057
[51,     1] loss: 892.686
[52,     1] loss: 920.293
[53,     1] loss: 889.697
[54,     1] loss: 928.003
[55,     1] loss: 907.474
[56,     1] loss: 879.981
[57,     1] loss: 866.048
[58,     1] loss: 887.736
[59,     1] loss: 870.331
[60,     1] loss: 866.626
[61,     1] loss: 852.934
[62,     1] loss: 856.120
[63,     1] loss: 817.354
[64,     1] loss: 821.589
[65,     1] loss: 834.196
[66,     1] loss: 839.827
[67,     1] loss: 860.230
[68,     1] loss: 834.705
[69,     1] loss: 796.268
[70,     1] loss: 807.778
[71,     1] loss: 819.150
[72,     1] loss: 751.945
[73,     1] loss: 750.373
[74,     1] loss: 786.890
[75,     1] loss: 794.593
[76,     1] loss: 789.255
[77,     1] loss: 736.635
[78,     1] loss: 735.949
[79,     1] loss: 741.187
[80,     1] loss: 715.034
[81,     1] loss: 680.638
[82,     1] loss: 652.385
[83,     1] loss: 720.831
[84,     1] loss: 678.643
[85,     1] loss: 705.285
[86,     1] loss: 722.189
[87,     1] loss: 701.126
[88,     1] loss: 677.085
[89,     1] loss: 661.400
[90,     1] loss: 653.204
[91,     1] loss: 640.358
[92,     1] loss: 736.901
[93,     1] loss: 632.170
[94,     1] loss: 672.505
[95,     1] loss: 577.970
[96,     1] loss: 622.690
[97,     1] loss: 627.767
[98,     1] loss: 638.632
[99,     1] loss: 601.705
[100,     1] loss: 643.676
[101,     1] loss: 637.436
[102,     1] loss: 608.400
[103,     1] loss: 570.520
[104,     1] loss: 632.529
[105,     1] loss: 588.832
[106,     1] loss: 543.409
Early stopping applied (best metric=0.34980008006095886)
Finished Training
Total time taken: 17.87790584564209
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1272.122
[2,     1] loss: 1263.636
[3,     1] loss: 1266.004
[4,     1] loss: 1262.236
[5,     1] loss: 1261.708
[6,     1] loss: 1259.087
[7,     1] loss: 1261.542
[8,     1] loss: 1255.510
[9,     1] loss: 1249.630
[10,     1] loss: 1243.624
[11,     1] loss: 1235.630
[12,     1] loss: 1223.055
[13,     1] loss: 1199.756
[14,     1] loss: 1185.011
[15,     1] loss: 1170.463
[16,     1] loss: 1156.654
[17,     1] loss: 1130.320
[18,     1] loss: 1116.732
[19,     1] loss: 1096.635
[20,     1] loss: 1060.378
[21,     1] loss: 1073.088
[22,     1] loss: 1043.068
[23,     1] loss: 1069.218
[24,     1] loss: 1044.764
[25,     1] loss: 1032.747
[26,     1] loss: 1063.797
[27,     1] loss: 1020.434
[28,     1] loss: 1002.875
[29,     1] loss: 1005.380
[30,     1] loss: 1045.974
[31,     1] loss: 1028.843
[32,     1] loss: 1013.101
[33,     1] loss: 963.308
[34,     1] loss: 999.973
[35,     1] loss: 1005.443
[36,     1] loss: 986.458
[37,     1] loss: 983.399
[38,     1] loss: 978.942
[39,     1] loss: 948.982
[40,     1] loss: 940.474
[41,     1] loss: 978.599
[42,     1] loss: 956.929
[43,     1] loss: 935.494
[44,     1] loss: 951.966
[45,     1] loss: 940.499
[46,     1] loss: 1027.166
[47,     1] loss: 922.403
[48,     1] loss: 972.017
[49,     1] loss: 884.918
[50,     1] loss: 936.622
[51,     1] loss: 925.904
[52,     1] loss: 851.334
[53,     1] loss: 877.077
[54,     1] loss: 907.300
[55,     1] loss: 872.533
[56,     1] loss: 882.200
[57,     1] loss: 849.708
[58,     1] loss: 860.292
[59,     1] loss: 837.633
[60,     1] loss: 843.007
[61,     1] loss: 839.160
[62,     1] loss: 822.829
[63,     1] loss: 857.051
[64,     1] loss: 873.789
[65,     1] loss: 841.495
[66,     1] loss: 817.663
[67,     1] loss: 825.828
[68,     1] loss: 794.864
[69,     1] loss: 791.827
[70,     1] loss: 776.319
[71,     1] loss: 817.596
[72,     1] loss: 772.040
[73,     1] loss: 822.759
[74,     1] loss: 791.857
[75,     1] loss: 767.434
[76,     1] loss: 714.192
[77,     1] loss: 759.343
[78,     1] loss: 728.710
[79,     1] loss: 745.687
[80,     1] loss: 765.299
[81,     1] loss: 720.478
[82,     1] loss: 703.078
[83,     1] loss: 704.153
[84,     1] loss: 658.240
[85,     1] loss: 738.712
[86,     1] loss: 740.099
[87,     1] loss: 732.324
[88,     1] loss: 673.404
[89,     1] loss: 679.913
[90,     1] loss: 694.990
[91,     1] loss: 654.893
[92,     1] loss: 631.218
[93,     1] loss: 636.577
[94,     1] loss: 640.798
[95,     1] loss: 646.891
[96,     1] loss: 629.215
[97,     1] loss: 592.703
[98,     1] loss: 649.352
[99,     1] loss: 632.187
[100,     1] loss: 616.427
[101,     1] loss: 642.852
Early stopping applied (best metric=0.37256166338920593)
Finished Training
Total time taken: 15.857691287994385
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1266.732
[2,     1] loss: 1265.113
[3,     1] loss: 1262.584
[4,     1] loss: 1263.776
[5,     1] loss: 1262.620
[6,     1] loss: 1261.144
[7,     1] loss: 1259.102
[8,     1] loss: 1254.885
[9,     1] loss: 1250.758
[10,     1] loss: 1239.817
[11,     1] loss: 1231.952
[12,     1] loss: 1215.837
[13,     1] loss: 1199.994
[14,     1] loss: 1182.145
[15,     1] loss: 1140.646
[16,     1] loss: 1130.073
[17,     1] loss: 1088.923
[18,     1] loss: 1095.973
[19,     1] loss: 1068.137
[20,     1] loss: 1049.115
[21,     1] loss: 1078.099
[22,     1] loss: 1065.157
[23,     1] loss: 1065.803
[24,     1] loss: 1036.062
[25,     1] loss: 1066.833
[26,     1] loss: 1056.144
[27,     1] loss: 1015.264
[28,     1] loss: 1020.393
[29,     1] loss: 1001.591
[30,     1] loss: 1010.929
[31,     1] loss: 999.168
[32,     1] loss: 992.888
[33,     1] loss: 964.687
[34,     1] loss: 969.645
[35,     1] loss: 939.667
[36,     1] loss: 966.726
[37,     1] loss: 990.304
[38,     1] loss: 973.161
[39,     1] loss: 950.583
[40,     1] loss: 925.443
[41,     1] loss: 955.102
[42,     1] loss: 921.450
[43,     1] loss: 885.986
[44,     1] loss: 883.040
[45,     1] loss: 911.658
[46,     1] loss: 929.915
[47,     1] loss: 889.176
[48,     1] loss: 878.102
[49,     1] loss: 938.959
[50,     1] loss: 883.139
[51,     1] loss: 891.099
[52,     1] loss: 855.088
[53,     1] loss: 893.289
[54,     1] loss: 862.777
[55,     1] loss: 868.788
[56,     1] loss: 859.291
[57,     1] loss: 860.384
[58,     1] loss: 821.094
[59,     1] loss: 857.440
[60,     1] loss: 831.479
[61,     1] loss: 810.040
[62,     1] loss: 823.617
[63,     1] loss: 852.168
[64,     1] loss: 771.464
[65,     1] loss: 800.009
[66,     1] loss: 811.119
[67,     1] loss: 748.999
[68,     1] loss: 791.764
[69,     1] loss: 774.482
[70,     1] loss: 823.076
[71,     1] loss: 770.905
[72,     1] loss: 756.922
[73,     1] loss: 768.889
[74,     1] loss: 755.761
[75,     1] loss: 693.665
[76,     1] loss: 698.752
[77,     1] loss: 770.653
[78,     1] loss: 697.517
[79,     1] loss: 739.016
[80,     1] loss: 731.427
[81,     1] loss: 725.972
[82,     1] loss: 693.286
[83,     1] loss: 679.336
[84,     1] loss: 672.278
[85,     1] loss: 640.089
[86,     1] loss: 658.893
[87,     1] loss: 681.358
[88,     1] loss: 629.596
[89,     1] loss: 637.866
[90,     1] loss: 672.941
[91,     1] loss: 611.052
[92,     1] loss: 634.623
[93,     1] loss: 587.108
[94,     1] loss: 577.867
[95,     1] loss: 562.780
[96,     1] loss: 614.861
[97,     1] loss: 589.534
[98,     1] loss: 582.773
[99,     1] loss: 547.521
[100,     1] loss: 582.472
[101,     1] loss: 549.110
[102,     1] loss: 608.749
[103,     1] loss: 504.092
[104,     1] loss: 584.373
[105,     1] loss: 559.070
[106,     1] loss: 556.981
[107,     1] loss: 579.251
[108,     1] loss: 507.860
[109,     1] loss: 588.163
[110,     1] loss: 579.561
[111,     1] loss: 532.113
[112,     1] loss: 580.105
[113,     1] loss: 507.040
[114,     1] loss: 535.992
[115,     1] loss: 538.373
[116,     1] loss: 514.738
[117,     1] loss: 488.482
[118,     1] loss: 482.866
[119,     1] loss: 495.954
[120,     1] loss: 451.108
[121,     1] loss: 472.386
[122,     1] loss: 479.166
[123,     1] loss: 453.877
[124,     1] loss: 493.154
[125,     1] loss: 452.516
Early stopping applied (best metric=0.35701775550842285)
Finished Training
Total time taken: 19.88912343978882
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1261.606
[2,     1] loss: 1262.729
[3,     1] loss: 1261.012
[4,     1] loss: 1259.557
[5,     1] loss: 1261.997
[6,     1] loss: 1257.692
[7,     1] loss: 1252.926
[8,     1] loss: 1250.244
[9,     1] loss: 1237.275
[10,     1] loss: 1235.422
[11,     1] loss: 1212.800
[12,     1] loss: 1185.501
[13,     1] loss: 1174.378
[14,     1] loss: 1131.881
[15,     1] loss: 1132.779
[16,     1] loss: 1076.826
[17,     1] loss: 1086.455
[18,     1] loss: 1063.757
[19,     1] loss: 1045.181
[20,     1] loss: 1070.679
[21,     1] loss: 1028.785
[22,     1] loss: 1036.862
[23,     1] loss: 971.805
[24,     1] loss: 1019.787
[25,     1] loss: 1074.103
[26,     1] loss: 1034.449
[27,     1] loss: 1034.950
[28,     1] loss: 960.557
[29,     1] loss: 1008.451
[30,     1] loss: 1021.943
[31,     1] loss: 1021.981
[32,     1] loss: 1007.045
[33,     1] loss: 976.581
[34,     1] loss: 975.849
[35,     1] loss: 980.557
[36,     1] loss: 989.741
[37,     1] loss: 945.071
[38,     1] loss: 958.380
[39,     1] loss: 940.992
[40,     1] loss: 931.206
[41,     1] loss: 930.007
[42,     1] loss: 987.670
[43,     1] loss: 992.024
[44,     1] loss: 926.551
[45,     1] loss: 868.023
[46,     1] loss: 922.498
[47,     1] loss: 910.395
[48,     1] loss: 907.468
[49,     1] loss: 890.795
[50,     1] loss: 940.124
[51,     1] loss: 869.804
[52,     1] loss: 886.580
[53,     1] loss: 881.008
[54,     1] loss: 864.498
[55,     1] loss: 862.084
[56,     1] loss: 910.918
[57,     1] loss: 879.344
[58,     1] loss: 854.878
[59,     1] loss: 929.375
[60,     1] loss: 849.049
[61,     1] loss: 831.248
[62,     1] loss: 862.453
[63,     1] loss: 806.365
[64,     1] loss: 850.578
[65,     1] loss: 823.035
[66,     1] loss: 813.405
[67,     1] loss: 819.870
[68,     1] loss: 747.383
[69,     1] loss: 824.292
[70,     1] loss: 838.308
[71,     1] loss: 817.309
[72,     1] loss: 788.208
[73,     1] loss: 777.369
[74,     1] loss: 804.850
[75,     1] loss: 774.022
[76,     1] loss: 750.261
[77,     1] loss: 776.209
[78,     1] loss: 709.524
[79,     1] loss: 771.135
[80,     1] loss: 748.588
[81,     1] loss: 792.473
[82,     1] loss: 687.201
[83,     1] loss: 704.476
[84,     1] loss: 716.063
[85,     1] loss: 702.212
[86,     1] loss: 713.447
[87,     1] loss: 659.049
[88,     1] loss: 709.361
[89,     1] loss: 685.412
[90,     1] loss: 654.368
[91,     1] loss: 671.854
[92,     1] loss: 664.344
[93,     1] loss: 693.833
[94,     1] loss: 665.854
[95,     1] loss: 678.954
[96,     1] loss: 685.330
[97,     1] loss: 665.561
[98,     1] loss: 663.227
[99,     1] loss: 610.043
[100,     1] loss: 600.383
[101,     1] loss: 621.672
[102,     1] loss: 545.983
[103,     1] loss: 591.677
[104,     1] loss: 558.436
[105,     1] loss: 561.187
[106,     1] loss: 541.597
[107,     1] loss: 581.347
[108,     1] loss: 582.117
[109,     1] loss: 559.887
[110,     1] loss: 547.058
[111,     1] loss: 539.128
[112,     1] loss: 547.201
Early stopping applied (best metric=0.4052891433238983)
Finished Training
Total time taken: 17.692890644073486
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1271.955
[2,     1] loss: 1262.698
[3,     1] loss: 1262.233
[4,     1] loss: 1265.161
[5,     1] loss: 1262.266
[6,     1] loss: 1261.618
[7,     1] loss: 1258.705
[8,     1] loss: 1260.258
[9,     1] loss: 1265.461
[10,     1] loss: 1258.109
[11,     1] loss: 1257.998
[12,     1] loss: 1253.305
[13,     1] loss: 1254.326
[14,     1] loss: 1249.348
[15,     1] loss: 1245.862
[16,     1] loss: 1235.434
[17,     1] loss: 1227.147
[18,     1] loss: 1218.703
[19,     1] loss: 1206.564
[20,     1] loss: 1178.604
[21,     1] loss: 1165.100
[22,     1] loss: 1137.982
[23,     1] loss: 1136.296
[24,     1] loss: 1105.616
[25,     1] loss: 1096.668
[26,     1] loss: 1078.716
[27,     1] loss: 1091.174
[28,     1] loss: 1076.353
[29,     1] loss: 1048.581
[30,     1] loss: 1102.397
[31,     1] loss: 1062.288
[32,     1] loss: 1054.445
[33,     1] loss: 1056.143
[34,     1] loss: 1034.609
[35,     1] loss: 1027.170
[36,     1] loss: 1035.603
[37,     1] loss: 1003.750
[38,     1] loss: 1018.408
[39,     1] loss: 1009.398
[40,     1] loss: 976.452
[41,     1] loss: 945.257
[42,     1] loss: 1002.400
[43,     1] loss: 971.956
[44,     1] loss: 957.828
[45,     1] loss: 919.657
[46,     1] loss: 969.992
[47,     1] loss: 900.334
[48,     1] loss: 928.247
[49,     1] loss: 916.933
[50,     1] loss: 949.501
[51,     1] loss: 930.754
[52,     1] loss: 956.911
[53,     1] loss: 891.216
[54,     1] loss: 893.966
[55,     1] loss: 865.526
[56,     1] loss: 868.286
[57,     1] loss: 885.458
[58,     1] loss: 888.175
[59,     1] loss: 838.728
[60,     1] loss: 841.506
[61,     1] loss: 903.246
[62,     1] loss: 855.097
[63,     1] loss: 881.622
[64,     1] loss: 781.769
[65,     1] loss: 892.112
[66,     1] loss: 829.415
[67,     1] loss: 795.654
[68,     1] loss: 849.354
[69,     1] loss: 823.298
[70,     1] loss: 840.245
[71,     1] loss: 815.713
[72,     1] loss: 803.535
[73,     1] loss: 860.349
[74,     1] loss: 806.367
[75,     1] loss: 750.518
[76,     1] loss: 801.941
[77,     1] loss: 720.413
[78,     1] loss: 738.643
[79,     1] loss: 777.303
[80,     1] loss: 713.055
[81,     1] loss: 782.424
[82,     1] loss: 744.424
[83,     1] loss: 701.349
[84,     1] loss: 699.813
[85,     1] loss: 699.527
[86,     1] loss: 667.028
[87,     1] loss: 693.476
[88,     1] loss: 694.736
[89,     1] loss: 656.553
[90,     1] loss: 655.133
[91,     1] loss: 683.991
[92,     1] loss: 672.542
[93,     1] loss: 647.917
[94,     1] loss: 656.523
[95,     1] loss: 624.774
[96,     1] loss: 649.803
[97,     1] loss: 670.222
Early stopping applied (best metric=0.34217569231987)
Finished Training
Total time taken: 15.053606986999512
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1262.408
[2,     1] loss: 1259.181
[3,     1] loss: 1258.009
[4,     1] loss: 1258.904
[5,     1] loss: 1269.604
[6,     1] loss: 1256.676
[7,     1] loss: 1260.466
[8,     1] loss: 1254.890
[9,     1] loss: 1249.477
[10,     1] loss: 1245.183
[11,     1] loss: 1240.942
[12,     1] loss: 1225.548
[13,     1] loss: 1210.151
[14,     1] loss: 1205.793
[15,     1] loss: 1181.689
[16,     1] loss: 1146.414
[17,     1] loss: 1141.828
[18,     1] loss: 1107.944
[19,     1] loss: 1123.298
[20,     1] loss: 1066.827
[21,     1] loss: 1067.475
[22,     1] loss: 1098.633
[23,     1] loss: 1051.992
[24,     1] loss: 1081.828
[25,     1] loss: 1023.786
[26,     1] loss: 1059.481
[27,     1] loss: 1082.146
[28,     1] loss: 1040.125
[29,     1] loss: 1059.025
[30,     1] loss: 1037.046
[31,     1] loss: 1007.398
[32,     1] loss: 1019.600
[33,     1] loss: 988.027
[34,     1] loss: 1013.555
[35,     1] loss: 984.855
[36,     1] loss: 1017.736
[37,     1] loss: 981.010
[38,     1] loss: 961.366
[39,     1] loss: 973.023
[40,     1] loss: 999.402
[41,     1] loss: 988.295
[42,     1] loss: 1003.637
[43,     1] loss: 995.906
[44,     1] loss: 968.439
[45,     1] loss: 923.334
[46,     1] loss: 925.487
[47,     1] loss: 946.162
[48,     1] loss: 944.549
[49,     1] loss: 915.796
[50,     1] loss: 962.139
[51,     1] loss: 915.498
[52,     1] loss: 875.930
[53,     1] loss: 914.712
[54,     1] loss: 939.245
[55,     1] loss: 925.533
[56,     1] loss: 865.860
[57,     1] loss: 916.534
[58,     1] loss: 902.729
[59,     1] loss: 907.546
[60,     1] loss: 804.425
[61,     1] loss: 852.254
[62,     1] loss: 824.798
[63,     1] loss: 882.837
[64,     1] loss: 825.252
[65,     1] loss: 884.630
[66,     1] loss: 841.662
[67,     1] loss: 809.989
[68,     1] loss: 835.292
[69,     1] loss: 844.781
[70,     1] loss: 827.208
[71,     1] loss: 777.470
[72,     1] loss: 790.808
[73,     1] loss: 821.354
[74,     1] loss: 802.793
[75,     1] loss: 780.413
[76,     1] loss: 792.194
[77,     1] loss: 792.052
[78,     1] loss: 706.579
[79,     1] loss: 788.292
[80,     1] loss: 754.508
[81,     1] loss: 736.600
[82,     1] loss: 785.411
[83,     1] loss: 753.150
[84,     1] loss: 789.949
[85,     1] loss: 725.186
[86,     1] loss: 759.816
[87,     1] loss: 717.036
[88,     1] loss: 749.338
[89,     1] loss: 725.307
[90,     1] loss: 722.216
[91,     1] loss: 710.062
[92,     1] loss: 686.675
Early stopping applied (best metric=0.35611093044281006)
Finished Training
Total time taken: 14.8425874710083
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1310
[1,     1] loss: 1264.736
[2,     1] loss: 1262.378
[3,     1] loss: 1264.489
[4,     1] loss: 1259.122
[5,     1] loss: 1257.597
[6,     1] loss: 1255.832
[7,     1] loss: 1256.495
[8,     1] loss: 1248.473
[9,     1] loss: 1240.614
[10,     1] loss: 1230.120
[11,     1] loss: 1210.655
[12,     1] loss: 1193.956
[13,     1] loss: 1175.536
[14,     1] loss: 1145.185
[15,     1] loss: 1130.513
[16,     1] loss: 1113.960
[17,     1] loss: 1070.281
[18,     1] loss: 1068.402
[19,     1] loss: 1065.064
[20,     1] loss: 1033.276
[21,     1] loss: 1030.628
[22,     1] loss: 1007.122
[23,     1] loss: 1005.964
[24,     1] loss: 1070.645
[25,     1] loss: 1018.952
[26,     1] loss: 1040.343
[27,     1] loss: 1016.501
[28,     1] loss: 1019.108
[29,     1] loss: 1061.367
[30,     1] loss: 1009.984
[31,     1] loss: 1010.882
[32,     1] loss: 998.497
[33,     1] loss: 983.993
[34,     1] loss: 980.782
[35,     1] loss: 1019.220
[36,     1] loss: 1003.851
[37,     1] loss: 995.757
[38,     1] loss: 968.893
[39,     1] loss: 975.051
[40,     1] loss: 959.329
[41,     1] loss: 970.439
[42,     1] loss: 947.976
[43,     1] loss: 983.257
[44,     1] loss: 941.158
[45,     1] loss: 926.857
[46,     1] loss: 1008.851
[47,     1] loss: 949.475
[48,     1] loss: 955.294
[49,     1] loss: 903.399
[50,     1] loss: 902.350
[51,     1] loss: 919.405
[52,     1] loss: 908.306
[53,     1] loss: 912.527
[54,     1] loss: 938.284
[55,     1] loss: 933.032
[56,     1] loss: 855.246
[57,     1] loss: 844.810
[58,     1] loss: 918.597
[59,     1] loss: 906.062
[60,     1] loss: 879.787
[61,     1] loss: 885.332
[62,     1] loss: 863.063
[63,     1] loss: 850.925
[64,     1] loss: 863.119
[65,     1] loss: 819.373
[66,     1] loss: 842.758
[67,     1] loss: 793.604
[68,     1] loss: 872.886
[69,     1] loss: 814.349
[70,     1] loss: 847.172
[71,     1] loss: 863.252
[72,     1] loss: 805.912
[73,     1] loss: 846.156
[74,     1] loss: 878.078
[75,     1] loss: 772.630
[76,     1] loss: 800.809
[77,     1] loss: 808.997
[78,     1] loss: 759.897
[79,     1] loss: 832.421
[80,     1] loss: 740.834
[81,     1] loss: 812.940
[82,     1] loss: 804.192
[83,     1] loss: 756.043
[84,     1] loss: 744.284
[85,     1] loss: 744.453
[86,     1] loss: 746.645
[87,     1] loss: 719.634
[88,     1] loss: 740.024
[89,     1] loss: 727.271
[90,     1] loss: 652.331
[91,     1] loss: 751.927
[92,     1] loss: 724.309
[93,     1] loss: 664.777
[94,     1] loss: 670.142
[95,     1] loss: 671.138
[96,     1] loss: 683.564
[97,     1] loss: 675.367
[98,     1] loss: 610.504
[99,     1] loss: 636.299
[100,     1] loss: 630.888
[101,     1] loss: 687.673
Early stopping applied (best metric=0.35078495740890503)
Finished Training
Total time taken: 16.899807691574097
(48, 33)
(190, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-K/indices (238 samples)
(176, 33)
(819, 33)
Loaded folder code/Thesis/dataset/train/Hydroxylation-P/indices (995 samples)
386 304
1663 1312
[1,     1] loss: 1268.289
[2,     1] loss: 1263.554
[3,     1] loss: 1262.294
[4,     1] loss: 1264.613
[5,     1] loss: 1261.331
[6,     1] loss: 1261.324
[7,     1] loss: 1255.087
[8,     1] loss: 1252.545
[9,     1] loss: 1251.688
[10,     1] loss: 1241.185
[11,     1] loss: 1223.309
[12,     1] loss: 1208.413
[13,     1] loss: 1194.874
[14,     1] loss: 1175.592
[15,     1] loss: 1147.693
[16,     1] loss: 1125.045
[17,     1] loss: 1100.360
[18,     1] loss: 1082.135
[19,     1] loss: 1080.252
[20,     1] loss: 1097.164
[21,     1] loss: 1044.953
[22,     1] loss: 1073.287
[23,     1] loss: 1054.131
[24,     1] loss: 1035.448
[25,     1] loss: 1071.477
[26,     1] loss: 1066.849
[27,     1] loss: 1026.540
[28,     1] loss: 1015.425
[29,     1] loss: 1013.529
[30,     1] loss: 996.833
[31,     1] loss: 1029.186
[32,     1] loss: 968.587
[33,     1] loss: 1006.284
[34,     1] loss: 965.830
[35,     1] loss: 1024.003
[36,     1] loss: 1020.869
[37,     1] loss: 942.578
[38,     1] loss: 966.467
[39,     1] loss: 992.242
[40,     1] loss: 990.619
[41,     1] loss: 1008.575
[42,     1] loss: 932.627
[43,     1] loss: 941.579
[44,     1] loss: 929.738
[45,     1] loss: 987.859
[46,     1] loss: 965.968
[47,     1] loss: 953.744
[48,     1] loss: 955.692
[49,     1] loss: 920.678
[50,     1] loss: 922.151
[51,     1] loss: 910.275
[52,     1] loss: 911.652
[53,     1] loss: 905.510
[54,     1] loss: 929.517
[55,     1] loss: 920.121
[56,     1] loss: 861.844
[57,     1] loss: 849.550
[58,     1] loss: 873.761
[59,     1] loss: 870.933
[60,     1] loss: 855.239
[61,     1] loss: 888.585
[62,     1] loss: 898.771
[63,     1] loss: 842.579
[64,     1] loss: 796.690
[65,     1] loss: 823.882
[66,     1] loss: 812.882
[67,     1] loss: 810.013
[68,     1] loss: 854.487
[69,     1] loss: 820.859
[70,     1] loss: 860.733
[71,     1] loss: 801.511
[72,     1] loss: 798.062
[73,     1] loss: 786.278
[74,     1] loss: 808.646
[75,     1] loss: 753.650
[76,     1] loss: 760.073
[77,     1] loss: 741.921
[78,     1] loss: 746.884
Early stopping applied (best metric=0.38546231389045715)
Finished Training
Total time taken: 12.827373027801514
results!
{'gpu_mode': True, 'epochs': 200, 'batch_size': 2048, 'learning_rate': 0.0014784941977442072, 'test_data_ratio': 0.2, 'data_sample_mode': ['oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample', 'oversample'], 'crossValidation': True, 'loss_function': <class 'torch.nn.modules.loss.BCELoss'>, 'optimizer': <class 'torch.optim.adamw.AdamW'>, 'folds': 5, 'earlyStopping': True, 'ValidationMetric': 'Validation Loss (Hydroxylation-P)', 'earlyStoppingPatience': 50, 'CV_Repeats': 5, 'Experiment Name': 'Model architecture - added max, ranges, bceloss', 'CreateFigures': False, 'weight_decay': 0.24071392247151047, 'embeddingType': 'adaptiveEmbedding', 'LSTM_layers': 1, 'LSTM_hidden_size': 64, 'LSTM_dropout': 0, 'MultiTask': True, 'MultiTask_sample_method': 'balanced', 'UseUncertaintyBasedLoss': False, 'useLrWeight': True, 'CNNType': 'Musite', 'FCType': 'Adapt', 'layerToSplitOn': 'FC', 'dontAverageLoss': False, 'useWeightDecayWeight': False, 'SeperateTuningLRandWD': True, 'aminoAcid': ['Hydroxylation-K', 'Hydroxylation-P'], 'n_trials': 500, 'FloatsToTune': {'learning_rate': [1e-05, 0.01], 'weight_decay': [0, 10], 'log_base': [1.01, 3], 'learning_rate_Hydroxylation-K': [1e-05, 0.01], 'learning_rate_Hydroxylation-P': [1e-05, 0.01], 'weight_decay_Hydroxylation-P': [0, 10], 'weight_decay_Hydroxylation-K': [0, 10]}, 'IntsToTune': {}, 'log_base': 2.7004697798865878, 'learning_rate_Hydroxylation-K': 0.00230545299195537, 'learning_rate_Hydroxylation-P': 0.007241029954947504, 'weight_decay_Hydroxylation-P': 4.4382413728625, 'weight_decay_Hydroxylation-K': 9.94639752906256, 'random_state': 525940888, 'current_CV_Repeat': 5, 'sample_weights': [1.6817377010395873, 1], 'WeightDecayWeights': [], 'currentFold': 4}
{'Hydroxylation-K Validation Accuracy': 0.7435106382978723, 'Hydroxylation-K Validation Sensitivity': 0.6044444444444445, 'Hydroxylation-K Validation Specificity': 0.7789473684210526, 'Hydroxylation-K Validation Precision': 0.4234331866963446, 'Hydroxylation-K AUC ROC': 0.762187134502924, 'Hydroxylation-K AUC PR': 0.544951329782559, 'Hydroxylation-K MCC': 0.3439981994198638, 'Hydroxylation-K F1': 0.4907983969378226, 'Validation Loss (Hydroxylation-K)': 0.5145801317691803, 'Hydroxylation-P Validation Accuracy': 0.7871538297548348, 'Hydroxylation-P Validation Sensitivity': 0.7990158730158731, 'Hydroxylation-P Validation Specificity': 0.7846386353434086, 'Hydroxylation-P Validation Precision': 0.44782107878454497, 'Hydroxylation-P AUC ROC': 0.8411557006762005, 'Hydroxylation-P AUC PR': 0.5628624187367125, 'Hydroxylation-P MCC': 0.4804865758139276, 'Hydroxylation-P F1': 0.5719474739549573, 'Validation Loss (Hydroxylation-P)': 0.37591254472732544, 'Validation Loss (total)': 0.8904926872253418, 'TimeToTrain': 15.596867008209228}
{'Hydroxylation-K Validation Accuracy': 0.06622742929590408, 'Hydroxylation-K Validation Sensitivity': 0.11898058077191859, 'Hydroxylation-K Validation Specificity': 0.08146568254827505, 'Hydroxylation-K Validation Precision': 0.10871130361983156, 'Hydroxylation-K AUC ROC': 0.06868839076867274, 'Hydroxylation-K AUC PR': 0.10265391506254658, 'Hydroxylation-K MCC': 0.1300995477085895, 'Hydroxylation-K F1': 0.09604768463757597, 'Validation Loss (Hydroxylation-K)': 0.08840158160617599, 'Hydroxylation-P Validation Accuracy': 0.03528180102072279, 'Hydroxylation-P Validation Sensitivity': 0.0734167309059804, 'Hydroxylation-P Validation Specificity': 0.04309245469114382, 'Hydroxylation-P Validation Precision': 0.050754774946076914, 'Hydroxylation-P AUC ROC': 0.0334108287651031, 'Hydroxylation-P AUC PR': 0.08734411998624524, 'Hydroxylation-P MCC': 0.06644574244277093, 'Hydroxylation-P F1': 0.05019675902199398, 'Validation Loss (Hydroxylation-P)': 0.03733499149035212, 'Validation Loss (total)': 0.08954945601773977, 'TimeToTrain': 2.4557977134802567}
